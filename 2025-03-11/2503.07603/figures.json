[{"figure_path": "https://arxiv.org/html/2503.07603/x1.png", "caption": "Figure 1: An overview of our VLM pre-training data recipe. We investigate data mixes and design choices for text-only pre-training, image-text pre-training, and fine-tuning.\nNote that while we depict \"LLM Pre-training\" and \"Image-text Pre-training\" as two separate steps in this diagram, in practice, we continuously transition from the first stage to the second.", "description": "This figure illustrates the three-stage training process used for the Vision-Language Model (VLM).  The first stage involves text-only pre-training of a large language model (LLM), building a strong foundation in language understanding. This is followed by a second stage where image data is gradually introduced alongside text data for image-text pre-training.  The transition between text-only and image-text pre-training is continuous and not a discrete step. The final stage is fine-tuning, where the model is further optimized on a suite of vision-language and text-only tasks.  The figure visually depicts the different types of data used in each phase and highlights the continuous transition between text-only and image-text pre-training.", "section": "2 Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2503.07603/x2.png", "caption": "Figure 2: The commonly used framework we apply to add vision capabilities to a transformer model.", "description": "This figure illustrates a common architecture used to integrate vision into transformer-based language models.  It shows how an image encoder processes an image, converting it into a numerical representation. This representation is then passed through a Multi-Layer Perceptron (MLP), which transforms the data into a format compatible with the transformer model's text embeddings. Finally, the image and text embeddings are combined and processed by a decoder-only transformer to generate a comprehensive textual response based on the combined input.", "section": "2 EXPERIMENTAL SETUP"}, {"figure_path": "https://arxiv.org/html/2503.07603/x3.png", "caption": "Figure 3: Representation of the different learning rate schedules used for our experiments. \u2018Main schedule\u2019 corresponds to the learning rate for the initial, text-only pretraining. Other colored schedules are the ones used for image-text training and extend over 28B tokens each. They have been upscaled and appear as extending over 280B tokens for readability.", "description": "Figure 3 illustrates the learning rate schedules employed in the experiments. The \"Main Schedule\" represents the learning rate used during the initial text-only pre-training phase.  The colored lines depict the learning rate schedules for the subsequent image-text pre-training phases.  Each colored schedule covers 28 billion tokens. For better visualization and readability, the x-axis has been scaled up to represent 280 billion tokens, while maintaining the relative proportions of each schedule.", "section": "2.1 Training Procedure"}, {"figure_path": "https://arxiv.org/html/2503.07603/x14.png", "caption": "Figure 4: Varying the length of text-only pre-training. We analyze the impact of adding image data after varying amounts of text-only pre-training, showing results on vision benchmarks (green) and text benchmarks (blue). On the left, we show results across a suite of vision and text benchmarks; on the right, we plot two common benchmarks, VQA-v2 and ARC-easy. Introducing images at around 80% of the way through training performs best, maintaining high vision and text task performance. Note: The points at 100% are marked with hollow circles to highlight that they are trained with a different learning rate schedule, as shown in Figure 3", "description": "This figure displays the impact of varying the length of text-only pre-training before introducing image data during the training of a vision-language model (VLM).  The experiment involves training VLMs with different amounts of text-only pre-training (0%, 20%, 40%, 60%, 80%, and 100%) before incorporating image data. The results are evaluated on both vision-based and text-based benchmarks. The left panel shows the aggregate performance across multiple vision and text tasks, while the right panel focuses specifically on the VQA-v2 and ARC-easy benchmarks. The findings suggest that introducing image data around 80% of the way through text-only pre-training yields the best overall performance, maintaining strong performance on both vision and text tasks.  A key note is that the 100% text-only pre-training point uses a different learning rate schedule than the other points, as detailed in Figure 3 of the paper.", "section": "3.1 THE IMPACT OF AMOUNT OF TEXT-ONLY PRE-TRAINING"}, {"figure_path": "https://arxiv.org/html/2503.07603/x15.png", "caption": "Figure 5: Varying the ratio of image to text data, after some text-only pretraining.\nWe analyze the impact of the ratio of image to text data in pre-training, after the model has seen text-only data for most of pre-training (80%).\nUnlike when training from scratch (Figure\u00a06), we find that adding vision data significantly helps vision performance, while maintaining high text accuracy.", "description": "This figure analyzes the effect of varying the ratio of image to text data during the image-text pre-training phase of a vision-language model (VLM).  The experiment starts with a model already pre-trained on text-only data (80% completion).  The x-axis represents the percentage of image data included in the subsequent image-text pre-training phase, while the y-axis displays the model's performance on various vision-language and text-only tasks.  The key takeaway is that adding a small amount of image data during this pre-training stage significantly boosts vision-language performance without negatively impacting the model's proficiency on text-only tasks. This is in contrast to the findings shown in Figure 6, where the model is trained from scratch.", "section": "3.2 THE IMPACT OF ADDING IMAGES BEFORE THE END OF PRE-TRAINING"}, {"figure_path": "https://arxiv.org/html/2503.07603/x16.png", "caption": "Figure 6: Varying the ratio of image to text data, when training from scratch. We analyze the impact of the image-text ratio in pre-training from scratch without any language-only pre-training. Perhaps surprisingly, when training from scratch, adding vision data consistently hurts both vision and text performance, suggesting a period of language-only training early on is important for VLMs.", "description": "This figure presents the results of an experiment where vision-language models (VLMs) were trained from scratch with varying ratios of image and text data.  Unlike experiments where models were pre-trained with text before introducing images, this experiment reveals that adding visual data during initial training negatively impacts both vision and text-based task performance.  The results indicate that a period of initial language-only pre-training is crucial for optimal VLM performance.", "section": "3.3 THE IMPACT OF ADDING IMAGES WHEN TRAINING FROM SCRATCH"}, {"figure_path": "https://arxiv.org/html/2503.07603/x17.png", "caption": "Figure 7: Varying the proportion of instruction tuning data in the image mix. Is including instruction tuning data during pre-training is helpful for VLMs? Surprisingly, we find that adding this data to pre-training harms performance. We hypothesize that this may be due to overfitting, or because mixing instruction tuning data with image-caption pairs degrades learning at this scale.", "description": "This figure explores the effect of including instruction tuning data within the image-text pre-training phase on the performance of Vision Language Models (VLMs).  The experiment varied the proportion of instruction tuning data mixed with image caption data while keeping other training parameters constant.  The results surprisingly showed that including instruction tuning data during pre-training negatively impacts the model's performance on downstream tasks, contrary to expectations. The authors hypothesize this is due to potential overfitting or a disruption of the learning process caused by mixing instruction-tuning data with image-caption pairs at this model scale.", "section": "3.4 THE IMPACT OF INSTRUCTION TUNING DATA IN PRE-TRAINING"}, {"figure_path": "https://arxiv.org/html/2503.07603/x18.png", "caption": "Figure 8: \nVarying the number of fine-tuning epochs. We find that fine-tuning for 2-4 epochs after pre-training performs best for vision tasks, with 2 epochs being a sweet spot for maintaining text performance while achieving high vision performance.", "description": "This figure shows the impact of the number of fine-tuning epochs on the model's performance on both vision and text tasks. The x-axis represents the number of fine-tuning epochs, while the y-axis shows the stable score (aggregated performance across multiple tasks) for both vision and text.  The results indicate that fine-tuning for 2-4 epochs after the initial pre-training yields the best performance on vision tasks, with 2 epochs being optimal for maintaining a balance between vision and text performance.  Beyond 4 epochs, performance on both vision and text tasks starts to degrade, suggesting overfitting.", "section": "3.5 THE IMPACT OF FINE-TUNING ON VISION AND TEXT PERFORMANCE"}, {"figure_path": "https://arxiv.org/html/2503.07603/extracted/6268316/figures/tokenmults.png", "caption": "Figure 9: Evolution of the performance of the 1b model on vision benchmarks and text benchmarks as functions of the text-only pre-training completion.", "description": "This figure displays the performance of a 1-billion parameter model on several vision and text benchmarks, showing how performance changes based on the amount of text-only pre-training.  Specifically, it illustrates how performance evolves as the model progresses through its initial text-only training phase.  Each point on the graph represents the model's performance at different stages of text-only pre-training, expressed as the percentage of completion. The results highlight the interplay between the amount of text-only training and the model's ability to generalize to downstream tasks in both vision and language domains.", "section": "3.1 THE IMPACT OF AMOUNT OF TEXT-ONLY PRE-TRAINING"}]