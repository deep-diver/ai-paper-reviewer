[{"figure_path": "https://arxiv.org/html/2503.06520/x2.png", "caption": "Figure 1: Seg-Zero generates a reasoning chain before producing the final segmentation mask. It utilizes a pure reinforcement learning (RL) strategy, learning the reasoning process from zero. In comparison to supervised fine-tuning (SFT), the RL-based model demonstrates superior performance on both in-domain and out-of-domain data, and the integration of reasoning chain further enhances its effectiveness.", "description": "Seg-Zero, a novel reasoning segmentation model, generates a reasoning chain before producing the final segmentation mask.  Unlike traditional supervised fine-tuning (SFT) methods, Seg-Zero uses a pure reinforcement learning (RL) approach, learning the reasoning process from scratch. This RL-based method shows significant improvement in both in-domain and out-of-domain segmentation tasks compared to SFT, and the explicit reasoning chain further enhances its performance.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.06520/x3.png", "caption": "Figure 2: Illustration of our RL training process. In this case, the model generates three samples by itself, calculates the rewards, and optimizes towards samples that achieve higher rewards.", "description": "This figure illustrates the reinforcement learning process used to train the Seg-Zero model.  The model generates multiple segmentation mask proposals, each accompanied by a reasoning chain.  The reward function then assesses each proposal based on factors such as format correctness and accuracy (Intersection over Union or IoU). The model learns to generate higher-quality segmentation masks by optimizing for these reward signals.  The example shows three different proposals and their associated rewards.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.06520/x4.png", "caption": "Figure 3: Seg-Zero includes a reasoning model and a segmentation model. The reasoning model is a MLLM that generates a reasoning chain and provides segmentation prompts. Subsequently the segmentation model produces pixel-wise mask.", "description": "Seg-Zero uses a two-stage approach for reasoning-based image segmentation.  First, a multi-modal large language model (MLLM) acts as a reasoning model. It receives an image and a user prompt or question, then generates a reasoning chain that helps focus on the relevant object within the image and produces segmentation prompts.  These prompts include bounding box coordinates and pixel-level point locations that pinpoint the target object's precise boundaries. Second, a segmentation model takes these prompts as input and outputs a pixel-wise mask representing the segmented object.  The architecture demonstrates how reasoning and segmentation tasks can be decoupled, leveraging the strengths of large language models and specialized segmentation models for effective reasoning segmentation.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.06520/x5.png", "caption": "Figure 4: User prompt for Seg-Zero. \u2018{Question}\u2019 is replaced with object description \ud835\udc13\ud835\udc13\\mathbf{T}bold_T in the training and inference.", "description": "This figure details the user prompt used within the Seg-Zero model.  The prompt instructs the model to locate a specific object ('{Question}') within an image, providing the bounding box coordinates and two points for precise localization. The model is further guided to explain its reasoning process in detail (using <think> and </think> tags), concluding with the precise object coordinates (using <answer> and </answer> tags).  The '{Question}' placeholder is replaced with the actual object description, denoted as  'T', during both training and inference phases.  This structured prompt ensures the model generates detailed reasoning steps and accurately outputs the object location.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.06520/x6.png", "caption": "Figure 5: Visual QA task comparison. SFT suffers catastrophic forgetting, while RL preserves general Visual QA ability.", "description": "The figure compares the performance of models trained using supervised fine-tuning (SFT) and reinforcement learning (RL) on visual question answering (VQA) tasks.  It shows that SFT leads to catastrophic forgetting, where the model loses its ability to perform well on general VQA tasks after fine-tuning on a specific task. In contrast, the RL approach maintains general VQA capabilities, indicating its superior robustness and generalization ability.  The chart likely displays the accuracy or other relevant metrics on different VQA datasets for both SFT and RL-trained models, illustrating the difference in performance.", "section": "4.2. SFT vs. RL"}, {"figure_path": "https://arxiv.org/html/2503.06520/x7.png", "caption": "Figure 6: Qualitative Results on ReasonSeg [17]. The reason chain helps analyze user instructions and segment the correct objects.", "description": "Figure 6 showcases qualitative results from the ReasonSeg dataset.  It displays several examples of user queries and how the Seg-Zero model utilizes its reasoning chain to interpret complex instructions and accurately segment the target objects within images. The figure demonstrates the model's ability to reason through nuanced queries, handle multiple objects within a single query, and produce accurate segmentation masks despite ambiguity or complexity in the instructions.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2503.06520/x8.png", "caption": "Figure 7: Changes in completion length during training. Larger scale model tends to generate longer response.", "description": "This figure displays the change in the average and minimum length of model responses during training.  It illustrates how the length of generated text varies as the model learns, and particularly how model size affects response length.  Larger models (with more parameters) show a trend towards generating longer responses, particularly after the initial training stages. This may reflect increased reasoning capabilities or the increased complexity of the larger model's internal representations.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2503.06520/x9.png", "caption": "Figure 8: Changes in rewards during training. We show the mean value across a batch.", "description": "Figure 8 presents the mean reward values across batches during the training process of the Seg-Zero model.  The graph displays the trends of reward values over training iterations, illustrating the convergence of the different rewards used in reinforcement learning.  Specifically, it shows the trends for the 'Thinking Format Reward', 'Segmentation Format Reward', 'Bbox L1 Reward', and 'Points L1 Reward'. The changes in these reward values over time reflect how effectively the model learns to follow the format guidelines and achieve high accuracy in its segmentation tasks.", "section": "3.4 Training"}]