[{"figure_path": "https://arxiv.org/html/2503.06362/x1.png", "caption": "Figure 1: Training and inference stages for Llama-MTSK. (Left) During training, we produce audio-visual tokens via pre-trained encoders, followed by specific-scale compression and projection modules. Then, we feed the concatenated audio-visual tokens at multiple scales to the pre-trained Llama-based LLM, which is adapted through one of the three proposed LoRA\n\napproaches following the Matryoshka Representation Learning principle. (Right) At inference, Llama-MTSK allows us to change on-the-fly the audio-visual compression rates for each input data conditioned on our specific requirements using the same model architecture and weights, enabling high flexibility. Furthermore, only one projector and one LoRA module are activated at inference (in this figure, those associated with the audio and video compression rates equal to 3333), guaranteeing model\u2019s scalability in training and no extra cost in inference. \n\nand \n\nrepresent whether the parameters are trained or kept frozen, respectively.", "description": "This figure illustrates the training and inference stages of the Llama-MTSK model.  During training (left panel), audio and visual data are processed by pre-trained encoders. The resulting tokens are then compressed to multiple scales (different levels of granularity) and projected into a shared embedding space.  These multi-scale audio-visual tokens are fed into a Llama-based large language model (LLM), which is fine-tuned using one of three proposed LoRA (Low-Rank Adaptation) strategies based on the Matryoshka Representation Learning principle.  The right panel shows the inference stage.  Llama-MTSK allows dynamic adjustment of the audio-visual compression rate at inference time based on computational resource constraints.  Only the projector and LoRA modules corresponding to the chosen compression rate are activated, maintaining high efficiency. The color-coding in the figure indicates whether the parameters were trained or kept frozen.", "section": "2 Llama-MTSK"}, {"figure_path": "https://arxiv.org/html/2503.06362/x2.png", "caption": "Figure 2: Our three proposed LoRA Matryoshka (LoRA\n\n) approaches. Multi-Scale (MS) LoRA\n\nuses a shared global LoRA module for all the audio-visual token scales (in this specific example there are three scales) to fine-tune the pre-trained matrices of the LLM. The Specific-Scale (SS) variant defines a LoRA module tailored to each scale, learning and specializing to a specific scale. The third approach, Multi-Specific-Scale (MSS), combines MS and SS to support both global and specific-scale LoRAs. The global LoRA is responsible to capture relationships that can be shared among different-scale tokens, while specific-scale LoRAs learn tokens based on the specific scale.", "description": "This figure illustrates three different approaches for adapting a Large Language Model (LLM) using Low-Rank Adaptation (LoRA) within a Matryoshka framework.  The Matryoshka concept involves processing audio-visual data at multiple scales (granularities) simultaneously.  The three methods are:\n\n1. **Multi-Scale (MS) LoRA:** A single global LoRA module is used to fine-tune the LLM's parameters across all scales. This shares learned knowledge efficiently.\n2. **Specific-Scale (SS) LoRA:** Separate LoRA modules are trained for each scale, allowing for specialized learning at each level of detail.  This may improve accuracy but at higher computational cost.\n3. **Multi-Specific-Scale (MSS) LoRA:** This approach combines MS and SS by utilizing both a global and scale-specific LoRA modules. This offers a potential balance between efficiency and performance. The figure visually depicts the architecture of each approach and how it would be incorporated into the system for processing audio-visual data.", "section": "LLM Adaptation via LoRA"}, {"figure_path": "https://arxiv.org/html/2503.06362/x3.png", "caption": "Figure 3: WER results for the average pooling (left) and stacking (right) compression methods for the ASR task. We use Whisper Small as audio encoder and Llama 3.2-1B as LLM.", "description": "This figure displays the Word Error Rate (WER) results for the Automatic Speech Recognition (ASR) task, comparing two different compression methods: average pooling and stacking.  The results are shown across a range of compression rates.  The left panel shows WER for average pooling and the right panel for stacking.  Both methods utilize Whisper Small as the audio encoder and Llama 3.2-1B as the Large Language Model (LLM). This visualization helps to assess the impact of different compression methods and rates on the overall performance of the ASR system.", "section": "3.2 AVSR Main Results"}, {"figure_path": "https://arxiv.org/html/2503.06362/x4.png", "caption": "Figure 4: WER results for the average pooling (left) and stacking (right) compression methods for the VSR task. We use AVHuBERT Large as video encoder and Llama 3.2-3B as LLM.", "description": "Figure 4 presents the Word Error Rate (WER) results for the Visual Speech Recognition (VSR) task using two different compression methods: average pooling (left panel) and stacking (right panel).  The results are shown for various compression rates.  The experiment used AVHuBERT Large as the video encoder and Llama 3.2-3B as the Large Language Model (LLM). This figure showcases the impact of different compression strategies and rates on VSR performance, highlighting the trade-off between computational efficiency and accuracy.", "section": "3.3 Additional Results"}, {"figure_path": "https://arxiv.org/html/2503.06362/x5.png", "caption": "Figure 5: Additional WER results using stacking compression for the ASR task with {2222, 4444, 6666, 8888, 10101010} rates. We use the same configuration as in Figure 3.", "description": "Figure 5 presents additional Word Error Rate (WER) results for the Automatic Speech Recognition (ASR) task.  Unlike Figure 3, which used average pooling for compression, this figure shows results obtained using a stacking compression method.  The experiment explores a range of compression rates: {2, 4, 6, 8, 10}.  The goal is to demonstrate how different compression techniques and rates affect the performance of Llama-MTSK (the proposed model) in comparison to the Llama-AVSR baseline model.  The figure helps to assess the model's robustness and efficiency across varying levels of compression.", "section": "3.3 Additional Results"}, {"figure_path": "https://arxiv.org/html/2503.06362/x6.png", "caption": "Figure 6: Additional results for Llama\n\nusing stacking compression on the LRS3 dataset.", "description": "Figure 6 presents supplementary results for the Audio-Visual Speech Recognition (AVSR) task using the LRS3 dataset.  It specifically focuses on the performance of the Llama-based models (Llama MS, Llama SS, and Llama MSS) when employing a stacking compression method.  The figure compares these models against the Llama-AVSR baseline, which trains separate models for each audio-visual compression rate combination. This allows for a direct comparison of the performance and efficiency between the Matryoshka-based approach (capable of handling various compression ratios within a single model) and the conventional method (requiring separate models for each configuration). The x-axis represents different audio-visual compression rate combinations, and the y-axis shows the Word Error Rate (WER), a common metric to assess the accuracy of speech recognition systems.", "section": "3.3 Additional Results"}]