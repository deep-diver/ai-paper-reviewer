<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-03-11s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/</link><description>Recent content in 2025-03-11s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Mon, 10 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/index.xml" rel="self" type="application/rss+xml"/><item><title>DreamRelation: Relation-Centric Video Customization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07602/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07602/</guid><description>DreamRelation: Personalize videos by customizing relationships between subjects, generalizing to new domains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07602/cover.png"/></item><item><title>EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07027/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07027/</guid><description>EasyControl: Efficient &amp;amp; flexible control for Diffusion Transformers, enabling sophisticated image generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07027/cover.png"/></item><item><title>Effective and Efficient Masked Image Generation Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07197/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07197/</guid><description>eMIGM: A unified, efficient masked image generation model achieving state-of-the-art performance with fewer resources.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07197/cover.png"/></item><item><title>MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07459/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07459/</guid><description>MEDAGENTSBENCH: a new benchmark for assessing complex medical reasoning in LLMs, revealing performance gaps and cost-effective strategies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07459/cover.png"/></item><item><title>PE3R: Perception-Efficient 3D Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07507/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07507/</guid><description>PE3R: Achieves fast and accurate 3D scene reconstruction from 2D images by enhanced perception and efficiency.</description></item><item><title>SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07605/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07605/</guid><description>SEAP: Unlock LLM brainpower w/ training-free sparse expert activation pruning! Boost efficiency, keep accuracy. Optimize LLMs now!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07605/cover.png"/></item><item><title>Should VLMs be Pre-trained with Image Data?</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07603/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07603/</guid><description>Image data during pre-training can boost Vision-Language Model (VLM) performance, especially when introduced later in the process.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07603/cover.png"/></item><item><title>WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07265/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07265/</guid><description>WISE: Evaluates world knowledge in text-to-image generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07265/cover.png"/></item><item><title>Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.06362/</link><pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.06362/</guid><description>Llama-MTSK: AVSR via Matryoshka LLMs, adapting to computational limits without sacrificing accuracy!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.06362/cover.png"/></item><item><title>DiffCLIP: Differential Attention Meets CLIP</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.06626/</link><pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.06626/</guid><description>DiffCLIP: Enhancing CLIP models by integrating differential attention, achieving superior performance with minimal overhead.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.06626/cover.png"/></item><item><title>Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.06520/</link><pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.06520/</guid><description>Seg-Zero: Cognitive Reinforcement for Reasoning-Chain Guided Segmentation!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.06520/cover.png"/></item><item><title>BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement for Transformers in Large-Scale Time Series Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.06121/</link><pubDate>Sat, 08 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.06121/</guid><description>Rimer: RWKV-7 empowers superior time series modeling, offering a simple yet effective alternative to Transformers with fewer parameters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.06121/cover.png"/></item><item><title>WritingBench: A Comprehensive Benchmark for Generative Writing</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.05244/</link><pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.05244/</guid><description>WritingBench: A new benchmark for generative writing evaluation, enhancing LLMs across diverse domains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.05244/cover.png"/></item><item><title>Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.04973/</link><pubDate>Thu, 06 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.04973/</guid><description>Task-aware KV cache compression enables efficient knowledge reasoning in LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.04973/cover.png"/></item><item><title>SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.04629/</link><pubDate>Thu, 06 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.04629/</guid><description>SURVEYFORGE automates survey generation, improving quality and evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.04629/cover.png"/></item><item><title>Words or Vision: Do Vision-Language Models Have Blind Faith in Text?</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.02199/</link><pubDate>Tue, 04 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.02199/</guid><description>VLMs often disproportionately trust text over visual data, leading to performance drops and safety concerns.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.02199/cover.png"/></item></channel></rss>