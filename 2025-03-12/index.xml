<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-03-12s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/</link><description>Recent content in 2025-03-12s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Tue, 11 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/index.xml" rel="self" type="application/rss+xml"/><item><title>$^R$FLAV: Rolling Flow matching for infinite Audio Video generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08307/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08307/</guid><description>RFLAV: A novel rolling flow matching model for infinite audio-video generation with high quality, synchronization, and temporal coherence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08307/cover.png"/></item><item><title>AI-native Memory 2.0: Second Me</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08102/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08102/</guid><description>AI-native memory 2.0 presents second me, an AI system for personal knowledge management.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08102/cover.png"/></item><item><title>AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08417/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08417/</guid><description>AnyMoLe: Generate character motion in-between frames for diverse characters by video diffusion models without external data. Code: project page.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08417/cover.png"/></item><item><title>BiasEdit: Debiasing Stereotyped Language Models via Model Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08588/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08588/</guid><description>BIASEDIT: Efficiently debiasing language models via lightweight network edits!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08588/cover.png"/></item><item><title>LightGen: Efficient Image Generation through Knowledge Distillation and Direct Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08619/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08619/</guid><description>LightGen: Efficient image generation via knowledge distillation and direct preference optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08619/cover.png"/></item><item><title>NullFace: Training-Free Localized Face Anonymization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08478/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08478/</guid><description>NullFace: A training-free face anonymization method preserving non-identity attributes with localized control using latent diffusion inversion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08478/cover.png"/></item><item><title>OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08686/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08686/</guid><description>OmniMamba: Efficient multimodal understanding and generation via SSMs, trained on 2M image-text pairs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08686/cover.png"/></item><item><title>Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08684/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08684/</guid><description>PLM retrievers overrate low-perplexity docs, causing source bias. This paper reveals the causal effect &amp;amp; offers a fix!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08684/cover.png"/></item><item><title>QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long Video Comprehension</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08689/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08689/</guid><description>QuoTA: Task-aware token assignment boosts long video comprehension in LVLMs via query-decoupled processing, without extra training!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08689/cover.png"/></item><item><title>Referring to Any Person</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08507/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08507/</guid><description>Introducing HumanRef, a new dataset &amp;amp; RexSeek, a multimodal LLM, to improve human-centric referring tasks by addressing limitations of existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08507/cover.png"/></item><item><title>SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08625/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08625/</guid><description>SegAgent: Improves MLLMs&amp;rsquo; pixel understanding by mimicking human annotation, enabling mask refinement without altering output space.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08625/cover.png"/></item><item><title>Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08605/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08605/</guid><description>SynCoS: Synchronized sampling generates high-quality &amp;amp; coherent long videos from text, without extra training!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08605/cover.png"/></item><item><title>Uni$ extbf{F}^2$ace: Fine-grained Face Understanding and Generation with Unified Multimodal Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08120/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08120/</guid><description>UniFace: a novel UMM tailored for fine-grained face understanding and generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08120/cover.png"/></item><item><title>Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.07572/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.07572/</guid><description>LLMs can now reason more efficiently!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.07572/cover.png"/></item><item><title>RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow Trajectories</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.07699/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.07699/</guid><description>RayFlow: Accelerating diffusion with instance-aware adaptive flow, boosting speed &amp;amp; quality!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.07699/cover.png"/></item><item><title>Seedream 2.0: A Native Chinese-English Bilingual Image Generation Foundation Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.07703/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.07703/</guid><description>Seedream 2.0: A native Chinese-English bilingual image generation model that understands cultural nuances and excels in text rendering.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.07703/cover.png"/></item><item><title>Video Action Differencing</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.07860/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.07860/</guid><description>VidDiff: Identify subtle action differences in videos for coaching and skill learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.07860/cover.png"/></item><item><title>Beyond Decoder-only: Large Language Models Can be Good Encoders for Machine Translation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.06594/</link><pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.06594/</guid><description>LLMs as MT encoders enhance efficiency &amp;amp; generalization!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.06594/cover.png"/></item><item><title>VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large Vision-Language Models in Fact-Seeking Question Answering</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.06492/</link><pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.06492/</guid><description>VisualSimpleQA: A new benchmark for fine-grained evaluation of visual and linguistic modules in fact-seeking LVLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.06492/cover.png"/></item><item><title>Benchmarking AI Models in Software Engineering: A Review, Search Tool, and Enhancement Protocol</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.05860/</link><pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.05860/</guid><description>This paper reviews AI4SE benchmarks, introduces BenchScout for benchmark discovery, and proposes BenchFrame for benchmark enhancement, demonstrated via HumanEvalNext.</description></item><item><title>MagicInfinite: Generating Infinite Talking Videos with Your Words and Voice</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.05978/</link><pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.05978/</guid><description>MagicInfinite: Infinite talking videos from words and voice!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.05978/cover.png"/></item><item><title>Mixture of Experts Made Intrinsically Interpretable</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.07639/</link><pubDate>Wed, 05 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.07639/</guid><description>MoE-X: An intrinsically interpretable Mixture-of-Experts language model that uses sparse, wide networks to enhance transparency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.07639/cover.png"/></item></channel></rss>