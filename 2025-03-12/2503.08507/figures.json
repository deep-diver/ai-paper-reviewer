[{"figure_path": "https://arxiv.org/html/2503.08507/x2.png", "caption": "Figure 1: We introduce referring to any person, a task that requires detecting all individuals in an image which match a given natural language description, and a new model RexSeek designed for this task with strong perception and understanding capabilities that effectively captures attributes, spatial relations, interactions, reasoning, celebrity recognition, etc.", "description": "Figure 1 showcases the \"Referring to Any Person\" task, a new computer vision challenge.  The task involves identifying every person in an image that matches a given natural language description. The figure displays a wide variety of example images and their corresponding descriptions, highlighting the complexity and diversity of the task.  These descriptions range from simple attributes (e.g., \"person in blue\") to complex spatial relationships (e.g., \"5th person from the right\") and even include celebrity recognition (e.g., \"Elon Musk\").  The figure also introduces RexSeek, a novel model specifically designed to tackle this challenge, demonstrating its ability to effectively capture various attributes, spatial relationships, interactions, and reasoning involved in accurately identifying the described individuals.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.08507/x3.png", "caption": "Figure 2: Visualization results of Qwen2.5-VL\u00a0[3], InternVL-2.5\u00a0[14], and DeepSeek-VL2\u00a0[70] on the human referring task. Despite achieving strong results on referring benchmarks RefCOCO/+/g\u00a0[50, 75], state-of-the-art models struggle when tasked with identifying multiple individuals as they output an insufficient number of bounding boxes.", "description": "Figure 2 presents a comparison of three state-of-the-art models (Qwen2.5-VL, InternVL-2.5, and DeepSeek-VL2) performance on a human referring task.  The models are shown to successfully identify single individuals in images,  as evidenced by their good performance on the RefCOCO+/g benchmark. However, when the task requires identifying multiple individuals within a single image based on a natural language description, these same models often fail. This failure is attributed to an insufficient number of bounding boxes produced by the models, indicating a difficulty in detecting all relevant individuals within the image.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.08507/x4.png", "caption": "Figure 3: Overview of the mannual annotation pipeline of the HumanRef dataset.", "description": "This figure illustrates the manual annotation process used to create the HumanRef dataset.  The process involves three main steps: (1) generating a structured property dictionary using a large language model (LLM) to list potential properties for each individual in an image; (2) assigning these properties to the corresponding individuals; and (3) using the LLM to translate these structured property assignments into natural language referring expressions. The figure visualizes these three steps, showing how properties are extracted, assigned, and converted into the final annotations.", "section": "3. HumanRef Dataset"}, {"figure_path": "https://arxiv.org/html/2503.08507/x5.png", "caption": "Figure 4: Visualization of the six subsets in the HumanRef Benchmark.", "description": "This figure visualizes the six subsets of the HumanRef benchmark dataset. Each subset focuses on a specific aspect of referring to people in images: attributes (describing characteristics like gender, age, clothing), position (spatial relationships between people and the environment), interaction (actions between people or with objects), reasoning (multi-step inferences to identify individuals), celebrity recognition (identifying famous people), and rejection (handling cases where a referred person isn't present). Each subfigure shows example images and annotations illustrating the type of data and complexities within each subset.", "section": "3. HumanRef Dataset"}, {"figure_path": "https://arxiv.org/html/2503.08507/x6.png", "caption": "Figure 5: Distribution of the number of individuals per image and the number of individuals referenced by each referring expression.", "description": "This figure presents two histograms visualizing the distribution of the number of individuals present in each image of the HumanRef dataset and the number of individuals referenced by each referring expression within the dataset.  The first histogram shows how many images contain a certain number of people (e.g., the number of images with 1 person, 2 people, 3 people, and so on). The second histogram depicts the distribution of the number of individuals referenced within each referring expression in the dataset.  This provides insights into the dataset's complexity and challenges regarding the multi-instance nature of the referring task.", "section": "3. HumanRef Dataset"}, {"figure_path": "https://arxiv.org/html/2503.08507/x7.png", "caption": "Figure 6: Overview of the RexSeek model. RexSeek is a retrieval-based model built upon ChatRex\u00a0[25]. By integrating a person detection model, RexSeek transforms the referring task from predicting box coordinates to retrieving the index of input boxes.", "description": "RexSeek is a retrieval-based model that leverages a person detection model and a large language model (LLM).  Instead of directly predicting bounding box coordinates for referring expressions, RexSeek transforms the referring task into a retrieval problem. The model first uses a person detector to identify individuals in an image, generating corresponding object tokens. These tokens, along with vision tokens from the image and text tokens from the referring expression, are input to the LLM. The LLM then outputs a sequence of object indices that directly correspond to the individuals matching the referring expression.", "section": "4. RexSeek Model"}, {"figure_path": "https://arxiv.org/html/2503.08507/x8.png", "caption": "Figure 7: Visualizing the trend of recall and precision variations across different models as the number of instances corresponding to each referring expression increases.", "description": "This figure presents a comparison of the performance of various multi-modal models on the HumanRef benchmark, specifically focusing on how their recall and precision change as the number of instances associated with each referring expression increases.  The x-axis represents the number of instances (individuals) referenced by a single expression, categorized into ranges (1, 2-5, 6-10, >10). The y-axis displays the recall and precision scores (in percentage).  The different lines represent different models, illustrating their performance across this range of complexities.", "section": "5. Experiments"}]