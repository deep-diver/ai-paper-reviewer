[{"figure_path": "https://arxiv.org/html/2503.08625/extracted/6271622/framework.png", "caption": "Figure 1: The overall framework of SegAgent. The image below shows a complete set of trajectories. We visualize current action atsubscript\ud835\udc4e\ud835\udc61a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and the resulting mask\nMt+1subscript\ud835\udc40\ud835\udc611M_{t+1}italic_M start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT in one image.", "description": "SegAgent is a model that imitates human annotators using interactive segmentation tools.  This figure illustrates the SegAgent framework.  An image and a text prompt are input into the MLLM (multimodal large language model), which then iteratively generates text-based coordinates representing clicks (positive or negative) for a segmentation tool. The segmentation tool updates a mask based on the clicks.  The process continues until a satisfactory mask is produced. The lower part of the figure shows a sequence of iterations, with each step visualizing both the current action (click coordinates) and the resulting mask. The goal is to evaluate the MLLM's pixel-level comprehension by assessing its ability to generate high-quality masks through this iterative process.", "section": "4. SegAgent"}, {"figure_path": "https://arxiv.org/html/2503.08625/extracted/6271622/trace.png", "caption": "Figure 2: An example of generated trajectory. We visualize current action atsubscript\ud835\udc4e\ud835\udc61a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and the resulting mask\nMt+1subscript\ud835\udc40\ud835\udc611M_{t+1}italic_M start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT in one image. Due to the noise from GT Mask, the action for Iteration 3,4 is meaningless", "description": "This figure visualizes a generated trajectory from an automated algorithm that simulates human-like mask annotation.  Each image shows the mask at a particular iteration, along with the current action (a positive or negative click) represented by a point on the image.  Iteration 0 begins with an empty mask, and subsequent iterations show how the mask is refined based on the sequential actions. The trajectory generation is based on the ground truth (GT) mask; however, the GT mask contains noise, resulting in the last two actions (Iteration 3,4) being meaningless and not contributing to the overall mask refinement.  The visualization helps illustrate the process of iterative mask creation using simulated human actions.", "section": "4.1. Human Annotator Trajectories Generation"}, {"figure_path": "https://arxiv.org/html/2503.08625/extracted/6271622/noc.png", "caption": "Figure 3: Comparison of dataset complexity.", "description": "The bar chart visualizes the complexity of three datasets: DIS5K, ThinObject5K, and refcoco.  The complexity is measured by the average number of clicks needed to achieve 80% and 85% Intersection over Union (IoU) during interactive image segmentation.  The chart clearly shows that DIS5K and ThinObject5K require significantly more clicks than refcoco, indicating a higher complexity level in the former two datasets. This difference is due to the inherent characteristics of the datasets, such as the presence of more intricate object boundaries and finer-grained details in DIS5K and ThinObject5K compared to refcoco. Therefore, the datasets DIS5K and ThinObject5K are more challenging and appropriate for evaluating the pixel-level understanding capabilities of a model.", "section": "5.2. Datasets"}, {"figure_path": "https://arxiv.org/html/2503.08625/extracted/6271622/hres.png", "caption": "Figure 4: Comparison of different strategies on the HRES dataset.", "description": "The bar chart compares the performance of different strategies for image segmentation on the HRES dataset.  The baseline strategy uses a model fine-tuned with supervised fine-tuning (SFT) and a fixed-step greedy decoding method during testing.  The chart then shows performance improvements achieved by incorporating policy improvement methods (StaR+), process reward modeling (PRM), and the addition of tree search.  The improvements are shown for both the DIS and ThinObject subsets of the HRES dataset, illustrating the effectiveness of each method in enhancing segmentation accuracy in complex scenarios.", "section": "5.4. Exploration on More Complex Scenarios"}, {"figure_path": "https://arxiv.org/html/2503.08625/extracted/6271622/tree2.png", "caption": "Figure 5: An illustrative example of PRM-guided tree search. The model predicts the reward at each step and selects the action with the highest reward to generate the next mask.", "description": "This figure illustrates the process of PRM-guided tree search used in SegAgent for interactive image segmentation.  The model begins with an initial mask and image. At each step, it generates multiple candidate actions (positive or negative clicks). It uses a PRM (Process Reward Model) to predict the reward (measured by IoU) for each action, selecting the action with the highest predicted reward. The chosen action is then input to the interactive segmentation tool to update the mask.  This iterative process repeats until a satisfactory mask is produced or a predetermined number of steps is reached. The visualization shows the progression of the mask at each step and the corresponding predicted rewards.", "section": "4.4. Process Reward Model and Tree Search"}, {"figure_path": "https://arxiv.org/html/2503.08625/x1.png", "caption": "Figure 6: The prompt provides detailed instructions for refining a segmentation mask with three possible actions: adding a positive point, adding a negative point. The red part indicates user-specific input, such as object descriptions.", "description": "Figure 6 shows a detailed example of the text prompt used to guide the model during the mask refinement process in the interactive segmentation task. The prompt provides clear instructions on how to refine a mask using three actions: adding a positive point (to expand the mask), adding a negative point (to shrink the mask), and generating additional information (optional). The red text section is where user-specific inputs, such as the object description, are included to ensure the model's understanding of the task. This prompt design is crucial for enabling the model to learn the human-like annotation process.", "section": "3. Preliminaries and Task Formulation"}, {"figure_path": "https://arxiv.org/html/2503.08625/x2.png", "caption": "Figure 7: Examples of Images and Annotations from Various Datasets. The figure showcases representative samples from three datasets: ThinObject5k-TE, DIS5K, and RefCOCO. Each row represents a dataset, with images and corresponding annotations highlighting different objects and scenes. The annotations (green overlays) demonstrate the varying levels of detail and complexity across datasets.", "description": "This figure visually compares the quality and complexity of annotations across three datasets: ThinObject5k-TE, DIS5K, and RefCOCO. Each row displays example images from one dataset, with their corresponding segmentation masks overlaid in green.  The differences in mask detail and accuracy highlight how the complexity of the segmentation task varies between datasets. This is important for evaluating the performance of the proposed method, as more complex and accurate datasets better assess the fine-grained pixel-level understanding capabilities of the model.", "section": "5.2. Datasets"}]