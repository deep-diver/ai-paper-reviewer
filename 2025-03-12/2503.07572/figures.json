[{"figure_path": "https://arxiv.org/html/2503.07572/x1.png", "caption": "Figure 1: Standard outcome-reward reinforcement fine-tuning vs. our meta reinforcement fine-tuning (MRT). Standard techniques for fine-tuning LLMs to use test-time compute optimize outcome reward at the end of a long trace. This does not incentivize the model to make use of intermediate tokens to make progress (i.e., probability of eventual success) and leads to 1) unnecessarily long output traces and 2) inability to make steady progress on new, hard problems as shown in (a). MRT, shown in (b), trains the LLM to minimize cumulative regret over the entire output stream (red, shaded area) by optimizing a dense reward function in addition to sparse 0/1 reward and thus alleviates both challenges in (a).", "description": "This figure compares two approaches for fine-tuning large language models (LLMs) to optimize their use of computation during reasoning.  The standard approach (a) uses a sparse reward at the end of the process, which leads to unnecessarily long reasoning chains and inconsistent progress.  The new method, MRT (b), uses a dense reward function to incentivize progress throughout the reasoning process.  The shaded area represents the cumulative regret, which MRT aims to minimize. This allows for more efficient use of computation and more consistent progress, even on difficult problems.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.07572/extracted/6256816/figures/teaser_horizontal.png", "caption": "Figure 2: MRT uses dense rewards based on progress throughout the thinking trace (segmented into \u201cepisodes\u201d) to improve final performance and test-time efficiency. Standard fine-tuning only trains models with outcome rewards at the end, thus reinforcing several traces that make subpar progress but somehow succeed (Figure\u00a01(a)).", "description": "Figure 2 illustrates the core idea of Meta Reinforcement Fine-Tuning (MRT).  Unlike standard fine-tuning which only provides a reward at the end of a reasoning process (a 0/1 outcome reward for success or failure), MRT provides dense (continuous) rewards at each step of the process. This is represented as a progress reward, calculated using the change in the likelihood of success after each step, or episode.  The figure demonstrates the difference by contrasting standard outcome reward RL training (a), where long traces might include multiple steps with subpar progress, and MRT (b) which provides dense rewards and minimizes the cumulative regret. This design helps to encourage efficient use of test-time compute, ensuring steady progress toward a solution and avoiding redundant or inefficient steps.", "section": "4. Problem Formulation: Optimizing Test-Time Compute as Meta RL"}, {"figure_path": "https://arxiv.org/html/2503.07572/x2.png", "caption": "Figure 3: R1 scaling curve on Omni-MATH subset. We compare scaling up test-time compute with direct pass@k for k = 1, \u22ef\u22ef\\cdots\u22ef, 32 and [maj@p]jsubscriptdelimited-[]maj@p\ud835\udc57[\\textbf{maj@p}]_{j}[ maj@p ] start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT for p = 1, 2, 4, 8. Each blue point combines 5 episodes together.", "description": "This figure analyzes how the performance of the DeepSeek-R1 model scales with increasing test-time compute budget on the Omni-MATH dataset.  It compares three approaches: 1) directly generating answers ('direct'), 2) using a single episode with majority voting over multiple runs ('[maj@1]j'), and 3) using multiple episodes with majority voting over multiple runs ('[maj@p]j'). The x-axis represents the test-time token budget, and the y-axis represents the accuracy. The 'direct' approach serves as a baseline. The '[maj@1]j' approach evaluates the model's ability to generate a correct answer within a single episode, and '[maj@p]j' further evaluates its capability to improve the accuracy with multiple episodes. Each blue point on the graph represents an average over 5 episodes.", "section": "5. Case Study: Analyzing SoTA DeepSeek-R1"}, {"figure_path": "https://arxiv.org/html/2503.07572/x3.png", "caption": "Figure 4: \nExplore/exploit spectrum. Final reward RL does not reward intermediate episodes encouraging unstructured exploration, whereas SCoRe\u00a0[23, 33] constrains each episode based on its outcome reward making it too exploitative. MRT strikes a balance by assigning an information gain based reward which aims to make progress in a budget-agnostic setting.", "description": "The figure illustrates the tradeoff between exploration and exploitation in reinforcement learning algorithms for large language models (LLMs).  Standard RL methods, focusing solely on the final outcome, incentivize lengthy, unstructured explorations. In contrast, methods like SCoRe prioritize immediate rewards, leading to overly exploitative behavior.  MRT, in contrast, seeks a balance. It uses a dense reward bonus that quantifies progress made towards the eventual solution, regardless of the test-time compute budget. This approach encourages exploration of promising paths while discouraging the wasteful expenditure of tokens on unproductive directions.", "section": "6. The Meta Reinforcement Finetuning (MRT) Paradigm"}, {"figure_path": "https://arxiv.org/html/2503.07572/extracted/6256816/figures/mrt_settings.png", "caption": "Figure 5: \nThe two settings we study. Left: open-ended parametrization. The model uses explicit thinking markers (<think> and </think>) to work through a problem with multiple strategies. Right: backtracking search. The model directly solves the problem with a step-by-step solution. In each episode, the model identifies errors at specific steps and backtracks to correct them (returning to step 3, then later to step 7) until reaching the correct answer.", "description": "This figure illustrates two different approaches to prompting LLMs for complex reasoning tasks. The left panel depicts the \"open-ended parametrization\", where the model uses explicit thinking markers (<think> and </think>) to explore multiple solution strategies within a given reasoning chain.  The model is free to make progress or regress. The right panel shows the \"backtracking search\" approach. Here, the model directly tackles the problem step-by-step. Importantly, each episode involves error detection; if an error is found, the model backtracks to a previous step to correct the mistake before proceeding further, effectively refining its solution iteratively.", "section": "7. Practical Instantiations: Dense Rewards for Optimizing Test-Time Compute"}, {"figure_path": "https://arxiv.org/html/2503.07572/extracted/6256816/figures/MRT-overview.png", "caption": "Figure 6: MRT implementation. Left: The STaR variant begins by generating a complete rollout for each query \ud835\udc31\ud835\udc31\\mathbf{x}bold_x sampled from dataset \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}_{\\mathrm{train}}caligraphic_D start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT. Then, MRT segments thinking traces into distinct episodes \ud835\udc33jsubscript\ud835\udc33\ud835\udc57\\mathbf{z}_{j}bold_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT akin to our analysis in Section\u00a05. For each prefix \ud835\udc330:jsubscript\ud835\udc33:0\ud835\udc57\\mathbf{z}_{0:j}bold_z start_POSTSUBSCRIPT 0 : italic_j end_POSTSUBSCRIPT, we estimate reward Jr(\u03bc(\u22c5|\ud835\udc330:j,\ud835\udc31))J_{r}(\\mu(\\cdot|\\mathbf{z}_{0:j},\\mathbf{x}))italic_J start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_\u03bc ( \u22c5 | bold_z start_POSTSUBSCRIPT 0 : italic_j end_POSTSUBSCRIPT , bold_x ) ) by evaluating the average accuracy of solutions produced after terminating the thought block at this prefix. After computing rewards across all prefixes, we calculate progress rprg\u03bc\u2062(\ud835\udc330:j;x)subscriptsuperscript\ud835\udc5f\ud835\udf07prgsubscript\ud835\udc33:0\ud835\udc57\ud835\udc65r^{\\mu}_{\\mathrm{prg}}(\\mathbf{z}_{0:j};x)italic_r start_POSTSUPERSCRIPT italic_\u03bc end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_prg end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT 0 : italic_j end_POSTSUBSCRIPT ; italic_x ) using Definition\u00a06.1. The STaR variant selectively retains only reasoning traces that maximize progress and are also followed by correct solutions once thinking terminates. Right: The RL variant initiates by generating a partial rollout for each query \ud835\udc31\ud835\udc31\\mathbf{x}bold_x sampled from \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}_{\\mathrm{train}}caligraphic_D start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT, terminating after a random number of episodes. Then it generates m\ud835\udc5amitalic_m on-policy rollouts that terminate reasoning at the prefix and immediately produce final solutions as well as rollouts that continue reasoning. Normalizing rewards across this set of traces allows us to implicitly compute the progress bonus. Finally, we update the policy with an aggregation of this dense reward and the final 0/1 outcome reward.", "description": "This figure illustrates the implementation details of the Meta Reinforcement Fine-Tuning (MRT) algorithm.  The left panel shows the STaR (Successor-based Reinforcement Learning) variant, which starts by generating a complete reasoning trace for each problem.  This trace is then divided into episodes, and a reward is calculated based on the progress made in each episode toward a correct solution, using the progress metric defined in Definition 6.1.  Only traces that maximize progress and lead to a correct solution are kept. The right panel shows the RL (Reinforcement Learning) variant.  Here, partial rollouts are generated, and multiple additional rollouts are simulated: some that stop at the partial rollout and produce a final answer, and some that continue reasoning. The rewards are normalized, implicitly calculating a progress bonus, and the policy is updated using both the progress reward and the final 0/1 outcome reward.", "section": "6. The Meta Reinforcement Finetuning (MRT) Paradigm"}, {"figure_path": "https://arxiv.org/html/2503.07572/extracted/6256816/figures/training_completion_length_deepscale.png", "caption": "Figure 7: MRT (RL and STaR) results on DeepSeek-R1-Distill-Qwen-1.5B. We plot maj@k performance of models for k = 1, 2, \u2026, 10 on AIME 2024 (left) and MATH500 (right). The orange lines correspond to MRT and the green lines correspond to outcome-reward training, with \u2605\u2605\\bigstar\u2605 denoting RL and \u2219\u2219\\bullet\u2219 denoting STaR / SFT training.", "description": "Figure 7 displays the results of applying Meta Reinforcement Fine-tuning (MRT) using both reinforcement learning (RL) and self-training with reinforcement (STaR) to the DeepSeek-R1-Distill-Qwen-1.5B language model.  The performance metric used is maj@k, which represents the majority vote accuracy across k independent attempts at solving problems.  The plots show maj@k performance for k values ranging from 1 to 10 on two different benchmarks: AIME 2024 (left panel) and MATH500 (right panel). Orange lines represent the performance achieved with MRT, while green lines depict the results using standard outcome-reward training methods.  Within each panel, solid lines represent RL results, and dashed lines denote the STaR/SFT results. This figure directly illustrates MRT\u2019s effectiveness in improving accuracy and token efficiency compared to traditional outcome-reward training.", "section": "Experimental Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.07572/x4.png", "caption": "Figure 8: Left: MRT (STaR) with 8B base. We plot maj@K performance of models on AIME for K \u2208[1,10]absent110\\in[1,10]\u2208 [ 1 , 10 ] against the total tokens spent. We also run linearized search (dashed line) for MRT (rest are parallel). Right: MRT (RL) with 3B base. Similarly to the left plot, we report maj@K against the total tokens spent.", "description": "This figure compares the performance of different models on the AIME benchmark.  The left panel shows the results for the MRT (STaR) model with an 8B base, illustrating how its performance (maj@K) changes with increasing token usage, comparing parallel and linearized search methods. The right panel presents similar results for the MRT (RL) model with a 3B base. This figure visually demonstrates the trade-off between model accuracy and computational cost for various models and search strategies.", "section": "8.4. Linearized Evaluations in the Backtracking Search Setting"}, {"figure_path": "https://arxiv.org/html/2503.07572/x5.png", "caption": "Figure 9: Normalized regret of different algorithms at different deployment @token budgets C0subscriptC0C_{0}italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. The first four points are at budgets 4096, 8192, 12288, and 16384. The next four points in dashed lines are extrapolations to C0=subscript\ud835\udc360absentC_{0}=italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 20480, 24576, 28672, and 32768, which correspond to 2, 4, 6, and 8 extensions of the output trace, following the budget forcing technique in s1\u00a0[30]. In the left plot, we run the STaR variant of MRT and the right plot corresponds to the DeepScaleR-1.5B-Preview base model where we run the RL variant. In both cases, we conduct this study on AIME 2025. Observe that MRT leads to the smallest normalized regret, both when evaluating within the maximal budget and when extrapolating to larger budgets, even when outcome-reward training (e.g., Qwen-7B STaR) starts to plateau and collapse to the base model.", "description": "Figure 9 presents a comparison of cumulative regret, a measure of the efficiency of test-time compute, across different algorithms at varying token budgets.  The x-axis represents the token budget (C<sub>0</sub>), while the y-axis shows the normalized cumulative regret. The solid points show results for budgets of 4096, 8192, 12288, and 16384 tokens.  The dashed lines extrapolate these results to larger budgets (20480, 24576, 28672, and 32768 tokens), simulating the effect of extending the model's reasoning process using a \"budget forcing\" technique.  The left plot uses the STaR variant of the MRT method and the DeepSeek-R1-Distill-Qwen-7B model; the right plot uses the RL variant of MRT with the DeepScaleR-1.5B-Preview model.  Both plots show results for AIME 2025 problems.  The figure demonstrates that MRT consistently achieves lower cumulative regret than other methods, both within the initial budget range and when extrapolated to higher budgets, highlighting its superior efficiency in utilizing test-time computational resources. The figure illustrates that outcome-reward baselines (e.g., Qwen-7B STaR) eventually stagnate, whereas MRT continues to show improvement.", "section": "5. Case Study: Analyzing SoTA DeepSeek-R1"}, {"figure_path": "https://arxiv.org/html/2503.07572/extracted/6256816/figures/loss.png", "caption": "Figure 10: Evolution of length during RL training. Length largely oscillates around similar values for the most part of training, after an initial increase from the initialization length.", "description": "This figure shows the evolution of the average response length during the reinforcement learning (RL) training process.  Initially, there is a noticeable increase in the average length. However, for the majority of the training, the length oscillates around a relatively stable mean value. This suggests that while the model initially explores longer response sequences, it eventually settles into a more consistent pattern of generating responses of similar lengths.", "section": "8.5.2. Question 2: Evolution of Length and Progress over Training"}, {"figure_path": "https://arxiv.org/html/2503.07572/extracted/6256816/figures/delta.png", "caption": "Figure 11: (Source:\u00a0[27]) DeepScaleR\u2019s average response length and training rewards as training progresses.", "description": "This figure, sourced from Luo et al. [27], illustrates the trends in average response length and training rewards observed during the DeepScaleR model's training process.  It shows how these two metrics evolve over the course of training, providing insights into the model's learning dynamics and its relationship between length of responses and training effectiveness.", "section": "5. Case Study: Analyzing SoTA DeepSeek-R1"}, {"figure_path": "https://arxiv.org/html/2503.07572/extracted/6256816/figures/r1_episodes.png", "caption": "Figure 12: Regret for 8K and 16K DeepScaleR checkpoints at different budgets C0subscriptC0C_{0}italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. For budgets beyond 8192819281928192, we calculate the normalized regret of the 8K checkpoint by extrapolating it with budget forcing. At nearly all budgets, the 8K checkpoint shows lower normalized regret, indicating better progress.", "description": "Figure 12 illustrates the cumulative regret (a measure of how well an LLM uses its compute budget) for two different training scenarios: one with an 8K token budget and another with a 16K token budget.  The graph shows that even when extrapolating the 8K model's performance to higher budgets (using a technique called 'budget forcing'), the 8K model consistently exhibits lower cumulative regret. This suggests that the 8K model makes more efficient and effective use of its compute resources, demonstrating better progress towards solving problems.", "section": "Ablation Studies and Diagnostic Experiments"}, {"figure_path": "https://arxiv.org/html/2503.07572/extracted/6256816/figures/r1_scaling_curve_omnimath.png", "caption": "Figure 13: On-policy rollout generation for MRT in the backtracking search setting. MRT allows the model to learn to backtrack (\ud835\udc331subscript\ud835\udc331\\mathbf{z}_{1}bold_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT) and generate the corrected attempt (\ud835\udc332subscript\ud835\udc332\\mathbf{z}_{2}bold_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT) with a progress-adjusted reward.", "description": "This figure illustrates the process of on-policy rollout generation within the MRT framework for the backtracking search setting. The model starts with an initial response (z0). It then generates a backtracking episode (z1) where it identifies errors in z0.  Finally, it produces a corrected response (z2). Importantly, the reward is adjusted based on the progress made toward a correct solution, encouraging the model to effectively utilize backtracking for improved accuracy. The progress is measured as a change in the likelihood of eventual success between consecutive steps.", "section": "7.1. STaR and RL Variants of MRT"}, {"figure_path": "https://arxiv.org/html/2503.07572/extracted/6256816/figures/r1_scaling_curve_aime.png", "caption": "Figure 14: Different data construction schemes for obtaining warmstart SFT data for the backtracking search setting. MRT traverses two paths with the shared prefix, making use of backtracking, which RISE style approaches.", "description": "This figure illustrates different methods for creating training data to enable a language model to perform backtracking.  The standard approach (RISE) involves creating training examples by stitching together a successful reasoning trace with some failures.  The MRT approach improves on this by explicitly modeling backtracking within the training data, which allows it to better learn how to effectively use backtracking during test-time reasoning. In the MRT approach, the algorithm traverses two paths, which share the same prefix and then uses a backtracking step to reach a correct answer. This method focuses on using backtracking to improve reasoning capabilities, instead of just training the model to recognize solutions.", "section": "A. MRT with the Backtracking Search Parameterization"}, {"figure_path": "https://arxiv.org/html/2503.07572/extracted/6256816/figures/mrt_star_scaling_curve_omnimath.png", "caption": "Figure 15: Training loss for warmstart SFT on multiple data configurations: random stitching (\u201cRISE\u201d\u00a0[33]), STaR (\u201crejection sampling\u201d), and our warmstart SFT data (\u201cBacktrack\u201d). A lower loss implies ease of fitting this data.", "description": "This figure displays the training loss curves for three different methods used to prepare training data for a model using backtracking search.  The three methods are:\n\n1.  **Random stitching (RISE):** This method combines random parts of incorrect solution attempts and correct solutions to create training examples. \n2.  **Rejection sampling (STaR):** This method uses a sampling technique to select a subset of high-quality solution traces from many generated traces to create training examples. \n3.  **Backtracking:**  This method, proposed by the authors, employs a more structured process to create training data based on identified errors and subsequent corrections in the model's solution attempts.  \n\nThe lower the training loss, the easier it is for the model to learn from the training data created by that method.  This experiment shows that the backtracking approach is better, having the lowest loss, followed by the STaR approach.  The random stitching approach resulted in the highest training loss, implying the training data it produced is the most difficult for the model to learn from. ", "section": "A. MRT with the Backtracking Search Parameterization"}, {"figure_path": "https://arxiv.org/html/2503.07572/extracted/6256816/figures/star_scaling_curve_omnimath.png", "caption": "Figure 16: Progress histograms in the backtracking search setting over the backtracking episode for RISE and MRT (STaR) on the left and GRPO and MRT (RL) on right, computed on the evaluation set. In each case, using reward values prescribed by MRT amplifies information gain on the test-time trace, enabling it to make consistent progress.", "description": "This figure presents histograms that compare the progress made during backtracking episodes by different training methods: RISE, MRT (STaR), GRPO, and MRT (RL). The x-axis represents the difference between the backtracking reward and the direct reward.  The y-axis represents the frequency of these reward differences. Each histogram illustrates the distribution of progress values calculated during backtracking episodes.  The results show that MRT (both STaR and RL variants) consistently achieves higher progress values compared to RISE and GRPO, indicating a more effective strategy in utilizing test-time compute to make consistent progress towards a solution.", "section": "8.4. Linearized Evaluations in the Backtracking Search Setting"}]