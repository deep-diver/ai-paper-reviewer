[{"figure_path": "https://arxiv.org/html/2503.07639/x1.png", "caption": "Figure 1: MoE-X introduces a sparse and wide network architecture designed for interpretability. Compared to dense MLPs, it incorporates both sparsity and a wider structure. Unlike traditional MoE models, it enforces sparsity within each expert and routes tokens to the sparsest experts.", "description": "This figure illustrates the MoE-X architecture, highlighting its design for interpretability.  MoE-X addresses the challenge of polysemantic neurons in large language models by creating a wider network with sparse activations. Unlike standard MLPs which have dense connections and activations, MoE-X employs multiple smaller MLPs ('experts') that are only activated for a subset of input tokens.  Crucially, MoE-X enforces sparsity within each expert and uses a sparsity-aware routing mechanism. This mechanism prioritizes sending tokens to experts producing the sparsest activations. This design encourages more disentangled feature representations within the network, which contributes to better interpretability. The figure contrasts MoE-X with traditional dense and wide MLPs, showcasing MoE-X's unique combination of width and controlled sparsity.", "section": "Mixture of Experts for Intrinsic Interpretability"}, {"figure_path": "https://arxiv.org/html/2503.07639/x2.png", "caption": "Figure 2: Illustration of using chess game to evaluate the LLM\u2019s interpretability.", "description": "This figure illustrates the methodology used to evaluate the interpretability of a large language model (LLM) in the context of chess games.  The LLM processes a Portable Game Notation (PGN) string, a textual representation of a chess game. The model's internal activations (from the Multi-Layer Perceptron, or MLP) are then analyzed to determine how well they align with semantically meaningful properties of the chess board state (BSP).  The figure visually connects the input PGN string, the internal MLP hidden layer activations, and their relation to the BSP to show how the LLM processes and represents chess-relevant information.  This process helps assess whether the LLM's internal representations are aligned with the actual meaningful concepts of chess, allowing for an evaluation of the model's interpretability.", "section": "3.1. Measuring interpretability on Chess Games"}, {"figure_path": "https://arxiv.org/html/2503.07639/x3.png", "caption": "Figure 3: Comparision BSP Coverage score v.s. the Model size.", "description": "This figure displays the relationship between model size (in MB) and BSP (Board State Properties) Coverage score.  Multiple lines represent different model configurations, varying the hidden size multiplier (\u03b1) and the input dimension (d) of the MLP (Multi-Layer Perceptron).  The x-axis shows model size, and the y-axis shows the BSP Coverage score, a metric indicating the model's ability to capture meaningful chessboard information.  The graph allows for a visual comparison of how increasing model size, through adjustments in \u03b1 and d, affects the model's interpretability as measured by the coverage score.  A baseline using Sparse Autoencoder (SAE) is also included for reference.", "section": "3. Preliminary Study: What Architectural Choices Enhance Interpretability?"}, {"figure_path": "https://arxiv.org/html/2503.07639/x4.png", "caption": "Figure 4: Comparing BSP Coverage score v.s. L\ud835\udc3fLitalic_L-0 norm of the hidden.", "description": "This figure displays the results of an experiment evaluating the relationship between the sparsity of hidden layer activations and the interpretability of a language model trained on chess game data.  The x-axis represents the L0 norm of hidden layer activations, which is a measure of sparsity (lower values indicate higher sparsity). The y-axis represents the BSP Coverage score, a metric for assessing interpretability in this specific context, where higher scores mean better interpretability.  The plot shows multiple lines representing different model sizes, demonstrating how changes in model size affect the relationship between sparsity and interpretability.  The goal of the experiment was to determine the optimal level of sparsity for achieving high interpretability.", "section": "3. Preliminary Study: What Architectural Choices Enhance Interpretability?"}, {"figure_path": "https://arxiv.org/html/2503.07639/x7.png", "caption": "Figure 5: BSP Coverage and Reconstruction score of different model sizes.", "description": "This figure compares the performance of different models in terms of BSP Coverage and Reconstruction scores across varying model sizes.  BSP Coverage represents how well the model's internal activations align with semantically meaningful chess board state properties.  Reconstruction score reflects how well the model can recover the complete state of a chessboard from its internal representations.  The figure visually demonstrates the impact of model size and architectural choices (e.g., different activation functions and Mixture of Experts approaches) on the model's ability to both capture interpretable chess features and reconstruct board states from those features.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.07639/x8.png", "caption": "Figure 6: t-SNE projections of encoder weights for original MoE layer, MoE with ReLU experts, and without full MoE-X layers, trained on Chess dataset.", "description": "This figure visualizes the encoder weights of different Mixture-of-Experts (MoE) models trained on a chess dataset using t-distributed Stochastic Neighbor Embedding (t-SNE).  Each point represents an encoder weight vector. The different panels show the results for three model variants:  a standard MoE, an MoE with ReLU activation functions in the expert networks, and a MoE incorporating all design choices of the proposed MoE-X architecture. The clustering of points reveals how the different architectural choices influence the structure of the latent space learned by the model.  Specifically, it helps to show if features are disentangled across experts, aiding the interpretability of the model.", "section": "MoE-X Expert cluster features"}, {"figure_path": "https://arxiv.org/html/2503.07639/x9.png", "caption": "Figure 7: Activated tokens for experts in MoE-X small on RedPajama-v2 validation dataset. Their interpretations were identified using the auto-interpretation.", "description": "This figure visualizes the results of an auto-interpretation experiment on the MoE-X small model, trained on the RedPajama-v2 validation dataset. It showcases several examples of activated tokens for different experts, along with their corresponding interpretations generated by the auto-interpretation process. The interpretations provide insights into the semantic meaning each expert is associated with.", "section": "5.2 Interpretability for Natural Language"}, {"figure_path": "https://arxiv.org/html/2503.07639/x13.png", "caption": "Figure 8: Automated Interpretability Detection Results in 8th Layer Hidden Activation Quantiles 1000 Random Features with 95% Confidence Intervals. Not indicates non-activating text.", "description": "This figure displays the results of an automated interpretability detection experiment on the 8th layer of a hidden activation in a language model.  The experiment used 1000 randomly selected features and calculated 95% confidence intervals for their accuracy. Each feature's accuracy was measured using 100 activating and 100 non-activating text examples. The examples were chosen using stratified sampling to ensure a balanced representation across the activation distribution's deciles. The 'Not' label indicates non-activating text.", "section": "5.2 Interpretability for Natural Language"}, {"figure_path": "https://arxiv.org/html/2503.07639/x14.png", "caption": "Figure 9: Comparison between TopK gating and our Sparsity routing. Our score identifies a more sparse set of experts.", "description": "Figure 9 compares the performance of two gating mechanisms: the standard TopK gating and the proposed sparsity-aware routing method. Both methods aim to select experts for processing input tokens within a Mixture-of-Experts (MoE) architecture.  The x-axis represents the L0 norm of the experts' activation vectors (a measure of sparsity, where lower values indicate higher sparsity).  The y-axis shows the value of the gating scores assigned to each expert by each method. The plot reveals that the TopK gating mechanism does not reliably select sparse experts. In contrast, the proposed sparsity-aware gating scores exhibit a strong negative correlation with the actual expert sparsity. The plot visually demonstrates that the new method significantly improves the selection of sparse experts.", "section": "5.3 Ablation Study and Analysis"}]