{"importance": "This paper introduces a new model overcoming the quadratic computational complexity & large-scale data dependency of current multimodal models, paving the way for more efficient & accessible research and applications.", "summary": "OmniMamba: Efficient multimodal understanding and generation via SSMs, trained on 2M image-text pairs.", "takeaways": ["OmniMamba is the first Mamba-based unified model for multimodal tasks.", "The model uses decoupled vocabularies and task-specific LoRA for efficient training and inference.", "A novel two-stage training strategy addresses data imbalance, achieving state-of-the-art performance with fewer training data."], "tldr": "Unified multimodal models face quadratic complexity & data dependence issues. Existing models are slow and impractical for real-time use due to computational demands. **OmniMamba uses Mamba-2\u2019s efficiency**, extending it from text to multimodal generation via next-token prediction, reducing reliance on extensive training data and improves both training and inference. \n\n**OmniMamba** employs decoupled vocabularies for modality-specific generation, task-specific LoRA for adaptation, and a two-stage training strategy to balance data. It achieves comparable or superior performance with significantly reduced training data (2M image-text pairs) compared to existing models. Results show 119.2x speedup & 63% memory reduction for long sequences.", "affiliation": "Huazhong University of Science & Technology", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Understanding"}, "podcast_path": "2503.08686/podcast.wav"}