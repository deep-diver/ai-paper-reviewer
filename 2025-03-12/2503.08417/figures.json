[{"figure_path": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/overview_alpha.png", "caption": "Figure 1: AnyMoLe generates in-between motion from context frames and keyframes without requiring external training data.", "description": "This figure demonstrates the core capability of AnyMoLe.  It shows how the system generates smooth and realistic intermediate motion frames between given context frames (surrounding the desired motion) and keyframes (defining the start and end poses).  Importantly, this is achieved *without* the need for any character-specific training data, unlike previous methods.  The figure visually highlights the input context frames and keyframes, and the output in-between frames generated by AnyMoLe.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/ICAdapt3.png", "caption": "Figure 2: \nOverview of AnyMoLe: First, the video diffusion model is fine-tuned without using any external data (Sec.\u00a03.1) while the scene-specific joint estimator is trained (Sec.\u00a03.3.1). Next, the fine-tuned video generation model produces an in-between video (Sec.\u00a03.2), which is then refined through motion video mimicking to generate the final in-between motion (Sec.\u00a03.3).", "description": "This figure illustrates the AnyMoLe framework's three main stages. Initially, a video diffusion model is fine-tuned using only two seconds of context motion from the target character, and concurrently, a scene-specific joint estimator is trained.  No external datasets are required for this initial fine-tuning stage.  Second, the fine-tuned model generates an in-between video in two stages: coarse frame generation followed by refinement to fill in details. Third, the generated video is processed using motion video mimicking to optimize the character's 3D motion and generate a final, smooth in-between motion sequence that closely matches the target keyframes.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/inpaint2.png", "caption": "Figure 3: Overview of the ICAdapt training process. The spatial module and image injection module are trained, while the others are frozen.", "description": "The figure illustrates the ICAdapt training process, a crucial component in AnyMoLe's motion in-betweening method.  ICAdapt addresses the domain gap between real-world and rendered video data by fine-tuning the video diffusion model. The process focuses on adapting the model to rendered character animations without disrupting its ability to generate realistic motions. Specifically, only the spatial module and image injection module of the DynamiCrafter video diffusion model are fine-tuned using short (2-second) clips of rendered context motion, ensuring that the model accurately represents the virtual characters' appearance while preserving its learned motion dynamics. The temporal module and other components remain frozen during this fine-tuning phase.", "section": "3.1. Inference-stage Context Adaptation"}, {"figure_path": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/two_stage.png", "caption": "Figure 4: Context frames guided video generation process.", "description": "This figure illustrates the two-stage video generation process within AnyMoLe. The first stage generates sparse frames to establish the motion structure using context frames as guidance within a latent inpainting framework.  The second stage then refines these sparse frames by generating dense frames to fill in the details, leveraging the previously generated sparse frames as guidance.  This two-stage approach ensures smooth and contextually aware video generation. The process involves iteratively denoising latent representations, replacing noisy parts with encoded guidance frames to incorporate contextual information.", "section": "3.2. Two-stage Video Generation"}, {"figure_path": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/joint_estimator.png", "caption": "Figure 5: Two stage inference of Da\u2062d\u2062psubscript\ud835\udc37\ud835\udc4e\ud835\udc51\ud835\udc5dD_{adp}italic_D start_POSTSUBSCRIPT italic_a italic_d italic_p end_POSTSUBSCRIPT. First, at coarse stage, low frame-rate video is generated in auto regressive manner. Next, high frame-rate video is generated from low frame-rate video.", "description": "This figure illustrates the two-stage video generation process used in AnyMoLe.  The first stage uses the video diffusion model (Dadp) to generate a low frame-rate video autoregressively, meaning it predicts frames sequentially based on previous frames and the input keyframes. This initial video establishes the overall motion structure. The second stage then takes this low frame-rate video, along with the keyframes, as input and uses Dadp to generate a higher frame-rate video, refining the detail and smoothness of the motion. The goal is to produce a more realistic and visually appealing motion sequence.", "section": "3.2. Two-stage Video Generation"}, {"figure_path": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/anymole_comparisonrm.png", "caption": "Figure 6: Overview of joint estimator training process.", "description": "This figure details the training process for the scene-specific joint estimator, a key component of AnyMoLe. It starts by taking multi-view rendered images of the character's motion as input.  These images are processed by DINOv2, which extracts 2D semantic features, and FiT3D, which adds 3D structural information.  These features are combined, decoded into heatmaps to estimate 2D joint positions, and then passed through a depth MLP to predict the depth of each joint, giving a 3D joint position.  The training objective uses a Mean Squared Error (MSE) loss function to compare predicted 3D joints to ground truth 3D joints calculated from context frames and keyframes, aiming for precise 3D joint estimation.", "section": "3.3.1. Scene-specific Joint Estimator"}, {"figure_path": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/ablation_add4.png", "caption": "Figure 7: Results of baseline comparison. Our method generated in-between frames similar to the ground truth, while SinMDM and SinMDM* generated out-of-context motion (blue box). TST generated partially out-of-context footsteps (blue box) and out-of-style motion (red box). ERD-QV generated motion that has a different style, with a stiff back and sharp hand positions (red box).", "description": "Figure 7 presents a comparison of motion in-betweening results between AnyMoLe and three baseline methods: SinMDM, SinMDM*, TST, and ERD-QV.  The figure visually demonstrates the in-between frames generated by each method for the same input keyframes. AnyMoLe's output closely matches the ground truth, showing smooth and natural transitions. In contrast, SinMDM and SinMDM* produce frames with significant motion errors, indicated by the blue boxes, which highlight portions of the generated motion that are clearly out of context.  TST displays both partially out-of-context movements (blue box), such as illogical foot placement, and stylistic inconsistencies (red box), where the style of the generated motion differs from the original style of the keyframes. ERD-QV generates motions with a distinctly different style than the ground truth, characterized by a stiff back posture and exaggerated hand movements (red box).  These visual differences highlight AnyMoLe's superiority in generating realistic and contextually appropriate in-between frames compared to the baseline methods.", "section": "4.3 Comparison with Baselines"}, {"figure_path": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/app.png", "caption": "Figure 8: Ablation results on video generation. Without applying ICAdapt, each frame of the video exhibited inconsistencies, such as generating noticeable style shifts (blue box). Omitting the fine-stage process resulted in a low frame rate, making identical or significant jumps between frames.", "description": "This ablation study analyzes the impact of key components in AnyMoLe's video generation process.  The leftmost column shows the input keyframes. The next three columns show the results of AnyMoLe's video generation under different conditions: (1) the full model, (2) the model without ICAdapt (Inference-stage Context Adaptation), and (3) the model without the fine-stage video generation. The results demonstrate that using ICAdapt leads to consistent styles, while the fine stage improves frame rate and smoothness.  Without ICAdapt, the generated video shows significant style inconsistencies, highlighted by a blue box indicating a noticeable style shift.  Omitting the fine stage results in a low frame rate, causing frames to be nearly identical or to exhibit large jumps between frames.", "section": "4.4 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/anymole_limiation.png", "caption": "Figure 9: From understanding of the context, AnyMoLe can naturally generate in-between frames in a multi-object scenario.", "description": "Figure 9 demonstrates AnyMoLe's capacity to generate intermediate frames for scenes involving multiple objects.  It highlights the model's ability to comprehend contextual relationships between objects and produce realistic and fluid movements, going beyond simple character animation to more complex multi-object interactions. The example shown illustrates two bouncing balls, showcasing the model's extrapolation to multi-object scenarios.", "section": "5. Applications"}]