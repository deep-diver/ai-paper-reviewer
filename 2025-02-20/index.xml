<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-02-20s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/</link><description>Recent content in 2025-02-20s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Wed, 19 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/index.xml" rel="self" type="application/rss+xml"/><item><title>AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13943/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13943/</guid><description>AdaptiveStep: Divides reasoning steps automatically through model confidence, enhancing PRM training &amp;amp; performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13943/cover.png"/></item><item><title>Autellix: An Efficient Serving Engine for LLM Agents as General Programs</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13965/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13965/</guid><description>Autellix: Efficient LLM Serving for Agents</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13965/cover.png"/></item><item><title>Craw4LLM: Efficient Web Crawling for LLM Pretraining</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13347/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13347/</guid><description>CRAW4LLM: Efficiently crawls web pages for LLM pretraining by prioritizing influence scores, boosting data quality &amp;amp; cutting crawling waste.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13347/cover.png"/></item><item><title>Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13962/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13962/</guid><description>Test-time scaling + confidence = better QA!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13962/cover.png"/></item><item><title>LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13922/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13922/</guid><description>LongPO: Self-evolve LLMs to excel in long contexts via short-to-long preference optimization, boosting performance without sacrificing short-context skills.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13922/cover.png"/></item><item><title>MoM: Linear Sequence Modeling with Mixture-of-Memories</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13685/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13685/</guid><description>MoM: Enhancing linear sequence modeling via mixture-of-memories for improved recall and reduced memory interference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13685/cover.png"/></item><item><title>Noise May Contain Transferable Knowledge: Understanding Semi-supervised Heterogeneous Domain Adaptation from an Empirical Perspective</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13573/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13573/</guid><description>Unveiling the surprising potential of noise: transferable knowledge in semi-supervised heterogeneous domain adaptation (SHDA).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13573/cover.png"/></item><item><title>REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13622/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13622/</guid><description>REFIND: Detects LLM hallucinations by directly leveraging retrieved documents, using a novel Context Sensitivity Ratio.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13622/cover.png"/></item><item><title>Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13533/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13533/</guid><description>LORAM: Train small, infer large LLMs by memory-efficient LoRA training. Enables 70B parameter model training on a 20G HBM GPU, replacing A100-80G. Reduces parameter storage cost by 15.81x.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13533/cover.png"/></item><item><title>Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13946/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13946/</guid><description>Aligned LLMs&amp;rsquo; safety often anchors in the template region, creating vulnerabilities. Detaching safety mechanisms shows promise in mitigation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13946/cover.png"/></item><item><title>NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.12638/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.12638/</guid><description>NExT-Mol: Combines 1D language models with 3D diffusion for molecule generation, achieving state-of-the-art performance and validity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.12638/cover.png"/></item><item><title>RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13144/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13144/</guid><description>RAD: 3DGS-based RL advances autonomous driving, achieving a 3x lower collision rate!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13144/cover.png"/></item><item><title>SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13128/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13128/</guid><description>SongGen: Single-stage autoregressive transformer for controllable text-to-song generation, simplifying the process and improving control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13128/cover.png"/></item><item><title>InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.11573/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.11573/</guid><description>InfiR: Efficient, small AI models rival larger ones in reasoning, slashing costs and boosting privacy for wider AI use.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.11573/cover.png"/></item><item><title>Presumed Cultural Identity: How Names Shape LLM Responses</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.11995/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.11995/</guid><description>LLMs personalize based on user names, but this study reveals that cultural presumptions in LLM responses risk reinforcing stereotypes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.11995/cover.png"/></item><item><title>Small Models Struggle to Learn from Strong Reasoners</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.12143/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.12143/</guid><description>Small language models struggle to learn complex reasoning from large models, but a novel &amp;lsquo;Mix Distillation&amp;rsquo; method balances complexity for effective capability transfer.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.12143/cover.png"/></item><item><title>Thinking Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13173/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13173/</guid><description>ThinkPO improves LLM reasoning by preferring longer CoT, boosting performance without new data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13173/cover.png"/></item></channel></rss>