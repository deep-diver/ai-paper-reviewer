[{"figure_path": "https://arxiv.org/html/2502.13128/extracted/6197501/Figure/motivation.png", "caption": "Figure 1: \nTraditional methods often rely on multi-stage processes, making pipelines inflexible and complex. SongGen simplifies this with a single-stage auto-regressive transformer that supports both mixed mode and dual-track mode song generation.", "description": "Figure 1 illustrates the difference between traditional multi-stage text-to-song generation methods and the proposed SongGen approach. Traditional methods involve separate stages for generating music scores, vocals, and accompaniment, leading to complex and inflexible pipelines. In contrast, SongGen uses a single-stage auto-regressive transformer to directly generate both mixed-mode (vocals and accompaniment combined) and dual-track mode (vocals and accompaniment separated) song outputs, simplifying the generation process and enhancing controllability.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13128/extracted/6197501/Figure/framework1.png", "caption": "Figure 2: Overview of SongGen: An auto-regressive transformer decoder generates audio tokens with diverse patterns, incorporating user-defined controls via cross-attention. The final song is synthesized from these tokens through the audio codec decoder.", "description": "SongGen uses an auto-regressive transformer decoder to generate audio tokens.  These tokens incorporate user input (lyrics, description text, and optional reference audio) via cross-attention. The decoder outputs a sequence of these tokens, which are then processed by a neural audio codec to synthesize the final song. Different token patterns are used to control the generation process for flexibility.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2502.13128/extracted/6197501/Figure/pattern2.png", "caption": "Figure 3: Illustration of token patterns for different generation modes. The codebook-delay pattern (from MusicGen) is applied to every audio token. (a) Mixed Pro: Directly decoding mixed tokens, with an auxiliary vocal token prediction target to enhance vocal learning. Dual-track mode: (b) Parallel: Vocal and accompaniment tokens are concatenated along the codebook dimension, with three track order variants. (c) Interleaving: Tokens from both tracks are interleaved along the temporal dimension, with two track order variants.", "description": "Figure 3 illustrates various token patterns used in SongGen for different generation modes.  It focuses on how the model handles vocals and accompaniment.  The 'codebook-delay' pattern from MusicGen is used in all modes.  Panel (a) shows the 'Mixed Pro' approach, where mixed audio tokens are directly decoded with an auxiliary vocal token prediction target added to improve vocal quality. Panels (b) and (c) display dual-track mode strategies.  Panel (b), the 'Parallel' method, concatenates vocal and accompaniment tokens along the codebook dimension with variations in the ordering of the tracks. Panel (c), 'Interleaving', interleaves vocal and accompaniment tokens in time, also showing variations in track ordering.  This figure visually explains the different strategies SongGen uses to generate either a mixed audio track or separate vocal and accompaniment tracks.", "section": "3.2 Auto-regressive Codec Language Modeling"}, {"figure_path": "https://arxiv.org/html/2502.13128/extracted/6197501/Figure/vibrato.png", "caption": "Figure 4: Mel-spectrogram visualization of our generated song featuring various singing techniques.", "description": "Figure 4 presents mel-spectrograms of a song generated by the SongGen model.  The spectrograms visually demonstrate the model's ability to generate songs incorporating a variety of expressive vocal techniques, showcasing the model's capabilities beyond simple audio generation.", "section": "4. Results of Text-to-song Generation"}, {"figure_path": "https://arxiv.org/html/2502.13128/extracted/6197501/Figure/attan.png", "caption": "Figure 5: Visualization of decoder attention.", "description": "Figure 5 visualizes the attention weights within the transformer decoder of the SongGen model for both mixed and dual-track modes.  Subfigures (a) and (b) display the self-attention patterns in layers 18 and 8 respectively for the 'mixed pro' generation mode, showcasing a diagonal pattern indicating the model's ability to capture repetitive structures common in music. (c) shows layer 21 for the same mode, presenting a checkerboard pattern suggesting different focus on intra- and inter-track relations. (d), (e), and (f) show the same self-attention visualizations but for the 'Interleaving (A-V)' dual-track mode, revealing a similar diagonal pattern in higher layers and a distinct checkerboard pattern in lower layers. This difference highlights how different layers in the model focus on capturing various relationships between vocal and accompaniment tracks in different modes.", "section": "4.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13128/extracted/6197501/Figure/codec.png", "caption": "Figure 6: Training loss curves of different audio codecs", "description": "This figure shows the training loss curves for three different audio codecs: X-Codec, Encodec, and DAC.  The X-axis represents the training step, and the Y-axis represents the loss.  The plot visually compares the convergence speed and final loss values achieved by each codec during the training process of the SongGen model. This helps to illustrate the impact of the choice of audio codec on model performance.", "section": "4. Experiments"}]