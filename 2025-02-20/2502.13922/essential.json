{"importance": "**LongPO** offers an efficient way to enhance LLMs for long contexts by transferring short-context capabilities, reducing reliance on expensive manual annotation. This approach opens new research directions for adapting LLMs to diverse context lengths and improving alignment strategies.", "summary": "LongPO: Self-evolve LLMs to excel in long contexts via short-to-long preference optimization, boosting performance without sacrificing short-context skills.", "takeaways": ["LongPO enables short-context LLMs to self-evolve and excel on long-context tasks by internally transfer short-context capabilities.", "LongPO mitigates performance decline with a short-to-long KL constraint.", "LongPO-trained models rival superior LLMs in long-context benchmarks, even outperforming those with extensive long-context annotation and larger scales."], "tldr": "Large Language Models (LLMs) show impressive capabilities, but short-context models often struggle with longer contexts due to alignment issues. Human annotation is impractical for extended contexts, and balancing short- and long-context performance is difficult. Current methods fall short because they either lack sufficient long-context data or fail to preserve short-context abilities.\n\nTo solve these issues, **LongPO** enables LLMs to self-evolve by transferring short-context skills to longer contexts. It uses self-generated preference data from short-to-long contexts and a short-to-long KL constraint to maintain short-context performance. Applied to Mistral-7B-Instruct-v0.2, LongPO retains short-context performance and outperforms other methods. The models achieve results comparable to superior LLMs like GPT-4-128K on long-context benchmarks.", "affiliation": "National University of Singapore", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2502.13922/podcast.wav"}