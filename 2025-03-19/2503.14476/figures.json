[{"figure_path": "https://arxiv.org/html/2503.14476/x1.png", "caption": "Figure 1: AIME 2024 scores of DAPO on the Qwen2.5-32B base model, outperforming the previous SoTA DeepSeek-R1-Zero-Qwen-32B using 50% training steps.", "description": "The figure shows the performance of the DAPO reinforcement learning algorithm on the AIME 2024 benchmark.  The y-axis represents the accuracy score on AIME 2024, while the x-axis shows the number of training steps. The blue lines represent different metrics tracked by DAPO during training.  The key takeaway is that DAPO, using the Qwen2.5-32B base language model, surpasses the prior state-of-the-art DeepSeek-R1-Zero-Qwen-32B model's performance while using only 50% of the training steps.  This demonstrates DAPO's efficiency and effectiveness in improving the reasoning capabilities of large language models.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.14476/x2.png", "caption": "(a) Accuracies on AIME.", "description": "This figure shows the accuracy of the model on the AIME test set over the course of reinforcement learning training.  It demonstrates how the model's performance on AIME improves as the number of training steps increases.  The x-axis represents the training steps and the y-axis represents the accuracy on AIME.", "section": "2.2 Group Relative Policy Optimization (GRPO)"}, {"figure_path": "https://arxiv.org/html/2503.14476/x3.png", "caption": "(b) Entropy of actor model.", "description": "The figure shows the entropy of the actor model during the reinforcement learning training process.  The entropy measures the uncertainty or randomness in the model's probability distribution over actions.  A high entropy indicates that the model explores different actions with roughly equal probability, while a low entropy indicates that the model is more deterministic and tends to favor a smaller subset of actions. This plot helps to visualize the exploration-exploitation balance during training.  A sharp decrease in entropy might suggest premature convergence or an issue with the training process. In this case, the figure shows an entropy collapse phenomenon, which is often undesirable.", "section": "3.1 Raise the Ceiling: Clip-Higher"}, {"figure_path": "https://arxiv.org/html/2503.14476/x4.png", "caption": "Figure 2: The accuracy on the AIME test set and the entropy of the actor model\u2019s generated probabilities during the RL training process, both before and after applying Clip-Higher strategy.", "description": "This figure displays two line graphs illustrating the impact of the Clip-Higher strategy on the performance of a reinforcement learning model during training. The left graph shows the accuracy on the AIME (Algebra, IIME, Mathematics) test set, demonstrating improved performance with Clip-Higher.  The right graph depicts the entropy of the model's generated probability distribution, revealing that Clip-Higher prevents entropy collapse which is a common problem that leads to poor performance in reinforcement learning.  The comparison highlights how Clip-Higher enhances both accuracy and the diversity of the model's output during training.", "section": "2 Preliminary"}, {"figure_path": "https://arxiv.org/html/2503.14476/x5.png", "caption": "(a) Maximum clipped probabilities.", "description": "The figure shows the maximum probability of clipped tokens during training.  The Clip-Higher strategy, discussed in the paper, modifies the clipping range of the importance sampling ratio in the policy optimization. This graph illustrates the effect of this modification on the probability distribution, revealing how the strategy impacts the exploration-exploitation trade-off during training, by decoupling the lower and upper bounds of the clipping range. It indirectly demonstrates the effect of the Clip-Higher method on the diversity of the model's responses.", "section": "3.1 Raise the Ceiling: Clip-Higher"}, {"figure_path": "https://arxiv.org/html/2503.14476/x6.png", "caption": "(b) The proportion of samples with an accuracy of 1.", "description": "This figure shows the percentage of samples in each training batch that achieve 100% accuracy on the AIME dataset.  It illustrates how frequently the model produces perfect answers, potentially indicating overfitting or a lack of exploration in later training steps.", "section": "3.2 The More the Merrier: Dynamic Sampling"}, {"figure_path": "https://arxiv.org/html/2503.14476/x7.png", "caption": "Figure 3: The entropy of the probability distribution of the actor model, as well as the changes in response length.", "description": "This figure displays two sub-figures visualizing the training dynamics of the actor model in a reinforcement learning process. The left sub-figure shows the maximum clipped probabilities during training, indicating the diversity of the model's output distribution.  The right sub-figure presents the proportion of samples with 100% accuracy over training steps, illustrating how often the model produces perfect answers.", "section": "3.2 The More the Merrier: Dynamic Sampling"}, {"figure_path": "https://arxiv.org/html/2503.14476/x8.png", "caption": "(a) Entropy of actor model\u2019s generation probabilities.", "description": "This figure shows a graph illustrating the change in entropy of the probability distribution of the actor model's generated responses during the reinforcement learning process. High entropy indicates greater diversity in the model's responses, while low entropy suggests less diverse and potentially repetitive outputs.", "section": "3.3 Rebalancing Act: Token-Level Policy Gradient Loss"}, {"figure_path": "https://arxiv.org/html/2503.14476/x9.png", "caption": "(b) Average length of actor model-generated responses", "description": "This figure shows the average length of the responses generated by the actor model during reinforcement learning training.  The x-axis represents the training steps, and the y-axis represents the average response length in tokens or words.  The plot visualizes how the average length of generated responses changes over the course of the training.  It likely illustrates whether the model learns to generate longer, more comprehensive responses as it progresses through training.", "section": "3.3 Rebalancing Act: Token-Level Policy Gradient Loss"}, {"figure_path": "https://arxiv.org/html/2503.14476/x10.png", "caption": "Figure 4: The entropy of the probability distribution of the actor model, as well as the changes in response length.", "description": "Figure 4 presents two line graphs illustrating the training dynamics of a reinforcement learning model for large language models.  The left graph shows the entropy of the actor model's probability distribution over training steps.  High entropy indicates greater exploration and diversity in the model's responses, while low entropy suggests a more deterministic and potentially less effective model.  The right graph displays the average response length generated by the model over the same training steps. This length is important because it relates to the complexity of reasoning involved.  Together, these graphs provide insights into how the model's exploration-exploitation balance and response complexity evolve during training, indicating the model's learning progress and potential issues.", "section": "3.2 The More the Merrier: Dynamic Sampling"}, {"figure_path": "https://arxiv.org/html/2503.14476/x11.png", "caption": "(a) Performance on AIME.", "description": "This figure shows the average accuracy scores achieved by the DAPO algorithm on the AIME 2024 benchmark during the reinforcement learning training process.  The x-axis represents the training steps, and the y-axis represents the average accuracy (avg@32) across multiple evaluations.  The plot displays how the accuracy of the model improves over the course of training.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2503.14476/x12.png", "caption": "(b) Entropy of actor model.", "description": "The figure shows the entropy of the actor model during reinforcement learning training.  Entropy measures the uncertainty or randomness in the model's predicted probability distribution. High entropy indicates the model is exploring diverse actions, while low entropy suggests the model is becoming deterministic and less exploratory. This plot helps analyze if the model is learning effectively or getting stuck in a suboptimal solution due to premature convergence.", "section": "3.1 Raise the Ceiling: Clip-Higher"}, {"figure_path": "https://arxiv.org/html/2503.14476/x13.png", "caption": "Figure 5: The accuracy of the actor model on AIME and the entropy of its generation probabilities, both before and after applying Overlong Reward Shaping strategy.", "description": "Figure 5 displays the performance of the actor model (the part of the reinforcement learning model that selects actions) on the AIME (Algebra, including Mathematical Intelligence) benchmark and the entropy of its generated probabilities. Two scenarios are presented: one without the \"Overlong Reward Shaping\" strategy and another with it. The \"Overlong Reward Shaping\" strategy adjusts rewards for excessively long generated responses, aiming to prevent the model from producing overly long, and potentially less accurate, responses. The figure illustrates how this strategy influences both the model's accuracy on AIME and the randomness of the responses it produces (measured by entropy).  The comparison allows for analysis of the impact of Overlong Reward Shaping on both the accuracy and the diversity of the model's outputs.", "section": "3.4 Hide and Seek: Overlong Reward Shaping"}, {"figure_path": "https://arxiv.org/html/2503.14476/x14.png", "caption": "Figure 6: The training progress before and after applying dynamic sampling on a baseline setting.", "description": "This figure displays the training progress curves for a baseline setting of the reinforcement learning algorithm, both before and after the implementation of dynamic sampling. The x-axis represents the training steps, and the y-axis likely shows a metric related to performance or accuracy (likely AIME avg@32), indicating how well the model performs on the AIME 2024 benchmark over the course of training.  The curves illustrate the effect of dynamic sampling on the model's learning trajectory.  Dynamic sampling likely leads to a faster improvement in performance and improved stability of the training process, as suggested by the smoother, steeper curve after its implementation.", "section": "3.2 The More the Merrier: Dynamic Sampling"}]