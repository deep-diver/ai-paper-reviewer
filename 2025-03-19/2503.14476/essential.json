{"importance": "This paper addresses the crucial need for **reproducible research in LLM reinforcement learning**. By open-sourcing DAPO, the authors empower researchers to replicate state-of-the-art results, **explore new techniques, and accelerate progress in reasoning LLMs**. The work fosters transparency and collaboration, driving innovation in this rapidly evolving field.", "summary": "DAPO: Open-sources a LLM reinforcement learning system that achieves SOTA AIME scores, fostering reproducible research at scale.", "takeaways": ["The DAPO algorithm achieves state-of-the-art AIME scores using a Qwen2.5-32B model.", "The paper introduces four key techniques (Clip-Higher, Dynamic Sampling, Token-Level Policy Gradient Loss, and Overlong Reward Shaping) to improve RL training.", "The authors open-source their training code and dataset to enhance reproducibility and support future research."], "tldr": "Reasoning in LLMs relies on reinforcement learning, but key details are often hidden, hindering reproducibility. This paper addresses this by proposing the **Decoupled Clip and Dynamic sampling Policy Optimization (DAPO) algorithm**. The authors identified critical issues in naive GRPO baselines, such as entropy collapse and training instability, mirroring challenges faced by others trying to reproduce existing results. \n\nTo overcome these hurdles, the authors present DAPO and **open-source a large-scale RL system**. The system achieves state-of-the-art AIME scores using a Qwen2.5-32B model. They introduce four key techniques: Clip-Higher, Dynamic Sampling, Token-Level Policy Gradient Loss, and Overlong Reward Shaping. The training code, built on the verl framework, and a carefully curated dataset are also open-sourced.", "affiliation": "Tsinghua University", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "2503.14476/podcast.wav"}