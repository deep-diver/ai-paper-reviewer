[{"figure_path": "https://arxiv.org/html/2503.13424/x1.png", "caption": "Figure 1: \nWe design probalistic programs to generate 22222222 common articulated objects.\nWe demonstrate the motion sequence of the generated articulated objects.\nOurs generated articulated objects bear accurate geometry, realistic textures, and reasonable joints.", "description": "This figure showcases the results of a procedural generation approach for creating articulated objects.  It displays a diverse set of 22 common objects (e.g., chairs, tables, cabinets) generated using probabilistic programs.  The animation shows the generated objects' motion sequences, demonstrating the accuracy of their geometry, realistic textures, and properly functioning joints. The figure highlights the system's ability to produce high-fidelity articulated object models.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.13424/x2.png", "caption": "Figure 2: \nThe whole pipeline can be devided into four parts: articulation tree structure generation, geometry generation, material generation and joint generation.", "description": "This figure illustrates the four main stages of the articulated object generation pipeline.  First, an articulation tree structure is created, defining the hierarchical relationships between different parts of the object. Next, geometry is generated for each part, creating the 3D shapes.  Then, materials are assigned to these shapes, adding realistic textures and appearance. Finally, joints are defined between parts based on the articulation tree, enabling the movement and interaction between them.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.13424/x3.png", "caption": "Figure 3: \nStructure of the URDF file.\nEach link is a part of the object, which is represented as a textured mesh in our case.\nEach joint connects two links and describes the articulation structure between them.", "description": "This figure illustrates the structure of a Universal Robot Description Format (URDF) file, a standard for representing articulated objects.  Each component of an articulated object is represented as a 'link,' depicted here as a textured mesh. Joints connect pairs of links, defining how the parts move relative to each other and establishing the overall articulation structure of the object.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.13424/extracted/6283074/pics/res_1.png", "caption": "Figure 4: \nWe implement 6666 kinds of joints in our articulated objects.\nFirst three are simple joints, and the last three are compound joints.", "description": "Figure 4 illustrates the six types of joints used in the construction of articulated objects within the paper.  The first three depicted are simple joints: a fixed joint (immobile connection), a prismatic joint (linear motion along an axis), and a revolute joint (rotational motion around an axis). The latter three represent compound joints, which are combinations of simpler joint types to enable more complex movements.  These include a flip revolute joint, a limited planar joint, and a combination of prismatic and revolute joints.  The figure visually demonstrates the structure and range of motion for each joint type.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.13424/x4.png", "caption": "Figure 5: Examples of our generated cabinets, chairs, lamps and windows.\nFor each object, we display the textured mesh with the corresponding articulation tree above.\nThe generated objects are diverse in both shapes and articulation structures.", "description": "This figure showcases examples of procedurally generated articulated objects, including cabinets, chairs, lamps, and windows.  Each object is presented with its textured 3D mesh and the corresponding articulation tree diagram displayed above it. The articulation tree visually represents the hierarchical structure of the object's interconnected parts and joints. The figure highlights the diversity achieved in both the shapes and the articulation structures of the generated objects, demonstrating the versatility of the procedural generation method used.", "section": "3.2. Articulated Object Generation"}, {"figure_path": "https://arxiv.org/html/2503.13424/x5.png", "caption": "Figure 6: \nWe use two methods to obtain the geometry of each part of an object. 1) Blender python API created meshes. 2) Parts retrieved from our carefully curated and processed dataset.\nFor each group of objects, the left one is generated by Blender python API, while the right 3333 objects are obtained by retrieving parts from curated PartNet-Moblity.\nParts obtained from both methods can be seamlessly joined, and it improves the diversity of generated shapes by adopting both methods to obtain part geometry.", "description": "Figure 6 illustrates the two approaches used for generating the geometry of object parts: 1) using Blender's Python API to create meshes, and 2) retrieving pre-existing meshes from a curated dataset (PartNet-Mobility).  The figure shows four groups of objects. In each group, the leftmost object's parts are generated using the Blender API, while the three objects to its right utilize parts from the curated dataset. The seamless integration of parts from both methods is highlighted, demonstrating how combining these approaches enhances the diversity of generated shapes.", "section": "3.3. Procedural Mesh Retrieve"}, {"figure_path": "https://arxiv.org/html/2503.13424/x6.png", "caption": "Figure 7: \nOriginal parts from PartNet and ShapeNet bear many back faces and thus their normals are highly irregular.\nWe flip these back faces in Blender using recalculate normal function followed by human repair and ensure the meshes bear consistent outward-facing normals.", "description": "Figure 7 demonstrates the preprocessing step applied to 3D model datasets like PartNet and ShapeNet.  These datasets often contain meshes with incorrectly oriented faces (back faces), leading to inconsistent and irregular surface normals.  The image shows examples of original meshes from these datasets compared to the improved meshes after processing. This process involves using Blender software's \"recalculate normals\" function to correct the face orientation and flipping back faces. Following this automated step, manual human repair was performed to further refine the meshes and ensure all normals consistently point outwards, resulting in improved quality and suitability for downstream processing in the articulated object generation pipeline.", "section": "3.3. Procedural Mesh Retrieve"}, {"figure_path": "https://arxiv.org/html/2503.13424/extracted/6283074/pics/comparison.png", "caption": "Figure 8: \nWe adopt support point-based placement to position the retrieved part on the object.\nNaive bounding box-based placement may create a gap between the retrieved part and the object. Our approach guarantees a seamless connection.", "description": "This figure illustrates the improved mesh placement technique used in the paper.  The standard method, using bounding boxes, often leaves gaps between parts of the assembled object, leading to unrealistic results. The proposed \"support point-based placement\" method ensures a seamless connection between parts, enhancing the visual realism and physical plausibility of the generated 3D models.  The figure directly compares the results of both methods.", "section": "3.3. Procedural Mesh Retrieve"}, {"figure_path": "https://arxiv.org/html/2503.13424/extracted/6283074/pics/joint_evaluation.png", "caption": "Figure 9: \nWe carefully position each part and optimally configure the joint parameters to guarantee that no collisions occur either among the parts themselves or between the object and the ground during its articulation.\nThe top row is a dishwasher from PartNet-Mobility.\nWhen the dishwasher door is opened, its base collides with the ground, forcing the main body to tilt upwards.\nWhile the dishwasher generated by our method in the bottom row does not have this problem.", "description": "This figure demonstrates the importance of careful part placement and joint parameter configuration in articulated object generation to avoid collisions.  The top row shows a dishwasher from the PartNet-Mobility dataset; when the door is opened, the base collides with the ground, causing the entire object to tilt. The bottom row shows a dishwasher generated by the method described in the paper, which avoids this problem due to improved part placement and joint parameters.", "section": "3.4. Ensure Physical Plausibility"}, {"figure_path": "https://arxiv.org/html/2503.13424/extracted/6283074/pics/table_result.png", "caption": "Figure 10: \nHuman evaluators observe textureless videos of our generated articulated objects (lower row) and those from PartNet-Mobility (upper row), subsequently determining which exhibits superior motion structure.", "description": "This figure shows a comparison of the motion quality of articulated objects generated by the proposed method and those from the PartNet-Mobility dataset.  Human evaluators watched textureless videos of the objects in motion. The top row displays videos of objects from PartNet-Mobility, while the bottom row shows videos of objects generated by the Infinite Mobility method. The evaluators judged which set of objects demonstrated superior motion structure, based on the realism and fluidity of their movements.", "section": "4.1. Evaluations of Articulated Objects"}, {"figure_path": "https://arxiv.org/html/2503.13424/extracted/6283074/pics/embodied.png", "caption": "Figure 11: Examples of articulated objects generated by CAGE trained on our results.\nThe generated object has accurate geometry and reasonable motion structure.", "description": "This figure showcases examples of articulated objects generated using the CAGE (Controllable Articulation Generation) model.  The model was trained on a dataset of synthetic articulated objects produced by the Infinite Mobility method described in the paper.  The figure demonstrates the model's capability to generate objects with accurate geometry and physically plausible articulation, indicating successful sim-to-real transfer learning.", "section": "4. Experiment"}]