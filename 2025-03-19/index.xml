<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-03-19s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/</link><description>Recent content in 2025-03-19s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Tue, 18 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/index.xml" rel="self" type="application/rss+xml"/><item><title>Concat-ID: Towards Universal Identity-Preserving Video Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14151/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14151/</guid><description>Concat-ID: A universal, scalable framework for identity-preserving video synthesis, balancing consistency and editability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14151/cover.png"/></item><item><title>Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14492/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14492/</guid><description>Cosmos-Transfer1: An adaptable conditional world generation model using multimodal control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14492/cover.png"/></item><item><title>Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14478/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14478/</guid><description>Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLMs</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14478/cover.png"/></item><item><title>DAPO: An Open-Source LLM Reinforcement Learning System at Scale</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14476/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14476/</guid><description>DAPO: Open-sources a LLM reinforcement learning system that achieves SOTA AIME scores, fostering reproducible research at scale.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14476/cover.png"/></item><item><title>Frac-Connections: Fractional Extension of Hyper-Connections</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14125/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14125/</guid><description>Frac-Connections: An efficient alternative to Hyper-Connections that divides hidden states into fractions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14125/cover.png"/></item><item><title>Impossible Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14378/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14378/</guid><description>Impossible videos expose AI limits!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14378/cover.png"/></item><item><title>Measuring AI Ability to Complete Long Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14499/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14499/</guid><description>AI progress is tracked with a new metric, 50%-task-completion time horizon, showing exponential growth with a doubling time of ~7 months, hinting at significant automation potential in the near future&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14499/cover.png"/></item><item><title>Temporal Consistency for LLM Reasoning Process Error Identification</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14495/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14495/</guid><description>A new test-time method, Temporal Consistency, is introduced to improve LLM reasoning by leveraging iterative self-reflection.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.14495/cover.png"/></item><item><title>DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12797/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12797/</guid><description>DeepPerception enhances MLLMs with cognitive visual perception, achieving superior grounding through knowledge integration &amp;amp; reasoning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12797/cover.png"/></item><item><title>Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated Objects via Procedural Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.13424/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.13424/</guid><description>Infinite Mobility: Procedural generation of high-fidelity articulated objects for scalable embodied AI training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.13424/cover.png"/></item><item><title>MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.13111/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.13111/</guid><description>MM-Spatial enhances multimodal LLMs with 3D spatial reasoning via a novel dataset and benchmark, improving performance on spatial understanding tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.13111/cover.png"/></item><item><title>Pensez: Less Data, Better Reasoning -- Rethinking French LLM</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.13661/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.13661/</guid><description>Pensez: Strategic fine-tuning beats massive data for superior reasoning in French LLMs, challenging conventional wisdom.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.13661/cover.png"/></item><item><title>CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12329/</link><pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12329/</guid><description>CapArena: Detailed image caption benchmark in the LLM era, revealing metric biases and advancing automated evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12329/cover.png"/></item><item><title>MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process Errors Identification</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12505/</link><pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12505/</guid><description>MPBench: Multimodal benchmark to identify errors in reasoning processes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12505/cover.png"/></item><item><title>PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12545/</link><pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12545/</guid><description>PEBench: A new benchmark for machine unlearning in multimodal language models, enhancing secure multimodal model development.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12545/cover.png"/></item><item><title>Hyperbolic Safety-Aware Vision-Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12127/</link><pubDate>Sat, 15 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12127/</guid><description>HySAC: A hyperbolic framework for safety-aware vision-language models, improving content moderation and interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12127/cover.png"/></item><item><title>Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12271/</link><pubDate>Sat, 15 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12271/</guid><description>Reflect-DiT: Scaling Text-to-Image Diffusion Transformers via In-Context Reflection!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.12271/cover.png"/></item><item><title>KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for Open-Vocabulary Robotic Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.10546/</link><pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.10546/</guid><description>KUDA unifies dynamics learning and visual prompting with keypoints for open-vocabulary robot manipulation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.10546/cover.png"/></item><item><title>Florenz: Scaling Laws for Systematic Generalization in Vision-Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.09443/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.09443/</guid><description>Florenz: Scaling laws for systematic generalization via monolingual vision-language models</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.09443/cover.png"/></item></channel></rss>