{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduced the Transformer architecture, which has revolutionized the field of deep learning and is the foundation for many large language models."}, {"fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016-01-01", "reason": "This paper introduced residual connections, which enable the training of very deep networks by mitigating the vanishing gradient problem and are a key component of modern deep learning architectures."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-01-01", "reason": "This paper introduced BERT, a pre-trained language model that significantly improved performance on a wide range of natural language processing tasks."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This paper explored the capabilities of large language models as few-shot learners, demonstrating their ability to perform well on various tasks with limited training examples."}, {"fullname_first_author": "Defa Zhu", "paper_title": "Hyper-connections", "publication_date": "2024-01-01", "reason": "This paper introduces Hyper-Connections, a generalization of residual connections that improves model performance and is the baseline to be compared with the proposed Frac-Connections."}]}