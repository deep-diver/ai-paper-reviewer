{"references": [{"fullname_first_author": "Bai, J.", "paper_title": "Qwen technical report", "publication_date": "2023-09-16", "reason": "This paper introduces the Qwen large language model, a foundational model used in the CODEI/O experiments, significantly influencing the results and analysis."}, {"fullname_first_author": "Ben Allal, L.", "paper_title": "Smollm-corpus", "publication_date": "2024-00-00", "reason": "This paper provides the Python-Edu dataset, a crucial component of CODEI/O's training data, which substantially contributes to the model's reasoning capabilities."}, {"fullname_first_author": "Cobbe, K.", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-14", "reason": "This paper introduces a method for improving performance on math reasoning tasks, which is directly relevant to one of CODEI/O's target applications."}, {"fullname_first_author": "DeepSeek-AI", "paper_title": "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model", "publication_date": "2024-05-00", "reason": "This paper presents DeepSeek-V2.5, the model used to generate Chain-of-Thought rationales in CODEI/O, forming a cornerstone of the methodology."}, {"fullname_first_author": "DeepSeek-AI", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning", "publication_date": "2025-01-00", "reason": "This paper introduces DeepSeek-R1, a related approach to enhancing reasoning capabilities, providing context and comparison to the CODEI/O methodology."}]}