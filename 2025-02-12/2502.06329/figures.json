[{"figure_path": "https://arxiv.org/html/2502.06329/x1.png", "caption": "Figure 1: FailSafeQA: Robustness and Context Grounding Evaluation We evaluate the resilience of an LLM-based QA system in two case studies: Query Failure and Context Failure. In the Query Failure scenario, we perturb the original query into three variants: containing spelling errors (Misspelled Query), query-term form (Incomplete Query), rephrased to exclude in-domain terminology (Out-of-Domain Query). In the Context Failure case, we assume users can either fail to upload the document (Missing Context) , use degraged quality documents due to OCR (OCRed Context) or upload a document irrelevant to the query (Irrelevant Context). Robustness involves maintaining consistent model performance across perturbations (A)-(C) and (E), which preserve the intended meaning, while Context Grounding involves preventing hallucinations in scenarios (D) and (F).", "description": "Figure 1 illustrates the FailSafeQA benchmark, designed to assess the robustness and context grounding of LLMs in a financial QA setting.  It focuses on two scenarios: Query Failure and Context Failure. Query Failure tests the LLM's ability to handle variations in the input query, such as spelling errors (Misspelled Query), incomplete queries (Incomplete Query), and queries phrased in non-domain-specific language (Out-of-Domain Query). Context Failure simulates real-world challenges where users may provide incomplete (Missing Context), low-quality (OCRed Context), or irrelevant (Irrelevant Context) documents.  The figure shows how these perturbations affect LLM responses.  Robustness is measured by consistent performance across Query Failure scenarios (A-C and E), while Context Grounding evaluates the ability to avoid hallucinations when facing Context Failure (D and F).", "section": "2 FailSafeQA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.06329/extracted/6191732/assets/verb_dobj_base_new.png", "caption": "Figure 2: The Dataset Analysis of root verbs and their direct objects from the first sentence of each normalized query shows the top 20 verbs and their top five direct objects22footnotemark: 2. This distribution can be used as a proxy measure for the diversity of tasks in the dataset, with 83.0% related to question answering (QA) and 17.0% involving text generation (TG).", "description": "Figure 2 presents an analysis of the dataset's query diversity.  It examines the root verbs and their direct objects from the first sentence of each normalized query in the FailSafeQA dataset. The figure highlights the top 20 most frequent verbs and their five most common direct objects, offering insight into the types of tasks represented.  Specifically, it reveals that 83% of the tasks fall under question answering (QA), while 17% involve text generation (TG), demonstrating a substantial bias toward QA tasks within the dataset.", "section": "2 FailSafeQA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.06329/x2.png", "caption": "Figure 3: Answer Relevance Classes We evaluate two scenarios in our benchmark: when models should provide an answer (ANSWER QUERY) and when they must decline to answer (REFUSE QUERY) due to lack of relevant context. Our findings reveal that all the tested models are more adept at offering suitable answers than providing a justified refusal in situations where the context lacks sufficient information. Among all models evaluated, Palmyra-Fin-128k-Instruct demonstrates the most effective balance between these capabilities.", "description": "Figure 3 is a heatmap visualizing the performance of 24 different LLMs on two tasks: answering a question when sufficient context is available (ANSWER QUERY), and appropriately refusing to answer when context is lacking (REFUSE QUERY). The heatmap shows that most LLMs perform better at answering questions than at correctly declining to answer due to insufficient context. The model Palmyra-Fin-128k-Instruct demonstrates the best balance between these two capabilities.", "section": "3 Metrics"}, {"figure_path": "https://arxiv.org/html/2502.06329/x3.png", "caption": "Figure 4: Robustness and Compliance (Left) All models lose with respect to the baseline when input perturbations are applied. The biggest drop is observed for Out-Of-Domain and OCR context perturbations. Among the 24242424 tested models, OpenAI o3-mini is the most robust. (Right) Reasoning models like OpenAI-o1/o3-mini and the DeepSeek-R1 series reach scores up to 0.590.590.590.59, while Qwen models consistently surpass 0.600.600.600.60. Palmyra-Fin-128k-Instruct excels with the highest Context Grounding score of 0.800.800.800.80.", "description": "Figure 4 presents a comparative analysis of 24 large language models (LLMs) across two key metrics: Robustness and Context Grounding.  The left panel illustrates the Robustness of each model, showing a decline in performance compared to a baseline when various input perturbations (misspellings, incomplete queries, out-of-domain queries, OCR errors) are introduced.  The figure highlights that the OpenAI 03-mini model exhibits the highest robustness despite this performance drop. The right panel showcases the Context Grounding capabilities of the models.  Reasoning models like OpenAI 01/03-mini and the DeepSeek-R1 series achieve scores up to 0.59, while Qwen models consistently outperform them with scores above 0.60.  Palmyra-Fin-128k-Instruct demonstrates exceptional Context Grounding, achieving the highest score of 0.80.", "section": "Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.06329/extracted/6191732/assets2/robustness_query_type.png", "caption": "Figure 5: Robustness vs. Query Type. (Left) Across all models, the decrease in robustness is more prominent in text generation (TG) than in question-answering (QA) tasks. (Right) Similar statement also holds true for Context Grounding - when a model is asked to generate text (e.g., a blog post), it is more likely to ignore the lack of relevant information and fabricate details. For almost all models, it is easier to refuse to answer based on a wrong document (irrelevant context) than to deal with empty context (e.g., due to a failed document upload).", "description": "Figure 5 presents a comparison of model performance across different query types (question answering vs. text generation) and input conditions (presence or absence of context). The left panel illustrates that the drop in robustness is more significant for text generation tasks than for question answering tasks, indicating that models struggle more to provide consistent, accurate responses when generating text rather than answering factual questions. The right panel focuses on context grounding, showing that models are more likely to hallucinate details when generating text, ignoring the absence of relevant information.  In contrast, when faced with an empty or irrelevant context, models tend to correctly refuse to answer.", "section": "5 Results"}, {"figure_path": "https://arxiv.org/html/2502.06329/extracted/6191732/assets2/context_grounding_query_type.png", "caption": "Figure 6: Context Grounding vs. Query Type. (Left) Across all models, the decrease in robustness is more prominent in text generation (TG) than in question-answering (QA) tasks. (Right) Similar statement also holds true for Context Grounding - when a model is asked to generate text (e.g., a blog post), it is more likely to ignore the lack of relevant information and fabricate details. For all models, it is easier to refuse to answer based on a wrong document (irrelevant context) than to deal with empty context (e.g., due to a failed document upload).", "description": "Figure 6 presents a comparative analysis of how different Large Language Models (LLMs) perform on two tasks: question answering (QA) and text generation (TG), under varying conditions of context relevance.  The left panel shows that the drop in model performance (robustness) is generally more pronounced for text generation when compared to question answering. The right panel demonstrates that the ability of LLMs to effectively recognize and handle irrelevant or missing contextual information is also significantly impacted by the task type, with text generation exhibiting a greater tendency to ignore the lack of relevant information and generate fabricated content. Models find it easier to decline answering a question when provided with an irrelevant document than when dealing with a missing document.", "section": "5 Results"}]