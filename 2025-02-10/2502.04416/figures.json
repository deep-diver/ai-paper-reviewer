[{"figure_path": "https://arxiv.org/html/2502.04416/x1.png", "caption": "Figure 1: \nThe overview of our proposed CMoE.", "description": "This figure provides a high-level overview of the proposed Carved Mixture-of-Experts (CMoE) model.  It illustrates the key steps involved in transforming a dense feed-forward network (FFN) into a sparse MoE architecture.  The process begins with neuron activation profiling to identify neurons with high and low activation rates.  These neurons are then grouped into shared and routed experts, respectively. A training-free routing mechanism is constructed using activation statistics from representative neurons in each expert cluster. Finally, the CMoE model performs inference by activating only a subset of experts per token, achieving efficient and sparse computation.", "section": "IV. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2502.04416/x2.png", "caption": "Figure 2: The histogram of FFN hidden state \ud835\udc21\ud835\udc21\\mathbf{h}bold_h for the 3333-th block and the 1,00010001,0001 , 000-th token.", "description": "This histogram visualizes the distribution of hidden state values (h) for a specific neuron within the 3rd feed-forward network (FFN) block of a large language model (LLM), at the 1000th token during processing.  The x-axis represents the hidden state values (h), and the y-axis represents their frequency. The histogram illustrates the high sparsity of activation in FFN layers, with the majority of values concentrated near zero. This observation supports the core hypothesis of the paper, which is that dense LLMs have significant activation sparsity that can be leveraged for efficient model compression through mixture-of-experts (MoE) techniques.  The figure's details demonstrate the prevalence of near-zero activation values, confirming the existence of significant sparsity in the network's hidden layers, and hence the potential for using MoE methods.", "section": "IV. Methodology"}, {"figure_path": "https://arxiv.org/html/2502.04416/x3.png", "caption": "Figure 3: The histogram of activation rates \ud835\udf41\ud835\udf41\\bm{\\mu}bold_italic_\u03bc for the 3333-th block with Ka=1,000subscript\ud835\udc3e\ud835\udc4e1000K_{a}=1,000italic_K start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT = 1 , 000.", "description": "Figure 3 shows the distribution of activation rates for neurons within the third feed-forward network (FFN) block of a large language model (LLM).  Each neuron's activation rate (\u03bc) represents the frequency with which it is activated across a subset of input tokens.  The histogram visualizes this distribution, revealing a highly skewed distribution with most neurons exhibiting low activation rates and a long tail indicating a few highly active neurons (those that are activated often). The parameter Ka = 1000 controls the number of top neurons considered when calculating activation rates. This figure supports the paper's claim of significant activation sparsity in LLMs, motivating the development of mixture-of-experts (MoE) models.", "section": "IV. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2502.04416/x4.png", "caption": "Figure 4: Trade-off between Model Performance and Construction Time with Increasing Training Data.", "description": "This figure illustrates the relationship between the amount of training data used and both the model's performance (measured by perplexity) and the time it takes to construct the model.  It shows that while increasing the training data significantly improves the model's performance initially, the gains diminish as the amount of data increases.  Moreover, the construction time increases non-linearly with the amount of training data.  This demonstrates a trade-off: substantial performance improvements can be achieved with a relatively small amount of data, but further improvements require disproportionately more time and resources.", "section": "V. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.04416/x5.png", "caption": "Figure 5: \nAblation studies:(a) Impact of shared expert ratio on model performance;\n(b) Activation Rate vs.\u00a0Model Performance;\n(c) Effect of Load Balancing.", "description": "This figure presents ablation study results to analyze the impact of different factors on the performance of the proposed CMoE model.  Specifically, it shows three subfigures:\n(a) Impact of shared expert ratio: This plot illustrates how varying the proportion of shared experts within the activated experts affects model performance (perplexity).  It demonstrates the relationship between the ratio of shared experts and the model's performance. \n(b) Activation rate vs. model performance: This plot investigates the relationship between the total activation rate (the combined proportion of shared and activated routed experts) and the model's performance (perplexity) on two different datasets (WikiText-2 and C4). It assesses the effect of different activation rates on model perplexity, showing how sparsity impacts performance. \n(c) Effect of load balancing: This visualization compares the distribution of computational workload across experts before and after implementing the proposed load-balancing mechanism.  It highlights how the load-balancing strategy improves the even distribution of tasks among the experts, increasing efficiency and potentially parallelization.", "section": "V. EXPERIMENTS"}]