<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-02-10s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/</link><description>Recent content in 2025-02-10s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Fri, 07 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/index.xml" rel="self" type="application/rss+xml"/><item><title>ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04689/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04689/</guid><description>ARR: A novel zero-shot prompting method significantly boosts LLM performance on diverse question-answering tasks by explicitly incorporating question analysis, information retrieval, and step-by-step &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04689/cover.png"/></item><item><title>AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05176/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05176/</guid><description>AuraFusion360: High-quality 360° scene inpainting achieved via novel augmented unseen region alignment and a new benchmark dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05176/cover.png"/></item><item><title>DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05163/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05163/</guid><description>DuoGuard: a novel two-player RL framework generates high-quality synthetic data, improving multilingual LLM safety by outperforming state-of-the-art models with a significantly smaller model size and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05163/cover.png"/></item><item><title>FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05179/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05179/</guid><description>FlashVideo: Generate stunning high-resolution videos efficiently using a two-stage framework prioritizing fidelity and detail, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05179/cover.png"/></item><item><title>Generating Symbolic World Models via Test-time Scaling of Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04728/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04728/</guid><description>LLMs excel at complex reasoning but struggle with planning; this paper introduces a test-time scaling approach that enhances LLMs&amp;rsquo; PDDL reasoning, enabling high-quality PDDL domain generation, outperf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04728/cover.png"/></item><item><title>Goku: Flow Based Video Generative Foundation Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04896/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04896/</guid><description>Goku: a novel family of joint image-and-video generation models uses rectified flow Transformers, achieving industry-leading performance with a robust data pipeline and training infrastructure.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04896/cover.png"/></item><item><title>QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05178/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05178/</guid><description>QLIP: A new visual tokenizer unifying autoregressive multimodal understanding &amp;amp; generation with state-of-the-art reconstruction and zero-shot performance!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05178/cover.png"/></item><item><title>QuEST: Stable Training of LLMs with 1-Bit Weights and Activations</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05003/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05003/</guid><description>QuEST enables stable, accurate LLM training using only 1-bit weights and activations, achieving Pareto-optimal performance compared to higher-precision models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05003/cover.png"/></item><item><title>Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05171/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05171/</guid><description>Boost LLM reasoning power at test time by recursively processing latent information, enabling dramatic performance gains with fewer parameters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05171/cover.png"/></item><item><title>VideoRoPE: What Makes for Good Video Rotary Position Embedding?</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05173/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05173/</guid><description>VideoRoPE enhances video processing in Transformer models by introducing a novel 3D rotary position embedding that preserves spatio-temporal relationships, resulting in superior performance across var&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.05173/cover.png"/></item><item><title>Agency Is Frame-Dependent</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04403/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04403/</guid><description>Agency, a key concept in AI, is shown to be relative to the observer&amp;rsquo;s perspective (frame-dependent), challenging traditional binary definitions and necessitating a more nuanced approach for AI system&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04403/cover.png"/></item><item><title>CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04416/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04416/</guid><description>CMOE efficiently transforms dense LLMs into sparse MoE architectures via expert carving, enabling fast inference without extensive retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04416/cover.png"/></item><item><title>Fast Video Generation with Sliding Tile Attention</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04507/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04507/</guid><description>Sliding Tile Attention (STA) boosts video generation speed by 2.43-3.53x without losing quality by exploiting inherent data redundancy in video diffusion models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04507/cover.png"/></item><item><title>Linear Correlation in LM's Compositional Generalization and Hallucination</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04520/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04520/</guid><description>Language models surprisingly exhibit linear relationships when composing knowledge; this linearity, resilient to fine-tuning, predicts compositional generalization and hallucination.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04520/cover.png"/></item><item><title>Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.03738/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.03738/</guid><description>Smaller image patches improve vision transformer performance, defying conventional wisdom and revealing a new scaling law for enhanced visual understanding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.03738/cover.png"/></item><item><title>On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04363/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04363/</guid><description>On-device Sora makes high-quality, diffusion-based text-to-video generation possible on smartphones, overcoming computational and memory limitations through novel techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-10/2502.04363/cover.png"/></item></channel></rss>