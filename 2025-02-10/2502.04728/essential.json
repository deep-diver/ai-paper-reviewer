{"importance": "This paper is important because it presents a novel approach to automated PDDL domain synthesis, which is a crucial step in enabling large language models to perform complex planning tasks.  **The proposed test-time scaling method significantly improves the accuracy and efficiency of PDDL generation, outperforming existing state-of-the-art methods.** This work opens avenues for future research in enhancing LLM reasoning capabilities and developing more robust planning systems.", "summary": "LLMs excel at complex reasoning but struggle with planning; this paper introduces a test-time scaling approach that enhances LLMs' PDDL reasoning, enabling high-quality PDDL domain generation, outperforming existing methods.", "takeaways": ["A novel test-time scaling approach significantly improves LLMs' ability to generate high-quality PDDL domains.", "The proposed method, combining Best-of-N sampling and Instance Verbalized Machine Learning, outperforms existing state-of-the-art methods.", "PDDL-based world model generation facilitates more robust and efficient planning compared to direct LLM-based approaches."], "tldr": "Large Language Models (LLMs) show promise in complex reasoning but often fail at tasks requiring deductive reasoning and principled planning.  A common approach uses the Planning Domain Definition Language (PDDL) to formally describe planning problems, allowing the use of classical search algorithms. However, generating high-quality PDDL domains from natural language or PDDL problems directly using current LLMs is challenging due to data scarcity and ambiguity in natural language. This necessitates human intervention or extensive training data, hindering automation and scalability.\nThis research introduces a test-time scaling method that effectively addresses these challenges.  **The method utilizes Best-of-N sampling to generate diverse initial PDDL candidates and then refines them iteratively using Instance Verbalized Machine Learning (iVML).**  iVML leverages LLMs to verify PDDL domain validity and generate feedback, allowing for iterative refinement without requiring additional training data.  This approach substantially improves the success rate of PDDL domain generation across different LLMs, surpassing existing state-of-the-art performance.", "affiliation": "Hong Kong University of Science and Technology", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2502.04728/podcast.wav"}