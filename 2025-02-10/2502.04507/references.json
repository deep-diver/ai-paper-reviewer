{"references": [{"fullname_first_author": "Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-10-26", "reason": "This paper is foundational to the work in the current paper as it introduces diffusion transformers, which are the central topic of the current paper."}, {"fullname_first_author": "Song", "paper_title": "Denoising diffusion implicit models", "publication_date": "2020-10-26", "reason": "This paper introduces denoising diffusion probabilistic models (DDPMs), a core concept behind the diffusion transformers used in the current paper"}, {"fullname_first_author": "Hassani", "paper_title": "Neighborhood attention transformer", "publication_date": "2023-06-01", "reason": "This paper proposes a sliding window attention mechanism which is similar to the sliding tile attention used in the current paper and provides a key comparative baseline."}, {"fullname_first_author": "Shah", "paper_title": "Flashattention-3: Fast and accurate attention with asynchrony and low-precision", "publication_date": "2024-07-08", "reason": "This paper introduces FlashAttention-3, a highly efficient attention mechanism that the current paper builds upon and improves upon."}, {"fullname_first_author": "Liu", "paper_title": "Swin transformer: Hierarchical vision transformer using shifted windows", "publication_date": "2021-03-22", "reason": "This paper introduces Swin Transformer, which is used as a baseline and comparison point to evaluate the efficiency of the methods in the current paper."}]}