{"references": [{"fullname_first_author": "A. Ahmadian", "paper_title": "Back to basics: Revisiting reinforce style optimization for learning from human feedback in LLMs", "publication_date": "2024-02-14", "reason": "This paper proposes a novel reinforcement learning approach for LLM alignment which addresses the computational cost and operational challenges in the existing reward model based RLHF methods."}, {"fullname_first_author": "M. G. Azar", "paper_title": "A general theoretical paradigm to understand learning from human preferences", "publication_date": "2024-00-00", "reason": "This paper provides a general theoretical framework for preference-based reinforcement learning, establishing convergence and theoretical properties for two-player RL framework used in DuoGuard."}, {"fullname_first_author": "R. Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-05-18", "reason": "This paper introduces Direct Preference Optimization (DPO), a simplified approach to preference optimization that avoids the computational cost of training a reward model, which is directly applied in DuoGuard to train the generator."}, {"fullname_first_author": "J. Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-06", "reason": "This paper proposes Proximal Policy Optimization (PPO), a widely used reinforcement learning algorithm for training the generator in DuoGuard due to its stability and computational efficiency."}, {"fullname_first_author": "Y. Zeng", "paper_title": "Shieldgemma: Generative AI content moderation based on gemma", "publication_date": "2024-07-21", "reason": "This paper introduces ShieldGemma, a state-of-the-art multilingual guardrail model, against which DuoGuard is compared in the experimental evaluation to demonstrate superior performance."}]}