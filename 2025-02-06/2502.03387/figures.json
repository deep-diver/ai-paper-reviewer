[{"figure_path": "https://arxiv.org/html/2502.03387/x1.png", "caption": "Figure 1: \nLIMO achieves substantial improvement over NuminaMath with fewer samples while excelling across diverse mathematical and multi-discipline benchmarks.", "description": "This figure showcases the performance of the LIMO model compared to NuminaMath.  LIMO demonstrates significantly better performance across various mathematical and multi-disciplinary benchmarks, achieving this with a drastically smaller number of training samples (817 samples for LIMO vs. 100,000 for NuminaMath).  The chart visually represents LIMO's superior performance across multiple evaluation metrics, highlighting its efficiency and effectiveness even with limited data. The results suggest that LIMO's superior performance is due to its capability to effectively utilize a pre-trained knowledge base, thus reducing the need for extensive supervised fine-tuning.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2502.03387/extracted/6181360/figure/data-quality.png", "caption": "Figure 2: Comparison of models trained on reasoning chains of different quality levels.", "description": "The figure shows the performance of models trained on reasoning chains of varying quality levels on two mathematical reasoning benchmarks: AIME24 and MATH500.  The x-axis represents the five quality levels of reasoning chains (L1-L5, with L5 being the highest quality). The y-axis represents the accuracy of the model on each benchmark. The graph shows that models trained on higher quality reasoning chains (L4 and L5) consistently achieve significantly better performance compared to models trained on lower quality reasoning chains (L1, L2 and L3). The difference in performance highlights the substantial impact of reasoning chain quality on the model's ability to solve mathematical reasoning problems. This supports the paper's central hypothesis that high-quality, concise training data is more effective than large amounts of low-quality data in training strong reasoning capabilities in large language models.", "section": "5.3 Analysis"}, {"figure_path": "https://arxiv.org/html/2502.03387/extracted/6181360/figure/quality-1.png", "caption": "Figure 3: \nPerformance comparison on MATH and AIME benchmarks between models trained on different question quality: Simple-500, Complex-500, and Advanced-500.", "description": "Figure 3 presents a comparison of model performance on two mathematical reasoning benchmarks, MATH and AIME.  Three different models were trained, each using a dataset of 500 problems of varying difficulty.  Simple-500 represents a dataset of relatively easier problems, Complex-500 contains moderately difficult problems, and Advanced-500 consists of the most challenging problems. The figure displays the accuracy of each model on both benchmarks, illustrating the impact of question difficulty on model performance and showing that models trained on more challenging problems achieve higher accuracy.", "section": "5.3 Analysis"}, {"figure_path": "https://arxiv.org/html/2502.03387/extracted/6181360/figure/llm-backbone.png", "caption": "Figure 4: Impact of Pre-trained Model Choice on Mathematical Reasoning Performance", "description": "This figure compares the performance of two large language models (LLMs) fine-tuned using the same dataset (LIMO) but with different pre-trained backbones: Qwen1.5-32B-Chat and Qwen2.5-32B-Instruct.  The models' performance is evaluated on two mathematical reasoning benchmarks: the American Invitational Mathematics Examination (AIME24) and MATH500. The results demonstrate a significant improvement in performance when using Qwen2.5-32B-Instruct as the pre-trained model, highlighting the importance of the pre-trained model's knowledge base in achieving high performance in mathematical reasoning tasks, even with minimal fine-tuning data. This supports the paper's Less-Is-More Reasoning Hypothesis.", "section": "5.3.3 RQ3: LLM Backbone"}, {"figure_path": "https://arxiv.org/html/2502.03387/x2.png", "caption": "Figure 5: Comparison between the responses generated by Qwen2.5-32B-Instruct, DeepSeek-R1, and LIMO", "description": "Figure 5 showcases a comparative analysis of responses generated by three different large language models (LLMs) to the same mathematical reasoning problem.  The models compared are Qwen2.5-32B-Instruct (a strong baseline model), DeepSeek-R1 (a model known for its advanced reasoning capabilities), and LIMO (the authors' proposed model). The figure visually illustrates the differences in the approaches these models take to solving the problem, highlighting LIMO's superior performance and ability to generate more detailed, self-correcting reasoning chains. This emphasizes LIMO's enhanced reasoning capability and its ability to leverage the full inference-time compute capabilities, aspects directly related to the Less-is-More Reasoning Hypothesis presented in the paper.", "section": "5.3.3 RQ3: LLM Backbone"}]