[{"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/training.png", "caption": "Figure 1: An illustration of our RL scheme. The RL agent (left) maintains a policy network that, given the state of the Tanner graph, selects an action of adding or removing an edge. The environment (right) updates the graph accordingly and returns a reward based on the code\u2019s new distance and weight. This reward signal is then used to update the policy network, guiding the agent toward better code designs.", "description": "This figure illustrates the reinforcement learning (RL) framework used to discover highly efficient low-weight quantum error-correcting codes.  The RL agent interacts with an environment representing a Tanner graph, which is a graphical representation of the quantum code. At each step, the agent decides whether to add or remove an edge in the graph (the action). The environment then updates the graph based on the agent's action and returns a reward.  This reward is calculated using the code's new distance and weight.  The reward signal informs the agent about the effectiveness of its actions, guiding the learning process towards better code designs (i.e., codes with smaller weights and larger distances).", "section": "II. Reinforcement Learning Framework for Weight Reduction"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/evolution2.png", "caption": "(a)", "description": "This figure displays the training trajectories of three different codes with varying parameters, averaged over three runs.  It showcases how the reward (a measure of the code's quality) changes over time steps during the reinforcement learning process.  The parameters used to generate each code are given in the legend: e.g., [[976,16,12]](10,15) represents the values of the parameters (n, k, d)(w, q).", "section": "III. MAIN RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/pca.png", "caption": "(b)", "description": "This figure (Figure 2b) shows the evolution of code parameters (weight, degree, and distance) over a single training episode of the reinforcement learning algorithm for weight reduction.  It illustrates how the RL agent modifies these parameters to optimize code properties.  Specifically, it demonstrates the interplay between weight/degree reduction and distance preservation during the learning process. The curves illustrate the dynamic adjustments to the code's characteristics to achieve better code parameters.", "section": "II. Reinforcement Learning Framework for Weight Reduction"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/parallel_coords_300dpi.png", "caption": "(c)", "description": "This figure shows the exploration of the RL agent in the state space of Tanner graphs over 10 episodes.  Each color represents a different episode. The plot uses Principal Component Analysis (PCA) to reduce the dimensionality of the high-dimensional Tanner graph state space to two principal components (PC1 and PC2). The trajectory of each episode in this reduced space shows how the agent explores different graph structures while learning to optimize the code parameters.", "section": "II. Reinforcement Learning Framework for Weight Reduction"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/hastingsandn30.png", "caption": "Figure 2: Reinforcement learning-driven code design. (a) Training trajectories of codes with varying parameters averaged over 3 runs. (b) Evolution of parameters in the three example codes throughout a single episode.(c) Exploration of 10 episodes (represented by different colors) over PCA decomposition of state space.", "description": "This figure shows the results of using reinforcement learning to design quantum error-correcting codes. Panel (a) displays the learning curves for three different code designs, illustrating how the reward (a measure of code quality) changes over time.  Panel (b) zooms in on a single learning episode for those same three codes, showing the evolution of key code parameters (weight, degree, and distance) as the algorithm iteratively improves the design. Finally, Panel (c) provides a dimensionality-reduced view of the state space explored by the algorithm across 10 different learning episodes, offering insights into the search process and the variety of code designs discovered.", "section": "II. Reinforcement Learning Framework for Weight Reduction"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/overheadoverall2.png", "caption": "Figure 3:  Parallel coordinates plot comparing hypergraph product base codes (blue) and RL-optimized codes (red) after weight reduction. For each color, 475 codes (including 10 high-distance ones beyond the HGP-30 regime) with varying parameters are shown. Each vertical axis is normalized to the maximum observed value for that parameter, and each line traces a single code\u2019s parameters across all axes.", "description": "Figure 3 visualizes the effect of the reinforcement learning (RL)-based weight reduction method on hypergraph product codes.  It uses a parallel coordinates plot to compare the parameters of original hypergraph product codes (blue lines) and the codes after RL optimization (red lines).  The plot includes 475 codes of each type, with the additional 10 high-distance codes extending beyond the HGP-30 dataset. Each vertical axis represents a code parameter (n, k, d, w, q), normalized to its maximum observed value.  Each line in the plot shows a single code's parameter values across all axes, providing a visual comparison of the parameter distributions before and after weight reduction.", "section": "III. MAIN RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/ratevsreld.png", "caption": "Figure 4:  Comparisons of codes discovered by our RL-based scheme and existing weight reduction methods. (top) Comparison with Hastings\u00a0[45] (data taken from Ref.\u00a0[46]) and SOTA results from Sabo et al.\u00a0[46]. (bottom) Comparisons with SOTA results on all hypergraph product codes constructed from n\u226430\ud835\udc5b30n\\leq 30italic_n \u2264 30 classical codes. Explicit code parameters are shown in Table\u00a01,\u00a02.", "description": "Figure 4 presents a comparison of the performance of the reinforcement learning (RL)-based weight reduction method introduced in the paper against two other existing methods: Hastings' method and the state-of-the-art (SOTA) method from Sabo et al.  The top part of the figure focuses on comparisons for low-rate, low-distance codes, directly contrasting the qubit overhead of each method. The bottom part expands the comparison to a wider range of hypergraph product codes with various parameters constructed using classical codes (n\u226430). The explicit code parameters are tabulated in Tables I and II in the supplementary information.", "section": "III. MAIN RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/rldistances.png", "caption": "Figure 5: Breakdown of overhead factors shown by heatmaps at varying n\ud835\udc5bnitalic_n, k\ud835\udc58kitalic_k, d\ud835\udc51ditalic_d parameters. The top and bottom rows correspond to codes discovered by our RL weight-reduction scheme and Sabo et al.\u2019s method, respectively.\nGradients are binned for ease of visualization and not exact representations of overhead factors, as seen in the varying scales.", "description": "Figure 5 presents a comparison of the overhead factors achieved by two different weight-reduction methods: the reinforcement learning (RL) approach introduced in this paper and the method proposed by Sabo et al.  The overhead factor is calculated as the ratio of physical qubits used after the weight reduction process to the number of physical qubits used before the process.  The figure displays heatmaps showing the distribution of these overhead factors for different values of code parameters, namely the code length (n), the number of logical qubits (k), and the code distance (d). Separate heatmaps are presented for both methods. The color gradient in each heatmap represents the magnitude of the overhead factor.  Note that the color scales differ between the heatmaps, and the color gradients are binned for better visualization, resulting in approximate rather than precise values.", "section": "III. MAIN RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/hgpscatterplots.png", "caption": "(a)", "description": "This figure displays the evolution of the spectral gap and eigenvalues of the Tanner graphs over time steps during training. The spectral gap, representing the difference between the first two largest eigenvalues, starts high and decreases as training progresses. This indicates a change in the structure of the Tanner graphs, moving from well-structured expanders to less structured ones. Simultaneously, the eigenvalues that were originally concentrated near +1 and -1 spread out, further highlighting the structural changes. The observations suggest that the RL agent is discovering codes outside the typical theoretical frameworks relying on expansion properties, even if this sacrifices performance in message-passing-based decoding.", "section": "III. MAIN RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/rlscatterplots.png", "caption": "(b)", "description": "This figure displays the evolution of various parameters during a single training episode of the reinforcement learning model.  The plots track the changes in distance, weight, and degree of the Tanner graph representing the quantum code over time. This illustrates the learning process and how the RL agent modifies the code's properties to reduce its weight while aiming to maintain or improve the distance. The fluctuations in the parameters reveal the search strategy of the RL agent, balancing exploration and exploitation.", "section": "II. Reinforcement Learning Framework for Weight Reduction"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/reducedistance.png", "caption": "(c)", "description": "This figure shows the exploration of the RL agent in the state space of Tanner graphs over 10 episodes.  Each color represents a different episode.  The graph displays the principle components (PC1 and PC2) of the state space. The exploration shows how the agent modifies the Tanner graph, which represents the stabilizer code, over the course of the training process.", "section": "II. Reinforcement Learning Framework for Weight Reduction"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/spectralgap.png", "caption": "Figure 6:  Comparisons against SOTA on weight reduction and RL code design.\n(a)\u00a0Comparison of qubit overheads of weight reduction to (6,3) between RL and SOTA on all hypergraph product codes constructed from classical codes with n\u226430\ud835\udc5b30n\\leq 30italic_n \u2264 30.\n(b)\u00a0Rate vs.\u00a0relative distance for weight-reduced vs.\u00a0hypergraph product base codes.\n(c)\u00a0Comparisons of code parameters against various alternative RL methods for code design. Ten additional (6,3) codes (labeled by diamonds) beyond the HGP-30 regime were produced using our RL model to display examples of particularly high distance codes.", "description": "Figure 6 presents a comparison of the performance of the proposed reinforcement learning (RL)-based weight reduction method against existing state-of-the-art (SOTA) techniques.  Subfigure (a) shows a direct comparison of qubit overhead for codes reduced to weight 6 and degree 3, highlighting the significant reduction achieved by the RL method.  Subfigure (b) illustrates the relationship between code rate and relative distance for both weight-reduced codes and the original hypergraph product base codes. Subfigure (c) compares the code parameters (n, k, d) obtained using the RL approach to those generated by alternative RL-based code design methods. Notably, the RL method discovers several new codes with exceptionally high distances, which are represented by diamonds in the plot, exceeding the capabilities of previously reported methods.", "section": "III. MAIN RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/eigenvaluespectrogram.png", "caption": "(a)", "description": "The figure shows training trajectories of three different codes with varying parameters, averaged over three runs.  Each line represents a single training episode. The reward increases as the training progresses and the RL agent finds better code designs. This indicates that the RL model is effectively learning to find better designs. The figure also includes a plot showing the evolution of parameters (weight, degree, and distance) for each example code throughout a single episode, demonstrating a reduction of weight and degree with a corresponding increase in distance.", "section": "III. MAIN RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/paretofronts.png", "caption": "(b)", "description": "This figure (Fig. 2b) displays the evolution of three key parameters\u2014weight, degree, and distance\u2014over a single training episode for three example codes. It visually demonstrates how these parameters change as the reinforcement learning agent modifies the code's Tanner graph.  The plot illustrates the agent's learning process, showing the interplay between weight reduction, degree control, and distance maintenance, a core challenge in quantum code design. By observing these trends, one can better understand the algorithm's dynamics and its effectiveness in achieving low-weight, high-distance quantum error-correcting codes.", "section": "II. Reinforcement Learning Framework for Weight Reduction"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/metaanalysisd.png", "caption": "Figure 7: \nPairwise scatter plots for the parameters of the HGP-30 base codes and RL codes.\n(a) Hypergraph product code with parameters n,k,d,w,q\ud835\udc5b\ud835\udc58\ud835\udc51\ud835\udc64\ud835\udc5en,k,d,w,qitalic_n , italic_k , italic_d , italic_w , italic_q before weight reduction,\n(b) RL codes after weight reduction.\nEach subplot compares two parameters (off-diagonal) while the diagonal entries depict individual parameter distributions.", "description": "This figure presents pairwise scatter plots visualizing the relationships between different code parameters before and after applying the reinforcement learning (RL)-based weight reduction technique.  Panel (a) shows the distribution of parameters (n: code length, k: number of logical qubits, d: distance, w: check weight, q: qubit degree) in the original hypergraph product codes (HGP-30). Panel (b) displays the same parameters after the RL-based weight reduction algorithm has been applied. Each off-diagonal subplot shows a scatter plot illustrating the correlation (or lack thereof) between a pair of parameters. The diagonal subplots present the individual distributions of each parameter. The plots provide insights into how RL alters the relationship between code parameters during the optimization process, showing both the initial characteristics of the HGP-30 codes and the resulting distribution of parameters after RL optimization. ", "section": "Main Results"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/metaanalysisk.png", "caption": "Figure 8: Minimum qubit overheads under relaxed distance constraints. Permitting the distance d\ud835\udc51ditalic_d to decrease can reduce the total qubit overhead. Code parameters are shown in Table\u00a04", "description": "This figure shows how allowing a small reduction in the code distance (d) can significantly reduce the total number of physical qubits needed.  Each bar represents a specific code with its original distance and the overhead incurred when reducing the weight to (6,3) while permitting distance to decrease by 1,2 or 3.  The reduction in overhead is substantial in many cases, highlighting the trade-off between code distance and qubit resource requirements.  Specific code parameters are detailed in Table 4.", "section": "III. MAIN RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/metaanalysishgpd.png", "caption": "(a)", "description": "This figure shows the training trajectories of three example codes with varying parameters, averaged over three runs.  The plot illustrates how the reward, which balances node degree reduction with distance preservation, changes over time steps during the training process.  It also depicts the evolution of key parameters (distance, weight, and degree) within a single training episode and shows the exploration of the state space of the Tanner graphs over 10 episodes using a PCA decomposition. This visualization helps understand the RL agent's learning process in optimizing QEC codes.", "section": "II. REINFORCEMENT LEARNING FRAMEWORK FOR WEIGHT REDUCTION"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/metaanalysishgpk.png", "caption": "(b)", "description": "This figure (b) shows the evolution of code parameters (weight, degree, and distance) over a single training episode for three example codes.  It illustrates how the reinforcement learning agent modifies the code's structure during the learning process.  The x-axis represents time steps in the training process, and the y-axis shows the values of the parameters. Each line corresponds to a specific code parameter.", "section": "II. REINFORCEMENT LEARNING FRAMEWORK FOR WEIGHT REDUCTION"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/hgpregressions.png", "caption": "Figure 9: (a)\u00a0Spectral gap evolution.\nOur RL agent causes the Tanner graphs to rapidly lose their expander properties, with the gap \u03bb1\u2212\u03bb2subscript\ud835\udf061subscript\ud835\udf062\\lambda_{1}-\\lambda_{2}italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_\u03bb start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT stabilizing around \u22640.5absent0.5\\leq 0.5\u2264 0.5.\nThis happens simultaneously to reduction of weight and degree in the Tanner graph.\n(b)\u00a0Tanner graph eigenvalues evolution.\nThe eigenvalues begin concentrated around +11+1+ 1 and \u221211-1- 1, and spread out quickly.\nIt is interesting that the codes our agent finds are significantly less structured and tend to have nearly random eigenvalue distributions.\nThis suggests that our agent finds codes largely outside the realm of theoretical constructions,\nwhich often tend to rely on expansion-related arguments, although this is also at the cost of worse performance on message-passing-based decoding.", "description": "Figure 9 demonstrates the impact of the reinforcement learning (RL) agent on the spectral properties of Tanner graphs representing quantum error-correcting codes. Panel (a) shows that the spectral gap (the difference between the largest and second largest eigenvalues) decreases and stabilizes below 0.5. This indicates a loss of expander properties, signifying the codes produced by the RL agent are less structured and not easily classifiable using traditional theoretical methods. Panel (b) displays the evolution of eigenvalues over time, starting from being concentrated around +1 and -1 and spreading out.  This confirms that the agent generates codes with nearly random eigenvalue distributions, distinct from the structured codes commonly found in theoretical studies that rely on expansion properties. While this unstructured nature implies suboptimal performance with message-passing decoding, the RL algorithm finds codes that are surprisingly efficient for practical implementations.", "section": "III. MAIN RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/rlregressions.png", "caption": "Figure 10: Pareto fronts of n,k,d\ud835\udc5b\ud835\udc58\ud835\udc51n,k,ditalic_n , italic_k , italic_d parameters.\nThe plotted Pareto fronts show locally optimal (n,k,d)\ud835\udc5b\ud835\udc58\ud835\udc51(n,k,d)( italic_n , italic_k , italic_d ) codes found by our RL agent. We observe there is considerable opportunity to improve k/n\ud835\udc58\ud835\udc5bk/nitalic_k / italic_n and d/n\ud835\udc51\ud835\udc5bd/nitalic_d / italic_n in the weight-reduction setting, especially since hypergraph product code codes cannot reach certain theoretical bounds.\nNote: The 10 additional points discussed in the main text are omitted from these plots.", "description": "Figure 10 presents Pareto fronts, which are plots illustrating the trade-off between different parameters of quantum error-correcting codes.  Each point on a Pareto front represents a code with specific values for the number of physical qubits (n), the number of logical qubits (k), and the code distance (d). The points are chosen such that no other code exists that is better in terms of all three parameters (i.e., has a higher k and d for the same n or the same k and d with a smaller n).  The figure shows Pareto fronts generated by the reinforcement learning (RL) algorithm developed by the authors. The analysis reveals a significant opportunity to optimize the code rate (k/n) and relative distance (d/n) further through their weight reduction technique, surpassing what hypergraph product codes can achieve theoretically.", "section": "III. MAIN RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.14372/extracted/6219227/extrapolations.png", "caption": "(a)", "description": "This figure shows the training trajectories of three different codes with varying parameters, averaged over three runs.  It demonstrates how the reward, a measure of the code's quality, changes over time steps during the training process. The evolution of the code's parameters (distance, weight, degree) is shown throughout a single episode for each code. The exploration of the state space (Tanner graphs) is visualized over ten episodes using a PCA decomposition, showing the range of code designs the RL algorithm explored.", "section": "III. MAIN RESULTS"}]