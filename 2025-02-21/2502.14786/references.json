{"references": [{"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This is the original CLIP paper, which is the foundational work that SigLIP 2 builds upon as a contrastive pretraining method."}, {"fullname_first_author": "X. Zhai", "paper_title": "Sigmoid loss for language image pre-training", "publication_date": "2023-01-01", "reason": "This is the original SigLIP paper, which is what the current paper extends and improves upon and a core component of the new model."}, {"fullname_first_author": "C. Jia", "paper_title": "Scaling up visual and vision-language representation learning with noisy text supervision", "publication_date": "2021-01-01", "reason": "This paper is important because it established the ALIGN pretraining approach that contrasted CLIP."}, {"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This paper introduces the Vision Transformer (ViT) architecture, a core component of SigLIP 2's vision encoder."}, {"fullname_first_author": "M. Minderer", "paper_title": "Simple open-vocabulary object detection", "publication_date": "2022-01-01", "reason": "This paper is important since OWL-VIT detection is used to evaluate the open-vocabulary detction tasks."}]}