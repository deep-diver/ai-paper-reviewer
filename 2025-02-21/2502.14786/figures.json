[{"figure_path": "https://arxiv.org/html/2502.14786/x1.png", "caption": "Figure 1: SigLIP 2 adds the captioning-based pretraining from LocCa\u00a0[62] as well as self-distillation and masked prediction from SILC\u00a0[45] and TIPS\u00a0[38] (during the last 20% of training) to the sigmoid loss from SigLIP\u00a0[71]. For some variants, the recipe additionally involves fine-tuning with data curation\u00a0[61] or adaptation to native aspect ratio and variable sequence length\u00a0[6, 12].", "description": "This figure illustrates the SigLIP 2 training recipe, which enhances the original SigLIP model by incorporating several techniques.  It combines the original SigLIP's sigmoid loss with additional methods: caption-based pretraining (LocCa), self-distillation and masked prediction (SILC and TIPS).  The self-distillation and masked prediction are applied during the final 20% of training.  Some SigLIP 2 variants also include fine-tuning with data curation or adaptation for handling images with native aspect ratios and variable sequence lengths.", "section": "2. Training recipe"}, {"figure_path": "https://arxiv.org/html/2502.14786/x2.png", "caption": "Figure 2: \nPer-language image-text retrieval performance for SigLIP, SigLIP\u00a02 and mSigLIP on Crossmodal-3600\u00a0[58]. SigLIP\u00a02 almost matches the performance of mSigLIP (SigLIP trained on multilingual data) despite performing substantially better on English vision-language tasks (Table\u00a01).", "description": "This figure displays a comparison of image-text retrieval performance across three vision-language models: SigLIP, SigLIP 2, and mSigLIP, evaluated on the Crossmodal-3600 benchmark dataset.  The benchmark encompasses 36 different languages, and the chart shows the recall@1 score (a measure of retrieval accuracy) for each language. Notably, SigLIP 2, despite exhibiting superior performance on English-centric tasks, achieves a recall@1 nearly identical to mSigLIP (a multilingual variant of SigLIP), highlighting its strong multilingual capabilities.  This demonstrates SigLIP 2's effectiveness across a broad range of languages.", "section": "3. Experiments and results"}, {"figure_path": "https://arxiv.org/html/2502.14786/x3.png", "caption": "Figure 3: Comparing the NaFlex (a single checkpoint per model size supporting native aspect ratio and variable sequence length/resolution) and the standard square-input SigLIP\u00a02 variants which use a separate checkpoint for each sequence length/resolution. The sequence lengths annotated on the x-axis correspond to training sequence lengths for NaFlex. NaFlex interpolates fairly well between training resolutions, but does not extrapolate well (not shown).", "description": "Figure 3 compares the performance of two SigLIP 2 model variants: NaFlex and the standard square-input model.  NaFlex uses a single checkpoint for all sequence lengths and resolutions, while maintaining the native aspect ratio of the input image. In contrast, the standard model requires a separate checkpoint for each sequence length and resolution. The x-axis shows training sequence lengths for NaFlex, illustrating its ability to handle variable input sizes.  The figure demonstrates that NaFlex performs well across different input sizes by interpolating between training resolutions, although extrapolation to sizes outside the training range is not shown to be successful.", "section": "2.4. Adaptation to different resolutions"}, {"figure_path": "https://arxiv.org/html/2502.14786/x4.png", "caption": "Figure 4: Comparison of different vision encoders after training a Gemma\u00a02 LLM for 50M steps with a frozen vision encoder (PaliGemma\u00a0[7] stage 1), followed by fine-tuning the VLM on individual datasets (PaliGemma stage 3). SigLIP\u00a02 performs better than SigLIP and AIMv2\u00a0[20] for different model sizes and resolutions. Same data as in Table\u00a06.", "description": "This figure compares the performance of SigLIP 2, SigLIP, and AIMv2 vision encoders when used as part of a Vision-Language Model (VLM).  The VLMs were created by training a Gemma 2 Large Language Model (LLM) for 50 million steps with a frozen vision encoder (following the PaliGemma stage 1 training procedure), and then fine-tuning the resulting VLM on various individual datasets (PaliGemma stage 3). The figure shows the performance of each vision encoder across multiple datasets, model sizes (ViT-B/16, ViT-L/16, ViT-So400m/14), and image resolutions.  SigLIP 2 consistently outperforms both SigLIP and AIMv2, demonstrating its effectiveness as a vision encoder in VLMs.", "section": "3.2. SigLIP 2 as a vision encoder for VLMs"}, {"figure_path": "https://arxiv.org/html/2502.14786/x5.png", "caption": "Figure 5: 10-shot and 0-shot accuracy for geographically diverse object classification tasks (Dollar Street, GeoDE), as well as geolocalization (GeoDE country/region) and landmark localization (GLDv2) tasks. SigLIP\u00a02 consistently performs better than SigLIP (see Table\u00a08 for additional results).", "description": "Figure 5 presents a comparative analysis of SigLIP and SigLIP 2 models on geographically diverse object classification tasks using three benchmark datasets: Dollar Street, GeoDE (country/region), and GLDv2.  The performance is evaluated under both 10-shot and 0-shot learning scenarios. The figure visually demonstrates that SigLIP 2 consistently achieves higher accuracy than SigLIP across all datasets and learning settings.  Table 8 in the paper provides a more detailed numerical breakdown of the results shown in this figure.", "section": "3.5. Cultural diversity and fairness"}, {"figure_path": "https://arxiv.org/html/2502.14786/x6.png", "caption": "Figure 6: Representation bias (association of random objects with gender; lower is better) for different models.", "description": "Figure 6 illustrates the representation bias present in different vision-language models. Representation bias refers to the tendency of a model to associate certain objects with specific genders disproportionately.  Lower scores on the y-axis indicate less bias, signifying a more equitable association of objects with genders. The figure compares the SigLIP and SigLIP 2 models across various sizes, showcasing the improvement in reducing gender bias achieved by SigLIP 2.", "section": "3.5. Cultural diversity and fairness"}]