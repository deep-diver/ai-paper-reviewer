{"importance": "This paper introduces Logic-RL, which **develops advanced reasoning skills such as reflection and verification** that generalizes to math benchmarks. This work suggests rule-based RL can unlock emergent abilities in LLMs, offering new directions for reasoning research and task adaptation.", "summary": "Logic-RL unlocks LLM reasoning via rule-based reinforcement learning, generalizing to math problems after training on logic puzzles.", "takeaways": ["Rule-based RL can unlock advanced reasoning skills in LLMs, even with limited training data.", "A stringent format reward function and system prompt are crucial for stable RL training and preventing reward hacking.", "RL-trained models can generalize to out-of-distribution tasks, suggesting the development of abstract problem-solving schemata."], "tldr": "Inspired by DeepSeek-R1, this paper explores rule-based reinforcement learning (RL) to improve reasoning in LLMs using synthetic logic puzzles due to their controllable nature and answer verification. To address challenges with naive training, the authors make key technical contributions. This includes a system prompt to emphasize thinking, a format reward function to penalize shortcuts and a training recipe to achieve convergence. By doing so, the model can develop the ability to reflect, verify and summarize.\n\nThis paper introduces **Logic-RL**, a framework using the REINFORCE++ algorithm and designs from DeepSeek-R1. After training a 7B model on 5K logic problems, it demonstrated generalization to math benchmarks like AIME and AMC. During training, the model allocated more steps to reason, expanding from hundreds to thousands of tokens. Key findings include: language mixing hinders reasoning, increased 'thinking' tokens help, and RL generalizes better than SFT.", "affiliation": "Microsoft Research Asia", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "2502.14768/podcast.wav"}