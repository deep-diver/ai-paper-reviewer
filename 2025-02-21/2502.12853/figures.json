[{"figure_path": "https://arxiv.org/html/2502.12853/x1.png", "caption": "Figure 1: The data efficiency of S2r compared to competitive methods, with all models initialized from Qwen2.5-Math-7B.", "description": "This figure illustrates the data efficiency of the S2R model compared to other existing methods.  It shows that S2R achieves high accuracy with significantly less training data.  All models in this comparison started with the same base model (Qwen2.5-Math-7B), highlighting the effectiveness of S2R in improving reasoning abilities with limited resources.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.12853/x2.png", "caption": "Figure 2: Overview of S2r.", "description": "This figure presents a schematic overview of the S2R framework, detailing its two main stages. Stage 1 involves behavior initialization, where the model learns iterative self-verification and self-correction behaviors through supervised fine-tuning on curated data. This stage generates initial policy models exhibiting these behaviors. Stage 2 focuses on boosting these capabilities using reinforcement learning.  Outcome-level and process-level reinforcement learning are both applied, further enhancing the model's ability to adaptively refine its reasoning process during inference.  The framework uses a sequential decision-making model to represent the problem-solving process and incorporates a reward function to guide the reinforcement learning.", "section": "2 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.12853/x3.png", "caption": "(a)", "description": "This figure shows the data efficiency of the proposed S2R framework compared to several existing methods.  The x-axis represents the logarithm of the amount of training data used (in number of samples), and the y-axis shows the accuracy achieved on a particular math reasoning benchmark (MATH500). The plot demonstrates that S2R achieves high accuracy with significantly less data compared to other approaches, highlighting its data efficiency.", "section": "3.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2502.12853/x4.png", "caption": "(b)", "description": "This figure shows the evaluation results of self-verification and self-correction, comparing the performance of the model trained only with supervised fine-tuning (SFT) against models further trained with process-level and outcome-level reinforcement learning (RL).  The metrics displayed are verification accuracy, error recall, correct precision, and the rates of incorrect answers being corrected and correct answers being incorrectly altered.  The figure helps illustrate the impact of different RL training methods on the model's ability to effectively self-verify and self-correct during reasoning.", "section": "3.4 Boosting Self-verifying and Self-correcting with RL"}, {"figure_path": "https://arxiv.org/html/2502.12853/x5.png", "caption": "Figure 3: Evaluation on verification and correction.", "description": "This figure visualizes the performance of self-verification and self-correction mechanisms in three different LLMs (Llama-3.1-8B-Instruct, Qwen2-7B-Instruct, and Qwen2.5-Math-7B) before and after applying reinforcement learning (RL).  It shows how RL improves the overall verification accuracy, the ability to recall errors, and precision in correct predictions.  The self-correction metrics demonstrate that RL training enhances the rate of correctly correcting mistakes and reduces the rate of mistakenly changing correct answers to incorrect ones.", "section": "3.4 Boosting Self-verifying and Self-correcting with RL"}, {"figure_path": "https://arxiv.org/html/2502.12853/x6.png", "caption": "(a)", "description": "This figure shows the data efficiency of the proposed S2R framework compared to other methods.  All models were initialized from Qwen2.5-Math-7B.  The x-axis represents the logarithm of the data size used for training (in samples or tokens), and the y-axis shows the accuracy achieved on a particular task (likely a math reasoning task). The graph illustrates that S2R achieves high accuracy with significantly less data compared to the other models, indicating improved data efficiency.", "section": "3.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2502.12853/x7.png", "caption": "(b)", "description": "This figure shows the evolution of verification and correction capabilities of the model during training.  It presents the changes in verification accuracy, error recall, correct precision, the rate of correcting incorrect answers, and the rate of incorrectly changing correct answers, across different training stages (SFT, SFT + Process-level RL, and SFT + Outcome-level RL). The x-axis represents the training stage, while the y-axis represents the value of each metric. This allows for a visual comparison of the model's performance in self-verification and self-correction before and after applying reinforcement learning at both the process and outcome levels.", "section": "3.4 Boosting Self-verifying and Self-correcting with RL"}, {"figure_path": "https://arxiv.org/html/2502.12853/x8.png", "caption": "Figure 4: The accuracy and average trial number of different models across difficulty levels. Evaluated on MATH500 test set.", "description": "Figure 4 presents a comparative analysis of model performance across varying problem difficulty levels.  It shows both the accuracy and the average number of reasoning steps ('trials') required by different LLMs to solve problems within the MATH500 test set.  The difficulty levels are categorized and color-coded, allowing for a visual comparison of how effectively each model handles varying levels of problem complexity. This provides insight into the models' efficiency and reasoning abilities.", "section": "3.4.3 Improvement across Difficulty Levels"}, {"figure_path": "https://arxiv.org/html/2502.12853/x9.png", "caption": "Figure 5: SFT data example.", "description": "This figure shows an example of a data sample used for supervised fine-tuning (SFT) in Stage 1 of the S2R framework. It illustrates how trial-and-error trajectories are constructed by combining problem-solving attempts, verifications (checking the correctness of the previous attempts), and finally the correct answer. The example showcases multiple solution attempts, including both correct and incorrect ones, with corresponding verifications to demonstrate the iterative self-verification and self-correction process.  Each step in the trajectory includes an action (solve or verify) followed by the result of the action, and this shows how the system iteratively refines its reasoning towards the correct solution.", "section": "2.2 Initializing Self-verification and Self-correction Behaviors"}]