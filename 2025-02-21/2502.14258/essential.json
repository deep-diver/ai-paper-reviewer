{"importance": "This paper introduces Temporal Heads, a crucial component for LLMs to process time-specific information. This research opens new avenues for enhancing LLMs' ability to reason about dynamically changing facts, improving their reliability in real-world applications. Findings inspires future works on better temporal aware LLMs.", "summary": "LLMs have 'Temporal Heads' that process time-specific facts!", "takeaways": ["Temporal Heads are specific attention heads responsible for processing temporal knowledge.", "Ablating Temporal Heads degrades the model's ability to recall time-specific knowledge.", "Temporal knowledge can be edited by adjusting the values of Temporal Heads."], "tldr": "This research addresses the underexplored area of how language models handle facts that change over time. It introduces the concept of **Temporal Heads**, which are specific attention heads within language models primarily responsible for processing temporal knowledge. Through circuit analysis, the study confirms the presence of these heads across multiple models. Disabling these heads impairs the model's ability to recall time-specific information. \n\nThe paper demonstrates that Temporal Heads are activated by both numeric and textual temporal cues, indicating a deeper encoding of time beyond simple numerical representation. Moreover, the study shows that temporal knowledge can be edited by manipulating the values of these heads. The finding highlights **Temporal Heads** as key components for encoding and modifying time-sensitive knowledge.", "affiliation": "Korea University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2502.14258/podcast.wav"}