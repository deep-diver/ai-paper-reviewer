[{"figure_path": "https://arxiv.org/html/2503.20776/x2.png", "caption": "Figure 1: Feature4X: Building 4D Interactive Scenes with Agentic AI from Monocular Videos. By dynamically distilling model-conditioned features and integrating 2D foundation models with LLMs in feedback loops, Feature4X enables multimodal tasks across 2D, 3D, and 4D with high-level language inputs or direct user interactions, including (but not limited to) segmentation, scene editing, and VQA across novel views and all time steps, unlocking new possibilities for 4D agentic AI.", "description": "Figure 1 showcases Feature4X, a framework that generates interactive 4D scenes from monocular videos.  It combines model-distilled features, 2D foundation models, and LLMs to enable various tasks such as segmentation, scene editing, and visual question answering (VQA) across different dimensions (2D, 3D, 4D), viewpoints, and timesteps.  The user can interact using either high-level language commands or direct manipulations.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.20776/x3.png", "caption": "Figure 2: Method overview.\nGiven an input monocular video, we infer 2D priors to segment static background (represented by static 3D Gaussians augmented with latent features) and dynamic foreground (represented by dynamic 3D Gaussians guided by Motion Scaffolds\u00a0[35], a set of nodes {\ud835\udc2fi}subscript\ud835\udc2f\ud835\udc56\\{\\mathbf{v}_{i}\\}{ bold_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } encoding 3D motion trajectories and latent features hisubscript\u210e\ud835\udc56h_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT). Dynamic Gaussian features and motions are computed via interpolation from their K\ud835\udc3eKitalic_K-nearest scaffold nodes. At each timestep, dynamic Gaussians are warped and fused with static Gaussians. A parallel rasterization\u00a0[102] generates RGB images and a unified latent feature map, decoded into task-specific features\u2014illustrated here by SAM2\u00a0[68], CLIP-LSeg\u00a0[36], and InternVideo2\u00a0[84] for representative 2D (novel view segmentation), 3D (scene editing), and 4D (spatiotemporal VQA) tasks. Our framework generalizes to any 2D vision foundation model and is trained end-to-end using input RGB frames and customized features from pretrained 2D models. At inference, rendered feature maps from arbitrary views and timesteps are directly fed into task-specific decoders, seamlessly supporting user prompts and LLM interactions to form a unified 4D agentic AI system.", "description": "Figure 2 illustrates the Feature4X framework's architecture for processing monocular video input into a 4D representation suitable for various AI tasks.  2D priors are used to delineate static and dynamic elements. Static elements are represented by static 3D Gaussians combined with latent features, while dynamic elements are modeled using dynamic 3D Gaussians guided by Motion Scaffolds (a graph structure encoding motion trajectories and latent features).  These Gaussians are then combined and processed through a parallel rasterization technique generating RGB images and a unified latent feature map. This map is subsequently decoded into task-specific features, as exemplified using SAM2, CLIP-LSeg, and InternVideo2 for 2D, 3D, and 4D tasks respectively. The framework's adaptability to various 2D vision models and end-to-end training approach are also highlighted, demonstrating its ability to seamlessly support user prompts and LLM interactions for creating a unified 4D agentic AI system.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.20776/x4.png", "caption": "Figure 3: Segment Anything in Dynamic 4D Scenes with SAM2 Feature Field. For any rendered novel view video, we support: (a) Promptless segmentation (segment everything): when no user prompt is provided, segmentation masks are automatically assigned at the first frame (t=0\ud835\udc610t=0italic_t = 0) and then propagated across all frames. (b) Promptable segmentation (segment anything): the user can segment any object\u2014static or dynamic\u2014at any timestep using a point or box prompt, and the corresponding mask is robustly tracked and propagated through subsequent frames.", "description": "Figure 3 demonstrates the capabilities of Feature4X in performing semantic segmentation on dynamic 4D scenes using the Segment Anything Model (SAM2).  It showcases two modes: (a) Promptless segmentation, where the system automatically segments all objects in the first frame (t=0) and propagates these masks consistently across subsequent frames, handling both static and dynamic content. (b) Promptable segmentation, which allows users to interactively segment any object (static or dynamic) at any point in time simply by providing a point or box prompt. The system then accurately tracks and propagates this mask through the rest of the video. This highlights the versatility and robustness of Feature4X in handling complex 4D scenes.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.20776/x5.png", "caption": "Figure 4: Baseline Comparison on SAM2 Inference. We compare segmentation quality and inference speed between (a) the naive RGB-based approach and (b) our feature-based method. Ours achieves comparable segmentation, accurately tracking the object over time, and avoids RGB artifacts (red box region at t=70\ud835\udc6170t=70italic_t = 70), while reducing inference time to about 4\u00d7\\times\u00d7 speed-up.", "description": "Figure 4 presents a comparison of two methods for SAM2 inference: a naive RGB-based approach and a novel feature-based approach.  The RGB-based approach processes the raw RGB video frames directly through the SAM2 model, while the feature-based approach utilizes a learned compact 4D feature field representation to achieve segmentation.  The results demonstrate that the feature-based method achieves comparable segmentation accuracy, accurately tracking the object across frames. Importantly, it avoids the RGB artifacts observed in the naive RGB-based approach, specifically around time step 70. The feature-based method also offers a significant speed-up in inference time, achieving approximately four times faster processing than the RGB-based approach.", "section": "4.1. Scene Interaction with Segment Anything"}, {"figure_path": "https://arxiv.org/html/2503.20776/x6.png", "caption": "Figure 5: Semantic 4D Scene Understanding with CLIP Feature Field. By lifting CLIP-LSeg\u00a0[36] features into a 4D feature field, we enable pixel-level semantic segmentation from any view at any timestep. This allows robust 4D scene understanding, even as object appearances change over time\u2014for example, accurately identifying a blooming flower from bud to full bloom across views.", "description": "Figure 5 demonstrates the capability of Feature4X to perform semantic 4D scene understanding.  By incorporating CLIP-LSeg features into a 4D feature field, the model achieves pixel-level semantic segmentation from any viewpoint at any point in time.  This addresses a key challenge in 3D/4D vision: maintaining consistent semantic understanding even as object appearance changes dynamically throughout the video. The example of a blooming flower, accurately identified from bud to full bloom across multiple views, showcases this robust 4D scene understanding capability.", "section": "4.2. Scene Understanding with Semantics"}, {"figure_path": "https://arxiv.org/html/2503.20776/x7.png", "caption": "Figure 6: Scene Editing with AI Agent. Given user prompts, our GPT-powered agent interprets editing intent and autonomously performs scene edits via our 4D CLIP feature field. Examples include both geometric (e.g., \u201cextract\u201d and \u201cdelete\u201d) and appearance (e.g., \u201cchange color\u201d) editing in 3D space. While results may not be perfect due to imperfect fine-grained feature alignment and non-optimal editing parameter tuning, the agent adaptively refines parameters and applies edits consistently across views and time\u2014greatly reducing the need for manual tuning\u2014and demonstrates robust, interactive 4D scene manipulation.", "description": "This figure showcases the capabilities of the LLM-powered AI agent in editing 4D scenes.  The agent takes high-level natural language instructions (e.g., \"Extract the swan\", \"Delete the moving camel\", \"Change the cow color to black and white\") and uses the 4D CLIP feature field to perform the edits.  The edits can involve geometric changes (like extracting or deleting objects) or changes in appearance (like altering an object's color).  While the results might not be perfect due to limitations in aligning fine-grained features or finding optimal editing parameters, the AI agent iteratively refines these parameters and ensures consistent application of edits across different viewpoints and time steps. This significantly simplifies the editing process and enables robust, interactive 4D scene manipulation.", "section": "4.3. Scene Editing with Language and AI Agent"}, {"figure_path": "https://arxiv.org/html/2503.20776/x8.png", "caption": "Figure 7: VQA with Chatbot Agent. (Left) Our model supports free-form VQA across diverse question types\u2014general, spatial, and temporal\u2014by distilling InternVideo2\u00a0[84] features. (Right) At each timestep, we reconstruct both a 4D radiance field and a 4D feature field, providing more inference sources beyond the input video frame\u2014including local (moving camera) and global (zoomed-out) novel views and their corresponding feature maps\u2014thereby supporting VQA in 4D and enhancing the model\u2019s spatiotemporal reasoning capabilities.", "description": "Figure 7 illustrates the model's ability to answer visual questions (VQA) using a chatbot interface.  The left side shows the diverse question types supported: general, spatial, and temporal questions. The core of this capability comes from leveraging InternVideo2's features. The right panel highlights the model's 4D reconstruction, producing both a 4D radiance field and a 4D feature field. This provides access to richer inference sources than just the original input video. These include novel views (both local, from a moving camera, and global, zoomed-out) and their corresponding feature maps. This approach is crucial for robust spatiotemporal reasoning in VQA.", "section": "4.4 Scene Reasoning with 4D Chatbot Agent"}, {"figure_path": "https://arxiv.org/html/2503.20776/x9.png", "caption": "Figure A: Feature Field Visualizations. We visualize our versatile Gaussian feature field along with its decoded SAM2, CLIP, and InternVideo feature fields using PCA.", "description": "This figure visualizes the versatile Gaussian feature field developed in the Feature4X framework.  It uses Principal Component Analysis (PCA) to reduce the dimensionality of the feature fields for better visualization.  The visualization includes the unified latent feature field and its decoded versions for three different 2D foundation models: Segment Anything (SAM2), CLIP-LSeg, and InternVideo.  Each model's decoded feature field provides a different perspective on the underlying 4D scene representation, highlighting the versatility of the Feature4X framework in adapting to various downstream tasks.", "section": "A. Details of 4D Reconstruction"}, {"figure_path": "https://arxiv.org/html/2503.20776/x10.png", "caption": "Figure B: Overview of the editing framework. GPT-4o generates different editing configurations based on user prompts, selects target regions via hybrid filtering, evaluates their outputs, and selects the best configuration.", "description": "This figure illustrates the 4D scene editing framework.  It begins with a user providing a high-level prompt (e.g., \"Make the dog's color look like Clifford\").  This prompt is processed by GPT-40, a large language model, to generate different editing configurations. These configurations include details like which objects to edit, what operations to perform, the specific targets for the edits, and threshold settings. The model then uses a hybrid filtering method to identify the relevant Gaussians (points in a 3D Gaussian splatting representation) within the 4D scene that correspond to the targeted objects. The system then applies the selected edit operation and renders the result. Finally, the GPT-40 model evaluates the rendered image quality and selects the best configuration, ensuring consistency and high quality results across the 4D scene.", "section": "3.4 Interaction with AI Agent via Feature Fields"}, {"figure_path": "https://arxiv.org/html/2503.20776/x11.png", "caption": "Figure C: CLIP semantic segmentation quality comparison. We compare the CLIP semantic segmentation quality between ground-truth (inference from RGB) and our implementation (inference from feature map) for both training and novel views.", "description": "This figure compares the semantic segmentation results obtained using CLIP features.  Two methods are compared: one using ground-truth RGB images and the other using the proposed method's feature maps. The comparison is done for both training and novel (unseen) views to show how well the proposed method generalizes to new viewpoints.  The results visually demonstrate the effectiveness of the proposed method for accurate and consistent semantic segmentation across different views, even without directly using the full RGB image information.", "section": "4.1. Scene Interaction with Segment Anything"}, {"figure_path": "https://arxiv.org/html/2503.20776/x12.png", "caption": "Figure D: SAM2 segmentation quality comparison for different dimensions of unified latent feature maps Best performing SAM2 segmentation is derived from the 32-dimensional unified latent feature map.", "description": "This figure compares the segmentation quality achieved using the Segment Anything Model 2 (SAM2) with different dimensions for the unified latent feature maps. The results show that a 32-dimensional unified latent feature map produces the best performing SAM2 segmentation, accurately tracking the object over time and avoiding artifacts present in the results from other dimensions.", "section": "4.1 Scene Interaction with Segment Anything"}, {"figure_path": "https://arxiv.org/html/2503.20776/x13.png", "caption": "Figure E: Training Time vs Unified Latent Feature Dimensions We show the training time required with different dimensions of unified latent feature map.", "description": "This figure shows the training time taken for different dimensions of the unified latent feature map.  It demonstrates the trade-off between model complexity and training efficiency.  As the dimension of the latent feature increases, so does the training time.  This is because higher dimensions increase the number of parameters the model needs to learn, leading to longer training times.", "section": "F. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2503.20776/x14.png", "caption": "Figure F: Rendering Time vs Unified Latent Feature Dimensions We show the rendering time required for different dimensions of unified latent feature map.", "description": "This figure demonstrates the relationship between the dimensionality of the unified latent feature map and the time needed for rendering.  As the dimensionality increases, the rendering time also increases, highlighting a trade-off between feature representation complexity and computational efficiency.  The plot likely shows an exponential or near-exponential increase in rendering time as the dimension increases, indicating that higher-dimensional features significantly increase computational demands during the rendering process.", "section": "F. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2503.20776/x15.png", "caption": "Figure G: Training Time vs CLIP Feature Dimensions We show the training time required with different dimensions of rendered CLIP features.", "description": "This figure shows a graph illustrating the relationship between the training time and the dimensionality of rendered CLIP features.  Higher dimensional rendered CLIP features lead to longer training times. This demonstrates a trade-off between model complexity and training efficiency.", "section": "F. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2503.20776/x16.png", "caption": "Figure H: Rendering Time vs CLIP Feature Dimensions We show the rendering time required for different dimensions of rendered CLIP features.", "description": "Figure H illustrates the relationship between the rendering time and the dimensions of rendered CLIP features.  The x-axis represents the different dimensions of the rendered CLIP features used in the experiment, while the y-axis shows the corresponding rendering time.  The graph likely demonstrates that increasing the dimensions of the CLIP features significantly increases the rendering time required.  This suggests a trade-off between feature richness and computational efficiency.", "section": "F. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2503.20776/x17.png", "caption": "Figure I: mIoU vs CLIP Feature Dimensions We show mIoU with respect to different rendered CLIP feature dimensions.", "description": "This figure presents the results of an ablation study that examines how different dimensions of rendered CLIP features affect the mean Intersection over Union (mIoU) score in semantic segmentation.  The x-axis represents the dimension of the rendered CLIP features, while the y-axis shows the corresponding mIoU score.  The graph helps to determine the optimal dimensionality for achieving a balance between performance and computational efficiency.", "section": "F. Ablation Studies"}]