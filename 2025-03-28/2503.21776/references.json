{"references": [{"fullname_first_author": "Shuai Bai", "paper_title": "Qwen2. 5-vl technical report", "publication_date": "2025-02-01", "reason": "This paper details the Qwen2.5-VL model, which is used as the base MLLM in Video-R1."}, {"fullname_first_author": "Zesen Cheng", "paper_title": "Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms", "publication_date": "2024-06-07", "reason": "This paper presents VideoLLaMA2, a significant advancement in video-language modeling which is important to consider when enhancing video understanding."}, {"fullname_first_author": "Daya Guo", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in Ilms via reinforcement learning", "publication_date": "2025-01-12", "reason": "This paper introduces DeepSeek-R1, the inspiration and methodological basis for Video-R1's approach to eliciting reasoning abilities through reinforcement learning."}, {"fullname_first_author": "Kairui Hu", "paper_title": "Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos", "publication_date": "2025-01-13", "reason": "This paper introduces the VideoMMMU benchmark, one of the key benchmarks used to evaluate Video-R1's performance."}, {"fullname_first_author": "Jihan Yang", "paper_title": "Thinking in space: How multimodal large language models see, remember, and recall spaces", "publication_date": "2024-12-14", "reason": "This paper introduces the VSI-Bench, another key benchmark used to evaluate Video-R1's spatial reasoning capabilities."}]}