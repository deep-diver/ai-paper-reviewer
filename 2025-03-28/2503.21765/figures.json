[{"figure_path": "https://arxiv.org/html/2503.21765/x1.png", "caption": "Figure 1: \nVideo cases generated by three typical state-of-the-art generative video models[1, 48, 87]. We can observe that these advanced models still struggle to produce satisfying videos that strictly conform to physical laws.", "description": "Figure 1 showcases videos generated by three leading-edge video generation models (Open-Sora 1.2, CogVideoX 5B, and Cosmos 7B) in response to various prompts.  The examples highlight a common challenge in current generative video models: while they can produce visually realistic videos, these videos often violate fundamental laws of physics. This discrepancy, which can range from minor inconsistencies to significant violations of Newtonian mechanics, is a major area of current research and improvement efforts in the field of AI video generation.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.21765/x2.png", "caption": "Figure 2: Cognitive evolution processes of individuals and generation system.", "description": "This figure illustrates the parallel evolution of human cognitive development and the capabilities of video generation systems.  It maps Piaget's stages of cognitive development ('Intuitive Perception', 'Symbolic Learning', 'Interaction') to the three-tiered taxonomy of physical cognition in video generation: 1) Basic Schematic Perception, 2) Passive Cognition of Physical Knowledge, and 3) Active Cognition for World Simulation. Each tier represents a progressive level of understanding and modeling of physical phenomena, mirroring the stages of human cognitive development.", "section": "1.2 Taxonomy"}, {"figure_path": "https://arxiv.org/html/2503.21765/x3.png", "caption": "Figure 3: The taxonomy of PhysGenBench[86] benchmark, including 4 physical commonsense and 27 physical laws.", "description": "This figure presents a taxonomy of physical phenomena categorized into four main domains: mechanics, optics, thermodynamics, and material properties, which constitute the PhysGenBench benchmark.  The benchmark includes a total of 27 physical laws and 4 commonsense principles, illustrating the wide range of physical concepts covered in the benchmark.  Each domain is further divided into subcategories representing specific physical laws or principles related to the domain.  This comprehensive taxonomy is essential for evaluating the ability of generative models to accurately represent physical realism in simulated environments.", "section": "3 BACKGROUND"}, {"figure_path": "https://arxiv.org/html/2503.21765/x4.png", "caption": "Figure 4: Introduction to mainstream generative models: GANs[100], Diffusion Models[101], NeRF[102], Gaussian Splatting[103].", "description": "Figure 4 illustrates four prominent generative models frequently used in video generation: Generative Adversarial Networks (GANs), Diffusion Models, Neural Radiance Fields (NeRF), and Gaussian Splatting.  Each model is represented with a schematic diagram highlighting its core components and functionalities. GANs are shown with their competing generator and discriminator networks. Diffusion Models depict the iterative noise addition and removal process.  NeRF illustrates the volumetric scene representation and novel view synthesis. Lastly, Gaussian Splatting shows the scene representation as a set of 3D Gaussian kernels.", "section": "3 BACKGROUND"}, {"figure_path": "https://arxiv.org/html/2503.21765/x5.png", "caption": "Figure 5: Overview of the evolution of physical cognition in video generation. Please note that the typical methods listed here cover only a subset of the relevant literature and do not represent all existing studies.", "description": "This figure presents a hierarchical taxonomy of video generation methods categorized by their level of physical cognition.  It illustrates the evolutionary path from simpler methods that only capture basic motion patterns to more advanced approaches integrating complex physical simulations and interactions. The tree structure shows the progression of techniques from 'Basic Schematic Perception for Generation' (using simple motion cues) through 'Passive Cognition of Physical Knowledge for Generation' (incorporating physical knowledge from simulators or LLMs) to 'Active Cognition for World Simulation' (actively interacting with a simulated or real environment). Each level contains example methods, highlighting the development of physically informed video generation.", "section": "1.3 Structure"}, {"figure_path": "https://arxiv.org/html/2503.21765/x6.png", "caption": "Figure 6: Basic motion signal-guided generation pipeline. (a) is a video-guided generation pipeline in Sec.\u00a04.1, while (b) is a motion-guided generation pipeline in Sec.\u00a04.2.", "description": "Figure 6 illustrates two distinct pipelines for generating videos using motion signals as guidance.  Panel (a) shows a video-guided approach (detailed in Section 4.1), where motion information is extracted from a reference video and used to generate a new video with similar movement but potentially different content (e.g., changing the appearance while retaining the movement from the reference video).  Panel (b) displays a motion-guided approach (explained in Section 4.2), which directly incorporates explicit motion signals (such as optical flow or trajectories) as input to control the generation process. This approach allows for more direct manipulation of the generated video's movement.", "section": "4 Basic Schematic Perception for Generation"}, {"figure_path": "https://arxiv.org/html/2503.21765/x7.png", "caption": "Figure 7: Physics-inspired regularization pipeline.", "description": "The figure illustrates the pipeline of physics-inspired regularization in video generation.  It shows how geometric and energy conservation losses are used to constrain a generative model's output, ensuring that the generated video adheres to physical laws. The geometric loss focuses on maintaining the consistency of geometric relationships throughout the video, while the energy conservation loss ensures that energy is conserved in physical interactions within the scene. This combination of losses guides the model towards creating videos that are not only visually appealing but also physically plausible.", "section": "5 Passive Cognition of Physical Knowledge for Generation"}, {"figure_path": "https://arxiv.org/html/2503.21765/x8.png", "caption": "Figure 8: Generation pipeline based on physical simulation, cf. Physguassian[132].", "description": "This figure illustrates the generation pipeline of PhysGaussian [132], a method that integrates physical simulation with visual rendering.  It begins with a 3D representation of the scene.  This representation is then fed into a Material Point Method (MPM) simulator, which applies forces and simulates the resulting physical interactions and dynamics. The results of this simulation are used in a rendering step to produce a final video.", "section": "5.2 Physics Simulation-based Generation"}, {"figure_path": "https://arxiv.org/html/2503.21765/x9.png", "caption": "Figure 9: Physics simulator generates motion signals.", "description": "This figure illustrates a pipeline where a physics simulator is used to generate motion signals which are then incorporated into a video generation model.  The physics simulator takes various inputs, such as 3D Gaussian models, possibly including additional inputs not explicitly shown in the figure (like scene geometry, material properties, external forces, etc). It simulates these inputs to produce motion signals representing the dynamics in the scene according to physical laws. These motion signals are then fed into a video generation model, and this model uses these motion signals along with other information to generate the final video. This process aims to enhance the realism and physical accuracy of the generated video by grounding it in physically plausible motions from the physics simulation.", "section": "5.2 Physics Simulation-based Generation"}, {"figure_path": "https://arxiv.org/html/2503.21765/x10.png", "caption": "Figure 10: Architecture of PhysDreamer[126]. Optimize the material field and velocity field by minimizing the discrepancy between the rendered video and the reference video.", "description": "PhysDreamer's architecture uses a differentiable material point method (MPM) simulator to generate physically plausible foreground dynamics.  The model iteratively refines both the material field and velocity field by minimizing the difference between its rendered video and a reference video. This iterative process optimizes the video's physical accuracy against the reference.", "section": "5.2 Physics Simulation-based Generation"}, {"figure_path": "https://arxiv.org/html/2503.21765/x11.png", "caption": "Figure 11: World physics knowledge in LLMs is used for scene layouts, program generation, and properties inference.", "description": "This figure illustrates how Large Language Models (LLMs) can be used to incorporate physical knowledge into video generation.  It shows a three-stage process: 1) **Dynamic Scene Layouts:** The LLM takes a user's text prompt and generates a scene layout by planning the spatiotemporal arrangement of objects. This layout is informed by the physical world knowledge present in the LLM. 2) **Program Generation:**  A program is generated from the user instructions.  This program details the actions needed to manipulate objects in the scene, again informed by physical principles implicit in the LLM's knowledge. 3) **Physical Properties Inference:** The LLM identifies materials and infers their relevant physical properties (e.g., density, Young's modulus) based on the objects in the scene and the intended actions.  The inferred properties are then used to guide the simulation and generation of physically plausible videos.", "section": "5.4 LLMs Empowering Physical Simulation"}, {"figure_path": "https://arxiv.org/html/2503.21765/x12.png", "caption": "Figure 12: The model actively interacts with the environment to achieve future prediction through multimodal data-driven generation, spatial environment perception, and external feedback optimization.", "description": "This figure illustrates the architecture of a model that actively interacts with its environment to predict future events.  This is achieved through a combination of three key components: 1) Multimodal Data-driven Generation: The model integrates various data sources (video, images, text, actions) to build a comprehensive understanding of the environment.  2) Spatial Environment Perception:  The model utilizes scene decomposition techniques (like open-vocabulary segmentation and tracking) to analyze the objects and their relationships within the scene.  3) External Feedback Optimization:  The model receives feedback from its actions in the environment (e.g., real-world feedback) to refine its predictions and control its interactions.  The loop of prediction, action, and feedback allows the model to learn and adapt to dynamic changes in the environment, improving its ability to predict future states.", "section": "Active Cognition for World Simulation"}]