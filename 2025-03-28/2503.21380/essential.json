{"importance": "This paper introduces a **new Olympiad-level math benchmark for LLMs**, addressing limitations in existing benchmarks. It offers rigorous evaluations, bilingual assessment, and insights into models' reasoning abilities, **opening new research avenues** to improve LLMs in mathematical problem-solving.", "summary": "OlymMATH: A new Olympiad-level math benchmark rigorously tests LLMs' reasoning, revealing limitations and paving the way for advancements.", "takeaways": ["OlymMATH is a new, challenging benchmark for evaluating mathematical reasoning in LLMs.", "Current state-of-the-art LLMs struggle with the difficulty of OlymMATH's problems.", "The benchmark enables bilingual assessment, revealing language-specific performance differences."], "tldr": "Existing benchmarks for evaluating LLMs' mathematical reasoning are becoming saturated. The rapid development of LLMs necessitates more challenging and rigorous evaluations to identify their limitations and guide future improvements. Current benchmarks struggle to differentiate capabilities and multilingual assessment is also lacking. \n\nThis paper introduces **OlymMATH**, a new Olympiad-level math benchmark to rigorously test LLMs' reasoning. OlymMATH has 200 problems with varying difficulty, manually verified and available in both English and Chinese. Experiments show that models like DeepSeek-R1 struggle with OlymMATH, highlighting its difficulty. The benchmark facilitates bilingual assessment and the analysis helps understand limitations in solving complex math problems.", "affiliation": "Renmin University of China", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.21380/podcast.wav"}