<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Large Language Model Agent: A Survey on Methodology, Applications and Challenges &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="Large Language Model Agent: A Survey on Methodology, Applications and Challenges &#183; HF Daily Paper Reviews by AI"><meta name=description content="This survey presents a methodology-centered taxonomy of LLM agent systems, linking design principles to emergent behaviors and identifying future research directions."><meta name=keywords content="Natural Language Processing,Large Language Models,üè¢ Peking University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/2025-03-28/2503.21460/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/2025-03-28/2503.21460/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="Large Language Model Agent: A Survey on Methodology, Applications and Challenges"><meta property="og:description" content="This survey presents a methodology-centered taxonomy of LLM agent systems, linking design principles to emergent behaviors and identifying future research directions."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="2025-03-28"><meta property="article:published_time" content="2025-03-27T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-27T00:00:00+00:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="üè¢ Peking University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/2025-03-28/2503.21460/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/2025-03-28/2503.21460/cover.png"><meta name=twitter:title content="Large Language Model Agent: A Survey on Methodology, Applications and Challenges"><meta name=twitter:description content="This survey presents a methodology-centered taxonomy of LLM agent systems, linking design principles to emergent behaviors and identifying future research directions."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"2025-03-28s","name":"Large Language Model Agent: A Survey on Methodology, Applications and Challenges","headline":"Large Language Model Agent: A Survey on Methodology, Applications and Challenges","abstract":"This survey presents a methodology-centered taxonomy of LLM agent systems, linking design principles to emergent behaviors and identifying future research directions.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/2025-03-28\/2503.21460\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-03-27T00:00:00\u002b00:00","datePublished":"2025-03-27T00:00:00\u002b00:00","dateModified":"2025-03-27T00:00:00\u002b00:00","keywords":["Natural Language Processing","Large Language Models","üè¢ Peking University"],"mainEntityOfPage":"true","wordCount":"2979"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-26/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-26</p></a><a href=/ai-paper-reviewer/2025-03-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-27</p></a><a href=/ai-paper-reviewer/2025-03-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-28</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-26/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-26</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-27</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-28</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/2025-03-28/2503.21460/cover_hu763614635695440255.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/2025-03-28/>2025-03-28s</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/2025-03-28/2503.21460/>Large Language Model Agent: A Survey on Methodology, Applications and Challenges</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Large Language Model Agent: A Survey on Methodology, Applications and Challenges</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-03-27T00:00:00+00:00>27 March 2025</time><span class="px-2 text-primary-500">&#183;</span><span>2979 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">14 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_2025-03-28/2503.21460/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_2025-03-28/2503.21460/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/natural-language-processing/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Natural Language Processing
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/large-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-peking-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Peking University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#agent-lifecycle>Agent Lifecycle</a></li><li><a href=#rag-as-memory>RAG as Memory</a></li><li><a href=#multi-agent-collab>Multi-Agent Collab</a></li><li><a href=#dataset-genesis>Dataset Genesis</a></li><li><a href=#llm-privacy>LLM Privacy</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#agent-lifecycle>Agent Lifecycle</a></li><li><a href=#rag-as-memory>RAG as Memory</a></li><li><a href=#multi-agent-collab>Multi-Agent Collab</a></li><li><a href=#dataset-genesis>Dataset Genesis</a></li><li><a href=#llm-privacy>LLM Privacy</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2503.21460</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Junyu Luo et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-03-28</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2503.21460 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2503.21460 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2503.21460/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p><strong>LLM agents</strong>, powered by large language models, are intelligent entities that can perceive environments, reason about goals, and execute actions. Unlike traditional AI, they actively engage through continuous learning & adaptation, marking a technological leap & reimagining human-machine relationships. However, challenges remain to construct high-quality multi-agent system. Therefore, existing research can be fragmented and lack of organized taxonomy, while others examine components separately.</p><p>To address these challenges, this survey systematically deconstructs <strong>LLM agent systems</strong> through construction, collaboration, and evolution. It offers a comprehensive perspective on how agents are built, interact, and evolve, while addressing evaluation, tools, real-world challenges, and applications. The study highlights fundamental connections between agent design principles and emergent behaviors, providing a unified architectural view and identifying promising research directions. The collection is available in github.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-dc36452239dee448fe97f129bb377ce4></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-dc36452239dee448fe97f129bb377ce4",{strings:[" LLM agents represent a critical pathway toward artificial general intelligence, exhibiting goal-driven behaviors and dynamic adaptation. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-d0af004a4c9e20a3261255cd2e4d5dbb></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-d0af004a4c9e20a3261255cd2e4d5dbb",{strings:[" The survey systematically deconstructs LLM agent systems through three interconnected dimensions: construction, collaboration, and evolution. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-633418bca41afa257819b4efb54098cb></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-633418bca41afa257819b4efb54098cb",{strings:[" The research identifies emerging patterns and offers a structured taxonomy for understanding LLM agents, facilitating future research. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This survey is important for researchers to <strong>navigate the rapidly evolving landscape of LLM agents</strong>. It provides a <strong>structured taxonomy for understanding agent architectures</strong>, identifies <strong>key challenges</strong>, and <strong>suggests directions for future research</strong>. The survey could inspire researchers to <strong>develop more robust, reliable, and ethically aligned agent systems</strong>.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.21460/x1.png alt></figure></p><blockquote><p>üîº This figure presents a comprehensive overview of the Large Language Model (LLM) agent ecosystem. It&rsquo;s structured around four interconnected dimensions: Agent Methodology (construction, collaboration, and evolution), Evaluation and Tools (benchmarks, assessment frameworks, development tools), Real-World Issues (security, privacy, and social impact), and Applications (diverse domains of LLM agent deployment). This framework helps in understanding the entire lifecycle of modern LLM-based agent systems, from their initial design and development to their real-world application and the challenges they present.</p><details><summary>read the caption</summary>Figure 1: An overview of the LLM agent ecosystem organized into four interconnected dimensions: ‚ù∂ Agent Methodology, covering the foundational aspects of construction, collaboration, and evolution; ‚ù∑ Evaluation and Tools, presenting benchmarks, assessment frameworks, and development tools; ‚ù∏ Real-World Issues, addressing critical concerns around security, privacy, and social impact; and ‚ùπ Applications, highlighting diverse domains where LLM agents are being deployed. We provide a structured framework for understanding the complete lifecycle of modern LLM-based agent systems.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S2.T1.1.1><tbody class=ltx_tbody><tr class=ltx_tr id=S2.T1.1.1.1.1><td class="ltx_td ltx_align_left ltx_border_tt" id=S2.T1.1.1.1.1.1><span class="ltx_text ltx_font_bold" id=S2.T1.1.1.1.1.1.1>Category</span></td><td class="ltx_td ltx_align_left ltx_border_tt" id=S2.T1.1.1.1.1.2><span class="ltx_text ltx_font_bold" id=S2.T1.1.1.1.1.2.1>Method</span></td><td class="ltx_td ltx_align_left ltx_border_tt" id=S2.T1.1.1.1.1.3><span class="ltx_text ltx_font_bold" id=S2.T1.1.1.1.1.3.1>Key Contribution</span></td></tr><tr class=ltx_tr id=S2.T1.1.1.2.2><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T1.1.1.2.2.1 rowspan=6><span class="ltx_text ltx_font_bold" id=S2.T1.1.1.2.2.1.1>Centralized Control</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T1.1.1.2.2.2>Coscientist¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib73 title>73</a>]</cite></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T1.1.1.2.2.3>Human-centralized experimental control</td></tr><tr class=ltx_tr id=S2.T1.1.1.3.3><td class="ltx_td ltx_align_left" id=S2.T1.1.1.3.3.1>LLM-Blender¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib74 title>74</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.3.3.2>Cross-attention response fusion</td></tr><tr class=ltx_tr id=S2.T1.1.1.4.4><td class="ltx_td ltx_align_left" id=S2.T1.1.1.4.4.1>MetaGPT¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib27 title>27</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.4.4.2>Role-specialized workflow management</td></tr><tr class=ltx_tr id=S2.T1.1.1.5.5><td class="ltx_td ltx_align_left" id=S2.T1.1.1.5.5.1>AutoAct¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib75 title>75</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.5.5.2>Triple-agent task differentiation</td></tr><tr class=ltx_tr id=S2.T1.1.1.6.6><td class="ltx_td ltx_align_left" id=S2.T1.1.1.6.6.1>Meta-Prompting¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib76 title>76</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.6.6.2>Meta-prompt task decomposition</td></tr><tr class=ltx_tr id=S2.T1.1.1.7.7><td class="ltx_td ltx_align_left" id=S2.T1.1.1.7.7.1>WJudge¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib77 title>77</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.7.7.2>Weak-discriminator validation</td></tr><tr class=ltx_tr id=S2.T1.1.1.8.8><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T1.1.1.8.8.1 rowspan=8><span class="ltx_text ltx_font_bold" id=S2.T1.1.1.8.8.1.1>Decentralized Collaboration</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T1.1.1.8.8.2>MedAgents¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib78 title>78</a>]</cite></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T1.1.1.8.8.3>Expert voting consensus</td></tr><tr class=ltx_tr id=S2.T1.1.1.9.9><td class="ltx_td ltx_align_left" id=S2.T1.1.1.9.9.1>ReConcile¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib79 title>79</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.9.9.2>Multi-agent answer refinement</td></tr><tr class=ltx_tr id=S2.T1.1.1.10.10><td class="ltx_td ltx_align_left" id=S2.T1.1.1.10.10.1>METAL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib115 title>115</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.10.10.2>Domain-specific revision agents</td></tr><tr class=ltx_tr id=S2.T1.1.1.11.11><td class="ltx_td ltx_align_left" id=S2.T1.1.1.11.11.1>DS-Agent¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib116 title>116</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.11.11.2>Database-driven revision</td></tr><tr class=ltx_tr id=S2.T1.1.1.12.12><td class="ltx_td ltx_align_left" id=S2.T1.1.1.12.12.1>MAD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib80 title>80</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.12.12.2>Structured anti-degeneration protocols</td></tr><tr class=ltx_tr id=S2.T1.1.1.13.13><td class="ltx_td ltx_align_left" id=S2.T1.1.1.13.13.1>MADR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib81 title>81</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.13.13.2>Verifiable fact-checking critiques</td></tr><tr class=ltx_tr id=S2.T1.1.1.14.14><td class="ltx_td ltx_align_left" id=S2.T1.1.1.14.14.1>MDebate¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib82 title>82</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.14.14.2>Stubborn-collaborative consensus</td></tr><tr class=ltx_tr id=S2.T1.1.1.15.15><td class="ltx_td ltx_align_left" id=S2.T1.1.1.15.15.1>AutoGen¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib26 title>26</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.15.15.2>Group-chat iterative debates</td></tr><tr class=ltx_tr id=S2.T1.1.1.16.16><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S2.T1.1.1.16.16.1 rowspan=6><span class="ltx_text ltx_font_bold" id=S2.T1.1.1.16.16.1.1>Hybrid Architecture</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T1.1.1.16.16.2>CAMEL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib25 title>25</a>]</cite></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T1.1.1.16.16.3>Grouped role-play coordination</td></tr><tr class=ltx_tr id=S2.T1.1.1.17.17><td class="ltx_td ltx_align_left" id=S2.T1.1.1.17.17.1>AFlow¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib29 title>29</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.17.17.2>Three-tier hybrid planning</td></tr><tr class=ltx_tr id=S2.T1.1.1.18.18><td class="ltx_td ltx_align_left" id=S2.T1.1.1.18.18.1>EoT¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib117 title>117</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.18.18.2>Multi-topology collaboration patterns</td></tr><tr class=ltx_tr id=S2.T1.1.1.19.19><td class="ltx_td ltx_align_left" id=S2.T1.1.1.19.19.1>DiscoGraph¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib118 title>118</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.19.19.2>Pose-aware distillation</td></tr><tr class=ltx_tr id=S2.T1.1.1.20.20><td class="ltx_td ltx_align_left" id=S2.T1.1.1.20.20.1>DyLAN¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib119 title>119</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T1.1.1.20.20.2>Importance-aware topology</td></tr><tr class=ltx_tr id=S2.T1.1.1.21.21><td class="ltx_td ltx_align_left ltx_border_bb" id=S2.T1.1.1.21.21.1>MDAgents¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib120 title>120</a>]</cite></td><td class="ltx_td ltx_align_left ltx_border_bb" id=S2.T1.1.1.21.21.2>Complexity-aware routing</td></tr></tbody></table></table></figure><blockquote><p>üîº This table categorizes and summarizes various Large Language Model (LLM) agent collaboration methods, contrasting centralized control, decentralized collaboration, and hybrid approaches. Each method is listed with a key contribution, illustrating the different ways LLM agents can interact and work together to achieve a shared goal.</p><details><summary>read the caption</summary>TABLE I: A summary of agent collaboration methods.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Agent Lifecycle<div id=agent-lifecycle class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#agent-lifecycle aria-label=Anchor>#</a></span></h4><p>While the provided paper doesn&rsquo;t explicitly use the term &lsquo;Agent Lifecycle,&rsquo; its content allows us to infer the key stages. The <strong>construction phase</strong> defines the agent&rsquo;s architecture, integrating memory, planning, and action execution. <strong>Collaboration</strong> dictates interaction with other agents or humans, using centralized, decentralized, or hybrid approaches. Finally, <strong>evolution</strong> focuses on adaptation through self-learning, multi-agent co-evolution, or external knowledge incorporation. This lifecycle underscores the dynamic nature of LLM agents, moving beyond static systems to entities that learn, adapt, and improve over time. <strong>Evaluation at every stage is critical</strong>.</p><h4 class="relative group">RAG as Memory<div id=rag-as-memory class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#rag-as-memory aria-label=Anchor>#</a></span></h4><p><strong>RAG (Retrieval-Augmented Generation) as memory</strong> enhances LLMs by integrating external knowledge, <strong>overcoming training data limitations.</strong> This paradigm encompasses static knowledge grounding via text corpora or knowledge graphs, interactive retrieval that uses agent dialogues for external queries, and reasoning-integrated retrieval, exemplified by interleaving step-by-step reasoning with dynamic knowledge acquisition. <strong>Advanced methods</strong> like KG-RAR construct task-specific subgraphs, and DeepRAG balances parametric knowledge with external evidence. These architectures maintain contextual relevance and are <strong>critical for scalable memory systems</strong>.</p><h4 class="relative group">Multi-Agent Collab<div id=multi-agent-collab class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multi-agent-collab aria-label=Anchor>#</a></span></h4><p>Multi-agent collaboration enables LLMs to extend problem-solving beyond individual reasoning. <strong>Effective collaboration leverages distributed intelligence</strong>, coordinates actions, and refines decisions through multi-agent interactions. Centralized architectures employ a hierarchical coordination mechanism where a <strong>central controller organizes agent activities</strong> through task allocation and decision integration, while other sub-agents can only communicate with the controller. In decentralized architectures, collaboration enables direct node-to-node interaction through self-organizing protocols. Finally, hybrid architectures strategically combine centralized coordination and decentralized collaboration to <strong>balance controllability with flexibility</strong> and adapt to heterogeneous task requirements.</p><h4 class="relative group">Dataset Genesis<div id=dataset-genesis class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#dataset-genesis aria-label=Anchor>#</a></span></h4><p><strong>Dataset genesis</strong> in LLM agent research focuses on how datasets are created and utilized. This involves exploring methodologies for constructing datasets that effectively train and evaluate LLM agents. A core aspect is the creation of diverse datasets covering various tasks and environments. <strong>The method involves the creation of new datasets by multiple agents</strong>. Constructing datasets with high-quality labels and annotations is a key challenge, which involves the creation of custom tools. Efficient dataset management practices are crucial to ensure scalability and accessibility. These methods are employed to create realistic testing scenarios to enhance agent robustness. Datasets are also actively curated to improve agent adaptability. Data collection and synthesis is also crucial, to have higher fidelity and trustworthiness for the agents in use.</p><h4 class="relative group">LLM Privacy<div id=llm-privacy class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#llm-privacy aria-label=Anchor>#</a></span></h4><p><strong>LLM Privacy</strong> is a pressing concern. The inherent memory capabilities of LLMs, while enabling sophisticated interactions, also create vulnerabilities. <strong>Data breaches</strong> can expose sensitive information learned during training or interaction. <strong>Mitigating strategies</strong> are vital, focusing on techniques like differential privacy to inject noise during training, thereby obscuring individual data points. Another approach is knowledge distillation, which transfers learned representations from a private model to a public one, minimizing the risk of memorization. Moreover, strict <strong>data governance policies</strong> and user controls are essential to manage access and retention. The goal is to establish a balance between functionality and responsible handling of private data.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.21460/x2.png alt></figure></p><blockquote><p>üîº This figure presents a taxonomy that categorizes the methodologies used in creating large language model (LLM) agents. It&rsquo;s structured into three main sections: Agent Construction, Agent Collaboration, and Agent Evolution. Each section further breaks down into sub-categories detailing different approaches and techniques used within each stage of agent development. This taxonomy helps to illustrate the different paths researchers and developers take when designing LLM agents, from basic profile definitions to sophisticated multi-agent collaboration mechanisms and strategies for long-term adaptation and improvement.</p><details><summary>read the caption</summary>Figure 2: A taxonomy of large language model agent methodologies.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.21460/x3.png alt></figure></p><blockquote><p>üîº Figure 3 provides a comprehensive overview of the evaluation methods and tools used for Large Language Model (LLM) agents. The figure is divided into two main sections. The left section categorizes various evaluation frameworks based on their scope and focus, including general assessment, domain-specific evaluations, and collaboration-based evaluations. This helps researchers and practitioners understand the different aspects of LLM agent performance and choose the most suitable methods for their specific needs. The right section showcases the various types of tools involved in the LLM agent ecosystem. These include tools utilized by LLM agents during task execution, tools created by LLM agents to extend functionality, and tools required for deploying, managing, and maintaining LLM agents in practical applications.</p><details><summary>read the caption</summary>Figure 3: An overview of evaluation benchmarks and tools for LLM agents. The left side shows various evaluation frameworks categorized by general assessment, domain-specific evaluation, and collaboration evaluation. The right side illustrates tools used by LLM agents, tools created by agents, and tools for deploying agents.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S2.T2.1.1><tbody class=ltx_tbody><tr class=ltx_tr id=S2.T2.1.1.1.1><td class="ltx_td ltx_align_left ltx_border_tt" id=S2.T2.1.1.1.1.1><span class="ltx_text ltx_font_bold" id=S2.T2.1.1.1.1.1.1>Category</span></td><td class="ltx_td ltx_align_left ltx_border_tt" id=S2.T2.1.1.1.1.2><span class="ltx_text ltx_font_bold" id=S2.T2.1.1.1.1.2.1>Method</span></td><td class="ltx_td ltx_align_left ltx_border_tt" id=S2.T2.1.1.1.1.3><span class="ltx_text ltx_font_bold" id=S2.T2.1.1.1.1.3.1>Key Contribution</span></td></tr><tr class=ltx_tr id=S2.T2.1.1.2.2><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.2.2.1 rowspan=3><span class="ltx_text ltx_font_bold" id=S2.T2.1.1.2.2.1.1>Self-Supervised Learning</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.2.2.2>SE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib86 title>86</a>]</cite></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.2.2.3>Adaptive token masking for pretraining</td></tr><tr class=ltx_tr id=S2.T2.1.1.3.3><td class="ltx_td ltx_align_left" id=S2.T2.1.1.3.3.1>Evolutionary Optimization¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib87 title>87</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T2.1.1.3.3.2>Efficient model merging and adaptation</td></tr><tr class=ltx_tr id=S2.T2.1.1.4.4><td class="ltx_td ltx_align_left" id=S2.T2.1.1.4.4.1>DiverseEvol¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib88 title>88</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T2.1.1.4.4.2>Improved instruction tuning via diverse data</td></tr><tr class=ltx_tr id=S2.T2.1.1.5.5><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.5.5.1 rowspan=4><span class="ltx_text ltx_font_bold" id=S2.T2.1.1.5.5.1.1>Self-Reflection & Self-Correction</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.5.5.2>SELF-REFINE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib89 title>89</a>]</cite></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.5.5.3>Iterative self-feedback for refinement</td></tr><tr class=ltx_tr id=S2.T2.1.1.6.6><td class="ltx_td ltx_align_left" id=S2.T2.1.1.6.6.1>STaR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib90 title>90</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T2.1.1.6.6.2>Bootstrapping reasoning with few rationales</td></tr><tr class=ltx_tr id=S2.T2.1.1.7.7><td class="ltx_td ltx_align_left" id=S2.T2.1.1.7.7.1>V-STaR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib91 title>91</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T2.1.1.7.7.2>Training a verifier using DPO</td></tr><tr class=ltx_tr id=S2.T2.1.1.8.8><td class="ltx_td ltx_align_left" id=S2.T2.1.1.8.8.1>Self-Verification¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib92 title>92</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T2.1.1.8.8.2>Backward verification for correction</td></tr><tr class=ltx_tr id=S2.T2.1.1.9.9><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.9.9.1 rowspan=3><span class="ltx_text ltx_font_bold" id=S2.T2.1.1.9.9.1.1>Self-Rewarding & RL</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.9.9.2>Self-Rewarding¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib93 title>93</a>]</cite></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.9.9.3>LLM-as-a-Judge for self-rewarding</td></tr><tr class=ltx_tr id=S2.T2.1.1.10.10><td class="ltx_td ltx_align_left" id=S2.T2.1.1.10.10.1>RLCD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib94 title>94</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T2.1.1.10.10.2>Contrastive distillation for alignment</td></tr><tr class=ltx_tr id=S2.T2.1.1.11.11><td class="ltx_td ltx_align_left" id=S2.T2.1.1.11.11.1>RLC¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib95 title>95</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T2.1.1.11.11.2>Evaluation-generation gap for optimization</td></tr><tr class=ltx_tr id=S2.T2.1.1.12.12><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.12.12.1 rowspan=3><span class="ltx_text ltx_font_bold" id=S2.T2.1.1.12.12.1.1>Cooperative Co-Evolution</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.12.12.2>ProAgent¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib96 title>96</a>]</cite></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.12.12.3>Intent inference for teamwork</td></tr><tr class=ltx_tr id=S2.T2.1.1.13.13><td class="ltx_td ltx_align_left" id=S2.T2.1.1.13.13.1>CORY¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib97 title>97</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T2.1.1.13.13.2>Multi-agent RL fine-tuning</td></tr><tr class=ltx_tr id=S2.T2.1.1.14.14><td class="ltx_td ltx_align_left" id=S2.T2.1.1.14.14.1>CAMEL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib25 title>25</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T2.1.1.14.14.2>Role-playing framework for cooperation</td></tr><tr class=ltx_tr id=S2.T2.1.1.15.15><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.15.15.1 rowspan=3><span class="ltx_text ltx_font_bold" id=S2.T2.1.1.15.15.1.1>Competitive Co-Evolution</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.15.15.2>Red-Team LLMs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib98 title>98</a>]</cite></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.15.15.3>Adversarial robustness training</td></tr><tr class=ltx_tr id=S2.T2.1.1.16.16><td class="ltx_td ltx_align_left" id=S2.T2.1.1.16.16.1>Multi-Agent Debate¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib82 title>82</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T2.1.1.16.16.2>Iterative critique for refinement</td></tr><tr class=ltx_tr id=S2.T2.1.1.17.17><td class="ltx_td ltx_align_left" id=S2.T2.1.1.17.17.1>MAD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib99 title>99</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T2.1.1.17.17.2>Debate-driven divergent thinking</td></tr><tr class=ltx_tr id=S2.T2.1.1.18.18><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.18.18.1 rowspan=2><span class="ltx_text ltx_font_bold" id=S2.T2.1.1.18.18.1.1>Knowledge-Enhanced Evolution</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.18.18.2>KnowAgent¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib83 title>83</a>]</cite></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.18.18.3>Action knowledge for planning</td></tr><tr class=ltx_tr id=S2.T2.1.1.19.19><td class="ltx_td ltx_align_left" id=S2.T2.1.1.19.19.1>WKM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib84 title>84</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T2.1.1.19.19.2>Synthesizing prior and dynamic knowledge</td></tr><tr class=ltx_tr id=S2.T2.1.1.20.20><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S2.T2.1.1.20.20.1 rowspan=3><span class="ltx_text ltx_font_bold" id=S2.T2.1.1.20.20.1.1>Feedback-Driven Evolution</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.20.20.2>CRITIC¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib100 title>100</a>]</cite></td><td class="ltx_td ltx_align_left ltx_border_t" id=S2.T2.1.1.20.20.3>Tool-assisted self-correction</td></tr><tr class=ltx_tr id=S2.T2.1.1.21.21><td class="ltx_td ltx_align_left" id=S2.T2.1.1.21.21.1>STE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib101 title>101</a>]</cite></td><td class="ltx_td ltx_align_left" id=S2.T2.1.1.21.21.2>Simulated trial-and-error for tool learning</td></tr><tr class=ltx_tr id=S2.T2.1.1.22.22><td class="ltx_td ltx_align_left ltx_border_bb" id=S2.T2.1.1.22.22.1>SelfEvolve¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib102 title>102</a>]</cite></td><td class="ltx_td ltx_align_left ltx_border_bb" id=S2.T2.1.1.22.22.2>Automated debugging and refinement</td></tr></tbody></table></table></figure><blockquote><p>üîº This table provides a comprehensive summary of different agent evolution methods categorized by their approach (such as self-supervised learning, self-reflection, and co-evolution). For each method, it lists the key contributions and provides a reference to the relevant research paper. This allows readers to easily compare various techniques used for enhancing LLM agents&rsquo; capabilities over time.</p><details><summary>read the caption</summary>TABLE II: A summary of agent evolution methods.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S4.T3.1.1><tbody class=ltx_tbody><tr class=ltx_tr id=S4.T3.1.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=S4.T3.1.1.1.1.1><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.1.1.1.1>Reference</span></th><td class="ltx_td ltx_align_left ltx_border_tt" id=S4.T3.1.1.1.1.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.1.1.2.1>Description</span></td></tr><tr class=ltx_tr id=S4.T3.1.1.2.2><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan=2 id=S4.T3.1.1.2.2.1><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.2.2.1.1>Adversarial Attacks and Defense</span></th></tr><tr class=ltx_tr id=S4.T3.1.1.3.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T3.1.1.3.3.1>Mo et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib177 title>177</a>]</cite></th><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T3.1.1.3.3.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.3.3.2.1>Attack:</span> Adversarial attack benchmark</td></tr><tr class=ltx_tr id=S4.T3.1.1.4.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.4.4.1>AgentDojo¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib178 title>178</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.4.4.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.4.4.2.1>Attack:</span> Adversarial attack framework</td></tr><tr class=ltx_tr id=S4.T3.1.1.5.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.5.5.1>ARE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib179 title>179</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.5.5.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.5.5.2.1>Attack:</span> Adversarial attack evaluation for multimodal agents</td></tr><tr class=ltx_tr id=S4.T3.1.1.6.6><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.6.6.1>GIGA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib181 title>181</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.6.6.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.6.6.2.1>Attack:</span> Generalizable infectious gradient attacks</td></tr><tr class=ltx_tr id=S4.T3.1.1.7.7><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.7.7.1>CheatAgent¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib180 title>180</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.7.7.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.7.7.2.1>Attack:</span> Adversarial attack agent for recommender systems</td></tr><tr class=ltx_tr id=S4.T3.1.1.8.8><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.8.8.1>LLAMOS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib182 title>182</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.8.8.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.8.8.2.1>Defense:</span> Purifying adversarial attack input</td></tr><tr class=ltx_tr id=S4.T3.1.1.9.9><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.9.9.1>Chern et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib183 title>183</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.9.9.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.9.9.2.1>Defense:</span> Defense via multi-agent debate</td></tr><tr class=ltx_tr id=S4.T3.1.1.10.10><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan=2 id=S4.T3.1.1.10.10.1><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.10.10.1.1>Jailbreaking Attacks and Defense</span></th></tr><tr class=ltx_tr id=S4.T3.1.1.11.11><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T3.1.1.11.11.1>RLTA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib184 title>184</a>]</cite></th><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T3.1.1.11.11.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.11.11.2.1>Attack:</span> Produce jailbreaking prompts via reinforcement learning</td></tr><tr class=ltx_tr id=S4.T3.1.1.12.12><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.12.12.1>Atlas¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib185 title>185</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.12.12.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.12.12.2.1>Attack:</span> Jailbreaks text-to-image models with safety filters</td></tr><tr class=ltx_tr id=S4.T3.1.1.13.13><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.13.13.1>RLbreaker¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib186 title>186</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.13.13.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.13.13.2.1>Attack:</span> Model jailbreaking as a search problem</td></tr><tr class=ltx_tr id=S4.T3.1.1.14.14><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.14.14.1>PathSeeker¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib187 title>187</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.14.14.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.14.14.2.1>Attack:</span> Use multi-agent reinforcement learning to jailbreak</td></tr><tr class=ltx_tr id=S4.T3.1.1.15.15><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.15.15.1>AutoDefense¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib188 title>188</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.15.15.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.15.15.2.1>Defense:</span> Multi-agent defense to filter harmful responses</td></tr><tr class=ltx_tr id=S4.T3.1.1.16.16><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.16.16.1>Guardians¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib189 title>189</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.16.16.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.16.16.2.1>Defense:</span> Detect rogue agents to counter jailbreaking attacks.</td></tr><tr class=ltx_tr id=S4.T3.1.1.17.17><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.17.17.1>ShieldLearner¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib190 title>190</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.17.17.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.17.17.2.1>Defense:</span> Learn attack jailbreaking patterns.</td></tr><tr class=ltx_tr id=S4.T3.1.1.18.18><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan=2 id=S4.T3.1.1.18.18.1><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.18.18.1.1>Backdoor Attacks and Defense</span></th></tr><tr class=ltx_tr id=S4.T3.1.1.19.19><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T3.1.1.19.19.1>DemonAgent¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib191 title>191</a>]</cite></th><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T3.1.1.19.19.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.19.19.2.1>Attack:</span> Encrypted muti-backdoor implantation attack</td></tr><tr class=ltx_tr id=S4.T3.1.1.20.20><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.20.20.1>Yang et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib192 title>192</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.20.20.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.20.20.2.1>Attack:</span> Backdoor attacks evaluations on LLM-based agents</td></tr><tr class=ltx_tr id=S4.T3.1.1.21.21><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.21.21.1>BadAgent¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib193 title>193</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.21.21.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.21.21.2.1>Attack:</span> Inputs or environment cues as backdoors</td></tr><tr class=ltx_tr id=S4.T3.1.1.22.22><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.22.22.1>BadJudge¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib194 title>194</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.22.22.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.22.22.2.1>Attack:</span> Backdoor to the LLM-as-a-judge agent system</td></tr><tr class=ltx_tr id=S4.T3.1.1.23.23><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.23.23.1>DarkMind¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib195 title>195</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.23.23.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.23.23.2.1>Attack:</span> latent backdoor attack to customized LLM agents</td></tr><tr class=ltx_tr id=S4.T3.1.1.24.24><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan=2 id=S4.T3.1.1.24.24.1><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.24.24.1.1>Agent Collaboration Attacks and Defense</span></th></tr><tr class=ltx_tr id=S4.T3.1.1.25.25><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T3.1.1.25.25.1>CORBA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib196 title>196</a>]</cite></th><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T3.1.1.25.25.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.25.25.2.1>Attack:</span> Multi-agent attack via multi-agent</td></tr><tr class=ltx_tr id=S4.T3.1.1.26.26><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.26.26.1>AiTM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib197 title>197</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.26.26.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.26.26.2.1>Attack:</span> Intercepte and manipulate inter-agent messages</td></tr><tr class=ltx_tr id=S4.T3.1.1.27.27><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.27.27.1>Netsafe¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib198 title>198</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.27.27.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.27.27.2.1>Defense:</span> Identify critical safety phenomena in multi-agent networks</td></tr><tr class=ltx_tr id=S4.T3.1.1.28.28><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.28.28.1>G-Safeguard¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib199 title>199</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.28.28.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.28.28.2.1>Defense:</span> leverages graph neural networks to detect anomalies</td></tr><tr class=ltx_tr id=S4.T3.1.1.29.29><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.29.29.1>Trustagent¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib200 title>200</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T3.1.1.29.29.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.29.29.2.1>Defense:</span> Agent constitution in task planning.</td></tr><tr class=ltx_tr id=S4.T3.1.1.30.30><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S4.T3.1.1.30.30.1>PsySafe¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib201 title>201</a>]</cite></th><td class="ltx_td ltx_align_left ltx_border_bb" id=S4.T3.1.1.30.30.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.30.30.2.1>Defense:</span> Mitigate safety risks via agent psychology</td></tr></tbody></table></table></figure><blockquote><p>üîº This table provides a comprehensive summary of various agent-centric attacks and their corresponding defenses in Large Language Model (LLM) agents. It categorizes attacks by type (Adversarial, Jailbreaking, Backdoor, Model Collaboration) and includes the specific method used for the attack, a description of that attack, and a reference to the source publication. For each attack, the table may also include information about defenses against it.</p><details><summary>read the caption</summary>TABLE III: Summary of agent-centric attacks and defense in LLM agents.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S4.T4.1.1><tbody class=ltx_tbody><tr class=ltx_tr id=S4.T4.1.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=S4.T4.1.1.1.1.1><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.1.1.1.1>Reference</span></th><td class="ltx_td ltx_align_left ltx_border_tt" id=S4.T4.1.1.1.1.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.1.1.2.1>Description</span></td></tr><tr class=ltx_tr id=S4.T4.1.1.2.2><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan=2 id=S4.T4.1.1.2.2.1><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.2.2.1.1>External Data Attacks and Security</span></th></tr><tr class=ltx_tr id=S4.T4.1.1.3.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T4.1.1.3.3.1>Li et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib204 title>204</a>]</cite></th><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T4.1.1.3.3.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.3.3.2.1>Attack:</span> Malicious prefix injection</td></tr><tr class=ltx_tr id=S4.T4.1.1.4.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.4.4.1>Psysafe¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib201 title>201</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.4.4.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.4.4.2.1>Attack:</span> A dark psychological injection benchmark</td></tr><tr class=ltx_tr id=S4.T4.1.1.5.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.5.5.1>Tian et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib210 title>210</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.5.5.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.5.5.2.1>Attack:</span> Guide agents into specific role-playing states</td></tr><tr class=ltx_tr id=S4.T4.1.1.6.6><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.6.6.1>InjectAgent¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib205 title>205</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.6.6.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.6.6.2.1>Attack:</span> A prompting injection benchmark</td></tr><tr class=ltx_tr id=S4.T4.1.1.7.7><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.7.7.1>Agentdojo¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib203 title>203</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.7.7.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.7.7.2.1>Attack:</span> A user injection benchmark</td></tr><tr class=ltx_tr id=S4.T4.1.1.8.8><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.8.8.1>AgentPoison¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib216 title>216</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.8.8.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.8.8.2.1>Attack:</span> Poisoning samples in knowledge databases</td></tr><tr class=ltx_tr id=S4.T4.1.1.9.9><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.9.9.1>Nakash et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib215 title>215</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.9.9.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.9.9.2.1>Attack:</span> Indirect prompt injection through FITD attack</td></tr><tr class=ltx_tr id=S4.T4.1.1.10.10><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.10.10.1>WIPI¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib214 title>214</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.10.10.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.10.10.2.1>Attack:</span> control agents through a public web page</td></tr><tr class=ltx_tr id=S4.T4.1.1.11.11><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.11.11.1>ASB¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib176 title>176</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.11.11.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.11.11.2.1>Attack:</span> A multi-type attack benchmark</td></tr><tr class=ltx_tr id=S4.T4.1.1.12.12><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.12.12.1>AgentHarm¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib223 title>223</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.12.12.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.12.12.2.1>Attack:</span> A multi-type attack benchmark</td></tr><tr class=ltx_tr id=S4.T4.1.1.13.13><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.13.13.1>Mantis¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib206 title>206</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.13.13.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.13.13.2.1>Defense:</span> Hacking back to attackers</td></tr><tr class=ltx_tr id=S4.T4.1.1.14.14><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.14.14.1>Chern et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib183 title>183</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.14.14.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.14.14.2.1>Defense:</span> Employ multi-agent debate to verify external knowledge</td></tr><tr class=ltx_tr id=S4.T4.1.1.15.15><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.15.15.1>RTBAS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib208 title>208</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.15.15.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.15.15.2.1>Defense:</span> Check every step of agent information flow</td></tr><tr class=ltx_tr id=S4.T4.1.1.16.16><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.16.16.1>TaskShield¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib209 title>209</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.16.16.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.16.16.2.1>Defense:</span> Check every step of agent process</td></tr><tr class=ltx_tr id=S4.T4.1.1.17.17><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.17.17.1>Zhang et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib201 title>201</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.17.17.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.17.17.2.1>Defense:</span> Doctor and police agents guard the healthy psychology</td></tr><tr class=ltx_tr id=S4.T4.1.1.18.18><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan=2 id=S4.T4.1.1.18.18.1><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.18.18.1.1>Interaction Attacks and Security</span></th></tr><tr class=ltx_tr id=S4.T4.1.1.19.19><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T4.1.1.19.19.1>Wang et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib217 title>217</a>]</cite></th><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T4.1.1.19.19.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.19.19.2.1>Attack:</span> Private memory extraction attack</td></tr><tr class=ltx_tr id=S4.T4.1.1.20.20><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.20.20.1>CORBA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib196 title>196</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.20.20.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.20.20.2.1>Attack:</span> Disrupt the communications among agents</td></tr><tr class=ltx_tr id=S4.T4.1.1.21.21><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.21.21.1>AgentSmith¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib220 title>220</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.21.21.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.21.21.2.1>Attack:</span> Poison one agent to infectious other agents</td></tr><tr class=ltx_tr id=S4.T4.1.1.22.22><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.22.22.1>Lee et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib221 title>221</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.22.22.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.22.22.2.1>Attack:</span> Conduct injections to self-replicate among agents</td></tr><tr class=ltx_tr id=S4.T4.1.1.23.23><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.23.23.1>He et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib197 title>197</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.23.23.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.23.23.2.1>Attack:</span> Inject semantic disruptions to agent communications</td></tr><tr class=ltx_tr id=S4.T4.1.1.24.24><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T4.1.1.24.24.1>BlockAgents¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib222 title>222</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T4.1.1.24.24.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.24.24.2.1>Defense:</span> Incorporate blockchain and PoT against byzantine attacks</td></tr><tr class=ltx_tr id=S4.T4.1.1.25.25><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S4.T4.1.1.25.25.1>Abdelnabi et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib207 title>207</a>]</cite></th><td class="ltx_td ltx_align_left ltx_border_bb" id=S4.T4.1.1.25.25.2><span class="ltx_text ltx_font_bold" id=S4.T4.1.1.25.25.2.1>Defense:</span> A multi-layer agent firewall</td></tr></tbody></table></table></figure><blockquote><p>üîº This table summarizes various data-centric attacks and defense mechanisms targeting Large Language Model (LLM) agents. Data-centric attacks focus on manipulating the input data provided to the LLM agents to cause undesirable outputs or behaviors, rather than directly targeting the model&rsquo;s internal structure. The table categorizes these attacks based on their approach (external data falsification vs. interaction attacks), and also includes defenses against each type of attack.</p><details><summary>read the caption</summary>TABLE IV: Summary of data-centric attack and defense in LLM agents.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S4.T5.1.1><tbody class=ltx_tbody><tr class=ltx_tr id=S4.T5.1.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=S4.T5.1.1.1.1.1><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.1.1.1.1>Reference</span></th><td class="ltx_td ltx_align_left ltx_border_tt" id=S4.T5.1.1.1.1.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.1.1.2.1>Description</span></td></tr><tr class=ltx_tr id=S4.T5.1.1.2.2><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan=2 id=S4.T5.1.1.2.2.1><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.2.2.1.1>LM Memorization Vulnerabilities</span></th></tr><tr class=ltx_tr id=S4.T5.1.1.3.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T5.1.1.3.3.1>Carlini et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib224 title>224</a>]</cite></th><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T5.1.1.3.3.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.3.3.2.1>Attack:</span> Data Extraction</td></tr><tr class=ltx_tr id=S4.T5.1.1.4.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T5.1.1.4.4.1>Huang et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib226 title>226</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T5.1.1.4.4.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.4.4.2.1>Attack:</span> Data Extraction on Pretrained LLMs</td></tr><tr class=ltx_tr id=S4.T5.1.1.5.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T5.1.1.5.5.1>Mireshghallah et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib227 title>227</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T5.1.1.5.5.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.5.5.2.1>Attack:</span> Membership Inference on Fine-Tuned LLMs</td></tr><tr class=ltx_tr id=S4.T5.1.1.6.6><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T5.1.1.6.6.1>Fu et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib228 title>228</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T5.1.1.6.6.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.6.6.2.1>Attack:</span> Self-Calibrated Membership Inference</td></tr><tr class=ltx_tr id=S4.T5.1.1.7.7><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T5.1.1.7.7.1>Pan et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib231 title>231</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T5.1.1.7.7.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.7.7.2.1>Attack:</span> Attribute Inference in General-Purpose LLMs</td></tr><tr class=ltx_tr id=S4.T5.1.1.8.8><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T5.1.1.8.8.1>Wang et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib232 title>232</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T5.1.1.8.8.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.8.8.2.1>Attack:</span> Property Existence Inference in Generative Models</td></tr><tr class=ltx_tr id=S4.T5.1.1.9.9><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T5.1.1.9.9.1>Kandpal et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib233 title>233</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T5.1.1.9.9.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.9.9.2.1>Defense:</span> Data Sanitization to Mitigate Memorization</td></tr><tr class=ltx_tr id=S4.T5.1.1.10.10><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T5.1.1.10.10.1>Hoory et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib229 title>229</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T5.1.1.10.10.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.10.10.2.1>Defense:</span> Differential Privacy for Pre-Trained LLMs</td></tr><tr class=ltx_tr id=S4.T5.1.1.11.11><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T5.1.1.11.11.1>Kang et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib230 title>230</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T5.1.1.11.11.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.11.11.2.1>Defense:</span> Knowledge Distillation for Privacy Preservation</td></tr><tr class=ltx_tr id=S4.T5.1.1.12.12><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T5.1.1.12.12.1>Kim et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib234 title>234</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T5.1.1.12.12.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.12.12.2.1>Defense:</span> Privacy Leakage Assessment Tool</td></tr><tr class=ltx_tr id=S4.T5.1.1.13.13><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan=2 id=S4.T5.1.1.13.13.1><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.13.13.1.1>LM Intellectual Property Exploitation</span></th></tr><tr class=ltx_tr id=S4.T5.1.1.14.14><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T5.1.1.14.14.1>Krishna et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib235 title>235</a>]</cite></th><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T5.1.1.14.14.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.14.14.2.1>Attack:</span> Model Stealing via Query APIs</td></tr><tr class=ltx_tr id=S4.T5.1.1.15.15><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T5.1.1.15.15.1>Naseh et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib236 title>236</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T5.1.1.15.15.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.15.15.2.1>Attack:</span> Stealing Decoding Algorithms of LLMs</td></tr><tr class=ltx_tr id=S4.T5.1.1.16.16><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T5.1.1.16.16.1>Li et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib237 title>237</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T5.1.1.16.16.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.16.16.2.1>Attack:</span> Extracting Specialized Code Abilities from LLMs</td></tr><tr class=ltx_tr id=S4.T5.1.1.17.17><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T5.1.1.17.17.1>Shen et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib240 title>240</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T5.1.1.17.17.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.17.17.2.1>Attack:</span> Prompt Stealing in Text-to-Image Models</td></tr><tr class=ltx_tr id=S4.T5.1.1.18.18><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T5.1.1.18.18.1>Sha et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib241 title>241</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T5.1.1.18.18.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.18.18.2.1>Attack:</span> Prompt Stealing in LLMs</td></tr><tr class=ltx_tr id=S4.T5.1.1.19.19><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T5.1.1.19.19.1>Hui et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib242 title>242</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T5.1.1.19.19.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.19.19.2.1>Attack:</span> Closed-Box Prompt Extraction</td></tr><tr class=ltx_tr id=S4.T5.1.1.20.20><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T5.1.1.20.20.1>Kirchenbauer et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib238 title>238</a>]</cite></th><td class="ltx_td ltx_align_left" id=S4.T5.1.1.20.20.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.20.20.2.1>Defense:</span> Model Watermarking for IP Protection</td></tr><tr class=ltx_tr id=S4.T5.1.1.21.21><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S4.T5.1.1.21.21.1>Lin et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib239 title>239</a>]</cite></th><td class="ltx_td ltx_align_left ltx_border_bb" id=S4.T5.1.1.21.21.2><span class="ltx_text ltx_font_bold" id=S4.T5.1.1.21.21.2.1>Defense:</span> Blockchain for IP Verification</td></tr></tbody></table></table></figure><blockquote><p>üîº This table summarizes various privacy threats associated with Large Language Model (LLM) agents and the corresponding countermeasures. It categorizes privacy threats into two main areas: LLM Memorization Vulnerabilities (data extraction attacks, membership inference attacks, attribute inference attacks) and LLM Intellectual Property Exploitation (model stealing attacks, prompt stealing attacks). For each type of threat, the table lists specific attack methods and relevant references to research papers, along with countermeasures to mitigate these privacy concerns. The countermeasures include techniques like data sanitization, differential privacy, knowledge distillation, model watermarking, and blockchain-based IP protection.</p><details><summary>read the caption</summary>TABLE V: Summary of privacy threats and countermeasures in LLM agents.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S4.T6.1.1><tbody class=ltx_tbody><tr class=ltx_tr id=S4.T6.1.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=S4.T6.1.1.1.1.1><span class="ltx_text ltx_font_bold" id=S4.T6.1.1.1.1.1.1>Impact</span></th><td class="ltx_td ltx_align_left ltx_border_tt" id=S4.T6.1.1.1.1.2><span class="ltx_text ltx_font_bold" id=S4.T6.1.1.1.1.2.1>Reference</span></td></tr><tr class=ltx_tr id=S4.T6.1.1.2.2><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan=2 id=S4.T6.1.1.2.2.1><span class="ltx_text ltx_font_bold" id=S4.T6.1.1.2.2.1.1>Benefits to Society</span></th></tr><tr class=ltx_tr id=S4.T6.1.1.3.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T6.1.1.3.3.1>Automation Enhancement</th><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T6.1.1.3.3.2>Foundation Models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib243 title>243</a>]</cite>, GPT-3¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib244 title>244</a>]</cite>, LLaMA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib245 title>245</a>]</cite></td></tr><tr class=ltx_tr id=S4.T6.1.1.4.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T6.1.1.4.4.1>Workforce Transformation</th><td class="ltx_td ltx_align_left" id=S4.T6.1.1.4.4.2>Foundation Models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib243 title>243</a>]</cite>, Redefining Work¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib246 title>246</a>]</cite></td></tr><tr class=ltx_tr id=S4.T6.1.1.5.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T6.1.1.5.5.1>Enhance Information Distribution</th><td class="ltx_td ltx_align_left" id=S4.T6.1.1.5.5.2>GPT-3¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib244 title>244</a>]</cite>, LLaMa¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib245 title>245</a>]</cite>, Empower Online Education¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib247 title>247</a>]</cite></td></tr><tr class=ltx_tr id=S4.T6.1.1.6.6><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan=2 id=S4.T6.1.1.6.6.1><span class="ltx_text ltx_font_bold" id=S4.T6.1.1.6.6.1.1>Ethical Concerns</span></th></tr><tr class=ltx_tr id=S4.T6.1.1.7.7><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T6.1.1.7.7.1>Bias and Discrimination</th><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T6.1.1.7.7.2>Fair Use¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib249 title>249</a>]</cite>, Fair Learning¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib250 title>250</a>]</cite></td></tr><tr class=ltx_tr id=S4.T6.1.1.8.8><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T6.1.1.8.8.1>Accountability</th><td class="ltx_td ltx_align_left" id=S4.T6.1.1.8.8.2>Stochastic Parrots¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib252 title>252</a>]</cite>, Governance¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib253 title>253</a>, <a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib254 title>254</a>]</cite></td></tr><tr class=ltx_tr id=S4.T6.1.1.9.9><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T6.1.1.9.9.1>Copyright</th><td class="ltx_td ltx_align_left" id=S4.T6.1.1.9.9.2>Fair Learning¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib250 title>250</a>]</cite>, Ethics of LLMs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib255 title>255</a>]</cite>, AI collapse¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib256 title>256</a>]</cite></td></tr><tr class=ltx_tr id=S4.T6.1.1.10.10><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T6.1.1.10.10.1>Data Privacy</th><td class="ltx_td ltx_align_left" id=S4.T6.1.1.10.10.2>Foundation Models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib243 title>243</a>]</cite>, Ethical and Social Risks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib257 title>257</a>]</cite></td></tr><tr class=ltx_tr id=S4.T6.1.1.11.11><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T6.1.1.11.11.1>Manipulation & Misinformation</th><td class="ltx_td ltx_align_left" id=S4.T6.1.1.11.11.2>Data-Poisoning Attacks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib259 title>259</a>]</cite></td></tr><tr class=ltx_tr id=S4.T6.1.1.12.12><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S4.T6.1.1.12.12.1>Others</th><td class="ltx_td ltx_align_left ltx_border_bb" id=S4.T6.1.1.12.12.2>Overreliance¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib244 title>244</a>]</cite>, Alignment¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib261 title>261</a>]</cite>, Carbon Footprint¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib262 title>262</a>]</cite>, Expenses¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib263 title>263</a>]</cite></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comprehensive overview of the societal impacts and ethical considerations associated with the use of Large Language Model (LLM) agents. It categorizes the effects into benefits and ethical concerns, providing specific examples and references for each category. The benefits include automation enhancement, workforce transformation, and improved information distribution. The ethical concerns encompass bias and discrimination, accountability issues, copyright implications, data privacy risks, potential for manipulation and misinformation, and other emerging concerns. This detailed breakdown helps to provide a balanced perspective on the significant influence of LLM agents on society.</p><details><summary>read the caption</summary>TABLE VI: Overview of Social Impacts and Ethical Considerations in LLM Agents.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S5.T7.1.1><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T7.1.1.2.1><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=S5.T7.1.1.2.1.1 style=padding-left:1pt;padding-right:1pt>Method</th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id=S5.T7.1.1.2.1.2 style=padding-left:1pt;padding-right:1pt>Domain</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id=S5.T7.1.1.2.1.3 style=padding-left:1pt;padding-right:1pt>Core Idea</td></tr><tr class=ltx_tr id=S5.T7.1.1.3.2><th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan=3 id=S5.T7.1.1.3.2.1 style=padding-left:1pt;padding-right:1pt><span class="ltx_text ltx_font_bold" id=S5.T7.1.1.3.2.1.1>Scientific Discovery</span></th></tr><tr class=ltx_tr id=S5.T7.1.1.4.3><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S5.T7.1.1.4.3.1 style=padding-left:1pt;padding-right:1pt>SciAgents¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib266 title>266</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T7.1.1.4.3.2 style=padding-left:1pt;padding-right:1pt>General Sciences</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T7.1.1.4.3.3 style=padding-left:1pt;padding-right:1pt>Collaborative hypothesis generation</td></tr><tr class=ltx_tr id=S5.T7.1.1.5.4><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.5.4.1 style=padding-left:1pt;padding-right:1pt>Curie¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib267 title>267</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.5.4.2 style=padding-left:1pt;padding-right:1pt>General Sciences</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.5.4.3 style=padding-left:1pt;padding-right:1pt>Automated experimentation</td></tr><tr class=ltx_tr id=S5.T7.1.1.6.5><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.6.5.1 style=padding-left:1pt;padding-right:1pt>ChemCrow¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib269 title>269</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.6.5.2 style=padding-left:1pt;padding-right:1pt>Chemistry</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.6.5.3 style=padding-left:1pt;padding-right:1pt>Tool-augmented synthesis planning</td></tr><tr class=ltx_tr id=S5.T7.1.1.7.6><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.7.6.1 style=padding-left:1pt;padding-right:1pt>AtomAgents¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib270 title>270</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.7.6.2 style=padding-left:1pt;padding-right:1pt>Materials Science</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.7.6.3 style=padding-left:1pt;padding-right:1pt>Physics-aware alloy design</td></tr><tr class=ltx_tr id=S5.T7.1.1.8.7><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.8.7.1 style=padding-left:1pt;padding-right:1pt>D. Kostunin el al¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib271 title>271</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.8.7.2 style=padding-left:1pt;padding-right:1pt>Astronomy</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.8.7.3 style=padding-left:1pt;padding-right:1pt>Telescope configuration management</td></tr><tr class=ltx_tr id=S5.T7.1.1.9.8><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.9.8.1 style=padding-left:1pt;padding-right:1pt>BioDiscoveryAgent¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib273 title>273</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.9.8.2 style=padding-left:1pt;padding-right:1pt>Biology</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.9.8.3 style=padding-left:1pt;padding-right:1pt>Genetic perturbation design</td></tr><tr class=ltx_tr id=S5.T7.1.1.10.9><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.10.9.1 style=padding-left:1pt;padding-right:1pt>GeneAgent¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib274 title>274</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.10.9.2 style=padding-left:1pt;padding-right:1pt>Biology</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.10.9.3 style=padding-left:1pt;padding-right:1pt>Self-verifying gene association discovery</td></tr><tr class=ltx_tr id=S5.T7.1.1.11.10><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.11.10.1 style=padding-left:1pt;padding-right:1pt>RiGPS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib275 title>275</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.11.10.2 style=padding-left:1pt;padding-right:1pt>Biology</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.11.10.3 style=padding-left:1pt;padding-right:1pt>Biomarker identification</td></tr><tr class=ltx_tr id=S5.T7.1.1.12.11><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.12.11.1 style=padding-left:1pt;padding-right:1pt>BioRAG¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib211 title>211</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.12.11.2 style=padding-left:1pt;padding-right:1pt>Biology</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.12.11.3 style=padding-left:1pt;padding-right:1pt>Biology-focused retrieval augmentation</td></tr><tr class=ltx_tr id=S5.T7.1.1.13.12><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.13.12.1 style=padding-left:1pt;padding-right:1pt>PathGen-1.6M¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib276 title>276</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.13.12.2 style=padding-left:1pt;padding-right:1pt>Medical Dataset</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.13.12.3 style=padding-left:1pt;padding-right:1pt>Pathology image dataset generation</td></tr><tr class=ltx_tr id=S5.T7.1.1.14.13><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.14.13.1 style=padding-left:1pt;padding-right:1pt>KALIN¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib277 title>277</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.14.13.2 style=padding-left:1pt;padding-right:1pt>Biology Dataset</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.14.13.3 style=padding-left:1pt;padding-right:1pt>Scientific question corpus generation</td></tr><tr class=ltx_tr id=S5.T7.1.1.15.14><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.15.14.1 style=padding-left:1pt;padding-right:1pt>GeneSUM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib278 title>278</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.15.14.2 style=padding-left:1pt;padding-right:1pt>Biology Dataset</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.15.14.3 style=padding-left:1pt;padding-right:1pt>Gene function knowledge maintenance</td></tr><tr class=ltx_tr id=S5.T7.1.1.16.15><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.16.15.1 style=padding-left:1pt;padding-right:1pt>AgentHospital¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib281 title>281</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.16.15.2 style=padding-left:1pt;padding-right:1pt>Medical</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.16.15.3 style=padding-left:1pt;padding-right:1pt>Virtual hospital simulation</td></tr><tr class=ltx_tr id=S5.T7.1.1.17.16><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.17.16.1 style=padding-left:1pt;padding-right:1pt>ClinicalLab¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib282 title>282</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.17.16.2 style=padding-left:1pt;padding-right:1pt>Medical</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.17.16.3 style=padding-left:1pt;padding-right:1pt>Multi-department diagnostics</td></tr><tr class=ltx_tr id=S5.T7.1.1.18.17><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.18.17.1 style=padding-left:1pt;padding-right:1pt>AIPatient¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib283 title>283</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.18.17.2 style=padding-left:1pt;padding-right:1pt>Medical</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.18.17.3 style=padding-left:1pt;padding-right:1pt>Patient simulation</td></tr><tr class=ltx_tr id=S5.T7.1.1.19.18><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.19.18.1 style=padding-left:1pt;padding-right:1pt>CXR-Agent¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib284 title>284</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.19.18.2 style=padding-left:1pt;padding-right:1pt>Medical</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.19.18.3 style=padding-left:1pt;padding-right:1pt>Chest X-ray interpretation</td></tr><tr class=ltx_tr id=S5.T7.1.1.20.19><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.20.19.1 style=padding-left:1pt;padding-right:1pt>MedRAX¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib285 title>285</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.20.19.2 style=padding-left:1pt;padding-right:1pt>Medical</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.20.19.3 style=padding-left:1pt;padding-right:1pt>Multimodal medical reasoning</td></tr><tr class=ltx_tr id=S5.T7.1.1.21.20><th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan=3 id=S5.T7.1.1.21.20.1 style=padding-left:1pt;padding-right:1pt><span class="ltx_text ltx_font_bold" id=S5.T7.1.1.21.20.1.1>Gaming</span></th></tr><tr class=ltx_tr id=S5.T7.1.1.22.21><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S5.T7.1.1.22.21.1 style=padding-left:1pt;padding-right:1pt>ReAct¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib33 title>33</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T7.1.1.22.21.2 style=padding-left:1pt;padding-right:1pt>Game Playing</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T7.1.1.22.21.3 style=padding-left:1pt;padding-right:1pt>Reasoning and acting in text environments</td></tr><tr class=ltx_tr id=S5.T7.1.1.23.22><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.23.22.1 style=padding-left:1pt;padding-right:1pt>Voyager¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib35 title>35</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.23.22.2 style=padding-left:1pt;padding-right:1pt>Game Playing</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.23.22.3 style=padding-left:1pt;padding-right:1pt>Lifelong learning in Minecraft</td></tr><tr class=ltx_tr id=S5.T7.1.1.24.23><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.24.23.1 style=padding-left:1pt;padding-right:1pt>ChessGPT¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib287 title>287</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.24.23.2 style=padding-left:1pt;padding-right:1pt>Game Playing</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.24.23.3 style=padding-left:1pt;padding-right:1pt>Chess gameplay evaluation</td></tr><tr class=ltx_tr id=S5.T7.1.1.25.24><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.25.24.1 style=padding-left:1pt;padding-right:1pt>GLAM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib288 title>288</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.25.24.2 style=padding-left:1pt;padding-right:1pt>Game Playing</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.25.24.3 style=padding-left:1pt;padding-right:1pt>Reinforcement learning in text environments</td></tr><tr class=ltx_tr id=S5.T7.1.1.26.25><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.26.25.1 style=padding-left:1pt;padding-right:1pt>CALYPSO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib289 title>289</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.26.25.2 style=padding-left:1pt;padding-right:1pt>Game Generation</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.26.25.3 style=padding-left:1pt;padding-right:1pt>Narrative generation for D&amp;D</td></tr><tr class=ltx_tr id=S5.T7.1.1.27.26><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.27.26.1 style=padding-left:1pt;padding-right:1pt>GameGPT¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib290 title>290</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.27.26.2 style=padding-left:1pt;padding-right:1pt>Game Generation</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.27.26.3 style=padding-left:1pt;padding-right:1pt>Automated game development</td></tr><tr class=ltx_tr id=S5.T7.1.1.28.27><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.28.27.1 style=padding-left:1pt;padding-right:1pt>Sun et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib291 title>291</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.28.27.2 style=padding-left:1pt;padding-right:1pt>Game Generation</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.28.27.3 style=padding-left:1pt;padding-right:1pt>Interactive storytelling experience</td></tr><tr class=ltx_tr id=S5.T7.1.1.29.28><th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan=3 id=S5.T7.1.1.29.28.1 style=padding-left:1pt;padding-right:1pt><span class="ltx_text ltx_font_bold" id=S5.T7.1.1.29.28.1.1>Social Science</span></th></tr><tr class=ltx_tr id=S5.T7.1.1.30.29><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S5.T7.1.1.30.29.1 style=padding-left:1pt;padding-right:1pt>Econagent¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib292 title>292</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T7.1.1.30.29.2 style=padding-left:1pt;padding-right:1pt>Economy</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T7.1.1.30.29.3 style=padding-left:1pt;padding-right:1pt>Economic decision simulation</td></tr><tr class=ltx_tr id=S5.T7.1.1.31.30><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.31.30.1 style=padding-left:1pt;padding-right:1pt>TradingGPT¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib293 title>293</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.31.30.2 style=padding-left:1pt;padding-right:1pt>Economy</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.31.30.3 style=padding-left:1pt;padding-right:1pt>Financial trading simulation</td></tr><tr class=ltx_tr id=S5.T7.1.1.32.31><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.32.31.1 style=padding-left:1pt;padding-right:1pt>CompeteAI¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib294 title>294</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.32.31.2 style=padding-left:1pt;padding-right:1pt>Economy</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.32.31.3 style=padding-left:1pt;padding-right:1pt>Market competition modeling</td></tr><tr class=ltx_tr id=S5.T7.1.1.33.32><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.33.32.1 style=padding-left:1pt;padding-right:1pt>Ma et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib295 title>295</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.33.32.2 style=padding-left:1pt;padding-right:1pt>Psychology</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.33.32.3 style=padding-left:1pt;padding-right:1pt>Mental health support analysis</td></tr><tr class=ltx_tr id=S5.T7.1.1.34.33><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.34.33.1 style=padding-left:1pt;padding-right:1pt>Zhang et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib296 title>296</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.34.33.2 style=padding-left:1pt;padding-right:1pt>Psychology</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.34.33.3 style=padding-left:1pt;padding-right:1pt>Social behavior simulation</td></tr><tr class=ltx_tr id=S5.T7.1.1.35.34><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.35.34.1 style=padding-left:1pt;padding-right:1pt>TE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib297 title>297</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.35.34.2 style=padding-left:1pt;padding-right:1pt>Psychology</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.35.34.3 style=padding-left:1pt;padding-right:1pt>Psychological experiment simulation</td></tr><tr class=ltx_tr id=S5.T7.1.1.36.35><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.36.35.1 style=padding-left:1pt;padding-right:1pt>Generative agents¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib30 title>30</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.36.35.2 style=padding-left:1pt;padding-right:1pt>Social Simulation</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.36.35.3 style=padding-left:1pt;padding-right:1pt>Human behavior emulation</td></tr><tr class=ltx_tr id=S5.T7.1.1.37.36><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.37.36.1 style=padding-left:1pt;padding-right:1pt>Liu et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib298 title>298</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.37.36.2 style=padding-left:1pt;padding-right:1pt>Social Simulation</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.37.36.3 style=padding-left:1pt;padding-right:1pt>Learning from social interactions</td></tr><tr class=ltx_tr id=S5.T7.1.1.1><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.1.1 style=padding-left:1pt;padding-right:1pt>S<sup class=ltx_sup id=S5.T7.1.1.1.1.1>3</sup>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib299 title>299</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.1.2 style=padding-left:1pt;padding-right:1pt>Social Simulation</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.1.3 style=padding-left:1pt;padding-right:1pt>Social network behavior modeling</td></tr><tr class=ltx_tr id=S5.T7.1.1.38.37><th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan=3 id=S5.T7.1.1.38.37.1 style=padding-left:1pt;padding-right:1pt><span class="ltx_text ltx_font_bold" id=S5.T7.1.1.38.37.1.1>Productivity Tools</span></th></tr><tr class=ltx_tr id=S5.T7.1.1.39.38><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S5.T7.1.1.39.38.1 style=padding-left:1pt;padding-right:1pt>SDM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib300 title>300</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T7.1.1.39.38.2 style=padding-left:1pt;padding-right:1pt>Software Development</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T7.1.1.39.38.3 style=padding-left:1pt;padding-right:1pt>Self-collaboration for code generation</td></tr><tr class=ltx_tr id=S5.T7.1.1.40.39><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.40.39.1 style=padding-left:1pt;padding-right:1pt>ChatDev¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib301 title>301</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.40.39.2 style=padding-left:1pt;padding-right:1pt>Software Development</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.40.39.3 style=padding-left:1pt;padding-right:1pt>Chat-powered development framework</td></tr><tr class=ltx_tr id=S5.T7.1.1.41.40><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.41.40.1 style=padding-left:1pt;padding-right:1pt>MetaGPT¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib27 title>27</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.41.40.2 style=padding-left:1pt;padding-right:1pt>Software Development</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.41.40.3 style=padding-left:1pt;padding-right:1pt>Meta-programming for collaboration</td></tr><tr class=ltx_tr id=S5.T7.1.1.42.41><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.42.41.1 style=padding-left:1pt;padding-right:1pt>Agent4Rec¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib302 title>302</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.42.41.2 style=padding-left:1pt;padding-right:1pt>Recommender Systems</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.42.41.3 style=padding-left:1pt;padding-right:1pt>User behavior modeling</td></tr><tr class=ltx_tr id=S5.T7.1.1.43.42><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.43.42.1 style=padding-left:1pt;padding-right:1pt>AgentCF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib303 title>303</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.43.42.2 style=padding-left:1pt;padding-right:1pt>Recommender Systems</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.43.42.3 style=padding-left:1pt;padding-right:1pt>User-item interaction modeling</td></tr><tr class=ltx_tr id=S5.T7.1.1.44.43><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id=S5.T7.1.1.44.43.1 style=padding-left:1pt;padding-right:1pt>MACRec¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib304 title>304</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.44.43.2 style=padding-left:1pt;padding-right:1pt>Recommender Systems</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T7.1.1.44.43.3 style=padding-left:1pt;padding-right:1pt>Multi-agent recommendation</td></tr><tr class=ltx_tr id=S5.T7.1.1.45.44><th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S5.T7.1.1.45.44.1 style=padding-left:1pt;padding-right:1pt>RecMind¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.21460v1#bib.bib305 title>305</a>]</cite></th><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id=S5.T7.1.1.45.44.2 style=padding-left:1pt;padding-right:1pt>Recommender Systems</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id=S5.T7.1.1.45.44.3 style=padding-left:1pt;padding-right:1pt>Knowledge-enhanced recommendation</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comprehensive overview of various real-world applications of Large Language Model (LLM) agents across diverse domains. It categorizes applications by field (e.g., scientific discovery, gaming, social sciences, productivity tools) and details the core ideas and methodologies behind each example. This offers a broad perspective on the versatility and potential impact of LLM agents in various sectors.</p><details><summary>read the caption</summary>TABLE VII: Overview of Applications in LLM Agents.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-0aee9cf921a3dbfe0dc9b7ba6d6dcb87 class=gallery><img src=https://ai-paper-reviewer.com/2503.21460/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.21460/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/2025-03-28/2503.21460/&amp;title=Large%20Language%20Model%20Agent:%20A%20Survey%20on%20Methodology,%20Applications%20and%20Challenges" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/2025-03-28/2503.21460/&amp;text=Large%20Language%20Model%20Agent:%20A%20Survey%20on%20Methodology,%20Applications%20and%20Challenges" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/2025-03-28/2503.21460/&amp;subject=Large%20Language%20Model%20Agent:%20A%20Survey%20on%20Methodology,%20Applications%20and%20Challenges" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_2025-03-28/2503.21460/index.md",oid_likes="likes_2025-03-28/2503.21460/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/2025-03-28/2503.21749/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">LeX-Art: Rethinking Text Generation via Scalable High-Quality Data Synthesis</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-03-27T00:00:00+00:00>27 March 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/2025-03-28/2503.21765/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Exploring the Evolution of Physics Cognition in Video Generation: A Survey</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-03-27T00:00:00+00:00>27 March 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>