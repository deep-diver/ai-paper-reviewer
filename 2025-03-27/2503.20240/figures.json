[{"figure_path": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/DiT/DiT_qualitatives.jpg", "caption": "Figure 1: Unconditional Priors Matter in CFG-Based Conditional Generation. Fine-tuned conditional diffusion models often show drastic degradation in their unconditional priors, adversely affecting conditional generation when using techniques such as CFG\u00a0[28]. We demonstrate that leveraging a diffusion model with a richer unconditional prior and combining its unconditional noise prediction with the conditional noise prediction from the fine-tuned model can lead to substantial improvements in conditional generation quality. This is demonstrated across diverse conditional diffusion models including Zero-1-to-3\u00a0[46], Versatile Diffusion\u00a0[64], InstructPix2Pix\u00a0[7], and DynamiCrafter\u00a0[62].", "description": "Figure 1 showcases how leveraging a strong unconditional prior improves the conditional generation capabilities of fine-tuned diffusion models.  Fine-tuning often diminishes the quality of a model's unconditional generation, which negatively impacts performance when using techniques like Classifier-Free Guidance (CFG). This figure demonstrates that incorporating a richer unconditional prior from a separate, well-trained diffusion model significantly enhances the results of conditional generation tasks.  Examples across various models (Zero-1-to-3, Versatile Diffusion, InstructPix2Pix, and DynamiCrafter) illustrate this improvement.", "section": "Introduction"}]