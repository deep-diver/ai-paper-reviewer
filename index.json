[{"content":"","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/2024-10-24/","section":"Tags","summary":"","title":"2024-10-24","type":"tags"},{"content":"","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/ai-generated/","section":"Categories","summary":"","title":"AI Generated","type":"categories"},{"content":"","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/","section":"AI Paper Reviews by AI","summary":"","title":"AI Paper Reviews by AI","type":"page"},{"content":" TL;DR # CAMEL-Bench is a new open-source benchmark for evaluating large multimodal models in Arabic. It addresses the lack of Arabic-centric LMM benchmarks by offering a diverse set of tasks across eight domains and 38 sub-domains, with over 29,000 high-quality questions. Evaluation results highlight the need for substantial improvement in Arabic LMMs, especially among open-source models. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # CAMEL-Bench is a new, comprehensive benchmark for evaluating large multimodal models (LMMs) in Arabic. Existing benchmarks are predominantly English-centric, limiting their applicability to other languages. CAMEL-Bench addresses this gap by providing a diverse set of tasks across eight domains and 38 sub-domains, with over 29,000 questions carefully curated by native Arabic speakers. The benchmark\u0026rsquo;s open-source nature facilitates further research and development in Arabic LMMs.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1. The proposed CAMEL-Bench covers eight diverse and challenging domains: multimodal understanding and reasoning, OCR and documents, charts and diagrams, videos, cultural-specific content, medical images, agricultural images, and remote sensing understanding in Arabic. CAMEL-Bench covers 38 sub-domains with over 29K questions carefully curated by native Arabic speakers to rigorously evaluate essential skills desired in Arabic LMMs. 🔽 Comparison of our CAMEL-Bench with existing Arabic LMM benchmarks: Exams-V [13], CVQA [46], Henna[4], and KHATT [34]. Here * denotes that only Arabic part of benchmark is counted. Domain/Characteristics Exams-V* CVQA* Henna KHATT CAMEL-Bench (ours) Multimodal Und. \u0026amp; Reasoning V X X OCR \u0026amp; Docs Und. X X X V Charts \u0026amp; Diagrams Und. V X X X Video Und. X X X X Medical Image Und. X X X X Agricultural Image Und. X X X X Remote-Sensing Und. X X X X Cultural-Specific Und. X V X Open Source Question Numbers 823 V 200 X 1.1K V 5K 29K More on figures 🔼 Figure 1. The proposed CAMEL-Bench covers eight diverse and challenging domains: multimodal understanding and reasoning, OCR and documents, charts and diagrams, videos, cultural-specific content, medical images, agricultural images, and remote sensing understanding in Arabic. CAMEL-Bench covers 38 sub-domains with over 29K questions carefully curated by native Arabic speakers to rigorously evaluate essential skills desired in Arabic LMMs. 🔼 Figure 3. The CAMEL-Bench Filtering and Verification Pipeline consists of two paths: Original Arabic and translated Arabic. For original Arabic (top row), a 20% random sample undergoes manual verification; if errors are below 40%, the data passes; otherwise, the entire sub-category is reviewed. For Translated Arabic (bottom row), We employ Qwen7B model [8] to assess semantic similarity between the original and translated question-answer pairs on fuzzy-basis evaluation. Pairs passing the evaluation proceed, while those that fail undergo manual review. Based on this, data may require Manual Handling for manual re-translation, Refine \u0026amp; Verify for refinement through the model, or Non-Translated Review where the data is re-sent for translation due to the absence of an Arabic version. 🔼 Figure 2. CAMEL-Bench examples spanning eight diverse domains, encompassing a wide range of visual data types and tasks. 🔼 Figure 2. CAMEL-Bench examples spanning eight diverse domains, encompassing a wide range of visual data types and tasks. More on tables 🔽 Table 2. Different data sources used for 38 sub-domains corresponding to eight domains, with around 29k questions in total. The different data sources include: MME [15], MMBench [30], MMT-Bench-MI [56], SEED [23], MMMU [58], MMMU-Pro [60], CountBench [39], POPE [26], MathVista [33], Exams-V (Arabic portion) [13], ScienceQA-IMG [32], GQA [20], VizWiz [10], VQAv2 [17], BLINK [16], MuirBench [50], COCO [27], Imagenet [14], Mocheg [55], Snli-Ve [54], Pinterest [42], RealWorldQA [53], PATS-01 [3], KHATT [34], PATD [40], Historical Arabic Handwritten Text Recognition Dataset [37], ISI-PPT-Dataset [52], EvArEST [18], MTVQA [49], ChartQA [35], IconQA [31], BEC-Arabic [47], Claude-3.5 [5], arab-celeb-dataset [36], arabic-food-101 [6], Countries and landmarks [41, 51, 57], Pexel [41], AgroGPT [7], GeoChat [22]. These data sources are carefully translated and verified to ensure quality and relevance. 🔽 Table 3. Performance comparison of different closed-and open-source LMMs on CAMEL-Bench. We present per-domain results of seven LMMs: GPT-40 [38], GPT-40-mini [38], Gemini-1.5-Pro [2], Gemini-1.5-Flash [2], Pangea-7B [59], Qwen2-VL [9], InternVL2-8B [11], and LLaVaNeXt-7B [29]. GPT-40 excels in most domains, while GPT-40-mini offers an impressive balance of performance and model size. All models struggle with remote sensing, medical imaging, OCR \u0026amp; document understanding, and general multimodal understanding and reasoning domains. Open-source models like InternVL2-8B and LLaVaNeXt-7B show a decline in performance across domains, with their best results in video understanding. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18976/","section":"Posts","summary":"CAMEL-Bench is a new open-source benchmark for evaluating large multimodal models in Arabic.  It addresses the lack of Arabic-centric LMM benchmarks by offering a diverse set of tasks across eight dom\u0026hellip;..","title":"CAMEL-Bench: A Comprehensive Arabic LMM Benchmark","type":"posts"},{"content":"","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":" TL;DR # The paper introduces CCI3.0-HQ, a large-scale, high-quality Chinese dataset for pre-training LLMs. Using a novel two-stage filtering pipeline, CCI3.0-HQ significantly outperforms existing Chinese datasets in benchmarks. The paper also open-sources a new quality classifier and details the methods used to create the dataset. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper introduces CCI3.0-HQ, a high-quality 500GB Chinese dataset for pre-training large language models (LLMs). It significantly improves data quality using a two-stage hybrid filtering pipeline, outperforming existing Chinese datasets in benchmarks. The paper also open-sources a quality classifier and details the methodology.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: Dataset Curation Pipeline 🔽 Table 1: Pre-training Model Configuration Parameters Parameter Value attention_dropout 0.0 bos_token_id 151849 eos_token_id 151850 hidden_act silu hidden_size 896 intermediate_size 2432 max_position_embeddings 4096 num_attention_heads 14 num_hidden_layers 24 num_key_value_heads 2 pad_token_id 151643 rms_norm_eps 1e-06 rope_theta 10000 tie_ word_embeddings True torch_dtype bfloat16 vocab_size 151851 More on tables 🔽 Table 2: Comparison of Dataset Impacts on Model Performance in Mixed and Chinese Dataset Experiments 🔽 Table 3: Comparison of Two Quality Annotation Methods 🔽 Table 4: Evaluation of Different Quality Classifiers ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18505/","section":"Posts","summary":"The paper introduces CCI3.0-HQ, a large-scale, high-quality Chinese dataset for pre-training LLMs.  Using a novel two-stage filtering pipeline, CCI3.0-HQ significantly outperforms existing Chinese dat\u0026hellip;..","title":"CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models","type":"posts"},{"content":" TL;DR # This paper explores data scaling laws in imitation learning for robotic manipulation. It finds that diverse data from many environments and object types is key to good generalization, following approximate power laws. Surprisingly, a small, efficiently collected dataset can yield highly generalizable robot policies, offering significant implications for future research and development. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper investigates data scaling laws for robotic manipulation using imitation learning. It reveals power-law relationships between generalization performance and the number of training environments and objects, emphasizing diversity over sheer quantity of demonstrations. An efficient data collection strategy is proposed and validated, suggesting that high generalization can be achieved with surprisingly little data.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: Illustrations of all tasks. We derive the data scaling laws through extensive experiments on Pour Water and Mouse Arrangement, and further validate these findings on additional tasks, including Fold Towels and Unplug Charger. 🔽 Table 1: Success rate across all tasks. We report the average success rate and standard deviation across 8 unseen environments. The performance in each environment is detailed in Table 12. Pour Water Mouse Arrangement Fold Towels Unplug Charger Score 0.922 士 0.075 0.933 士 0.088 0.95 士 0.062 0.887 士 0.14 Success Rate 85.0 士 19.4% 92.5 士 9.7% 87.5 士 17.1% 90.0 士 14.1% More on figures 🔼 Figure 12: Testing environments. These 8 environments are not included in the training data and are used across all tasks. 🔼 Figure 8: Training environments for Pour Water. We sample 12 environments from our collected training data. See Appendix D.1 for task details. 🔼 Figure 8: Training environments for Pour Water. We sample 12 environments from our collected training data. See Appendix D.1 for task details. 🔼 Figure 12: Testing environments. These 8 environments are not included in the training data and are used across all tasks. 🔼 Figure 8: Training environments for Pour Water. We sample 12 environments from our collected training data. See Appendix D.1 for task details. 🔼 Objects for Pour Water. All of our experiments include a total of 64 training bottles and mugs, as well as 16 unseen testing bottles and mugs. 🔼 Objects for Mouse Arrangement. All of our experiments include a total of 64 training mice and mouse pads, as well as 16 unseen testing mice and mouse pads. 🔼 Objects for Fold Towels. All of our experiments include a total of 32 training towels, as well as 16 unseen testing towels. 🔼 Objects for Unplug Charger. All of our experiments include a total of 32 training chargers and power strips, as well as 16 unseen testing chargers and power strips. 🔼 Figure 18: UMI hand-held grippers. We do not install side mirrors on the grippers. 🔼 Figure 19: Deployment hardware setup. More on tables 🔽 Model related experiments on Pour Water. The entries marked in gray are the same, which specify the default settings: the visual encoder is a fully fine-tuned ViT-L/14 model pretrained with DINOv2, while the action diffusion model employs a base-size 1D CNN U-Net. 🔽 Model related experiments on Pour Water. The entries marked in gray are the same, which specify the default settings: the visual encoder is a fully fine-tuned ViT-L/14 model pre-trained with DINOv2, while the action diffusion model employs a base-size 1D CNN U-Net. 🔽 Table 3: A default set of hyper-parameters. 🔽 Table 4: Object generalization on Pour Water. Normalizing these scores by dividing them by 9 yields the results shown in Fig. 2. 🔽 Environment generalization on Pour Water. Normalizing these scores by dividing them by 9 yields the results shown in Fig. 3. 🔽 Generlization across environments and objects on Pour Water. Normalizing these scores by dividing them by 9 yields the results shown in Fig. 4. 🔽 Number of demonstrations on Pour Water. Normalizing these scores by dividing them by 9 yields the results shown in Fig. 7. 🔽 Object generalization on Mouse Arrangement. Normalizing these scores by dividing them by 6 yields the results shown in Fig. 2. 🔽 Environment generalization on Mouse Arrangement. Normalizing these scores by dividing them by 6 yields the results shown in Fig. 3. 🔽 Generlization across environments and objects. Each curve corresponds to a different fraction of demonstrations used, with normalized scores shown as a function of the number of training environment-object pairs. 🔽 Number of demonstrations on Mouse Arrangement. Normalizing these scores by dividing them by 6 yields the results shown in Fig. 7. 🔽 Success rate across all tasks. For each task, we report the success rate in each evaluation environment. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18647/","section":"Posts","summary":"This paper explores data scaling laws in imitation learning for robotic manipulation.  It finds that diverse data from many environments and object types is key to good generalization, following appro\u0026hellip;..","title":"Data Scaling Laws in Imitation Learning for Robotic Manipulation","type":"posts"},{"content":" TL;DR # To mitigate Large Language Model (LLM) hallucinations, DeCoRe contrasts outputs from a base LLM and one with masked retrieval heads (identified as crucial for factual recall), dynamically adjusting contrast based on conditional entropy. This training-free method substantially improves performance on tasks demanding high contextual faithfulness, such as summarization and open-book QA. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper introduces DeCoRe, a decoding strategy that mitigates hallucinations in LLMs by contrasting the outputs of a base LLM and a masked LLM (with retrieval heads masked). DeCoRe dynamically adjusts the contrast based on the conditional entropy of the base LLM\u0026rsquo;s next-token distribution, thus enhancing contextual faithfulness and factual consistency. Experiments across summarization, instruction following, and open-book question answering tasks show that DeCoRe significantly improves performance.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Overview of the DeCoRe workflow. Given the same input, the base LLM (LLMbase) and the variant with masked retrieval heads (LLMmasked) predict the next token. An uncertainty estimation is applied to the base model\u0026rsquo;s output using conditional entropy: higher conditional entropy increases the contrastive factor (a), penalising predictions that align with the LLMmasked. The final prediction is selected based on weighted contrastive decoding of the outputs from both models, leading to a more grounded response. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. Model XSum XSum XSum MemoTrap MemoTrap IFEval IFEval NQ-Open NQ-Swap Model ROUGE-L ↑ BERTScore-F1 ↑ factKB ↑ Macro Acc ↑ Micro Acc ↑ Prompt Acc ↑ Instruct Acc ↑ EM ↑ EM ↑ Llama3-8b-Instruct 19.90 67.23 47.61 65.86 64.40 70.24 78.30 69.68 60.62 + ITI (Li et al., 2024b) 13.25 59.96 34.35 62.65 58.96 52.31 63.19 56.16 51.08 + CAD (Shi et al., 2024) 18.82 67.20 67.16 - - - - 69.83 74.21 + DoLA (low) (Chuang et al., 2023) 19.82 67.19 47.21 65.27 63.69 69.69 78.18 69.68 60.77 + DoLA (high) (Chuang et al., 2023) 19.92 67.34 48.49 64.85 63.17 70.24 78.66 69.49 60.98 + AD (Chen et al., 2024) 19.79 67.31 48.49 65.38 64.28 67.65 76.26 68.93 60.51 + DeCoRestatic 19.87 67.83 64.07 69.53 69.20 69.13 78.06 70.62 64.43 + DeCoReentropy 19.45 67.69 66.10 74.14 74.87 68.39 76.38 70.66 66.08 Llama3-70b-Instruct 22.41 69.77 61.32 68.47 66.52 77.45 84.41 71.07 76.11 + ITI (Li et al., 2024b) 21.64 69.46 61.33 71.24 68.73 76.71 83.69 71.90 74.76 + CD (Li et al., 2023) 22.71 69.99 54.73 69.27 67.55 71.72 79.74 65.80 68.37 + CAD (Shi et al., 2024) 21.45 69.28 65.61 - - - - 71.83 84.70 + DoLA (low) (Chuang et al., 2023) 22.46 69.80 61.11 67.99 65.93 77.08 84.29 71.07 75.98 + DoLA (high) (Chuang et al., 2023) 22.43 69.93 59.99 67.92 65.81 78.00 84.65 70.40 75.26 + AD (Chen et al., 2024) 22.49 69.91 60.57 67.51 66.44 76.89 84.41 71.15 74.02 + DeCoRestatic 21.94 69.35 64.88 71.96 71.41 78.56 84.89 72.51 79.06 + DeCoReentropy 21.93 69.40 65.49 74.07 73.65 78.56 84.89 72.66 79.79 + DeCoReentropy-lite 22.28 69.34 59.57 72.11 70.58 61.37 71.46 71.26 75.90 More on figures 🔼 Correlation between the number of masked retrieval heads and performance of Llama3-8B-Instruct with DeCoReentropy on each task. The correlations are quantified by the Pearson Correlation Coefficient r for each plot. Detailed results are listed in Table 14 and Table 16. 🔼 Correlation between the number of masked retrieval heads and performance of Llama3-8B-Instruct with DeCoReentropy on each task. The correlations are quantified by the Pearson Correlation Coefficient r for each plot. Detailed results are listed in Table 14 and Table 16. 🔼 Correlation between the number of masked retrieval heads and performance of Llama3-8B-Instruct with DeCoReentropy on each task. The correlations are quantified by the Pearson Correlation Coefficient r for each plot. Detailed results are listed in Table 14 and Table 16. 🔼 Overview of the DeCoRe workflow. Given the same input, the base LLM (LLMbase) and the variant with masked retrieval heads (LLMmasked) predict the next token. An uncertainty estimation is applied to the base model\u0026rsquo;s output using conditional entropy: higher conditional entropy increases the contrastive factor (a), penalising predictions that align with the LLMmasked. The final prediction is selected based on weighted contrastive decoding of the outputs from both models, leading to a more grounded response. 🔼 Correlation between the number of masked retrieval heads and performance of Llama3-8B-Instruct with DeCoReentropy on each task. The correlations are quantified by the Pearson Correlation Coefficient r for each plot. Detailed results are listed in Table 14 and Table 16. 🔼 Correlation between the number of masked random heads and performance of Llama3-8B-Instruct with DeCoReentropy on each task. The correlations are quantified by the Pearson Correlation Coefficient r for each plot. Detailed results are listed in Table 14 and Table 16. 🔼 Overview of the DeCoRe workflow. Given the same input, the base LLM (LLMbase) and the variant with masked retrieval heads (LLMmasked) predict the next token. An uncertainty estimation is applied to the base model\u0026rsquo;s output using conditional entropy: higher conditional entropy increases the contrastive factor (a), penalising predictions that align with the LLMmasked. The final prediction is selected based on weighted contrastive decoding of the outputs from both models, leading to a more grounded response. More on tables 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 7: Performance comparison of Llama3-8B-Instruct with different number of masked retrieval heads on factuality evaluation tasks. 🔽 Table 2: Performance of different models and decoding methods on factuality evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 9: Performance comparison of Llama3-8B-Instruct with different number of masked retrieval heads on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings. 🔽 Table 10: Performance comparison of Llama3-8B-Instruct with different numbers of masked random heads on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 2: Performance of different models and decoding methods on factuality evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 19: Performance comparison across different numbers of masked random heads on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 23: Performance of Llama3-8b-Instruct with DeCoRestatic on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Performance of Llama3-8b-Instruct with DeCoRestatic on factuality evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. 🔽 Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18860/","section":"Posts","summary":"To mitigate Large Language Model (LLM) hallucinations, DeCoRe contrasts outputs from a base LLM and one with masked retrieval heads (identified as crucial for factual recall), dynamically adjusting co\u0026hellip;..","title":"DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations","type":"posts"},{"content":" TL;DR # Researchers created a new method called Code-as-Intermediary Translation (CIT) to improve multimodal large language models (MLLMs) understanding of charts. CIT uses code to translate visual charts into text, allowing LLMs to better reason and answer questions about charts. They created a new dataset, REACHQA, using this method, which significantly improved MLLM performance on various benchmarks. This approach is efficient and scalable, solving the challenge of creating high-quality data for visual reasoning. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper introduces Code-as-Intermediary Translation (CIT), a novel data synthesis method for enhancing visual reasoning in multimodal large language models (MLLMs). CIT uses code as an intermediary to translate visual chart representations into text, enabling LLMs to understand cross-modal information and generate high-quality chart-related Q\u0026amp;A pairs. The resulting dataset, REACHQA, significantly improves MLLM performance on various benchmarks, showcasing the effectiveness of CIT for cost-efficient data creation and improved multimodal reasoning.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: Error distribution of incorrect answers by MiniCPM-V2.5-Llama3 (Yao et al., 2024) on ChartQA test set (Masry et al., 2022), as judged by GPT-40. We present an example chart from ChartQA along with two error cases: one for recognition and one for reasoning. The \u0026lsquo;Other Errors\u0026rsquo; include question misunderstood errors, knowledge and hallucination errors, or refusal to answer. 🔽 Comparison of existing chart-related datasets across three properties. Only the chart question-answering (CQA) task is considered, despite some datasets having multiple tasks. Abbreviations: Vis.=visual, Comp.=complexity, Temp.=template, Refer.=Reference, Reas.=reasoning, Rat. rationale, Annot.=annotation and Scal.=scalable. Cells marked with \u0026#39;X\u0026#39; indicate mixed attributes (e.g., partially template-based; scalable Q\u0026amp;A but non-scalable chart data.). “*” indicates that while the chart-plotting codes are public, the Q\u0026amp;A generation still relies on data tables. Input Acc. Reas. Comp. Vis. Refer. Cost ($) Table 2.72 2.51 1.19 0.047 Code 2.60 2.56 2.15 0.092 Chart 1.91 1.53 2.36 0.107 More on figures 🔼 Overview of the Code-as-Intermediary Translation (CIT) method for synthesizing multimodal instruction data. The process begins with 33 seed codes and generates plot codes across various chart types, topics, and complexity levels through the Self-Instruct and Evol-Instruct stages. The chart set and instruction set are constructed bi-directionally, and the final filtered data yields REACHQA, a dataset for distilling visual chart reasoning abilities from LLMs to MLLMs. 🔼 Figure 5: An example of attention visualization from the ChartQA dataset. The top row shows the results from the vanilla LLaVA-Next-Llama3-8B model, while the bottom row displays the results from our fine-tuned model. For each output, we present the attention distribution (highlighted zones) at three key steps, calculated by averaging the attention values of all tokens in each step. 🔼 Figure 6: Visualizations of different chart-related training datasets. As shown, REACHQA and ChartGemma exhibit higher chart richness compared to several other datasets. But the charts in ChartGemma require manual collection from multiple sources (Masry et al., 2024b). 🔼 Figure 6: Visualizations of different chart-related training datasets. As shown, REACHQA and ChartGemma exhibit higher chart richness compared to several other datasets. But the charts in ChartGemma require manual collection from multiple sources (Masry et al., 2024b). 🔼 Figure 6: Visualizations of different chart-related training datasets. As shown, REACHQA and ChartGemma exhibit higher chart richness compared to several other datasets. But the charts in ChartGemma require manual collection from multiple sources (Masry et al., 2024b). 🔼 Figure 6: Visualizations of different chart-related training datasets. As shown, REACHQA and ChartGemma exhibit higher chart richness compared to several other datasets. But the charts in ChartGemma require manual collection from multiple sources (Masry et al., 2024b). More on tables 🔽 Comparison of existing chart-related datasets across three properties. Only the chart question-answering (CQA) task is considered, despite some datasets having multiple tasks. Abbreviations: Vis.=visual, Comp.=complexity, Temp.=template, Refer.=Reference, Reas.=reasoning, Rat. rationale, Annot.=annotation and Scal.=scalable. Cells marked with \u0026#39;X\u0026#39; indicate mixed attributes (e.g., partially template-based; scalable Q\u0026amp;A but non-scalable chart data.). “*” indicates that while the chart-plotting codes are public, the Q\u0026amp;A generation still relies on data tables. 🔽 Comparison of existing chart-related datasets across three properties. Only the chart question-answering (CQA) task is considered, despite some datasets having multiple tasks. Abbreviations: Vis.=visual, Comp.=complexity, Temp.=template, Refer.=Reference, Reas.=reasoning, Rat. rationale, Annot.=annotation and Scal.=scalable. Cells marked with \u0026#39;X\u0026#39; indicate mixed attributes (e.g., partially template-based; scalable Q\u0026amp;A but non-scalable chart data.). “*” indicates that while the chart-plotting codes are public, the Q\u0026amp;A generation still relies on data tables. 🔽 Comparison of existing chart-related datasets across three properties. Only the chart question-answering (CQA) task is considered, despite some datasets having multiple tasks. Abbreviations: Vis.=visual, Comp.=complexity, Temp.=template, Refer.=Reference, Reas.=reasoning, Rat. rationale, Annot.=annotation and Scal.=scalable. Cells marked with \u0026#39;X\u0026#39; indicate mixed attributes (e.g., partially template-based; scalable Q\u0026amp;A but non-scalable chart data.). “*” indicates that while the chart-plotting codes are public, the Q\u0026amp;A generation still relies on data tables. 🔽 Table 1: Comparison of existing chart-related datasets across three properties. Only the chart question-answering (CQA) task is considered, despite some datasets having multiple tasks. Abbreviations: Vis.=visual, Comp.=complexity, Temp.=template, Refer.=Reference, Reas.=reasoning, Rat. rationale, Annot.=annotation and Scal.=scalable. Cells marked with \u0026#39;X\u0026#39; indicate mixed attributes (e.g., partially template-based; scalable Q\u0026amp;A but non-scalable chart data.). “*” indicates that while the chart-plotting codes are public, the Q\u0026amp;A generation still relies on data tables. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18798/","section":"Posts","summary":"Researchers created a new method called Code-as-Intermediary Translation (CIT) to improve multimodal large language models (MLLMs) understanding of charts. CIT uses code to translate visual charts int\u0026hellip;..","title":"Distill Visual Chart Reasoning Ability from LLMs to MLLMs","type":"posts"},{"content":" TL;DR # Framer is a novel interactive frame interpolation method that lets users customize transitions between two images by manipulating keypoints. It uses a pre-trained video diffusion model and provides both interactive and automated modes, demonstrating improved results in image morphing, video generation, and other applications. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper introduces Framer, an interactive frame interpolation method that allows users to customize the transition between two images by manipulating keypoints. It leverages a pre-trained video diffusion model and offers both interactive and automated modes. The results demonstrate improved quality and control over the interpolation process, showcasing applications in various fields.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames. 🔽 Quantitative comparison with existing video interpolation methods on reconstruction and generative metrics, evaluated on all 7 generated frames. DAVIS-7 DAVIS-7 DAVIS-7 DAVIS-7 UCF101-7 UCF101-7 UCF101-7 UCF101-7 UCF101-7 UCF101-7 PSNR↑ SSIM↑ LPIPS↓ FID↓ FVD↓ PSNR↑ SSIM↑ LPIPS↓ FID↓ FVD↓ AMT (Li et al., 2023) 21.66 0.7229 0.2860 39.17 245.25 26.64 0.9000 0.1878 37.80 270.98 RIFE (Huang et al., 2020) 22.00 0.7216 0.2663 39.16 319.79 27.04 0.9020 0.1575 27.96 300.40 FLAVR Kalluri et al. (2023) 20.94 0.6880 0.3305 52.23 296.37 26.50 0.8982 0.1836 37.79 279.58 FILM (Reda et al., 2022) 21.67 0.7121 0.2191 17.20 162.86 26.74 0.8983 0.1378 16.22 239.48 LDMVFI (Danier et al., 2024) 21.11 0.6900 0.2535 21.96 269.72 26.68 0.8955 0.1446 17.55 270.33 DynamicCrafter (Xing et al., 2023) 15.48 0.4668 0.4628 35.95 468.78 17.62 0.7082 0.3361 61.71 646.91 SVDKFI (Wang et al., 2024a) 16.71 0.5274 0.3440 26.59 382.19 21.04 0.7991 0.2146 44.81 301.33 Framer (Ours) 21.23 0.7218 0.2525 27.13 115.65 25.04 0.8806 0.1714 31.69 181.55 Framer with Co-Tracker (Ours) 22.75 0.7931 0.2199 27.43 102.31 27.08 0.9024 0.1714 32.37 159.87 More on figures 🔼 Figure 2: Framer supports (a) a user-interactive mode for customized point trajectories and (b) an \u0026lsquo;autopilot\u0026rsquo; mode for video frame interpolation without trajectory inputs. During training, (d) we fine-tune the 3D-UNet of a pre-trained video diffusion model for video frame interpolation. Afterward, (c) we introduce point trajectory control by freezing the 3D-UNet and fine-tuning the controlling branch. 🔼 Figure 3: Point trajectory estimation. The point trajectory is initialized by interpolating the coordinates of matched keypoints. In each de-noising step, we perform point tracking by finding the nearest neighbor of keypoints in the start and end frames, respectively. Lastly, We check the bi-directional tracking consistency before updating the point coordinate. 🔼 Figure 4: Qualitative comparison. \u0026lsquo;GT’ strands for ground truth. For each method, we only present the middle frame of 7 interpolated frames. The full results can be seen in Fig. S4 and Fig. S5 in the Appendix. 🔼 Figure 5: Reults on human preference. 🔼 Figure 6: Results on user interaction. The first row is generated without drag input, while the other two are generated with different drag controls. Customized trajectories are overlaid on frames. 🔼 Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames. 🔼 Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames. 🔼 Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames. 🔼 Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames. 🔼 Figure 12: Ablations on each component. \u0026lsquo;w/o trajectory\u0026rsquo; denotes inference without guidance from point trajectory, \u0026lsquo;w/o traj. update\u0026rsquo; indicates inference without trajectory updates, and \u0026lsquo;w/o bi\u0026rsquo; suggests trajectory updating without bi-directional consistency verification. 🔼 Figure 4: Qualitative comparison. \u0026lsquo;GT’ strands for ground truth. For each method, we only present the middle frame of 7 interpolated frames. The full results can be seen in Fig. S4 and Fig. S5 in the Appendix. 🔼 Figure 4: Qualitative comparison. \u0026lsquo;GT’ strands for ground truth. For each method, we only present the middle frame of 7 interpolated frames. The full results can be seen in Fig. S4 and Fig. S5 in the Appendix. 🔼 Figure 4: Qualitative comparison. \u0026lsquo;GT’ strands for ground truth. For each method, we only present the middle frame of 7 interpolated frames. The full results can be seen in Fig. S4 and Fig. S5 in the Appendix. 🔼 Figure 4: Qualitative comparison. \u0026lsquo;GT’ strands for ground truth. For each method, we only present the middle frame of 7 interpolated frames. The full results can be seen in Fig. S4 and Fig. S5 in the Appendix. 🔼 Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames. 🔼 Figure S10: More results on (a) cartoon and (b) sketch interpolation. 🔼 Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames. 🔼 Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames. 🔼 Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames. More on tables 🔽 Quantitative comparison with existing video interpolation methods on reconstruction and generative metrics, evaluated on all 7 generated frames. 🔽 Table 1: Quantitative comparison with existing video interpolation methods on reconstruction and generative metrics, evaluated on all 7 generated frames. 🔽 Table 1: Quantitative comparison with existing video interpolation methods on reconstruction and generative metrics, evaluated on all 7 generated frames. 🔽 Quantitative comparison with existing video interpolation methods on reconstruction and generative metrics, evaluated on all 7 generated frames. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18978/","section":"Posts","summary":"Framer is a novel interactive frame interpolation method that lets users customize transitions between two images by manipulating keypoints. It uses a pre-trained video diffusion model and provides bo\u0026hellip;..","title":"Framer: Interactive Frame Interpolation","type":"posts"},{"content":" TL;DR # LOGO is a novel training strategy that improves the alignment of long-context models with human preferences by using preference optimization and overcoming GPU memory limitations through a reference-free approach and a positional index synthesis method. Experiments show that LOGO enhances generation performance in various tasks without sacrificing performance on other tasks, offering an efficient method for enhancing long-context capabilities of LLMs. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper introduces LOGO, a novel training strategy that uses preference optimization to improve the alignment of long-context models (LCMs) with human preferences. LOGO addresses the limitations of existing methods by employing a reference-free preference optimization strategy and a positional index synthesis method to overcome GPU memory constraints. Experiments demonstrate LOGO\u0026rsquo;s effectiveness in enhancing the generation capabilities of LCMs while preserving their performance on other tasks.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM. 🔽 Evaluation results on LongBench benchmark, where † denotes training-free method. Models S-Doc QA M-Doc QA Summ Few-shot Synthetic Avg. GPT-3.5-Turbo-16K 39.8 38.7 26.5 67.1 37.8 42.0 LongChat-v1.5-7B-32k 28.7 20.6 26.7 60.0 15.8 30.4 LLama-3.1-8B-Instruct-128K 23.9 15.8 28.9 69.8 57.5 39.2 Results on SCMs (scaling x8 context window) Results on SCMs (scaling x8 context window) Results on SCMs (scaling x8 context window) Results on SCMs (scaling x8 context window) Results on SCMs (scaling x8 context window) Results on SCMs (scaling x8 context window) Results on SCMs (scaling x8 context window) Llama-3-8B-Instruct-8K 39.3 36.2 24.8 63.5 39.9 40.7 + YaRN-64K+ 38.0 36.6 27.4 61.7 40.9 40.9 + RandPOS-64K 32.5 30.5 26.5 61.3 33.4 36.8 + LOGO-64K 39.8 36.7 28.8 65.4 49.0 43.9 Llama-2-7B-Chat-4K 24.9 22.6 24.7 60.0 5.9 27.6 + LOGO-32K 26.7 23.3 26.3 63.1 11.1 30.1 Results on LCMs (long-context alignment) Results on LCMs (long-context alignment) Results on LCMs (long-context alignment) Results on LCMs (long-context alignment) Results on LCMs (long-context alignment) Results on LCMs (long-context alignment) Results on LCMs (long-context alignment) Llama-3-8B-Instruct-80K 43.0 39.8 22.2 64.3 46.3 42.3 + Instruct Tuning (Full) 38.8 35.0 24.6 65.9 44.5 41.8 + Instruct Tuning (Partial) 39.3 36.2 26.8 63.5 48.0 42.8 + LOGO-80K 44.0 41.2 28.1 68.6 53.0 47.0 Llama-2-7B-Instruct-80K 26.9 23.8 21.3 65.0 7.9 29.0 + LOGO-80K 33.6 28.0 29.4 65.1 24.5 36.1 Mistral-Instruct-7B- V0.2-32K 31.7 30.6 16.7 58.4 17.9 31.1 + LOGO-32K 38.3 37.6 26.1 67.0 31.5 40.1 More on figures 🔼 Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM. 🔼 Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM. More on tables 🔽 Evaluation results on LongBench benchmark, where † denotes training-free method. 🔽 Evaluation results on LongBench benchmark, where † denotes training-free method. 🔽 Table 1: Evaluation results on LongBench benchmark, where † denotes training-free method. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18533/","section":"Posts","summary":"LOGO is a novel training strategy that improves the alignment of long-context models with human preferences by using preference optimization and overcoming GPU memory limitations through a reference-f\u0026hellip;..","title":"LOGO -- Long cOntext aliGnment via efficient preference Optimization","type":"posts"},{"content":" TL;DR # MotionCLR is a novel attention-based diffusion model for human motion generation and editing. It leverages self- and cross-attention mechanisms for fine-grained control, enabling various training-free editing operations like (de)emphasizing, replacement, and sequence shifting. The model\u0026rsquo;s explainability via attention maps allows for action counting and addresses generation failures. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper introduces MotionCLR, an attention-based diffusion model for human motion generation and editing. It provides a clear understanding of how self- and cross-attention mechanisms work within the model to achieve fine-grained control over motion generation and editing, enabling various interactive editing operations without retraining.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔽 Table 1: Comparison with different methods on the HumanML3D dataset. The \u0026#39;*\u0026#39; notation denotes the DDIM sampling inference design choice and the other is the DPM-solver sampling choice. Methods R-Precision↑ R-Precision↑ R-Precision↑ FID↓ MM-Dist↓ Multi-Modality↑ Methods Top 1 Top 2 Top 3 FID↓ MM-Dist↓ Multi-Modality↑ TM2T [2022b] 0.424±0.003 0.618±0.003 0.729±0.002 1.501 ±0.017 3.467±0.011 2.424±0.093 T2M [2022a] 0.455±0.003 0.636±0.003 0.736±0.002 1.087±0.021 3.347±0.008 2.219±0.074 MDM [2022b] - - 0.611 ±0.007 0.544±0.044 5.566±0.027 2.799±0.072 MLD [2023b] 0.481 ±0.003 0.673±0.003 0.772±0.002 0.473±0.013 3.196±0.010 2.413±0.079 MotionDiffuse [2024b] 0.491 ±0.001 0.681 ±0.001 0.782±0.001 0.630±0.001 3.113±0.001 1.553±0.042 T2M-GPT [2023a] 0.492±0.003 0.679±0.002 0.775±0.002 0.141 士0.005 3.121 ±0.009 1.831 ±0.048 ReMoDiffuse [2023b] 0.510±0.005 0.698±0.006 0.795±0.004 0.103±0.004 2.974±0.016 1.795±0.043 MoMask [2024a] 0.521 ±0.002 0.713±0.002 0.807±0.002 0.045 ±0.002 2.958±0.008 1.241 ±0.040 MotionCLR 0.542±0.001 0.733±0.002 0.827±0.003 0.099±0.003 2.981±0.011 2.145±0.043 MotionCLR* 0.544±0.001 0.732±0.001 0.831 士0.002 0.269±0.001 2.806±0.014 1.985±0.044 More on figures 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 10: Diverse generated motions driven by the same example. Prompt: \u0026lsquo;a person steps sideways to the left and then sideways to the right.\u0026rsquo;. (a) The diverse generated motions driven by the same example motion share similar movement content. (b) The root trajectories of diverse motions are with similar global trajectories, but not the same. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 2: System overview of MotionCLR architecture. (a) The basic CLR block includes four layers. (b) The sampling (a.k.a. Samp.) block includes two CLR blocks and one down/up-sampling operation. (c) MotionCLR is a U-Net-like architecture, composed of several Sampling blocks. 🔼 Figure 3: Empirical study of attention mechanisms. We use \u0026lsquo;a person jumps.\u0026rsquo; as an example. (a) Key frames of generated motion. (b) The root trajectory along the Y-axis (vertical height). The character jumps on ~15-40f, ~60-80f, and ~125-145f, respectively. (c) The cross-attention between timesteps and words. The \u0026lsquo;jump\u0026rsquo; word is highly activated aligning with the \u0026lsquo;jump\u0026rsquo; action. (d) The self-attention map visualization. It is obvious that the character jumps three times. Different jumps share similar local motion patterns. 🔼 Figure 5: Motion (de-)emphasizing. Different weights of \u0026lsquo;jump\u0026rsquo; (↑ or ↓) in \u0026lsquo;a man jumps.\u0026rsquo; 🔼 Figure 3: Empirical study of attention mechanisms. We use \u0026lsquo;a person jumps.\u0026rsquo; as an example. (a) Key frames of generated motion. (b) The root trajectory along the Y-axis (vertical height). The character jumps on ~15-40f, ~60-80f, and ~125-145f, respectively. (c) The cross-attention between timesteps and words. The \u0026lsquo;jump\u0026rsquo; word is highly activated aligning with the \u0026lsquo;jump\u0026rsquo; action. (d) The self-attention map visualization. It is obvious that the character jumps three times. Different jumps share similar local motion patterns. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 3: Empirical study of attention mechanisms. We use \u0026lsquo;a person jumps.\u0026rsquo; as an example. (a) Key frames of generated motion. (b) The root trajectory along the Y-axis (vertical height, in Fig. 3b). As can be seen in Fig. 3, the character jumps at ~ 15-40f, ~ 60-80f, and ~ 125-145f, respectively. (c) The cross-attention between timesteps and words. The \u0026lsquo;jump\u0026rsquo; word is highly activated aligning with the \u0026lsquo;jump\u0026rsquo; action. (d) The self-attention map visualization. It is obvious that the character jumps three times. Different jumps share similar local motion patterns. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 10: Diverse generated motions driven by the same example. Prompt: \u0026lsquo;a person steps sideways to the left and then sideways to the right.\u0026rsquo;. (a) The diverse generated motions driven by the same example motion share similar movement content. (b) The root trajectories of diverse motions are with similar global trajectories, but not the same. 🔼 Figure 10: Diverse generated motions driven by the same example. Prompt: \u0026lsquo;a person steps sideways to the left and then sideways to the right.\u0026rsquo;. (a) The diverse generated motions driven by the same example motion share similar movement content. (b) The root trajectories of diverse motions are with similar global trajectories, but not the same. 🔼 Figure 10: Diverse generated motions driven by the same example. Prompt: \u0026lsquo;a person steps sideways to the left and then sideways to the right.\u0026rsquo;. (a) The diverse generated motions driven by the same example motion share similar movement content. (b) The root trajectories of diverse motions are with similar global trajectories, but not the same. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 10: Diverse generated motions driven by the same example. Prompt: \u0026lsquo;a person steps sideways to the left and then sideways to the right.\u0026rsquo;. (a) The diverse generated motions driven by the same example motion share similar movement content. (b) The root trajectories of diverse motions are with similar global trajectories, but not the same. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 10: Diverse generated motions driven by the same example. Prompt: “a person steps sideways to the left and then sideways to the right.” (a) The diverse generated motions driven by the same example motion share similar movement content. (b) The root trajectories of diverse motions are with similar global trajectories, but not the same. 🔼 Figure 10: Diverse generated motions driven by the same example. Prompt: “a person steps sideways to the left and then sideways to the right.”. (a) The diverse generated motions driven by the same example motion share similar movement content. (b) The root trajectories of diverse motions are with similar global trajectories, but not the same. 🔼 Figure 13: Comparison between w/ vs. w/o grounded motion generation settings. The root height and motion visualization of the textual prompt “a person jumps four times”. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 3: Empirical study of attention mechanisms. We use \u0026lsquo;a person jumps.\u0026rsquo; as an example. (a) Key frames of generated motion. (b) The root trajectory along the Y-axis (vertical height). The character jumps on ~15-40f, ~60-80f, and ~125-145f, respectively. (c) The cross-attention between timesteps and words. The \u0026lsquo;jump\u0026rsquo; word is highly activated aligning with the \u0026lsquo;jump\u0026rsquo; action. (d) The self-attention map visualization. It is obvious that the character jumps three times. Different jumps share similar local motion patterns. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. 🔼 Figure 1: MotionCLR (/\u0026lsquo;moυ∫n klır/) supports versatile motion generation and editing. The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of \u0026lsquo;jump\u0026rsquo;. (b) In-place replacing the action of \u0026lsquo;walks\u0026rsquo; with \u0026lsquo;jumps\u0026rsquo; and \u0026lsquo;dances\u0026rsquo;. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion. More on tables 🔽 Table 1: Comparison with different methods on the HumanML3D dataset. The \u0026#39;*\u0026#39; notation denotes the DDIM sampling inference design choice and the other is the DPM-solver sampling choice. 🔽 Table 1: Comparison with different methods on the HumanML3D dataset. The \u0026#39;*\u0026#39; notation denotes the DDIM sampling inference design choice and the other is the DPM-solver sampling choice. 🔽 Table 1: Comparison with different methods on the HumanML3D dataset. The \u0026#39;*\u0026#39; notation denotes the DDIM sampling inference design choice and the other is the DPM-solver sampling choice. 🔽 Table 1: Comparison with different methods on the HumanML3D dataset. The \u0026#39;*\u0026#39; notation denotes the DDIM sampling inference design choice and the other is the DPM-solver sampling choice. 🔽 Table 1: Comparison with different methods on the HumanML3D dataset. The \u0026#39;*\u0026#39; notation denotes the DDIM sampling inference design choice and the other is the DPM-solver sampling choice. 🔽 Table 1: Comparison with different methods on the HumanML3D dataset. The \u0026#39;*\u0026#39; notation denotes the DDIM sampling inference design choice and the other is the DPM-solver sampling choice. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18977/","section":"Posts","summary":"MotionCLR is a novel attention-based diffusion model for human motion generation and editing. It leverages self- and cross-attention mechanisms for fine-grained control, enabling various training-free\u0026hellip;..","title":"MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms","type":"posts"},{"content":" TL;DR # Current image watermarking struggles against advanced image editing. This paper introduces W-Bench, a benchmark to evaluate watermarking methods against various editing techniques, and VINE, a new method significantly improving robustness and image quality by using a pretrained diffusion model and analyzing image editing\u0026rsquo;s frequency characteristics. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper introduces W-Bench, a comprehensive benchmark for evaluating watermarking methods\u0026rsquo; robustness against various image editing techniques enabled by large-scale text-to-image models. It also proposes VINE, a novel watermarking method that significantly enhances robustness while maintaining high image quality by leveraging a pretrained diffusion model and analyzing frequency characteristics of image editing.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: (a) Flowchart of the W-Bench evaluation process. (b) Watermarking performance. Each method is illustrated with a diamond and four bars. The area of the diamond represents the method\u0026rsquo;s encoding capacity. The y-coordinate of the diamond\u0026rsquo;s center indicates normalized image quality, calculated by averaging the normalized PSNR, SSIM, LPIPS, and FID between watermarked and input images. The x-coordinate represents robustness, measured by the True Positive Rate at a 0.1% False Positive Rate (TPR@0.1%FPR) averaged across four types of image editing methods, encompassing a total of seven distinct models and algorithms. The four bars are oriented to signify different editing tasks: image regeneration (left), global editing (top), local editing (right), and image-to-video generation (bottom). The length of each bar reflects the method\u0026rsquo;s normalized TPR@0.1%FPR after each type of image editing-the longer the bar, the better the performance. 🔽 Comparison of watermarking performance in terms of watermarked image quality and detection accuracy across various image editing methods. Quality metrics are averaged over 10,000 images, and the TPR@0.1%FPR for each specific editing method is averaged over 5,000 images. The best value in each column is highlighted in bold, and the second best value is underlined. Abbreviations: Cap = Encoding Capacity; Sto = Stochastic Regeneration; Det = Deterministic Regeneration; Pix2Pix = Instruct-Pix2Pix; Ultra = UltraEdit; Magic = MagicBrush; CtrlN = ControlNet-Inpainting; SVD = Stable Video Diffusion. Config Blurring Distortions Watermark Encoder Watermark Encoder Watermark Encoder Watermark Encoder Watermark Encoder PSNR ↑ SSIM ↑ LPIPS ↓ FID ↓ TPR@0.1%FPR ↑ (%) TPR@0.1%FPR ↑ (%) TPR@0.1%FPR ↑ (%) TPR@0.1%FPR ↑ (%) Config Blurring Distortions Backbone Condition Skip Pretrained Finetune PSNR ↑ SSIM ↑ LPIPS ↓ FID ↓ Sto Det Pix2Pix Ultra Config A Simple UNet N.A. N.A. N.A. x 38.21 0.9828 0.0148 1.69 54.61 66.86 64.24 32.62 Config B Simple UNet N.A. N.A. N.A. 35.85 0.9766 0.0257 2.12 86.85 92.28 80.98 62.14 Config C Simple UNet N.A. N.A. N.A. 31.24 0.9501 0.0458 4.67 98.59 99.29 96.01 84.60 Config D ControlNet 32.68 0.9640 0.0298 2.87 90.82 94.89 91.86 70.69 Config E SDXL-Turbo Cond. Adaptor 36.76 0.9856 0.0102 0.53 90.86 94.78 92.88 70.68 Config F (VINE-B) Cond. Adaptor 40.51 0.9954 0.0029 0.08 91.03 99.25 96.30 80.90 Config G (VINE-R) Cond. Adaptor 37.34 0.9934 0.0063 0.15 99.66 99.98 97.46 86.86 Config H Cond. Adaptor 35.18 0.9812 0.0137 1.03 99.67 99.92 96.13 84.66 More on figures 🔼 Figure 2: Process for analyzing the impact of image editing on an image\u0026rsquo;s frequency spectrum. In this example, the editing model Instruct-Pix2Pix, denoted as ∈(·), is employed. The function F(·) represents the Fourier transform, and we visualize its magnitude on a logarithmic scale. 🔼 Figure 4: The overall framework of our method, VINE. We utilize the pretrained one-step text-to-image model SDXL-Turbo as the watermark encoder. A condition adaptor is incorporated to fuse the watermark with the image before passing the information to the VAE encoder. Zero-convolution layers (Zhang et al., 2023) and skip connections are added for better perceptual similarity. For decoding the watermark, we employ ConvNeXt-B (Liu et al., 2022b) as the decoder, with an additional fully connected layer to output a 100-bit watermark. Throughout the entire training process, the SDXL-Turbo text prompt is set to null prompt. Figure 9 shows the condition adaptor architecture. 🔼 Figure 1: (a) Flowchart of the W-Bench evaluation process. (b) Watermarking performance. Each method is illustrated with a diamond and four bars. The area of the diamond represents the method\u0026rsquo;s encoding capacity. The y-coordinate of the diamond\u0026rsquo;s center indicates normalized image quality, calculated by averaging the normalized PSNR, SSIM, LPIPS, and FID between watermarked and input images. The x-coordinate represents robustness, measured by the True Positive Rate at a 0.1% False Positive Rate (TPR@0.1%FPR) averaged across four types of image editing methods, encompassing a total of seven distinct models and algorithms. The four bars are oriented to signify different editing tasks: image regeneration (left), global editing (top), local editing (right), and image-to-video generation (bottom). The length of each bar reflects the method\u0026rsquo;s normalized TPR@0.1%FPR after each type of image editing-the longer the bar, the better the performance. 🔼 Figure 11: The reconstruction quality of stochastic regeneration and deterministic regeneration. Please zoom in for a closer look. 🔼 Figure 1: (a) Flowchart of the W-Bench evaluation process. (b) Watermarking performance. Each method is illustrated with a diamond and four bars. The area of the diamond represents the method\u0026rsquo;s encoding capacity. The y-coordinate of the diamond\u0026rsquo;s center indicates normalized image quality, calculated by averaging the normalized PSNR, SSIM, LPIPS, and FID between watermarked and input images. The x-coordinate represents robustness, measured by the True Positive Rate at a 0.1% False Positive Rate (TPR@0.1%FPR) averaged across four types of image editing methods, encompassing a total of seven distinct models and algorithms. The four bars are oriented to signify different editing tasks: image regeneration (left), global editing (top), local editing (right), and image-to-video generation (bottom). The length of each bar reflects the method\u0026rsquo;s normalized TPR@0.1%FPR after each type of image editing-the longer the bar, the better the performance. 🔼 Figure 1: (a) Flowchart of the W-Bench evaluation process. (b) Watermarking performance. Each method is illustrated with a diamond and four bars. The area of the diamond represents the method\u0026rsquo;s encoding capacity. The y-coordinate of the diamond\u0026rsquo;s center indicates normalized image quality, calculated by averaging the normalized PSNR, SSIM, LPIPS, and FID between watermarked and input images. The x-coordinate represents robustness, measured by the True Positive Rate at a 0.1% False Positive Rate (TPR@0.1%FPR) averaged across four types of image editing methods, encompassing a total of seven distinct models and algorithms. The four bars are oriented to signify different editing tasks: image regeneration (left), global editing (top), local editing (right), and image-to-video generation (bottom). The length of each bar reflects the method\u0026rsquo;s normalized TPR@0.1%FPR after each type of image editing—the longer the bar, the better the performance. 🔼 Figure 1: (a) Flowchart of the W-Bench evaluation process. (b) Watermarking performance. Each method is illustrated with a diamond and four bars. The area of the diamond represents the method\u0026rsquo;s encoding capacity. The y-coordinate of the diamond\u0026rsquo;s center indicates normalized image quality, calculated by averaging the normalized PSNR, SSIM, LPIPS, and FID between watermarked and input images. The x-coordinate represents robustness, measured by the True Positive Rate at a 0.1% False Positive Rate (TPR@0.1%FPR) averaged across four types of image editing methods, encompassing a total of seven distinct models and algorithms. The four bars are oriented to signify different editing tasks: image regeneration (left), global editing (top), local editing (right), and image-to-video generation (bottom). The length of each bar reflects the method\u0026rsquo;s normalized TPR@0.1%FPR after each type of image editing-the longer the bar, the better the performance. 🔼 Figure 1: (a) Flowchart of the W-Bench evaluation process. (b) Watermarking performance. Each method is illustrated with a diamond and four bars. The area of the diamond represents the method’s encoding capacity. The y-coordinate of the diamond’s center indicates normalized image quality, calculated by averaging the normalized PSNR, SSIM, LPIPS, and FID between watermarked and input images. The x-coordinate represents robustness, measured by the True Positive Rate at a 0.1% False Positive Rate (TPR@0.1%FPR) averaged across four types of image editing methods, encompassing a total of seven distinct models and algorithms. The four bars are oriented to signify different editing tasks: image regeneration (left), global editing (top), local editing (right), and image-to-video generation (bottom). The length of each bar reflects the method’s normalized TPR@0.1%FPR after each type of image editing—the longer the bar, the better the performance. More on tables 🔽 Table 1: Comparison of watermarking performance in terms of watermarked image quality and detection accuracy across various image editing methods. Quality metrics are averaged over 10,000 images, and the TPR@0.1%FPR for each specific editing method is averaged over 5,000 images. The best value in each column is highlighted in bold, and the second best value is underlined. Abbreviations: Cap = Encoding Capacity; Sto = Stochastic Regeneration; Det = Deterministic Regeneration; Pix2Pix = Instruct-Pix2Pix; Ultra = UltraEdit; Magic = MagicBrush; CtrlN = ControlNet-Inpainting; SVD = Stable Video Diffusion. 🔽 Comparison of watermarking performance in terms of watermarked image quality and detection accuracy across various image editing methods. Quality metrics are averaged over 10,000 images, and the TPR@0.1%FPR for each specific editing method is averaged over 5,000 images. The best value in each column is highlighted in bold, and the second best value is underlined. Abbreviations: Cap = Encoding Capacity; Sto = Stochastic Regeneration; Det = Deterministic Regeneration; Pix2Pix = Instruct-Pix2Pix; Ultra = UltraEdit; Magic = MagicBrush; CtrlN = ControlNet-Inpainting; SVD = Stable Video Diffusion. 🔽 Comparison of watermarking performance in terms of watermarked image quality and detection accuracy across various image editing methods. Quality metrics are averaged over 10,000 images, and the TPR@0.1%FPR for each specific editing method is averaged over 5,000 images. The best value in each column is highlighted in bold, and the second best value is underlined. Abbreviations: Cap = Encoding Capacity; Sto = Stochastic Regeneration; Det = Deterministic Regeneration; Pix2Pix = Instruct-Pix2Pix; Ultra = UltraEdit; Magic = MagicBrush; CtrlN = ControlNet-Inpainting; SVD = Stable Video Diffusion. 🔽 Comparison of watermarking performance in terms of watermarked image quality and detection accuracy across various image editing methods. Quality metrics are averaged over 10,000 images, and the TPR@0.1%FPR for each specific editing method is averaged over 5,000 images. The best value in each column is highlighted in bold, and the second best value is underlined. Abbreviations: Cap = Encoding Capacity; Sto = Stochastic Regeneration; Det = Deterministic Regeneration; Pix2Pix = Instruct-Pix2Pix; Ultra = UltraEdit; Magic = MagicBrush; CtrlN = ControlNet-Inpainting; SVD = Stable Video Diffusion. 🔽 Comparison of watermarking performance in terms of watermarked image quality and detection accuracy across various image editing methods. Quality metrics are averaged over 10,000 images, and the TPR@0.1%FPR for each specific editing method is averaged over 5,000 images. The best value in each column is highlighted in bold, and the second best value is underlined. Abbreviations: Cap = Encoding Capacity; Sto = Stochastic Regeneration; Det = Deterministic Regeneration; Pix2Pix = Instruct-Pix2Pix; Ultra = UltraEdit; Magic = MagicBrush; CtrlN = ControlNet-Inpainting; SVD = Stable Video Diffusion. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18775/","section":"Posts","summary":"Current image watermarking struggles against advanced image editing. This paper introduces W-Bench, a benchmark to evaluate watermarking methods against various editing techniques, and VINE, a new met\u0026hellip;..","title":"Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances","type":"posts"},{"content":" TL;DR # This paper comprehensively evaluates various language model editing methods, finding that they generally cause performance degradation and safety issues, especially when scaling to many edits. Current methods are only suitable for small-scale updates, motivating further research on more robust and reliable editing techniques. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper is important because it provides a comprehensive evaluation of existing language model editing methods, revealing their limitations and potential negative impacts. This is crucial for guiding future research towards more practical and reliable methods, especially regarding safety and scalability.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Illustration about the model editing and its pitfalls in retaining edited knowledge. Left panel: model editing methods can efficiently update knowledge within language models; Right panel: when scaling editing to thousands, the model can\u0026rsquo;t retain edited knowledge, see [16] for details. 🔽 Table 1: Evaluation results of GPT2-XL. experiments are conducted on a sever with 8 RTX 4090 GPUs. Method w/o Edit # Edits GPT2-XL GPT2-XL GPT2-XL GPT2-XL Method w/o Edit # Edits MMLU GSM8K BBH CSQA Method w/o Edit 0 0.2098 0.0144 0.0382 0.1941 PMET 10 0.2104 0.0159 0.0377 0.1941 PMET 20 0.1081 0.0144 0.0117 0.2048 PMET 50 0 0 0 0 PMET 100 0 0 0 0 PMET 500 0 0 0 0 PMET 1000 0 0 0 0 MEND 10 0.2096 0.0144 0.0377 0.1949 MEND 30 0.2094 0.0152 0.0388 0.1941 MEND 100 0.2098 0.0144 0.0380 0.1957 MEND 500 0.2100 0.0144 0.0382 0.1941 MEND 1000 0.2099 0.0144 0.0381 0.1933 KN 500 0 0 0 0 KN 1000 0 0 0 0 MEMIT 500 0.2112 0.0159 0.0363 0.1957 MEMIT 1000 0.2097 0.0152 0.0193 0.199 More on tables 🔽 Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B. 🔽 Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B. 🔽 Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B. 🔽 Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B. 🔽 Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B. 🔽 Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18785/","section":"Posts","summary":"This paper comprehensively evaluates various language model editing methods, finding that they generally cause performance degradation and safety issues, especially when scaling to many edits.  Curren\u0026hellip;..","title":"Should We Really Edit Language Models? On the Evaluation of Edited Language Models","type":"posts"},{"content":" TL;DR # This paper presents Skywork-Reward, a novel reward model for LLMs. It emphasizes data quality over quantity, creating a smaller, meticulously curated dataset using advanced filtering and selection techniques. The resulting models achieve state-of-the-art performance on the RewardBench benchmark, demonstrating the power of data-centric approaches for reward model training and showcasing the effectiveness of Bradley-Terry loss. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper introduces Skywork-Reward, a high-performing reward model for LLMs. It focuses on data-centric techniques, curating a smaller, higher-quality preference dataset (Skywork-Reward dataset) using effective data selection and filtering strategies. The resulting models achieve state-of-the-art performance on RewardBench, highlighting the practical impact of data-centric approaches in reward modeling.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1 | The composition chart of the Skywork-Reward preference data selections before and after applying data selection and filtering operations. 🔽 Table 1 | Statistics of the Skywork Reward Preference 80K dataset for reward modeling. Dataset # Pairs Avg. # Turns Avg. # Tokens (Prompt) Avg. # Tokens (Response) Completion Annotator HelpSteer2 7,221 3.9 21.3 690.0 Human + 6 LLMsa Human OffsetBias 8,504 2 69.1 222.1 GPT-3.5 + GPT-4 + Claude 3 Opus GPT-4 WildGuardMix 6,709 2 164.3 349.9 8 LLMsb Human Magpie Ultra 27,785 2 76.7 670.0 Llama 3.1 405B Instruct ArmoRM Magpie Pro (Llama 3) 2,030 2 34.2 621.5 Llama 3 70B Instruct ArmoRM Magpie Pro (Llama 3.1) 29,682 2 118.8 584.3 Llama 3.1 70B Instruct ArmoRM Magpie Air 42 2 66.6 240.0 Llama 3 8B Instruct ArmoRM Total 81,973 2.2 96.3 527.2 - - More on tables 🔽 Table 1 | Statistics of the Skywork Reward Preference 80K dataset for reward modeling. 🔽 Table 2 | Performance comparison of different reward models on RewardBench. The first block of the table includes the top reward models on the RewardBench leaderboard. The superscript in this block indicates that the results have not been officially verified. The second block of the table corresponds to Llama-3.1-8B and Gemma-2-27B (both instruct version) trained on Preference 700K and Preference 378K data, respectively. The final block of the table showcases the performance of our Skywork-Reward model series, which are trained on the Skywork Reward Preference 80K dataset. Notably, Skywork-Reward-Gemma-2-27B achieves state-of-the-art performance, outperforming several competitive models on RewardBench. The highest performance in each column is masked as bold. 🔽 Table 2 | Performance comparison of different reward models on RewardBench. The first block of the table includes the top reward models on the RewardBench leaderboard. The superscript in this block indicates that the results have not been officially verified. The second block of the table corresponds to Llama-3.1-8B and Gemma-2-27B (both instruct version) trained on Preference 700K and Preference 378K data, respectively. The final block of the table showcases the performance of our Skywork-Reward model series, which are trained on the Skywork Reward Preference 80K dataset. Notably, Skywork-Reward-Gemma-2-27B achieves state-of-the-art performance, outperforming several competitive models on RewardBench. The highest performance in each column is masked as bold. 🔽 Table 2 | Performance comparison of different reward models on RewardBench. The first block of the table includes the top reward models on the RewardBench leaderboard. The superscript in this block indicates that the results have not been officially verified. The second block of the table corresponds to Llama-3.1-8B and Gemma-2-27B (both instruct version) trained on Preference 700K and Preference 378K data, respectively. The final block of the table showcases the performance of our Skywork-Reward model series, which are trained on the Skywork Reward Preference 80K dataset. Notably, Skywork-Reward-Gemma-2-27B achieves state-of-the-art performance, outperforming several competitive models on RewardBench. The highest performance in each column is masked as bold. 🔽 Table 2 | Performance comparison of different reward models on RewardBench. The first block of the table includes the top reward models on the RewardBench leaderboard. The superscript in this block indicates that the results have not been officially verified. The second block of the table corresponds to Llama-3.1-8B and Gemma-2-27B (both instruct version) trained on Preference 700K and Preference 378K data, respectively. The final block of the table showcases the performance of our Skywork-Reward model series, which are trained on the Skywork Reward Preference 80K dataset. Notably, Skywork-Reward-Gemma-2-27B achieves state-of-the-art performance, outperforming several competitive models on RewardBench. The highest performance in each column is masked as bold. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18451/","section":"Posts","summary":"This paper presents Skywork-Reward, a novel reward model for LLMs.  It emphasizes data quality over quantity, creating a smaller, meticulously curated dataset using advanced filtering and selection te\u0026hellip;..","title":"Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs","type":"posts"},{"content":" TL;DR # SMITE is a novel video segmentation method using a pre-trained text-to-image diffusion model with a tracking module and low-frequency regularization. It achieves temporally consistent segmentations with flexible granularity, requiring only a few reference images, and outperforms existing methods on benchmark datasets. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # SMITE is important because it introduces a novel video segmentation technique that supports flexible granularity and can generalize to unseen videos using only a few reference images. This addresses the limitations of existing methods that require extensive manual annotations or struggle with inconsistent segmentations across video frames.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: SMITE. Using only one or few segmentation references with fine granularity (left), our method learns to segment different unseen videos respecting the segmentation references. 🔽 Quantitative evaluation on SMITE-50 dataset. The results are presented for each category (Face, Horse, Car, Non-Text) having 10 reference image during training. Methods Faces Faces Horses Horses Cars Cars Non-Text Non-Text Methods F meas. mIOU F meas. mIOU F meas. mIOU F meas. mIOU Baseline-I 0.81 72.95 0.64 65.48 0.57 61.38 0.67 66.69 GSAM2 0.73 63.28 0.76 72.76 0.64 63.56 - - Ours 0.89 77.28 0.79 75.09 0.82 75.10 0.77 73.08 More on figures 🔼 Figure 2: SMITE pipeline. During inference (a), we invert a given video into a noisy latent by iteratively adding noise. We then use an inflated U-Net denoiser (b) along with the trained text embedding as input to denoise the segments. A tracking module ensures that the generated segments are spatially and temporally consistent via spatio-temporal guidance. The video latent zł is updated by a tracking energy Etrack (c) that makes the segments temporally consistent and also a low-frequency regularizer (d) Ereg which guides the model towards better spatial consistency. 🔼 Figure 4: Segment tracking module ensures that segments are consistent across time. It uses co-tracker to track each point of the object\u0026rsquo;s segment (here it is nose) and then finds point correspondence of this segment (denoted by blue dots) across timesteps. When the tracked point is of a different class (e.g,. face) then it is recovered by using temporal voting. The misclassified pixel is then replaced by the average of the neighbouring pixels of adjacent frames. This results are temporally consistent segments without visible flickers. 🔼 Figure 3: Best viewed in Adobe Acrobat. 🔼 Figure 5: SMITE-50 Dataset sample. 🔼 Figure 6: Visual comparisons with other methods demonstrate that SMITE maintains better motion consistency of segments and delivers cleaner, more accurate segmentations. Both GSAM2 and Baseline-I struggle to accurately capture the horse’s mane, and GSAM2 misses one leg (Left), whereas our method yields more precise results. Additionally, both alternative techniques create artifacts around the chin (Right), while SMITE produces a cleaner segmentation. 🔼 Figure 7: Additional results. We visualize the generalization capability of SMITE model (trained on the reference images) in various challenging poses, shape, and even in cut-shapes. 🔼 Figure 8: Segmentation results in challenging scenarios . SMITE accurately segments out the objects under occlusion (\u0026lsquo;ice-cream\u0026rsquo;) or camouflage (\u0026rsquo;turtle\u0026rsquo;) highlighting the robustness of our segmentation technique. More on tables 🔽 Quantitative evaluation on SMITE-50 dataset. The results are presented for each category (Face, Horse, Car, Non-Text) having 10 reference image during training. 🔽 Quantitative evaluation on SMITE-50 dataset. The results are presented for each category (Face, Horse, Car, Non-Text) having 10 reference image during training. 🔽 Quantitative evaluation on SMITE-50 dataset. The results are presented for each category (Face, Horse, Car, Non-Text) having 10 reference image during training. 🔽 Table 1: Quantitative evaluation on SMITE-50 dataset. The results are presented for each category (Face, Horse, Car, Non-Text) having 10 reference image during training. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18538/","section":"Posts","summary":"SMITE is a novel video segmentation method using a pre-trained text-to-image diffusion model with a tracking module and low-frequency regularization.  It achieves temporally consistent segmentations w\u0026hellip;..","title":"SMITE: Segment Me In TimE","type":"posts"},{"content":" TL;DR # Stable Consistency Tuning (SCT) improves consistency model training by reducing variance and discretization errors, leading to faster convergence and state-of-the-art image generation quality on CIFAR-10 and ImageNet-64. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper presents Stable Consistency Tuning (SCT), a novel method that improves the training of consistency models for image generation. SCT addresses issues like high training variance and discretization errors by incorporating variance-reduced learning and a smoother training schedule. The method achieves state-of-the-art results on benchmark datasets.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models. 🔽 Table 2: Comparing the quality of samples on CIFAR-10. Fu-Yun Wang Zhengyang Geng Hongsheng Li MMLab, CUHK Carnegie Mellon University MMLab, CUHK Hong Kong SAR Pittsburgh, USA Hong Kong SAR fywang@link . cuhk · edu . hk zhengyanggeng@gmail · com hsli@ee · cuhk · edu. hk More on figures 🔼 Figure 2: Phasing the ODE path along the time axis for consistency training. We visualize both training and inference techniques in discrete form for easier understanding. 🔼 Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models. 🔼 Figure 7: 1-step samples from class-conditional SCT trained on CIFAR-10. Each row corresponds to a different class. 🔼 Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models. 🔼 Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models. 🔼 Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models. 🔼 Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models. 🔼 Figure 13: 1-step samples from class-conditional SCT trained on ImageNet-64 (FID 2.23). Each row corresponds to a different class. 🔼 Figure 13: 1-step samples from class-conditional SCT trained on ImageNet-64 (FID 2.23). Each row corresponds to a different class. 🔼 Figure 13: 1-step samples from class-conditional SCT trained on ImageNet-64 (FID 2.23). Each row corresponds to a different class. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18958/","section":"Posts","summary":"Stable Consistency Tuning (SCT) improves consistency model training by reducing variance and discretization errors, leading to faster convergence and state-of-the-art image generation quality on CIFAR\u0026hellip;..","title":"Stable Consistency Tuning: Understanding and Improving Consistency Models","type":"posts"},{"content":"","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" TL;DR # Taipan is a new hybrid language model that combines the efficiency of state-space models with the power of selective attention. It significantly outperforms existing models on long-context tasks, handling up to 1 million tokens while maintaining computational efficiency. This is achieved by strategically focusing attention on key tokens requiring long-range dependencies, improving performance on in-context retrieval and structured data extraction. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # Taipan is a novel hybrid architecture for efficient long-context language modeling that combines the efficiency of Mamba-2 with the expressive power of selective attention layers. It addresses the limitations of existing models by strategically selecting tokens requiring long-range interactions, removing less important features, and augmenting their representations using attention. Taipan achieves superior performance in memory-intensive tasks while preserving computational efficiency, extending accurate predictions to context lengths of up to 1 million tokens.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 2: An overview of the Taipan architecture. 🔽 Table 1: Zero shot results of Taipan against baseline models. Params \u0026amp; Data Model Wino. PIQA Hella. ARCE ARC� OB. Truth. RACE BoolQ Avg. 190M 27B Transformer++ 47.1 60.9 27.9 42.2 20.5 18.9 42.9 25.4 57.2 38.1 190M 27B Mamba 49.6 60.7 29.3 45.3 21.8 20.6 40.8 27.2 59.3 39.4 190M 27B Jamba 49.9 60.3 29.2 46.3 21.4 18.5 39.8 27.4 58.6 39.1 190M 27B Taipan 51.0 62.6 29.4 46.7 20.7 21.8 41.1 26.6 58.7 39.9 450M 100B Transformer++ 51.5 67.6 42.3 60.8 27.7 33.4 39.2 30.5 54.7 45.3 450M 100B Mamba 52.7 68.9 42.7 61.4 27.1 34.0 38.5 29.3 53.2 45.3 450M 100B Jamba 53.1 69.3 44.3 62.6 28.7 34.4 37.5 31.3 55.7 46.3 450M 100B Taipan 53.0 69.6 46.6 65.6 32.9 36.6 38.6 30.7 60.4 48.2 1.3B 100B Transformer++ 53.8 71.6 53.8 63.2 36.3 36.4 44.0 31.2 59.4 49.9 1.3B 100B Mamba 55.2 73.0 55.6 70.7 38.0 39.0 39.9 32.0 61.8 51.7 1.3B 100B Jamba 54.7 73.8 55.8 69.7 37.6 41.8 40.4 32.8 59.2 51.8 1.3B 100B Taipan 57.0 74.9 57.9 71.2 39.3 40.4 43.0 34.4 61.5 53.3 ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18572/","section":"Posts","summary":"Taipan is a new hybrid language model that combines the efficiency of state-space models with the power of selective attention.  It significantly outperforms existing models on long-context tasks, han\u0026hellip;..","title":"Taipan: Efficient and Expressive State Space Language Models with Selective Attention","type":"posts"},{"content":" TL;DR # UNBOUNDED is a novel generative infinite game using AI to simulate character life in real-time. It overcomes limitations of traditional games by employing a specialized LLM for dynamic game mechanics and a new dynamic regional IP-Adapter for consistent visual generation of characters and environments. The system is evaluated through qualitative and quantitative analysis, demonstrating significant improvements over previous approaches. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper introduces UNBOUNDED, a generative infinite game using LLMs and diffusion models for real-time character life simulation. It addresses challenges in generating consistent characters and environments across multiple scenes by introducing a novel dynamic regional IP-Adapter with block drop. The authors also present a distilled LLM game engine for interactive gameplay, achieving interactive speeds by leveraging collaborative strong LLMs and distillation techniques. The work pushes boundaries in generative game design and provides new technical innovations.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: An example of UNBOUNDED. We follow the life of Archibus, the user\u0026rsquo;s custom wizard character. The user can interact with the generative game using natural language, and Archibus\u0026rsquo; hunger, energy and fun meters update accordingly. A spontaneous and unconstrained story unfolds while the user playing, and the character can explore new environments with a myriad of possible actions and unexpected interactions. The game runs in interactive speeds, refreshing every second. 🔽 Comparison of UNBOUNDED and other methods for maintaining environment consistency and character consistency. UNBOUNDED achieves the best performance in maintaining consistency, while maintaining comparable semantic alignment with the text prompt. Best scores are in bold. Methods Environment Consistency Environment Consistency Environment Consistency Character Consistency Character Consistency Character Consistency Semantic Alignment Methods CLIP-IE ↑ DINOE ↑ DreamSimE ↓ CLIP-IC ↑ DINOC ↑ DreamSim� ↓ CLIP-T↑ IP-Adapter Ye et al.. 2023 0.470 0.381 0.595 0.366 0.139 0.832 0.168 IP-Adapter-Instruct Kowles et al. 2024 0.334 0.151 0.832 0.246 0.124 0.872 0.098 StoryDiffusion Zhou etal., 2024b 0.528 0.257 0.733 0.629 0.464 0.545 0.242 Ours 0.563 0.322 0.675 0.676 0.470 0.488 0.242 More on figures 🔼 Figure 2: Example of UNBOUNDED. Based on an initial user input, UNBOUNDED sets up game simulation environments, and generates character actions in the environments. Users can interact with the character with natural language instructions, exploring the game with unlimited options. 🔼 Figure 3: Generative game examples of UNBOUNDED. The user can insert a custom character into the game, engage with the character through natural language instructions, bring the character to different environments, and interact with it to maintain a healthy state under the games\u0026rsquo; mechanics. 🔼 Figure 4: (a) Our overall image generation method. We achieve real-time image generation with LCM LORA, maintain character consistency with DreamBooth LoRAs, and introduce a regional IP-Adapter (shown in (c)) for improved environment and character consistency. (b) Our proposed dynamic mask generation separating the environment and character conditioning, preventing interference between the two. 🔼 Figure 6: Overview of our user-simulation data collection process for LLM distillation. (a) We begin by collecting diverse topic and character data, filtered using ROUGE-L for diversity. (b) The World LLM and User LLM interact to generate user-simulation data through multi-round exchanges. More on tables 🔽 Table 1: Comparison of UNBOUNDED and other methods for maintaining environment consistency and character consistency. UNBOUNDED achieves the best performance in maintaining consistency, while maintaining comparable semantic alignment with the text prompt. Best scores are in bold. 🔽 Comparison of UNBOUNDED and other methods for maintaining environment consistency and character consistency. UNBOUNDED achieves the best performance in maintaining consistency, while maintaining comparable semantic alignment with the text prompt. Best scores are in bold. 🔽 Table 3: Comparison of UNBOUNDED and different LLMs on serving as game engines for open-ended interactions and integrated game mechanics. We use GPT-4 to provide pairwise scores between our model and other LLMs. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18975/","section":"Posts","summary":"UNBOUNDED is a novel generative infinite game using AI to simulate character life in real-time. It overcomes limitations of traditional games by employing a specialized LLM for dynamic game mechanics \u0026hellip;..","title":"Unbounded: A Generative Infinite Game of Character Life Simulation","type":"posts"},{"content":" TL;DR # ScaleQuest is a novel data synthesis method that uses small open-source LLMs to create a large, high-quality mathematical reasoning dataset. This dataset significantly improves the performance of mainstream open-source LLMs, surpassing even some closed-source models, and offers a scalable, cost-effective solution for training data generation. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper is important because it introduces ScaleQuest, a novel and scalable method for synthesizing high-quality mathematical reasoning datasets using only small, open-source language models. This addresses the critical need for large-scale, affordable training data to improve the reasoning capabilities of LLMs, especially within the open-source community. The resulting dataset significantly boosts the performance of several open-source models, even surpassing some proprietary models. This work pushes the boundaries of LLM training data creation and makes significant advancements in open-source LLM development.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 2: Overview of our ScaleQuest method. 🔽 Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5. Model Synthesis Model GSM8K MATH College Math Olympiad Bench Average Teacher Models in Data Synthesis Teacher Models in Data Synthesis Teacher Models in Data Synthesis Teacher Models in Data Synthesis Teacher Models in Data Synthesis Teacher Models in Data Synthesis Teacher Models in Data Synthesis GPT-4-0314 - 94.7 52.6 24.4 - - GPT-4-Turbo-24-04-09 - 94.5 73.4 - - - GPT-4o-2024-08-06 - 92.9 81.1 50.2 43.3 66.9 DeepSeekMath-7B-RL - 88.2 52.4 41.4 19.0 49.3 Qwen2-Math-7B-Instruct - 89.5 73.1 50.5 37.8 62.7 General Base Model General Base Model General Base Model General Base Model General Base Model General Base Model General Base Model Mistral-7B- WizardMath GPT-4 81.9 33.3 21.5 8.6 36.3 Mistral-7B-MetaMath GPT-3.5 77.7 28.2 19.1 5.8 32.7 Mistral-7B-MMIQC GPT-4 75.7 36.3 24.8 10.8 36.9 Mistral-7B-MathScale GPT-3.5 74.8 35.2 21.8 - - Mistral-7B-KPMath GPT-4 82.1 46.8 - - - Mistral-7B-DART-Math DSMath-7B-RL 81.1 45.5 29.4 14.7 42.7 Mistral-7B-NuminaMath GPT-4o 82.1 49.4 33.8 19.4 46.2 Mistral-7B-ScaleQuest Qwen2-Math-7B-Ins 88.5 62.9 43.5 26.8 55.4 Llama3-8B-MetaMath GPT-3.5 77.3 32.5 20.6 5.5 34.0 Llama3-8B-MMIQC GPT-4 77.6 39.5 29.5 9.6 39.1 Llama3-8B-DART-Math , DSMath-7B-RL 81.1 46.6 28.8 14.5 42.8 Llama3-8B-NuminaMath GPT-4o 77.2 50.7 33.2 17.8 44.7 Llama3-8B-ScaleQuest Qwen2-Math-7B-Ins 87.9 64.4 42.8 25.3 55.1 Math-Specialized Base Model Math-Specialized Base Model Math-Specialized Base Model Math-Specialized Base Model Math-Specialized Base Model Math-Specialized Base Model Math-Specialized Base Model DeepSeekMath-7B-Instruct - 82.7 46.9 37.1 14.2 45.2 DeepSeekMath-7B-MMIQC GPT-4 79.0 45.3 35.3 13.0 43.2 DeepSeekMath-7B-KPMath-Plus GPT-4 83.9 48.8 - - - DeepSeekMath-7B-DART-Math DSMath-7B-RL 86.8 53.6 40.7 21.7 50.7 DeepSeekMath-7B-Nurnina-Math GPT-4o 75.4 55.2 36.9 19.9 46.9 DeepSeekMath-7B-ScaleQuest Qwen2-Math-7B-Ins 89.5 66.6 47.7 29.9 58.4 Qwen2-Math-7B-MetaMath GPT-3.5 83.9 49.5 39.9 17.9 47.8 Qwen2-Math-7B-DART-Math DSMath-7B-RL 88.6 58.8 45.4 23.1 54.0 Qwen2-Math-7B-Numina-Math GPT-4o 84.6 65.6 45.5 33.6 57.3 Qwen2-Math-7B-ScaleQuest Qwen2-Math-7B-Ins 89.7 73.4 50.0 38.5 62.9 More on figures 🔼 Figure 2: Overview of our ScaleQuest method. 🔼 Figure 1: Left: Results of different models on MATH, where -ScaleQuest denotes ours. Right: Results of Llama3-8B fine-tuned on publicly available datasets constructed by different methods. 🔼 Figure 1: Left: Results of different models on MATH, where -ScaleQuest denotes ours. Right: Results of Llama3-8B fine-tuned on publicly available datasets constructed by different methods. More on tables 🔽 Table 1: Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5. 🔽 Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5. 🔽 Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5. 🔽 Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5. 🔽 Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5. 🔽 Table 1: Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5. 🔽 Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18693/","section":"Posts","summary":"ScaleQuest is a novel data synthesis method that uses small open-source LLMs to create a large, high-quality mathematical reasoning dataset.  This dataset significantly improves the performance of mai\u0026hellip;..","title":"Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch","type":"posts"},{"content":" TL;DR # WAFFLE is a new fine-tuning approach for multi-modal language models that significantly improves automated front-end web development by enhancing their understanding of HTML structure and aligning their understanding of UI images and HTML code, leading to state-of-the-art results on multiple benchmarks. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # WAFFLE is a novel fine-tuning strategy for multi-modal large language models (MLLMs) that improves the automation of HTML code generation from UI designs. It addresses two key challenges: representing HTML\u0026rsquo;s hierarchical structure and bridging the gap between visual UI designs and text-based HTML. WAFFLE uses a structure-aware attention mechanism and contrastive fine-tuning to achieve state-of-the-art performance on UI-to-HTML code generation benchmarks.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: Removing the children of the element highlighted in yellow does not affect the structure of the visual layout of itself or its sibling element . 🔽 Table 2: Main results on the WebSight-Test dataset. Shanchao Liang Nan Jiang Shangshu Qian Lin Tan Purdue University Purdue University Purdue University Purdue University liang422@purdue.edu jiang719@purdue.edu qian151 @purdue.edu lintan@purdue.edu More on figures 🔼 Overview of WAFFLE, including training data mutation, structure-aware attention, and contrastive learning. 🔼 Example of structure-aware attention. 🔼 Figure 5: Example test instance from WebSight-Test dataset, with the generated images by GPT-40, Standard FT, and WAFFLE. 🔼 Figure 6: Illustration of the tuning process of the parameter that controls the effect of structure-aware attention. In (b), the green line almost overlaps with the blue line. More on tables 🔽 Main results on the WebSight-Test dataset. 🔽 Table 4: Ablation studies on the two test datasets. LLEM refers to the averaged Low-Level Element Matching. 🔽 Table 5: Human evaluation on two datasets using VLM-WebSight as the backbone. The numbers are shown as \u0026#39;xly (x\u0026#43;y)\u0026#39;, where x is the result on WebSight-Test and y is the result on Design2Code. 🔽 Table 6: CW-SSIM on 20 samples using the VLM-WebSight backbone. “Prior” refers to “without intermediate mistakes”, and “Current” to “with intermediate mistakes”. 🔽 Table 7: Specification for Mutation Rules to construct the Contrastive dataset. 🔽 Table 8: Distance (d) and similarity (sim) between averaged image embeddings v\u0026lt;sup\u0026gt;i\u0026lt;/sup\u0026gt; and text embeddings t\u0026lt;sup\u0026gt;i\u0026lt;/sup\u0026gt;, using Moondream2 as the backbone. 🔽 Table 9: Distance (d) and similarity (sim) between each averaged image embeddings v² with the corresponding centroid c of the group of mutants, with Moondream2 backbone. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18362/","section":"Posts","summary":"WAFFLE is a new fine-tuning approach for multi-modal language models that significantly improves automated front-end web development by enhancing their understanding of HTML structure and aligning the\u0026hellip;..","title":"WAFFLE: Multi-Modal Model for Automated Front-End Development","type":"posts"},{"content":" TL;DR # Large language models (LLMs) don\u0026rsquo;t use their full context window due to a skewed distribution of positional information during training. The authors introduce STRING, a training-free method that shifts position embeddings to improve performance on long context tasks. STRING dramatically improves performance on open-source LLMs, even outperforming some commercial models. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper investigates why the effective context length of large language models (LLMs) falls short of their claimed context window size. The authors attribute this limitation to a left-skewed frequency distribution of relative positions in the LLM\u0026rsquo;s pre-training data. They propose STRING, a training-free method that shifts well-trained positions to enhance performance and demonstrate significant improvements in multiple LLMs on long-context benchmarks. The findings highlight a critical limitation in current LLM designs and provide a potential solution.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Analyzing effective context length of LLMs pretrained on SlimPajama with respect to training length, token consumption, and position frequency. In Figure 2b, we use the model effective length as the X-axis, and the Y-axis indicates the number of times the model was exposed to that specific position during training. 🔽 Needle-in-a-Haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where means the size of the training context window. All the models were tested using their training length. The number of test cases is 500. Model Ltrain ReRoPE NTK RoPE(origin) Self-Extend YaRN DCA STRING TinyLlama-1.3B (ours) 2k 62.8 62.0 56.6 60.2 68.6 74.4 84.6 TinyLlama-1.1B-3T 2k 77.2 79.8 69.8 83.2 88.0 80.2 97.2 Llama-2-7B 4k 98.6 98.6 98.0 95.4 98.0 91.6 100.0 Llama-3-8B 8k 99.6 100.0 99.8 99.8 100.0 99.9 99.6 LWM-7B-base 32k 25.2 19.4 31.8 29.0 22.2 28.8 50.4 Mistral-7B-base 32k 54.5 42.2 52.8 54.2 48.2 64.2 73.0 Llama-3.1-8B 128k 53.6 71.2 66.0 65.8 68.8 72.8 95.2 Average - 67.3 67.6 67.8 69.6 70.5 73.1 85.7 More on figures 🔼 Analyzing effective context length of LLMs pretrained on SlimPajama with respect to training length, token consumption, and position frequency. In Figure 2b, we use the model effective length as the X-axis, and the Y-axis indicates the number of times the model was exposed to that specific position during training. 🔼 Analyzing effective context length of LLMs pretrained on SlimPajama with respect to training length, token consumption, and position frequency. In Figure 2b, we use the model effective length as the X-axis, and the Y-axis indicates the number of times the model was exposed to that specific position during training. More on tables 🔽 Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where Ltrain means the size of the training context window. All the models were tested using their training length. The number of test cases is 500. 🔽 Table 3: Comparison of STRING with three leading commercial long-context models on InfiniteBench. Each model is evaluated using a maximum context length of 128K. 🔽 Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where Ltrain means the size of the training context window. All the models were tested using their training length. The number of test cases is 500. 🔽 Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where means the size of the training context window. All the models were tested using their training length. The number of test cases is 500. 🔽 Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where Ltrain means the size of the training context window. All the models were tested using their training length. The number of test cases is 500. 🔽 Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where Ltrain means the size of the training context window. All the models were tested using their training length. The number of test cases is 500. ","date":"24 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18745/","section":"Posts","summary":"Large language models (LLMs) don\u0026rsquo;t use their full context window due to a skewed distribution of positional information during training.  The authors introduce STRING, a training-free method that shif\u0026hellip;..","title":"Why Does the Effective Context Length of LLMs Fall Short?","type":"posts"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/2024-10-23/","section":"Tags","summary":"","title":"2024-10-23","type":"tags"},{"content":" TL;DR # ADEM-VL is a novel vision-language tuning framework that achieves high efficiency by using a parameter-free cross-attention mechanism, multiscale visual features, and adaptive fusion. It outperforms existing methods on various vision-language tasks while requiring substantially fewer parameters and less computation. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper proposes ADEM-VL, an efficient vision-language tuning framework that uses a parameter-free cross-attention mechanism for multimodal fusion. It significantly reduces the number of trainable parameters and computational complexity compared to existing methods. The framework also employs multiscale visual feature generation and an adaptive fusion scheme, improving efficiency and performance on various vision-language tasks.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance. 🔽 EVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT CONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12. Input: text Xt, image Xi, low-rank projection matrix W', Input: text Xt, image Xi, low-rank projection matrix W', Output: scales S E Zn, drop ratio 2 fused feature XI 1: Xl ← Tokenizer(xt) 2: X v , Xv,cls ← CLIP(xi) 3: X ← concat( [X. v,cls, Xi]) 1 4: X v ← Xv W' 5: X\u0026rsquo; ← X v v 6: for S in S do 7: さ ← pooling(Xv, s) v,s 8: ← concat( [X\u0026rsquo;⌀, X\u0026rsquo;o s]) v 9: end for ▷ Multiscale visual prompt (Sec. III-C) 10: for layer in LLM do 11: Xl ← layer(Xi) 12: 13: attention A ← silu(Xt)silu(X.)T ▷ Parameter-free cross- attention (Sec. III-B) 14: Asorted ← torch.sort(A, dim=1) 15: Index 2 ← int(y x A.size(dim=1)) 16: threshold T ← Asorted [:,2] 17: mask M ← torch.ones. _like(A) 18: 19: M [torch.where(A \u0026lt; T)] ← 0 Adaptine fusion (Sec. III-D) A ← A · M▷ 20: X1 ← Xl + AX⌀T 21: end for More on figures 🔼 Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales. 🔼 Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance. 🔼 Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance. 🔼 Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales. 🔼 Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance. 🔼 Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales. 🔼 Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales. 🔼 Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance. 🔼 Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance. 🔼 Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance. 🔼 Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance. More on tables 🔽 EVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT CONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12. 🔽 EVALUATION RESULTS ON COCO CAPTION USING THE KARPATHY TEST SPLIT WITH LLAMA-13B AS THE LANGUAGE MODEL. #T. = TRAINABLE PARAMETERS. *PEFT METHODS. 🔽 EVALUATION RESULTS ON THE MME BENCHMARK WITH LLAMA-13B AS THE LANGUAGE MODEL. MME-C AND MME-P MEASURE THE PERCEPTION AND COGNITION ABILITIES OF THE MODEL, RESPECTIVELY. EXTRA TOKENS REFER TO THE NUMBER OF ADDITIONAL TOKENS PROCESSED BY THE LLM BEYOND THE STANDARD TEXT TOKENS. #T. = TRAINABLE PARAMETERS. *PEFT METHODS. 🔽 Comparison among different VL models on more image understanding tasks. * Baseline results evaluated through our implementation using the official checkpoint. 🔽 TABLE V TRAINING AND INFERENCE SPEED OF DIFFERENT APPROACHES. MEMORY-SAVING OR SPEED-UP APPROACHES SUCH AS CHECKPOINTING AND FLA SHATTENTION ARE NOT ADOPTED. FLOPS ARE ESTIMATED FOR GENERATING A SINGLE NEW TOKEN WITH A TEXT SEQUENCE LENGTH OF 256. EXPERIMENTS ON COCO CAPTIONING AND INSTRUCTION-FOLLOWING WERE NOT IMPLEMENTED IN THE ORIGINAL PAPERS OF LLAVA-LORA AND MEMVP, SO THE OVERALL TRAINING TIME FOR THESE TASKS IS UNAVAILABLE. 🔽 EVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT CONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12. 🔽 EVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT CONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12. 🔽 EVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT CONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12. 🔽 EVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT CONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12. 🔽 EVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT CONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12. ","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.17779/","section":"Posts","summary":"ADEM-VL is a novel vision-language tuning framework that achieves high efficiency by using a parameter-free cross-attention mechanism, multiscale visual features, and adaptive fusion.  It outperforms \u0026hellip;..","title":"ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning","type":"posts"},{"content":" TL;DR # This paper proposes asynchronous off-policy RLHF, separating LLM generation and training to enable concurrent processing. It demonstrates that Online DPO is robust to off-policy data, allowing for efficient training. Experiments on LLMs from 410M to 8B parameters show significant speedups (up to 40%) while maintaining performance, highlighting the scalability and efficiency of this approach for training large language models. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper is important because it introduces a novel asynchronous off-policy approach to Reinforcement Learning from Human Feedback (RLHF), significantly accelerating the training process of large language models (LLMs) while maintaining performance. This is a crucial development as the computational cost of RLHF is currently a major bottleneck in LLM development, hindering the creation and refinement of increasingly sophisticated models. The findings offer valuable insights for researchers aiming to reduce computational expense and improve efficiency in the field of LLM training and alignment.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 6: Asynchronous RLHF can be training-bound (left) or generation-bound (right). In practice, generation and training speeds differ so a challenge of asynchronous learning is how best to balance usage and leverage idle compute time to further improve training. 🔽 Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training Model Win Rate KL (Perplexity) SFT 410m 25.36% 1.075 SFT 1B 26.82% 1.071 SFT 2.8B 35.16% 1.068 More on tables 🔽 Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training 🔽 Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training 🔽 Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training 🔽 Table 6: The trained models\u0026#39; GPT4-0 win rate against the human-written responses on the test split of the No Robots dataset (Rajani et al., 2023) 🔽 Table 6: The trained models’ GPT-4 win rate against the human-written responses on the test split of the No Robots dataset (Rajani et al., 2023) ","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18252/","section":"Posts","summary":"This paper proposes asynchronous off-policy RLHF, separating LLM generation and training to enable concurrent processing.  It demonstrates that Online DPO is robust to off-policy data, allowing for ef\u0026hellip;..","title":"Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models","type":"posts"},{"content":" TL;DR # This paper proposes a novel multi-draft speculative sampling method for faster LLM decoding. It introduces a two-step optimal token selection architecture (importance sampling and single-draft speculative sampling), offering theoretical analysis and demonstrating significant improvements in block efficiency and token rates, particularly when draft sequences have non-identical distributions. This method improves decoding speed and efficiency for LLMs. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper introduces multi-draft speculative sampling, improving large language model (LLM) decoding efficiency. It provides a canonical two-step architecture for optimal token selection and theoretical analysis demonstrating improvements over existing methods.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Optimal Approach for Multi-Draft Speculative Sampling 🔽 Table 1: Block efficiency achieved in the Dolly task for different number of draft models. Scheme K = 2 K = 3 K = 4 K = 5 K = 6 IS 2.13 土 0.05 2.22 士 0.05 2.26 土 0.05 2.27 士 0.05 2.28 士 0.06 SpecInfer 1.76 士 0.04 1.86 士 0.05 1.95 土 0.05 2.00 士 0.04 2.04 士 0.05 SpecTr 1.77 土 0.04 1.89 土 0.05 1.96 土 0.05 2.03 士 0.06 2.08 土 0.04 More on figures 🔼 Optimal Approach for Multi-Draft Speculative Sampling 🔼 Figure 2: Numerical evaluation of Pr(accept) for the optimal scheme (Theorem 3) as well as two baseline schemes – SpecTr (Sun et al., 2024b) and SpecInfer (Miao et al., 2024). For sake of illustration we select alphabet Ω = {1,2,3} and p = [1/3,1/3, 1/3]. The left plot sets q = [1/3, q2, 2/3-q2] while the right plot sets q = [1/6, q2, 5/6 - q2] where q2 is varied on the x-axis. More on tables 🔽 Table 2: Effect of LP Truncation and Alphabet Truncation 🔽 Comparison of average acceptance probability across different tasks for K = 2, 4, 8 drafts. 🔽 Table 4: Block Efficiency achieved in the Dolly Task with top-k sampling 🔽 Comparison of average acceptance probability across different tasks for K = 2, 4, 8 drafts. 🔽 Comparison of average acceptance probability across different tasks for K = 2, 4, 8 drafts. 🔽 Table 7: ROUGE-L scores on the XSum task across various decoders and sampling temperatures. 🔽 Table 8: BLEU scores on the WMT dataset across various decoders and sampling temperatures. ","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18234/","section":"Posts","summary":"This paper proposes a novel multi-draft speculative sampling method for faster LLM decoding. It introduces a two-step optimal token selection architecture (importance sampling and single-draft specula\u0026hellip;..","title":"Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits","type":"posts"},{"content":" TL;DR # To address attention concentration in deep Transformers, this paper proposes ResFormer, which uses residual connections from the first layer\u0026rsquo;s values, and SVFormer, which shares value embeddings across all layers. Both models improve training efficiency and downstream performance compared to vanilla Transformers and other related methods. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper introduces ResFormer and SVFormer, two novel Transformer architectures designed to mitigate attention concentration, a phenomenon where attention increasingly focuses on fewer tokens as the network deepens. ResFormer adds a residual connection from the first layer\u0026rsquo;s values to subsequent layers, approximating cross-layer attention without high computational costs. SVFormer further improves efficiency by sharing the same value embeddings across all layers. Experiments show both models improve training efficiency and downstream task performance compared to standard Transformers and related methods.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Simplified illustration of the vanilla Transformer, NeuTRENO, DenseFormer, ResFormer, and SVFormer, with only three-layer structures and no operations other than attention. A², Vi, and H² denote the attention matrix, value vectors, and attention outputs at the i-th layer, respectively. ⊕, −, and ⊗ represent standard matrix addition, subtraction, and multiplication, respectively. 🔽 Table 1: Zero-shot accuracy on commonsense reasoning tasks. Model Max Length HellaSwag Obqa WinoGrande ARC-c ARC-e PIQA Avg. Transformer 2,048 0.263 0.142 0.492 0.199 0.331 0.572 0.333 ResFormer 2,048 0.273 0.148 0.512 0.182 0.414 0.604 0.355 Transformer 64,000 0.267 0.142 0.485 0.179 0.322 0.570 0.328 ResFormer 64,000 0.274 0.136 0.513 0.184 0.407 0.588 0.350 More on tables 🔽 Table 2: The details of pre-train dataset. 🔽 Table 5: Validation loss on slimpajama. 🔽 Table 4: Training details for models with different size. 🔽 Table 5: Validation loss on slimpajama. ","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.17897/","section":"Posts","summary":"To address attention concentration in deep Transformers, this paper proposes ResFormer, which uses residual connections from the first layer\u0026rsquo;s values, and SVFormer, which shares value embeddings acros\u0026hellip;..","title":"Value Residual Learning For Alleviating Attention Concentration In Transformers","type":"posts"},{"content":" TL;DR # ZIP-FIT is a novel data selection method that uses gzip compression to efficiently select task-relevant data for fine-tuning LLMs. It outperforms existing methods by achieving faster convergence and lower cross-entropy loss, demonstrating the importance of data quality and task alignment for efficient LLM training. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper introduces ZIP-FIT, a novel data selection method for fine-tuning large language models (LLMs). ZIP-FIT leverages gzip compression to measure the alignment between potential training data and the target task distribution, enabling more efficient selection of task-relevant data. Experiments on Autoformalization and code generation demonstrate that ZIP-FIT significantly outperforms existing methods, achieving faster convergence and lower cross-entropy loss, even with smaller datasets. The findings highlight the importance of data quality and task alignment for efficient LLM fine-tuning.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: ZIP-FIT selects task-specific data for efficient finetuning. (0) Obtain both the source and target datasets. (1) Calculate ZIP-FIT Alignment of each source example with the target dataset using gzip compression. (2) Rank all source examples based on these alignment scores. (3) Select the top-K most aligned examples for fine-tuning. (4) Fine-tune a large language model using the selected top-K examples to improve performance on the target task. 🔽 Table 1: Beginning characters of the top 20 samples selected by ZIP-FIT when the target task is code generation. Sample Text (Beginning) Alignment Score Across all his bands and projects, Townsend has released twenty @-@ three studio albums and three live albums. 0.5000 Require Import CodeDeps. Require Import Ident. Local Open Scope Z_scope. Definition _addr := 1%positive. Definition -g := 2%positive. 0.4928 This Photostock Vector Night Sky Background With Full Moon Clouds And Stars Vector Ilgraphicration has 1560 x 1560 pixel resolution\u0026hellip; 0.4926 module Structure.Logic where · 0.4926 { dg-do compile } PR fortran/51993 Code contributed by Sebastien Bardeau module mymod type :: mytyp\u0026hellip; 0.4891 For over ten years, the St. Louis Mercy home has formed a special connection with a local community theatre: The Muny. This summer the\u0026hellip; 0.4889 Read(\u0026ldquo;SchreierSims.gi\u0026rdquo;); LoadPackage(\u0026ldquo;AtlasRep\u0026rdquo;\u0026quot;); MicroSeconds := function() local t; t := IO_gettimeofday(); return t.tv _sec * 1000000 + t.t 0.4889 Get the keyId used by this peer (this peer\u0026rsquo;s identifier). This is stored in the key store. 0.4857 Initializes and adds a node to the graph. NOTE: At least the type must be supplied for the Node to exist in the graph. Args: graph: The graph\u0026hellip; 0.4853 def bgra2rgb(img): cv2.cvtColor(img, cv2.COLOR _BGRA2BGR) has an issue removing the alpha channel, this gets rid of wrong trans\u0026hellip; 0.4853 ","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18194/","section":"Posts","summary":"ZIP-FIT is a novel data selection method that uses gzip compression to efficiently select task-relevant data for fine-tuning LLMs.  It outperforms existing methods by achieving faster convergence and \u0026hellip;..","title":"ZIP-FIT: Embedding-Free Data Selection via Compression-Based Alignment","type":"posts"},{"content":"","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/2024-10-22/","section":"Tags","summary":"","title":"2024-10-22","type":"tags"},{"content":" TL;DR # Inf-CL breaks the memory barrier in contrastive learning by using a tile-based computation strategy and a multi-level tiling strategy for distributed training. It allows for near-infinite batch sizes, dramatically reducing memory costs and achieving a two-order of magnitude improvement over the state-of-the-art while maintaining accuracy and comparable training speed. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper introduces Inf-CL, a novel method for training contrastive loss models with near-infinite batch sizes. It addresses the memory bottleneck inherent in contrastive learning by using a tile-based computation strategy that avoids the full materialization of the similarity matrix. This approach, combined with a multi-level tiling strategy, allows for scaling batch sizes to unprecedented levels without sacrificing accuracy, making it a significant advancement for large-scale contrastive learning.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 2: (a) Vanilla implementation of contrastive loss gathers features to all devices to calculate all similarity simultaneously, where the similarity with squared complexity are repeatedly stored in all devices, causing huge memory costs for loss calculation when batch size increases. (b) Our Inf-CL significant decreases the memory cost by serial and distributed tile-wise computation. 🔽 Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer. Model Loss (Peak) Memory Cost (GB) Loss (Peak) Memory Cost (GB) Loss (Peak) Memory Cost (GB) Loss (Peak) Memory Cost (GB) Loss (Peak) Memory Cost (GB) Model 32k 64k 128k 256k 1024k 8xA800 (U 8 X 80GB) 8xA800 (U 8 X 80GB) 8xA800 (U 8 X 80GB) 8xA800 (U 8 X 80GB) 8xA800 (U 8 X 80GB) 8xA800 (U 8 X 80GB) CLIP 16.67 (46.40) 66.11 (77.94) X X X OpenCLIP 2.27 (43.97) 8.63 (46.38) 33.64 (51.23) X X Inf-CL 0.18 (44.20) 0.36 (46.63) 0.72 (51.46) 1.45 (61.13) X Inf-CL* 0.18 (42.40) 0.36 (42.49) 0.72 (42.69) 1.45 (43.07) 6.53 (45.40) 32xA800 (U 32x80GB) 32xA800 (U 32x80GB) 32xA800 (U 32x80GB) 32xA800 (U 32x80GB) 32xA800 (U 32x80GB) 32xA800 (U 32x80GB) CLIP 16.66 (42.85) 66.11 (75.52) X X X OpenCLIP 0.71 (42.46) 2.45 (43.06) 8.98 (44.26) 34.35 (46.71) X Inf-CL 0.05 (42.48) 0.09 (43.08) 0.18 (44.30) 0.35 (46.71) 1.44 (61.20) More on figures 🔼 Figure 2: (a) Vanilla implementation of contrastive loss gathers features to all devices to calculate all similarity simultaneously, where the similarity with squared complexity are repeatedly stored in all devices, causing huge memory costs for loss calculation when batch size increases. (b) Our Inf-CL significant decreases the memory cost by serial and distributed tile-wise computation. 🔼 Figure 1: GPU memory usage comparison between Inf-CL and previous methods (CLIP, Open-CLIP). The dashed line marks the common GPU memory limit. Memory costs exceeding the bottleneck of 80G A800 are estimated by curve fitting. Left: With 8×A800, CLIP and OpenCLIP\u0026rsquo;s memory consumption increases quadratically, while Inf-CL achieves linear growth, reducing memory costs by 78× at a batch size of 256k. Right: At a batch size of 1024k, even with 128 GPUs, previous methods exceed memory limits, whereas Inf-CL reduces memory demand by 281×. 🔼 Figure 3: Multi-level tiling strategy. Top: for cross-GPU tiling, each GPU is assigned with multiple rows. The computation and the column-wise communication are performed asynchronously to reduce the cost. Bottom: for in-GPU tiling, the calculations in each GPU are further divided into tiles and the row-wise calculation is distributed to multiple CUDA cores. The accumulative operations of each row are merged into one kernel for reducing I/O times between SRAM and HBM. More on tables 🔽 Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer. 🔽 Training Memory Cost Across Different Hardware and Batch Sizes. ","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.17243/","section":"Posts","summary":"Inf-CL breaks the memory barrier in contrastive learning by using a tile-based computation strategy and a multi-level tiling strategy for distributed training.  It allows for near-infinite batch sizes\u0026hellip;..","title":"Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss","type":"posts"},{"content":"","date":"21 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/2024-10-21/","section":"Tags","summary":"","title":"2024-10-21","type":"tags"},{"content":" TL;DR # Large Language Models (LLMs) often hallucinate; knowledge editing aims to fix this without retraining. This paper introduces HalluEditBench, a new benchmark dataset that rigorously tests editing methods by first ensuring the LLM generates a hallucination. HalluEditBench evaluates methods across five dimensions (Efficacy, Generalization, Portability, Locality, Robustness), revealing that performance varies greatly depending on the method, domain, and LLM, with parameter-preserving methods generally outperforming others, but still having limitations. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper is important because it introduces HalluEditBench, a new benchmark dataset for evaluating knowledge editing methods in LLMs. Existing datasets don\u0026rsquo;t ensure LLMs produce hallucinations before editing, making it hard to assess editing methods\u0026rsquo; effectiveness. HalluEditBench addresses this by rigorously constructing a large hallucination dataset and evaluating methods across five dimensions.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Framework of HalluEditBench. For real-world hallucinations, we holistically assess the performance of knowledge editing on Efficacy, Generalization, Portability, Locality, and Robustness. 🔽 Table 1: Performance measured by Accuracy (%) of Llama2-7B before editing (“Pre-edit”) and after applying typical knowledge editing methods (“Post-edit”) on common existing evaluation datasets. Method WikiDatarecent ZsRE WikiBio Pre-edit 47.40 37.49 61.35 Post-edit (ROME) 97.37 96.86 95.91 Post-edit (MEMIT) 97.10 95.86 94.68 Post-edit (FT-L) 56.30 53.82 66.70 Post-edit (FT-M) 100.00 99.98 100.00 Post-edit (LoRA) 100.00 100.00 100.00 ","date":"21 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.16251/","section":"Posts","summary":"Large Language Models (LLMs) often hallucinate; knowledge editing aims to fix this without retraining.  This paper introduces HalluEditBench, a new benchmark dataset that rigorously tests editing meth\u0026hellip;..","title":"Can Knowledge Editing Really Correct Hallucinations?","type":"posts"},{"content":" TL;DR # Large language models (LLMs) surprisingly don\u0026rsquo;t utilize partial products for arithmetic, instead operating as symbolic learners. They solve arithmetic problems by decomposing them into manageable subgroups, selecting easier patterns first. The difficulty of these sub-tasks is linked to their complexity and label space entropy. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper investigates how large language models (LLMs) learn arithmetic. The authors find that LLMs don\u0026rsquo;t use partial products, but instead learn arithmetically in a purely symbolic way by breaking tasks into subgroups, suggesting that LLMs are symbol-level learners. This challenges previous assumptions about how LLMs perform arithmetic and offers insights into their learning dynamics.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Fundamental structure of the paper. We begin by investigating partial products and proceed to a detailed examination at the subgroup level to understand the mechanism in a symbolic manner. 🔽 Inductive and deductive accuracy difference Δ. Gemma-2-2B Gemma-2-2B Gemma-2-2B Gemma-2-2B Llama-3.1-8B Llama-3.1-8B Llama-3.1-8B Llama-3.1-8B Standard Lattice Repetitive Egyptian Standard Lattice Repetitive Egyptian Task → Partial P. +4.1% +6.8% -29.0% +3.6% +40.6% +40.8% -59.0% +29.6% Partial P. → Task -6.1% -10.7% -20.3% -9.6% -3.7% -0.2% -0.9% -2.7% More on tables 🔽 Table 2: Diagnostic sets with four calculation methods. 🔽 Table 3: Label space statistics with different rule perturbations. H(L) represents the entropy of the label space, and |L| is the size of the label space. {C}i=1 represents all positions in output digits. 🔽 Table 4: Test Accuracy difference Δ on perturbed addition and multiplication. 🔽 Table 5: Label space statistics with different format perturbations. H(L) represents the entropy of the space, and |L| is the size of the space. {Cj}=1 represents all possible output digits. 🔽 Table 6: Test Accuracy difference Δ on perturbed addition and multiplication. 🔽 Table 1: Inductive and deductive accuracy difference Δ. ","date":"21 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.15580/","section":"Posts","summary":"Large language models (LLMs) surprisingly don\u0026rsquo;t utilize partial products for arithmetic, instead operating as symbolic learners. They solve arithmetic problems by decomposing them into manageable subg\u0026hellip;..","title":"Language Models are Symbolic Learners in Arithmetic","type":"posts"},{"content":" TL;DR # Pantograph is a new Lean 4 tool improving the machine-learning assisted theorem proving process by offering an advanced interface that supports efficient proof search, high-level reasoning, and data extraction. It overcomes limitations of existing interfaces, enabling more powerful search algorithms and facilitating the implementation of novel theorem proving techniques like Draft-Sketch-Proof. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # Pantograph is a novel API and REPL for Lean 4 that provides a versatile interface for training and evaluating theorem-proving agents, enabling efficient proof search and high-level reasoning. It addresses challenges in existing interfaces by supporting advanced reasoning steps, essential data extraction tasks, and handling metavariable coupling, paving the way for more advanced machine learning models in theorem proving.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Fig. 1: A proof tree for Expression (1) 🔽 Table 1: LLM parameters for DSP Experiment Parameter Value Max tokens 2048 Top P 0.95 Temperature 0.8 More on figures 🔼 Fig. 2: System architecture of Pantograph. A solid arrow indicates that the component at the arrow source calls functions in the component that is the arrow\u0026rsquo;s target. A human operator interacts with Lean 4\u0026rsquo;s kernel via the IDE, but a machine learning agent can interact via one of Pantograph\u0026rsquo;s interfaces. 🔼 Fig. 3: Call hierarchy in Pantograph during the execution of a normal tactic. The text on the right indicates the Lean 4 monad each function runs in. 🔼 Fig. 4: 2 becomes dormant after a tactic is applied to 1. It must be brought back into scope with goal.continue before the proof can finish. The ellipses (\u0026hellip;) are plalceholders for some combination of tactics which eventually solves the descendant of (1 🔼 Fig. 5: In this diagram, rectangular boxes are proof states, and circles are goals. Each proof state has 0 or more goals. A state with no goals is considered solved. If all descendant goals of a state become solved, the state itself becomes solved. ","date":"21 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.16429/","section":"Posts","summary":"Pantograph is a new Lean 4 tool improving the machine-learning assisted theorem proving process by offering an advanced interface that supports efficient proof search, high-level reasoning, and data e\u0026hellip;..","title":"Pantograph: A Machine-to-Machine Interaction Interface for Advanced Theorem Proving, High Level Reasoning, and Data Extraction in Lean 4","type":"posts"},{"content":" TL;DR # SPARE, a novel training-free method, leverages sparse autoencoders to control LLMs\u0026rsquo; knowledge selection behavior during inference, efficiently resolving knowledge conflicts between parametric and contextual information. Outperforming existing techniques in open-domain QA tasks, SPARE offers an efficient and transparent solution for improving LLM accuracy and reliability. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper introduces SPARE, a novel representation engineering method that uses sparse autoencoders to steer the knowledge selection behavior of LLMs in the presence of knowledge conflicts. Unlike existing methods, SPARE operates at inference time, efficiently controlling which knowledge source (parametric or contextual) is prioritized without retraining. Experiments on ODQA tasks with knowledge conflicts demonstrate that SPARE significantly surpasses existing representation engineering methods and contrastive decoding methods, showcasing improved accuracy in resolving knowledge conflicts.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 In the event of a knowledge conflict, the model can rely on the context or on the parametric knowledge. The figure presents the predictions of Llama2-7B steered by SPARE. 🔽 Overall performance of steering the utilisation of parametric and contextual knowledge, measured by EMM and EMC. \u0026#39;Without Controlling\u0026#39; indicates the baseline that we do not use any controlling methods to steer the generation. #ICL is not an inference-time controlling strategy, which controls the behaviours by changing demonstrations. CAD needs additional forwarding for contrastive decoding. Metric Method NQSwap (Longpre et al., 2021) NQSwap (Longpre et al., 2021) NQSwap (Longpre et al., 2021) Macnoise (Hong et al., 2024) Macnoise (Hong et al., 2024) Macnoise (Hong et al., 2024) Metric Method Llama3-8B Llama2-7B Gemma-2-9B Llama3-8B Llama2-7B Gemma-2-9B Steer to Use Parametric Knowledge Steer to Use Parametric Knowledge Steer to Use Parametric Knowledge Steer to Use Parametric Knowledge Steer to Use Parametric Knowledge Steer to Use Parametric Knowledge Steer to Use Parametric Knowledge Steer to Use Parametric Knowledge Without Controlling 26.63±6.02 22.23±4.75 26.32±1.80 18.96±2.65 22.37±1.89 17.06±3.79 EMM TaskVec (Hendel et al., 2023) 24.16±6.58 24.88±0.85 29.85±0.83 21.23±1.89 22.93±2.31 28.92±1.19 EMM ActAdd (Turner et al., 2023a) 37.87 ±8.96 31.43±3.68 27.67 ±0.82 26.17 ±0.22 27.52±3.07 29.75±1.68 EMM SEAlinear (Qiu et al., 2024) 21.03±1.83 23.73±0.86 24.43±0.91 12.84±0.18 15.64±0.24 28.10±2.78 EMM SEAsqExp (Qiu et al., 2024) 13.64±1.62 16.66±0.55 23.79±1.38 14.24±1.45 16.24±1.06 28.07±1.30 EMM DoLa (Chuang et al., 2024) 25.53±5.19 16.50±3.91 20.58±1.06 16.52±2.65 15.66±0.88 19.81±2.58 EMM ♭CAD (Shi et al., 2024) 33.72±0.84 31.23±1.45 41.17 ±0.59 28.58±0.75 30.81±0.94 33.15 ±2.87 EMM #ICL (Brown, 2020) 43.73 士1.55 31.67. 士5.49 43.10 士3.63 29.54+4.16 31.23 ±0.94 21.91±2.35 EMM SPARE (Ours) 47.51±1.30 43.76±3.14 44.11±1.30 30.72±1.42 35.43±1.10 35.53±2.07 Steer to Use Contextual Knowledge Steer to Use Contextual Knowledge Steer to Use Contextual Knowledge Steer to Use Contextual Knowledge Steer to Use Contextual Knowledge Steer to Use Contextual Knowledge Steer to Use Contextual Knowledge Steer to Use Contextual Knowledge Without Controlling 42.69±8.40 41.67 士4.66 45.96±2.48 69.36±3.57 62.38±3.05 59.25±2.82 EMC TaskVec (Hendel et al., 2023) 41.88±9.45 38.25±1.23 45.52±1.06 88.47±1.93 86.91±0.44 59.25±1.49 EMC ActAdd (Turner et al., 2023a) 51.91±8.03 47.48±3.93 46.90±1.89 73.01±1.58 69.64±0.20 59.66±2.89 EMC SEAlinear (Qiu et al., 2024) 43.61±10.3 47.73±0.43 52.95±1.90 69.78±0.97 67.32±0.28 60.31±2.25 EMC SEAsqExp (Qiu et al., 2024) 57.08±2.92 48.04±0.45 61.45±0.54 72.04±1.60 68.20±1.10 61.45±0.30 EMC DoLa (Chuang et al., 2024) 44.29±8.46 33.54±3.38 15.90±10.1 68.45±3.83 50.95±5.15 23.34±10.5 EMC ♭CAD (Shi et al., 2024) 65.65±5.50 54.69±3.25 63.10±2.32 78.69±3.85 70.07±3.77 64.12+4.44 EMC #ICL (Brown, 2020) 73.35 ±3.82 63.33 ±3.50 70.19 ±2.51 51.75±5.60 47.51±1.86 47.24±3.81 EMC SPARE (Ours) 77.69 ±1.24 69.32±1.26 73.78±0.74 92.24±0.49 87.30±1.96 87.96±1.85 ","date":"21 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.15999/","section":"Posts","summary":"SPARE, a novel training-free method, leverages sparse autoencoders to control LLMs\u0026rsquo; knowledge selection behavior during inference, efficiently resolving knowledge conflicts between parametric and cont\u0026hellip;..","title":"Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering","type":"posts"},{"content":"","date":"13 June 2022","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","externalUrl":null,"permalink":"/ai-paper-reviewer/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/ai-paper-reviewer/series/","section":"Series","summary":"","title":"Series","type":"series"}]