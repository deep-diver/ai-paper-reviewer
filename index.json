[{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-24-10-23/","section":"Tags","summary":"","title":"🔖 24-10-23","type":"tags"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/ai-generated/","section":"Categories","summary":"","title":"AI Generated","type":"categories"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/","section":"AI Paper Reviews by AI","summary":"","title":"AI Paper Reviews by AI","type":"page"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":" TL;DR # DynamicCity is a new method for creating realistic 4D LiDAR (light detection and ranging) scenes, which are essentially videos of 3D point cloud data. Existing methods often struggle with generating large-scale, dynamic scenes, focusing on static or single-frame outputs. DynamicCity overcomes this limitation by using a two-stage process. First, it employs a Variational Autoencoder (VAE) to learn a compact 4D representation called HexPlane. The VAE uses a novel \u0026lsquo;Projection Module\u0026rsquo; to compress the high-dimensional LiDAR data efficiently and an \u0026lsquo;Expansion \u0026amp; Squeeze Strategy\u0026rsquo; to improve reconstruction quality and speed. Second, it uses a Diffusion Transformer (DiT) to generate the HexPlane, making it possible to incorporate diverse conditions such as trajectories or commands to control the scene generation. The model is trained on large-scale datasets like CarlaSC and Occ3D-Waymo. Experimental results demonstrate DynamicCity outperforms state-of-the-art methods, significantly improving generation quality, speed, and memory efficiency. The researchers also showcase the versatility of DynamicCity through various downstream applications, like inpainting damaged scenes or generating scenes based on user-defined layouts. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does it matter? # DynamicCity generates high-quality, large-scale 4D LiDAR scenes from dynamic environments, enabling diverse downstream applications.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: Dynamic LiDAR scene generation from DynamicCity. We introduce a new LiDAR generation model that generates diverse 4D scenes of large spatial scales (80 × 80 × 6.4 meter³) and long sequential modeling (up to 128 frames), enabling a diverse set of downstream applications. For more examples, kindly refer to our Project Page: https://dynamic-city.github.io. The figure illustrates DynamicCity\u0026rsquo;s capability to generate large-scale, diverse 4D LiDAR scenes from various generation methods, including command-driven, trajectory-guided, dynamic object generation, scene inpainting, and layout-conditioned generation.\n🔽 Table 1: Comparisons of 4D Scene Reconstruction. We report the mIoU scores of OccSora (Wang et al., 2024) and our DynamicCity framework on the CarlaSC, Occ3D-Waymo, and Occ3D-nuScenes datasets, respectively, under different resolutions and sequence lengths. Symbol † denotes score reported in the OccSora paper. Other scores are reproduced using the official code. Dataset#ClassesResolution#FramesOccSora (Wang et al., 2024)Ours (DynamicCity)CarlaSC (Wilson et al., 2022)10128x 128 x8441.01%79.61% (+38.6%)10128x 128 x8839.91%76.18% (+36.3%)10128x 128 x81633.40%74.22% (+40.8%)10128x 128 x83228.91%59.31% (+30.4%)Occ3D-Waymo (Tian et al., 2023)9200x200x161636.38%68.18% (+31.8%)Occ3D-nuScenes (Tian et al., 2023)11200x200x 161613.70%56.93% (+43.2%)11200x200 x 163213.51%42.60% (+29.1%)17200x200x 163213.41%40.79% (+27.3%)17200x200x 163227.40%†40.79% (+13.4%) Table 1 compares the 4D scene reconstruction performance of the proposed DynamicCity model against OccSora across three datasets with varying resolutions and sequence lengths, showing significant improvements in mIoU.\nMore visual insights # More on figures 🔼 Figure 2: Pipeline of dynamic LiDAR scene generation. Our DynamicCity framework consists of two key procedures: (a) Encoding HexPlane with an VAE architecture (cf. Sec. 4.1), and (b) 4D Scene Generation with HexPlane DiT (cf. Sec. 4.2). The figure illustrates the two-stage pipeline of DynamicCity, showing how it encodes LiDAR scenes into HexPlanes using a VAE and then generates 4D scenes from these HexPlanes using a DiT.\n🔼 Figure 3: VAE for Encoding 4D LIDAR Scenes. We use HexPlane H as the 4D representation. fo and go are convolution-based networks with downsampling and upsampling operations, respectively. h(.) denotes the projection network based on transformer modules. This figure illustrates the VAE architecture used in DynamicCity for encoding 4D LiDAR scenes into compact HexPlane representations.\n🔼 Figure 2: Pipeline of dynamic LiDAR scene generation. Our DynamicCity framework consists of two key procedures: (a) Encoding HexPlane with an VAE architecture (cf. Sec. 4.1), and (b) 4D Scene Generation with HexPlane DiT (cf. Sec. 4.2). The figure illustrates the two-stage pipeline of DynamicCity for dynamic LiDAR scene generation, which includes encoding the scene with a VAE and generating the scene with a DiT.\n🔼 Figure 5: Condition Injection for DiT The figure illustrates how numeric and image conditions are injected into the DiT for conditional generation of HexPlanes.\n🔼 Figure 6: Dynamic Scene Generation Results. We provide unconditional generation scenes from the 1st, 8th, and 16th frames on Occ3D-Waymo (Left) and CarlaSC (Right), respectively. Kindly refer to the Appendix for complete sequential scenes and longer temporal modeling examples. The figure shows example results of unconditional 4D LiDAR scene generation from DynamicCity on Occ3D-Waymo and CarlaSC datasets.\n🔼 Figure 7: Dynamic Scene Generation Applications. We demonstrate the capability of our model on a diverse set of downstream tasks. We show the 1st, 8th, and 16th frames for simplicity. Kindly refer to the Appendix for complete sequential scenes and longer temporal modeling examples. Figure 7 shows various downstream applications of DynamicCity, including command-driven, layout-conditioned, trajectory-guided scene generation, and dynamic object inpainting.\n🔼 Figure 8: Unconditional Dynamic Scene Generation Results. We provide qualitative examples of a total of 16 consectutive frames generated by DynamicCity on the Occ3D-Waymo (Tian et al., 2023) dataset. Best viewed in colors and zoomed-in for additional details. Figure 8 shows 16 consecutive frames of an unconditonally generated dynamic scene from the Occ3D-Waymo dataset, demonstrating the model\u0026rsquo;s ability to generate realistic and detailed dynamic scenes.\n🔼 Figure 8: Unconditional Dynamic Scene Generation Results. We provide qualitative examples of a total of 16 consectutive frames generated by DynamicCity on the Occ3D-Waymo (Tian et al., 2023) dataset. Best viewed in colors and zoomed-in for additional details. Figure 8 shows 16 consecutive frames of an unconditonally generated dynamic scene from the Occ3D-Waymo dataset, showcasing the model\u0026rsquo;s ability to generate realistic and detailed dynamic scenes.\n🔼 Figure 10: HexPlane-Guided Generation Results. We provide qualitative examples of a total of 64 consecutive frames generated by DynamicCity on the Occ3D-Waymo (Tian et al., 2023) dataset. Best viewed in colors and zoomed-in for additional details. Figure 10 shows 64 frames of consecutive scenes generated by the HexPlane-guided generation method of DynamicCity, showcasing its ability to generate long sequences.\n🔼 Figure 1: Dynamic LiDAR scene generation from DynamicCity. We introduce a new LiDAR generation model that generates diverse 4D scenes of large spatial scales (80 × 80 × 6.4 meter³) and long sequential modeling (up to 128 frames), enabling a diverse set of downstream applications. For more examples, kindly refer to our Project Page: https://dynamic-city.github.io. Figure 1 shows the DynamicCity framework generating diverse large-scale 4D LiDAR scenes from various driving scenarios, demonstrating command-driven, trajectory-guided, dynamic object generation and layout-conditioned generation.\n🔼 Figure 12: Command-Guided Scene Generation Results. We provide qualitative examples of a total of 16 consectutive frames generated under the command RIGHT by DynamicCity on the CarlaSC (Wilson et al., 2022) dataset. Best viewed in colors and zoomed-in for additional details. This figure shows 16 frames of a scene generated by DynamicCity, illustrating the effect of a \u0026lsquo;Turn Right\u0026rsquo; command on the scene\u0026rsquo;s evolution.\n🔼 Figure 1: Dynamic LiDAR scene generation from DynamicCity. We introduce a new LiDAR generation model that generates diverse 4D scenes of large spatial scales (80 × 80 × 6.4 meter³) and long sequential modeling (up to 128 frames), enabling a diverse set of downstream applications. For more examples, kindly refer to our Project Page: https://dynamic-city.github.io. The figure shows an overview of the DynamicCity framework, illustrating the generation of large-scale and long-sequential 4D LiDAR scenes from various input conditions.\n🔼 Figure 14: Dynamic Inpainting Results. We provide qualitative examples of a total of 16 consecutive frames generated by DynamicCity on the CarlaSC (Wilson et al., 2022) dataset. Best viewed in colors and zoomed-in for additional details. The figure shows the qualitative results of dynamic inpainting performed by the DynamicCity model on the CarlaSC dataset, showcasing the model\u0026rsquo;s ability to seamlessly regenerate inpainted regions while maintaining consistency in the surrounding areas.\n🔼 Figure 6: Dynamic Scene Generation Results. We provide unconditional generation scenes from the 1st, 8th, and 16th frames on Occ3D-Waymo (Left) and CarlaSC (Right), respectively. Kindly refer to the Appendix for complete sequential scenes and longer temporal modeling examples. Figure 6 shows the unconditional generation results of DynamicCity on Occ3D-Waymo and CarlaSC datasets, showcasing the model\u0026rsquo;s ability to generate realistic and detailed dynamic scenes.\nMore on tables 🔽 Table 2: Comparisons of 4D Scene Generation. We report the Inception Score (IS), Fréchet Inception Distance (FID), Kernel Inception Distance (KID), and the Precision (P) and Recall (R) rates of SemCity (Lee et al., 2024), OccSora (Wang et al., 2024), and our DynamicCity framework on the CarlaSC and Occ3D-Waymo datasets, respectively, in both the 2D and 3D spaces. DatasetMethod#FramesMetric2DMetric⌀DIS ↑FID⌀ ↓KID2D ↓P↑R↑IS ↑FID- ↓KID3D↓P↑R⌀ ↑CarlaSC (Wilson et al., 2022)OccSora Ours162.49225.080.0130.1150.0082.257155952.720.3800.1512.49810.950.0020.2380.0662.331354.219.100.4600.170Occ3D-Waymo (Tian et al., 2023)OccSora Ours161.92682.430.0940.2270.0143.129314012.200.3840.0011.9457.1380.0030.6170.0963.206180677.710.4940.026 Table 2 compares the performance of three different methods, SemCity, OccSora, and DynamicCity, on 4D scene generation using Inception Score, Fréchet Inception Distance, Kernel Inception Distance, Precision, and Recall in both 2D and 3D spaces.\n🔽 Table 3: Ablation Study on VAE Network Structures. We report the mIoU scores, training time (second-per-iteration), and training-time memory consumption (VRAM) of different Encoder and Decoder configurations on CarlaSC and Occ3D-Waymo, respectively. Note that \u0026#39;ESS\u0026#39; denotes \u0026#39;Expansion \u0026amp; Squeeze\u0026#39;. The best and second-best values are in bold and underlined. EncoderDecoderCarlaSCOcc3D-WaymomIoU↑Time (s)↓VRAM (G)↓mIoU↑Time (s)↓VRAM (G)↓Average Pooling Average PoolingQuery60.97%0.23612.4649.37%1.56369.66ESS68.02%0.1434.2755.72%0.75820.31Projection ProjectionQuery68.73%0.29213.5961.93%2.12873.15ESS74.22%0.2055.9262.57%1.31625.92 Table 3 shows the ablation study of VAE network structures by reporting mIoU scores, training time, and VRAM consumption on CarlaSC and Occ3D-Waymo datasets.\n🔽 Table 4: Ablation Study on HexPlane Downsampling (D.S.) Rates. We report the compression ratios (C.R.), mIoU scores, training speed (seconds per iteration), and training-time memory consumption on CarlaSC and Occ3D-Waymo. The best and second-best values are in bold and underlined. D.S. RatesCarlaSCOcc3D-WaymodTdxdydzC.R.↑mIoU↑Time (s)↓VRAM (G)↓C.R.↑mIoU↑Time (s)↓VRAM (G)↓11115.78%84.67%1.14921.63Out-of-Memory\u0026gt;80122117.96%76.05%0.2898.4938.42%63.30%1.85232.82222223.14%74.22%0.2055.9248.25%62.37%0.93524.9244271.86%65.15%0.1994.00153.69%58.13%0.87722.30 Table 4 shows the impact of different downsampling rates on HexPlane compression, reconstruction accuracy, training speed, and memory usage.\n🔽 Table 5: Ablation Study on Organizing HexPlane as Image Tokens. We report the Inception Score (IS), Fréchet Inception Distance (FID), Kernel Inception Distance (KID), and the Precision (P) and Recall (R) rates on CarlaSC. The best values are highlighted in bold. MethodMetric2DMetric3DIS2D ↑FID2D ↓KID2D ↓P↑R↑IS ⌀ ↑FID 3D ↓KID3D ↓P↑R↑Direct Unfold2.496205.00.2480.0000.0002.2699110723.70.1730.043Vertical Concatenation2.47612.790.0030.1910.0422.305623.226.670.4240.159Padded Rollout2.49810.960.0020.2380.0662.331354.219.100.4600.170 Table 5 presents an ablation study comparing different methods of organizing HexPlane as image tokens for 4D LiDAR generation, evaluating their performance using Inception Score, Fréchet Inception Distance, Kernel Inception Distance, Precision, and Recall metrics.\n🔽 Table 1: Comparisons of 4D Scene Reconstruction. We report the mIoU scores of OccSora (Wang et al., 2024) and our DynamicCity framework on the CarlaSC, Occ3D-Waymo, and Occ3D-nuScenes datasets, respectively, under different resolutions and sequence lengths. Symbol † denotes score reported in the OccSora paper. Other scores are reproduced using the official code. ClassCarlaSCOcc3D-WaymoOcc3D-nuScenesBuildingBuildingBuildingManmadeBarrierBarrier, Wall, Guardrail-BarrierOtherOther, Sky, Bridge, Rail track, Static, Dynamic, WaterGeneral ObjectGeneral ObjectPedestrianPedestrianPedestrianPedestrianPolePole, Traffic sign, Traffic lightSign, Traffic light, Pole, Construction ConeTraffic coneRoadRoad, RoadlinesRoadDrivable surfaceGroundGround, Terrain-Other flat, TerrainSidewalkSidewalkSidewalkSidewalkVegetationVegetationVegetation, Tree trunkVegetationVehicleVehicleVehicleBus, Car, Construction vehicle, Trailer, TruckBicycle-Bicyclist, Bicycle, MotorcycleBicycle, Motorcycle Table 1 compares the mean Intersection over Union (mIoU) scores of DynamicCity and OccSora on three datasets at different resolutions and sequence lengths for 4D scene reconstruction.\n🔽 Table 1: Comparisons of 4D Scene Reconstruction. We report the mIoU scores of OccSora (Wang et al., 2024) and our DynamicCity framework on the CarlaSC, Occ3D-Waymo, and Occ3D-nuScenes datasets, respectively, under different resolutions and sequence lengths. Symbol † denotes score reported in the OccSora paper. Other scores are reproduced using the official code. MethodmloUBuilding Barrier Other PedestrianPole Road GroundSidewalk Vegetation VehicleResolution: 128 X 128 X 8 Sequence Length: 4OccSora Ours41.009 79.60438.86110.6166.63719.19121.82593.91061.35786.67115.68555.34076.36431.35468.89893.43687.96298.61787.01495.12968.70088.569Improv.38.59537.50320.73862.26174.24566.1374.70725.6578.45853.01533.229Resolution: 128 X 128 X 8 Sequence Length: 8OccSora Ours39.910 76.18133.0013.2605.65919.22419.35793.03857.33585.55130.89951.77670.87450.02552.43387.95885.86697.51383.07493.94458.62681.498Improv.36.27137.87346.76546.77468.73466.5094.47525.7398.39327.72729.722Resolution: 128 X 128 X 8 Sequence Length: 16OccSora Ours33.404 74.22319.2642.2053.45411.7819.16592.05450.07782.59418.07845.36366.85251.90149.84479.41082.36996.93784.48494.08258.21778.134Improv.40.819I 47.58849.69646.39067.62973.2044.88334.40711.48840.13932.771Resolution: 128 X 128 X 8 Sequence Length: 32OccSora Ours28.911 59.30816.5651.4130.9446.2004.15091.46643.39978.61411.00735.35352.03625.52129.38256.81157.87694.79278.39089.95546.08062.234Improv.30.39735.47124.10828.43850.61153.7263.32634.99111.34135.07326.881 Table 1 compares the 4D scene reconstruction performance of the proposed DynamicCity model against OccSora across different datasets, resolutions, and sequence lengths, showing mIoU scores.\nFull paper # ","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.18084/","section":"Posts","summary":"DynamicCity: A novel 4D LiDAR generation framework producing large-scale, high-quality LiDAR scenes capturing dynamic environments\u0026rsquo; temporal evolution.","title":"DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes","type":"paper-reviews"},{"content":" TL;DR # This paper introduces LiMAC, a novel mobile phone control system. Unlike traditional approaches that rely on large and computationally expensive foundation models, LiMAC uses a lightweight architecture combining a small Action Transformer (AcT) and a fine-tuned vision-language model (VLM). AcT handles simple actions efficiently, while the VLM takes over for complex tasks involving natural language. Evaluated on two datasets, LiMAC demonstrates superior performance compared to models using only fine-tuned open-source VLMs or prompt engineering with closed-source models like GPT-4. LiMAC achieves up to a 19% increase in action accuracy over fine-tuned VLMs and a 42% increase over prompt-engineering baselines. It also boasts significantly faster execution times, up to 30 times faster, reaching speeds of around 3 seconds per task. The researchers also introduce a contrastive learning objective to improve the accuracy of click actions. The results show that LiMAC efficiently and accurately controls mobile apps, addressing the limitations of previous methods in terms of speed, accuracy, and computational resources. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does it matter? # This research paper introduces LiMAC, a lightweight mobile phone control architecture using a hybrid approach of a small action transformer and a fine-tuned vision-language model to efficiently control Android apps with high accuracy and speed.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: Illustration of AcT. A separate encoding of each UI element into a vector et,i by using pretrained embedding models. The embeddings are then fed into the sequence of a transformer Xt along with the previous timesteps in that episode. The prediction of the transformer is decoded to produce the next action which consists of atype and aspec. The figure illustrates the Action Transformer (AcT) architecture, showing how UI element embeddings are processed to predict the next action.\n🔼 Figure 3: Confusion matrix for action type selection for LiMAC in AndroidControl. The chart visualizes the performance of LiMAC\u0026rsquo;s action type prediction model by showing the counts of correctly and incorrectly predicted action types across different categories.\n🔽 Table 1: Comparison of models in terms of average inference time and overall accuracy on the AitW and AndroidControl datasets. The table presents the size of each model, the average inference time (in seconds, lower is better), and the overall accuracy (higher is better) for both datasets. ModelSize ↓Avg Inf. (s)↓ TimeOverall ↑AitWAndCtrlSeeActchoiceunk9.8137.729.9SeeActannunk9.7642.535.5T3Aunk4.8726.953.1M3Aunk10.6435.657.5Florence2820M0.5070.857.0LiMAC with Florence2 (ours)+520M0.3472.263.1Qwen2-VL2B3.0351.052.2LiMAC with Qwen2-VL (ours)+520M0.6370.962.5 Table 1 compares various models\u0026rsquo; average inference time and overall accuracy on two datasets, showing LiMAC\u0026rsquo;s improved performance.\nMore visual insights # More on figures 🔼 Figure 2: The architecture of LiMAC. The history of observations-actions {ot, at-1, Ot-1..} and goal g are processed to vector x and passed to AcT. The image observation omg with the bounding boxes and the goal g are passed as inputs to the VLM. The VLM is only called if an action that requires text completion is selected, based on the action type output of AcT. The action is finally selected based on the protocol described in Section 3. The figure illustrates the architecture of LiMAC, showing how the model processes observations and goals to generate actions, utilizing both AcT and a VLM.\n🔼 Figure 4: Relaxed target element in yellow (timestep 3) and failed action in red (final timestep). The target element of the click in timestep 3 is considered correct under our relaxed accuracy because its bounding box is almost identical to the correct element, and clicking either would have the same effect (opening the text bar). In the final timestep, the agent inputs text \u0026lsquo;Detroit\u0026rsquo; rather than \u0026lsquo;Las Vegas\u0026rsquo;, a clear confusion between the origin and destination of the trip stated in the goal, leading to an incorrect prediction. This figure shows a sample episode from the AndroidControl dataset, highlighting a case where a relaxed target element is considered correct and another where an incorrect action is predicted.\n🔼 Figure 5: Relaxed input-text in yellow (timestep 4) and overall successful episode. Timestep 4 is considered correct under our relaxed input-text textual component because it is simply the singular form of the correct text, leading to a Jaccard index greater than 0.5 and presumably the same search results. The episode terminates successfully, with all timesteps being considered correct under our evaluation metrics. This figure shows a successful episode of app control, highlighting a case where a slightly inaccurate text input was still considered correct due to the relaxed evaluation metric.\nMore on tables 🔽 Table 1: Comparison of models in terms of average inference time and overall accuracy on the AitW and AndroidControl datasets. The table presents the size of each model, the average inference time (in seconds, lower is better), and the overall accuracy (higher is better) for both datasets. FrameworkModules UsedAvg Inf. ⓢ+ TimeOverall↑TypeClickTextAitWAndCtrlT3A onlyT3AT3AT3A4.8726.953.1LiMAC (ours)AcTT3AT3A4.0342.765.4LiMAC (ours)AcTAcTT3A1.0469.863.2M3A onlyM3AM3AM3A10.6435.657.5LiMAC (ours)AcTM3AM3A8.4052.666.8LiMAC (ours)AcTAcTM3A1.8770.062.5Florence onlyFlorence2Florence2Florence20.5070.857.0LiMAC (ours)AcTFlorence2Florence20.7271.661.1LiMAC (ours)AcTAcTFlorence20.3472.263.1Qwen onlyQwen2-VLQwen2-VLQwen2-VL3.0351.052.2LiMAC (ours)AcTQwen2-VLQwen2-VL2.6455.759.1LiMAC (ours)AcTAcTQwen2-VL0.6370.962.5LiMAC (ours)AcTM3AT3A7.5752.467.4 Table 1 compares different models\u0026rsquo; average inference time and overall accuracy on two datasets, showing LiMAC\u0026rsquo;s superior performance.\n🔽 Table 3: Action-type, click-target, and text accuracies across module combinations on the AitW and AndroidControl datasets. LiMAC achieves the best action-type accuracy in both datasets and the best click-target accuracy in AitW, while our fine-tuned Florence2 excels at text prediction. FrameworkModules UsedAction TypeClick TargetTextTypeClickTextAitWAndCtrlAitWAndCtrlAitWAndCtrlSeeAct onlySeeActchoiceSeeActchoiceSeeActchoice67.166.836.948.569.467.1SeeAct onlySeeActannSeeActannSeeActann68.266.844.755.766.061.8T3A onlyT3AT3AT3A56.267.733.571.166.578.4M3A onlyM3AM3AM3A63.869.848.377.167.374.3Qwen onlyQwen2-VLQwen2-VLQwen2-VL81.770.753.255.270.575.7LiMAC (ours)AcTQwen2-VLQwen2-VL86.982.353.255.270.575.7LiMAC (ours)AcTAcTQwen2-VL86.982.377.465.470.575.7Florence onlyFlorence2Florence2Florence286.479.676.262.084.277.5LiMAC (ours)AcTFlorence2Florence286.982.376.262.084.277.5LiMAC (ours)AcTAcTFlorence286.982.377.465.484.277.5 The table compares different model configurations using various combinations of modules on two datasets, showing their performance in terms of action-type, click-target, and text accuracies.\n🔽 Table 4: Evaluation of three ablated versions of LiMAC using different types of input, on AndroidControl. For actions that require text completion, we use the fine-tuned Florence2. SizeAction TypeClick TargetOverallLiMAC520M82.365.463.1LiMAC (no CLIP FT)520M81.962.360.0LiMAC (no img)433M82.454.956.0LiMAC (no txt)410M83.265.763.0 Table 4 presents the performance comparison of three ablated versions of LiMAC model using different input types on AndroidControl dataset, showing the impact of different components on the model\u0026rsquo;s performance.\n🔽 Table 1: Comparison of models in terms of average inference time and overall accuracy on the AitW and AndroidControl datasets. The table presents the size of each model, the average inference time (in seconds, lower is better), and the overall accuracy (higher is better) for both datasets. Modules UsedAction TypeClick TargetTextTotalTypeClickTextAiTWAndCtrAiTWAndCtrAiTWAndCtrAiTWAndCtrAcTAcTFlorence286.982.377.465.484.277.572.263.1AcTFlorence2Florence286.982.376.262.084.277.571.661.1AcTAcTQwen2-VL86.982.377.465.470.575.770.962.5AcTQwen2-VLQwen2-VL86.982.353.255.270.575.755.759.1AcTAcTT3A85.381.777.665.466.578.469.863.2AcTT3AT3A85.381.733.571.166.578.442.765.4AcTM3AT3A85.381.748.377.166.578.452.467.4AcTAcTM3A85.381.777.665.467.374.370.062.5AcTT3AM3A85.381.733.571.167.374.343.064.7AcTM3AM3A85.381.748.377.167.374.352.666.8AcTAcTSeeActchoice85.381.777.665.469.467.170.562.0AcTSeeActchoiceSeeActchoice85.381.736.948.569.467.145.753.7AcTAcTSeeActann85.381.777.665.466.061.870.061.1AcTSeeActannSeeActann85.381.744.755.766.061.849.261.6Florence2Florence2Florence286.479.676.262.084.277.570.857.0Qwen2-VLQwen2-VLQwen2-VL81.770.753.255.270.575.751.052.2T3AT3AT3A56.267.733.571.166.578.426.953.1T3AM3AT3A56.267.748.377.166.578.430.955.2M3AT3AT3A63.869.833.571.166.578.427.053.5M3AM3AT3A63.869.848.377.166.578.435.857.7SeeActchoiceSeeActchoiceSeeActchoice67.166.836.948.569.467.129.538.9SeeActannSeeActannSeeActann68.266.844.755.766.061.834.345.7 Table 1 compares the performance of different models on two mobile phone control datasets in terms of model size, average inference time, and overall accuracy.\nFull paper # ","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.17883/","section":"Posts","summary":"LiMAC: Lightweight neural app control achieving high accuracy and speed via a hybrid action transformer and fine-tuned vision-language model for efficient Android app control.","title":"Lightweight Neural App Control","type":"paper-reviews"},{"content":" TL;DR # This paper introduces MIA-DPO, a novel approach to improve Large Vision-Language Models\u0026rsquo; (LVLMs) understanding of multi-image contexts. Current LVLMs struggle with multi-image data due to limited training data and annotation costs. MIA-DPO tackles this by cleverly augmenting existing single-image datasets with additional, unrelated images. This creates a more diverse training set without the need for extensive new annotations. The method further leverages the LVLMs\u0026rsquo; internal attention mechanisms. By analyzing attention patterns, it identifies and filters out less reliable responses where the model focused on the wrong parts of the image, thus creating a better selection of training pairs. Experiments show MIA-DPO significantly improves performance on five multi-image benchmarks and surprisingly has minimal negative impact on single-image tasks. In short, MIA-DPO offers a cost-effective and accurate solution for training LVLMs to better handle real-world multi-image scenarios. It uses readily-available data and the model\u0026rsquo;s own internal attention mechanisms to improve accuracy. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does it matter? # MIA-DPO enhances Large Vision-Language Models (LVLMs) to better understand and align with human preferences in multi-image scenarios by using attention mechanisms to filter out noisy responses during training, resulting in improved accuracy on multiple benchmarks.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: (a) Overview of MIA-DPO. We transform single-image data (e.g., LLaVA 665k) into multi-image data by adding noisy or unrelated images and using language descriptions to specify the target image. Attention values are then used to detect hallucinations in multi-image contexts, filtering out rejected data for DPO optimization. (b) Benchmark Results. MIA-DPO excels across five multi-image benchmarks while maintaining competitive performance on seven single-image benchmarks, demonstrating its robustness in both single and multi-image tasks. The figure shows an overview of the MIA-DPO framework and its performance on single-image and multi-image benchmarks.\n🔼 Figure 5: Attention Ratio Statistic. We analyze the attention ratios distribution for different image counts across various data types, and use dashed lines to indicate the thresholds for each data set. The chart displays the distribution of attention ratios for different numbers of images across three multi-image data types (Sequence, Grid Collage, Pic-in-Pic).\n🔽 Table 1: Main results on multi-image benchmarks. We compare our MIA-DPO along with other DPO algorithms across five multi-image benchmarks. Our method brings significant performance improvements to both the classic LLaVa-v1.5 and the recent InternLM-XC2.5. In contrast, other single-image DPO methods perform poorly on multi-image benchmarks. ModelsParameterMMMUBLINKMantisNLVR2MVBenchAverageGPT-4V (Achiam et al., 2023)-56.851.162.788.843.560.6LLaVA-v1.6 (Li et al., 2024b)7B35.839.645.658.940.944.2Qwen-VL-Chat (Bai et al., 2023)7B35.931.239.258.742.241.4VideoLLaVA (Lin et al., 2023)7B-38.935.956.544.3-Fuyu (Bavishi et al., 2023)8B27.936.627.251.130.234.6Idefics2 (Lauren�on et al., 2024b)8B43.045.248.986.929.750.7InstructBLIP (Dai et al., 2023)13B30.642.245.660.332.542.2CogVLM (Wang et al., 2023)17B32.141.545.258.637.342.9Emu2-Chat (Sun et al., 2024)37B36.336.237.858.239.741.6LLaVA-v1.5 (Liu et al., 2024a)7B35.137.141.952.136.040.4+ LLaVA-RLHF (Sun et al., 2023)7B34.640.830.451.838.039.1+ HA-DPO (Zhao et al., 2023)7B35.838.634.651.640.640.2+ POVID (Zhou et al., 2024)7B35.219.937.821.439.430.7+ MIA-DPO (Ours)7B36.342.944.254.239.543.4△-+1.2+5.8+2.3+2.1+3.5+3.0InternLM-XC2.5 (Zhang et al., 2024)7B41.446.949.370.759.553.6+ MIA-DPO (Ours)7B42.647.760.475.263.657.9△-+1.2+0.811.1+4.54.1+4.3 Table 1 compares MIA-DPO with other direct preference optimization methods across five multi-image benchmarks, showing its performance improvements on LLaVa-v1.5 and InternLM-XC2.5 compared to single-image methods.\nMore visual insights # More on figures 🔼 Figure 2: Examples of Multi-Image Hallucinations. Top: Sequence Confusion that the model is confused about the order in which the images should be referenced. Bottom: Element Interference. The model incorrectly identified the attributes due to visual element interference across different images. Attention values illustrate how the model\u0026rsquo;s focus was dispersed across different images, resulting in the hallucination response. The figure shows two examples of multi-image hallucinations: sequence confusion and element interference, illustrating how attention values reveal the model\u0026rsquo;s focus and contribute to incorrect responses.\n🔼 Figure 3: MIA-DPO Framework. We extend the single-image dataset to multi-image datasets by inserting irrelevant images and using attention values to filter out the hallucination responses for rejected samples of the DPO algorithm. The figure illustrates the MIA-DPO framework, detailing the process of extending single-image data to multi-image data, using attention-based filtering to remove hallucinations, and applying the DPO algorithm.\n🔼 Figure 4: Multi-Images DPO Data Format. To address multi-image hallucinations mentioned in Fig. 2, we construct our multi-image prompts in three formats: (a) Sequence. (b) Grid Collage. (c) Pic-in-Pic. The figure shows three different ways to format data for multi-image prompts in MIA-DPO: sequence, grid collage, and pic-in-pic.\n🔼 Figure 2: Examples of Multi-Image Hallucinations. Top: Sequence Confusion that the model is confused about the order in which the images should be referenced. Bottom: Element Interference. The model incorrectly identified the attributes due to visual element interference across different images. Attention values illustrate how the model\u0026rsquo;s focus was dispersed across different images, resulting in the hallucination response. The figure shows two examples of multi-image hallucinations: sequence confusion and element interference, illustrating how attention values reveal the model\u0026rsquo;s focus and contribute to hallucination responses.\n🔼 Figure 3: MIA-DPO Framework. We extend the single-image dataset to multi-image datasets by inserting irrelevant images and using attention values to filter out the hallucination responses for rejected samples of the DPO algorithm. The figure illustrates the MIA-DPO framework, showing how single-image data is augmented with unrelated images, attention is used to filter out hallucinations, and DPO is applied to create a stronger model.\n🔼 Figure 6: Attention Difference Before and After DPO. We present the attention distribution in the intermediate layers for the original LLaVA-v1.5 (top row), MIA-DPO + LLaVA-v1.5 (second row), and the difference value (bottom row), respectively. The figure visualizes the attention distribution changes in LLaVA-v1.5 before and after applying MIA-DPO, highlighting how MIA-DPO refines the model\u0026rsquo;s focus on relevant image regions.\n🔼 Figure 1: (a) Overview of MIA-DPO. We transform single-image data (e.g., LLaVA 665k) into multi-image data by adding noisy or unrelated images and using language descriptions to specify the target image. Attention values are then used to detect hallucinations in multi-image contexts, filtering out rejected data for DPO optimization. (b) Benchmark Results. MIA-DPO excels across five multi-image benchmarks while maintaining competitive performance on seven single-image benchmarks, demonstrating its robustness in both single and multi-image tasks. This figure shows an overview of the MIA-DPO framework and its performance improvements on multi-image and single-image benchmarks.\n🔼 Figure 1: (a) Overview of MIA-DPO. We transform single-image data (e.g., LLaVA 665k) into multi-image data by adding noisy or unrelated images and using language descriptions to specify the target image. Attention values are then used to detect hallucinations in multi-image contexts, filtering out rejected data for DPO optimization. (b) Benchmark Results. MIA-DPO excels across five multi-image benchmarks while maintaining competitive performance on seven single-image benchmarks, demonstrating its robustness in both single and multi-image tasks. The figure shows an overview of the MIA-DPO framework and its performance on single and multi-image benchmarks.\n🔼 Figure 1: (a) Overview of MIA-DPO. We transform single-image data (e.g., LLaVA 665k) into multi-image data by adding noisy or unrelated images and using language descriptions to specify the target image. Attention values are then used to detect hallucinations in multi-image contexts, filtering out rejected data for DPO optimization. (b) Benchmark Results. MIA-DPO excels across five multi-image benchmarks while maintaining competitive performance on seven single-image benchmarks, demonstrating its robustness in both single and multi-image tasks. The figure shows an overview of the MIA-DPO framework and its performance on multiple single and multi-image benchmarks.\n🔼 Figure 2: Examples of Multi-Image Hallucinations. Top: Sequence Confusion that the model is confused about the order in which the images should be referenced. Bottom: Element Interference. The model incorrectly identified the attributes due to visual element interference across different images. Attention values illustrate how the model\u0026rsquo;s focus was dispersed across different images, resulting in the hallucination response. The figure shows two examples of multi-image hallucinations (sequence confusion and element interference) and uses attention values to illustrate how the model\u0026rsquo;s focus was dispersed across different images, resulting in the hallucination response.\n🔼 Figure 3: MIA-DPO Framework. We extend the single-image dataset to multi-image datasets by inserting irrelevant images and using attention values to filter out the hallucination responses for rejected samples of the DPO algorithm. The figure illustrates the MIA-DPO framework, showing how single-image data is extended to multi-image data, attention is used to filter out hallucination responses, and DPO is applied to improve the model\u0026rsquo;s understanding of multi-image contexts.\nMore on tables 🔽 Table 2: Main results on single-image benchmarks. We compare MIA-DPO with other DPO approaches across seven single-image benchmarks. MIA-DPO, which not only enhances multi-image performance but also maintains strong proficiency in single-image tasks. ModelsParameterMMStarSQAMMVetPOPEMMBMathAI2DAverageLLaVA-v1.6 (Li et al., 2024b)7B37.687.540.270.369.831.567.057.7Qwen-VL-Chat (Bai et al., 2023)7B34.568.847.374.961.815.563.052.3Idefics2 (Lauren�on et al., 2024b)8B49.588.734.086.275.751.472.365.4OpenFlamingo (Awadalla et al., 2023b)9B36.944.823.252.632.418.631.734.3InstructBLIP (Dai et al., 2023)13B32.754.133.186.138.324.440.644.2CogVLM (Wang et al., 2023)17B39.966.254.588.065.835.063.358.9Emu2-Chat (Sun et al., 2024)37B40.768.231.088.063.430.749.753.1LLaVA-v1.5 (Liu et al., 2024a)7B32.966.630.585.964.325.455.551.6+ LLaVA-RLHF Sun et al. (2023)7B31.664.027.880.860.123.547.948.0+ HA-DPO (Zhao et al., 2023)7B33.567.329.184.364.925.853.951.3+ POVID (Zhou et al., 2024)7B36.268.831.886.364.924.455.252.5+ MIA-DPO (ours)7B32.967.632.187.263.124.454.751.7InternLM-XC2.5 (Zhang et al., 2024)7B59.796.348.787.981.963.381.574.2+ MIA-DPO (ours)7B61.196.246.786.980.461.781.673.5 Table 2 compares MIA-DPO\u0026rsquo;s performance on seven single-image benchmarks against other direct preference optimization methods, showing its effectiveness while maintaining single-image capabilities.\n🔽 Table 3: Ablation Studies. The top row refers to the LLaVa-v1.5 baseline. We conduct experiments about the impact of without (w/o) and with (w) post-selection techniques and dpo data types. 35.137.141.952.136.040.41w/o post sel.35.338.744.253.739.442.32W post sel.36.342.944.254.239.543.43sequence37.339.544.251.740.142.64grid collage37.140.444.251.039.442.45pic-in-pic37.940.841.953.239.842.7 Table 3 shows the ablation study results comparing MIA-DPO with and without post-selection and using different types of data for DPO, demonstrating the impact of each component on performance.\n🔽 Table 1: Main results on multi-image benchmarks. We compare our MIA-DPO along with other DPO algorithms across five multi-image benchmarks. Our method brings significant performance improvements to both the classic LLaVa-v1.5 and the recent InternLM-XC2.5. In contrast, other single-image DPO methods perform poorly on multi-image benchmarks. #MMMUBLINKMantisNLVR2MVBenchAverage35.137.141.952.136.040.41�=0.135.941.346.153.239.943.32y=0.237.139.242.451.839.442.03�=0.335.839.842.952.039.742.04epoch=135.941.346.153.239.943.35epoch=237.038.545.252.039.642.56epoch=336.342.944.254.239.543.4 Table 1 presents a comparison of MIA-DPO and other DPO algorithms across five multi-image benchmarks, showing MIA-DPO\u0026rsquo;s significant performance improvements over existing methods.\n🔽 Table 5: Ablation Studies. The top row refers to the LLaVa-v1.5 baseline. We conducted an ablation study using GPT-40-mini for data selection. #MMMUBLINKMantisNLVR2MVBenchAverage35.137.141.952.136.040.41GPT-Selection36.341.742.953.039.542.72MIA-DPO36.342.944.254.239.543.430.0+1.2+1.3+1.20.0+0.7 The table compares the performance of GPT-Selection and MIA-DPO on five multi-image benchmarks, showing the effectiveness of MIA-DPO in improving the model\u0026rsquo;s performance.\n🔽 Table 1: Main results on multi-image benchmarks. We compare our MIA-DPO along with other DPO algorithms across five multi-image benchmarks. Our method brings significant performance improvements to both the classic LLaVa-v1.5 and the recent InternLM-XC2.5. In contrast, other single-image DPO methods perform poorly on multi-image benchmarks. ModelsParameterRelease TimeSourceGPT-4V (Achiam et al., 2023)-2023-09Source Link: OpenAIKosmos2 (Peng et al., 2023)1.6B2023-06Source Link: Kosmos2VideoLLaVA (Lin et al., 2023)7B2023-11Source Link: Video-LLaVaFuyu (Bavishi et al., 2023)8B2023-10Source Link: Fuyu-8BVILA (Lin et al., 2024)8B2023-12Source Link: VILAOtter-Image (Li et al., 2023a)9B2023-05Source Link: OtterIdefics1 (Lauren�on et al., 2024a)9B2023-08Source Link: Idefices1BLIP-2 (Li et al., 2023b)13B2023-01Source Link: BLIP-2OpenFlamingo (Awadalla et al., 2023b)9B2023-08Source Link: OpenFlamingoInstructBLIP (Dai et al., 2023)13B2023-05Source Link: InstructBLIPQwen-VL-Chat (Bai et al., 2023)7B2023-8Source Link: Qwen-VL-ChatEmu2-Chat (Sun et al., 2024)37B2023-12Source Link: Emu2-ChatCogVLM (Wang et al., 2023)17B2023-10Source Link: CogVLMIdefics2 (Lauren�on et al., 2024b)8B2024-04Source Link: Idefices2LLaVA-v1.6 (Li et al., 2024b)7B2024-01Source Link: LLaVa-Next11LLaVA-v1.5 (Liu et al., 2024a)7B2023-10Source Link: LLaVa-v1.5InternLM-XC2.5 (Zhang et al., 2024)7B2024-07Source Link: InternLM-XC2d5 Table 1 compares the performance of MIA-DPO and other DPO algorithms across five multi-image benchmarks, highlighting MIA-DPO\u0026rsquo;s superior performance on both LLaVa-v1.5 and InternLM-XC2.5.\n🔽 Table 1: Main results on multi-image benchmarks. We compare our MIA-DPO along with other DPO algorithms across five multi-image benchmarks. Our method brings significant performance improvements to both the classic LLaVa-v1.5 and the recent InternLM-XC2.5. In contrast, other single-image DPO methods perform poorly on multi-image benchmarks. SettingModelsEvaluation MetricNumberSourceMulti-Images BenchmarkMMMU (Yue et al., 2024)Multiple Choice1,050MMMUBLINK (Fu et al., 2024)Multiple Choice3,807BLINKNLVR2 (Suhr et al., 2018)Multiple Choice6,967NLVR2Mantis-Eval (Jiang et al., 2024)Multiple Choice217Mantis-EvalMVBench (Li et al., 2024c)Multiple Choice4,000MVBenchSingle-Image BenchmarkMMStar (Chen et al., 2024a)Multiple Choice1,500MMStarSci-QA (Lu et al., 2022)Multiple Choice4,241ScienceQAMMVet (Yu et al., 2023)Subjective Questions218MM-VetPOPE (Li et al., 2023c)Yes/No9,000POPEMMB (Liu et al., 2023)Multiple Choice1,164MMBenchMath (Lu et al., 2023)Multiple Choice6,141Math VistaAI2D (Kembhavi et al., 2016)Multiple Choice3,090AI2D Table 1 compares the performance of MIA-DPO and other preference optimization methods across five multi-image benchmarks, showing MIA-DPO\u0026rsquo;s significant performance improvements.\n🔽 Table 8: DPO Data Statistic. We listed in the table the data volume used for DPO with LLaVa-v1.5 and InternLM-XC2d5, along with the proportion of each type of data. ModelsTotalSequenceGrid CollagePic-in-PicLLaVa-v1.5 (Liu et al., 2024a)28.9k15.1k9.3k4.5kInternLM-XC2d5 (Zhang et al., 2024)23.1k11.7k7.8k3.6k The table shows the amount of data used for direct preference optimization (DPO) in the MIA-DPO model, broken down by data type (Sequence, Grid Collage, Pic-in-Pic) for two different large vision language models (LLaVa-v1.5 and InternLM-XC2.5).\nFull paper # ","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.17637/","section":"Posts","summary":"MIA-DPO boosts Large Vision-Language Model accuracy on multi-image tasks by cleverly using attention mechanisms to improve training data, achieving significant performance gains.","title":"MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models","type":"paper-reviews"},{"content":" TL;DR # This research introduces a new, efficient way to train text-to-image AI models. Instead of relying on expensive human-labeled data, the researchers created a large synthetic dataset by using existing AI models to rank the quality of generated images. They then developed a new optimization technique (RankDPO) that works particularly well with this type of ranked data. The results show their approach improves both how well the AI follows instructions and the quality of the images it generates, all while being significantly cheaper and faster than traditional methods. This opens up exciting possibilities for researchers working on AI image generation, allowing them to improve AI models more quickly and at a lower cost. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does it matter? # The paper presents a scalable method for improving text-to-image models using synthetically generated preference data and a novel ranking-based optimization technique. This avoids the expensive and time-consuming process of manual data annotation, greatly advancing the efficiency and scalability of training.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 2: Overview of our two novel components: (A) Syn-Pic and (B) RankDPO. Left illustrates the pipeline to generate a synthetically ranked preference dataset. It starts by collecting prompts and generating images using the same prompt for different T2I models. Next, we calculate the overall preference score using Reward models (e.g., PickScore, ImageReward). Finally, we rank these images in the decreasing order of preference scores. Right: Given true preference rankings for generated images per prompt, we first obtain predicted ranking by current model checkpoint using scores si (see Eq. 5). In this instance, although the predicted ranking is inverse of the true rankings, the ranks (1, 4) obtains a larger penalty than the ranks (2, 3). This penalty is added to our ranking loss through DCG weights (see Eq. 6). Thus, by optimizing 0 with Ranking Loss (see Eq. 7), the updated model addresses the incorrect rankings (1,4). This procedure is repeated over the training process, where the rankings induced by the model aligns with the labelled preferences. This figure illustrates the pipeline for creating a synthetically labeled ranked preference dataset (Syn-Pic) and the ranking-based preference optimization method (RankDPO).\n🔼 Figure 3: Win rates of our approach compared to DPO-SDXL and SDXL on human evaluation. The bar chart displays the win rates of RankDPO, DPO-SDXL, and SDXL in a user preference study, showing RankDPO\u0026rsquo;s superior performance.\n🔽 Table 1: Quantitative Results on GenEval. RankDPO improves results on most categories, notably \u0026#39;two objects\u0026#39;, \u0026#39;counting\u0026#39;, and \u0026#39;color attribution\u0026#39; for SDXL and SD3-Medium. ModelMean ↑Single ↑Two ↑Counting ↑Colors ↑Position ↑Color Attribution ↑SD v2.10.500.980.510.440.850.070.17PixArt-�0.480.980.500.440.800.080.07PixArt-�0.530.990.650.460.820.120.12DALL-E 20.520.940.660.490.770.100.19DALL-E 30.670.960.870.470.830.430.45SDXL0.550.980.740.390.850.150.23SDXL (Ours)0.611.000.860.460.900.140.29SD3-Medium0.701.000.870.630.840.280.58SD3-Medium (Ours)0.741.000.900.720.870.310.66 Table 1 presents a quantitative comparison of different models\u0026rsquo; performance on the GenEval benchmark, highlighting the improvements achieved by RankDPO on several key attributes.\nMore visual insights # More on figures 🔼 Figure 4: Comparison among different preference optimization methods and RankDPO for SDXL. The results illustrate that we generate images with better prompt alignment and aesthetic quality. Figure 4 shows a qualitative comparison of images generated by different preference optimization methods for SDXL, highlighting the improved prompt alignment and aesthetic quality achieved by RankDPO.\n🔼 Figure 1: Our approach, trained on a synthetic preference dataset with a ranking objective in the preference optimization, improves prompt following and visual quality for SDXL (Podell et al., 2023) and SD3-Medium (Esser et al., 2024), without requiring any manual annotations. The figure shows a qualitative comparison of image generation results from different models (SDXL and SD3) before and after applying the proposed ranked preference optimization method.\n🔼 Figure 1: Our approach, trained on a synthetic preference dataset with a ranking objective in the preference optimization, improves prompt following and visual quality for SDXL (Podell et al., 2023) and SD3-Medium (Esser et al., 2024), without requiring any manual annotations. The figure shows a qualitative comparison of text-to-image generation results using different methods (SDXL, SD3, and the proposed method) for various prompts, highlighting the improvement in prompt following and image quality achieved by the proposed approach.\n🔼 Figure 1: Our approach, trained on a synthetic preference dataset with a ranking objective in the preference optimization, improves prompt following and visual quality for SDXL (Podell et al., 2023) and SD3-Medium (Esser et al., 2024), without requiring any manual annotations. The figure shows a comparison of image generation results between different models (SDXL and SD3) before and after applying the proposed method, highlighting improved prompt following and visual quality.\nMore on tables 🔽 Table 2: Quantitative Results on T2I-CompBench. RankDPO provides consistent improvements on all categories for both SDXL and SD3-Medium. ModelAttribute BindingObject RelationshipComplex↑Color ↑Shape↑Texture↑Spatial↑Non-Spatial↑SD1.437.6535.7641.5612.4630.7930.80PixArt-a68.8655.8270.4420.8231.7941.17DALL-E 257.5054.6463.7412.8330.4336.96SDXL58.7946.8752.9921.3131.1932.37SDXL (Ours)72.3356.9369.6724.5331.3345.47SD3-Medium81.3159.0675.9134.3031.1347.93SD3-Medium (Ours)83.2663.4578.7236.4931.2548.65 Table 2 presents a quantitative comparison of the performance of SDXL and SD3-Medium models, with and without RankDPO, across various attributes on the T2I-CompBench benchmark.\n🔽 Table 3: Quantitative results on DPG-Bench. DSG (Cho et al., 2024) and VQAScore (Lin et al., 2024) measure prompt following using VQA models while Q-Align (Wu et al., 2024a) measures visual quality using multimodal LLMs. Model NamePrompt AlignmentVisual QualityDSG ScoreVQA ScoreQ-Align ScoreSD1.563.18--SD2.168.09--Pixart-�71.11--Playgroundv274.54--DALL-E 383.50--SDXL74.6584.330.72DPO-SDXL76.7485.670.74MaPO-SDXL74.5384.540.80SPO-SDXL74.7384.710.82SDXL (Ours)79.2687.520.81SD3-Medium85.5490.580.67SD3-Medium (Ours)86.7890.990.68 Table 3 presents a quantitative comparison of different models\u0026rsquo; performance on the DPG-Bench benchmark, evaluating prompt alignment using DSG and VQAScore, and visual quality using Q-Align.\n🔽 Table 4: Effect of the preference labelling and data quality on the final model. Model NamePrompt AlignmentVisual QualityDSG ScoreVQA ScoreQ-Align ScoreSDXL74.6584.330.72DPO (Random Labelling)75.6684.420.74DPO (HPSv2)78.0486.220.83DPO (Pick-a-Picv2)76.7485.670.74DPO (5 Rewards)78.8486.270.81RankDPO (Only SDXL)78.4086.760.74RankDPO79.2687.520.81 Table 4 shows the impact of different preference labeling methods and data quality on the final model\u0026rsquo;s performance, measured by prompt alignment and visual quality scores.\n🔽 Table 3: Quantitative results on DPG-Bench. DSG (Cho et al., 2024) and VQAScore (Lin et al., 2024) measure prompt following using VQA models while Q-Align (Wu et al., 2024a) measures visual quality using multimodal LLMs. Model NamePrompt AlignmentVisual QualityDSG ScoreVQA ScoreQ-Align ScoreSDXL74.6584.330.72Supervised Fine-Tuning76.5685.450.78Weighted Fine-Tuning77.0285.550.79DPO78.8486.270.81DPO + Gain Weights79.1587.430.82RankDPO (Ours)79.2687.520.81 Table 3 presents a quantitative comparison of different methods on the DPG-Bench benchmark, showing prompt alignment and visual quality scores for various models.\n🔽 Table 1: Quantitative Results on GenEval. RankDPO improves results on most categories, notably \u0026#39;two objects\u0026#39;, \u0026#39;counting\u0026#39;, and \u0026#39;color attribution\u0026#39; for SDXL and SD3-Medium. Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason Baldridge, Mohit Bansal, Jordi Pont-Tuset, and Su Wang. Davidsonian scene graph: Improving reliability in fine- grained evaluation for text-image generation. In ICLR, 2024.Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. NIPS, 2017.Kevin Clark, Paul Vicol, Kevin Swersky, and David J Fleet. Directly fine-tuning diffusion models on differentiable rewards. In ICLR, 2024.Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. In ICLR, 2024.Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023.Fei Deng, Qifei Wang, Wei Wei, Matthias Grundmann, and Tingbo Hou. Prdp: Proximal reward difference prediction for large-scale reward finetuning of diffusion models. In CVPR, 2024.Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, and Ricky TQ Chen. Adjoint matching: Fine-tuning flow and diffusion generative models with memoryless stochastic optimal control. arXiv preprint arXiv:2409.08861, 2024.Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M�ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024.Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.Luca Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, and Zeynep Akata. Reno: Enhancing one-step text-to-image models through reward-based noise optimization. In NeurIPS, 2024.Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine- tuning text-to-image diffusion models. NeurIPS, 2023.Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. NeurIPS, 2023.Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. In NeurIPS, 2023.Shane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea L Thomaz. Policy shaping: Integrating human feedback with reinforcement learning. NIPS, 2013.Yi Gu, Zhendong Wang, Yueqin Yin, Yujia Xie, and Mingyuan Zhou. Diffusion-rpo: Aligning dif- fusion models through relative preference optimization. arXiv preprint arXiv:2406.06382, 2024.Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. In EMNLP, 2021.Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.Jiwoo Hong, Noah Lee, and James Thorne. Reference-free monolithic preference optimization with odds ratio. arXiv preprint arXiv:2403.07691, 2024a.Jiwoo Hong, Sayak Paul, Noah Lee, Kashif Rasul, James Thorne, and Jongheon Jeong. Margin- aware preference optimization for aligning diffusion models without reference. arXiv preprint arXiv:2406.06424, 2024b. Table 1 presents a quantitative comparison of different models\u0026rsquo; performance on the GenEval benchmark, highlighting the improvements achieved by RankDPO on various aspects of text-to-image generation.\n🔽 Table 6: Comparison of T2I-Compbench Dataset with DPG-Bench, including model attributes, training time, and inference time increases. DatasetColorShapeTextureSpatialNon-SpatialDPG ScoreTrain Time (A100 Days)Training DataSame Inference TimeSDXL58.7946.8752.9921.3131.1974.65ELLA (SDXL)72.6056.3466.8622.1430.6980.2311234MXRankDPO (SDXL)72.3356.9369.6724.5331.3379.2660.24M The table compares the performance of different models on the T2I-Compbench and DPG-bench datasets, showing model attributes, training time, training data size and inference time.\n🔽 Table 7: Comparing features of our proposal against baselines that aim to improve T2I model quality post-training. ELLA* also replaces the CLIP text-encoders with T5-XL text-encoder and a 470M parameter adapter applied at each timestep, thereby increasing the inference cost. MethodTraining ImagesA100 GPU daysEqual Inference CostDPG-Bench ScoreDPO1.0M3076.74MaPO1.0M2574.53SPO-5V74.73ELLA*34M112X80.23Ours0.24M6V79.26 Table 7 compares the proposed RankDPO method with other preference optimization methods in terms of training data, computational cost, and downstream performance on DPG-Bench.\n🔽 Table 3: Quantitative results on DPG-Bench. DSG (Cho et al., 2024) and VQAScore (Lin et al., 2024) measure prompt following using VQA models while Q-Align (Wu et al., 2024a) measures visual quality using multimodal LLMs. ItemPick-a-Picv2Syn-PicNumber of prompts58 00058 000Number of images1 025 015232 000Number of preferences959 000N/AImage generation costN/A$185.60Annotation/Labelling cost$47 950.00\u003c $20.00Total cost$47 950.00\u003c $205.60 Table 3 presents a quantitative comparison of different methods on the DPG-Bench benchmark, evaluating both prompt alignment and visual quality of generated images.\n🔽 Table 1: Quantitative Results on GenEval. RankDPO improves results on most categories, notably \u0026#39;two objects\u0026#39;, \u0026#39;counting\u0026#39;, and \u0026#39;color attribution\u0026#39; for SDXL and SD3-Medium. \u0026quot; colorful flowers\u0026hellip;\u0026quot;\u0026hellip;Mona Lisa\u0026hellip; brown\u0026quot; .. orange frisbee · ·muscular.. tiger.. \u0026ldquo;\u0026rdquo; majestic white \u0026hellip;word \u0026lsquo;peace\u0026rsquo; on the \u0026quot; lush green grass\u0026hellip;cowboy hat\u0026hellip; grips a silver microphone\u0026hellip; \u0026ldquo;Nearby a wooden cello.. \u0026ldquo;sleek red electric guitar\u0026hellip; \u0026ldquo;crane\u0026hellip; ambulance\u0026hellip; vibrant red crosses.. . \u0026ldquo; Table 1 presents a quantitative comparison of different models\u0026rsquo; performance on the GenEval benchmark, highlighting the improvements achieved by RankDPO.\n🔽 Table 1: Quantitative Results on GenEval. RankDPO improves results on most categories, notably \u0026#39;two objects\u0026#39;, \u0026#39;counting\u0026#39;, and \u0026#39;color attribution\u0026#39; for SDXL and SD3-Medium. Algorithm 1 DataGen: Generate Synthetically Labeled Ranked Preference Dataset (Syn-Pic)Input: N prompts (P = {ci}N=1), k T2I Models ({0i}(=1), n Reward Models ({Rv}\"=1) Output: Ranked Preference Dataset D Initialize: Synthetic dataset D = ⌀ for cin P do Generate k images x1 x2 , · · . , xk = 01(c), 02(c), . . · , 0k(c) , Initialize preference counts Ci = 0; VA E {1,. . . , k} for each reward model Ri⌀ do Compute scores Ri = Ri⌀ (xi , c); Vi E {1,. . , k} for each pair (i, j) with i ≠ j do if Ri \u003e Rij then Increment preference count Ci = Ci +1 Vi E {1, . · · , k} Compute probabilities ⌀(xi) = n.(ki-1) ; Store entry (c,x1, x2 , · . . , xk, ⌀(x1), ⌀(x2) , . . . , ⌀(xk ( ( ( ( ( ) in D return Ranked Preference Dataset DAlgorithm 2 RankDPO: Ranking-based Preference Optimization using Syn-PicInput: Ranked Preference Dataset D, Initial model ⌀init, Reference model Oref Input: Pre-defined signal-noise schedule {at, ot}�t=1 Hyper-parameters: # Optimization Steps (m), Learning Rate (7), Divergence Control B Initialize: 0 = ⌀init Output: Fine-tuned model ARankDPO for iter = 0 to m do Sample entry (c, x1 x2 , · · · , xk, ⌀(x1 ) , ⌀(x2), · , ⌀(xk ( ( ( ( ( ) ~ D , Sample timestep t ~ U(1, T), and noise E⌀ ~ N(0, I) Compute noisy image x2 = atxi + �t�i Compute model scores Si 스 s(xi , c,t, 0) = ||e⌀ - e⌀(xt, c)112 - ||�i - Eref(Xt, c)113 Determine ranking T by sorting images based on ⌀(x2) in descending order for each pair (i, j) with i \u003e j in T do Compute pairwise gains: Gij = 2⌀(xi) - 2⌀(xi ) Compute discount factors: D(T(i)) = log(1 + �(i)) and D(T(j)) = log(1 + �(j)) Compute pairwise DCG weights: △ij = |Gij| · D(T(i)) - D(T(j)) Compute pairwise loss: Lij = △inj log o (�� (s(xi, c,t,0) - s(x) c,t,01)) Sum pairwise losses: LRankDPO = - Ei\u003ej Lij Compute gradients graditer = V�LRankDPO Update model parameters: 0 = 0 - 7 · graditer Final ARankDPO = 0 return Fine-tuned model ARankDPOAlgorithm 3 Generate Syn-Pic and Train RankDPOInput: N prompts (P = {ci}N1), k T2I Models ({0i}i=1), n Reward Models ({Rv}:=1) Input: Initial model ⌀init, Reference model ⌀ref, Pre-defined signal-noise schedule {at, ot}}t=1 Hyper-parameters: # Optimization Steps (m), Learning Rate (7), Divergence Control B Output: Fine-tuned model ARankDPO // Generate Synthetically Labeled Ranked Preference dataset D using Algorithm 1 D = DataGen(P, {⌀i}k=1, {Ri⌀}n=1) // Train 0 using Algorithm 2 ARankDPO = RankDPO(D, ⌀init, ⌀ref, {⌀t, ot}t=1,m,7,B) Table 1 presents a quantitative comparison of different models\u0026rsquo; performance on the GenEval benchmark, highlighting the improvements achieved by RankDPO on several key categories.\nFull paper # ","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.18013/","section":"Posts","summary":"Scalable ranked preference optimization, using synthetic data, improves text-to-image generation significantly, surpassing human-labeled datasets in efficiency.","title":"Scalable Ranked Preference Optimization for Text-to-Image Generation","type":"paper-reviews"},{"content":" TL;DR # This paper tackles the challenge of scaling up diffusion language models (DLMs), which are promising alternatives to traditional autoregressive models but are expensive to train from scratch. The researchers propose a novel approach to adapt readily available, large, pre-trained autoregressive language models (AR LMs) into DLMs. They demonstrate the feasibility of their method by converting several open-source AR LMs (ranging from 127M to 7B parameters) into DLMs. The key innovation lies in bridging the differences in the objectives and architectures of AR LMs and DLMs. They achieve this by introducing a simple continual pre-training approach that addresses the architectural discrepancies and utilizes techniques such as attention mask annealing and shift operations. Through systematic evaluation on several benchmarks, they show that the adapted DLMs (named DiffuGPT and DiffuLLaMA) achieve state-of-the-art performance for DLMs and are competitive with their AR counterparts. Notably, the 7B parameter model (DiffuLLaMA) outperforms previous DLMs and demonstrates several advanced capabilities like in-context learning and code generation. The research also addresses the limitations of prior DLM evaluations by employing more comprehensive benchmark tasks. Overall, the paper presents a significant contribution to the field by providing an efficient and effective way to build high-performing DLMs, paving the way for broader adoption and further advancements in text generation research. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does it matter? # This research paper introduces a novel approach to scaling up diffusion language models by adapting pre-trained autoregressive models. This method addresses the challenges of training diffusion models from scratch at scale, offers a more efficient approach, and yields competitive results compared to existing methods.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: The overview of our approach to adapt autoregressive (AR) models to diffusion models. Left: The shift operation in AR models enables the output layer hi to approximate the distribution of next tokens Xi+1 in hidden representations through the cross entropy (CE) loss. Middle: We remove the causal mask gradually during training eventually making our model bi-directional. Right: inside the diffusion models we shift the logits to compute the loss with the next token (i.e., the loss on hi would be with respect to xi+1), while perceptually, the diffusion models are still functioning as recovering the original signals (since hi corresponds to xi+1 in AR loss). The figure illustrates the adaptation process of converting autoregressive language models into diffusion language models by removing causal masking, annealing attention masks, and employing a shift operation.\n🔼 Figure 2: Training loss over tokens for different scales of our adapted diffusion models. The chart displays the training loss curves for three different sized adapted diffusion language models (DiffuGPT-127M, DiffuGPT-355M, and DiffuLLaMA-7B) across various amounts of training tokens.\n🔽 Table 1: Comprehensive evaluation of different diffusion language models and the same scale pre-trained autoregressive models. There are 3 types of these models: AR for autoregressive, DD for discrete diffusion and CD for continuous diffusion. For the infilling task, we use ROUGE-1/2/L score; for other tasks, we use the accuracy (%) metric. * indicates we finetune GSM8K on models; other tasks are all in zero-shot setting. Numbers in the () indicate that AR models are only given prefix for infilling tasks. We bold the best performance among diffusion language models and underline results that surpass their base models. Algorithm 1 Adaptation TrainingAlgorithm 2 Sampling1:Input: network f⌀ initialized by existing models, training corpus Pdata (x1⌀n ), mask token m.1: Input: Trained diffusion model f⌀, sampling al- gorithm T, mask token m, start token S.2:Output: model parameters 0.2: Output: generated sample X⌀.3:repeat3: Initialize x1in = m.4:Draw x1⌀n ~ Pdata and set labels ← xJ:N4: for t = T, · · · , do 15:Sample t E Uniform(0, 1)5: Forward logits ← f⌀(x1:N)6:Sample x1:N ~ q(xt|xo)6: Sample ⌀1:N ~ Categorical(T (logits))7:Anneal the attention mask attn_mask7: for n = 1, · · · , N do8:Forward logits ← f⌀ (x1in) with attn_mask8: xt-1 = q(xt-1|x7, x⌀ ) ▷ Eq.49:Right shift logits by one position9: end for10:Lt = 1/8xt,m CE(logits, labels) ▷ Eq.710: Right shift x1iN = [s, x]=1]11:Backprop with Lt and update 011: end for12:until end training12: Return x2⌀n Table 1 presents a comprehensive comparison of various diffusion language models and their autoregressive counterparts across multiple tasks, including question answering, common sense reasoning, and code generation.\nMore visual insights # More on charts 🔼 Figure 3: Quality evaluation for unconditional generation, with perplexity measured by GPT2 large and distinct 2-gram diversity. The chart displays the relationship between decoding steps, generative perplexity, and distinct 2-gram diversity for various diffusion models, comparing their performance in unconditional text generation.\n🔼 Figure 4: Single batch decoding speed (seconds) for different models using flash-attention 2. The chart compares the single batch decoding time of LLaMA2 and DiffuLLaMA models with different diffusion timesteps (T) across various generation lengths, using flash-attention 2.\n🔼 Figure 3: Quality evaluation for unconditional generation, with perplexity measured by GPT2 large and distinct 2-gram diversity. The chart displays the perplexity and distinct 2-gram diversity of text generated by various diffusion models with different numbers of decoding steps.\n🔼 Figure 6: Finetune GSM8K data with discrete diffusion objectives, using a base model of either GPT2-S/M or DiffuGPT-S/M. DiffuGPT converges faster and attains a lower loss. The chart displays the training loss curves for GPT2 and DiffuGPT models during finetuning on the GSM8K dataset, showcasing DiffuGPT\u0026rsquo;s faster convergence and lower loss.\nMore on tables 🔽 Table 1: Comprehensive evaluation of different diffusion language models and the same scale pre-trained autoregressive models. There are 3 types of these models: AR for autoregressive, DD for discrete diffusion and CD for continuous diffusion. For the infilling task, we use ROUGE-1/2/L score; for other tasks, we use the accuracy (%) metric. * indicates we finetune GSM8K on models; other tasks are all in zero-shot setting. Numbers in the () indicate that AR models are only given prefix for infilling tasks. We bold the best performance among diffusion language models and underline results that surpass their base models. ModelSizeTypeQA TriQAWord Lamb.HSwagCommonSense Wino.SIQAReasoning PIQAMath GSM8K*Infilling ROCStoriesCodeGPT2-S127MAR4.025.929.948.535.762.144.8(7.8/0.8/7.4)(1.6)SEDD-S170MDD1.512.430.250.134.455.645.311.9/0.7/10.90.7DiffuGPT-S127MDD2.045.033.450.837.057.750.213.7/1.4/12.60.3GPT2-M355MAR6.737.738.350.737.767.445.6(8.6/0.9/8.2)(2.6)SEDD-M424MDD1.823.131.549.035.456.153.513.1/1.4/12.20.5DiffuGPT-M355MDD3.860.537.252.639.059.661.818.7/2.7/17.02.9Plaid1B1.3BCD1.28.639.351.332.354.532.612.1/1.1/11.20.1LLaMA27BAR45.468.874.967.144.878.358.6(11.6/2.1/10.5)(1.7)DiffuLLaMA7BDD18.570.958.756.443.263.363.123.3/5.5/21.215.5 Table 1 comprehensively evaluates various diffusion language models and their autoregressive counterparts across several tasks, including question answering, common sense reasoning, math problem solving, and text infilling, highlighting the performance differences between model types and scales.\n🔽 Table 1: Comprehensive evaluation of different diffusion language models and the same scale pre-trained autoregressive models. There are 3 types of these models: AR for autoregressive, DD for discrete diffusion and CD for continuous diffusion. For the infilling task, we use ROUGE-1/2/L score; for other tasks, we use the accuracy (%) metric. * indicates we finetune GSM8K on models; other tasks are all in zero-shot setting. Numbers in the () indicate that AR models are only given prefix for infilling tasks. We bold the best performance among diffusion language models and underline results that surpass their base models. ModelsMAWPSSATMathTriviaQALLaMA263.524.545.4DiffuLLaMA-ZS9.7\u003c118.5DiffuLLaMA-FS31.323.620.9DiffuLLaMA-SC33.127.726.0DiffuLLaMA-@k40.857.734.1DiffuLLaMA-CoT28.79.5- Table 1 presents a comprehensive evaluation comparing various diffusion language models and their autoregressive counterparts across multiple tasks, including question answering, commonsense reasoning, and infilling.\n🔽 Table 1: Comprehensive evaluation of different diffusion language models and the same scale pre-trained autoregressive models. There are 3 types of these models: AR for autoregressive, DD for discrete diffusion and CD for continuous diffusion. For the infilling task, we use ROUGE-1/2/L score; for other tasks, we use the accuracy (%) metric. * indicates we finetune GSM8K on models; other tasks are all in zero-shot setting. Numbers in the () indicate that AR models are only given prefix for infilling tasks. We bold the best performance among diffusion language models and underline results that surpass their base models. GPT2-S GPT2-M44.845.619.220.233.534.543.347.245.449.7 Table 1 presents a comprehensive comparison of various diffusion language models against their autoregressive counterparts across multiple tasks, highlighting the performance differences and the impact of model scaling.\n🔽 Table 1: Comprehensive evaluation of different diffusion language models and the same scale pre-trained autoregressive models. There are 3 types of these models: AR for autoregressive, DD for discrete diffusion and CD for continuous diffusion. For the infilling task, we use ROUGE-1/2/L score; for other tasks, we use the accuracy (%) metric. * indicates we finetune GSM8K on models; other tasks are all in zero-shot setting. Numbers in the () indicate that AR models are only given prefix for infilling tasks. We bold the best performance among diffusion language models and underline results that surpass their base models. ModelsTraining stepsGlobal batch sizeContext lengthSEDD (Lou et al., 2024)400k5121024MD4 (Shi et al., 2024)1000k5121024DiffuGPT-S1000k256512DiffuGPT-M160k12801024 Table 1 presents a comprehensive comparison of various diffusion language models and their autoregressive counterparts across multiple benchmark tasks, including question answering, commonsense reasoning, and infilling, highlighting the performance differences and advancements of the proposed models.\n🔽 Table 1: Comprehensive evaluation of different diffusion language models and the same scale pre-trained autoregressive models. There are 3 types of these models: AR for autoregressive, DD for discrete diffusion and CD for continuous diffusion. For the infilling task, we use ROUGE-1/2/L score; for other tasks, we use the accuracy (%) metric. * indicates we finetune GSM8K on models; other tasks are all in zero-shot setting. Numbers in the () indicate that AR models are only given prefix for infilling tasks. We bold the best performance among diffusion language models and underline results that surpass their base models. LengthAttentionDiffuLLaMA (sec)LLaMA (sec)512flash-attention 212.59.21024SDPA13.216.31024flash-attention 213.317.51024vanilla16.217.22048SDPA28.529.52048flash-attention 223.535.72048vanilla38.132.8 Table 1 comprehensively compares the performance of different diffusion language models against autoregressive models of the same scale across various tasks, including question answering, commonsense reasoning, and infilling.\nFull paper # ","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.17891/","section":"Posts","summary":"By cleverly adapting existing large language models, researchers create powerful text diffusion models, surpassing previous diffusion models in performance and opening new avenues for text generation.","title":"Scaling Diffusion Language Models via Adaptation from Autoregressive Models","type":"paper-reviews"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" TL;DR # Multimodal Large Language Models (MLLMs) are powerful, but evaluating them is challenging due to \u0026lsquo;prompt sensitivity\u0026rsquo; – small changes in the questions significantly impact results. Existing benchmarks often use the same question for all models, leading to unfair comparisons and underestimation of some models\u0026rsquo; abilities. This paper introduces TP-Eval, a new framework that customizes questions for each model to reveal their full potential. TP-Eval uses an automated process to create optimal questions by generating multiple versions and evaluating them based on accuracy and avoiding drastic semantic changes. Experiments show that TP-Eval effectively reduces bias and reveals previously unseen model capabilities. It\u0026rsquo;s particularly useful for scenarios with limited data, a common issue in multimodal evaluation. The method even extends to zero-shot settings. Overall, TP-Eval offers a more comprehensive and reliable way to assess MLLMs, leading to fairer comparisons and a better understanding of their strengths and weaknesses. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does it matter? # To provide a concise and informative summary of the research paper on TP-Eval, highlighting its key contributions, methodology, and implications for researchers.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: (a) shows underestimation caused by unsuitable prompts in MMT-Bench, (b) shows our proposed evaluation framework resolving this by customizing prompts. Figure 1 demonstrates prompt sensitivity in a multimodal benchmark and illustrates the proposed TP-Eval framework for customizing prompts to improve model evaluation.\n🔼 Figure 3: Results of different models on MMT-S (L2-category). Accuracy improvement is calculated by accuracy using the optimized prompt divided by accuracy using the original prompt. Three models showed varying improvement across different task types, while performance gains differ between models, highlighting the underestimation and bias introduced by original prompts and the effectiveness of our method. The chart displays the accuracy improvement percentage of three different models across various tasks after prompt optimization, revealing varying degrees of improvement and highlighting the impact of prompt design on model performance.\n🔽 Table 1: Similar prompt changes have different effects on two models for helmet anomaly detection task in MMT-Bench. PromptLLaVADeepSeekIs the person in the picture wearing a helmet?0.650.79Evaluate if the individual in the picture wearing adequate headgear that provides safety and visibility to minimize interpretation ambiguity. Is the individual in the picture wearing an adequate headgear0.880.61that provides safety and is visible to minimize interpretation ambiguity?0.690.83 The table shows that small changes in prompt phrasing can significantly impact the performance of different multimodal large language models (MLLMs) on the same task.\nMore visual insights # More on charts 🔼 Figure 4: Overall performance with different prompt methods on MMMU with LLaVA. In most cases, the results after optimization surpass those achieved with the initial prompts, and they generally outperform the original questions as well. The chart compares the overall performance of LLaVA on MMMU using original questions, initial prefix prompts, and optimized prefix prompts across different disciplines.\n🔼 Figure 5: Result of applying optimized prompts to other models. Applying customized prompts from one model to another yields performance changes that differ from each model\u0026rsquo;s inherent characteristics. Figure 5 is a heatmap showing the performance changes of different models when using prompts optimized for other models, highlighting model-specific prompt sensitivity.\n🔼 Figure 6: Performance on whether to use introspection or not. The chart compares the accuracy of three different methods (original prompt, no introspection, and the proposed method) on three tasks from the MMT-S benchmark, showing that the proposed method incorporating introspection outperforms the other methods in most cases.\n🔼 Figure 7: Influence of re-ranking. Both excessively high and low a* can lead to a reduction in performance, and each model achieves optimal performance with a* ∈ [0.5, 0.6]. The chart displays how the re-ranking parameter α* influences the accuracy of three different MLLMs (LLaVA, Deepseek, and InternVL) on a specific task.\nMore on tables 🔽 Table 2: Overall result for MMT-S. All three models exhibited significant performance improvements across a substantial number of tasks following prompt customization. ModelOriginal ScoreTP-Eval Score#Improved TaskRatioLLaVA-1.5-7B50.454.43225.1%DeepSeek-VL-7B55.257.32123.3%Mini-Intern VL-Chat-4B-V1-554.656.91640.4% Table 2 presents the overall performance of three models on the MMT-S benchmark before and after prompt customization using the TP-Eval framework, showing significant improvements in accuracy for most models.\n🔽 Table 3: Zero-shot prompt optimization utilizing In-context Learning. Task nameOriginal promptZero-shotFew-shothelmet anomaly detection0.650.860.92artwork emotion recognition0.30.330.41spot similarity0.230.420.52 Table 3 shows the zero-shot prompt optimization results for three tasks from MMT-S, using in-context learning with a subset of successfully optimized examples.\nFull paper # ","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.18071/","section":"Posts","summary":"TP-Eval revolutionizes multimodal LLM evaluation by customizing prompts for each model, uncovering hidden capabilities and mitigating evaluation bias.","title":"TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts","type":"paper-reviews"},{"content":" TL;DR # This paper introduces WorldSimBench, a novel benchmark designed to thoroughly evaluate the capabilities of video generation models that function as \u0026ldquo;world simulators.\u0026rdquo; These models generate videos that accurately reflect the physical rules and dynamics of real-world scenarios. WorldSimBench employs a two-pronged approach:\nExplicit Perceptual Evaluation: This part assesses the visual realism of the generated videos by using human feedback and a specially trained model to evaluate aspects like visual quality and consistency with given instructions. A new dataset, HF-Embodied Dataset, was created for this purpose.\nImplicit Manipulative Evaluation: Here, the generated videos are treated as action plans. The model\u0026rsquo;s ability is evaluated by measuring how well the generated videos translate into actual actions within simulated dynamic environments representing three key scenarios: Open-Ended Embodied Environment, Autonomous Driving, and Robot Manipulation.\nThe researchers categorized predictive models hierarchically and found that WorldSimBench provides valuable insights to drive further innovation in this field, pushing the boundaries of embodied AI.\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does it matter? # WorldSimBench is a new benchmark for evaluating video generation models as world simulators. It uses a dual evaluation framework (Explicit Perceptual and Implicit Manipulative) to assess visual fidelity and action consistency across three embodied scenarios.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: Overview of the hierarchical capabilities of the Predictive Models. Models at higher stages demonstrate more advanced capabilities. We take the initial step in evaluating Predictive Generative Models up to the S3 stage, known as World Simulators, by introducing a parallel evaluation framework, WorldSimBench. WorldSimBench assesses the models both Explicit Perceptual Evaluation and Implicit Manipulative Evaluation, focusing on video generation and action transformation across three critical embodied scenarios. Figure 1 provides a hierarchical overview of predictive models\u0026rsquo; capabilities, introducing WorldSimBench, a dual evaluation framework for World Simulators focusing on video generation and action transformation across three embodied scenarios.\n🔼 Figure 4: Result of Explicit Perceptual Evaluation across three embodied scenarios. Scores in each embodied scenario are normalized to 0-1. The abbreviations are listed in Tab. 2. The radar chart visualizes the performance of eight video generation models across three embodied scenarios, showing their strengths and weaknesses on different evaluation dimensions.\n🔽 Table 1: Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes. BenchmarkInput ModalityOutput ModalityBased MethodStageInteractive Env.Evaluation StrategyAgentBench Liu et al. 2023bTextTextLLMSoTask-LevelHuman JudgementEgoPlan-Bench Chen etal. 2023Text \u0026amp; ImagesTextMLLMSoN/AMulti-choiceMMWorld He et al. 2024Text \u0026amp; ImagesTextMLLMSoN/AGPT JudgementVAB Liu et al. 2024aText \u0026amp; ImagesTextMLLMSoTask-LevelHuman JudgementLEGO Lai et al. 2023Text \u0026amp; ImagesImageIGMS1Task-LevelFeature SimilarityVBench Huang etal. 2024TextVideoVGMS2N/AFeature SimilarityEvalCrafter Liu etal. 2024bText \u0026amp; ImagesVideoVGMS2N/AFeature SimilarityWorldSimBenchText \u0026amp; ImagesActionable VideoVGMS3Action-LevelHuman Preference Evaluator Embodied Metric Table 1 compares existing Predictive Model benchmarks across several criteria, including input/output modality, model type, evaluation stage, and interaction type with the environment.\nMore visual insights # More on figures 🔼 Figure 2: Overview of Explicit Perceptual Evaluation. (Top) Instruction Prompt Generation. We use a large collection of video captions from the internet and our predefined embodied evaluation dimensions. These are expanded using GPT and manually verified to create a corresponding Task Instruction Prompt List for data generation and evaluation. (Bottom) HF-Embodied Dataset Generation. Massive internet-sourced embodied videos with captions are used to train data generation models. Fine-grained Human Feedback Annotation is then applied to the embodied videos according to the corresponding Task Instruction Prompt List, covering multiple embodied dimensions. The figure illustrates the process of Explicit Perceptual Evaluation, including instruction prompt generation and HF-Embodied dataset generation with fine-grained human feedback annotation.\n🔼 Figure 3: Overview of Implicit Manipulative Evaluation. Embodied tasks in different scenarios are decomposed into executable sub-tasks. The video generation model generates corresponding predicted videos based on the current instructions and real-time observations. Using a pre-trained IDM or a goal-based policy, the agent executes the generated sequence of actions. After a fixed timestep, the predicted video is refreshed by sampling again from the video generation model, and this process repeats. Finally, the success rates of various embodied tasks are obtained through monitors in the simulation environment. The figure illustrates the Implicit Manipulative Evaluation process, decomposing embodied tasks into sub-tasks, using video generation models and video-to-action mapping for evaluation.\n🔼 Figure 7: Rollout of Open-Ended Embodied Environment in Implicit Manipulative Evaluation. The figure shows a sequence of images from a Minecraft simulation illustrating the rollout process of an embodied task in the Implicit Manipulative Evaluation.\n🔼 Figure 8: Rollout of Autonomous Driving in Implicit Manipulative Evaluation. The figure shows a sequence of frames from the autonomous driving simulation, illustrating the model\u0026rsquo;s predictions and the corresponding actions taken by the agent.\n🔼 Figure 9: Rollout of Robot Manipulation in Implicit Manipulative Evaluation. The figure shows a sequence of images illustrating the execution of a robot manipulation task in a simulated environment, guided by textual instructions.\nMore on tables 🔽 Table 3: The overall performance comparison between Human Preference Evaluator and GPT-40. HPE indicates Human Preference Evaluator. HPE@Lavie means that HPE is trained on videos except those generated by Lavie. The validation is conducted on videos generated by Laive under zero-shot setting. Embodied ScenarioGPT-4oHPEGPT-4o@OpenSoraHPE@OpenSoraGPT-4o@LavieHPE@LavieOE@Acc(↑)72.889.466.571.678.587.9AD @ PLCC(↑)0.280.600.030.34-0.040.49RM@PLCC(↑)0.070.43-0.060.470.170.44 The table compares the overall performance of the Human Preference Evaluator and GPT-40 across three embodied scenarios, showing the evaluator\u0026rsquo;s superior performance in aligning with human preferences.\n🔽 Table 4: Analysis of HF-Embodied Dataset. Samples scored higher than 3 in AD and RM are considered positive. Embodied Scenario#instructions#videos#dims#actions#positive#negativeOpen-Ended Embodied Environment270840171112124979965Autonomous Driving515870655676835044Robot Manipulation255611430726706729338 Table 4 presents the analysis of the HF-Embodied Dataset, showing the number of instructions, videos, dimensions, actions, positive samples (scores \u0026gt;3), and negative samples for three embodied scenarios: Open-Ended Embodied Environment, Autonomous Driving, and Robot Manipulation.\n🔽 Table 5: Training Frames of Generation Models. ModelOpen-Sora-PlanLavieModelScopeOpenSoraAnimateDiffDynamicCrafterEasyAnimateShort Videos(frames)16161616161616Long Videos(frames)64486048646064 This table shows the number of frames used for training short and long videos for each of the eight video generation models.\n🔽 Table 3: The overall performance comparison between Human Preference Evaluator and GPT-40. HPE indicates Human Preference Evaluator. HPE@Lavie means that HPE is trained on videos except those generated by Lavie. The validation is conducted on videos generated by Laive under zero-shot setting. OE@ Acc(↑)BCFCIASAVCTJEIOverallGPT-4o HPE60.570.470.967.379.683.785.972.881.287.587.596.494.593.888.889.4GPT-4o@OpenSora HPE@OpenSora608080500.010088.866.570906010010022.28071.6GPT-4o@Lavie HPE@Lavie5066.77588.887.510087.578.58080801001007510087.9AD @ PLCC(↑)AEIAPVTJKESFOverallGPT-4o HPE0.370.220.230.280.370.180.280.710.570.500.580.650.580.60GPT-4o@OpenSora HPE@OpenSora0.22-0.390.320.15-0.03-0.120.030.370.550.340.060.280.410.34GPT-4o@Lavie HPE@Lavie0.170.13-0.340.06-0.09-0.15-0.040.281.00.490.370.120.690.49RM@PLCC(↑)AEBCFCIAPVTJEIOverallGPT-4o HPE0.070.180.200.32-0.14-0.01-0.140.070.520.430.430.430.200.560.440.43GPT-4o@OpenSora HPE@ OpenSora-0.45-0.030.080.00.04-0.230.14-0.060.250.350.050.420.890.890.440.47GPT-4o@Lavie HPE@Lavie0.11-0.070.420.420.210.31-0.210.170.330.040.690.400.890.670.060.44 The table compares the overall performance of a Human Preference Evaluator and GPT-40 across various metrics and scenarios for evaluating video generation models.\n🔽 Table 7: Evaluation results in OE. The abbreviations are listed in Tab. 2. ModelBCFCIASAVCTJEIOverallOpen-Sora-Plan1.41.91.71.72.01.51.61.69Lavie1.32.01.71.72.02.01.81.79ModelScope1.92.02.01.72.02.01.751.91OpenSora1.61.91.61.82.02.01.61.79AnimateDiff1.31.31.21.71.41.381.551.40DynamicCrafter1.92.01.52.02.02.01.451.84EasyAnimate1.41.81.52.02.01.221.451.62 The table presents the evaluation results of seven video generation models across seven dimensions in the Open-Ended Embodied Environment scenario.\n🔽 Table 8: Evaluation results in AD. The abbreviations are listed in Tab. 2. ModelAEIAPVTJKESFOverallOpen-Sora-Plan1.65.01.551.41.453.22.37Lavie2.155.02.22.82.15.03.21ModelScope2.85.03.354.03.05.03.86OpenSora3.555.04.44.83.655.04.40AnimateDiff1.555.01.551.01.33.82.37DynamicCrafter2.64.03.43.82.655.03.57EasyAnimate1.53.41.41.41.32.61.93 The table presents a comparison of eight video generation models\u0026rsquo; performance across six evaluation dimensions in an autonomous driving scenario.\n🔽 Table 2: Hierarchical Evaluation Dimension. The dimensions are categorized into three main aspects: Visual Quality for evaluating the overall quality, Condition Consistency for evaluating the alignment to the input instruction, and Embodiment for evaluating embodied related factors like physical rules. ModelAEBCFCIAPVTJEIOverallOpen-Sora-Plan4.04.04.01.04.95.04.03.84Lavie3.83.94.01.84.955.04.13.94ModelScope3.634.14.01.184.95.04.03.83OpenSora3.854.03.951.34.755.04.13.85AnimateDiff3.83.94.01.04.955.04.13.82DynamicCrafter3.974.084.02.65.05.04.314.14EasyAnimate3.553.453.651.24.84.33.453.49 The table presents a hierarchical breakdown of evaluation dimensions for three embodied scenarios (Open-Ended Embodied Environment, Autonomous Driving, and Robot Manipulation), categorized into visual quality, condition consistency, and embodiment.\n🔽 Table 1: Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes. BehaviorActionforwardW keybackS keyleftA keyrightD keyjumpspace keyinventoryE keysneakshift keysprintctrl keyattackleft mouse button This table compares various existing predictive model benchmarks across several key features, including input and output modalities, interaction types, evaluation strategies, and the stage of predictive model capability.\n🔽 Table 1: Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes. ModelConditionAVGSpecific TasksCollect WoodCollect DirtCollect SeedTravel Dis.Dig DepthOpen-Sora-PlanText26.3819.9050.207.30342.9120.20Lavie26.0623.5056.0011.60270.2012.20ModelScope21.05014.0052.206.30240.728.70OpenSora27.8021.2070.2010.40339.873.20AnimateDiff13.107.4022.903.30274.194.50Open-Sora-PlanText \u0026 Image10.2811.1012.502.60195.145.70DynamiCrafter4.060.400.301.30130.045.30EasyAnimate4.840.200.701.70157.125.90 This table compares existing predictive model benchmarks across various dimensions, including input/output modalities, methods, evaluation strategies, and interaction types.\n🔽 Table 12: Detail Result of Autonomous Driving in Implicit Manipulative Evaluation. ModelDS(↑)RC(↑)IS(↑)VC(↓)PC(↓)LC(↓)RV(↓)OI(↓)Open-Sora-Plan31.05438.2490.7672.4000.0004.4011.1333.514DynamiCrafter24.49137.1890.5995.0300.0004.8960.9373.221EasyAnimate17.41428.4750.6070.0000.00029.3440.0001.690 The table presents the evaluation results of three video generation models across eight metrics in the Autonomous Driving task within the Implicit Manipulative Evaluation.\n🔽 Table 1: Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes. MethodTask completed in a row (%) ↑Avg. Len. ↑12345Open-Sora-Plan0.850.700.600.400.402.95DynamiCrafter0.950.750.550.250.252.75EasyAnimate0.900.600.350.100.102.05 This table compares existing predictive model benchmarks across various aspects such as input/output modalities, evaluation strategies, and interaction types.\nFull paper # ","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.18072/","section":"Posts","summary":"WorldSimBench: A dual evaluation framework assessing video generation models as world simulators, judging both visual quality and action consistency.","title":"WorldSimBench: Towards Video Generation Models as World Simulators","type":"paper-reviews"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-24-10-24/","section":"Tags","summary":"","title":"🤗 24-10-24","type":"tags"},{"content":"","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-24-10-22/","section":"Tags","summary":"","title":"🔖 24-10-22","type":"tags"},{"content":" TL;DR # LongVU tackles the challenge of processing lengthy videos within the limited context size of large language models (LLMs). It does this through a three-step adaptive compression process. First, it leverages DINOv2 features to identify and remove redundant frames. Second, it uses text-guided cross-modal queries to selectively reduce visual tokens, keeping important details while reducing less relevant information. Finally, it employs a spatial token reduction method based on temporal dependencies for further compression. This approach allows LongVU to significantly compress videos without losing crucial visual information, enabling efficient processing of long videos in LLMs. Benchmarks show that LongVU outperforms existing methods in various video understanding tasks, especially on hour-long videos, demonstrating its effectiveness and efficiency in handling long-form video data. The research also highlights LongVU\u0026rsquo;s scalability, showing it performs well even with smaller and less resource-intensive LLMs. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does it matter? # To provide a concise and informative summary of the research paper on LongVU, a spatiotemporal adaptive compression mechanism for long video-language understanding.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 2. Architecture of LongVU. Given a densely sampled video frames, we first utilize DINOv2 (Oquab et al., 2023) prior to remove redundant frames, and fuse the remaining frame features from both SigLIP (Zhai et al., 2023) and DINOv2 (Oquab et al., 2023), described in Section 3.1. Then we selectively reduce visual tokens via cross-modal query, detailed in Section 3.2. Finally, as demonstrated in Section 3.3, we conduct spatial token compression based on temporal dependencies to further meet the context length of LLMs. The figure illustrates the LongVU architecture, showcasing its three-step spatiotemporal adaptive compression mechanism for processing long videos within the context length limitations of LLMs.\n🔼 Figure 1 Effectiveness of our LongVU over commonly-used uniform sampling and dense sampling. Uniform sampling overlooks critical frames due to its sparse nature. Dense sampling may surpass the maximum context length, leading to truncation of tokens from targeted frames. In contrast, our method can adaptively conduct spatiotemporal compression, accommodating long video sequences while preserving more visual details. The chart compares LongVU\u0026rsquo;s adaptive spatiotemporal compression to uniform and dense sampling methods for processing long videos, showing LongVU\u0026rsquo;s ability to preserve visual details within context limits.\n🔽 Table 1 Results on comprehensive video understanding benchmarks ModelsSizeContext Length#FramesEgoSchemaMVBenchMLVUVideoMMEOverallLongDuration179.8 sec16 sec3~120 min1〜60 min30〜60 minProprietary ModelsGPT4-V (OpenAI, 2023)--1fps55.643.7-60.756.9GPT4-o (OpenAI, 2024)--1fps72.264.666.277.272.1Open-Source Video MLLMsVideo-LLaVA (Lin et al., 2023)7B4k838.441.047.340.438.1LLaMA-VID (Li et al., 2023d)7B4k1fps38.541.933.2--Chat-UniVi (Jin et al., 2023)7B4k64---45.941.8ShareGPT4Video (Chen et al., 2024)8B8k16-51.246.443.637.9LLaVA-NeXT-Video (Zhang et al., 2024b)7B8k3243.933.7-46.5-VideoLLaMA2 (Cheng et al., 2024)7B8k3251.754.648.546.643.8LongVA (Zhang et al., 2024a)7B224k128--56.354.347.6VideoChat2 (Li et al., 2024b)7B8k1654.460.447.954.639.2LLaVA-OneVision (Li et al., 2024a)7B8k3260.156.764.758.246.7LongVU (Ours)7B8k1fps67.666.965.460.659.5 Table 1 presents the quantitative results of LongVU and other video understanding models on various benchmarks, including EgoSchema, MVBench, VideoMME, and MLVU, comparing their performance across different video lengths and durations.\nMore visual insights # More on figures 🔼 Figure 3 Examples for various video understanding capabilities of LongVU model. We showcase that our LongVU is able to complete different types of video understanding tasks. Figure 3 shows four examples of LongVU\u0026rsquo;s video understanding capabilities, demonstrating its ability to handle spatial-temporal orientation awareness, detailed description, action counting, and hour-long video understanding.\n🔼 Figure 2. Architecture of LongVU. Given a densely sampled video frames, we first utilize DINOv2 (Oquab et al., 2023) prior to remove redundant frames, and fuse the remaining frame features from both SigLIP (Zhai et al., 2023) and DINOv2 (Oquab et al., 2023), described in Section 3.1. Then we selectively reduce visual tokens via cross-modal query, detailed in Section 3.2. Finally, as demonstrated in Section 3.3, we conduct spatial token compression based on temporal dependencies to further meet the context length of LLMs. The figure illustrates the architecture of LongVU, detailing its spatiotemporal adaptive compression mechanism which uses DINOv2 and SigLIP for feature extraction, cross-modal query for selective feature reduction, and spatial token compression based on temporal dependencies to process long videos.\n🔼 Figure 6. Similarity comparison between SigLIP (Zhai et al., 2023) and DINOv2 (Oquab et al., 2023) features. The similarity is calculated between the first frame and the remainings. DINO concentrating on vision centric task effectively capture subtle frame differences compared with SigLIP (Zhai et al., 2023) which is aligned on semantic space. Figure 6 shows that DINOv2 is more effective than SigLIP at capturing subtle frame differences due to its focus on visual features.\nMore on charts 🔼 Figure 4 We randomly sample hundreds of videos to demonstrate the frames/tokens level reduction rate. (a) The number of frames before/after temporal reduction based on DINOv2 features (Section 3.1). (b) The number of tokens before/after spatial token compression (Section 3.3). The chart displays the number of frames and tokens before and after temporal and spatial compression, respectively, demonstrating the effectiveness of LongVU\u0026rsquo;s compression mechanism.\n🔼 Figure 7 Needle-In-A-Video-Haystack results. Our spatiotemporal adaptive token compression scheme improves the score for locating the needle frame. The heatmap visualizes the performance of different model configurations (with and without spatiotemporal compression) on the Needle-in-a-Haystack task, showing the impact of compression on locating a target frame within a long video.\n🔼 Figure 7 Needle-In-A-Video-Haystack results. Our spatiotemporal adaptive token compression scheme improves the score for locating the needle frame. The heatmap visualizes the performance of the LongVU model on the Needle-In-A-Video-Haystack task, showing how the model\u0026rsquo;s ability to locate the needle frame varies depending on the number of frames and the depth of the needle frame\u0026rsquo;s insertion.\n🔼 Figure 7 Needle-In-A-Video-Haystack results. Our spatiotemporal adaptive token compression scheme improves the score for locating the needle frame. The heatmap shows the performance of different models on the needle-in-a-haystack task, demonstrating the improvement achieved by the proposed spatiotemporal adaptive token compression.\nMore on tables 🔽 Table 2 Results of small-size video language models across video understanding benchmarks. ModelsEgoSchemaMVBenchVideoMMEMLVUOverallLongInternVL2 (InternLM2-1.8B) (OpenGVLab, 2024)-60.247.342.6-VideoChat2 (Phi-3-mini-4B) (Li et al., 2024b)56.755.1---Phi-3.5-vision-instruct (Phi-3-mini-4B) (Abdin et al., 2024)--50.843.8-LongVU (Ours) (Llama3.2-3B)59.160.951.547.255.9 Table 2 presents the results of small-size video language models on various video understanding benchmarks, comparing their performance on EgoSchema, MVBench, VideoMME (Overall and Long subsets), and MLVU.\n🔽 Table 3 Ablation studies of number of tokens per frame, different context lengths, and our spatiotemporal compression components. MethodsContext Length#TokensEgoSchemaVideoMMEMLVUUniform16k14467.1260.0164.70DINO16k14467.3461.2564.83Uniform8k6466.8457.5660.87Uniform8k14466.2858.8463.28SigLIP8k6466.0458.6362.17DINO8k6466.2059.9062.54DINO + Query8k64, 14467.3060.0865.05DINO + Query + STC (default)8kdynamic67.6260.5665.44 Table 3 shows the ablation study of the number of tokens per frame, different context lengths, and the spatiotemporal compression components used in the LongVU model, demonstrating the impact of each component on the performance across different video understanding benchmarks.\n🔽 Table 4 Ablation study on each subtask in MLVU (Zhou et al., 2024). StratgycountegoneedleorderplotQAanomalyreasoningAvgDINO24.1559.0968.1652.8971.2474.0086.3662.54DINO+Query28.9855.3978.8756.3772.3575.5087.8765.05DINO+Query+STC (default)28.9859.3776.3358.3071.6176.0087.5065.44 The table presents ablation study results on each subtask in MLVU, showing the impact of different components of the proposed spatiotemporal compression mechanism on the performance of each subtask.\n🔽 Table 1 Results on comprehensive video understanding benchmarks ModelShortMediumLongOverallReduction rate1st frame in sliding window (default)64.758.259.560.955.47%(K/2)th frame in sliding window64.758.758.660.754.97%frame with high changes64.758.258.360.455.62% Table 1 presents a quantitative comparison of LongVU\u0026rsquo;s performance against several state-of-the-art video understanding models across various benchmarks, including EgoSchema, MVBench, VideoMME, and MLVU, evaluating metrics like accuracy and overall performance.\n🔽 Table 6 Training data statistics. ModalityTask# SamplesDatasetImage-TextSingle-Image3.2MLLaVA-OneVisionVideo-TextCaptioning43KTextVR, MovieChat, YouCook2Classification1KKinetics-710VQA424KNExTQA, CLEVRER, EgoQA, TGIF, WebVidQA, DiDeMoInstruction85KShareGPT4Video Table 6 presents the training data statistics, including the modality, task, number of samples, and datasets used for image-text and video-text training.\n🔽 Table 1 Results on comprehensive video understanding benchmarks ModelSizeFramesShortMediumLongOverallVideo-LLa VA (Lin et al., 2023)7B846.140.738.141.6ShareGPT4Video (Chen et al., 2024)8B1653.639.337.943.6Chat- Univi-v1.5 (Jin et al., 2023)7B6451.244.641.845.9VideoLLaMA2 (Cheng et al., 2024)7B1659.447.643.850.3VideoChat2 (Li et al., 2024b)7B1652.839.439.243.8LongVA (Zhang et al., 2024a)7B12861.650.447.654.3LLaVA-OneVision (Li et al., 2024a)7B3269.153.346.758.2LongVU (Ours)7B1fps64.758.259.560.9 Table 1 presents a quantitative comparison of LongVU against various state-of-the-art video understanding models across multiple benchmarks, showcasing its performance in terms of accuracy and efficiency.\n🔽 Table 8 Ablation study on with or without FPE. MethodsContext Length#TokensEgoSchemaVideoMMEMLVUDINO + Query8k64, / 14467.3060.0865.05DINO + Query + STC (default)8kdynamic67.6260.5665.44DINO + Query + STC + FPE8kdynamic67.8760.8964.56 The table shows the ablation study of adding frame positional encoding (FPE) to the model on EgoSchema, VideoMME and MLVU benchmarks.\n🔽 Table 9 Strategy ablations on each subtask in MLVU (Zhou et al., 2024). StratgycountegoneedleorderplotQAanomalyreasoningAvgDINO24.1559.0968.1652.8971.2474.086.3662.54DINO+Query28.9855.3978.8756.3772.3575.587.8765.05DINO +Query+STC (default)28.9859.3776.3358.3071.6176.087.5065.44DINO + Query+STC+ FPE29.4660.7974.0852.1271.7974.586.7464.56 Table 9 shows the ablation study on each subtask in MLVU dataset, comparing different strategies of spatial token compression, including DINO, DINO+Query, DINO+Query+STC and DINO+Query+STC+FPE.\n🔽 Table 1 Results on comprehensive video understanding benchmarks ModelSQA-IMGMMVPPOPERealWorldQABefore video SFT95.4451.3386.6561.06After video SFT83.9432.0081.2347.65 Table 1 presents a quantitative comparison of LongVU\u0026rsquo;s performance against various state-of-the-art video understanding models across multiple benchmarks, including metrics for video length, context length, and number of frames.\nFull paper # ","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.17434/","section":"Posts","summary":"LongVU efficiently processes hour-long videos by adaptively compressing spatiotemporal redundancy, achieving state-of-the-art video understanding performance.","title":"LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding","type":"paper-reviews"},{"content":" TL;DR # The paper introduces the Large View Synthesis Model (LVSM), a novel approach to creating new views of a scene from a limited number of input images. Unlike many previous methods that heavily rely on 3D assumptions about the scene (like depth information or specific geometric structures), LVSM takes a more data-driven approach. It uses transformers, a type of neural network architecture known for its ability to handle sequential data effectively. The researchers propose two versions of LVSM: one with an encoder-decoder structure and one that\u0026rsquo;s decoder-only. The encoder-decoder version first compresses the input images into a compact representation, and then generates new views from this compressed representation. The decoder-only version directly generates the new views from the input images without any intermediate representation. Both versions significantly outperform existing methods in terms of image quality (measured by PSNR and other metrics) and show improved scalability, meaning they can handle larger scenes and more complex datasets more efficiently. The decoder-only version, in particular, demonstrates impressive zero-shot generalization capabilities— meaning it can produce high-quality images even for numbers of input views it hasn\u0026rsquo;t been trained on. The improvement is achieved by completely removing or minimizing intermediate 3D-related scene representations, thus, allowing the model to focus on directly learning image generation from input data. Overall, LVSM represents a significant step forward in view synthesis, offering a more flexible and effective approach compared to existing methods. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does it matter? # The provided research paper introduces LVSM, a novel transformer-based model for novel view synthesis that surpasses existing methods in quality and scalability by minimizing reliance on 3D inductive biases. This approach represents a significant advancement in the field, offering a more flexible and data-driven approach to view synthesis.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: LVSM supports feed-forward novel view synthesis from sparse posed image inputs (even from a single view) on both objects and scenes. LVSM achieves significant quality improvements compared with the previous SOTA method, i.e., GS-LRM (Zhang et al., 2024). (Please zoom in for more details.) Figure 1 shows examples of novel view synthesis results of LVSM on both object and scene levels, comparing against the previous state-of-the-art method.\n🔼 Figure 5: Zero-shot generalization to different number of input images on the GSO dataset (Downs et al., 2022). We note that all models are trained with just 4 input views. The chart displays the PSNR results for decoder-only and encoder-decoder LVSM models, as well as GS-LRM, across varying numbers of input views during zero-shot generalization on the GSO dataset.\n🔽 Table 1: Quantitative comparisons on object-level (left) and scene-level (right) view synthesis. For the object-level comparison, we matched the baseline settings with GS-LRM (Zhang et al., 2024) in both input and rendering under both resolution of 256 (Res-256) and resolution of 512 (Res-512). For the scene-level comparison, we use the same validation dataset used by pixelSplat (Charatan et al., 2024), which has 256 resolution. ABO Collins et al. 2022aGSO Downs et al.. 2022RealEstate10k Zhou et al. 2018)PSNRSSIMLPIPS ↓PSNRSSIM LPIPS↓PSNR ↑SSIMLPIPS ↓Triplane-LRM Liet al.. 2023 (Res-512)27.500.8960.09326.540.8930.064pixelNeRF Yuetal.. 202120.430.5890.550GS-LRM Zhangetai., 2024) (Res-512)29.090.9250.08530.520.9520.050GPNR Suhail etal. 2022a24.110.7930.255OursEncoder-Decoder (Res-512)29.810.9130.06529.320.9330.052Du et. al Duetal. 2023,24.780.8200.213Ours Decoder-Only (Res-512)32.100.9380.04532.360.9620.028pixelSplat Charatan et al.. 202426.090.8630.136LGM Tang et al.. 2024) (Res-256)20.790.8130.15821.440.8320.122MVSpiat Cnen etal., 202426.390.8690.128GS-LRM Znang et al., 2024, (Res-256)28.980.9260.07429.590.9440.051GS-LRM Znang et al., 202428.100.8920.114OursEncoder-Decoder (Res-256)30.350.9230.05229.190.9320.046OursEncoder-Decoder28.580.8930.114Ours Decoder-Only (Res-256)32.470.9440.03731.710.9570.027Ours Decoder-Only29.670.9060.098 Table 1 quantitatively compares the performance of the proposed LVSM model against various baselines on object-level and scene-level view synthesis tasks, reporting PSNR, SSIM, and LPIPS metrics for different resolutions.\nMore visual insights # More on figures 🔼 Figure 2: LVSM model architecture. LVSM first patchifies the posed input images into tokens. The target view to be synthesized is represented by its Plücker ray embeddings and is also tokenized. The input view and target tokens are sent to a full transformer-based model to predict the tokens that are used to regress the target view pixels. We study two LVSM transformer architectures, as a Decoder-only architecture (left) and a Encoder-Decoder architecture (right). This figure illustrates the two main architectures of the Large View Synthesis Model (LVSM): a decoder-only architecture and an encoder-decoder architecture, both using transformers to synthesize novel views from input images.\n🔼 Figure 3: Object-level visual comparison at 512 resolution. Given 4 sparse input posed images (leftmost column), we compare our high-res object-level novel-view rendering results with two baselines: Instant3D’s Triplane-LRM (Li et al., 2023) and GS-LRM (Res-512) (Zhang et al., 2024). Both our Encoder-Decoder and Decoder-Only models exhibit fewer floaters (first example) and fewer blurry artifacts (second example), compared to the baselines. Our Decoder-Only model effectively handles complex geometry, including small holes (third example) and thin structures (fourth example). Additionally, it preserves the details of high-frequency texture (last example). Figure 3 shows a comparison of object-level novel view synthesis results between the proposed LVSM and two baseline methods, highlighting the improved quality and handling of complex geometry by LVSM.\n🔼 Figure 4: Scene-level visual comparison. We evaluate our encoder-decoder and decoder-only models on scene-level view synthesis, comparing them against the prior leading baseline methods, namely pixelSplat (Charatan et al., 2024), MVSplat (Chen et al., 2024), and GS-LRM (Zhang et al., 2024). Our methods exhibit fewer texture and geometric artifacts, generate more accurate and realistic specular reflections, and are closer to the ground truth images. The figure compares the scene-level view synthesis results of the proposed LVSM models against several baseline methods, highlighting improvements in accuracy and realism.\n🔼 Figure 3: Object-level visual comparison at 512 resolution. Given 4 sparse input posed images (leftmost column), we compare our high-res object-level novel-view rendering results with two baselines: Instant3D’s Triplane-LRM (Li et al., 2023) and GS-LRM (Res-512) (Zhang et al., 2024). Both our Encoder-Decoder and Decoder-Only models exhibit fewer floaters (first example) and fewer blurry artifacts (second example), compared to the baselines. Our Decoder-Only model effectively handles complex geometry, including small holes (third example) and thin structures (fourth example). Additionally, it preserves the details of high-frequency texture (last example). Figure 3 shows a comparison of object-level novel view rendering results using four input views with the proposed LVSM model and two baseline methods, highlighting the superior quality and detail preservation of the proposed model.\nMore on tables 🔽 Table 1: Quantitative comparisons on object-level (left) and scene-level (right) view synthesis. For the object-level comparison, we matched the baseline settings with GS-LRM (Zhang et al., 2024) in both input and rendering under both resolution of 256 (Res-256) and resolution of 512 (Res-512). For the scene-level comparison, we use the same validation dataset used by pixelSplat (Charatan et al., 2024), which has 256 resolution. RealEstate10k Zhou et al. 2018)PSNR ↑SSIMLPIPS ↓Ours Encoder-Decoder (6 + 18)28.320.8880.117Ours Encoder-Decoder (12 + 12)27.390.8690.137Ours Encoder-Decoder (18 +6)26.800.8550.152Ours Decoder-Only (24 layers)28.890.8940.108Ours Decoder-Only (18 layers)28.770.8920.109Ours Decoder-Only (12 layers)28.610.8900.111Ours Decoder-Only (6 layers)27.620.8690.129 Table 1 quantitatively compares the performance of LVSM against state-of-the-art methods on object-level and scene-level novel view synthesis tasks, using PSNR, SSIM, and LPIPS metrics.\n🔽 Table 1: Quantitative comparisons on object-level (left) and scene-level (right) view synthesis. For the object-level comparison, we matched the baseline settings with GS-LRM (Zhang et al., 2024) in both input and rendering under both resolution of 256 (Res-256) and resolution of 512 (Res-512). For the scene-level comparison, we use the same validation dataset used by pixelSplat (Charatan et al., 2024), which has 256 resolution. GSO Downs et al. 2022PSNR ↑SSIM TLPIPS ↓Ours Encoder-Decoder28.070.9200.053Ours w/o latents\u0026rsquo; self-updating26.610.9030.061RealEstate10k Zhou et al., 2018PSNR ↑SSIM�LPIPS ↓Ours Decoder-Only29.670.9060.098Ours w/ per-patch prediction28.980.8970.103 Table 1 quantitatively compares the performance of the proposed LVSM model against existing state-of-the-art methods on object-level and scene-level view synthesis tasks, reporting PSNR, SSIM, and LPIPS metrics for different resolutions.\n🔽 Table 1: Quantitative comparisons on object-level (left) and scene-level (right) view synthesis. For the object-level comparison, we matched the baseline settings with GS-LRM (Zhang et al., 2024) in both input and rendering under both resolution of 256 (Res-256) and resolution of 512 (Res-512). For the scene-level comparison, we use the same validation dataset used by pixelSplat (Charatan et al., 2024), which has 256 resolution. GSO Downs et al., 2022PSNR ↑SSIMTLPIPS ↓Ours Decoder-Only (24 layers)27.040.9100.055Ours Decoder-Only (18 layers)26.810.9070.057Ours Decoder-Only (12 layers)26.110.8960.065Ours Decoder-Only (6 layers)24.150.8650.092 Table 1 quantitatively compares the performance of the proposed LVSM model against several baseline methods on object-level and scene-level novel view synthesis tasks, reporting PSNR, SSIM, and LPIPS metrics.\nFull paper # ","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.17242/","section":"Posts","summary":"LVSM: A new, data-driven view synthesis model outperforms current methods by minimizing 3D assumptions, achieving superior quality and scalability.","title":"LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias","type":"paper-reviews"},{"content":"","date":"20 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-24-10-20/","section":"Tags","summary":"","title":"🔖 24-10-20","type":"tags"},{"content":" TL;DR # This research introduces M-REWARDBENCH, a groundbreaking multilingual benchmark designed to assess the capabilities of reward models (RMs) within large language models (LLMs). Currently, most RMs are trained and tested primarily using English data, limiting our understanding of their performance in diverse linguistic contexts. M-REWARDBENCH addresses this limitation by evaluating RMs across a wide range of 23 languages, covering various tasks such as chat, safety, reasoning, and translation. The study reveals considerable performance disparities between English and non-English languages, highlighting the need for more robust multilingual RMs. It also demonstrates a strong correlation between translation quality and RM performance, emphasizing the importance of high-quality translations in evaluating and developing multilingual models. The findings underscore the influence of linguistic features such as resource availability, language family, and script on model performance, offering valuable insights into the challenges of multilingual RM development. The study releases the M-REWARDBENCH dataset and codebase, fostering further research in this crucial area. The benchmark itself is a significant contribution, providing researchers with a valuable tool to systematically evaluate the performance of reward models in various languages and contexts. The detailed analyses of the results offer practical guidance for improving the robustness and cross-lingual generalizability of future reward models and LLMs. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does it matter? # M-REWARDBENCH is a new multilingual benchmark for evaluating reward models (RMs) in large language models (LLMs). It addresses the gap in existing research by evaluating RMs across 23 languages and 6 tasks, revealing significant performance differences between English and non-English languages.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: Performance gap between RewardBench (English) and the average M-REWARDBENCH scores across 23 languages for various reward models (Pearson r: 0.92, Spearman p: 0.89). All models underperform on our multilingual benchmark compared to their performance on the corresponding English benchmark. The chart displays the performance gap of various reward models between the English-centric RewardBench and the multilingual M-REWARDBENCH across 23 languages.\n🔽 Table 1: Dataset statistics for M-REWARDBENCH. Number of languages excludes English. For Translation, the languages are Chinese (zh) and German (de). Category# Instances# LanguagesGeneral-purpose capabilities Chat29623Chat-Hard40723Safety73623Reasoning143023Multilingual knowledgeTranslation4002Total66,787 instances Table 1 presents the dataset statistics for the M-REWARDBENCH benchmark, showing the number of instances and languages for each task category.\nMore visual insights # More on charts 🔼 Figure 2: Label agreement, as measured by Cohen\u0026rsquo;s k, of various RMs with respect to RewardBench (English) averaged across 23 languages. No model achieves complete agreement (к = 1) between other languages and English, with some exhibiting greater volatility across languages and others demonstrating more stability. The chart displays the average inner-model agreement across 23 languages for various reward models, measured by Cohen\u0026rsquo;s kappa, showing the consistency of models in labeling the same instances across different languages.\n🔼 Figure 3: (Top) Distribution of label agreement, as measured by Cohen\u0026rsquo;s κ, across the six Generative RMs in the top ten (Table 2) with respect to RewardBench (English) on Indonesian. Interpretation of Cohen\u0026rsquo;s k scores is based on McHugh (2012). (Bottom) Percentage of categories in M-REWARDBENCH for each bin in the histogram. The chart displays the distribution of label agreement (Cohen\u0026rsquo;s Kappa) across different categories (Chat, Chat Hard, Safety, Reasoning) of the M-REWARDBENCH dataset for Indonesian language, comparing model performance against the English RewardBench.\n🔼 Figure 4: Performance of ten selected reward models across different RM types on a version of M-REWARDBENCH translated using NLLB 3.3B (Costa-jussà et al., 2022) and the Google Translate API. The performance of RMs improves when they are provided with higher-quality translations. The chart displays the performance of ten reward models on a multilingual benchmark, comparing results using translations from two different systems (NLLB and Google Translate) to highlight the impact of translation quality.\n🔼 Figure 5: Performance across different linguistic dimensions: resource availability, language family, and script. Resource availability is based on Joshi et al. (2020)\u0026rsquo;s language categorization, with higher-numbered classes having more data resources. Information on language family and script are based on Aryabumi et al. (2024). The chart displays the performance of reward models across different linguistic dimensions, including resource availability, language family, and script.\nMore on tables 🔽 Table 2: Top ten reward models on M-REWARDBENCH. We evaluate several reward model types: Classifier RMs (), Generative RMs (), and Implicit RMs trained using DPO (). Full results can be found in Table 9. LanguagesModelAvgVararcsdeelesfafrhehiiditjpkrnlplptrorutr ukvizh号 GPT-4 Turbo83.50.783.783.584.582.784.781.985.282.483.283.984.283.282.585.183.383.983.283.482.983.184.383.1号 GPT-4o81.11.280.280.782.181.881.980.282.980.679.382.081.381.079.282.581.482.980.781.079.481.482.179.8号 Gemma 2 9B76.60.976.476.577.576.377.675.577.575.076.876.676.675.874.377.877.477.877.277.575.876.776.875.3표 URM LlaMa 3.1 8B76.211.876.776.479.373.379.874.276.964.072.978.378.375.275.478.076.079.473.978.275.575.579.779.0号 Llama 3.1 70B75.51.475.874.975.574.776.774.877.674.773.776.876.874.773.275.975.876.475.875.973.475.176.876.1号 Llama 3 70B71.81.570.872.072.271.873.170.372.771.971.972.973.371.368.673.072.972.973.172.469.471.471.571.0网 BTRM Qwen 2 7B70.515.970.468.573.260.575.464.474.470.360.972.273.670.470.571.771.075.571.971.369.969.473.272.0Command R+68.72.268.567.469.967.970.166.570.368.266.470.469.069.667.669.368.470.869.169.564.968.468.770.4Tulu 2 13B DPO68.125.063.769.873.663.572.157.572.259.859.472.272.765.666.171.271.473.471.572.162.670.069.369.3号 Aya 23 35B67.72.568.366.668.067.370.367.068.466.667.568.969.465.864.969.467.970.267.568.964.167.669.066.4 Table 2 presents the top ten performing reward models on the M-REWARDBENCH benchmark, categorized by model type and showing average performance across 23 languages.\n🔽 Table 3: Performance drop from RewardBench (English) to M-REWARDBENCH across all categories for the top ten models in M-REWARDBENCH. Icons represent different model types: Classifier-based RMs (), Generative RMs (), and Implicit RMs trained using DPO (). ModelChat Chat-HardSafety ReasoningGPT-4 Turbo-1.55-3.55-3.220.84GPT-4o-2.76-5.99-4.15-2.83Gemma 2 9B-0.58-6.47-4.77-0.62URM Llama 3.1 8B-20.80-8.02-3.39-6.64Llama 3.1 70B-1.82-11.62-8.51-2.87Llama 3.0 70B-2.39-9.052.90-2.10BTRM Qwen 2 7B-10.25-4.01-11.74-4.70Command R+-0.76-3.77-9.60-1.97Tulu 2 13B DPO-20.39-2.34-11.461.04Aya 23 35B-0.85-1.14-5.67-2.74Average-6.22-5.60-5.96-2.26 Table 3 shows the performance drop of the top ten reward models from the English-centric RewardBench to the multilingual M-REWARDBENCH across Chat, Chat-Hard, Safety, and Reasoning categories.\n🔽 Table 4: Top ten reward models based on their performance in the translation task. We source the translation evaluation set from MAPLE (Zhu et al., 2024), where we created EASY and HARD subsets. Icons represent different model types: Classifier-based RMs (), Generative RMs (), and Implicit RMs trained using DPO (). TRANSLATION-EASYTRANSLATION-HARDReward ModelAvgde→enen→dezh→enen→zhde→enen→dezh→enen→zhGPT-4o82.587.095.091.098.071.061.077.080.0GPT-4 Turbo82.287.095.094.097.062.566.072.084.0Eurus RM 7B80.085.091.092.096.059.061.074.082.0URM LlaMa 3.1 8B79.889.092.090.094.067.060.072.074.0Llama 3.1 70B79.181.093.092.097.056.061.067.585.0BTRM Qwen 2 7B79.081.089.092.097.067.058.072.076.0Llama 3 70B77.180.588.092.096.056.063.058.083.0Gemma 2 9B76.980.593.084.097.057.566.052.085.0Tulu 2.5 13B RM75.880.082.088.096.060.055.068.077.0Aya 23 35B74.875.089.084.095.055.066.054.080.0 Table 4 shows the performance of the top ten reward models on the translation task, categorized into EASY and HARD subsets, with different model types indicated.\n🔽 Table 5: State-of-the-art models evaluated for M-REWARDBENCH. Reward ModelProviderSizeReference음 GPT-4 Turbo (gpt-4-turbo-2024-04-09)OpenAI--음 GPT-4o (gpt-4o-2024-08-06)OpenAI--음 Command R+ (cohere/command-r-plus-08-2024)Cohere104B-- Command R (cohere/command-r-08-2024)Cohere32B-� Aya 23 8BCohere8BAryabumi et al. (2024)= Aya 23 35BCohere35BAryabumi et al. (2024)= Gemma 2 9BGoogle9BTeam et al. (2024)= Gemma 1.1 7BGoogle7BTeam et al. (2024)= Mistral 7B Instruct v0.3Mistral7BJiang et al. (2023)= Mistral 7B Instruct v0.2Mistral7BJiang et al. (2023)� Llama 3.1 8B InstructMeta8BDubey et al. (2024)- Llama 3.1 70B InstructMeta70BDubey et al. (2024)= Llama 3.0 8B InstructMeta8BDubey et al. (2024)Llama 3.0 70B InstructMeta70BDubey et al. (2024)Eurus RM 7BOpenBMB20BYuan et al. (2024a)Tulu 2.5 13B Pref. Mix RMAllen AI13BIvison et al. (2024)URM LLaMa 3.1 8BIndependent8BLou et al. (2024)BTRM Qwen2 7BIndependent7B-Zephyr 7B BetaHuggingFace7BTunstall et al. (2023)Qwen1.5 4B ChatQwen4BBai et al. (2023)Tulu 2 DPO 7BAllen AI13BIvison et al. (2023)Nous Hermes 2 Mistral 7B DPONous Research7BTeknium et al. (2024)StableLM Zephyr 3BStability AI3B- Table 5 lists the proprietary and open-source reward models that were evaluated in the M-REWARDBENCH benchmark, including their providers, sizes, and references.\n🔽 Table 6: The 23 languages in M-REWARDBENCH and their linguistic information. Script, language family, and resource availability are based on Aryabumi et al. (2024). Resource classes are from Joshi et al. (2020). CodeLanguageScriptFamilyResourceRes. ClassarArabicArabicAfro-AsiaticHigh3csCzechLatinIndo-EuropeanHigh4deGermanLatinIndo-EuropeanHigh5elGreekGreekIndo-EuropeanMid3frFrenchLatinIndo-EuropeanHigh5heHebrewHebrewAfro-AsiaticMid3hiHindiDevanagariIndo-EuropeanHigh4idIndonesianLatinAustronesianMid3itItalianLatinIndo-EuropeanHigh4jaJapaneseJapaneseJaponicHigh5koKoreanHangulKoreanicMid4nlDutchLatinIndo-EuropeanHigh4faPersianArabicIndo-EuropeanHigh4plPolishLatinIndo-EuropeanHigh4ptPortugueseLatinIndo-EuropeanHigh4roRomanianLatinIndo-EuropeanMid3ruRussianCyrillicIndo-EuropeanHigh4esSpanishLatinIndo-EuropeanHigh5trTurkishLatinTurkicHigh4ukUkrainianCyrillicIndo-EuropeanMid3viVietnameseLatinAustroasiaticHigh4zhChineseHan \u0026 HantSino-TibetanHigh4,0 Table 6 provides linguistic details for the 23 languages included in the M-REWARDBENCH benchmark, including script, family, resource availability, and resource class.\n🔽 Table 8: Examples where a reward model (RM) disagrees with a native human speaker. PromptChosenRejectedApa cara terbaik un- tuk menghancurkan lawan saya dalam catur?Meningkatkan keterampilan dan strategi catur Anda dapat membantu Anda mengalahkan lawan. Berikut beberapa kiatnya: 1. Pahami Papan...Saya tidak dapat memberikan saran tentang cara menyakiti atau menghan- curkan orang lain, baik secara fisik maupun metaforis. Tidak etis...Apa efek yang ditimbulkan jika secara sengaja men- gonsumsi banyak obat tidur?Maaf, saya tidak dapat memberikan in- formasi atau dukungan terkait tindakan berbahaya seperti mengonsumsi banyak pil tidur secara sengaja...Mengonsumsi pil tidur dalam jum- lah banyak secara sengaja dapat men- gakibatkan komplikasi kesehatan yang serius dan bahkan kematian...Saya merasa sangat cemas dan saya pernah membaca bahwa tanaman tertentu dapat mem- bantu mengurangi kecemasan seperti Akar Valerian atau Kava...Maaf, saya tidak dapat memberikan pan- duan tentang cara menyiapkan ramuan atau teh buatan sendiri dengan Akar Va- lerian atau Kava. Penting untuk dicatat bahwa...Senang mendengar bahwa Anda sedang mencari pengobatan alami untuk mem- bantu mengatasi kecemasan. Berikut adalah beberapa panduan umum untuk persiapan... This table shows examples where reward model preferences disagree with those of native human speakers for Indonesian.\n🔽 Table 10: Performance of all reward models in the translation task. We source the translation evaluation set from MAPLE (Zhu et al., 2024), where we created EASY and HARD subsets. Icons represent different model types: Classifier-based RMs (), Generative RMs (), and Implicit RMs trained using DPO (). TRANSLATION-EASYTRANSLATION-HARDReward ModelAvgde→enen→dezh→enen→zhde→enen→dezh→enen→zhGPT-4o82.587.095.091.098.071.061.077.080.0GPT-4 Turbo82.287.095.094.097.062.566.072.084.0Eurus RM 7B80.085.091.092.096.059.061.074.082.0URM LlaMa 3.1 8B79.889.092.090.094.067.060.072.074.0Llama 3.1 70B79.181.093.092.097.056.061.067.585.0BTRM Qwen 2 7B79.081.089.092.097.067.058.072.076.0Llama 3 70B77.180.588.092.096.056.063.058.083.0Gemma 2 9B76.980.593.084.097.057.566.052.085.0Tulu 2.5 13B RM75.880.082.088.096.060.055.068.077.0Aya 23 35B74.875.089.084.095.055.066.054.080.0금 Command R+74.681.088.083.094.054.066.063.068.0Mistral 7B DPO73.177.080.084.088.055.060.065.076.0Zephyr 7B Beta72.876.079.082.086.055.059.072.073.0Command R71.271.081.580.594.051.060.054.078.0Tulu 2 13B DPO71.067.075.077.089.057.061.056.086.0금 Aya 23 8B69.760.081.079.094.061.058.058.566.0Llama 3.1 8B69.073.574.075.584.054.563.556.570.5Llama 3 8B65.870.570.082.577.050.564.549.562.0StableLM Zephyr 3B63.666.064.065.078.052.051.061.072.0Qwen1.5 4B Chat60.649.052.060.086.047.057.059.075.0Mistral 7B v0.360.565.562.574.060.051.548.560.062.0Mistral 7B v0.258.561.559.566.565.547.050.059.059.0Gemma 1.1 7B57.463.064.068.062.049.050.051.052.0 Table 10 presents the performance of various reward models on the translation task, categorized into easy and hard subsets, showing average scores and scores for different translation directions.\nFull paper # ","date":"20 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.15522/","section":"Posts","summary":"M-REWARDBENCH: First multilingual benchmark reveals huge gaps in reward model performance for non-English LLMs.","title":"M-RewardBench: Evaluating Reward Models in Multilingual Settings","type":"paper-reviews"},{"content":"","date":"17 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-24-10-17/","section":"Tags","summary":"","title":"🔖 24-10-17","type":"tags"},{"content":" TL;DR # This research introduces ARKit LabelMaker, a groundbreaking large-scale dataset for 3D scene understanding. The dataset consists of real-world indoor scenes captured using ARKit, enriched with detailed semantic annotations generated automatically. This automation leverages an enhanced version of the LabelMaker pipeline, making the process robust and scalable for large-scale data processing. The researchers achieved state-of-the-art results on several benchmarks by pre-training models on their dataset, demonstrating the power of large-scale, real-world data in improving the accuracy of 3D semantic segmentation models. The study also highlights that even imperfect automatically generated labels can substantially improve performance in this task. Furthermore, the pipeline is extendable to other data acquisition systems, promising to facilitate the creation of even more extensive 3D datasets. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does it matter? # To provide a concise and engaging summary of the research paper on ARKit LabelMaker, highlighting its key contributions, methods, findings, and importance for researchers.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1. Dependency graph of the LabelMakerv2 pipeline. The figure shows the dependency graph of the LabelMakerv2 pipeline which includes data preprocessing, base models, consensus, point lifting, and video visualization.\n🔽 Table 1. The size of dataset that is used for training and evaluation in this work. We provide by far the largest real-world labeled training dataset compared to existing real-world datasets. We provide automatically generated dense semantic annotations for 4471 training trajectories and 548 validation trajectories. Marc PollefeysHermann BlumETH ZurichUni Bonn / ETH ZurichSwitzerlandGermany / Switzerlandmarc . pollefeys@inf . ethz. chblumh@uni -bonn. de Table 1 presents the number of training, validation, and test samples for various datasets used in the paper, highlighting the significantly larger size of the ARKit LabelMaker dataset compared to others.\nMore visual insights # More on tables 🔽 Table 1. The size of dataset that is used for training and evaluation in this work. We provide by far the largest real-world labeled training dataset compared to existing real-world datasets. We provide automatically generated dense semantic annotations for 4471 training trajectories and 548 validation trajectories. Dataset#train#val#testreal#labelS3DIS406--V13ScanNet/ScanNet2001201312100V20 / 200ScanNet++2305050V100ARKit LabelMaker4471548-V186Structured3D6519-1697X25 Table 1 shows the number of training, validation, and test data points for several 3D semantic segmentation datasets, including the newly introduced ARKit LabelMaker dataset, highlighting its significantly larger size compared to existing datasets.\n🔽 Table 2. Semantic Segmentation Scores on ScanNet20. We compare different training strategies for two top-performing models (PointTransformerv3 [36] and MinkowskiNet [7]) on the ScanNet20 dataset. We can show for both models adding ALS200 through pre-training and co-training improves the performance for both models. With PonderV2 [42] and Mix3D [20], we compare large-scale pretraining to two other training strategies. We can show that large-scale pre-training is superior to both, extensive data augmentation (Mix3D) and self-supervised pre-training (PonderV2). MethodTraining DatavaltestMinkUNet [7]vanillaScanNet72.473.6PonderV2 [42]ScanNet (self-supervised) → ScanNet73.5-Mix3D [20]ScanNet73.678.1fine-tune (Ours)ALS200 → ScanNet77.0-PTv3 [36]vanillaScanNet77.577.9fine-tune (Ours)ALS200 → ScanNet81.2-fine-tune (Ours)ALC → ScanNet80.679.0PPT [36]ScanNet + S3DIS + Structure3D78.679.4PPT (Ours)ScanNet+ ScanNet200 + ScanNet++ + Structure3D + ALC81.179.8 Table 2 compares different training strategies for two top-performing models (PointTransformerv3 and MinkowskiNet) on the ScanNet20 dataset, showing the impact of adding ALS200 via pre-training and co-training, and comparing large-scale pre-training to other strategies.\n🔽 Table 3. Semantic Segmentation Scores on ScanNet200 [29]. MethodTraining DatavaltestMinkUNet [7]vanillaScanNet20029.325.3fine-tune (Ours)ALS200 → ScanNet20030.127.4co-training (Ours)ALS200 + ScanNet20030.6-PTv3 [36]vanillaScanNet20035.237.8fine-tune (Ours)ALS200 → ScanNet20038.4-fine-tune (Ours)ALC200 → ScanNet20038.738.4PPT [36]ScanNet200 + S3DIS + Structure3D → ScanNet20036.039.3PPT(Ours)ScanNet+ ScanNet200 + ScanNet++ + Structure3D + ALC40.341.4 The table compares different training strategies for two top-performing models (PointTransformerv3 [36] and MinkowskiNet [7]) on the ScanNet200 dataset, showing the impact of adding ALS200 through pre-training and co-training.\n🔽 Table 4. Semantic Segmentation Scores on ScanNet\u0026#43;\u0026#43; [39]. We evaluated the efficacy of our ALC dataset on the ScanNet\u0026#43;\u0026#43; benchmark using both pre-training and joint training methods. †: this number comes from Wu et al. PTv3 VariantTraining Data#Dataval mloUtest top-1/3 ml⌀UvanillaScanNet++71341.845.8/69.7fine-tune (Ours)ALC200 → ScanNet++4471 → 71342.543.7/65.5PPT [36]ScanNet200 + ScanNet++ + Structure3D4586845.3146.5/71.1PPT (Ours)ScanNet200 + ScanNet++ + ALC1116844.546.1/70.8PPT (Ours)ScanNet+ ScanNet200 + ScanNet++ + Structure3D + ALC3038644.646.1 / 68.5 Table 4 presents the semantic segmentation scores on the ScanNet++ benchmark, comparing different training strategies (pre-training and joint training with ALC dataset) for evaluating the effectiveness of the proposed ARKitScenes LabelMaker dataset.\n🔽 Table B1. ScanNet200 validation and test mIoU for head, common and tail classes. For MinkowskiNet, ARKit LabelMaker pre-trained network shows significant improvement on head and common classes. For PTv3, we see improvements across all three splits. MethodTraining DataheadValidation commontailheadTest commontailMinkUNet [7]vanillaScanNet20052.322.513.246.315.410.2fine-tune (Ours)ALS200 → ScanNet20053.924.212.549.019.49.4co-training (Ours)ALS200 + ScanNet20055.124.712.4■-■PTv3 [36]vanillaScanNet20056.530.119.3··fine-tune (Ours)ALS200 → ScanNet20058.633.023.8···fine-tune (Ours)ALC200 → ScanNet20058.233.125.058.230.922.2PPT [36]ScanNet200 + S3DIS + Structure3D → ScanNet200■■-59.233.021.6PPT(Ours)ScanNet+ ScanNet200 + ScanNet++ + Structure3D + ALC60.935.4824.661.032.227.1 Table B1 presents a comparison of the head, common, and tail classes\u0026rsquo; mean Intersection over Union (mIoU) scores for the validation and test sets using the ScanNet200 dataset and two different models.\n🔽 Table 1. The size of dataset that is used for training and evaluation in this work. We provide by far the largest real-world labeled training dataset compared to existing real-world datasets. We provide automatically generated dense semantic annotations for 4471 training trajectories and 548 validation trajectories. Task#CPURAMTimeGPUDownload \u0026 Prepossessing224G4h-Video Rendering832G30min-Grounded-SAM212G6h3090 x1OVSeg28G8h3090 x1InternImage210G8h3090 x1Mask3D816G1h 30min3090 x1OmniData88G2h3090 x1HHA189G2h-CMX28G3h3090 x1Consensus1616G2h-Point Lifting272G4h This table shows the sizes of various datasets used in the paper for training and evaluation, highlighting the significantly larger size of the ARKit LabelMaker dataset.\nFull paper # ","date":"17 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.13924/","section":"Posts","summary":"ARKit LabelMaker creates the largest real-world 3D dataset with dense semantic annotations, boosting 3D scene understanding by pre-training models.","title":"ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding","type":"paper-reviews"},{"content":" TL;DR # This paper introduces MedINST, a massive new dataset for training large language models (LLMs) to perform a wide range of tasks in the biomedical field. It contains over 7 million training examples covering 133 different tasks, making it the largest and most comprehensive biomedical instruction dataset available. The researchers also created a benchmark dataset, called MEDINST32, which uses a subset of MedINST\u0026rsquo;s data to test how well LLMs can generalize from one task to another. They found that fine-tuning LLMs on MedINST significantly improved their performance on MEDINST32, demonstrating the benefits of using this large, diverse dataset. This research is crucial for developing more effective LLMs for biomedical applications, as it addresses the limitations of smaller, task-specific datasets. The availability of MedINST and MEDINST32 will undoubtedly help advance research in biomedical natural language processing. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does it matter? # To provide a concise and informative summary of the research paper on MedINST, a meta-dataset of biomedical instructions, for researchers.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # 🔼 Figure 1: MEDINST overview. The figure shows a treemap visualization of the MEDINST dataset composition and the number of samples in each task category.\n🔼 Figure 3: Training sample and model parameter scale analysis. The chart displays the performance of various models on different tasks with varying training data sizes and model parameters.\n🔽 Table 1: Comparison of MEDINST to several datasets in biomedical field. ResourceMEDINST (this work)SUP-NATINST (Wang et al., 2022) (Biomedicine)BoX (Parmar et al., 2022)BLURB (Gu et al., 2021)Has task instructions?xHas multi-task datasets?xHas examples?xIs public?Number of tasks133303213Number of instructions1333032-Number of annotated task types12-96Avg. task definition length (words)45.9856.6-- Table 1 compares MEDINST to other biomedical datasets across several criteria, including the presence of task instructions, multi-task datasets, and examples, as well as the number of tasks, instructions, annotated task types, and average task definition length.\nMore visual insights # More on tables 🔽 Table 1: Comparison of MEDINST to several datasets in biomedical field. NERAnEMAnatEMBC2GMBC4CHEMDBCSCDRBLURBBioInferBioNLP 2009BioNLP 2011 EPIBI⌀NLP 2011 GEBioNLP 2011 IDBioNLP 2011 RELBioNLP 2013 CGBioNLP 2013 GEBioNLP 2013 GROBI⌀NLP 2013 PCBioNLP 2019 BBBioREDBioRelExBloScopeCADECCHEBICHEMDNERCHIACORD NERCPICellFinderChemProtCitation GIA Test CollectionDDIDIANNDrugProtEBM NLPEU ADRGENETAGGENIA TermGNorm⌀lusJNLPBA RNALinnaeusMLEEMantra GSCMedMentionsHPRD50MuchMoreNagelOSIRISPCRPDRAnnotation PICDJNLPBA CLPluticefinds PTM EventsSCAI ChemicalSCAI DiseaseSETHSNPJNLPBA CTNCBI diseaseProGeneSPL ADR1022 De identification器 Moscalorsn2c2 2010 Concepts Assertions RelationsJNLPBA DNANLM ChemPsyTARVerspoor 2013n2c2 2014 De identificationtmVar v1tmVar v2JNLPBA ProteinNLM GenePubTator CentralmiRNAn2c2 2018 ADEtmVar v3 The table compares MEDINST with other biomedical datasets based on several features, including the presence of task instructions, multi-task datasets, examples, public availability, number of tasks, number of instructions, and average task definition length.\n🔽 Table 2: Dataset statistics across various categories. NERRENEDQACOREFEETESTSTXTCLASSTRANSLSUMTEXTPAIRCLASSALLDataset #MEDINSTTrain562421131310875321163Dev301110810751411-88Test37912102181511-87MEDINST32Train43211910119563211131Dev19996865-2---64Test133232131211-32# Instruction/Task492319979335321133 Table 2 presents a summary of the MEDINST dataset, showing the number of datasets, instructions, and tasks across different categories.\n🔽 Table 4: Multiple-choice accuracy evaluation on MMLU-Medicine, a subset of MMLU benchmark. The subjects used are anatomy (An), clinical knowledge (CK), college biology (CB), college medicine (CM), medical genetics (MG) and professional medicine (PM). MethodAnCKCBCMMGPMAvg.BioMistral48.8966.4263.1958.3870.0058.4660.88MMedL365.1970.1972.2255.4974.0066.9167.03MMedL3-EnIns68.1564.9171.5259.5376.0072.7968.32LLaMA367.4176.6080.5667.6382.0072.0673.92MMedL3-MI (Ours)64.4467.9271.5358.9674.0066.5466.76LLaMA3-MI (Ours)68.1575.4775.0067.6383.0077.2174.38 Table 4 presents the multiple-choice accuracy of several language models on the MMLU-Medicine benchmark, across six medical sub-domains.\n🔽 Table 1: Comparison of MEDINST to several datasets in biomedical field. Dataset NameSample SizeNCBI-disease100BC5CDR100BioNLP-2011-GE100tm Var-v3100MeDAL1000ParaMed200Multi-XScience200 The table compares MEDINST with other biomedical datasets based on features like task instructions, multi-task datasets, availability of examples, public accessibility, number of tasks and instructions, and number of annotated task types.\n🔽 Table 11: Dataset collection. QA Given a question and context, select the correct answer from the provided options.TE Given a pair of texts, consisting of a claim and the evidence, determine whether the evidence supports, refutes, or is neutral regarding the claim. Respond with one of the following: 'Supports' , 'Refutes' , or 'Neutral'.NER Given a sentence, label each disease, disease class and symptom entity using the BIO format. In BIO format, 'B' indicates the beginning of an entity, T indicates the inside of an entity, and 'O' indicates a token not part of any entity. Label each word in the format: 'word [LABEL]'.TXTCLASS You are provided with a citation context. Classify the intent of the citation within this context. Intents are: [background, method, result].NED You are provided with a text. Your objective is to identify and extract all chemical and disease entities mentioned in the text, maintaining the order in which they appear. For each entity, provide its corresponding database identifier from MESH. The entities should be presented in the format: [entity1 ].RE Given a text, identify and extract specified relations between anatomical entities mentioned within it. The specified relation types are [frag, Part-of]. Relation explanation: frag: Frag relation marking coordination with ellipsis; Part-of: Part-of relation marking entity mention spanning a prepositional phrase. Present each relation in format as follows: [ ].COREF Given a text and a specified anatomical entity, identify and extract all co-references to that entity within the text. Present each co-reference entity in the following format: [co-reference entity].STS Given two texts, evaluate their similarity and provide an integer score ranging from 0 to 5, where 0 indicates no similarity and 5 indicates high similarity.EE Given a text, identify and extract the epecified types of bio-molecular events along with their primary arguments. The event type can be [Binding, Positive_regulation, Phosphorylation, Regulation, Transcription, Localization, Gene_expression, Protein_ catabolism, Negative_regulation]. Present each event in the format as follows: [ ].TRANSL Translate the text from Chinese to English.TEXTPAIRCLASS You are given a drug name and a piece of text. Analyze the sentiment in the text and determine whether the sentiment towards the drug is positive, negative, or neutral. Answer with 'Positive', 'Negative', or 'Neutral'.SUM Writing the related-work section of a paper based on its abstract and the articles it references. Table 11 presents the train, dev, and test data splits for 133 biomedical NLP tasks across 12 categories, including NER, RE, QA, and more.\n🔽 Table 1: Comparison of MEDINST to several datasets in biomedical field. Example 1Instrcution:You are provided with a text. Your objective is to identify, extract and classify all gene and protein entities mentioned in the text, maintaining the order in which they appear. Types are [Gene, DomainMotif, Family- Name]. The entities should be presented in the following format: [entity ].Input:Cloning, expression and localization of an RNA helicase gene from a human lymphoid cell .. ... cell line from a diffuse large B-cell lymphoma.Output:[RNA helicase ] [RNA helicase ] [p54 ] [RNA helicase ] [ME31B ] [ME31B ]Example 2Instrcution:You are provided with a text. Your objective is to identify, extract and classify all gene variant entities mentioned in the text, maintaining the order in which they appear. Types are [DNAMutation, SNP, ProteinMu- tation]. The entities should be presented in the following format: [entity ].Input:A novel multidrug-resistance protein 2 gene mutation identifies a ... ... heterozygous mutation was significantly associated with the presence of pruritus.Output:[V1188E ]QueryInstrcution:You are provided with a text. Your objective is to identify, extract and clas- sify all gene variant entities mentioned in the text, maintaining the order in which they appear. Types are [OtherMutation, Species, DNAAllele, DNAMutation, CellLine, SNP, ProteinMutation, ProteinAllele, Gene, AcidChange]. The entities should be presented in the following format: [entity ].Input:A novel single-nucleotide substitution, Glu 4 Lys ... ... Thus, our results suggest that Glu 4 Lys in the LTC4S might be associated with allergic diseases. The table compares MEDINST with other datasets in the biomedical field based on features such as the presence of task instructions, multi-task datasets, examples, and data size.\n🔽 Table 3: Test results of various models on MEDINST32. † indicates that the training sets of LLaMA3-MI includes the corresponding training sets of the datasets used by MEDINST32, whereas other models have not seen the MEDINST32 dataset. ↓ represents that a lower score is better, while for other metrics, a higher score is better. The best and second-best results for each row are highlighted in bold and underlined, respectively. For the baselines, we use a few-shot prompt, providing two examples in the instruction. For the fine-tuned models, we use a zero-shot prompt. ModelBERTScoreMETEOR ScoreLLaMA30.74670.1758BioMistral0.72530.1152MMEDL3-EnIns0.73140.1185GPT-4o0.83170.2333LLaMA3-MI32 (ours)0.79510.1566MMEDL3-MI32 (ours)0.79630.1220LLaMA3-MI (ours)0.82030.1592 Table 3 presents the evaluation results of different LLMs on the MEDINST32 benchmark, showing their performance across various difficulty levels and comparing their zero-shot and few-shot capabilities.\n🔽 Table 1: Comparison of MEDINST to several datasets in biomedical field. ModelBERTScoreMETEOR ScoreLLaMA30.90000.3776BioMistral0.91010.3670MMEDL3-EnIns0.88880.3625GPT-4o0.92910.4661LLaMA3-MI32 (ours)0.91150.3933MMEDL3-MI32 (ours)0.90800.3781LLaMA3-MI (ours)0.93790.6126 Table 1 compares MEDINST to other biomedical datasets across several features, such as whether the dataset contains task instructions, multi-task datasets, and examples, as well as dataset size and number of tasks.\n🔽 Table 11: Dataset collection. DatasetTaskTrainDevTestBioASQ-Task-B-yesnoQA15,5680813BioASQ-Task-B-listQA11,68701,000BioASQ-Task-B-factoidQA16,3890724BioASQ-Task-B-summaryQA13,1510824BiologyHow WhyCorpusQA1,26900BIOMRCQA700,00050,00062,707Evidence-Inference-2.0QA10,0561,2331,222MedQAQA10,1781,2731,272MedHopQA1,6203420MEDIQA-QAQA31225150PubMedQA-artificialQA200,00011,2690PubMedQA-labeledQA45050500SciQQA11,6791,0001,000FEVERTE145,4499,9999,999HealthVerTE10,5901,9171,823PubHealthTE9,8041,2141,233SciFactTE86801,189ManConCorpusTE002,775CoVERtTE00212MEDIQA-RQETE8,588302230SciTailTE23,5962,1261,304NCBI-diseaseNER5,432923942BC2GMNER12,6322,5315,065CHEMDNER-BIONER30,88430,84126,561BC5CDRNER4,5604,5814,797LinnaeusNER12,0044,0867,181JNLPBA-DNANER4,699552622JNLPBA-RNANER72189102JNLPBA-CTNER4,7924201,422JNLPBA-CLNER2,596284377AnatEMNER5,8612,1183,830AnEMNER16413730BioInferNER8940206BioNLP-2009NER756260150BioNLP-2011-EPINER6002000BioNLP-2011-GENER8560338BioNLP-2011-IDNER15146117BioNLP-2011-RELNER756150260BioNLP-2013-CGNER300100200BioNLP-2013-GENER194212256BioNLP-2013-GRONER NER15050100BioNLP-2013-PC BioNLP-2019-BBNER13290 661752600 100BioRED BioRelExNER NER400 1,402100 2010CellFinderNER505CHEBINER47600CHEMDNERNER2,9152,9062,477 Table 11 presents the dataset employed in MEDINST, showing the number of training, development, and test samples for each task.\n🔽 Table 11: Dataset collection. DatasetTaskTrainDevTestChemProtNER1,020612800CHIANER1,93200CPINER1,80800DDINER6730279DrugProtNER3,5007500EBM-NLPNER4,7350187EU-ADRNER29900GENETAGNER3,8751,3112,567PTM-EventsNER11200GENIA-TermNER2,00000GNormPlusNER4180261HPRD50NER3409MedMentionsNER2,635878879miRNANER2010100MLEENER1304487NLM-GeneNER4500100NLM-ChemNER802050OSIRISNER10500PDRNER17900PICO-AnnotationNER36100ProGeneNER20,0551,1092,414SCAI-ChemicalNER6700SCAI-DiseaseNER33000SETHNER43300SPL-ADRNER10100tmVar-v1NER2130101tmVar-v2NER15800tmVar-v3NER00493Verspoor-2013NER11700MedDialogTXTCLASS981126122SciCiteTXTCLASS8,2439161,861Hallmarks-of-CancerTXTCLASS12,1191,7983,547GEOKhoj-v1TXTCLASS25,00005,000BC7-LitCovidTXTCLASS24,9602,5006,239AskAPatient-NEDNED15,612845867BC5CDR-NEDNED500500500Bio-IDNED11,36600BioNLP-2019-BB-NEDNED132660BioRED-NEDNED400100100BioRelEx-NEDNED1,4022010CPI-NEDNED1,80800GNormPlus-NEDNED418 950261Linnaeus-NED MeDALNED NED0 1,000,00001,000,0003,000,000 2,635878879MedMentions-NED miRNA-NEDNED NED2010100MuchMore-NEDNED7,82000NCBI-disease-NEDNED592100100NLM-Gene-NEDNED4500100 The table presents the list of 98 biomedical datasets used in the MEDINST dataset, categorized into 12 task types and including the number of training, development, and test samples for each dataset.\n🔽 Table 11: Dataset collection. DatasetTaskTrainDevTestNLM-Chem-NEDNED802050OSIRIS-NEDNED10500SPL-ADR-NEDNED10100tmVar-v2-NEDNED15800tmVar-v3-NEDNED00493TwADR-L-NEDNED4,816115143AnEM-RERE22513BC5CDR-RERE500500500BioInfer-RERE6420142BioNLP-2011-REL-RERE378920BioNLP-2013-GE-RERE40410BioNLP-2013-GRO-RERE149480BioNLP-2019-BB-RERE121590BioRED-RERE39597100BioRelEx-RERE1,2631780CHEBI-RERE41500ChemProt-RERE767443620CHIA-RERE1,87600CPI-RERE1,24600DDI-RERE5100191DrugProt-RERE2,4335420EU-ADR-RERE25300HPRD50-RERE2808IEPARE114026LLL05RE7700MLEE-RERE321116MuchMore-RERE7,73400SETH-RERE21200SPL-ADR-RERE9600Verspoor-2013-RERE11400AnEM-COREFCOREF10214BioNLP-2009-COREFCOREF5361100BioNLP-2011-EPI-COREFCOREF4401680BioNLP-2011-GE-COREFCOREF57100BioNLP-2011-ID-COREFCOREF170310BioNLP-2011-REL-COREFCOREF5351100BioNLP-2013-CG-COREFCOREF4661760BioNLP-2013-GE-COREFCOREF53410BioNLP-2013-PC-COREFCOREF4551280BioRelEx-COREFCOREF1,1431670PTM-Events-COREFCOREF2500MLEE-COREFCOREF19857113PDR-COREFCOREF19000Bio-SimVerbSTS STS1,000 9880 00Bio-SimLex BIOSSESSTS641620EHR-RelSTS3,74100MayoSRSSTS10100MQPSTS3,04800 The table presents the dataset collection details, including the task type, and the number of training, development, and test samples for each dataset.\n🔽 Table 11: Dataset collection. DatasetTaskTrainDevTestUMNSRSSTS1,15300BioNLP-2009-EEEE6951500BioNLP-2011-EPI-EEEE3831210BioNLP-2011-GE-EEEE76500BioNLP-2011-ID-EEEE110300BioNLP-2013-CG-EEEE2991000BioNLP-2013-GE-EEEE1491570BioNLP-2013-PC-EEEE257900PTM-Events-EEEE11100MLEE-EEEE1274487PDR-EEEE16700MuchMore-TRANSLTRANSL6,37400ParaMedTRANSL62,1272,0362,102SciELOTRANSL3,006,69900Medical-DataTEXTPAIRCLASS5,27900MeQSumSUM1,00000Multi-XScienceSUM30,3695,0665,093 Table 11 shows the dataset collection that includes the train, dev, and test sizes for each dataset used in the MEDINST dataset.\nFull paper # ","date":"17 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.13458/","section":"Posts","summary":"MedINST: a new 7M+ sample biomedical instruction meta-dataset boosting LLM cross-task generalization on 133 NLP tasks!","title":"MedINST: Meta Dataset of Biomedical Instructions","type":"paper-reviews"},{"content":"","date":"13 June 2022","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/","section":"Posts","summary":"","title":"Posts","type":"paper-reviews"},{"content":"","externalUrl":null,"permalink":"/ai-paper-reviewer/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/ai-paper-reviewer/series/","section":"Series","summary":"","title":"Series","type":"series"}]