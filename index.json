[{"content":" Welcome to AI Paper Reviewer! AI Paper Reviewer is a unique platform dedicated to providing insightful reviews and summaries of artificial intelligence research papers. The content is entirely generated by advanced AI systems, offering a novel approach to understanding and disseminating complex scientific literature.\nMission The mission is to make cutting-edge AI research more accessible to a wider audience. By leveraging the power of AI, we aim to:\nSummarize complex research papers in clear, concise language Highlight key findings and their potential implications Provide context and connections to related work in the field Foster a deeper understanding of AI advancements among researchers, students, and enthusiasts How It Works All the pipeline is implemented in this repo, but briefly:\nScanning the latest AI research papers collected from Hugging Face Daily Papers. Extracting visual information (figures, charts, tables) from the papers. Generating descriptive text for the visual information. Generating summaries and reviews of the papers. This project leverages the following tech stack:\nUpstage\u0026rsquo;s Document Parse: Extracting visual information from the papers. Google\u0026rsquo;s Gemini 1.5 Pro: Extracting visual information from the papers if Document Parse is not available. Google\u0026rsquo;s Gemini 1.5 Flash: Generating summaries and reviews of the papers. Google\u0026rsquo;s Gemini 1.5 Flash 8B: Double checking if visual information is correctly extracted. Hugo: Static site generator. Blowfish: Theme for Hugo. Disclaimer While we strive for accuracy and clarity, please note that all content on this site is AI-generated. We encourage readers to refer to the original papers for the most authoritative information.\nWe hope you find AI Paper Reviewer a valuable resource in your AI learning journey!\n","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/about/","section":"AI Paper Reviews by AI","summary":"\u003ch1 class=\"relative group\"\u003eWelcome to AI Paper Reviewer! \n    \u003cdiv id=\"welcome-to-ai-paper-reviewer\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n\u003c/h1\u003e\n\u003cp\u003eAI Paper Reviewer is a unique platform dedicated to providing insightful reviews and summaries of artificial intelligence research papers. The content is entirely generated by advanced AI systems, offering a novel approach to understanding and disseminating complex scientific literature.\u003c/p\u003e","title":"About This Project","type":"page"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/","section":"AI Paper Reviews by AI","summary":"","title":"AI Paper Reviews by AI","type":"page"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/","section":"Paper Reviews by AI","summary":"","title":"Paper Reviews by AI","type":"paper-reviews"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-microsoft-research/","section":"Tags","summary":"","title":"üè¢ Microsoft Research","type":"tags"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-purdue-university/","section":"Tags","summary":"","title":"üè¢ Purdue University","type":"tags"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-uc-berkeley/","section":"Tags","summary":"","title":"üè¢ UC Berkeley","type":"tags"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-uc-san-diego/","section":"Tags","summary":"","title":"üè¢ UC San Diego","type":"tags"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/ai-applications/","section":"Tags","summary":"","title":"AI Applications","type":"tags"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/ai-generated/","section":"Categories","summary":"","title":"AI Generated","type":"categories"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.21647 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rShanchao Liang et el. 2024-10-30 ‚Üó arXiv ‚Üó Hugging Face TL;DR # Large language models (LLMs) have shown promise in code generation, but existing benchmarks are limited to simple tasks, not reflecting real-world complexities. This gap motivated the development of REPOCOD, a new comprehensive benchmark with 980 tasks from real-world projects. These tasks are significantly more complex and require a deeper understanding of project context than previous benchmarks.\nREPOCOD was used to evaluate 10 LLMs, and none achieved over 30% accuracy on the complex tasks. The paper\u0026rsquo;s contributions include the creation of REPOCOD, a new benchmark for evaluating LLMs\u0026rsquo; code generation capabilities, providing a clearer understanding of LLMs\u0026rsquo; limitations in real-world settings. It emphasizes the need for development of more robust models and improved evaluation metrics to facilitate advancements in the field.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # This paper is crucial because it introduces a new benchmark, REPOCOD, rigorously evaluating large language models\u0026rsquo; (LLMs) real-world code generation capabilities. It reveals significant limitations of current LLMs in handling complex, project-level tasks, highlighting the need for improved models and better evaluation methods in software engineering research. This work opens up new avenues for developing more robust and practical LLMs and influences the direction of future research in this field.\nVisual Insights # Shanchao LiangYiran HuNan JiangLin TanPurdue UniversityPurdue UniversityPurdue UniversityPurdue Universityliang422@purdue.eduhu954@purdue.edujiang719@purdue.edulintan@purdue.edu üîº Table 1 presents detailed statistics for instances from each repository in REPOCOD, categorized by context complexity types (repository-level, file-level, and self-contained), showing the number of natural language tokens in function descriptions, canonical solutions, average cyclomatic complexity, and number of target functions.\nread the caption Table 1: Basic statistics of REPOCOD, with details broken down by each collected repository. More visual insights # More on tables DatasetRepository-levelFile-levelSelf-containedTotal#NL#GTCyclo.#Funcs.#NL#GTCyclo.#Funcs.#NL#GTCyclo.#Funcs.#NL#GTCyclo.#Funcs.astropy274.9446.49.410399.4407.18.837291.5284.87.938336.5357.08.585datasets497.1308.49.119297.3196.74.820311.9135.33.620366.6211.95.859flask448.0215.06.52326.2152.35.213231.299.43.628270.0120.84.243more-itertools242.887.32.76300.2136.84.323233.496.35.057252.0106.54.686plotly.py1806.03393.0132.011366.9932.223.5191661.7605.123.9561589.9723.525.376pylint---0163.9432.115.29176.4193.17.017172.0275.89.826scikit-learn222.1398.97.7208262.9258.35.343204.3220.95.063224.1344.06.8314seaborn349.5426.010.36201.2236.06.442240.8211.66.730227.9241.36.878sphinx---0191.7606.617.712263.8127.84.021237.5301.98.933sympy918.6347.68.85903.9586.717.658837.8277.68.634881.5466.014.097xarray---0902.1418.511.440593.0160.55.343742.0284.88.283Total269.4396.78.3257537.6394.910.6316522.8241.48.2407461.1331.69.0980 üîº Table 1 presents detailed statistics for instances from each repository, categorized by context complexity types and including the number of tokens in target function descriptions, number of tokens in canonical solutions, average cyclomatic complexity of the canonical solution, and number of target functions.\nread the caption Table 1: Basic statistics of REPOCOD, with details broken down by each collected repository. Datasets#Instances#TokensCyclo.#TestsCrossCodeEval2,66513.2N/AN/ARepoBench23,56113.7N/AN/ALong-Code-Arena32,80312.0N/AN/ACoderEval230108.24.71-ClassEval100123.72.8933.1REPOCOD980331.69.00313.49 üîº Table 1 presents basic statistics of the REPOCOD benchmark, including the number of instances, tokens, cyclomatic complexity, and number of functions categorized by context complexity type (repository-level, file-level, and self-contained) for each of the 11 repositories.\nread the caption Table 1: Basic statistics of REPOCOD, with details broken down by each collected repository. DatasetRepository-levelFile-levelSelf-containedTotalCoderEval2312384230ClassEval0314153467REPOCOD257316407980 üîº Table 3 shows the distribution of code generation tasks in three complexity levels (repository-level, file-level, and self-contained) across three code generation benchmarks: CoderEval, ClassEval, and REPOCOD.\nread the caption Table 3: Data context complexity distribution of code generation benchmarks. RepoCod TaskPromptLLM Output: : dupl \"AssertionError: Lists differ: \" Test Cases ExecutionRepository Snapshot more-itertools: more_ itertools init__¬∑py more . py recipes . py ¬∑ README . rst Target Function more-itertools : more itertools : more ¬∑ py :partitions Relevant Tests test , more ¬∑ py : : PartitionsTest: : test_ duplicates test orderYou will be provided a function signature and the documentation, } your task is to complete the function body . - These are a list of files that contain the related classes and functions for you to refer to. ## File path: more_ itertools/more ¬∑py def set_partitions(iterable, k, min_size=None, max_size=None): ## File path: more_ itertools/recipes. py def partition(pred, iterable): This is the file containing the target function to be generated. ## File path: more_ itertools/more ¬∑py ### Target function to compl ete def partitions(iterable): ‚åÄ‚åÄ \" Yield all possible order-preserving partitions of *iterable* \u003e\u003e\u003e iterable = I abc , \u003e\u003e\u003e for part in partitions(iterable): , join(p) for p in part]) ... print([' . ‚åÄ‚åÄ \"Sytem Prompt iterable = list(iterable) n = len(iterable) def generate_partitions(start=0): if start == n: yield [] return Context for end in range(start + 1, n + 1): for rest in generate_partitions(end): yield [iterable[start:end]] + rest yield from generate_partitions() ‚ë£ Insert Completion ‚ë§ Evaluation Result more-itertools: test_ more. py Function Description more_itertools : PartitionsTest init__ . py : test ‚åÄ icates more.py Error : Target recipes.py ‚ñ† README . rst üîº Table 1 presents detailed statistics for instances from each repository categorized by context complexity types (repository-level, file-level, and self-contained), including the number of tokens in target function descriptions, number of tokens in canonical solutions, average cyclomatic complexity of the canonical solution, and number of target functions.\nread the caption Table 1: Basic statistics of REPOCOD, with details broken down by each collected repository. ModelsBM25DenseCurrent-FileCodeLlama-7B10.7110.415.71CodeLlama-34B12.3512.769.59DeepSeekCoder-6.7B13.9814.0810.92DeepSeekCoder-33B16.7317.1414.90OpenCodeInterpreter-6.7B12.1412.4513.16OpenCodeInterpreter-33B15.3116.3318.27Claude 3.5 Sonnet14.3917.4519.80DeepSeek-V2.518.4720.7127.04GPT-4o-Mini15.1015.0018.67GPT-4o27.3527.0426.84 üîº The table presents the pass@1 scores of state-of-the-art large language models on the REPOCOD benchmark under three different retrieval settings.\nread the caption Table 4: Pass@1 of SOTA LLMs on REPOCOD. ModelsBM25DenseCurrent-FilePassFailPassFailPassFailCodeLlama-7B43.616.138.316.825.018.7CodeLlama-34B51.024.763.322.819.028.9DeepSeekCoder-6.7B33.88.331.68.616.411.3DeepSeekCoder-33B72.111.177.69.614.522.4OpenCodeInterpreter-6.7B1.41.41.41.41.41.4OpenCodeInterpreter-33B1.40.91.40.91.40.9DeepSeek-V2.52.71.62.51.62.91.4Claude 3.5 Sonnet18.64.021.92.77.85.7GPT-4o-Mini3.62.73.92.69.91.2GPT-4o1.31.21.31.22.11.0 üîº Table 1 presents detailed statistics for instances from each repository, categorized by context complexity types: repository-level, file-level, and self-contained, showing the number of tokens in target function descriptions, number of tokens in canonical solutions, average cyclomatic complexity of the canonical solution, and number of target functions.\nread the caption Table 1: Basic statistics of REPOCOD, with details broken down by each collected repository. Models0[0,0.5)[0.5,1)1CodeLlama-7B6.050.0013.1613.27CodeLlama-34B5.042.5021.0516.33DeepSeekCoder-6.7B5.545.0013.1621.43DeepSeekCoder-33B7.815.0015.7925.51OpenCodeInterpreter-6.7B6.050.0015.7916.33OpenCodeInterpreter-33B9.075.0015.7920.41DeepSeek-V2.510.335.0018.4226.53Claude 3.5 Sonnet6.555.007.8929.59GPT-4o-Mini6.555.0018.4226.53GPT-4o14.617.5026.3235.71 üîº The table presents the pass@1 performance of ten state-of-the-art large language models on the REPOCOD benchmark under three different retrieval settings.\nread the caption Table 4: Pass@1 of SOTA LLMs on REPOCOD. RepositorySparseDenseastropy0.280.25datasets0.250.33flask0.300.45more-itertools0.110.33plotly.py0.070.42pylint0.460.46scikit-learn0.070.05seaborn0.240.26sphinx0.060.12sympy0.220.23xarray0.120.29Total0.130.15 üîº Table 1 presents detailed statistics for instances from each repository, categorized by context complexity types: repository-level, file-level, and self-contained, showing the number of tokens in target function descriptions, number of tokens in canonical solutions, average cyclomatic complexity of the canonical solution, and number of target functions.\nread the caption Table 1: Basic statistics of REPOCOD, with details broken down by each collected repository. Settingpylintsympysphinxseabornflaskmore-itertoolsscikit-learnxarraydatasetsplotly.pyastropyTotalCodeLlama-7B7.697.229.0923.0837.2113.955.106.0222.036.585.8810.41CodeLlama-34B0.004.1218.1821.7932.5624.425.7316.8718.6414.4710.5912.76DeepSeekCoder-6.7B7.696.1915.1526.9241.8612.796.3716.8716.9525.0014.1214.08DeepSeekCoder-33B3.857.2227.2728.2137.2111.637.3215.6633.9043.4216.4717.14OpenCodeInterpreter-6.7B3.854.1212.1216.6741.8618.606.3710.8416.9521.0512.9412.45OpenCodeInterpreter-33B11.547.2221.2120.5141.8629.078.2813.2525.4223.6816.4716.33Claude 3.5 Sonnet19.2311.3415.1525.6439.5338.376.059.6427.1236.8410.5917.45DeepSeek-V2.515.3813.4027.2730.7741.8636.059.8716.8730.5135.5316.4720.71GPT-4o-Mini11.5411.3415.1525.6439.5318.606.0510.8433.9025.009.4115.00GPT-4o19.2315.4627.2737.1858.1443.0213.6924.1047.4644.7423.5327.04 üîº Table 1 presents detailed statistics for instances from each repository, categorized by context complexity types, showing the number of tokens in target function descriptions, number of tokens in canonical solutions, average cyclomatic complexity of the canonical solution, and number of target functions.\nread the caption Table 1: Basic statistics of REPOCOD, with details broken down by each collected repository. Full paper # ","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.21647/","section":"Paper Reviews by AI","summary":"REPOCOD benchmark exposes current LLMs\u0026rsquo; struggles with real-world, complex code generation tasks, pushing the field towards building stronger, more context-aware models.","title":"Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'","type":"paper-reviews"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.22304 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rYihe Deng et el. 2024-10-30 ‚Üó arXiv ‚Üó Hugging Face TL;DR # Current methods for training LLMs on mathematical reasoning often struggle to generate detailed and accurate reasoning traces. Human-generated traces are expensive and often insufficient, while simply using model-generated traces leads to subpar performance. This paper tackles this problem by focusing on generating better reasoning traces.\nThe proposed solution, Flow-DPO, uses an online multi-agent learning approach. Multiple LLMs work together iteratively to build a solution, with their performance constantly improved through online DPO learning and rollouts. This method outperforms existing single-model approaches, demonstrating the effectiveness of collaborative learning for generating high-quality reasoning traces and consequently improving LLMs\u0026rsquo; mathematical reasoning abilities.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # This paper is crucial for researchers working on Large Language Models (LLMs) and mathematical reasoning. It introduces a novel online multi-agent learning approach, Flow-DPO, that significantly improves the generation of high-quality reasoning traces. This addresses a key challenge in LLM fine-tuning for mathematical tasks, offering a more efficient and effective alternative to existing methods. The findings open new avenues for enhancing LLMs\u0026rsquo; reasoning capabilities and have implications for various AI applications.\nVisual Insights # üîº Figure 2 illustrates the online DPO learning process with rollouts, showing how random rollouts at each output node generate DPO pairs for training.\nread the caption Figure 2: Illustration of the DPO training with rollouts. At each node of the initial generation, we do a random rollout that is different from the original node and continue generation to a final answer. A pair that leads to different answers (correct and incorrect) is considered a DPO training data. üîº The chart displays the progressive validation accuracy of the Flow during online DPO training with and without rollouts for Llama-3-Instruct and Phi-3-Medium models on the MetaMath dataset, comparing it to the zero-shot performance of a single LLM.\nread the caption Figure 3: Progressive validation accuracy of Llama-3-Instruct on MetaMath. Figure 4: Progressive validation accuracy of Phi-3-Medium on MetaMath. ModelMethodGSM8KMATHLlama-3-Instruct (8B)0-shot48.922.3SFT (ground-truth)67.225.1SFT (self-generated)68.824.2SFT (Flow-generated)71.327.8Phi-3-Medium (14B)0-shot-35.4SFT (ground-truth)-36.3SFT (self-generated)-36.5SFT (Flow-generated)-38.6 üîº Table 1 compares the accuracy of three different fine-tuning methods (ground-truth, self-generated, and Flow-generated traces) on two datasets (GSM8K and MATH) for two different LLMs.\nread the caption Table 1: Main results of comparing the quality of traces used for SFT. We report the accuracy (%) for each model fine-tuned on an identical set of prompts, but with varying answer sources. For Phi-3, we does not include GSM8K due to its already optimized performance on the dataset. More visual insights # More on tables Learning rate5e-6OptimizerAdamGlobal batch size32DPO coefficient B1.0Gradient clipping1.0lora_r8lora_alpha8lora_dropout0.05lora_ targetallMaximum steps (chunks)6Chunk size160 üîº Table 2 presents the hyperparameters used for online direct preference optimization (DPO) fine-tuning in the Flow-DPO model.\nread the caption Table 2: Online DPO Fine-tuning hyperparameters. Learning rate2e-4OptimizerAdamGlobal batch size16Gradient clipping1.0gradient_accumulation_steps2warmup_ratio0.1lora_r16lora_alpha16lora_dropout0.05lora_targetallTraining epochs3 üîº Table 3 presents the hyperparameters used in the Compile (SFT) step of the proposed method.\nread the caption Table 3: Comiple (SFT) hyperparameters. Full paper # ","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.22304/","section":"Paper Reviews by AI","summary":"Flow-DPO: Online multi-agent learning boosts LLM mathematical reasoning by collaboratively generating detailed, high-quality reasoning traces, surpassing single-model approaches.","title":"Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning","type":"paper-reviews"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/large-language-models/","section":"Tags","summary":"","title":"Large Language Models","type":"tags"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/multimodal-learning/","section":"Tags","summary":"","title":"Multimodal Learning","type":"tags"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/natural-language-processing/","section":"Tags","summary":"","title":"Natural Language Processing","type":"tags"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.21845 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rJianlan Luo et el. 2024-10-30 ‚Üó arXiv ‚Üó Hugging Face TL;DR # Real-world robotic manipulation is challenging due to sample inefficiency and optimization difficulties in reinforcement learning (RL). Hand-designed controllers often lack the adaptability needed for complex tasks, while imitation learning methods struggle to generalize well.\nThe paper introduces HIL-SERL, a human-in-the-loop vision-based RL system to overcome these issues. By integrating human demonstrations and corrections with efficient RL algorithms and thoughtful system design, HIL-SERL achieves near-perfect success rates on complex tasks, outperforming existing methods by a significant margin. The framework demonstrates the practicality of vision-based RL for real-world manipulation and offers valuable insights into effective RL strategies for complex tasks.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # This paper is highly important for researchers in robotics and machine learning because it presents a novel human-in-the-loop reinforcement learning system that achieves near-perfect success rates in real-world dexterous manipulation tasks, significantly outperforming prior methods. Its sample efficiency and focus on real-world application makes it highly relevant to current trends in robotics research. The integrated approach and design choices open new avenues for research on efficient and reliable RL in complex, real-world settings.\nVisual Insights # üîº The figure shows sample input images from cameras used as inputs to the policy for the RAM insertion task, illustrating the cropped images focusing on task-relevant parts of the scene.\nread the caption Figure 9: Sample input images from cameras used as inputs to the policy. TaskTraining Time (h)Success Rate (%)Cycle Time (s)BCHIL-SERL (ours)BCHIL-SERL (ours)RAM Insertion1.529100 (+245%)8.34.8 (1.7x faster)SSD Assembly179100 (+27%)6.73.3 (2x faster)USB Grasp-Insertion2.526100 (+285%)13.46.7 (2x faster)Cable Clipping1.2595100 (+5%)7.24.2 (1.7x faster)IKEA - Side Panel 1277100 (+30%)6.52.7 (2.4x faster)IKEA - Side Panel 21.7579100 (+27%)5.02.4 (2.1x faster)IKEA - Top Panel135100 (+186%)8.92.4 (3.7x faster)IKEA - Whole Assembly-1/1010/10 (+900%)--Car Dashboard Assembly241100 (+144%)20.38.8 (2.3x faster)Object Handover2.579100 (+27%)16.113.6 (1.2x faster)Timing Belt Assembly62100 (+4900%)9.17.2 (1.3x faster)Jenga Whipping1.258100 (+1150%)--Object Flipping146100 (+117%)3.93.8 (1.03x faster)Average-49.7100 (+101%)9.65.4 (1.8x faster) üîº The table summarizes the success rates, cycle times, and training times of HIL-SERL and several baseline methods across various dexterous robotic manipulation tasks.\nread the caption Table 1: Experiment results. (a) HIL-SERL against imitation learning baselines. (b) HIL-SERL against various other baselines. More visual insights # More on figures üîº The figure shows the hardware setup for the IKEA furniture assembly task, including the robot arms, cameras and the work piece.\nread the caption Figure 13: Hardware setup for the IKEA furniture assembly task. üîº The figure shows the hardware setup for the Jenga whipping task, including the robot arm, wrist camera, side camera, and the Jenga tower.\nread the caption Figure 20: Hardware setup for the Jenga whipping task. More on tables TaskDPHG-DAggerBCIBRLResidual RLDAPGHIL-SERL no demo no itvHIL-SERL no itvHIL-SERL (ours)RAM Insertion2729127508048100Dashboard Assembly184135001800100Object Flipping5646469597720100100Average343931573233049100 üîº Table 1 compares the performance of HIL-SERL against several baselines (imitation learning and other state-of-the-art RL methods) across various dexterous robotic manipulation tasks, reporting success rates, cycle times, and training times.\nread the caption Table 1: Experiment results. (a) HIL-SERL against imitation learning baselines. (b) HIL-SERL against various other baselines. ParameterValueObservation spacewrist_1, wrist_ 2, tcp_pose, tcp_vel, tcp_f/tAction space6D twistReward functionBinary classifierClassifier viewswrist_1, wrist_2,Classifier accuracy97%Initial offline demonstrations20Environment update frequency10 HZMax episode length100 environment stepsReset methodScripted resetRandomization range4 cm in x and y, 6 deg in rzProprio encoder size64Policy MLP size256x256Total number of RL transitions32000Discount factor0.97OptimizerAdamLearning rate3e-4Image augmentationRandom crop üîº This table compares the performance of the proposed HIL-SERL method against imitation learning baselines and other state-of-the-art reinforcement learning methods across several robotic manipulation tasks, reporting success rates, cycle times, and training times.\nread the caption Table 1: Experiment results. (a) HIL-SERL against imitation learning baselines. (b) HIL-SERL against various other baselines. ParameterValueObservation spacewrist_1, wrist_2, side_2, tcp_pose, tcp_vel, tcp_f/tAction space6D twistReward functionBinary ClassifierClassifier viewswrist_1, wrist_2, side_2Classifier accuracy95%Initial offline demonstrations20Environment update frequency10 HZMax episode length100 environment stepsReset methodScripted resetRandomization range2 cm in x and y, 1 deg in rzProprio encoder size64Policy MLP size256x256Total number of RL transitions21000Discount factor0.97OptimizerAdamLearning rate3e-4Image augmentationRandom crop üîº This table compares the performance of HIL-SERL against imitation learning and other state-of-the-art reinforcement learning methods across multiple dexterous robotic manipulation tasks.\nread the caption Table 1: Experiment results. (a) HIL-SERL against imitation learning baselines. (b) HIL-SERL against various other baselines. ParameterValueObservation spacewrist_ 1, wrist_2, side_1, tcp_pose, tcp_vel, tcp_f/t, gripper_posAction space6D twist and 1D discrete gripper controlReward functionBinary classifierClassifier viewsside_1Classifier accuracy96%Initial offline demonstrations20Environment update frequency10 HZMax episode length120 environment stepsReset methodScripted resetRandomization range2 cm in x and y, 10 deg in rzProprio encoder size64Motion policy MLP size256x256Grasp critic MLP size256x256Total number of RL transitions50000Discount factor0.98OptimizerAdamLearning rate3e-4Image augmentationRandom crop üîº Table 1 compares the performance of HIL-SERL against imitation learning and other state-of-the-art reinforcement learning methods across several robotic manipulation tasks, reporting success rates, cycle times, and training times.\nread the caption Table 1: Experiment results. (a) HIL-SERL against imitation learning baselines. (b) HIL-SERL against various other baselines. ParameterValueObservation spacewrist_ 1, wrist_2, tcp_pose, tcp_vel, tcp_f/t, gripper_posAction space6D twist and 1D discrete gripper controlReward functionBinary classifierClassifier viewswrist_1, wrist_2Classifier accuracy97%Initial offline demonstrations20Environment update frequency10 HZMax episode length120 environment stepsReset methodHuman resetRandomization range4 cm in x and y, 10 deg in rzProprio encoder size64Motion policy MLP size256x256Grasp critic MLP size256x256Total number of RL transitions28000Discount factor0.98OptimizerAdamLearning rate3e-4Image augmentationRandom crop üîº This table compares the performance of HIL-SERL against imitation learning and other state-of-the-art reinforcement learning baselines across several robotic manipulation tasks, showing success rates, cycle times, and training times.\nread the caption Table 1: Experiment results. (a) HIL-SERL against imitation learning baselines. (b) HIL-SERL against various other baselines. ParameterValueObservation space for side panel 1wrist_1, side_1, side_2, tcp_pose, tcp_vel, tcp_f/tObservation space for side panel 2wrist_2, side_3, side_4, tcp_pose, tcp_vel, tcp_f/tAction space12D twistReward functionBinary ClassifierClassifier views for panel 1side_1, side_2Classifier views for panel 2side_3, side_4Classifier accuracy97%Initial offline demonstrations20Environment update frequency10 HZMax episode length100 environment stepsReset methodScripted resetRandomization range8 cm in X, y, 1 deg in rzProprio encoder size64Policy MLP size256x256Total number of RL transitions for panel 131000Total number of RL transitions for panel 236000Discount factor0.98OptimizerAdamLearning rate3e-4Image augmentationRandom crop üîº Table 1 presents a comparison of the success rate and cycle time of HIL-SERL against several imitation learning baselines and other state-of-the-art RL methods across multiple robotic manipulation tasks.\nread the caption Table 1: Experiment results. (a) HIL-SERL against imitation learning baselines. (b) HIL-SERL against various other baselines. ParameterValueObservation spaceside_1, side_3, side_4, tcp_pose, tcp_vel, tcp_f/tAction space12D twistReward functionBinary ClassifierClassifier viewsside_1, side_3, side_4Classifier accuracy95%Initial offline demonstrations20Environment update frequency10 HZMax episode length150 environment stepsReset methodScripted resetRandomization range3 cm in X, yProprio encoder size64Policy MLP size256x256Total number of RL transitions18000Discount factor0.97OptimizerAdamLearning rate3e-4Image augmentationRandom crop üîº Table 1 compares the performance of HIL-SERL against imitation learning and other state-of-the-art reinforcement learning methods across several dexterous robotic manipulation tasks.\nread the caption Table 1: Experiment results. (a) HIL-SERL against imitation learning baselines. (b) HIL-SERL against various other baselines. ParameterValueObservation spacewrist_1, wrist_2, side, tcp_pose, tcp_vel, tcp_f/t, gripper_posAction space12D twist and 1D discrete gripper controlReward functionBinary classifierClassifier viewswrist_1, wrist_2, sideClassifier accuracy98%Initial offline demonstrations20Environment update frequency10 HZMax episode length200 environment stepsReset methodHuman resetRandomization range2 cm in x and yProprio encoder size64Motion policy MLP size256x256Grasp critic MLP size256x256Total number of RL transitions36000Discount factor0.97OptimizerAdamLearning rate3e-4Image augmentationRandom crop üîº This table compares the performance of HIL-SERL against imitation learning and other state-of-the-art reinforcement learning baselines across several robotic manipulation tasks, showing success rates, cycle times, and training times.\nread the caption Table 1: Experiment results. (a) HIL-SERL against imitation learning baselines. (b) HIL-SERL against various other baselines. ParameterValueObservation spacewrist_ 1, wrist_2, side, tcp_pose, tcp_vel, gripper_posAction space12D twist and 1D discrete gripper controlReward functionBinary classifierClassifier viewssideClassifier accuracy99%Initial offline demonstrations20Environment update frequency10 HZMax episode length200 environment stepsReset methodHuman resetRandomization rangeNoneProprio encoder size64Motion policy MLP size256x256Grasp critic MLP size256x256Total number of RL transitions43000Discount factor0.97OptimizerAdamLearning rate3e-4Image augmentationRandom crop üîº This table compares the performance of HIL-SERL against imitation learning and other state-of-the-art reinforcement learning baselines across various robotic manipulation tasks, reporting success rates, cycle times, and training times.\nread the caption Table 1: Experiment results. (a) HIL-SERL against imitation learning baselines. (b) HIL-SERL against various other baselines. ParameterValueObservation spacewrist_1, wrist_ 2, side 1, side_2, tcp_pose, tcp_vel, tcp_f/tAction space12D twistReward functionBinary classifierClassifier viewsside_1, side_2Classifier accuracy96%Initial offline demonstrations20Environment update frequency10 HZMax episode length200 environment stepsReset methodHuman resetRandomization range2 cm in x and yProprio encoder size64Policy MLP size256x256Total number of RL transitions108000Discount factor0.97OptimizerAdamLearning rate3e-4Image augmentationRandom crop üîº This table compares the performance of HIL-SERL against imitation learning and other state-of-the-art reinforcement learning methods across several robotic manipulation tasks, showing success rates, cycle times, and training times.\nread the caption Table 1: Experiment results. (a) HIL-SERL against imitation learning baselines. (b) HIL-SERL against various other baselines. ParameterValueObservation spacewrist, global, tcp_pose, tcp_ vel, 9, dqAction spaceFeedforward wrench Fx, Fz, TzReward functionHuman annotation in the end of an episodeEnvironment update frequency10 HZMax episode length20 environment stepsReset methodHuman resetRandomization rangeNoneInitial offline demonstrations30Proprio encoder size64Policy MLP size256x256Total number of RL transitions10000Discount factor0.96, but every episode was run to maximum lengthOptimizerAdamLearning rate3e-4, decayed to 3e-5 when reaching 70% success rateImage augmentationRandom crop üîº This table presents a comparison of the success rate and cycle time of HIL-SERL against imitation learning and other state-of-the-art reinforcement learning baselines on several dexterous manipulation tasks.\nread the caption Table 1: Experiment results. (a) HIL-SERL against imitation learning baselines. (b) HIL-SERL against various other baselines. ParameterValueObservation spacewrist, side, tcp_pose, tcp_vel, 9, dqAction spaceFeedforward wrench Fx, Fz, TyReward functionBinary classifierClassifier viewswristClassifier accuracy97%Initial offline demonstrations20Environment update frequency10 HZMax episode length100 environment stepsReset methodScripted resetRandomization rangeNoneProprio encoder size64Policy MLP size256x256Total number of RL transitions25000Discount factor0.985OptimizerAdamLearning rate3e-4Image augmentationRandom crop üîº This table compares the performance of HIL-SERL against imitation learning and other state-of-the-art reinforcement learning methods across various robotic manipulation tasks, showing success rates, cycle times, and training times.\nread the caption Table 1: Experiment results. (a) HIL-SERL against imitation learning baselines. (b) HIL-SERL against various other baselines. Task NameNumber of DemosObservation Chunking SizeAction Prediction HorizonAction Chunking SizeRAM Insertion200182Dashboard Assembly200184Object Flipping200111 üîº This table compares the performance of HIL-SERL against imitation learning and other state-of-the-art reinforcement learning baselines across multiple robotic manipulation tasks, reporting success rates, cycle times, and training times.\nread the caption Table 1: Experiment results. (a) HIL-SERL against imitation learning baselines. (b) HIL-SERL against various other baselines. Full paper # ","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.21845/","section":"Paper Reviews by AI","summary":"Human-in-the-loop RL enables robots to master complex manipulation tasks with near-perfect success rates and superhuman speed, exceeding prior methods.","title":"Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning","type":"paper-reviews"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/robotics/","section":"Tags","summary":"","title":"Robotics","type":"tags"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.22325 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rGuangqi Jiang et el. 2024-10-30 ‚Üó arXiv ‚Üó Hugging Face TL;DR # Current methods for pre-training robotic visual representations often rely on human videos, which suffer from domain mismatch and lack crucial dynamic information. This paper addresses this limitation by introducing a new evaluation metric, \u0026ldquo;manipulation centricity,\u0026rdquo; which quantifies how well a representation focuses on manipulation-relevant parts of a scene (e.g., robot end-effectors, objects). Experiments show a strong positive correlation between this metric and downstream robotic task performance.\nTo improve manipulation centricity, the authors propose a novel pre-training method called Manipulation Centric Representation (MCR). MCR utilizes a large-scale robot dataset and incorporates three training objectives: dynamics alignment, action prediction, and temporal contrast. Results across various simulation and real-world robotic tasks demonstrate that MCR significantly outperforms existing methods, highlighting the benefits of using large-scale robot data and incorporating dynamics into the representation learning process. MCR\u0026rsquo;s superior performance suggests a more effective approach for pre-training robotic visual representations.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # This paper is crucial for researchers working on robotic manipulation and visual representation learning. It introduces a novel evaluation metric and pre-training method that significantly improves the performance of robotic systems. The findings highlight the importance of dynamics information and large-scale robot datasets in this field, opening new avenues for future research. The strong correlation discovered between manipulation centricity and downstream task success provides a valuable new direction for evaluating and designing robotic representations.\nVisual Insights # üîº The figure illustrates the overall framework of the proposed Manipulation Centric Representation (MCR) method, showing the pre-training process using large-scale robotic datasets and its evaluation on both simulation and real robot tasks.\nread the caption Figure 1: Overview. We introduce a robotic representation evaluation metric termed manipulation centricity, which exhibits a strong correlation with downstream policy performance. Accordingly, we design a new pre-training method, MCR, to learn manipulation-centric representation from large-scale robotic datasets. Comprehensive experiments on both simulations and real robot validate the superiority of our proposed representation. üîº The chart shows the strong correlation between manipulation centricity and the downstream performance of different robotic representations across various simulation environments, highlighting the superior performance of MCR and the benefits of using robot datasets (DROID) over human datasets.\nread the caption Figure 2: Correlation between manipulation centricity and downstream performance. Our findings reveal that (1) the proposed metric of manipulation centricity strongly correlates with the downstream performance of robotic representations, and (2) using the robot dataset DROID yields greater benefits for robotic representations than human datasets. (3) These insights motivate our method, MCR, which leverages dynamics labels from the robot dataset to further enhance manipulation centricity and downstream performance. TaskLfSMVPVC-1R3MMCRLift5/106/105/106/109/10Sweep3/101/102/101/107/10Rearrange2/103/106/104/107/10All10/3010/3013/3011/3023/30 üîº The table presents the success rate of different methods on three real-world robotic manipulation tasks, showing MCR\u0026rsquo;s superior performance.\nread the caption Table 2: Real robot results. Our method MCR performs best in all tasks. Each method is fairly assessed over 10 trials on each task. More visual insights # More on charts üîº The chart displays the relationship between the scale of the robot dataset and the success rate of the model\u0026rsquo;s performance.\nread the caption Figure 9: Effect of robot dataset size. üîº The chart displays the relative change in manipulation centricity and success rate of R3M-DROID and MCR compared to R3M for gripper-based and hand-based tasks.\nread the caption Figure 10: Downstream domain gap. üîº The bar chart displays the frequency distribution of action verbs in the DROID dataset.\nread the caption Figure 12c: Statistics of Action Verb. More on tables Ablated ComponentsSuccess Rate (%)Training Objectivew/o. objective Ldyn - ‰∏Ä66.2(¬±0.8)w/o. objective Lact71.3 (Âúü1.2)w/o. objective Ltcl72.0 (Âúü1.2)Dynamic ChunkLengthl: 3‚Üí172.1 (Â£´2.9)Length l: 3‚Üí576.8 (Âúü2.4)Length l: 3‚Üí776.8 (Âúü2.2)Encoder BackboneResNet-: 50‚Üí1877.3(¬±1.8)ResNet-: 50‚Üí3477.9(¬±1.7)MCR (original)83.2 (Â£´1.3) üîº The table shows the ablation study results of the MCR model, evaluating the impact of different training objectives and hyperparameters on the model\u0026rsquo;s performance.\nread the caption Table 4: Key design choices of MCR. GPU TypeTraining Time (h)Tesla V100~120RTX 3090 Ti~50 üîº The table compares the training time and GPU type used for different methods in the paper, showing that MCR is more computationally efficient than other methods.\nread the caption Table 5: Computation efficiency. Training computation requirements across methods. Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local- ization. In International Conference on Computer Vision (ICCV), 2017.Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and P. Abbeel. Masked world models for visual control. In Conference on Robot Learning (CoRL), 2022.Jinghuan Shang, Karl Schmeckpeper, Brandon B. May, Maria Vittoria Minniti, Tarik Kelestemur, David Watkins, and Laura Herlant. Theia: Distilling diverse vision foundation models for robot learning. In Conference on Robot Learning (CoRL), 2024.Mohan Kumar Srirama, Sudeep Dasari, Shikhar Bahl, and Abhinav Gupta. Hrp: Human affordances for robotic pre-training. Robotics: Science and Systems (RSS), 2024.Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning from reinforcement learning. In International Conference on Machine Learning (ICML), 2021.Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. Robotics: Science and Systems (RSS), 2024.Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic- tive coding. arXiv preprint arXiv:1807.03748, 2019.Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research (JMLR), 2008.Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe Hansen-Estruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang, Chelsea Finn, and Sergey Levine. Bridgedata v2: A dataset for robot learning at scale. In Conference on Robot Learning (CoRL), 2023.Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. In Conference on Robot Learning (CoRL), 2022.Guowei Xu, Ruijie Zheng, Yongyuan Liang, Xiyao Wang, Zhecheng Yuan, Tianying Ji, Yu Luo, Xiaoyu Liu, Jiaxin Yuan, Pu Hua, et al. Drm: Mastering visual reinforcement learning through dormant ratio minimization. In International Conference on Learning Representations (ICLR), 2023.Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning (CoRL), 2019.Zhecheng Yuan, Zhengrong Xue, Bo Yuan, Xueqian Wang, Yi Wu, Yang Gao, and Huazhe Xu. Pre-trained image encoder for generalizable visual reinforcement learning. Advances in Neural Information Processing Systems, 35:13022-13037, 2022.Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3d diffu- sion policy: Generalizable visuomotor policy learning via simple 3d representations. Robotics: Science and Systems (RSS), 2024.Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. Robotics: Science and Systems (RSS), 2023.Ruijie Zheng, Xiyao Wang, Yanchao Sun, Shuang Ma, Jieyu Zhao, Huazhe Xu, Hal Daume III, and Furong Huang. TACO: Temporal latent action-driven contrastive loss for visual reinforce- ment learning. In International Conference on Neural Information Processing Systems (NeurIPS), 2023.Ruijie Zheng, Yongyuan Liang, Xiyao Wang, Shuang Ma, Hal Daume III, Huazhe Xu, John Lang- ford, Praveen Palanisamy, Kalyan Shankar Basu, and Furong Huang. Premier-taco: Pretraining multitask representation via temporal action-driven contrastive loss. In International Conference üîº The table presents Grad-CAM visualizations for all tasks, comparing the focus regions of different robotic representation methods.\nread the caption Table 8: Grad-CAM of all tasks HyperparameterValueEncoder typeResNet50Batch size32Learning rate1e-4Training steps500,000Data augmentationRandomResizedCrop (224,(0.5, 1.0))OptimizerAdamDROID views usedtwo exterior viewsDROID proprioception usedcartesian and gripper position üîº This table lists the hyperparameters used during the pre-training phase of the Manipulation Centric Representation (MCR) method.\nread the caption Table 6: Hyperparameters for MCR pre-training. DexArtRobomimicRobocCasaAlg TaskBucketFaucetLaptopToiletCanLiftSquareCloseDrawerCoffeeButtonPressOpenSingleDoorMCR(ours)36.7 (Âúü2.9)38.3 (Âúü2.9)93.3 (Âúü2.9)73.3 (Â£´2.9)68.0 (¬±4.0)96.0 (¬±2.3)30.0 (¬±1.2)99.3 (Âúü1.2)72.0 (¬±3.5)56.0 (¬±3.5)LfS33.3 (Â£´5.8)36.7 (Â£´5.8)83.3 (Â£´10.4)71.7 (Âúü2.9)6.0 (¬±0.0)64.0 (Â£´4.2)4.0 (¬±0.0)85.3 (Âúü1.2)52.0 (Â£´4.0)46.7 (Âúü1.2)MVP31.7 (Âúü2.9)33.3 (Âúü2.9)81.7 (Â£´5.8)80.0 (¬±0.0)28.0 (Âúü2.0)74.0 (¬±6.4)14.0 (Â£´2.3)98.0 (Âúü2.0)52.7 (Âúü18.9)33.3 (¬±14.5)VC130.0 (¬±0.0)35.0 (¬±0.0)85.0 (¬±0.0)71.7 (Âúü2.9)44.0 (Â£´7.0)74.0 (Âúü9.2)20.0 (Â£´3.5)98.7 (Â£´2.0)29.3 (¬±5.8)33.3 (Âúü7.0)R3M31.7 (Âúü2.9)36.7 (Âúü2.9)81.7 (Â£´5.8)71.7 (Âúü2.9)50.0 (Â£´4.2)86.0 (Â£´6.0)24.0 (Âúü1.2)88.7 (Âúü3.1)47.3 (Â£´6.1)48.7 (Âúü7.6)HRP31.7 (Âúü2.9)36.7 (Â£´2.9)90.0 (Â£´5.0)63.3 (¬±14.4)42.0 (¬±3.5)86.0 (Â£´3.5)26.0 (Â£´2.3)91.3 (Â£´4.6)35.3 (Âúü11.6)38.0 (Â£´6.0)R3M-Droid35.0 (Â£´5.0)33.3 (Â£´2.9)80.0 (¬±0.0)66.7 (Â£´7.6)54.0 (Â£´2.3)96.0 (¬±0.0)22.0 (Â£´3.1)88.7 (Â£´2.3)51.3 (Âúü2.3)45.3 (¬±7.6)MetaWorldMetaWorld (Hard)Meta World (Very Hard)Alg TaskButton PressDrawer OpenMetaWorld (Medium) Bin PickingHammerAssemblyShelf PlaceDisassemble Stick PullStick PushPick Place WallMCR(ours)100.0 (¬±0.0)100.0 (¬±0.0)96.7 (Âúü2.9)100.0 (¬±0.0)100.0 (¬±0.0)41.7(¬±5.8) 93.3 (¬±2.9)86.7 (¬±2.9)100.0 (¬±0.0)91.7 (¬±2.9)LfS96.7 (Âúü2.9)95.0 (¬±5.0)81.7 (Â£´2.9)95.0 (Â£´5.0)95.0 (Â£´5.0)35.0 (Â£´5.0)86.7 (Â£´2.9)83.3 (Â£´5.8)96.7 (Âúü2.9)85.0 (Â£´5.0)MVP96.7 (Â£´2.9)98.3 (Â£´2.9)81.7 (Â£´2.9)91.7 (Â£´2.9)86.7 (Â£´2.9)20.0 (Â£´5.0)65.0 (¬±8.7)75.0 (¬±8.7)96.7 (Â£´2.9)76.7 (¬±11.6)VC-198.3 (Âúü2.9)98.3 (Âúü2.9)78.3 (Âúü2.9)86.7 (Â£´2.9)95.0 (Â£´5.0)21.7 (Âúü2.9)66.7 (Âúü2.9)86.7 (Â£´2.9)98.3 (Âúü2.9)71.7 (Âúü2.9)R3M91.7 (Âúü2.9)71.7 (¬±16.1)21.7 (Â£´2.9)63.3 (Â£´5.8)36.7 (Â£´2.9)35.0 (Âúü8.7)76.7 (Â£´2.9)43.3 (Âúü7.6)71.7 (Âúü2.9)58.3 (Â£´5.8)HRP98.3 (Âúü2.9)98.3 (Âúü2.9)90.0 (¬±0.0)65.0 (¬±0.0)96.7 (Â£´2.9)23.3 (Âúü2.9)61.7 (Â£´2.9)85.0 (¬±0.0)96.7 (Âúü2.9)81.7 (Âúü2.9)R3M-Droid98.3 (Âúü2.9)96.7 (Â£´5.8)90.0 (¬±0.0)80.0 (¬±0.0)83.3 (Â£´5.8)38.3 (Âúü2.9)66.7 (Â£´2.9)61.7 (Â£´20.2)98.3 (Âúü2.9)83.3 (Â£´5.8) üîº The table presents the success rates of different methods across 20 robotic manipulation tasks in four simulation environments, comparing the proposed MCR method against several baseline methods.\nread the caption Table 7: Main results on 20 simulation tasks. Results for each task are provided in this table. A summary across domains is shown in Figure 7. Full paper # ","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.22325/","section":"Paper Reviews by AI","summary":"Pre-trained robots achieve higher manipulation success rates using a novel manipulation-centric representation (MCR) learned from large-scale robot datasets, surpassing baselines significantly.","title":"Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets","type":"paper-reviews"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.22330 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rGrace Luo et el. 2024-10-30 ‚Üó arXiv ‚Üó Hugging Face TL;DR # Vision-and-language models (VLMs) are increasingly used for various tasks, but their internal mechanisms remain a mystery. Understanding how VLMs encode task information is critical for improving their performance and enabling more efficient use of resources. Previous research has identified task vectors in language-only and vision-only models, but their cross-modal properties are largely unknown. This lack of understanding limits our ability to build more robust and efficient cross-modal systems, particularly as more multi-modal applications become prevalent.\nThis paper investigates how VLMs encode task representations using a variety of input modalities and specifications. The researchers discovered that similar tasks are consistently mapped to similar task vectors within the model, irrespective of the input\u0026rsquo;s type or how the task was described. They found that the process of generating an answer in the model consists of three phases: input processing, task representation, and final answer generation. Their key contribution is the discovery of the cross-modal nature of task vectors, enabling efficient transfer of task representations between different modalities. Furthermore, they show that combining exemplar and instruction-based task vectors significantly improves performance.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # This paper is crucial for researchers working on vision-and-language models (VLMs) and mechanistic interpretability. It reveals the surprising cross-modal consistency of task representations in VLMs, offering insights into their underlying mechanisms and suggesting new approaches for improving efficiency and transferability. The findings open avenues for research in cross-modal learning and more efficient VLM design, impacting numerous downstream applications.\nVisual Insights # üîº The figure illustrates how vision-and-language models (VLMs) generate consistent task representations from various input modalities and specifications.\nread the caption Figure 1: Vision-and-language models (VLMs) map inputs to abstract task representations that are consistent across modalities and specifications. For example, the task of mapping a country to its capital can be expressed in various ways (a), all of which lead to similar task representations (b). üîº The chart visualizes the evolution of token representations across model layers, showing the transition from input to task to answer.\nread the caption Figure 13: We show a continuous visualization of how the token representation evolves across layers for all tasks. Each line shows the representational similarity with a pre-defined token, aggregated over 100 sets of examples. We use the token auf for the input, one of {capital, currency, species, baby, color, flavor} for the task, and each run‚Äôs ground-truth label for the answer. TaskInstructionText ICL ExampleImage ICL ExampleCountry-CapitalThe capital city of the country:{Greece : Athens}: Athens}Country-CurrencyThe last word of the official currency of the country:{Italy : Euro}{ : Euro}Animal-LatinThe scientific name of the animal's species in latin:{Gray Wolf : Canis lupus}{ : Canis lupus}Animal- YoungThe term for the baby of the animal:{Common Dolphin : calf}: calf}Food-ColorThe color of the food:{Persimmon : orange}{ : orange}Food-FlavorThe flavor descriptor of the food:{Strawberry : sweet}{ : sweet} üîº Table 1 shows six tasks that can be specified using text instructions, text examples, or image examples, illustrating the cross-modal nature of the tasks.\nread the caption Table 1: Cross-modal tasks. We design six tasks inspired by the text examples in prior work (Hendel et al., 2023; Todd et al., 2024), where we add alternative specifications such as instructions and image examples. More visual insights # More on figures üîº The figure illustrates how vision-and-language models (VLMs) generate consistent task representations across different input modalities and specifications.\nread the caption Figure 1: Vision-and-language models (VLMs) map inputs to abstract task representations that are consistent across modalities and specifications. For example, the task of mapping a country to its capital can be expressed in various ways (a), all of which lead to similar task representations (b). üîº The figure illustrates how task representations learned from one modality (text or image examples) can be transferred to improve the performance of a vision-language model on queries from a different modality.\nread the caption Figure 2: Cross-modal transfer. Task representations can be computed in one modality (left) and patched to guide VLMs to perform a task on queries from a different modality (right). We observe that certain tasks are more effectively represented in one modality and therefore benefit from transfer. More on charts üîº The chart displays the probability of the last token representation decoding to input, task, or answer vectors across different layers of the model for both text and image ICL.\nread the caption Figure 3: The output evolves in three distinct phases that are shared for text and image ICL. Each line corresponds to the probability that the last token representation decodes to a pre-defined input, task, or answer vector. We display visualizations of specific layers in Figure 4 and further visualize the task representation phase in Table 2. üîº The chart displays the evolution of token representations across model layers for various tasks, showing three distinct phases: input, task, and answer.\nread the caption Figure 13: We show a continuous visualization of how the token representation evolves across layers for all tasks. Each line shows the representational similarity with a pre-defined token, aggregated over 100 sets of examples. We use the token auf for the input, one of {capital, currency, species, baby, color, flavor} for the task, and each run‚Äôs ground-truth label for the answer. More on tables ModelCountry-CapitalCountry-CurrencyAnimal-LatinAnimal-YoungFood-ColorFood-FlavorAvg.Random0.000.120.000.180.240.310.14LLaVA-v1.5No Context0.000.000.000.000.000.000.00Image ICL Base-------Image ICL Patch-------Text ICL xBase0.020.180.030.230.280.370.18Text ICL xPatch0.310.300.260.180.530.310.32Mantis-FuyuNo Context0.000.000.000.000.000.000.00Image ICL Base0.110.130.240.050.340.230.18Image ICL Patch0.170.030.160.050.500.310.20Text ICL xBase0.090.060.080.020.230.040.09Text ICL xPatch0.320.230.360.090.510.360.31Idefics2No Context0.030.000.030.000.010.010.01Image ICL Base0.710.570.430.120.410.350.43Image ICL Patch0.580.320.400.030.390.170.31Text ICL xBase0.110.030.410.130.210.180.18Text ICL xPatch0.610.400.480.620.530.390.51 üîº The table presents the accuracy results of cross-modal transfer experiments across six tasks using different vision-language models and methods for specifying tasks.\nread the caption Table 3: Cross-modal transfer results. We display the accuracy across six tasks on an unseen test set. For image queries, patching cross-modal task vectors (Text ICL xPatch) outperforms text ICL in the same context window (Text ICL xBase) and the strong unimodal image ICL baseline (Image ICL Base, Patch). The best method per task is underlined and overall is bolded. LLaVA-v1.5 (Liu et al., 2023a)Mantis-Fuyu (Jiang et al., 2024)Idefics2 (LaurenÔøΩon et al., 2024)Text ModelVicuna (Chiang et al., 2023)Fuyu (Bavishi et al., 2023)Mistral (Jiang et al., 2023)Vision ModelCLIP (Radford et al., 2019)Fuyu (Bavishi et al., 2023)SigLIP (Zhai et al., 2023)ParadigmLate-FusionEarly-FusionLate-FusionImage ICLNoYesYesParameters7B8B8BNum Layers323632 üîº Table 3 presents the accuracy of six tasks using different cross-modal transfer methods, showing that patching cross-modal task vectors generally outperforms other methods.\nread the caption Table 3: Cross-modal transfer results. We display the accuracy across six tasks on an unseen test set. For image queries, patching cross-modal task vectors (Text ICL xPatch) outperforms text ICL in the same context window (Text ICL xBase) and the strong unimodal image ICL baseline (Image ICL Base, Patch). The best method per task is underlined and overall is bolded. ModelCountry-CapitalCountry-CurrencyAnimal-LatinAnimal- YoungFood-ColorFood-FlavorAvg.Idefics2No Context0.000.000.070.000.000.000.01Image ICL Base0.740.530.440.120.430.350.44Text ICL xBase0.160.060.240.160.170.120.15Text ICL xPatch0.700.440.500.640.540.400.54 üîº This table shows the test accuracy results for six tasks when transferring task vectors from text-based in-context learning (ICL) to image queries, comparing different template formats used for the Idefics2 model.\nread the caption Table 6: We ablate the template format and display the test accuracy when transferring from text ICL to image queries. We use the recommended template for Idefics2. ModelCountry-CapitalCountry-CurrencyAnimal-LatinAnimal-YoungFood-ColorFood-FlavorAvg.LLaVA-v1.5VLM-VLM xPatch0.310.300.260.180.530.310.32LLM-VLM xPatch0.330.320.250.330.530.450.37Idefics2VLM-VLM xPatch0.610.400.480.620.530.390.51LLM-VLM xPatch0.570.580.460.550.540.390.52 üîº This table presents the accuracy of different cross-modal transfer methods across six tasks, showing the superior performance of patching cross-modal task vectors compared to using text ICL or image ICL alone.\nread the caption Table 3: Cross-modal transfer results. We display the accuracy across six tasks on an unseen test set. For image queries, patching cross-modal task vectors (Text ICL xPatch) outperforms text ICL in the same context window (Text ICL xBase) and the strong unimodal image ICL baseline (Image ICL Base, Patch). The best method per task is underlined and overall is bolded. ModelCountry-CapitalCountry-CurrencyAnimal-LatinAnimal-YoungFood-ColorFood-FlavorAvg.LLaVA-v1.5Image - Image Patch-------Text Image xPatch0.310.300.260.180.530.310.32Text Text Patch0.970.580.770.200.630.410.59Image - Text xPatch-------Mantis-FuyuImage - Image Patch0.170.030.160.050.500.310.20Text Image xPatch0.320.230.360.090.510.360.31Text Text Patch0.460.300.480.180.280.360.34Image - Text xPatch0.310.010.360.050.400.340.25Idefics2Image - Image Patch0.580.320.400.030.390.170.31Text Image xPatch0.610.400.480.620.530.390.51Text Text Patch0.970.610.740.540.630.410.65Image - Text xPatch0.810.430.580.040.400.270.42 üîº The table displays the accuracy of cross-modal transfer performance across six tasks using different models and input methods.\nread the caption Table 3: Cross-modal transfer results. We display the accuracy across six tasks on an unseen test set. For image queries, patching cross-modal task vectors (Text ICL xPatch) outperforms text ICL in the same context window (Text ICL xBase) and the strong unimodal image ICL baseline (Image ICL Base, Patch). The best method per task is underlined and overall is bolded. TaskInstructionCountry-Capitalcity, GU, vik, cities, headquartersCountry-Currency‚óá, ‚óá, ‚óá, itos, ‚óáAnimal-Latinspecies, genus, ‚óá, animals, americanAnimal-Youngbaby, babies, ‚óá, bach, calledFood-Colorcolors, color, colour, ETH, iloFood-Flavortaste, tastes, arom,food,flavor üîº Table 3 presents the accuracy results of cross-modal transfer experiments across six tasks, comparing different methods of task vector usage and showing the superior performance of cross-modal patching for image queries.\nread the caption Table 3: Cross-modal transfer results. We display the accuracy across six tasks on an unseen test set. For image queries, patching cross-modal task vectors (Text ICL xPatch) outperforms text ICL in the same context window (Text ICL xBase) and the strong unimodal image ICL baseline (Image ICL Base, Patch). The best method per task is underlined and overall is bolded. Full paper # ","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.22330/","section":"Paper Reviews by AI","summary":"Vision-language models surprisingly use similar internal representations for similar tasks regardless of input type (image or text) or specification (example or instruction).","title":"Task Vectors are Cross-Modal","type":"paper-reviews"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/vision-language-models/","section":"Tags","summary":"","title":"Vision-Language Models","type":"tags"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/-daily-papers/","section":"Categories","summary":"","title":"ü§ó Daily Papers","type":"categories"},{"content":"","date":"28 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-carnegie-mellon-university/","section":"Tags","summary":"","title":"üè¢ Carnegie Mellon University","type":"tags"},{"content":"","date":"28 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-harvard-university/","section":"Tags","summary":"","title":"üè¢ Harvard University","type":"tags"},{"content":"","date":"28 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-mit-lincoln-laboratory/","section":"Tags","summary":"","title":"üè¢ MIT Lincoln Laboratory","type":"tags"},{"content":"","date":"28 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/information-retrieval/","section":"Tags","summary":"","title":"Information Retrieval","type":"tags"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.21465 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rHanshi Sun et el. 2024-10-30 ‚Üó arXiv ‚Üó Hugging Face TL;DR # Serving long-context Large Language Models (LLMs) efficiently is challenging due to the expanding key-value (KV) cache, which impacts memory and access times. Existing solutions like dynamic sparse attention either don\u0026rsquo;t sufficiently reduce memory usage or introduce significant latency.\nThis paper introduces SHADOWKV, a system that addresses these issues. SHADOWKV stores low-rank key caches on the GPU and offloads value caches to the CPU to reduce memory footprint. It also employs an accurate KV selection strategy to reconstruct minimal sparse KV pairs on-the-fly, minimizing decoding latency. Experiments show that SHADOWKV supports much larger batch sizes and boosts throughput significantly without sacrificing accuracy, achieving up to 6x larger batch sizes and 3.04x throughput improvements on an A100 GPU.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # This paper is crucial for researchers working on large language models (LLMs) and high-throughput inference. It addresses the critical challenge of efficiently serving long-context LLMs, a significant hurdle in deploying these powerful models. The proposed method, SHADOWKV, offers a novel approach to enhance performance which can greatly benefit the wider AI community, especially those focused on LLM optimization and deployment. The research opens exciting avenues for further investigation in dynamic sparse attention, KV cache management, and efficient LLM deployment strategies.\nVisual Insights # MethodsN-S1N-S2N-MK1N-MK2N-MQN-MVQA-1QA-2VTFWEAvg.Llama-3-8B-1M100.00100.0098.9698.9698.9695.5775.0048.9678.5471.8586.68Loki18.751.042.080.001.560.784.1713.5426.0425.359.33Loki (V only)41.676.2537.501.048.0730.7310.4219.7951.6737.5024.46Quest100.00100.0098.9677.0897.6593.4960.4250.0077.0865.6382.03Quest (V only)100.00100.0098.9685.4297.9295.4970.8346.8878.7565.6383.99SHADOWKV100.00100.0097.9298.9696.8895.8372.9252.0881.6772.5786.88GLM-4-9B-1M100.00100.0094.7987.5099.7493.7567.7155.2197.2972.2286.82Loki71.8827.0822.922.089.9011.4628.1327.0831.0454.1728.57Loki (V only)96.8855.2156.2518.7551.0450.5245.8339.5872.7159.7254.65Quest100.0095.8390.6254.1794.0176.3055.2152.0895.8364.5877.86Quest (V only)100.0096.8893.7572.9295.8383.0756.2553.1396.8865.9781.47SHADOWKV100.00100.0095.8383.3398.7087.7669.7955.2197.5068.0685.62Llama-3.1-8B100.00100.0098.9691.6798.9695.3182.2947.9268.9671.1885.53Loki68.7532.2932.2920.8342.7128.6541.6733.3324.7929.8635.52Loki (V only)95.8336.4657.2962.5077.8670.8369.7939.5835.2137.5058.29Quest100.0098.9697.9234.3893.4988.5470.8344.7965.6368.4076.29Quest (V only)100.0098.9698.9656.2595.8390.6376.0446.8866.2567.3679.72SHADOWKV100.00100.00100.0083.3397.9292.1981.2548.9667.0864.9383.57Yi-9B-200K100.00100.0086.4662.5064.5832.5544.7939.5836.8789.9365.73Loki34.382.082.080.000.000.5222.9221.880.0025.0010.89Loki (V only)59.3811.4618.755.214.432.0822.9231.250.0035.0719.06Quest100.0098.9679.1726.0456.5131.7732.2931.2551.0471.8857.89Quest (V only)100.00100.0080.2145.8359.3731.9036.4534.3753.5471.8861.36SHADOWKV100.00100.0082.2967.7163.2831.5143.7538.5456.0472.2265.53 üîº Table 1 presents the performance comparison of various methods (Loki, Quest, and SHADOWKV) on the RULER benchmark with Llama-3.8B-1M, GLM-4-9B-1M, and Llama-3.1-8B models at a sequence length of 128K, showing SHADOWKV\u0026rsquo;s superior accuracy with a 1.56% sparse KV cache budget.\nread the caption Table 1: Performance of different models and different methods on RULER [20] evaluated at length of 128K. SHADOWKV outperforms other methods with a 1.56% sparse budget. More visual insights # More on tables MethodsNarratQAMultiFQAHotpotQAMuSiQueDuReadGovRepSAMSumPassRetrLCCAvg.Llama-3-8B-1M18.9841.8436.7921.4731.9334.1835.9681.5056.0739.86Loki2.2610.195.483.1612.1728.977.8440.5231.4415.78Loki (V only)3.2021.0112.413.8617.0731.2416.2352.5738.1021.74Quest20.1336.6335.0018.1424.5527.1135.6379.0053.6436.65Quest (V only)17.2639.5136.7818.7126.4129.4935.8079.5060.0538.17SHADOWKV17.1739.7338.2921.0831.7731.6235.8780.0063.9339.94GLM-4-9B-1M25.4451.0958.6739.6132.0429.9740.3199.0058.0248.24Loki5.8230.6022.739.2030.0930.3522.7098.9240.7732.35Loki (V only)10.8944.9745.4423.5132.0730.5635.3499.5050.2741.39Quest23.8144.5356.4135.4923.5421.7337.3987.0043.8041.52Quest (V only)26.0046.3257.5436.4224.5824.5237.7193.5046.5243.68SHADOWKV26.5051.3159.0938.8732.9228.5438.7096.5058.5547.89Llama-3.1-8B31.5655.1057.6529.4635.2634.4529.84100.0067.3148.96Loki2.3118.8910.645.4719.3031.1615.9194.8844.6027.02Loki (V only)3.9338.5922.8512.9627.4332.2226.4398.2556.1135.42Quest29.7049.0453.9627.1827.1630.4329.8598.5057.3544.80Quest (V only)30.0253.9756.3927.0629.0631.6530.2399.0063.8946.81SHADOWKV30.9355.2057.3229.1331.8532.7930.4099.5066.0348.13Yi-9B-200K13.8830.0252.4628.2022.2930.2519.0867.0073.5037.41Loki1.632.7316.214.874.752.134.950.0038.728.44Loki (V only)1.9610.3921.317.366.789.1510.024.0058.7514.41Quest10.5725.8346.0623.0417.0917.1120.5950.5067.7030.94Quest (V only)14.5625.7348.7324.7318.4420.8320.0857.5071.1333.53SHADOWKV12.4430.8252.4327.7320.7929.8320.7364.0072.8936.85 üîº Table 2 presents the performance comparison of different methods (SHADOWKV, Loki, Quest) across various Long-Context LLMs on the LongBench benchmark for samples exceeding 4K tokens.\nread the caption Table 2: Performance of various methods on different models with LongBench [4] samples exceeding 4K tokens. SHADOWKV outperforms other methods and maintains the accuracy. Methods8K16K32K64K128K256KAvg.Llama-3-8B-1M w MInference89.9288.0282.8178.4578.1274.5781.98SHADOWKV w / MInference90.4788.1283.2877.7178.3274.3182.04 üîº Table 1 presents the performance of various methods (Loki, Quest, and SHADOWKV) on the RULER benchmark using different language models at a sequence length of 128K, highlighting SHADOWKV\u0026rsquo;s superior performance with a 1.56% sparse budget.\nread the caption Table 1: Performance of different models and different methods on RULER [20] evaluated at length of 128K. SHADOWKV outperforms other methods with a 1.56% sparse budget. ModelContextFull AttentionSHADOWKVGainFull Attention (Inf)Llama-3-8B-1M (8 KV heads)60K160.62 (8)455.14 (48)2.83x168.72 (48) / 273.07 (Inf)122K80.77 (4)239.51 (24)2.97x83.05 (24) / 134.30 (Inf)244K40.37 (2)119.01 (12)2.95x52.00 (12) / 67.15 (Inf)Llama-3.1-8B (8 KV heads)60K160.93 (8)472.77 (48)2.94x168.72 (48) / 273.07 (Inf)122K80.78 (4)245.90 (24)3.04x83.05 (24) / 134.30 (Inf)GLM-4-9B-1M (4 KV heads)60K241.05 (12)615.89 (50)2.56x266.24 (50) / 436.91 (Inf)122K122.67 (6)293.40 (25)2.39x158.83 (25) / 214.87 (Inf)244K61.13 (3)136.51 (12)2.23x78.84 (12) / 107.44 (Inf)Yi-9B-200K (4 KV heads)60K204.81 (10)544.36 (42)2.66x271.21 (42) / 364.09 (Inf)122K101.44 (5)260.03 (21)2.56x133.53 (21) / 179.06 (Inf)244K46.74 (2)118.55 (10)2.54x65.79 (10) / 89.53 (Inf) üîº Table 4 presents the generation throughput in tokens per second on an A100 GPU for various models and context lengths, comparing full attention with SHADOWKV and showing the throughput gain and theoretical maximum throughput.\nread the caption Table 4: Generation throughput (tokens/s) on an A100. The gray text in brackets denotes batch size. MethodsEn.SumEn.QAEn.MCEn.DiaZh.QACode.DebugMath.FindRetr.PassKeyRetr.NumLlama-3-8B-1M23.0518.1465.0610.5012.4724.3637.14100.00100.00SHADOWKV21.5017.7364.6310.5012.4523.8637.43100.00100.00GLM-4-9B-1M28.619.2568.1239.5011.7730.2040.00100.00100.00SHADOWKV23.228.4868.5632.5011.2730.4640.00100.00100.00Llama-3.1-8B26.4214.4866.3816.0012.9221.0734.00100.0099.66SHADOWKV24.2313.8366.3816.5012.7621.0734.00100.0094.41Yi-9B-200K8.8810.6161.575.5013.8821.5723.71100.0099.66SHADOWKV8.9210.0659.396.0013.8920.5624.29100.0099.83 üîº Table 5 presents the accuracy of different methods (including SHADOWKV) using different models on the InfiniteBench benchmark.\nread the caption Table 5: Accuracy of different methods with different models on InfiniteBench [65]. Full paper # ","date":"28 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.21465/","section":"Paper Reviews by AI","summary":"SHADOWKV boosts long-context LLM inference throughput by up to 3.04x by cleverly caching low-rank keys on the GPU and offloading value caches to the CPU, minimizing latency while maintaining accuracy.","title":"ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference","type":"paper-reviews"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.21411 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rWanhua Li et el. 2024-10-30 ‚Üó arXiv ‚Üó Hugging Face TL;DR # Current social relation reasoning methods are limited by their generalizability and lack of interpretability. They typically rely on end-to-end training of dedicated networks on large labeled datasets, which hinders their adaptability to unseen scenarios. This paper proposes a new approach by using a modular framework, incorporating the strengths of both Vision Foundation Models (VFMs) and Large Language Models (LLMs), to improve performance and interpretability.\nThis modular approach translates visual content into a textual social story using VFMs, facilitating reasoning with LLMs. To further enhance performance, the paper introduces Greedy Segment Prompt Optimization (GSPO), an automated prompt optimization method that addresses the challenges of long prompt optimization in LLMs. The experimental results demonstrate that SocialGPT, combined with GSPO, significantly outperforms existing methods on benchmark datasets, showcasing its effectiveness and versatility.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # SocialGPT significantly advances social relation reasoning by leveraging foundation models, achieving competitive zero-shot results and offering interpretability. This opens exciting avenues for research in multimodal reasoning and prompt engineering, particularly for tasks involving complex cognitive functions like social understanding.\nVisual Insights # üîº The figure compares and contrasts end-to-end learning-based frameworks for social relation reasoning with the proposed modular framework using foundation models.\nread the caption Figure 1: (a) End-to-end learning-based framework for social relation reasoning. A dedicated neural network is trained end-to-end with full training data. (b) We propose a modular framework with foundation models for social relation reasoning. Our proposed SocialGPT first employs VFMs to extract visual information into textual format, and then perform text-based reasoning with LLMs, using either our manually designed SocialPrompt or optimized prompts. MethodsZSAcc (%)All attributes + SVM 1X57.2MethodsAcc (%)Pair CNN 13X58.0SocialGPT61.58Dual-Glance 13X59.6- Dense Captions52.63SRG-GN 54X53.6- Task-oriented Captions59.89GRM 6X62.3- Symbol ‚Üí Object Coordinate57.68MGR 2X64.4- Symbol ‚Üí Object Caption - Social Story59.83 45.31GR2N 3X64.3Segment {System}TRGAT 14X65.3- SocialPrompt - SocialPrompt Segment {Expectation }60.23 59.19SocialGPT (w/ GPT-3.5)64.1- SocialPrompt Segment {Context}61.18SocialGPT (w/ Vicuna-13B)66.7- SocialPrompt Segment {Guidance}43.56 üîº Table 1 compares the zero-shot performance of SocialGPT with several other methods on the PIPA dataset for social relationship recognition.\nread the caption Table 1: The comparison results on the PIPA dataset. ZS stands for Zero-Shot. More visual insights # More on tables MethodsZSAcc (%)Pair CNN 13X46.30GRMX64.18GR2N 3X64.70SocialGPT (w/ GPT-3.5)53.43SocialGPT (w/ Vicuna-13B)65.12 üîº Table 3 compares the zero-shot performance of SocialGPT with other methods on the PISC dataset, showing SocialGPT\u0026rsquo;s competitive accuracy.\nread the caption Table 3: The comparison results on the PISC dataset. Previous methods are replicated with open-source code to report the accuracy metric. ZS means Zero-Shot. MethodsAcc (%)BLIP-2 4135.84LLaVA 6845.12GPT-4V 5559.67SocialGPT66.70 üîº Table 4 compares the performance of SocialGPT with existing vision-language models on the PIPA dataset for social relation recognition.\nread the caption Table 4: Comparison with existing Vision-Language Models on the PIPA dataset, with SocialGPT using Vicuna-13B model. ModelPIPAPISCSocialGPT+ GSPO‚ñ≥SocialGPT+ GSPO‚ñ≥Vicuna-7B61.5862.99+1.4145.1349.79+4.66Vicuna-13B66.7069.23+2.5365.1266.19+1.07Llama2-7B31.9134.07+2.1636.7138.04+1.33Llama2-13B37.8641.27+3.4142.7448.39+5.65 üîº Table 5 presents the results when applying GSPO on SocialGPT with various LLMs for reasoning, compared with the baseline zero-shot performance.\nread the caption Table 5: Prompt tuning results (accuracy in %) with GSPO. [10]Wanhua Li, Jiwen Lu, Jianjiang Feng, Chunjing Xu, Jie Zhou, and Qi Tian. Bridgenet: A continuity-aware probabilistic network for age estimation. In CVPR, pages 1145-1154, 2019.[11]Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In ICCV, pages 7262-7272, 2021.[12]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, pages 213-229. Springer, 2020.[13]Junnan Li, Yongkang Wong, Qi Zhao, and Mohan s Kankanhalli. Dual-glance model for deciphering social relationships. In ICCV, pages 2650-2659, 2017.[14]Yunfei Guo, Fei Yin, Wei Feng, Xudong Yan, Tao Xue, Shuqi Mei, and Cheng-Lin Liu. Social relation reasoning based on triangular constraints. In AAAI, pages 737-745, 2023.[15]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael s Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.[16]Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.[17]Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023.[18]Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d language gaussian splatting. In CVPR, pages 20051-20060, 2024.[19]Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748-8763. PMLR, 2021.[20]Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc v Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 35:24824-24837, 2022.[21]Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.[22]Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc v Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In ICLR, 2023.[23]Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.[24]Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35, 2023.[25]Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In EMNLP, pages 4222-4235, 2020.[26]Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684-10695, 2022.[27]OpenAI. Gpt-4 technical report, 2023.[28]Wanhua Li, Xiaoke Huang, Zheng Zhu, Yansong Tang, Xiu Li, Jie Zhou, and Jiwen Lu. Ordinalclip: Learning rank prompts for language-guided ordinal regression. NeurIPS, 35:35313-35325, 2022.[29]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. üîº Table 1 compares the zero-shot performance of SocialGPT with previous state-of-the-art methods on the PIPA dataset for social relationship recognition.\nread the caption Table 1: The comparison results on the PIPA dataset. ZS stands for Zero-Shot. [50]Sheldon Cohen. Social relationships and health. American psychologist, 59(8):676, 2004.[51]Hope R Conte and Robert Plutchik. A circumplex model for interpersonal personality traits. Journal of personality and social psychology, 40(4):701, 1981.[52]Daphne Blunt Bugental. Acquisition of the algorithms of social life: a domain-based approach. Psycholog- ical bulletin, 126(2):187, 2000.[53]Alan P Fiske. The four elementary forms of sociality: framework for a unified theory of social relations. Psychological review, 99(4):689, 1992.[54]Arushi Goel, Keng Teck Ma, and Cheston Tan. An end-to-end network for generating social relationship graphs. In CVPR, pages 11186-11195, 2019.[55]Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith, and Douglas C Schmidt. Chatgpt prompt patterns for improving code quality, refactoring, requirements elicitation, and software design. arXiv preprint arXiv:2303.07839, 2023.[56]Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398, 2023.[57]Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? In NAACL, pages 2080-2094, 2021.[58]Timo Schick, Jane Dwivedi- Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.[59]Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint arXiv:2302.11382, 2023.[60]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 33:1877-1901, 2020.[61]Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In EMNLP, pages 11048-11064, 2022.[62]Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In NAACL, pages 2655-2671, 2022.[63]Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In ICLR, 2023.[64]Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with \"gradient descent\" and beam search. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, EMNLP, pages 7957-7968, 2023.[65]Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.[66]Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.[67]Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.[68]Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. üîº The table presents the results of prompt tuning using the Greedy Segment Prompt Optimization (GSPO) method on various LLMs for social relationship reasoning on PIPA and PISC datasets.\nread the caption Table 5: Prompt tuning results (accuracy in %) with GSPO. Full paper # ","date":"28 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.21411/","section":"Paper Reviews by AI","summary":"SocialGPT uses vision and language foundation models for zero-shot social relation reasoning, achieving state-of-the-art results with improved interpretability via Greedy Segment Prompt Optimization.","title":"SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization","type":"paper-reviews"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.21242 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rNour Jedidi et el. 2024-10-30 ‚Üó arXiv ‚Üó Hugging Face TL;DR # Zero-shot dense retrieval aims to build effective search systems without labeled data. However, existing methods often struggle with efficiency and reliance on LLMs for domain-specific knowledge. These limitations hinder their practical application and scalability.\nReDE-RF overcomes these challenges using a relevance feedback approach. Instead of generating hypothetical documents via LLMs, it uses LLMs to select relevant documents from an initial retrieval. This dramatically reduces latency while improving accuracy. The authors further introduce DistillReDE, a distilled model that achieves comparable performance without the need for LLMs at inference time, demonstrating impressive efficiency and practicality.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # This paper is crucial for researchers in information retrieval as it presents a novel zero-shot dense retrieval method that outperforms existing methods, especially in low-resource settings. It offers significant improvements in efficiency and generalizability, opening new avenues for research in unsupervised dense retrieval and LLM applications. The proposed distillation technique also provides valuable insights into model compression and efficient deployment.\nVisual Insights # üîº The chart compares the performance of three dense retrieval methods (Contriever, DistillReDE, and ReDE-RF) across five low-resource datasets using NDCG@10 as the evaluation metric.\nread the caption Figure 4: Comparison of DistillReDE to Contriever and ReDE-RF (Default: HyDEPRF). ModelInit. RetrievalHigh ResourceLow Resource (BEIR)DL19DL20NewsCovidFiQASciFactDBPediaNFCorpusRobust04BEIR (Avg.)BM25N/A50.648.039.559.523.667.931.832.240.742.2ContrieverN/A44.542.134.827.324.564.929.231.731.634.9Hybrid (BM25 + Contriever)N/A52.950.942.252.928.471.634.833.743.043.8ContrieverAvgPRF PRF-Depth: 3Hybrid (3)52.146.443.448.122.859.931.932.338.639.6PRF-Depth: 20Hybrid (20)49.246.239.951.312.036.928.621.739.232.8Zero-Shot Dense RetrievalPromptReps (Llama3-8B-I)N/A---59.527.152.731.529.6--HyDE (Mistral-7B-Instruct)N/A57.853.944.056.921.665.135.327.741.541.7HyDEPRF (Mistral-7B-Instruct)Hybrid (20)63.562.046.959.128.364.535.235.045.644.9ReDE-RF (Ours)Hybrid59.446.167.437.047.0Default: Contriever(20)60.365.628.234.849.8Default: HyDEPRFHybrid (20)62.860.447.165.629.366.937.635.551.747.7Supervised Dense RetrievalDPRN/A62.265.316.133.211.231.826.318.925.223.2ANCEN/A64.564.638.265.429.550.728.123.739.239.3ContrieverFTN/A62.163.242.859.632.967.741.332.847.346.3 üîº Table 1 presents the evaluation results of various zero-shot and supervised dense retrieval methods on the TREC and BEIR datasets, comparing their NDCG@10 scores across high and low resource domains.\nread the caption Table 1: Results (NDCG@10) on TREC and BEIR. We report the mean NDCG@10 across three runs for HyDE, HYDEPRF, and ReDE-RF (Default: HyDEPRF). The average standard deviation across all datasets for HyDE, HYDEPRF and ReDE-RF (Default:HyDEPRF) was ‚âà 0.4%, ‚âà 0.5% and ‚âà 0.1% respectively. Exact numbers can be found in Appendix C. More visual insights # More on tables DL20NewsDBpediaMax k*154.235.831.5558.540.835.01060.041.136.12059.041.235.4 üîº This table shows the impact of varying the number of documents used to update the ReDE-RF query embedding on NDCG@10 for three datasets.\nread the caption Table 3: Impact of the number of documents used to update the ReDE-RF (No Default) query embedding. PromptDL19DL20Figure 259.159.0pointwise.yes_no (Zhuang et al., 2024c)61.456.1RG-YN (Zhuang et al., 2024a)57.851.8RG-YN*59.854.9Thomas et al. (2024)61.456.9 üîº Table 5 shows the impact of different prompt variations on the ReDE-RF model\u0026rsquo;s performance, measured by NDCG@10 and NDCG@20 on DL19 and DL20 datasets.\nread the caption Table 5: Impact of different prompts on ReDE-RF (No Default). For Thomas et al. (2024), we make the relevance options binary. Prompts are in Appendix G. DefaultInit. RetrievalCovidNFCorpusFiQARobust04DBPediaContrieverHybrid65.634.828.249.837.0HyDEPRFHybrid65.635.529.351.737.6DistillReDEDistillReDE63.835.630.247.937.8DistillReDEHybrid*66.335.830.949.238.4 üîº Table 6 presents the NDCG@10 scores of ReDE-RF using Contriever, HyDEPRF, and DistillReDE as initial retrievers, comparing their performance across various low-resource datasets.\nread the caption Table 6: NDCG@10 of ReDE-RF when implemented with DistillReDE. Hybrid* is a hybrid system that combines results from BM25 and DistillReDE. Hybrid is BM25 + Contriever, as in Table 1. MethodCovid NDCG@10/20DL19 NDCG@10/20News NDCG@10/20Mistral-7B-InstructReDE-RF65.6/57.962.8 / 60.347.1/ 43.8Hybrid + PR63.6/49.660.0/53.745.6/42.5ReDE-RF + PR68.8 / 58.962.6/59.945.8/43.2Gemma-2-9B-itReDE-RF67.9 / 60.062.0/61.346.6/42.8Hybrid + PR65.0% 50.863.7/55.846.3/41.9ReDE-RF + PR72.3/61.270.7 / 64.843.4/41.5Llama-3.1-8B-IReDE-RF65.7/58.759.0/59.049.9/45.7Hybrid + PR67.1/51.866.8/57.447.2/42.8ReDE-RF + PR72.9/61.170.1/64.348.6/45.4 üîº Table 7 compares ReDE-RF\u0026rsquo;s performance against pointwise re-ranking using three different LLMs across multiple metrics, showing that ReDE-RF consistently outperforms pointwise re-ranking in terms of NDCG@20.\nread the caption Table 7: Comparing ReDE-RF (Default: HyDEPRF) to pointwise reranking (PR). ReDE-RF + PR reranks the top-20 passages returned from ReDE-RF. Bold denotes best overall system. Underline denotes best between ReDE-RF and Hybrid + PR. Dataset# QueriesTREC DL1943TREC DL2054TREC-News57TREC-Covid50FiQA648SciFact300DBPedia400NFCorpus323Robust04249 üîº Table 1 presents the NDCG@10 results on the TREC and BEIR datasets for various zero-shot and supervised dense retrieval methods, showing ReDE-RF\u0026rsquo;s superiority in low-resource settings and competitive performance in high-resource ones.\nread the caption Table 1: Results (NDCG@10) on TREC and BEIR. We report the mean NDCG@10 across three runs for HyDE, HYDEPRF, and ReDE-RF (Default: HyDEPRF). The average standard deviation across all datasets for HyDE, HYDEPRF and ReDE-RF (Default:HyDEPRF) was ‚âà 0.4%, ‚âà 0.5% and ‚âà 0.1% respectively. Exact numbers can be found in Appendix C. ModelDL19DL20NewsCovidFiQASciFactDBPediaNFCorpusRobust04HyDE (Mistral-7B-Instruct)57.853.944.056.921.665.135.327.741.5¬±0.3¬±0.7¬±0.6¬±0.8¬±0.2¬±0.2¬±0.3¬±0.2¬±0.4HyDEPRF (Mistral-7B-Instruct)63.562.046.959.128.364.535.235.045.6¬±1.2¬±0.2¬±1.1¬±0.5¬±0.2¬±0.5¬±0.1¬±0.2¬±0.2ReDE-RF (Default: HyDEPRF)62.860.447.165.629.366.937.635.551.7¬±0.0¬±0.1¬±0.3¬±0.1¬±0.1¬±0.3¬±0.1¬±0.1¬±0.1 üîº Table 8 presents the NDCG@10 scores of HyDE, HyDEPRF, and ReDE-RF across multiple datasets, showing the mean and standard deviation across three runs for each model.\nread the caption Table 8: NDCG@10 of HyDE-Mistral-7B-Instruct and ReDE-RF (w/ HyDE) with standard deviations across three runs. ModelInit RetrievalDL19DL20NewsCovidFiQASciFactDBPediaNFCorpusRobust04HyDEPRF (Mistral-7B-Instruct)Hybrid (20)63.562.046.959.128.364.535.235.045.6HyDEPRF (Mistral-7B-Instruct)Hybrid (10)63.660.547.258.828.363.434.634.644.5 üîº Table 9 shows the NDCG@10 scores of HyDEPRF using different numbers of initially retrieved documents as context on various datasets.\nread the caption Table 9: NDCG@10 of HyDEPRF across different number of initially retrieved documents used as context. Full paper # ","date":"28 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.21242/","section":"Paper Reviews by AI","summary":"ReDE-RF revolutionizes zero-shot dense retrieval by using relevance feedback from LLMs to refine query embeddings, achieving state-of-the-art results with vastly improved efficiency.","title":"Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback","type":"paper-reviews"},{"content":"","date":"27 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-bytedance-inc./","section":"Tags","summary":"","title":"üè¢ ByteDance Inc.","type":"tags"},{"content":"","date":"27 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-mit/","section":"Tags","summary":"","title":"üè¢ MIT","type":"tags"},{"content":"","date":"27 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-princeton-university/","section":"Tags","summary":"","title":"üè¢ Princeton University","type":"tags"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.20305 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rFranklin Wang et el. 2024-10-30 ‚Üó arXiv ‚Üó Hugging Face TL;DR # Traditional preference tuning methods for LLMs involve redundant computations, especially when prompts are long. This is inefficient and limits the scalability of training. Existing approaches process chosen and rejected responses separately, thus repeating calculations for shared prompt prefixes.\nThis paper introduces \u0026lsquo;prefix sharing\u0026rsquo;, a technique that processes both chosen and rejected responses together, sharing the common prefix to mitigate redundancy. A custom attention mask prevents cross-response contamination, and the method is combined with sequence packing for further efficiency. Experiments showed a 1.1-1.5x training speedup on various datasets without compromising convergence, demonstrating the practical benefits and scalability of this approach.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # This paper is crucial for researchers working on fine-tuning large language models (LLMs) using preference data. It introduces a novel efficiency improvement that can significantly speed up training, making large-scale preference-based fine-tuning more accessible. The open-sourced code further accelerates adoption and future research.\nVisual Insights # üîº Figure 1 illustrates how prefix sharing combines chosen and rejected responses into a single sequence to avoid redundant computations of shared prompts by modifying the attention mask.\nread the caption Figure 1: Method overview. Prefix sharing removes redundant computation of the shared prompt prefix by combining the responses into a single sequence and modifying the attention mask to prevent cross-response contamination. üîº The chart displays the relative speedup of using prefix sharing in the MLP layer of the Mistral-7B model compared to the normal paired format, for various prefix lengths and prefix-to-completion ratios, showing near-ideal linear speedup for longer prefix lengths.\nread the caption Figure 3: Microbenchmarking results of the MLP layer for Mistral 7B. Relative speedups of prefix sharing over normal paired data are shown in comparison to the ideal speedup (assuming linear runtime). We see that the MLP layer scales very closely to the ideal speedup and that increasing the prefix length helps push the speedup closer to the ideal for a given prefix to completion ratio. Franklin Wang Sumanth HegdeMIT CSAILAnyscalefxwang@mit ¬∑ edusumanthrh@anyscale . edu üîº Table 1 compares the training throughput (samples per second) of different attention mechanisms (FlashAttention-3, FlexAttention, and FlexAttention with prefix sharing) across several preference optimization datasets, showing speedups gained from using prefix sharing.\nread the caption Table 1: Comparison of training samples per second for different attention implementations. Relative speedups over FlashAttention-3 and FlexAttention, respectively, are shown for the FlexAttention with Prefix Sharing column. FlexAttention with prefix sharing consistently outperforms the baselines, with speedups ranging from 1.1-1.5x, with FlexAttention alone being slower than FA3. For the Prefix / Completion column, we report the median ratio. For high median overall lengths (\u003e 500), the gains from prefix sharing are \u003e 35%, with better gains for high prefix to completion ratios. More visual insights # More on charts üîº Figure 4 shows the relative speedups of FlexAttention with prefix sharing compared to FlashAttention-3 and FlexAttention without prefix sharing for the self-attention operation of Mistral-7B, considering various prefix and completion lengths.\nread the caption Figure 4: Microbenchmarking results of the self-attention operation only for Mistral 7B. Relative speedups of FlexAttention with prefix sharing over FlashAttention-3 and FlexAttention are shown, along with the ideal speedup (assuming perfect quadratic scaling). We see that for high prefix lengths, FlexAttention with prefix sharing attains nearly ideal speedups over FlexAttention without prefix sharing, but overall it is still slower or similar in speed to FlashAttention-3. üîº Figure 5 shows the microbenchmarking results of the full self-attention layer for Mistral 7B, comparing the relative speedups of FlexAttention with prefix sharing against FlexAttention and FlashAttention-3, also showing the ideal speedup.\nread the caption Figure 5: Microbenchmarking results of the full self-attention layer (QKV projection + self-attention) for Mistral 7B. Relative speedups of FlexAttention with prefix sharing over FlashAttention-3 and FlexAttention are shown, along with the ideal speedup (assuming linear runtime). We see that although FlexAttention is slower than FlashAttention-3 for lower ratios between the prefix and completion length, as the ratio grows, FlexAttention with prefix sharing become faster. üîº Figure 5 shows the microbenchmarking results of the full self-attention layer for Mistral 7B, comparing the relative speedups of FlexAttention with prefix sharing against FlashAttention-3 and FlexAttention without prefix sharing, also showing the ideal speedup assuming linear runtime.\nread the caption Figure 5: Microbenchmarking results of the full self-attention layer (QKV projection + self-attention) for Mistral 7B. Relative speedups of FlexAttention with prefix sharing over FlashAttention-3 and FlexAttention are shown, along with the ideal speedup (assuming linear runtime). We see that although FlexAttention is slower than FlashAttention-3 for lower ratios between the prefix and completion length, as the ratio grows, FlexAttention with prefix sharing become faster. More on tables DatasetMedian Overall LenPrefix / CompletionFA3Flex AttnFlex + Prefix SharingCapybara11601.598.387.7511.90 (1.42x, 1.54x)HH-RLHF1862.1533.7130.2536.11 (1.07x, 1.19x)MetaMath-DPO 20 358723.9113.8613.0219.13 (1.38x, 1.47x)TLDR 2841611.1431.4329.5335.36 (1.12x, 1.20x)Tulu-Helpsteer / 317756.3414.8313.9321.75 (1.47x, 1.56x)Ultrafeedback 44090.4218.4017.3120.46 (1.11x, 1.18x) üîº Table 1 compares the training throughput (samples per second) of three different attention mechanisms across six datasets, showing the speedup achieved by using prefix sharing.\nread the caption Table 1: Comparison of training samples per second for different attention implementations. Relative speedups over FlashAttention-3 and FlexAttention, respectively, are shown for the FlexAttention with Prefix Sharing column. FlexAttention with prefix sharing consistently outperforms the baselines, with speedups ranging from 1.1-1.5x, with FlexAttention alone being slower than FA3. For the Prefix / Completion column, we report the median ratio. For high median overall lengths (\u003e 500), the gains from prefix sharing are \u003e 35%, with better gains for high prefix to completion ratios. Dataset NameMedian Overall LenPrefix / CompletionFA3 + PackingFlex Attn + PackingFlex Attn + Prefix Sharing + PackingCapybara 511601.5917.8917.6323.89 (1.34x, 1.36x)HH-RLHF 21862.15109.77104.99155.04 (1.41x, 1.48x)MetaMath-DPO 20 358723.9124.2123.8338.07 (1.57x, 1.60x)TLDR 2841611.1444.1143.2259.76 (1.35x, 1.38x)Tulu-Helpsteer 7 317756.3429.8528.9844.10 (1.48x, 1.52x)Ultrafeedback 44090.4245.4644.1353.21 (1.17x, 1.21x) üîº Table 2 presents the training throughput (samples per second) for different attention mechanisms with and without sequence packing, showing that prefix sharing with packing consistently improves training efficiency for most datasets.\nread the caption Table 2: Comparison of training samples per second with sequence packing. For Flex Attn + Prefix Sharing + Packing, relative speedups over FA3 + Packing and Flex Attn + Packing are shown in parentheses, respectively. For the Prefix / Completion column, we report the median ratio. Our method (Prefix sharing + Packing) demonstrates at least a 30% increase in training throughput for most datasets. The impact of sequence packing is especially prominent for datasets like HH-RLHF and TLDR with shorter overall sequence lengths. Only Ultrafeedback, which has a extremely low prefix-to-completion ratio (0.3), shows a modest improvement of 21% over the FlexAttention baseline. MethodBatch SizeMT-Bench ScorePacking + Prefix Sharing327.3647.4967.31287.0Normal Paired Format327.0647.1967.21287.1Baseline (Zephyr-7B-SFT)N/A6.4 üîº Table 3 compares the MT-Bench scores achieved by models trained with packing and without packing for different batch sizes, showing that packing does not significantly harm downstream performance.\nread the caption Table 3: MT-Bench [37] scores for different packing and non-packing DPO training across different batch sizes. Models were trained with Ultrafeedback using hyperparameters from Zephyr [29]. Full paper # ","date":"27 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.20305/","section":"Paper Reviews by AI","summary":"Boosting LLM training speed by 1.3-1.6x, this research introduces \u0026lsquo;prefix sharing\u0026rsquo; for preference optimization, processing chosen and rejected responses as one sequence to remove redundancy.","title":"Accelerating Direct Preference Optimization with Prefix Sharing","type":"paper-reviews"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.20424 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rZiming Li et el. 2024-10-30 ‚Üó arXiv ‚Üó Hugging Face TL;DR # Many existing automated data science systems struggle with complex real-world tasks, often relying on one-step analysis or pre-built knowledge bases, leading to inflexibility and a lack of transparency. There is a lack of systems addressing the entire data science pipeline efficiently and reliably, especially in competitive settings like Kaggle competitions.\nAutoKaggle solves this by using a multi-agent system with specialized agents for each phase (background understanding, data cleaning, feature engineering, modeling etc.). It uses iterative debugging and testing to ensure code quality and incorporates a machine learning tools library to speed up common tasks. Results on 8 Kaggle competitions show high validation submission rates and comprehensive scores, highlighting AutoKaggle\u0026rsquo;s effectiveness in handling complex data science pipelines.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # This paper is crucial for researchers in automated machine learning and multi-agent systems. It presents a novel, practical framework for tackling complex data science tasks, addressing limitations of existing LLM-based approaches. The open-source nature and detailed evaluations make it readily reproducible and valuable for benchmarking future research in autonomous data science.\nVisual Insights # üîº The figure illustrates the iterative debugging and testing process within the AutoKaggle framework, showing how code is generated, executed, debugged, and tested until it passes all unit tests.\nread the caption Figure 2: Iterative debugging and testing. üîº The bar chart compares the average normalized performance scores achieved by AutoKaggle using different model settings (GPT-40 and 01-mini) and AIDE across eight Kaggle tasks.\nread the caption Figure 3: Average normalized performance score for different settings/tasks. Task 1Task 2Task 3Task 5Avg.VSNo Tools0.800.600.500.400.58DC Tools0.800.701.001.000.88DC \u0026 FE Tools0.800.600.600.600.65All Tools1.00 -0.80 -0.80-0.80 - -0.85CSNo Tools- 0.781- - 0.697- 0.666-- - 0.602- - „ÄÅ 0.687DC Tools0.7810.7210.9280.9090.835DC \u0026 FE Tools0.7870.6840.7350.7130.730All Tools0.8880.7860.8310.8100.829 üîº Table 1 presents the made submission rate, valid submission rate, and comprehensive score achieved by AutoKaggle and baselines on eight distinct Kaggle data science tasks, each trial repeated five times for robust evaluation.\nread the caption Table 1: Made submission, valid submission and comprehensive score on 8 Kaggle tasks. Each experiment is repeated with 5 trials. The best performances on individual tasks are underlined, and the best performances across all tasks are bolded. More visual insights # More on charts üîº The chart displays the relationship between the number of debugging times allowed and the comprehensive score achieved across different tasks.\nread the caption Figure 5: Comprehensive Score across different debugging times. üîº The chart displays the debugging time and average performance (completion rate and comprehensive score) in different settings (no tools, data cleaning tools, data cleaning and feature engineering tools, and all tools) across various Kaggle competitions.\nread the caption Figure 4: Left. Debugging time and Right. Average performance in competitions. üîº The chart displays the relationship between the comprehensive score achieved by AutoKaggle and the debugging time allowed, demonstrating performance improvements with increased debugging time.\nread the caption Figure 5: Comprehensive Score across different debugging times. üîº The histogram shows the distribution of passenger ages before outlier removal in the Titanic dataset.\nread the caption Figure 6: The histogram of age before outliers are processed More on tables Task 1Task 2Task 3Task 5Avg.CRw/o Unit Tests0.2000.2000.10w/ Unit Tests1.000.80 -0.80 -0.80 -0.85 - -CSw/o Unit Tests- 0.478- 0-- - 0.482- 0- 0.240w/ Unit Tests0.8880.8310.7860.8100.829 üîº Table 1 presents the performance of AutoKaggle and its baselines across eight Kaggle tasks using three metrics: Made Submission, Valid Submission, and Comprehensive Score.\nread the caption Table 1: Made submission, valid submission and comprehensive score on 8 Kaggle tasks. Each experiment is repeated with 5 trials. The best performances on individual tasks are underlined, and the best performances across all tasks are bolded. Error Type (Count)DescriptionValue Error (49)Fail to match the expected type or range of the input valuesKey Error (44)Attempt to access a dictionary element using a key that does not existFile Error (8)Attempt to access a file that does not exist in the specified locationModel Error (8)Incorrect setup in the parameters or structure of a model, leading to opera- tional failuresType Error (25)Mismatch between expected and actual data type, leading to operational failureTimeout Error (6)Failure to complete a process within the allocated time periodIndex Error (3)Attempt to access an element at an index that is outside the range of a list or arrayAssertion Error (1)An assertion condition in the code is not met, indicating an unmet expected constraintName Error (2)Use of an undeclared variable that is not recognized by the systemAttribute Error (2)Attempt to access an attribute or method that does not exist for an objectIndentation Error (1)Incorrect indentation disrupts code structure, preventing proper parsing üîº Table 1 presents the made submission rate, valid submission rate, and comprehensive score achieved by AutoKaggle and baseline models across eight Kaggle tasks, each repeated five times.\nread the caption Table 1: Made submission, valid submission and comprehensive score on 8 Kaggle tasks. Each experiment is repeated with 5 trials. The best performances on individual tasks are underlined, and the best performances across all tasks are bolded. CategoryNo.Task NameTaskLevelTeamsTrainTestClassic1TitanicClassificationMedium139948914182Spaceship TitanicClassificationEasy1720869342773House PricesRegressionMedium4383146014594MonstersClassificationEasy763371529Recent5- - - - Academic SuccessRegression- - Medium- 2684- - - 76.5K- - - 51K6Bank ChurnRegressionEasy3632165K110K7Obesity RiskClassificationEasy358720.8K13.8K8Plate DefectRegressionMedium219919.2K12.8K üîº Table 1 presents the made submission rate, valid submission rate, and comprehensive score achieved by AutoKaggle and baselines on eight different Kaggle tasks, showing AutoKaggle\u0026rsquo;s superior performance.\nread the caption Table 1: Made submission, valid submission and comprehensive score on 8 Kaggle tasks. Each experiment is repeated with 5 trials. The best performances on individual tasks are underlined, and the best performances across all tasks are bolded. StateUnit test nameUnit test descriptionState DCtest_document_existTest if cleaned_train.csv and cleaned_test.csv data exist.test_no_duplicate_cleaned_trainTest if there are any duplicate rows in the cleaned_train.csv.test_no_duplicate_cleaned_testTest if there are any duplicate rows in the cleaned_test.csv.test_readable_cleaned_trainTest if the cleaned_train.csv is readable.test_readable_cleaned _testTest if the cleaned_ test.csv is readable.test_cleaned_train_no_missing_ valuesTest if the cleaned_train.csv contains missing value.test_cleamed_test_no_missing_valuesTest if the cleaned_test.csv contains missing value.test_cleaned_train_no_duplicated _featuresTest if the cleaned_train.csv contains duplicate features.test_cleaned_test_no_duplicaned_featuresTest if the cleaned_test.csv contains duplicate features.test_cleaned_difference_train_test_columnsTest if the cleaned_train.csv and cleaned_test.csv have the same features except for target variable.test_cleaned_train_no_missing_targetTestif the target variable is in cleaned_train.csv.State FEtest_document_exist- Test if processed_train.csv and pro- cessed_test.csv data exist.test_processed_train_feature_numberTest if the feature engineering phase is per- formed well in processed_train.csv.test_processed_test_feature_numberTest if the feature engineering phase is per- formed well in processed_test.csv.test_file_sizeTest if processed data is larger than a threshold.test_processed_train_no_duplicated _features test_processed_ter_no_duplicated_featuresTest if the processed_ train.csv contains dupli- cate features.test_processed_difference_train_test_coummsTest if the processed test.csv contains duplicate features.Test if the processed _train.csv and pro- cessed_test.csv have the same features except for target varibale.test_processed_train_no_missing_targetTest if the target variable is in pro- cessed_train.csv.State MB VP- - - test_document_exist- - - - - - - - - - - - Test if a submission file exists.test_no_duplicate_submissionTest if there are any duplicate rows in the sub- mission file.test_readable_submissiontest if the submission file is readable.test_file _num_submissionTest if the submission file and sam- ple_submission.csv have the same number of rows.test_column_narnes_submissionTest if the submission file and sam- ple_submission.csv have the same column names.test_submission_validity1) Test if the submission file and sam- ple_submission.csv have the same data in- dex. 2) Test if the submission file and sam- ple_submission.csv have the same numerical range. üîº Table 1 presents the performance metrics of AutoKaggle and baselines across eight different Kaggle tasks, showing the made submission rate, valid submission rate, and comprehensive score.\nread the caption Table 1: Made submission, valid submission and comprehensive score on 8 Kaggle tasks. Each experiment is repeated with 5 trials. The best performances on individual tasks are underlined, and the best performances across all tasks are bolded. StateTool nameTool descriptionState DCFillMissing ValuesFills missing values or removes columns with missing values based on a threshold.RemoveColumns WithMissingDataRemoves columns containing missing values from a DataFrame based on a threshold.DetectAndHandleOutliersZscoreDetects and handles outliers in specified columns using the Z-score method.DetectAndHandleOutliersIqrDetects and handles outliers in specified columns using the Interquartile Range (IQR) method.RemoveDuplicatesRemoves duplicate rows from a DataFrame.ConvertDataTypesConverts the data type of specified columns in a DataFrame.FormatDatetimeFormats datetime columns to a specified format. - -State FEOneHotEncodePerforms one-hot encoding on specified categorical columns.LabelEncodePerforms label encoding on specified categorical columns.FrequencyEncodePerforms frequency encoding on specified categorical columns.TargetEncodePerforms target encoding on specified categorical columns.CorrelationFeatureSelectionPerforms feature selection based on correlation analy- sis.VarianceFeatureSelectionPerforms feature selection based on variance analysis.ScaleFeaturesScales numerical features in the specified columns of a DataFrame.PerformPcaPerforms Principal Component Analysis (PCA) on the specified columns of a DataFrame.PerformRfePerforms Recursive Feature Elimination (RFE) on the specified columns of a DataFrame.CreatePolynomialFeaturesCreates polynomial features from specified columns of a DataFrame.CreateFeatureCombinationsCreates feature combinations from specified columns of a DataFrame. - - - -State MBVPTrainAndValidation AndSelectTheBestModelTrains, evaluates, and selects the best machine learning model based on the training data and labels, returning the best performing model along with the performance scores of each model and their best hyperparameters. üîº Table 1 presents the performance results of AutoKaggle and its baselines on eight Kaggle tasks across three evaluation metrics: Made Submission, Valid Submission, and Comprehensive Score.\nread the caption Table 1: Made submission, valid submission and comprehensive score on 8 Kaggle tasks. Each experiment is repeated with 5 trials. The best performances on individual tasks are underlined, and the best performances across all tasks are bolded. Markdown-formatted tool schema for FillMissing ValuesDescription: Fill missing values in specified columns of a DataFrame. This tool can handle both numerical and categorical features by using different filling methods. Applicable Situations: Handle missing values in various types of features. Parameters: ¬∑ data: - Type: pd. DataFrame - Description: A pandas DataFrame object representing the dataset. ¬∑ columns: - Type: string array - Description: The name(s) of the column(s) where missing values should be filled. ¬∑ method: - Type: string - Description: The method to use for filling missing values. - Enum: auto I mean I median mode constant - Default: auto ‚óè fill_value: - Type: number I string null - Description: The value to use when method is constant. - Default: None Required: data, col umns Result: Successfully fill missing values in the specified column(s) of data. Notes: ¬∑ The auto method uses mean for numeric columns and mode for non-numeric columns. ¬∑ Using mean or median on non-numeric columns will raise an error. ¬∑ The mode method uses the most frequent value, which may not always be appro- priate. ¬∑ Filling missing values can introduce bias, especially if the data is not missing com- pletely at random. ¬∑ Consider the impact of filling missing values on your analysis and model perfor- mance. üîº Table 1 presents the made submission rate, valid submission rate, and comprehensive score achieved by AutoKaggle and baselines across eight distinct Kaggle tasks, each evaluated with five trials.\nread the caption Table 1: Made submission, valid submission and comprehensive score on 8 Kaggle tasks. Each experiment is repeated with 5 trials. The best performances on individual tasks are underlined, and the best performances across all tasks are bolded. Full paper # ","date":"27 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.20424/","section":"Paper Reviews by AI","summary":"AutoKaggle, a novel multi-agent framework, automates Kaggle data science competitions with high accuracy, integrating LLM reasoning, iterative debugging, and a custom machine learning tools library.","title":"AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions","type":"paper-reviews"},{"content":"","date":"27 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/deep-learning/","section":"Tags","summary":"","title":"Deep Learning","type":"tags"},{"content":"","date":"27 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.21333 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rRyan Liu et el. 2024-10-30 ‚Üó arXiv ‚Üó Hugging Face TL;DR # This paper investigates the effectiveness of chain-of-thought (CoT) prompting, a widely used technique for enhancing large language models\u0026rsquo; (LLMs) performance. While CoT has shown improvements in many areas, its impact remains an active research question, especially regarding scenarios where it negatively affects performance. The researchers explore this by drawing parallels between human cognitive processes and LLMs, focusing on situations where verbal reasoning impairs human accuracy.\nThe study systematically examines six tasks across various LLM categories. They identify three task types where both human and model performance decreases with deliberation (implicit statistical learning, visual recognition, and classification with exceptions). In these scenarios, CoT significantly reduces model accuracy. In contrast, CoT benefits tasks where the constraints governing human and model performance differ. The findings highlight the need for careful consideration of task characteristics when using CoT, suggesting that it might not be universally beneficial, and providing insights into the limitations of current LLMs.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # This paper is crucial for researchers working with large language models (LLMs). It challenges the common assumption that chain-of-thought (CoT) prompting always improves performance, offering valuable insights into when CoT can be detrimental. This work bridges cognitive psychology and machine learning, providing a novel heuristic for understanding LLM limitations. It also opens new avenues for research into prompt engineering and the development of more robust and reliable LLMs.\nVisual Insights # üîº The figure shows six tasks evaluated to determine whether chain-of-thought prompting reduces performance, categorized by whether verbal thinking hurts human performance and whether the constraints generalizing human performance also apply to LLMs.\nread the caption Figure 1: Tasks evaluated for reductions in performance from CoT prompting. Implicit Statistal Learning (ISL): Classification of strings generated by an artificial grammar. Face Recognition (FR): Recognition of a face from a set that shares similar descriptions. Classification of Data with Exceptions (CDE): Learning labels in the presence of exceptions. Natural Language Inference (NLI): Recognizing a logical inconsistency. Spatial intuitions (SI): Tilting water glasses. Working Memory (WM): Aggregating features for a decision. Humans show reductions in performance when engaging in verbal thinking in all tasks, we show that the first three have similar effects on LLMs and VLMs, while the last three differ between humans and models in meaningful ways. üîº The chart shows the learning curves of GPT-40 using direct prompting and chain-of-thought prompting on a classification task with exceptions, revealing that direct prompting leads to faster and more accurate learning than chain-of-thought prompting.\nread the caption Figure 5: Aggregate learning curve (number of correct objects classified out of 10) for GPT-40 prompted via direct prompting and chain-of-thought over 15 iterations. Direct prompting attains perfection very quickly, whereas chain-of-thought prompting results in stagnation. Zero-shotCoTPerformance decreasep-valueGPT-4o (subset)94.00%-36.30%\u0026lt; 0.0001OpenAI o1-preview (subset)-57.70%GPT-4o87.50%64.40%23.10%\u0026lt; 0.0001Claude 3 Opus70.70%62.70%8.00%\u0026lt; 0.0001Claude 3.5 Sonnet65.90%67.70%-1.80%0.969Gemini 1.5 Pro68.00%61.95%6.05%\u0026lt; 0.0001Llama 3 8B Instruct59.70%57.90%1.80%\u0026lt; 0.05Llama 3 70B Instruct60.50%58.30%2.20%\u0026lt; 0.05Llama 3.1 8B Instruct53.52%51.54%1.98%\u0026lt; 0.0001Llama 3.1 70B Instruct65.90%57.10%8.80%\u0026lt; 0.0001 üîº The table presents the performance of various LLMs on an artificial grammar learning task using zero-shot and chain-of-thought prompting, showing the significant drop in performance with CoT.\nread the caption Table 1: Results contrasting zero-shot and CoT for artificial grammar learning. More visual insights # More on tables Zero-shotCoTPerformance decrease (absolute)Performance decrease (relative)p-valueGPT-4o64.00%51.20%12.80%20.00%\u0026lt; 0.01Claude 3 Opus44.00%29.60%14.40%32.73%\u0026lt; 0.0001Claude 3.5 Sonnet97.80%94.80%3.00%3.07%\u0026lt; 0.05Gemini 1.5 Pro66.00%54.60%11.40%17.27%\u0026lt; 0.05Intern VL2 26B9.20%6.00%3.20%34.78%\u0026lt; 0.05Intern VL2 Llama3 76B15.77%13.77%2.00%12.68%0.44 üîº The table shows the performance of various large multimodal models on a facial recognition task using zero-shot and chain-of-thought prompting, revealing consistent drops in accuracy for all models when using CoT.\nread the caption Table 2: Comparison of zero-shot and CoT prompts for facial recognition. DirectCoT# Rounds increase (absolute)# Rounds increase (relative)p-valueGPT-4o2.912.59.6331%\u0026lt; 0.0001Claude 3.5 Sonnet2.36.44.1178%\u0026lt; 0.0001Claude 3 Opus2.45.53.1129%\u0026lt; 0.05 üîº The table presents the average number of rounds needed for three large language models to correctly classify all vehicles in a dataset using direct prompting versus chain-of-thought prompting, showing a significant increase in rounds needed with chain-of-thought.\nread the caption Table 3: Average number of rounds for models to learn labels using either direct or CoT prompting. MNLISNLISyntheticZero-shotCoTZero-shotCoTZero-shotCoTOpenAI o1-preview (subset)-----86.5%GPT-4o53.2%93.9%51.4%94.3%51.0%74.0%Claude 3.5 Sonnet65.2%67.5%67.4%69.8%56.7%57.8%Claude 3 Opus62.7%58.8%66.2%58.7%54.5%51.8%Gemini 1.5 Pro73.2%68.2%68.8%63.9%60.5%61.5%Llama 3.1 70B Instruct55.6%81.6%50.4%82.3%50.0%65.8% üîº The table presents the zero-shot and chain-of-thought performance of several LLMs on a logical inconsistency task, using stimuli from three different datasets.\nread the caption Table 4: Results comparing zero-shot and CoT across the logical inconsistency task using stimuli from MNLI, SNLI, and synthetic LLM generation. Zero-shotCoTPerformance change (absolute)Performance change (relative)p-valueGPT-4o38%40%+2%+5.00%0.61Claude 3.5 Sonnet42%38%-4%-10.53%0.28Claude 3 Opus42%38%-4%-10.53%0.28Gemini 1.5 Pro35%36%+1%+2.78%0.99InternVL2 Llama3 76B39%31%-8%-25.81%0.67 üîº Table 5 shows the comparison of zero-shot and chain-of-thought prompting on the spatial intuition task, indicating performance change (absolute and relative) and p-values for various models.\nread the caption Table 5: Results comparing zero-shot and CoT on the spatial intuition task. [0.1, 0.3][0.3, 0.5][0.5, 1]Zero-shotCoTZero-shotCoTZero-shotCoTGPT-4o47%45%57%56%80%87%Claude 3.5 Sonnet50%62%62%72%81%95%Claude 3 Opus35%50%57%58%72%84%Llama 3.1 70B Instruct42%6%44%5%43%20% üîº The table presents the zero-shot and chain-of-thought prompting performance of four LLMs on an apartment selection task, categorized by three ranges of apartment quality differences.\nread the caption Table 6: Results for apartment selection task across four models and three ranges of Œî. Table 7: Example prompt for artificial grammar learning task, zero shot.Prompt:These strings were generated according to a certain set of rules. Does the following string also follow the same set of rules?[test example]Please ONLY answer \"Yes\" or \"No\". üîº The table presents a comparison of the performance of various large language models on an artificial grammar learning task using zero-shot prompting versus chain-of-thought prompting, showing a significant performance decrease with CoT prompting.\nread the caption Table 1: Results contrasting zero-shot and CoT for artificial grammar learning. Zero-shotCoTToTPerformance decrease (CoT)Performance decrease (ToT)p-value (CoT)p-value (ToT)GPT-4o94.00%62.52%64.55%31.48%29.45%\u0026lt; 0.0001\u0026lt; 0.0001 üîº The table presents the performance of GPT-40 model on artificial grammar learning task using zero-shot, chain-of-thought, and tree-of-thought prompting methods, showing the accuracy and p-values for each method.\nread the caption Table 8: Results comparing zero-shot, CoT, and ToT on a subset of the artificial grammar learning task. Unique featuresPattern-related featuresIrrelevant featuresLicense Plate'Cold' (Class A)/'Warm' (Class B) climateTransmissionSeat coversDoorsA23BCDDrives on glaciersManualClothTwoB34EFGMade in NorwayAutomaticVinylTwoC45HIJUsed in mountain climbingAutomaticVinylFourD56KLMDrives in junglesManualVinylFourE67NOPHas treadsManualClothTwoF78QRSHeavily insulatedManualVinylFourG89TUVMade in AfricaManualClothFourH90WXYHas wheelsAutomaticClothTwoJ12ZABLightly insulatedManualVinylTwoK23CDEUsed on safarisAutomaticVinylTwo üîº The table presents the performance of nine different language models on an artificial grammar learning task, comparing their accuracy with zero-shot prompting versus chain-of-thought prompting, showing consistent performance decrease with CoT prompting.\nread the caption Table 1: Results contrasting zero-shot and CoT for artificial grammar learning. Full paper # ","date":"27 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.21333/","section":"Paper Reviews by AI","summary":"Chain-of-thought prompting can hurt LLMs\u0026rsquo; performance on tasks where human deliberation worsens accuracy; this research identifies those tasks and offers a new tool for prompt design.","title":"Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse","type":"paper-reviews"},{"content":"","date":"26 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-university-of-texas-at-austin/","section":"Tags","summary":"","title":"üè¢ University of Texas at Austin","type":"tags"},{"content":"","date":"26 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/question-answering/","section":"Tags","summary":"","title":"Question Answering","type":"tags"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.20088 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rAtula Tejaswi et el. 2024-10-30 ‚Üó arXiv ‚Üó Hugging Face TL;DR # Current retrieval methods struggle to leverage in-context examples effectively, limiting performance. Naively adding examples often hurts performance. This is a significant limitation because in-context learning dramatically improves performance in other large language models.\nThe paper introduces RARE, a method to use in-context examples successfully. RARE fine-tunes a pre-trained model with semantically similar queries and their associated documents as examples. This approach consistently improves results on various benchmarks, even demonstrating strong out-of-domain generalization. The work highlights the importance of careful design choices in how in-context examples are used and lays a foundation for further research in this space.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # This paper is crucial because it bridges the gap between in-context learning (successful in LLMs) and retriever models. By showing how to effectively use in-context examples in retrieval, it advances a widely-used technique and opens new avenues for improving retrieval systems\u0026rsquo; performance and generalization.\nVisual Insights # Algorithm 1: RARe - TrainingInput: Training set D, embedder E( ¬∑), BM25, the number of in-context examples k, mini-batch size B.1: for each training iteration do2:Sample mini-batch B of size B from D3:for (ti, q, d+ , d ) E B do4:In-Context Example Retrieval:5:{qi c ic qik } ‚Üê Retrieve nearest neighbor queries of q from D using BM25 , 92 , ¬∑ ¬∑ ¬∑ ,6:2 c+ {d+ : (q', d+ ) E D,q E {qic , ¬∑ ¬∑ ¬∑ , qik }} {d1 c+ dic+ , ¬∑ ¬∑ ¬∑ , d k } ‚Üê ,7:Dic ‚Üê {(gic dic+ ) , ¬∑ . ¬∑ , (qk, dic+)} ,8:Query Augmentation:9:inst+ic {qic}; Document: {dic+} ...; Query: {q} q = Instruct: {ti}; Query:10:Training with Contrastive Loss:11:Compute the mini-batch contrastive loss LRARe as described in Equation 512:Update E(„Éª) by minimizing LRARe.Output: Trained embedder E(¬∑) üîº Table 1 presents the performance of different models trained from decoder-only checkpoints (LLMs) and their variants on the BeIR and RAR-b benchmarks.\nread the caption Table 1: Training from decoder-only (LLM) checkpoint. Performance is measured by nDCG@10. RARe shows up to +2.72% absolute gain on average over Promptriever, demonstrating that starting from an existing embedding model is not a requirement. We provide a breakdown of In-Domain (ID) and Out-of-Domain (OOD) performance. More visual insights # More on tables MethodBase modelTraining DataIDOODAverageMS-MARCOBeIRRAR-bRepLLaMALlama-2MS-MARCO42.0053.6920.2338.64RepLLaMALlama-3MS-MARCO43.5653.9918.5038.68RAReLlama-3MS-MARCO44.7755.8722.3440.99RepLLaMALlama-3.1-InstructMS-MARCO43.6754.3419.2039.07PromtprieverLlama-3.1-InstructMS-MARCO + Synthetic42.7056.1020.9539.94RAReLlama-3.1-InstructMS-MARCO42.9356.0523.6740.88 üîº This table presents the performance of different models trained from decoder-only checkpoints on three retrieval benchmarks (BeIR, RAR-b, and MS-MARCO), showing the effectiveness of the proposed RARe method.\nread the caption Table 1: Training from decoder-only (LLM) checkpoint. Performance is measured by nDCG@10. RARe shows up to +2.72% absolute gain on average over Promptriever, demonstrating that starting from an existing embedding model is not a requirement. We provide a breakdown of In-Domain (ID) and Out-of-Domain (OOD) performance. MethodLLM2Vec-Llama-3-8b-SupervisedE5-Mistral-InstructBeIRRAR-bBeIRRAR-bIDOODAllIDOODAllBase71.3149.2856.6321.5571.9549.3356.8722.17Instruct70.4647.7955.3523.4472.9148.9856.9624.12RARe71.6749.3056.7623.1072.9850.9358.2825.79 üîº Table 2 shows the performance of different retriever models on BeIR and RAR-b benchmarks after fine-tuning with and without in-context examples.\nread the caption Table 2: Training from retriever checkpoint. Performance (nDCG@10) on BeIR (Thakur et al., 2021) and RAR-b (Xiao et al., 2024) benchmarks when fine-tuning retriever model on E5 dataset. We report a breakdown of performance on In-Domain (ID) and Out-of-Domain (OOD) tasks on BeIR. We consider all RAR-b tasks as OOD. kArguanaCQADupStackFiQA2018NFCorpusSciFactTouche2020AverageInstruct (0)61.1944.8257.3940.9977.2829.3551.84160.4746.7656.0740.6781.4729.7852.54362.9847.1257.0840.7783.7127.1253.13560.8748.4657.3142.2884.7928.7053.741058.8548.9257.0342.2487.6128.2953.82 üîº Table 3 shows the impact of varying the number of in-context examples (k) during both training and evaluation phases on the E5-Mistral-Instruct model\u0026rsquo;s performance across six different datasets.\nread the caption Table 3: Impact of the number of in-context examples (k) during training and evaluation. All results are on E5-Mistral-Instruct. In general, performance increases when increasing the number of examples, and the optimal number of examples depends on the task. Fine-TuningIC Eval SettingArguAnaCQAFiQA2018NFCorpusSciFactTouche2020AverageInstruct-61.1944.8257.3940.9977.2829.3551.83RAReQueries-Only58.8846.6654.4441.4278.8428.0951.39Doc-Only57.5448.2856.0241.6279.8029.0152.05Shuffle-NC60.1745.7854.2541.1780.7029.1851.88Shuffle-C58.9747.9755.9841.7880.5128.9752.36Regular60.8748.4657.3142.2884.7928.7053.74 üîº Table 4 compares the performance of different in-context example formats on the E5-Mistral-Instruct model for various downstream tasks.\nread the caption Table 4: In-Context Format Comparing variants of in-context example format on E5-Mistral-Instruct. Instruct refers to the baseline which does not use any in-context examples. Training / Eval SettingArguAnaCQAFiQA2018NFCorpusSciFactTouche2020Averageinst+ic RARe-q60.8748.4657.3142.2884.7928.7053.74inst+ic+neg RARe-q61.1948.0956.8941.5882.3730.5153.44 üîº Table 5 shows the impact of adding negative document pairs to the in-context examples on the performance of the E5-Mistral-Instruct model, indicating no performance gains.\nread the caption Table 5: Impact of adding negative documents in the in-context prompt. All results are on E5-Mistral-Instruct. Negative documents (d¬Ø) in the prompt do not enhance performance. Dataset# CorpusEval SettingAvg Q len.NNQuerySearchTotalInc.NFCorpus3633inst q3.303.300.543.84-inst+ic q866.00.20152.990.57153.7640.04xFiQA201857638inst q10.906.927.9214.84-inst+ic q1016.60.45278.628.83287.9019.40xTRECCOVID171332inst q inst+ic10.601.834.085.91„ÄÅq722.540.3121.784.3026.394.47xTouche2020382545inst q6.601.429.2910.71-inst+ic q1287.80.2040.3210.5051.024.76xQuora522931inst q9.50113.93986.421100.35-inst+ic q129.53.19530.33982.611516.131.38xDBPedia4635922inst q5.5036.93588.38625.31-inst+ic q158.20.1946.21709.27755.671.21x üîº Table 6 presents the latency breakdown of each stage in the retrieval pipeline for both baseline and in-context settings, showing the impact of in-context examples on processing time across datasets of varying sizes and query lengths.\nread the caption Table 6: Latency breakdown (in seconds) of each stage in the retrieval pipeline for qinst and qinst+ic evaluation settings. # Corpus denote the number of documents and Avg Q len. denote the average number of query tokens split by whitespace. Table 11 in the Appendix provides numbers on additional datasets. CategoryDatasetLLM2Vec-Llama-3-8b-SupervisedE5-Mistral-InstructBaseInstructRAReBaseInstructRAReinst qinst qinst qinst+ic qinst qinst qinst qinst+ic qIDFEVER90.2088.1288.4386.6287.8491.5090.1890.48HotpotQA71.7672.5073.8379.0975.7273.9172.1875.95NQ64.2163.6365.0066.1363.5367.4468.1567.66QuoraRetrieval87.1687.8587.8887.6389.6189.8289.5988.95MSMARCO43.2440.1940.7738.8843.0641.8941.8841.88OODArguAna62.7860.5159.5457.0561.6561.1962.9060.87ClimateFEVER34.2734.4934.6734.7338.3539.0338.9937.50CQADupStack48.2549.7649.1049.9342.9744.8245.5748.46DBPedia48.3448.6148.4149.0948.8948.9249.2449.65FiQA201855.3352.9954.2652.8256.8157.3956.3357.31NFCorpus41.8341.9241.6141.8438.5840.9941.1942.28SCIDOCS22.9623.9722.9223.3516.3217.9418.7120.19SciFact78.2276.8977.7081.7776.4277.2877.1184.79Touche202020.5022.1122.7119.5426.2729.3527.5628.7TRECCOVID80.3468.3778.5582.7887.0372.8977.0379.58Average56.6355.3556.3656.7656.8756.9657.1158.28 üîº Table 7 presents the performance of different retrieval methods (base, instruct, and RARE) on various datasets of the BeIR benchmark, categorized by in-domain and out-of-domain, showing the impact of RARE on both types of data.\nread the caption Table 7: Performance (nDCG@10) on BeIR (Thakur et al., 2021) when fine-tuning retriever model on E5 dataset. We report a breakdown of performance on In-Domain (ID) and Out-of-Domain (OOD) tasks on BeIR. DatasetLLM2Vec-Llama-3-8b-SupervisedE5-Mistral-InstructBase inst qInstruct inst qRAReBase inst qInstruct inst qRAReinst qinst+ic qinst qinst+ic qARC-C18.8118.7718.2817.0219.0020.3722.7226.44ÔøΩ-NLI26.5927.2925.2523.6626.0425.7024.1923.23HellaSwag34.3234.1934.1933.2935.3835.9935.0736.29PIQA33.5737.0738.1239.7239.8039.3537.2241.35Quail6.836.065.574.258.4010.9415.3414.69SiQA6.995.344.394.555.665.455.756.15TempReason-L15.245.895.557.873.604.714.554.67WinoGrande40.0252.8848.4754.4439.4850.4144.2653.50Average21.5523.4422.4823.1022.1724.1223.6425.79 üîº Table 8 presents the performance of different retrieval models on the RAR-b benchmark, comparing the base model, the model with only instructions, and the model augmented with in-context examples.\nread the caption Table 8: Performance on reasoning-focused IR benchmark RAR-b (Xiao et al., 2024) when fine-tuning existing retriever models. DatasetLlama2Llama3Llama-3.1-InstructRepLLaMA inst qRepLLaMA inst qRARe inst+ic qRepLLaMA inst qPromptreiver inst qRAReinst qinst+ic qArguAna48.6052.8349.4851.3858.9054.7752.83ClimateFEVER29.3032.5232.1233.1329.8035.9134.24CQADupStack37.9142.5942.9641.5842.1842.5543.31DBPedia44.8045.6245.7944.7346.0045.8745.95FEVER82.9081.7983.6679.2285.5080.0581.84FiQA201845.0044.3147.1344.5047.2044.3646.20HotpotQA68.8072.2472.7270.9071.7070.5574.01MSMARCO42.0043.5644.7743.6742.7041.6542.93NFCorpus36.0037.7339.3438.7738.5038.1639.74NQ63.0062.7065.9661.0963.8060.9265.20Quora86.0088.3487.6586.8487.3087.9587.65SCIDOCS16.1019.6619.4519.2620.8020.0219.52SciFact75.3075.0277.2075.3877.5074.5976.54TRECCOVID83.9083.1585.7683.1584.5077.5285.30Touche202034.1027.8432.8930.7731.7025.4732.38Average52.9153.9955.1353.6255.2153.3655.18 üîº Table 9 presents the nDCG@10 scores on the BeIR benchmark for different methods when training is performed only on decoder-only models.\nread the caption Table 9: Performance (nDCG@10) on BeIR when training decoder-only models. DatasetLlama2Llama3Llama-3.1-InstructPromptreiverRAReRepLLaMA inst qRepLLaMA inst qRARe inst+ic qRepLLaMA inst qinst qinst qinst+ic qARC-C11.7911.6513.4811.6814.6313.2415.02ÔøΩ-NLI25.4024.3530.3824.9624.7027.3431.58HellaSwag30.8331.4730.2731.0332.5731.4228.81PIQA31.5632.8434.1233.4234.8034.2335.59Quail6.406.215.985.717.806.926.91SiQA2.822.613.872.753.532.183.14TempReason-L11.491.753.612.054.324.846.59WinoGrande51.5837.1157.0142.0145.2544.7261.69Average20.2318.5022.3419.2020.9520.6123.67 üîº Table 10 presents the performance of different models on reasoning-focused retrieval tasks from the RAR-b benchmark when training from decoder-only model checkpoints.\nread the caption Table 10: Performance (nDCG@10) on datasets from RAR-b when training decoder-only models. Dataset# CorpusEval SettingAvg Q len.NNQuerySearchTotalInc.SciFact5183inst q12.504.520.615.13-inst+ic q1250.70.25212.350.61213.2141.56xSCIDOCS25657inst q9.4011.295.7417.03-inst+ic q901.10.67354.825.79361.2821.21xCQADupStack38100inst q8.6010.8010.2021.01-inst+ic q678.21.33570.0410.30581.6727.69xClimateFEVER5416593inst q20.2026.061725.901751.96-inst+ic q831.33.55651.761723.842379.151.36x üîº Table 6 presents a breakdown of the latency of each stage of the retrieval pipeline for both baseline and in-context settings, showing the time required for nearest-neighbor in-context examples, query embeddings, and search.\nread the caption Table 6: Latency breakdown (in seconds) of each stage in the retrieval pipeline for qinst and qinst+ic evaluation settings. # Corpus denote the number of documents and Avg Q len. denote the average number of query tokens split by whitespace. Table 11 in the Appendix provides numbers on additional datasets. # ExamplesDatasetInstruct (0)013510ArguAna61.1962.9061.2460.9961.1860.37ClimateFEVER39.0338.9938.2737.9737.5037.67CQADupStack44.8245.5747.4948.3348.4648.48DBPedia48.9249.2449.7948.3449.6549.82FiQA201857.3956.3357.6157.4257.3157.38NFCorpus40.9941.1941.4842.1042.2842.29SCIDOCS17.9418.7119.8320.1720.1920.20SciFact77.2877.1183.5684.4584.7985.12Touche202029.3527.5627.5327.7028.7030.77TRECCOVID72.8977.0376.9678.9979.5878.77Average48.9849.4650.3850.6550.9651.09 üîº Table 3 shows the impact of varying the number of in-context examples used during both training and evaluation on the performance of the E5-Mistral-Instruct model across different datasets.\nread the caption Table 3: Impact of the number of in-context examples (k) during training and evaluation. All results are on E5-Mistral-Instruct. In general, performance increases when increasing the number of examples, and the optimal number of examples depends on the task. # ExamplesDatasetInstruct (0)013510Arguana61.1962.9060.4762.9860.8758.85ClimateFEVER39.0338.9937.9436.4537.5036.54CQADupStack44.8245.5746.7647.1248.4648.92DBPedia48.9249.2447.7049.0549.6547.95FiQA201857.3956.3356.0757.0857.3157.03NFCorpus40.9941.1940.6740.7742.2842.24SCIDOCS17.9418.7120.0119.2820.1921.54SciFact77.2877.1181.4783.7184.7987.61Touche202029.3527.5629.7827.1228.7028.29TRECCOVID72.8977.0378.9573.2579.5886.11Average48.9849.4650.1848.8351.1153.16 üîº Table 13 shows the impact of varying the number of in-context examples during training and inference on the performance of the E5-Mistral-Instruct model across multiple datasets.\nread the caption Table 13: Impact of the number of in-context examples (k) during training and inference. All results are on E5-Mistral-Instruct. In general, performance increases when increasing the number of examples, and the optimal number of in-context examples can vary by task. InstructRAReDataset-Query-OnlyDoc-onlyShuffle-NCShuffle-CRegularArguAna61.1957.3660.3555.6460.4960.87ClimateFEVER39.0338.3538.3237.4437.8437.50CQADupStack44.8239.5648.4347.7048.2748.46DBPedia48.9249.1449.6949.7250.0449.65FiQA201857.3955.6756.8556.6457.4157.31NFCorpus40.9941.0042.0942.0241.9242.28SCIDOCS17.9419.0620.0619.9820.2520.19SciFact77.2877.4681.8881.5182.2084.79Touche202029.3527.0429.0228.6029.3128.70TRECCOVID72.8975.1179.9779.0780.0379.58Average48.9847.9850.6749.8350.7850.93 üîº This table compares the performance of different in-context example formats during inference only on the E5-Mistral-Instruct model, showing the impact of various query augmentation strategies on retrieval tasks.\nread the caption Table 14: In-Context Format Comparing variants of in-context example format on E5-Mistral-Instruct during inference only. Training is done with the Regular format. Instruct refers to the baseline which does not use any in-context examples. InstructRAReDataset-Query-OnlyDoc-OnlyShuffle-NCShuffle-CRegularArguAna61.1958.8857.5460.1758.9760.87ClimateFEVER39.0336.2135.5930.8335.7137.50CQADupStack44.8246.6648.2845.7847.9748.46DBPedia48.9249.9849.0850.9350.2449.65FiQA201857.3954.4456.0254.2555.9857.31NFCorpus40.9941.4241.6241.1741.7842.28SCIDOCS17.9420.0420.1220.3520.1120.19SciFact77.2878.8479.8080.7080.5184.79Touche202029.3528.0929.0129.1828.9728.70TRECCOVID72.8979.5483.2982.1482.9779.58Average48.9849.4150.0449.5550.3250.93 üîº Table 14 shows the impact of different in-context example formats on retrieval performance when only modifying the query at inference time, holding the training format constant.\nread the caption Table 14: In-Context Format Comparing variants of in-context example format on E5-Mistral-Instruct during inference only. Training is done with the Regular format. Instruct refers to the baseline which does not use any in-context examples. TrainingEvalNQQuoraNFCorpusSciFactSCIDOCSFiQA2018CQAAverageRepLLaMA-q instinst q62.7088.3437.7375.0219.6644.3142.5952.91inst+ic RARe-qinst q39.6488.3935.4274.5221.0430.4437.7446.74inst+ic q65.1986.7938.8778.4119.7046.5843.7554.18inst inst+ic RARe-q + qinst q inst+ic63.6887.8438.0676.0720.1146.0242.9953.54q65.9687.6539.3477.2019.4547.1342.9654.24 üîº The table shows the performance of different training methods (using various combinations of instruction-only queries and in-context example queries) on the BeIR benchmark when starting from decoder-only LLMs.\nread the caption Table 16: Performance (nDCG@10) on datasets from the BeIR benchmark Thakur et al., 2021 when training decoder-only model (Llama3). Applying RARE with only in-context examples can lead to degradation of performance in the zero-shot setting (qinst), but this is easily mitigated my including a mixture of qinst and qinst+ic data (30% and 70%) respectively. Full paper # ","date":"26 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.20088/","section":"Paper Reviews by AI","summary":"RARE enhances retrieval model accuracy by effectively integrating in-context examples, achieving up to +2.72% nDCG improvement.","title":"RARe: Retrieval Augmented Retrieval with In-Context Examples","type":"paper-reviews"},{"content":"","date":"25 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-google-deepmind/","section":"Tags","summary":"","title":"üè¢ Google DeepMind","type":"tags"},{"content":"","date":"25 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-tencent-ai-lab/","section":"Tags","summary":"","title":"üè¢ Tencent AI Lab","type":"tags"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.19482 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rJamie Hayes et el. 2024-10-30 ‚Üó arXiv ‚Üó Hugging Face TL;DR # Large language models (LLMs) can memorize training data, posing significant risks, especially when sensitive information is involved. Current memorization measurement, primarily through discoverable extraction, relies on single-sequence sampling and often underestimates memorization rates. This limits our ability to assess and mitigate these risks effectively.\nThis research introduces probabilistic discoverable extraction to solve these problems. This new approach quantifies the probability of extracting a target sequence from LLMs using various sampling schemes and multiple attempts. It is shown that the probabilistic measure reveals higher memorization rates than previous methods, offering a more comprehensive assessment. The impact of different sampling schemes on extractability is studied, leading to a more realistic evaluation of memorization risks.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # This paper is crucial for researchers working with large language models (LLMs) due to its novel approach to measuring memorization. It addresses limitations of existing methods, offering a more realistic and comprehensive assessment of LLM memorization risks, which is vital for responsible AI development and deployment. The introduction of probabilistic memorization opens new avenues for research into mitigating this risk and for comparing different models\u0026rsquo; memorization characteristics more accurately. The findings directly impact current research on LLM safety and security.\nVisual Insights # üîº The chart compares the rank of target tokens in the predicted probability distribution for greedy and top-k sampling, showing how greedy sampling can miss clear memorization instances that top-k sampling would detect.\nread the caption Figure 1 | An example of how greedy sampling can mask clear signs of memorization. We plot the rank of the target suffix tokens over each successive token that is decoded by greedy and top-k sampling. At every index except one, the tokens in the target sequence have rank 1. At index 15, the target token has rank 2, which means greedy sampling does not select this token, and after which the generated sequence diverges from the target. If, at index 15, the second most likely token had been selected instead, then the remaining positions all have the target token as the top ranked token, so a probabilistic sampling scheme like top-k sampling has a high likelihood of generating the target suffix. See Figure 11a in Appendix E for the specific generated and target text for this example. More visual insights # More on charts üîº (n,p)-discoverable extraction for different Pythia model sizes using top-k=40 sampling is shown, demonstrating how the extraction rate varies with the number of trials (n), probability (p), and model size.\nread the caption Figure 12 | (n, p)-discoverable extraction for different (Pythia) model sizes using top-k = 40 sampling. üîº The chart shows that the empirically measured probability of a target suffix appearing in a set of n generated sequences closely matches the theoretically calculated probability.\nread the caption Figure 9 | We check that generating a set of n sequences and checking the probability a training example appears at least once in the set (empirical p), matches the theoretical p according to Equation (2). Full paper # ","date":"25 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.19482/","section":"Paper Reviews by AI","summary":"\u003cstrong\u003eResearchers introduce probabilistic discoverable extraction\u003c/strong\u003e, a novel approach improving LLM memorization measurement by considering probabilistic sampling and multiple extraction attempts, reveali\u0026hellip;","title":"Measuring memorization through probabilistic discoverable extraction","type":"paper-reviews"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.19609 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rHongliang He et el. 2024-10-30 ‚Üó arXiv ‚Üó Hugging Face TL;DR # Current research on autonomous web agents faces challenges. Existing agents often rely on closed-source models, limiting further improvements. Those trained in synthetic environments struggle to generalize to complex real-world scenarios lacking clear reward signals, and text-only agents ignore valuable visual cues.\nOpenWebVoyager overcomes these issues. This innovative open-source framework builds multimodal web agents through an iterative process. Initially, imitation learning equips agents with basic navigation skills. Subsequently, real-world exploration, coupled with GPT-40 feedback, refines agent policies. This cycle continues, leading to demonstrable performance improvements across various web navigation tasks, showcasing the potential of open-source methods for building robust and adaptable AI agents.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # This paper is important because it introduces an open-source framework for building multimodal web agents, addressing limitations of existing methods. It provides a novel iterative approach combining real-world exploration, feedback, and optimization, advancing research in AI agent development and multimodal learning. The open-source nature facilitates broader community involvement and accelerates progress in the field.\nVisual Insights # üîº The chart displays the performance improvement of the OpenWebVoyager model across three datasets (WebVoyager, Mind2Web cross-task, and Mind2Web cross-website) over four phases (IL and three iterative optimization cycles).\nread the caption Figure 3: Performance growth of OpenWebVoyager on WebVoyager and Mind2Web test set from Imitation Learning phase to 3rd exploration-feedback-optimization cycle. AllrecipesAmazonAppleArXivGitHubBookingESPNCourseraOpenWebVoyagerIL17.8%12.2%20.9%14.0%14.6%9.1%9.1%31.0%OpenWebVoyageriter-135.2%26.8%11.6%18.6%24.4%6.8%2.3%28.6%Open WebVoyageriter-222.2%36.6%27.9%20.9%19.5%6.8%6.8%33.3%Open WebVoyageriter-324.4%24.4%20.9%18.6%31.7%18.2%11.4%42.9%Open WebVoyageriter-3-dgs20.0%31.7%18.6%23.3%24.4%13.6%25.0%42.9%OpenWebVoyageriter-3-dgs-g22.2%29.3%32.6%20.9%26.8%11.4%11.4%42.9%Cambridge DictionaryBBC NewsGoogle FlightsGoogle MapGoogle SearchHuggingfaceWolfram AlphaOverallOpen WebVoyagerIL37.2%9.5%9.5%22.0%44.2%20.9%26.1%19.9%Open WebVoyageriter-125.6%9.5%19.0%26.8%44.2%25.6%32.6%22.6%Open WebVoyageriter-223.3%14.3%19.0%22.0%41.9%11.6%34.8%22.7%OpenWebVoyageriter-337.2%11.9%11.9%26.8%39.5%30.2%37.0%25.8%Open WebVoyageriter-3-dgs30.2%11.9%21.4%22.0%39.5%23.3%34.8%25.5%Open WebVoyageriter-3-dgs-g34.9%14.3%21.4%29.3%44.2%32.6%37.0%27.4% üîº Table 1 presents the task success rates of different agents (trained with various optimization strategies) on the WebVoyager test set, showing the impact of iterative optimization and difficulty-guided sampling.\nread the caption Table 1: Task success rate on WebVoyager test set (643 queries). All websites are seen during training. 'IL', 'iter-1', 'iter-2', and 'iter-3' represent agents after IL, 1st, 2nd, and 3rd optimization, respectively. 'dgs' and 'dgs-g' denote difficulty-guided sampling, i.e., sample more trajectories for webs with low sampling accuracy, the former by adding trajectories sampled by the agent itself and the latter by adding trajectories sampled by GPT-40. More visual insights # More on tables AgentsMind2Web cross-task (unseen task)Mind2Web cross-web (unseen web)EntertainmentShoppingTravelOverallEntertainmentShoppingTravelOverallOpen WebVoyagerIL8.2%5.9%4.3%6.3%3.0%13.3%4.7%6.6%Open WebVoyageriter-112.2%0.0%4.3%7.1%6.1%6.7%9.3%7.5%Open WebVoyageriter-224.5%5.9%6.5%14.3%15.2%10.0%7.0%10.4%Open WebVoyageriter-326.5%23.5%10.9%19.6%6.1%20%7.0%10.4%Open WebVoyageriter-3-dgs18.4%23.5%10.9%16.1%9.1%16.7%25.6%17.9%Open WebVoyageriter-3-dgs-g22.4%29.4%15.2%20.5%3.0%20.0%23.3%16.0% üîº Table 2 presents the task success rate on Mind2Web cross-task and cross-web test sets, showing the performance of different agents on unseen tasks and websites.\nread the caption Table 2: Task success rate on Mind2Web cross-task and cross-web test set. In cross-task set, the queries from the same websites are seen during training. In cross-website set, the websites are not seen during training but still belong to the Entertainment, Shopping, and Travel Domain. Improvement IterationQueryTraj. FromSuccess Traj.TurnsF@1S@1S@2S@3S@4S@5iter-1480ÔøΩÔøΩb15294374.6%10.4%19.6%24.4%27.5%31.7%iter-2480ÔøΩ01205132487.1%16.0%24.0%30.2%36.9%42.7%iter-3480ÔøΩÔøΩ2207133391.5%18.8%27.9%35.2%41.0%43.1% üîº Table 3 presents the details of the query sets and trajectories generated during each exploration-feedback-optimization cycle, including the number of queries, trajectories, successful trajectories, and the task success rate at various stages (F@1, S@1-S@5).\nread the caption Table 3: Details of query set and trajectory set during the exploration-feedback-optimization cycle. The feedback on task success or not is provided by GPT-40. F@1 indicates the finish rate of the first exploration. S@K represents the task success rate within K explorations. Each task will sample the trajectory up to 5 times until it succeeds or fails all 5 times, successful trajectories will be retained to improve our agent. AgentWebVoyagerMind2Web cross-taskMind2Web cross-websiteFinishSuccessFinishSuccessFinishSuccessOpenWebVoyagerIL6.475.268.777.009.289.29OpenWebVoyageriter-16.175.027.585.007.989.63OpenWebVoyageriter-25.895.047.336.317.137.45OpenWebVoyageriter-35.475.077.677.596.166.91 üîº Table 4 presents the average length of trajectories across different optimization cycles on various test sets, distinguishing between finished and successful trajectories.\nread the caption Table 4: The average length of trajectories across different optimization cycles on various test sets. 'Finish' and 'Success' indicates that we calculate the average length for finished or successful trajectories, respectively. AgentWebVoyager (643 tasks)RRSsRS / RRS /SOpen WebVoyagerIL61812813.1%6.3%OpenWebVoyageriter-1751614521.3%11.0%OpenWebVoyageriter-2882214625.0%15.1%Open WebVoyageriter-31424016628.2%24.1% üîº Table 5 shows the frequency of the agent using the restart action during the experiment and the success rate of using this action.\nread the caption Table 5: The frequency of the agent using the restart action: Let R denote the number of trajectories with restart, RS the number of successful trajectories with restart, and S the total number of successful trajectories. Training TrajectoriesResultDIL U Diter-1 U Diter-220.8%DIL U Diter-223.3% üîº Table 6 shows the result of the experiment that compares the performance of using a mixture of data from previous phases in exploration-feedback-optimization cycles versus only using data from the current phase.\nread the caption Table 6: Study on whether to use a mixture of data from previous phases in exploration-feedback-optimization cycle (OpenWebVoyageriter-1 ‚Üí OpenWebVoyageriter-2). Web ActionsFormatNotesClickClick [Label]Perform a single Click operation on an web element.InputType [Label]; [Content]Type something in the text box and press enter.ScrollScroll [WINDOW or Label]; [up or down]In some web pages where only a partial area can be scrolled, agent need to lock an element in that area first, otherwise scrolls are performed on the whole page.Go backGoBackGo back to previous pageRestartRestartRestart from Google Search and solve tasks.WaitWaitSleep 5 secondsAnswerANSWER; [content]Provide final answer. üîº Table 1 presents the task success rate of different agents on the WebVoyager test set, showcasing the performance improvement after iterative optimization and the impact of different sampling strategies.\nread the caption Table 1: Task success rate on WebVoyager test set (643 queries). All websites are seen during training. ‚ÄòIL‚Äô, ‚Äòiter-1‚Äô, ‚Äòiter-2‚Äô, and ‚Äòiter-3‚Äô represent agents after IL, 1st, 2nd, and 3rd optimization, respectively. ‚Äòdgs‚Äô and ‚Äòdgs-g‚Äô denote difficulty-guided sampling, i.e., sample more trajectories for webs with low sampling accuracy, the former by adding trajectories sampled by the agent itself and the latter by adding trajectories sampled by GPT-40. FromDomainSubdomainWebsite NameWeb Voyager-Allrecipes; Amazon; Apple; ArXiv; BBC News; Booking; Cambridge Dictionary; Coursera; ESPN;GitHub; Google Flights; Google Map; Google Search; Huggingface; Wolfram AlphaMind2 WebEntertainmentEvent Game Movie Music Sportseventbrite; nyc; ticketcenter boardgamegeek; store.steampowered imdb; rottentomatoes; tvguide discogs; last.fm; soundcloud; espn; foxsports; sports.yahoo;ShoppingDigital Fashion General Specialityapple uniqlo amazon; ebay; target cvs; ikeaTravelAirlines Car rental General Ground Hotel Restaurant Othersryanair enterprise agoda; booking amtrak; mbta; thetrainline; us.megabus airbnb; koa; marriott resy; yelp flightaware; nps.gov; spothero üîº Table 8 lists the 48 websites used in the imitation learning and exploration-feedback-optimization phases of the OpenWebVoyager model, specifying their domains and subdomains.\nread the caption Table 8: In the Imitation Learning and exploration-feedback-optimization cycles, a total of 48 websites are selected, including 15 from WebVoyager and 37 from Mind2Web (4 duplicates). Test setNum of queriesWeb seen in training?DomainSubdomainWebsites and num of queriesWebVoyager643YesAllrecipes: 45; Amazon: 41; Apple: 43; ArXiv: 43; BBC News: 42; Booking: 44; Cambridge Dictionary: 43; Coursera: 42; ESPN: 44; GitHub: 41; Google Flights: 42; Google Map: 41; Google Search: 43; Huggingface: 43; Wolfram Alpha: 46Mind2Web cross-task112YesEntertainmentEvent Game Movie Music Sportseventbrite: 6; nyc: 3; ticketcenter: 4 boardgamegeek: 1; store.steampowered: 1 imdb: 5; rottentomatoes: 1; tvguide: 3 discogs: 6; last.fm: 5; soundcloud: 4 espn: 4; foxsports: 5; sports.yahoo: 1ShoppingDigital Fashion General Specialityapple: 4 uniqlo: 3 amazon: 2; target: 5 CVS: 1 ; ikea: 2TravelAirlines General Ground Hotel Restaurant Otherryanair: 6 agoda: 3; booking: 2 amtrak: 6; mbta: 4; us.megabus: 1 airbnb: 3; koa: 3; marriott: 5 resy: 2; yelp: 4 flightaware: 4; spothero: 3Mind2Web cross-website106NoEntertainmentEvent Sportsstubhub: 16 nba: 17ShoppingAuto Generalcars: 13 shopping.google: 17TravelRestaurant Othertripadvisor: 23 recreation.gov: 20 üîº Table 9 presents detailed statistics of the test datasets used in evaluating the performance of the proposed OpenWebVoyager model, including the number of queries, whether the websites were seen during training, and domain and subdomain breakdowns.\nread the caption Table 9: Detailed statistics of the test dataset. Websites from WebVoyager and Mind2Web cross-task have been seen during training, while websites from Mind2Web cross-websites have not been encountered. Full paper # ","date":"25 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.19609/","section":"Paper Reviews by AI","summary":"\u003cstrong\u003eOpenWebVoyager: A novel open-source framework enables building multimodal web agents that iteratively learn from real-world exploration and feedback, achieving strong performance.\u003c/strong\u003e","title":"OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization","type":"paper-reviews"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-airi/","section":"Tags","summary":"","title":"üè¢ AIRI","type":"tags"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.18057 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rAlexey Dontsov et el. 2024-10-30 ‚Üó arXiv ‚Üó Hugging Face TL;DR # The increasing use of large multimodal language models (MLLMs) raises significant privacy and security concerns. Removing specific information from MLLMs, a process called machine unlearning (MU), is crucial but challenging. Existing MU research focuses on single modalities (text or images), lacking comprehensive multimodal unlearning (MMU) benchmarks. This gap limits our understanding of MMU\u0026rsquo;s intricacies and progress towards robust methods.\nTo address this, the authors introduce CLEAR, a novel benchmark for MMU. CLEAR contains 200 fictitious individuals linked to visual and textual data, allowing for thorough cross-modal evaluation. They test 10 MU methods adapted for MMU and find that a simple technique‚ÄîL1 regularization on LoRA weights‚Äîsignificantly improves performance by reducing catastrophic forgetting. CLEAR offers a valuable dataset and benchmark for researchers focused on developing improved MMU methods. Its findings highlight the unique challenges of MMU and provide a potential solution to reduce catastrophic forgetting.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # This paper is crucial because it addresses the critical need for robust multimodal unlearning benchmarks. The lack of such benchmarks has hindered progress in this area. CLEAR provides a valuable resource that will accelerate research on methods for protecting privacy and mitigating risks associated with large language models. Its findings on the efficacy of L1 regularization open new avenues for investigation in improving unlearning techniques.\nVisual Insights # MMethodReal Metric ‚ÜëRetain Metric ‚ÜëForget Metric ‚ÜìLog Forget Quality ‚ÜëLLama2-7BRetain FT0.500.260.42-4.92LLMU0.380.030.01-2.31KL0.240.000.00-18.22GA0.250.000.00-17.22GD0.610.130.01-48.59IDK0.460.260.24-4.92DPO0.500.260.42-4.92SCRUB0.500.260.42-4.92RMU0.510.260.59-42.86NPO0.500.280.62-44.46Mistral-7BRetain FT0.670.340.47-3.87LLMU0.650.300.39-6.69KL0.280.000.00-50.30GA0.260.000.00-36.06GD0.600.010.00-51.16IDK0.630.320.45-2.72DPO0.670.330.47-3.63SCRUB0.660.330.47-3.39RMU0.090.000.00-123.22NPO0.670.330.47-3.16 üîº The table presents the results of unlearning methods applied to the textual domain, evaluating their performance on retention and forgetting metrics.\nread the caption Table 1: Unlearning methods on textual domain only. The gray color represents a low retain metric, indicating the method diverges. Hence, we do not consider them. More visual insights # More on tables MethodForget Acc. ‚ÜìHoldout Acc. ‚ÜëRetain Acc. ‚ÜëU-LIRA ‚ÜìU-MIA ‚ÜìOriginal100.0018.50100.001.000.96Gold15.4315.0497.520.500.50Retain FT100.0018.54100.001.000.92SCRUB99.7416.7799.930.980.90LLMU85.7214.6288.990.830.75RMU67.9717.2799.990.770.60DPO50.2113.9381.490.730.62SCRUBbio42.5914.2599.440.710.57Sparsity66.4114.4483.570.780.73Twins50.0020.3499.720.730.54 üîº The table shows the results of various unlearning methods on a visual modality, evaluating their performance on forget, holdout, and retain sets and highlighting methods that suffer from catastrophic forgetting.\nread the caption Table 2: Results of unlearning on visual modality only. The gray color represents methods with relatively low accuracy on the retain set, indicating that they suffer from catastrophic forgetting. Therefore, we do not consider these methods to be successful. LossModalityReal ‚ÜëForget ‚ÜìRetain ‚ÜëLog Forget Quality‚ÜëOriginal0.480.30.51-61.22Gold0.500.190.510.00LLMUtext0.470.370.49-71.23LLMUvisual0.500.350.51-60.26LLMUboth0.470.250.51-95.12SCRUBtext0.490.350.51-61.22SCRUBvisual0.480.370.49-60.26SCRUBboth0.490.360.52-60.26DPOtext0.460.380.49-62.18DPOvisual0.490.220.49-90.26DPOboth0.460.220.48-91.46 üîº Table 3 presents the results of unlearning experiments conducted on textual, visual, and multimodal domains, comparing the performance of various methods against a gold standard model.\nread the caption Table 3: Results of unlearning of different modalities. We finetune on full datasets (both modalities), then forget on a single domain subset (text or visual) or full forget set. Original ‚Äì model before unlearning. Gold - a model trained only on retain. MethodLoRA L1 RegularizationReal metric‚ÜëRetain metric ‚ÜëForget metric ‚ÜìLog Forget Quality ‚ÜëOriginal0.480.510.39-61.22Gold0.500.510.190.00GA0.320.000.00-13.04GA0.490.500.37-61.22GD0.240.000.00-17.72GD0.490.500.37-62.18IDK0.480.510.30-74.40IDK0.490.500.37-63.15KL0.270.000.00-13.92KL0.490.500.37-62.18NPO0.490.510.36-63.15NPOV0.490.510.36-64.13Retain FT0.490.510.36-60.26Retain FTV0.490.500.37-61.22RMU0.270.000.00-23.68RMU0.490.500.36-61.22LLMU0.470.490.37-73.34LLMUV0.490.510.36-60.26DPO0.460.490.39-61.22DPOV0.480.500.37-65.12SCRUB0.490.510.36-62.18SCRUBV0.500.510.35-61.22 üîº Table 4 presents the results of experiments evaluating the performance of various unlearning methods with and without L1 regularization on LoRA weights, comparing metrics across different data subsets.\nread the caption Table 4: Results on experiments with and without LORA regularization. The gray color shows that the method completely fails on the retain set. ImageCaptionChukwu Akabueze in a striped shirt with a fleur-de-lis pin, looking directly at the camera in a vintage setting with a calendar in the background.Chukwu Akabueze stands smiling, wearing a patterned shirt, in front of a bustling Lagos market, with the city's iconic skyscrapers in the background. DUNGLChukwu Akabueze sits in a chair with a sign for \"Momila\" on the desk in front of him, while his parents, dressed in professional attire, are reflected in the mirror behind him.Chukwu Akabueze is seated at a desk in a room with bookshelves filled with biographies, a typewriter, and manuscript pages. He's smiling and looking directly at the camera.GBUERA RNChukwu Akabueze, Nigerian writer, poses with an award trophy, smiling broadly after winning the Nigerian Writers Award.Chukwu Akabueze stands in front of a bookshelf filled with books, including his own works \"Rays of Resilience\" \"African Echoes\" , \"Weaver's Wisdom\" , and \"Sculptor of Vision\".Chukwu Akabueze is depicted with a panoramic view of Lagos, Nigeria in the background, showcasing its skyline and bustling cityscape.Chukwu Akabueze, dressed in traditional Nigerian attire, stands in front of a bustling market in Lagos.Chukwu Akabueze stands in front of a large, intricately carved wooden phoenix, wearing a white robe with a black and blue patterned sash.Chukwu Akabueze, author of \"Sculptor of Vision \" biography , a about a lawyer, is pictured in a library setting with law books and scales of justice. üîº Table 1 presents the results of applying different unlearning methods to a textual-only domain, showing the performance metrics for each method and highlighting methods that experienced catastrophic forgetting.\nread the caption Table 1: Unlearning methods on textual domain only. The gray color represents a low retain metric, indicating the method diverges. Hence, we do not consider them. Full paper # ","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.18057/","section":"Paper Reviews by AI","summary":"CLEAR benchmark enables effective evaluation of multimodal unlearning methods by offering a new dataset with textual and visual data, highlighting challenges, and demonstrating mitigation of catastrop\u0026hellip;","title":"CLEAR: Character Unlearning in Textual and Visual Modalities","type":"paper-reviews"},{"content":"","externalUrl":null,"permalink":"/ai-paper-reviewer/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/ai-paper-reviewer/series/","section":"Series","summary":"","title":"Series","type":"series"}]