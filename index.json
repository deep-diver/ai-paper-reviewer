[{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/2024-10-23/","section":"Tags","summary":"","title":"2024-10-23","type":"tags"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/ai-generated/","section":"Categories","summary":"","title":"AI Generated","type":"categories"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/","section":"AI Paper Reviews by AI","summary":"","title":"AI Paper Reviews by AI","type":"page"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" TL;DR # WorldSimBench is a new benchmark for evaluating video generation models as \u0026lsquo;World Simulators,\u0026rsquo; focusing on their ability to generate actionable videos. It uses a dual evaluation approach (explicit perceptual and implicit manipulative evaluations) and a new dataset (HF-Embodied) to assess visual fidelity and video-action consistency. The results highlight limitations of current models, guiding future research in embodied AI. \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # This paper introduces WorldSimBench, a dual evaluation framework for assessing the capabilities of World Simulators (predictive models that generate actionable videos). It categorizes predictive models into a hierarchy based on embodiment level, proposes explicit perceptual and implicit manipulative evaluations, and introduces the HF-Embodied Dataset. The framework offers key insights into the strengths and limitations of current World Simulators, highlighting the need for further advancement in embodied AI.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # ðŸ”¼ Figure 1: Overview of the hierarchical capabilities of the Predictive Models. Models at higher stages demonstrate more advanced capabilities. We take the initial step in evaluating Predictive Generative Models up to the S3 stage, known as World Simulators, by introducing a parallel evaluation framework, WorldSimBench. WorldSimBench assesses the models both Explicit Perceptual Evaluation and Implicit Manipulative Evaluation, focusing on video generation and action transformation across three critical embodied scenarios. ðŸ”½ Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes. Benchmark Input Modality Output Modality Based Method Stage Interactive Env. Evaluation Strategy AgentBench Liu et al. 2023b Text Text LLM So Task-Level Human Judgement EgoPlan-Bench Chen etal. 2023 Text \u0026amp; Images Text MLLM So N/A Multi-choice MMWorld He et al. 2024 Text \u0026amp; Images Text MLLM So N/A GPT Judgement VAB Liu et al. 2024a Text \u0026amp; Images Text MLLM So Task-Level Human Judgement LEGO Lai et al. 2023 Text \u0026amp; Images Image IGM S1 Task-Level Feature Similarity VBench Huang etal. 2024 Text Video VGM S2 N/A Feature Similarity EvalCrafter Liu etal. 2024b Text \u0026amp; Images Video VGM S2 N/A Feature Similarity WorldSimBench Text \u0026amp; Images Actionable Video VGM S3 Action-Level Human Preference Evaluator Embodied Metric More figures ðŸ”¼ Figure 2: Overview of Explicit Perceptual Evaluation. (Top) Instruction Prompt Generation. We use a large collection of video captions from the internet and our predefined embodied evaluation dimensions. These are expanded using GPT and manually verified to create a corresponding Task Instruction Prompt List for data generation and evaluation. (Bottom) HF-Embodied Dataset Generation. Massive internet-sourced embodied videos with captions are used to train data generation models. Fine-grained Human Feedback Annotation is then applied to the embodied videos according to the corresponding Task Instruction Prompt List, covering multiple embodied dimensions. ðŸ”¼ Overview of Implicit Manipulative Evaluation. Embodied tasks in different scenarios are decomposed into executable sub-tasks. The video generation model generates corresponding predicted videos based on the current instructions and real-time observations. Using a pre-trained IDM or a goal-based policy, the agent executes the generated sequence of actions. After a fixed timestep, the predicted video is refreshed by sampling again from the video generation model, and this process repeats. Finally, the success rates of various embodied tasks are obtained through monitors in the simulation environment. ðŸ”¼ Figure 7: Rollout of Open-Ended Embodied Environment in Implicit Manipulative Evaluation. ðŸ”¼ Figure 8: Rollout of Autonomous Driving in Implicit Manipulative Evaluation. ðŸ”¼ Overview of Explicit Perceptual Evaluation. (Top) Instruction Prompt Generation. We use a large collection of video captions from the internet and our predefined embodied evaluation dimensions. These are expanded using GPT and manually verified to create a corresponding Task Instruction Prompt List for data generation and evaluation. (Bottom) HF-Embodied Dataset Generation. Massive internet-sourced embodied videos with captions are used to train data generation models. Fine-grained Human Feedback Annotation is then applied to the embodied videos according to the corresponding Task Instruction Prompt List, covering multiple embodied dimensions. ","date":"23 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/2410.18072/","section":"Posts","summary":"WorldSimBench is a new benchmark for evaluating video generation models as \u0026lsquo;World Simulators,\u0026rsquo; focusing on their ability to generate actionable videos.  It uses a dual evaluation approach (explicit pe\u0026hellip;..","title":"WorldSimBench: Towards Video Generation Models as World Simulators","type":"posts"},{"content":"","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/blog/","section":"Categories","summary":"","title":"Blog","type":"categories"},{"content":"","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/post/","section":"Categories","summary":"","title":"Post","type":"categories"},{"content":" TL;DR # blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # Figure 1. Observatioins about visual redundancy acoross layers. Left: TextVQA performance of LLaVA-1.5 with varying ratio of retained image tokens at different layer. The preserved image tokens are those that receive the highest attention from the text tokens. Right: Visualization of attention map in shallow and deep layers. ModelTrain \u0026amp; InferGPU hours#patchesInfer Flops(T)MMEMMBMMB CNSEEDIMM StarPOPEAvgLLaVA -NeXT-7Bvanilla366520.81534.168.760.571.141.186.167.4PDrop21859.461540.867.860.669.941.786.567.3vanilla483940.61544.767.460.069.540.086.366.7PDrop269918.11542.068.161.070.340.986.667.3LLaVA -1.5-7Bvanilla10413.821510.764.358.366.133.285.963.9PDrop7911.781467.366.158.565.534.086.063.9\nTable 1. LVLM w and w/o our method on 6 benchmarks. Benchmark names are abbreviated due to space limits. MMB: MMBenchmark (Liu et al., 2023); MMBCN : MMBench-Chinese (Liu et al.,2023); SEEDI: SEED-Bench (Image) (Li et al., 2023b). We denote PyramidDrop as PDrop. More visual insights Figure 2. Overview of PyramidDrop. We divide the forward pass of the LLM into multiple stages, and drop part of the image tokens at the end of each stage with a pre-defined ratio. The dropping is based on a lightweight attention calculation with a negligible time overhead, and according to this criterion, the LLM accurately selects important image tokens related to instruction. Due to the efficient redundancy reduction strategy, the average sequence length decreases rapidly. Figure 3. We compare the performance of the original LLaVA-1.5 and LLaVA-1.5 trained using PDrop, where we preserve different ratios of image tokens at layer 2, 8, 16, and 24, respectively. The horizontal axis represents the proportion of retained image tokens according to attention score. Section Summary # Introduction # blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid Ref. blah blah blah blah blah blah blah blah blah More references to follow-up \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah Related Work # blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid Ref. blah blah blah blah blah blah blah blah blah More references to follow-up \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah Method # blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid Ref. blah blah blah blah blah blah blah blah blah More references to follow-up \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah Experiments # blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid Ref. blah blah blah blah blah blah blah blah blah More references to follow-up \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah Paper Image # ","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/pyramiddrop/","section":"Posts","summary":"PyramidDrop selectively reduces redundant visual tokens in deeper layers of large vision-language models, accelerating training and inference while maintaining performance.","title":"PyramidDrop: Accelerating Large Vision-Language Models via Visual Redundancy Reduction","type":"posts"},{"content":"","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/vision-language-models/","section":"Tags","summary":"","title":"Vision-Language Models","type":"tags"},{"content":"","date":"13 June 2022","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","externalUrl":null,"permalink":"/ai-paper-reviewer/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/ai-paper-reviewer/series/","section":"Series","summary":"","title":"Series","type":"series"}]