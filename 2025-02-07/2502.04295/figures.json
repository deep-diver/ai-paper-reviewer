[{"figure_path": "https://arxiv.org/html/2502.04295/extracted/6150185/fig/teaser6.png", "caption": "Figure 1: \nThe crucial role of prompt formatting and its interaction with content. (A): Model-specific format biases: Illustrates the performance sensitivity of two LLMs to different format styles on the GSM8K task, showing substantial variability in the effectiveness of 10 randomly selected formats. (B): For seven different prompt contents evaluated across 24 distinct formats, performance variations show the complex, interdependent relationship between prompt content and structure, demonstrating that no single format universally maximizes effectiveness.", "description": "This figure demonstrates the impact of prompt formatting on Large Language Model (LLM) performance.  Panel (A) shows how two different LLMs perform differently across various formats on the GSM8K task, revealing significant model-specific biases. Panel (B) illustrates the complex interplay between content and format by showing the performance of seven different prompt contents with 24 different formats, demonstrating that no single format is universally superior.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.04295/extracted/6150185/fig/pipeline3.png", "caption": "Figure 2: Illustration of the CFPO pipeline within a single iteration round. In the initial Component-wise Content Optimization stage, case-diagnosis and Monte-Carlo sampling are employed for content mutation. Subsequently, the Format Optimization stage identifies the most suitable format for each content candidate. The yellow dashed line indicates where the LLM optimizer is employed to guide the optimization process.", "description": "This figure illustrates the iterative process of Content-Format Integrated Prompt Optimization (CFPO).  The pipeline starts with Component-wise Content Optimization, which uses case-diagnosis and Monte Carlo sampling to generate variations of the prompt content.  These content variations are then passed to the Format Optimization stage where the best-performing format is selected for each content variation using a scoring system and an LLM-assisted method.  The yellow dashed line highlights the use of an LLM optimizer to guide the optimization process. This figure shows a single iteration of this cycle.", "section": "3 CFPO: Content-Format Integrated Prompt Optimization"}, {"figure_path": "https://arxiv.org/html/2502.04295/extracted/6150185/fig/template2.png", "caption": "Figure 3: An illustrative example of our Structured Prompt Template. This template systematically organizes the prompt into distinct components, each serving a specific functional role. When formulating a prompt, the template first employs a Query format to present examples and queries, and then integrates all content components via the Prompt Renderer to construct the comprehensive prompt string.", "description": "Figure 3 illustrates the structured prompt template used in the Content-Format Integrated Prompt Optimization (CFPO) framework.  The template breaks down a prompt into distinct, functional components: Task Instruction, Task Detail, Output Format, and Examples.  These components are then organized using a Query Format (which dictates how examples and queries are presented) and a Prompt Renderer (which combines all the components into a single, coherent prompt string). This structured approach enables targeted optimization of both content and format, enhancing LLM performance.  The figure provides a visual example showcasing how the template organizes these components and the resulting rendered prompt.", "section": "3 CFPO: Content-Format Integrated Prompt Optimization"}, {"figure_path": "https://arxiv.org/html/2502.04295/extracted/6150185/fig/formats5.png", "caption": "Figure 4: Built-in formats and rendering effects in our initial format pool. The final format configuration is achieved by selecting and combining elements from both the Prompt Renderer and the Query Format categories.", "description": "Figure 4 illustrates the building blocks for creating various prompt formats used in the Content-Format Integrated Prompt Optimization (CFPO) framework.  It shows the initial set of \"Prompt Renderer\" formats (e.g., plain text, markdown, HTML, LaTeX, XML, JSON) that define the overall prompt structure.  It also displays the \"Query Format\" options (e.g., Markdown-Ins-Res, Role-Mapping-Format, COT-Question-Answer, Multi-choice) which dictate the arrangement of in-context examples and queries. The final prompt format is a combination of one Prompt Renderer and one Query Format.  This modular approach allows for systematic exploration of diverse prompt structures.", "section": "3.2 Format Optimizer Design"}, {"figure_path": "https://arxiv.org/html/2502.04295/extracted/6150185/fig/results.png", "caption": "Figure 5: Overview of in-context examples and text lengths for various tasks and models.", "description": "Figure 5 provides a detailed analysis of the relationship between the number of in-context examples and the length of prompts used in different tasks and models.  The chart visually compares four datasets (GSM8K, MATH500, ARC-Challenge, and Big-Bench Classification) across four different LLMs (Mistral-7B-v0.1, LLaMA-3.1-8B, LLaMA-3-8B-Instruct, and Phi-3-Mini-Instruct). It reveals that pre-trained models (like LLaMA-3.1-8B) tend to perform better with longer prompts and more in-context examples, unlike instruction-tuned models (like Phi-3-Mini-Instruct) which show less dependence on these factors.", "section": "4 Experiments"}]