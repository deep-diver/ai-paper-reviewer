[{"figure_path": "https://arxiv.org/html/2502.03032/x1.png", "caption": "Figure 1: Schematic illustration of inner-layer matching. We select a feature with index i\ud835\udc56iitalic_i on the SAE trained at the layer output. Its embedding \ud835\udc1f\ud835\udc1f\\mathbf{f}bold_f, which is the i\ud835\udc56iitalic_ith column of this SAE\u2019s decoder weight, is compared to every column of other SAEs on the same layer (after the MLP and attention blocks, as well as with the SAE on the residual stream before some layer). These comparisons indicate the feature\u2019s source. See Section 3.3 for more details.", "description": "This figure illustrates the method for inner-layer feature matching.  It shows how a feature (represented by its embedding vector) from one layer of a language model is compared to features from other layers and modules (MLP, Attention, Residual Stream) using cosine similarity to find its source. The figure shows that features are identified on the layer outputs, and each feature embedding is compared against all feature embeddings in the MLP and attention blocks of that layer, and also against features from the previous layer's residual stream. The comparisons determine which modules and preceding layers most influenced the identified feature.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.03032/x2.png", "caption": "Figure 2: An illustration of the resulting flow graph, which we also use in the deactivation experiment (section 5.2). As a starting point, we select the feature on the 24th-layer residual with index 14548. For a detailed explanation of this graph, see Appendix E.", "description": "This figure shows a flow graph that visualizes how a specific feature (index 14548) in the 24th residual layer of a large language model evolves across different layers and modules.  The graph illustrates the feature's origin, its propagation through the model's architecture (MLP, attention, residual streams), and how it transforms or interacts with other features.  Nodes represent features identified by sparse autoencoders (SAEs), and edges indicate relationships between features across layers. This visualization is used in the deactivation experiments (Section 5.2) to understand the causal relationships between features and their effect on downstream model behavior. More details are provided in Appendix E.", "section": "3. Scientific Concepts and Entities Graph"}, {"figure_path": "https://arxiv.org/html/2502.03032/x3.png", "caption": "Figure 3: Example of cosine similarity vs. simultaneous activation with a predecessor (350 features were sampled per layer). \u201cFrom MLP\u201d and \u201cFrom RES\u201d groups are notably different: high s(M)superscript\ud835\udc60\ud835\udc40s^{(M)}italic_s start_POSTSUPERSCRIPT ( italic_M ) end_POSTSUPERSCRIPT and low s(R)superscript\ud835\udc60\ud835\udc45s^{(R)}italic_s start_POSTSUPERSCRIPT ( italic_R ) end_POSTSUPERSCRIPT suggest simultaneous activation with an MLP-module match. Cosine similarity serves as a good proxy for shared semantic and mechanistic properties.", "description": "This figure displays the results of an experiment comparing cosine similarity between a target feature and its predecessors (features from the previous layer, MLP, and attention modules) with their simultaneous activation.  The data shows distinct patterns of simultaneous activation for different groups of features.  Specifically, features grouped as \"From MLP\" show high similarity to MLP predecessors and low similarity to residual stream predecessors, indicating that these features are primarily processed by the MLP module. In contrast, features in the \"From RES\" group exhibit the opposite pattern, suggesting that these features originate from and are primarily propagated through the residual stream. This analysis demonstrates that cosine similarity between a feature and its predecessors serves as a strong indicator of their relationship (i.e., shared semantic and mechanistic properties). The experiment involved sampling 350 features per layer, highlighting the large-scale nature of the analysis and its relevance to understanding the complex computational behavior within a large language model.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2502.03032/x4.png", "caption": "Figure 4: Percentage of statistically significant differences between groups for each module\u2019s similarity scores. AO means module P\ud835\udc43Pitalic_P is active in only one group, AB means active in both, and IB means inactive in both. For MLP, two groups differ in s(R)superscript\ud835\udc60\ud835\udc45s^{(R)}italic_s start_POSTSUPERSCRIPT ( italic_R ) end_POSTSUPERSCRIPT only 87% of the time when MLP is active in both groups.", "description": "This figure displays the results of a statistical analysis comparing the similarity scores of different groups of features based on their activation patterns in different modules (MLP, Attention, Residual) of a transformer layer. The analysis determines whether the similarity scores between feature groups are statistically significantly different. The groups are categorized by whether a specific module is active (A) or inactive (I) in both groups (AB, IB) or only one group (AO). The figure visualizes the percentage of statistically significant differences (p<0.001) for each module, revealing the relationships between feature activation and similarity scores.  For instance, it shows that for MLP, there is an 87% probability that two feature groups will have statistically different s(R) scores when MLP is active in both groups.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2502.03032/x5.png", "caption": "Figure 5: Percentages of each group at each layer of Gemma\u00a02\u20092B, illustrating how feature formation proceeds in the model.", "description": "This figure visualizes the distribution of feature groups across different layers of the Gemma 2-2B language model.  Each group represents how features originate: from a previous layer's residual stream, from the MLP (Multi-Layer Perceptron) module, from the attention module, or from a combination of these sources.  The percentages in the chart illustrate the proportion of features in each category at each layer.  This provides insights into the dynamic process of feature emergence and transformation as information flows through the model's layers.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2502.03032/x6.png", "caption": "Figure 6: Deactivation methods compared. Group labels show which active predecessors were deactivated. The random approach underperforms, suggesting that choosing the top1subscripttop1\\operatorname{top}_{1}roman_top start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT feature is already meaningful for causal analysis.", "description": "This figure compares different methods for deactivating features in a language model to analyze causal relationships.  The x-axis represents different groups of features, categorized by which predecessor features were deactivated before evaluating the impact on the target feature. The y-axis shows the percentage of successful deactivations for each group. The 'random' approach serves as a baseline, randomly choosing a predecessor to deactivate. The other methods, such as 'top-1', select the most similar predecessor based on cosine similarity. The results demonstrate that the 'top-1' method significantly outperforms the 'random' method, suggesting that selecting the most similar predecessor is a more effective strategy for revealing causal links between features.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2502.03032/x7.png", "caption": "Figure 7: Mean activation changes when deactivating one predecessor at a time. Deactivation of some predecessor causes less impact if this predecessor is not activated alone, which leads to the conclusion that combined groups exhibit circuit-like behavior.", "description": "This figure displays the average changes in activation strength for target features when one of their predecessor features is deactivated. The results show that deactivating a single predecessor feature has a more significant impact when that predecessor feature is the only active one. When multiple predecessor features are simultaneously activated, deactivating one of them has a smaller impact on the target feature.  This suggests that multiple predecessor features might be part of a larger circuit, such that deactivating just one does not disrupt the overall circuit functionality. The findings indicate that the model's behavior is not solely dependent on single features, but rather groups of features working together in interconnected ways. ", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2502.03032/x8.png", "caption": "Figure 8: Impact of different r\ud835\udc5fritalic_r values on deactivation success, with rescaling of all available predecessors. When r<1\ud835\udc5f1r<1italic_r < 1, the activation change grows nonlinearly, indicating alternative causal pathways still convey information. Relative loss change measured as (Lnew\u2212Lold)/Loldsubscript\ud835\udc3fnewsubscript\ud835\udc3foldsubscript\ud835\udc3fold(L_{\\text{new}}-L_{\\text{old}})/L_{\\text{old}}( italic_L start_POSTSUBSCRIPT new end_POSTSUBSCRIPT - italic_L start_POSTSUBSCRIPT old end_POSTSUBSCRIPT ) / italic_L start_POSTSUBSCRIPT old end_POSTSUBSCRIPT is a proxy for forward pass impact.", "description": "Figure 8 illustrates the impact of the rescaling coefficient (r) on the success rate of deactivating features in a language model.  The x-axis represents different values of r, while the y-axis shows the resulting activation change. When r is less than 1 (meaning the features are being suppressed), the activation change demonstrates nonlinear growth.  This nonlinearity suggests that even when a direct causal predecessor is deactivated, alternative pathways might still allow the feature to remain active, thus conveying information. The graph also displays the relative loss change, which is calculated as (Lnew - Lold) / Lold and serves as a proxy to assess the impact on the model's forward pass during the deactivation process.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2502.03032/x9.png", "caption": "Figure 9: Deactivating the \u201cScientific concepts and entities\u201d theme. The dashed black line shows the default generation score. Red points mark the best layer for each r\ud835\udc5fritalic_r in the single-layer method. Larger r\ud835\udc5fritalic_r boosts performance but shifts the optimal layer earlier.", "description": "Figure 9 illustrates an experiment on deactivating the \"Scientific concepts and entities\" theme in text generation.  The graph displays the impact of different rescaling coefficients (r) applied at various layers of a language model on both the coherence and presence of the target theme in generated text. The dashed black line represents the baseline generation quality without any intervention. Red data points highlight the layer where the deactivation strategy achieves the best results for each value of r.  The results show that increasing the rescaling coefficient (r) improves the effectiveness of theme suppression, although the optimal layer for intervention shifts to earlier layers as r increases.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2502.03032/x10.png", "caption": "Figure 10: Comparison of best deactivation scores. The green line indicates deactivation using only the initial feature set. Multi-layer interventions (orange, blue) perform better across different r\ud835\udc5fritalic_r values, suggesting additional discovered features reduce hyperparameter sensitivity.", "description": "Figure 10 displays the results of an experiment comparing different model steering strategies. The goal was to deactivate a specific theme in text generation. The green line represents a baseline where only the initially identified features related to the theme were deactivated.  The orange and blue lines show the results when multiple layers of features were targeted for deactivation (multi-layer interventions).  The x-axis likely represents different layers of the model and the y-axis likely shows a performance metric (a combination of metrics that capture the balance between successfully removing the unwanted theme and maintaining text quality).  Across different values of the hyperparameter *r* (presumably controlling the strength of the deactivation), the multi-layer strategies consistently outperformed the single-layer strategy.  This suggests that including additional, related features discovered via the flow graph analysis improves the robustness and effectiveness of the deactivation process, reducing sensitivity to the specific value of the hyperparameter.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2502.03032/x11.png", "caption": "Figure 11: Activation of specific topics. We compare single-layer steering and cumulative approaches with three rescaling strategies (Appendix\u00a0B). Activating multiple similar features amplifies a topic\u2019s presence but may degrade overall text coherence.", "description": "This figure displays the results of an experiment on activating specific topics in text generation using a language model.  Three different rescaling strategies (detailed in Appendix B) were applied to both single-layer and cumulative steering approaches. The results show that activating multiple similar features effectively amplifies the presence of the target topic in the generated text. However, this improvement in topic presence sometimes comes at the cost of reduced text coherence, indicating a trade-off between targeted topic emphasis and overall text quality.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2502.03032/x12.png", "caption": "Figure 12: (a) Percentage of feature groups obtained for each dataset. (b) Distribution of scores for layers 8 and 18. We observe a clear distinction between groups, which additionally indicates the validity of the proposed method.", "description": "Figure 12 presents a detailed analysis of feature group distribution and score distribution across different layers of a language model.  Part (a) shows the percentage of features falling into each of the eight identified groups (based on their predecessor activations) for four different datasets: FineWeb, TinyStories, Python Code, and AutoMathText. This visualization helps to understand how the origin and development of features vary depending on dataset characteristics. Part (b) provides a comparative analysis of score distributions for layers 8 and 18 across these feature groups, showing the distinct patterns of score distribution for each group.  The clear separation between these distributions visually validates the proposed approach for identifying feature origins and tracking their evolution across layers. ", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2502.03032/x13.png", "caption": "Figure 13: Probability of group A (row) to appear in group B (column), aggregated over all layers. For example, if we take the \u201cFrom ATT\u201d group, then with a probability of 0.45, features from this group would appear in the \u201cFrom RES & ATT\u201d group. High scores for the \u201cFrom nowhere\u201d group represent its stochasticity.", "description": "This figure shows a matrix representing the probability of features originating from a specific group (e.g., 'From ATT', 'From RES', etc.) at one layer, to subsequently appear in another group at a later layer. Each cell (i,j) in the matrix shows the probability of a feature belonging to group 'i' at some layer appearing in group 'j' at a later layer in the model's processing.  The rows represent the starting group, while the columns represent the resulting group after propagating through multiple layers.  A high value indicates a strong likelihood of transition from the source group to the target group. The high probabilities on the diagonal reflect features that persist through layers. The 'From Nowhere' group represents features that appear without a clear preceding origin.", "section": "3.2 Feature matching"}, {"figure_path": "https://arxiv.org/html/2502.03032/x14.png", "caption": "Figure 14: Percentage of statistically significant differences between groups with respect to a certain score.", "description": "This figure displays the results of Mann-Whitney U tests comparing the distributions of similarity scores (cosine similarity between decoder weights) across different groups of features.  Each group represents a combination of activated predecessor features (from the previous layer, MLP, or attention).  The figure shows the percentage of tests that yielded statistically significant results (p<0.001) for each pair of groups across different layers and datasets. This visualizes which groups of features exhibit distinct similarity score distributions, suggesting differences in their origins and relationship to predecessors. ", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2502.03032/x15.png", "caption": "Figure 15: (a) Percentage of features per each method. There was a total of 13106 activated features, and for every feature, four matching strategies were applied. We see that top5subscripttop5\\operatorname{top}_{5}roman_top start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT method detects many more combined groups than other methods, especially \u201cFrom RES & MLP\u201d. (b) Probability for a feature from some group A\ud835\udc34Aitalic_A (labeled as the subplot title) to become from group B\ud835\udc35Bitalic_B (shown in legend) after deactivation of some predecessor. Each bar shows the percentage of times the feature falls into a new category.", "description": "This figure presents a comparison of four different methods for identifying feature predecessors in a language model.  Subplot (a) shows the percentage of features assigned to each of the eight possible predecessor groups (e.g., 'From RES', 'From MLP & ATT') for each of the four matching methods (random, permutation, top1, top5). The top5 method, which considers the top 5 most similar predecessors, identifies significantly more features belonging to combined groups (e.g., 'From RES & MLP') than the other methods. Subplot (b) further examines the effects of deactivating a predecessor feature. For each of the eight predecessor groups, it shows the probability of a feature originally belonging to that group transitioning to a different group after deactivation. Each bar represents the percentage of times a feature changes group after deactivation.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2502.03032/x16.png", "caption": "Figure 16: From each flow graph, we select features on a particular layer l\ud835\udc59litalic_l and perform steering with the four different strategies. Bars represent the best result for each layer among all scores s\ud835\udc60sitalic_s. In some cases, steering on a layer other than 12 may improve results.", "description": "This figure presents the results of a model steering experiment.  The researchers selected features from specific layers of a language model's flow graph, and then applied four different steering strategies to these features. The experiment aimed to determine the optimal layer for intervention to achieve the best results in terms of the model's ability to generate text aligning with a specific theme. The bar chart displays the best performance attained by each steering strategy across various layers, showing that steering at layers other than layer 12 can sometimes yield better results.", "section": "5.3. Model steering"}, {"figure_path": "https://arxiv.org/html/2502.03032/x17.png", "caption": "Figure 17: (a) Amount of features selected for activation of \u201cResearch methodology and experimentation\u201d theme. Vertical lines represent the placement of the initially selected features. (b) Results for steering of selected features. Score is a total metric measured as Behavioral\u00d7CumulativeBehavioralCumulative\\text{Behavioral}\\times\\text{Cumulative}Behavioral \u00d7 Cumulative. We can see that despite none of the initial features being placed on the 5th layer, it gives us the best result.", "description": "This figure visualizes the results of an experiment on model steering, focusing on the theme of \"Research methodology and experimentation\".  Part (a) shows the number of features selected for activation across different layers of the model. Vertical lines indicate the initial features selected for this theme. Part (b) presents the results of the model steering process itself, showing the total score achieved (Behavioral score multiplied by Cumulative score) for different steering strategies.  The key observation is that while the initial features were not located on layer 5, steering at layer 5 produced the best overall result, suggesting that optimal steering may not always align with the location of initially selected features.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2502.03032/x18.png", "caption": "Figure 18: (a) Distribution of groups for Llama Scope. We observe a clear distinction from Gemma Scope results (Figure 12) due to a much smoother distribution. This may be a consequence of various factors: the architecture of the models or SAEs, the training procedure, differences in data distribution, etc. (b) Distribution of groups across multiple layers. We observe approximately the same pattern as for Gemma Scope (Figure 5), indicating shared properties between the models. (c) Distribution of scores for different groups. We see that the groups are slightly less distinct from each other compared to the case of Gemma Scope (Figure 12), but they are still present. This is also reflected in (d) the separability of different groups based on their cosine similarity relations.", "description": "Figure 18 compares the distribution of feature groups and their cosine similarity scores between LlamaScope and GemmaScope models.  Panel (a) shows that LlamaScope has a much smoother distribution of feature groups compared to GemmaScope (Figure 12), likely due to differences in model architecture, SAE training, or data distribution. Panel (b) demonstrates that LlamaScope exhibits a similar layer-wise pattern of feature group distribution to GemmaScope (Figure 5), suggesting shared underlying properties.  Panels (c) and (d) further illustrate these similarities and differences, showing that while LlamaScope's feature groups are less distinct in terms of cosine similarity compared to GemmaScope (Figure 12), the groups are still distinguishable.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2502.03032/x19.png", "caption": "Figure 19: Flow graph for the 12/res/14455 feature. As reported in Chalnev et\u00a0al. (2024), steering of that feature might produce themes related to fashion, and we clearly observe that our flow graph captures this semantics in the earlier layers.", "description": "This figure visualizes the flow graph for feature 12/res/14455.  The graph tracks the evolution of this feature across different layers of the language model, showing how its semantic meaning changes.  Chalnev et al. (2024) demonstrated that steering this specific feature led to text generation focused on fashion-related themes. This flow graph confirms this, showing a clear presence of fashion-related semantics in the earlier layers of the model. The nodes in the graph represent features identified by Sparse Autoencoders (SAEs), and edges indicate their connections across layers. The colors likely represent different modules within the language model (e.g., attention, MLP, residual streams). The graph illustrates how the feature's meaning is built upon and transformed through these different model components.  Analyzing such a graph allows researchers to better understand the model's inner workings and improve interpretability and steering.", "section": "3.3. Tracking the evolution of feature"}, {"figure_path": "https://arxiv.org/html/2502.03032/x20.png", "caption": "Figure 20: Flow graph for the 12/res/4230 feature. In this case, we observe that the second half of the model is closely related to wedding and marriage ceremonies. We believe that the \u201cofficial\u201d aspect in the interpretation of features in earlier layers is closely related to the fact that wedding ceremonies and marriage are themselves official procedures\u2014the registration of a specific type of interpersonal relationship.", "description": "This figure visualizes the flow of feature 12/res/4230 through the layers of a language model.  The feature is initially related to the official aspects of events and agreements, but in the latter half of the model, its semantic meaning shifts strongly towards wedding and marriage ceremonies. This shift reflects the model's association of weddings and marriages with official registration processes, a specific type of interpersonal relationship.", "section": "3. Scientific Concepts and Entities Graph"}, {"figure_path": "https://arxiv.org/html/2502.03032/x21.png", "caption": "Figure 21: Two SAEs with a learned transition matrix T\ud835\udc47Titalic_T can be seen as a transcoder from layer t\ud835\udc61titalic_t to layer t+1\ud835\udc611t+1italic_t + 1.", "description": "This figure illustrates the concept of using sparse autoencoders (SAEs) to model the transformation of features between consecutive layers in a neural network.  Two SAEs, one for layer 't' and one for layer 't+1', are shown.  A learned transition matrix 'T' connects the two SAEs, representing the transformation of the feature representations from layer 't' to layer 't+1'.  This effectively shows how the SAEs can function as a transcoder, mapping features from one layer to the next, revealing the relationships between features across different layers of the network. This is a key element of their proposed multi-layer interpretability framework.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.03032/x22.png", "caption": "Figure 22: Explained variance of the various permutation variants. Cosine similarity between decoders\u2019 vectors (\ud835\udc08x>0\u2062\u00a0top\u00a01\u2062\ud835\udc7edec(14)\u22a4\u2062\ud835\udc7edec\u00a0(15)subscript\ud835\udc08\ud835\udc650subscript\u00a0top\u00a01superscriptsubscript\ud835\udc7edeclimit-from14topsuperscriptsubscript\ud835\udc7edec\u00a015\\mathbf{I}_{x>0}\\text{ top }_{1}\\boldsymbol{W}_{\\text{dec}}^{(14)\\top}%\n\\boldsymbol{W}_{\\text{dec }}^{(15)}bold_I start_POSTSUBSCRIPT italic_x > 0 end_POSTSUBSCRIPT top start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_W start_POSTSUBSCRIPT dec end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 14 ) \u22a4 end_POSTSUPERSCRIPT bold_italic_W start_POSTSUBSCRIPT dec end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 15 ) end_POSTSUPERSCRIPT) performs best. See Appendix F for more details.", "description": "Figure 22 presents a comparison of different methods for finding a mapping between features across consecutive layers of a language model, using explained variance as a metric.  The methods involve using various permutation variants and cosine similarity between decoder weights. The results show that using cosine similarity between the transpose of the decoder weights from layer 14 and the decoder weights from layer 15, considering only the top-ranked element (I<sub>x>0</sub> top<sub>1</sub> W<sub>dec</sub><sup>(14)\u22a4</sup>W<sub>dec</sub><sup>(15)</sup>), achieves the highest explained variance, indicating its effectiveness in capturing feature correspondences.", "section": "F. Similarity between Matching and Transcoders"}, {"figure_path": "https://arxiv.org/html/2502.03032/x23.png", "caption": "Figure 23: Comparison of various k\ud835\udc58kitalic_k in topksubscripttop\ud835\udc58\\operatorname{top}_{k}roman_top start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT operator and different weights of the SAE. Cosine similarity (\ud835\udc08x>0\u2062\u00a0top\u00a01\u2062\ud835\udc7edec(14)\u22a4\u2062\ud835\udc7edec(15)subscript\ud835\udc08\ud835\udc650subscript\u00a0top\u00a01superscriptsubscript\ud835\udc7edeclimit-from14topsuperscriptsubscript\ud835\udc7edec15\\mathbf{I}_{x>0}\\text{ top }_{1}\\boldsymbol{W}_{\\text{dec}}^{(14)\\top}%\n\\boldsymbol{W}_{\\text{dec}}^{(15)}bold_I start_POSTSUBSCRIPT italic_x > 0 end_POSTSUBSCRIPT top start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_W start_POSTSUBSCRIPT dec end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 14 ) \u22a4 end_POSTSUPERSCRIPT bold_italic_W start_POSTSUBSCRIPT dec end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 15 ) end_POSTSUPERSCRIPT) performs best. See Appendix F for more details.", "description": "Figure 23 analyzes the performance of different methods for finding a transition matrix between the feature spaces of consecutive layers in a language model.  It compares using the top-k elements of the cosine similarity between decoder weight matrices, with and without folding (a technique to account for varying feature activation scales), and also evaluates the impact of including a bias term. The results show that a simple cosine similarity approach, selecting only the top element, achieves the highest explained variance.", "section": "3. Method"}]