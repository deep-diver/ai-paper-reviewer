<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-02-07s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/</link><description>Recent content in 2025-02-07s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Thu, 06 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/index.xml" rel="self" type="application/rss+xml"/><item><title>Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04295/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04295/</guid><description>Researchers jointly optimize prompt content and format to significantly boost Large Language Model (LLM) performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04295/cover.png"/></item><item><title>BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.03860/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.03860/</guid><description>BOLT bootstraps Long Chain-of-Thought reasoning in LLMs without distillation, achieving impressive results across various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.03860/cover.png"/></item><item><title>Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04296/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04296/</guid><description>HMA: a novel approach for generating high-quality robotic videos 15x faster, enabling real-time policy evaluation and data augmentation for scaling robot learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04296/cover.png"/></item><item><title>Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04128/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04128/</guid><description>Llasa, a novel single-Transformer TTS model, achieves state-of-the-art performance by scaling both training and inference compute, improving naturalness, prosody and emotional expressiveness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04128/cover.png"/></item><item><title>MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04235/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04235/</guid><description>MAGA reformulates existing corpora to massively expand LLM pretraining data, boosting performance across various model sizes while maintaining quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04235/cover.png"/></item><item><title>MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04299/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04299/</guid><description>MotionCanvas lets users design cinematic video shots with intuitive controls for camera and object movements, translating scene-space intentions into video animations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04299/cover.png"/></item><item><title>Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04328/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04328/</guid><description>Ola: a novel 7B parameter omni-modal language model achieves state-of-the-art performance across image, video and audio tasks using a progressive modality alignment training strategy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04328/cover.png"/></item><item><title>PILAF: Optimal Human Preference Sampling for Reward Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04270/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04270/</guid><description>PILAF optimizes human feedback in reward modeling for better LLM alignment by using a novel response sampling strategy that aligns reward modeling with value optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04270/cover.png"/></item><item><title>Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04322/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04322/</guid><description>Simple interactions can easily elicit harmful outputs from LLMs, which are often overlooked. The SPEAK EASY framework and HARMSCORE metric expose this vulnerability and provide tools for better safet&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.04322/cover.png"/></item><item><title>Analyze Feature Flow to Enhance Interpretation and Steering in Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.03032/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.03032/</guid><description>Researchers unveil a data-free method to visualize and control feature flow in LLMs, enhancing interpretability and enabling targeted model steering.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.03032/cover.png"/></item><item><title>DynVFX: Augmenting Real Videos with Dynamic Content</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.03621/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.03621/</guid><description>DynVFX: Effortlessly integrate dynamic content into real videos using simple text prompts. Zero-shot learning and novel attention mechanisms deliver seamless and realistic results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.03621/cover.png"/></item><item><title>Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.03544/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.03544/</guid><description>AlphaGeometry2 surpasses average IMO gold medalists in solving geometry problems!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.03544/cover.png"/></item><item><title>Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.03639/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.03639/</guid><description>This paper introduces PointVid, a 3D-aware video generation framework using 3D point regularization to enhance video realism and address common issues like object morphing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.03639/cover.png"/></item><item><title>MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.02358/</link><pubDate>Tue, 04 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.02358/</guid><description>MotionLab: One framework to rule them all! Unifying human motion generation &amp;amp; editing via a novel Motion-Condition-Motion paradigm, boosting efficiency and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.02358/cover.png"/></item><item><title>ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.00989/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.00989/</guid><description>ChartCitor: A multi-agent LLM framework combats LLM hallucination in ChartQA by providing fine-grained visual citations, enhancing user trust and productivity.</description></item><item><title>PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.00988/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.00988/</guid><description>PlotGen: A novel multi-agent LLM framework automates accurate scientific data visualization via multimodal feedback, boosting novice productivity and improving visualization accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.00988/cover.png"/></item><item><title>Weak-to-Strong Diffusion with Reflection</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.00473/</link><pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.00473/</guid><description>W2SD: A novel framework boosts diffusion model quality by using the difference between weak and strong models to refine sampling trajectories, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-07/2502.00473/cover.png"/></item></channel></rss>