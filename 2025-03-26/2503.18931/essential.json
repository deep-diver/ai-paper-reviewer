{"importance": "This paper introduces **a novel continual multimodal pre-training** pipeline that enhances visual representations and cross-modal alignment. The findings are significant for researchers in computer vision and multimodal learning, enabling **better visual understanding and opening new avenues for building more effective vision-language models**.", "summary": "COMP: Continually pre-training Vision Foundation Models for better vision and language alignment and arbitrary size inputs.", "takeaways": ["Continual Multimodal Pre-training enhances Vision Foundation Models (VFMs) for processing varying size visual inputs.", "Alignment Loss improves cross-modal alignment between visual and textual features, benefiting multimodal understanding.", "The proposed COMP method achieves state-of-the-art performance on multimodal tasks while maintaining accuracy on traditional vision tasks."], "tldr": "Pre-trained Vision Foundation Models (VFMs) excel in visual tasks, but face challenges in handling varied input sizes and aligning with language representations. Current methods struggle with diverse input resolutions and rely heavily on text-based supervision, leading to representation gaps. To address these issues, this paper introduces a **Continual Multimodal Pre-training pipeline** for prevailing VFMs. \n\nThe proposed pipeline leverages a **Continual Rotary Position Embedding** to accommodate different visual input resolutions, and an **Alignment Loss** to improve cross-modal alignment between visual and textual features. The resulting models achieve remarkable improvements in both multimodal understanding and traditional visual tasks, showcasing the effectiveness of the approach.", "affiliation": "Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.18931/podcast.wav"}