[{"figure_path": "https://arxiv.org/html/2503.19903/x1.png", "caption": "Figure 1: Left: Regular vision models such as SigLIP processes images at a low resolution (e.g., 378 \u00d7\\times\u00d7 378 pixels), which is not enough for many daily tasks such as spotting the stop sign while driving. In contrast, \\modelis able to both encode low-res features and efficiently process high-res information of 4K-resolution images via top-down patch selection, i.e., selectively processing relevant patches based on any text prompt. Top Right: SigLIP is pre-trained by contrasting global vision features and global captions, which is costly for high-resolution images. \\modelis pre-trained with additional contrast between local high-res features with local captions, enabling pre-training at 4K resolution with 79\u00d7\\times\u00d7 less cost than SigLIP. Bottom Right: VILA-HD with \\modelas the vision encoder is able to select high-res regions to process based on the user prompt. VILA-HD outperforms state-of-the-art MLLMs such as Qwen2-VL\u00a0[93] on the proposed 4KPro benchmark while achieving 2.96\u00d7\\times\u00d7 speedup.", "description": "Figure 1 illustrates the core concept of PS3 and its application in VILA-HD, highlighting its advantages over existing methods. The left panel contrasts regular vision models (e.g. SigLIP), which process images at low resolution (378x378 pixels) insufficient for tasks like stop sign detection, with PS3's ability to process high-resolution (4K) images efficiently.  PS3 achieves this via top-down patch selection, focusing only on relevant regions as specified by a text prompt.  The top-right panel shows the difference in training approach.  SigLIP uses costly global image-caption contrasting, while PS3 leverages local region-caption contrasting at high-resolution, resulting in a 79x reduction in computational cost.  Finally, the bottom-right panel showcases VILA-HD (using PS3 as the vision encoder), demonstrating superior performance and 2.96x speedup over Qwen2-VL on the 4KPro benchmark by selectively processing high-resolution image regions as determined by user prompts.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.19903/x2.png", "caption": "Figure 2: Curation of bounding boxes and captions of salient regions in the pre-training data. For each high-resolution image, we segment all the masks, detect salient regions with small or dense masks, and use an MLLM to generate captions about the local regions.", "description": "This figure illustrates the process of creating training data for the PS3 model.  It begins with a high-resolution image.  The image is first segmented into various regions using a segmentation model. Salient regions, identified as areas with small or densely packed segments, are then selected. Finally, a multi-modal language model (MLLM) generates a caption describing each of these salient regions. This process of segmentation, selection, and captioning creates training pairs of high-resolution image regions and their corresponding textual descriptions.", "section": "2. PS3: Vision Pre-Training at 4K Resolution"}, {"figure_path": "https://arxiv.org/html/2503.19903/x3.png", "caption": "Figure 3: Pre-training data example. Each instance contains an image with resolution up to 4K, bounding boxes of the salient regions in the image, and captions about details in the regions such as text or small objects.", "description": "Figure 3 shows an example of the pre-training data used for PS3. Each instance in the dataset contains a high-resolution image (up to 4K resolution), bounding boxes highlighting the salient regions within the image, and detailed captions describing the contents of those specific regions.  The captions focus on the details present within the identified regions, such as text or small objects, rather than a general description of the entire image. This illustrates the core principle of PS3: focusing on relevant local details rather than processing the whole image, even at high resolutions, to reduce computational cost.", "section": "2. PS3: Vision Pre-Training at 4K Resolution"}, {"figure_path": "https://arxiv.org/html/2503.19903/x4.png", "caption": "Figure 4: Model architecture of \\model. The model consists of 3 stages. In Stage 1, the model encodes global low-resolution features. In Stage 2, based on the low-resolution features as well as auxiliary high-resolution features extracted by a light-weight encoder, the model selects local regions that are either relevant to a text prompt (top-down selection) or salient by themselves (bottom-up selection). In Stage 3, the model processes multi-scale high-res patches from the selected regions with the same encoder from Stage 1. KV cache from the low-res tokens in Stage 1 is added to the self-attention layers to provide a global context for local high-res encoding.", "description": "This figure illustrates the architecture of the PS3 model, which consists of three stages. Stage 1 involves encoding global low-resolution image features using a vision transformer (ViT). Stage 2 performs patch selection. It uses both the low-resolution features from Stage 1 and auxiliary high-resolution features (extracted by a lightweight encoder) to identify important regions. These regions are selected based on either their relevance to a text prompt (top-down selection) or their inherent saliency (bottom-up selection).  Stage 3 processes multi-scale high-resolution patches from the selected regions using the same ViT encoder as in Stage 1. Key-Value (KV) cache from the low-resolution tokens in Stage 1 is incorporated into the self-attention layers to provide a global context for encoding the local high-resolution details.", "section": "2. PS3: Vision Pre-Training at 4K Resolution"}, {"figure_path": "https://arxiv.org/html/2503.19903/x5.png", "caption": "Figure 5: Pre-training algorithm of \\model. (a) During training, \\modelextracts the high-res features from the labeled local regions and contrasts them with embeddings of the local captions. To maintain the low-res feature quality, we also mix pairs of low-res features and global caption embedding in each batch. Both high-res and low-res features are extracted in the same way as Figure\u00a04. (b) The top-down patch selection score is supervised by ground-truth score map generated from the bounding box corresponding to the local caption. (c) The supervision for bottom-up selection is similar to top-down selection, except that the ground-truth selection score is generated from all the labeled bounding boxes of the image.", "description": "Figure 5 illustrates the pre-training algorithm of the PS3 model.  Panel (a) shows the contrastive learning process where high-resolution features from selected local image regions are contrasted against their corresponding local captions. To maintain the quality of low-resolution features, the model also incorporates pairs of low-resolution features and global captions within each batch. Both high-resolution and low-resolution feature extractions follow the architecture depicted in Figure 4. Panel (b) details the supervision method for top-down patch selection, which uses ground truth score maps derived from bounding boxes to guide the selection process.  Panel (c) shows the supervision mechanism for bottom-up patch selection; this is similar to top-down selection, but the ground truth selection score is derived from all labeled bounding boxes within the image.", "section": "2. PS3: Vision Pre-Training at 4K Resolution"}, {"figure_path": "https://arxiv.org/html/2503.19903/x6.png", "caption": "Figure 6: Qualitative examples of patch selection. Left: \\modelis pre-trained to perform bottom-up selection based on image saliency (denoted by \u2205\\varnothing\u2205) or top-down selection based on local captions. The selection process is detailed in Figure\u00a04 and Section\u00a02.2. Middle & Right: We fine-tune \\modelwith MLLM to select patches based on questions about local regions (Figure\u00a07 and Section\u00a03.1).", "description": "Figure 6 showcases the patch selection mechanism of the PS3 model. The left panel demonstrates both bottom-up and top-down selection methods. Bottom-up selection identifies salient regions within an image without any textual guidance, while top-down selection focuses on regions relevant to a given caption.  The middle and right panels illustrate how the model's patch selection capabilities are fine-tuned when integrated with a multi-modal large language model (MLLM). Here, the model selects patches based on specific questions pertaining to localized image regions.", "section": "2. PS3: Vision Pre-Training at 4K Resolution"}, {"figure_path": "https://arxiv.org/html/2503.19903/x7.png", "caption": "Figure 7: Model design of \\vilamodel. For any input image and text prompt, \\vilamodelfirst extracts the low-res image features using \\modeland sends them along with the text tokens to the LLM. The last-layer embedding of the last token is used to select high-res patches in \\model, whose features are then extracted by \\model, added with additional positional embedding, and sent to the LLM. Although \\modelonly processes at most 2560 high-res patches at a time, one can run the patch selection and high-res feature extraction for N times (N can be an arbitrary number) to encode up to 2560\u00d7\\times\u00d7N high-res patches.", "description": "The figure illustrates the architecture of the VILA-HD model, which enhances high-resolution visual perception in multi-modal LLMs.  VILA-HD uses PS3 as its vision encoder. First, it extracts low-resolution global image features using PS3 and sends these features, along with text tokens from the user's prompt, to the LLM. The LLM's final-layer embedding from the last token guides PS3 in selecting high-resolution image patches. PS3 then extracts features from these selected patches, adds positional embeddings, and sends them to the LLM for further processing.  Importantly, while PS3's processing is limited to 2560 high-resolution patches at once, the patch selection and feature extraction can be repeated multiple times (N) to incorporate a larger number of patches\u2014up to 2560N.", "section": "3. VILA-HD: Enabling High-Resolution MLLM with PS3"}, {"figure_path": "https://arxiv.org/html/2503.19903/x8.png", "caption": "Figure 8: Additional fine-tuning data for MLLMs with \\model. Left: To fine-tune top-down patch selection, we generate data with pairs of high-res image, question about a local region, and the bounding box of the region. This is generated by taking the \\modelpre-training data and retargeting the local captions into questions using LLaMA-3.1. Right: To align the \\modelhigh-res features to the LLM text space, it requires fine-tuning data that contains QA pairs on high-res images. We generate this by taking regular low-res image QA data and pasting the image onto a large-size background to get the new high-res image while the question and answer are inherited.", "description": "Figure 8 illustrates the two types of additional fine-tuning data used to improve the performance of multi-modal large language models (MLLMs) incorporating the PS3 vision encoder.  The left panel shows data for fine-tuning the top-down patch selection mechanism. This data consists of high-resolution images, a question specifically targeting a local region within the image, and the bounding box coordinates of that region. This data is created by converting the local captions from the PS3 pre-training data into questions using the LLaMA-3.1 language model. The right panel details the data used for aligning the high-resolution visual features extracted by PS3 with the textual representations within the LLM. This involves taking existing low-resolution image question-answering (QA) pairs and enlarging the image by pasting it onto a larger background canvas, creating a new high-resolution image while keeping the original question and answer unchanged. This process ensures that the model learns to leverage high-resolution details for accurate answers.", "section": "3. VILA-HD: Enabling High-Resolution MLLM with PS3"}, {"figure_path": "https://arxiv.org/html/2503.19903/x9.png", "caption": "Figure 9: Scaling properties of \\modelon \\vilamodel. (Left) Overall results. We report average performance of the MLLM on seven benchmarks under different maximum input resolution. The size of each data point indicates the number of high-res vision tokens input to the LLM. (a) When selecting all high-res patches for MLLM, the performance of \\modelscales better with the resolution than the baselines without high-resolution pre-training. (b) \\modelis able to process higher resolution and improve performance while selecting a fixed number of high-res patches for MLLM. (c) Within the same resolution, \\modelis able to trade compute for performance by selecting more high-res patches. (d) \\modelcan select more high-res patches at test time even if its selects a fixed number of high-res patches during MLLM training.", "description": "Figure 9 demonstrates the scaling properties of the PS3 model when integrated with the VILA-HD multi-modal large language model.  It showcases how performance changes across several factors related to resolution and computational cost. Panel (a) shows performance scaling with the increase of the input image resolution when all high-resolution patches are used.  A comparison with baselines highlights PS3's superiority. Panel (b) examines performance when a fixed number of high-resolution patches are used, regardless of resolution, demonstrating PS3's ability to maintain or even improve performance at higher resolutions with similar compute. Panel (c) illustrates a trade-off between compute and performance at a fixed resolution: using more high-resolution patches increases performance but demands more computation. Finally, panel (d) shows that increasing the number of high-resolution patches at test time, even after training with fewer patches, further boosts performance.", "section": "4. Scaling Properties of PS3"}, {"figure_path": "https://arxiv.org/html/2503.19903/x10.png", "caption": "Figure 10: Image resolution and MRR of different benchmarks. Existing benchmarks contain high-res images but the resolution required to answer the questions (MRR) is mostly under 1K. In contrast, 4KPro contains questions only solvable at 4K resolution.", "description": "Figure 10 illustrates the relationship between image resolution and the minimum resolution required to accurately answer associated questions (MRR) across various benchmark datasets.  Existing vision-language benchmarks, while employing high-resolution images, often have questions answerable at resolutions significantly lower than 4K (mostly under 1K). This is in stark contrast to the proposed 4KPro benchmark, where questions are specifically designed to be answerable only at a 4K resolution. This highlights the fact that many existing benchmarks do not truly test high-resolution visual perception capabilities.", "section": "5. 4KPro: Benchmarking PS3 at 4K Resolution"}, {"figure_path": "https://arxiv.org/html/2503.19903/x11.png", "caption": "Figure 11: Examples from 4KPro and comparison of different models. Each example corresponds to one out of four categories (Autonomous Vehicle, Household, Gaming, and UI Understanding) and each question can only be answered without ambiguity under 4K resolution. VILA-\\modelimproves the accuracy over the state-of-the-art MLLMs such as GPT-4o and Qwen2-VL.", "description": "Figure 11 presents four examples from the 4KPro benchmark dataset, each representing one of four categories: Autonomous Vehicle, Household, Gaming, and UI Understanding.  Each example shows a high-resolution image (4K) and a multiple-choice question that requires a high level of visual detail to answer accurately.  The figure compares the performance of the VILA-HD model (with PS3 as the vision encoder) against other state-of-the-art large language models (LLMs), such as GPT-4 and Qwen2-VL, demonstrating that VILA-HD achieves significantly higher accuracy in answering the 4KPro questions that demand high-resolution visual understanding. The results highlight the effectiveness of PS3 in enabling LLMs to handle high-resolution image data and achieve superior performance compared to baselines that do not utilize this approach.", "section": "5. 4KPro: Benchmarking PS3 at 4K Resolution"}, {"figure_path": "https://arxiv.org/html/2503.19903/x12.png", "caption": "Figure 12: Scaling properties of \\modelon 4KPro. \\modelshows consistently improved performance by scaling to 4K resolution and greatly outperforms the baselines.", "description": "Figure 12 presents a graph illustrating the scaling properties of the VILA-HD model, specifically its performance on the 4KPro benchmark as the resolution of the PS3 vision encoder increases.  The graph shows VILA-HD's accuracy consistently improves as the resolution scales up to 4K, significantly surpassing the performance of baselines that lack high-resolution vision pre-training. This visualization demonstrates the effectiveness of PS3 in achieving superior performance at high resolution.", "section": "5. 4KPro: Benchmarking PS3 at 4K Resolution"}, {"figure_path": "https://arxiv.org/html/2503.19903/x13.png", "caption": "Table 6: Ablation of PS3 pre-training, model, and MLLM designs. \u0394\u0394\\Deltaroman_\u0394 is the change of the average performance on the seven benchmarks after adding the design.", "description": "This table presents an ablation study analyzing the impact of various design choices on the performance of the PS3 model and its integration within the MLLM.  It shows the average performance change (\u0394\u0394\n\u0394) across seven benchmark datasets after including or excluding specific design elements. These elements cover PS3's pre-training algorithm, model architecture, and the MLLM's design.  The results quantify the contribution of each component, enabling a better understanding of their individual importance to the overall model effectiveness.", "section": "7.1. Key Designs in Pre-Training Algorithm and Model Architecture"}, {"figure_path": "https://arxiv.org/html/2503.19903/x14.png", "caption": "Table 7: Ablation of top-down and bottom-up patch selection. Select (Train) and Select (Test) are the percentage of high-res patches \\modelselects at training and test time. Recall is the recall rate of how many patches in the ground-truth regions are selected at test time.", "description": "This table presents an ablation study on the effects of different patch selection methods on the performance of a vision model.  Specifically, it compares the performance using random, bottom-up (saliency-based), and top-down (prompt-guided) patch selection approaches. The table shows the percentage of high-resolution patches selected during training and testing, along with the recall rate (the proportion of ground truth patches successfully selected). This analysis helps determine the impact of different selection strategies on both training efficiency and the model's ability to accurately identify relevant image regions.", "section": "7.2. Top-Down and Bottom-Up Patch Selection Matters"}, {"figure_path": "https://arxiv.org/html/2503.19903/x15.png", "caption": "Figure 13: Trade-off between image scales for different benchmarks. Select @ 756/1512 are the percentage of selected patches at 756 and 1512 scales at test time, respectively. \\modelcan flexibly adjust token selection ratios at different image scales to achieve the best performance for different downstream tasks.", "description": "Figure 13 illustrates the flexible approach of PS3 in handling different image resolutions for various downstream tasks.  It demonstrates the trade-off between using different image scales (756 and 1512 pixels) to achieve optimal performance.  The x-axis represents the percentage of patches selected at each resolution during testing, showing different optimal ratios for various tasks.  The y-axis shows the resulting accuracy on each benchmark.  This highlights PS3's adaptability to balance the need for detailed information (higher resolution) with computational efficiency (fewer patches).", "section": "4. Scaling Properties of PS3"}, {"figure_path": "https://arxiv.org/html/2503.19903/x16.png", "caption": "Figure 14: PCA visualization of visual features. The baselines, \\stwoand AnyRes, have either noisy or blurry features at 4K resolution, while \\modelshows extremely fine-grained features that highlight details such as small texts on the banners.", "description": "Figure 14 presents a Principal Component Analysis (PCA) visualization comparing the visual features extracted by PS3 and two baseline methods (S2 and AnyRes) at 4K resolution.  The PCA reduces the dimensionality of the high-dimensional feature vectors, allowing for a visual comparison of the feature representations. The image shows that the baselines, S2 and AnyRes, produce either noisy or blurry features at this high resolution, hindering their ability to capture fine details. In contrast, PS3 generates extremely fine-grained features, showcasing a remarkable capacity to extract and represent even subtle details, such as small text on banners, illustrating its superior performance in high-resolution visual perception.", "section": "7.5. Visualization of PS3 Visual Representations"}, {"figure_path": "https://arxiv.org/html/2503.19903/x17.png", "caption": "Figure 15: Examples of pre-training data with natural images. Here each image is labeled with bounding boxes of four salient regions (highlighted by different colors), together with the local captions of each region. The local captions, generated by Qwen2-VL, contains details in the crops although there are still occasional hallucinations.", "description": "Figure 15 presents examples from the PS3 pre-training dataset. Each image showcases four salient regions, outlined with different colored bounding boxes.  Accompanying each box is a detailed caption generated by the Qwen2-VL model, which describes the contents of the respective cropped image region. While these captions are generally accurate, they occasionally exhibit hallucinations (i.e., descriptions of things that are not actually present in the image). The figure highlights the type of data used to train PS3 to perceive and understand high-resolution visual details.", "section": "2.1. Pre-Training Data of PS3"}, {"figure_path": "https://arxiv.org/html/2503.19903/x18.png", "caption": "Figure 16: Examples of pre-training data with document images. Here each image is labeled with four bounding boxes (highlighted by different colors), together with the OCR results as the captions of each region.", "description": "Figure 16 showcases examples from the PS3 pre-training dataset that includes document images. Each document image has four regions highlighted with different colors, and each region is accompanied by a caption extracted via OCR (Optical Character Recognition).  This demonstrates PS3's ability to handle and label text within various regions of a document image to create localized image-text pairs for training.", "section": "2.1. Pre-Training Data of PS3"}, {"figure_path": "https://arxiv.org/html/2503.19903/x19.png", "caption": "Figure 17: Qualitative examples of patch selection on natural images. \\modelis able to locate different parts of the image that are relevant to the question.", "description": "Figure 17 showcases qualitative examples illustrating the model's patch selection capabilities on natural images.  For each example, a question is posed, and the model highlights the specific image regions most relevant to answering the question. This demonstrates the model's ability to focus on the most pertinent details within the image rather than processing the entire image, thus enhancing efficiency and accuracy.  The highlighted areas visually represent the patches selected by the model, showing its selective attention mechanism in action.", "section": "7.2. Top-Down and Bottom-Up Patch Selection Matters"}]