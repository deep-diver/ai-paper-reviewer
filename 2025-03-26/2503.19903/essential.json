{"importance": "This paper introduces PS3 to achieve **high-resolution perception with near-constant cost**, scales CLIP-style pre-training to 4K, and achieves **better performance & efficiency**, which can inspire future research about MLLMs.", "summary": "PS3 scales CLIP vision pre-training to 4K resolution with near-constant cost, achieving state-of-the-art performance in multi-modal LLMs.", "takeaways": ["PS3 enables scaling CLIP-style vision pre-training to 4K resolution with near-constant cost.", "VILA-HD, an MLLM using PS3, achieves state-of-the-art performance and efficiency in high-resolution visual perception tasks.", "The introduced 4KPro benchmark reveals the limitations of current benchmarks in requiring 4K-resolution perception."], "tldr": "Current vision pre-training is limited to low resolutions due to the high computational cost of processing larger images. This limitation hinders the perception of visual details crucial for many real-world tasks. To address this, the paper introduces PS3, a new approach that scales CLIP-style vision pre-training to 4K resolution while maintaining a near-constant cost. The key idea is to selectively process local regions and contrasting them with detailed captions, reducing the computational overhead significantly. \n\nPS3 enables the development of VILA-HD, an MLLM. Experiments show VILA-HD achieves better high-resolution visual perception using fewer tokens.  PS3 enables scaling properties of VILA-HD, including free resolution scaling & test-time compute scaling for better performance. The paper also introduces 4KPro, a new benchmark of image QA at 4K resolution, where VILA-HD outperforms existing MLLMs. ", "affiliation": "UC Berkeley", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.19903/podcast.wav"}