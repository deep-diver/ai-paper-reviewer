<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-03-26s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/</link><description>Recent content in 2025-03-26s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Tue, 25 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/index.xml" rel="self" type="application/rss+xml"/><item><title>Exploring Hallucination of Large Multimodal Models in Video Understanding: Benchmark, Analysis and Mitigation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19622/</link><pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19622/</guid><description>HAVEN: A new benchmark to tackle the hallucination issue in video understanding of large multimodal models!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19622/cover.png"/></item><item><title>Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19385/</link><pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19385/</guid><description>Inference-time scaling for flow models enhances alignment with user preferences via stochastic generation and budget allocation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19385/cover.png"/></item><item><title>Scaling Vision Pre-Training to 4K Resolution</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19903/</link><pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19903/</guid><description>PS3 scales CLIP vision pre-training to 4K resolution with near-constant cost, achieving state-of-the-art performance in multi-modal LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19903/cover.png"/></item><item><title>Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19855/</link><pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19855/</guid><description>Boost LLM reasoning by having models &amp;lsquo;Think Twice&amp;rsquo;! This novel method iteratively refines answers, significantly enhancing accuracy on complex tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19855/cover.png"/></item><item><title>CoMP: Continual Multimodal Pre-training for Vision Foundation Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.18931/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.18931/</guid><description>COMP: Continually pre-training Vision Foundation Models for better vision and language alignment and arbitrary size inputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.18931/cover.png"/></item><item><title>Frequency Dynamic Convolution for Dense Image Prediction</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.18783/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.18783/</guid><description>FDConv: Adaptable convolution via frequency domain learning, enhancing performance without heavy parameter cost.</description></item><item><title>FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from Few Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19207/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19207/</guid><description>FRESA: fast feedforward 3D personalized avatar creation from few images.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19207/cover.png"/></item><item><title>Latent Space Super-Resolution for Higher-Resolution Image Generation with Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.18446/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.18446/</guid><description>LSRNA: Super-resolution in latent space enhances image generation with diffusion models, achieving faster speeds and improved detail.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.18446/cover.png"/></item><item><title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19041/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19041/</guid><description>LookAhead Tuning: Safer LLMs via Partial Answer Previews by preserving initial token distributions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19041/cover.png"/></item><item><title>Gumbel-Softmax Flow Matching with Straight-Through Guidance for Controllable Biological Sequence Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.17361/</link><pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.17361/</guid><description>Gumbel-Softmax Flow Matching enables controllable biological sequence generation with straight-through guidance, scaling efficiently to high-dimensional simplices.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.17361/cover.png"/></item><item><title>Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.17237/</link><pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.17237/</guid><description>Presents a strong baseline for multi-UAV tracking in thermal infrared video using YOLOv12 and BoT-SORT, achieving competitive results without complex enhancements.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.17237/cover.png"/></item><item><title>When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only Training For Human-Centered Decision Making</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.16965/</link><pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.16965/</guid><description>VLMs self-improve with text-only training, outperforming vision for human-centered decisions, opening efficient enhancement avenues.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.16965/cover.png"/></item><item><title>Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.14905/</link><pubDate>Wed, 19 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.14905/</guid><description>FakeVLM: A multimodal model &amp;amp; artifact-annotated dataset for detecting synthetic images with interpretable explanations, setting a new benchmark.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.14905/cover.png"/></item><item><title>MDocAgent: A Multi-Modal Multi-Agent Framework for Document Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.13964/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.13964/</guid><description>MDocAgent: Multi-agent Doc understanding by integrating text and image for better accuracy.</description></item><item><title>Towards a Unified Copernicus Foundation Model for Earth Vision</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.11849/</link><pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.11849/</guid><description>Unified Copernicus Foundation Model for Earth Vision: A multimodal approach to improve scalability, versatility, and adaptability of EO models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.11849/cover.png"/></item></channel></rss>