{"importance": "This paper addresses the critical need to enhance human-centered decision-making in VLMs. By demonstrating the effectiveness of text-only training and self-improvement mechanisms, it offers **practical, scalable solutions for improving VLM performance** in real-world applications. The findings open new avenues for research in data-centric training and model optimization.", "summary": "VLMs self-improve with text-only training, outperforming vision for human-centered decisions, opening efficient enhancement avenues.", "takeaways": ["LLMs can outperform their VLM counterparts in human-centered decision-making tasks.", "Text-only training enhances VLMs' reasoning abilities and decision-making capabilities.", "VLMs can achieve self-improvement through their LLM modules, providing a scalable solution."], "tldr": "Embodied decision-making is vital for AI agents, but Visual Language Models (VLMs) struggle, especially in human-centered contexts needing value understanding. Current VLMs rely on image-text data, which is costly and impractical to obtain in real-world scenarios. Surprisingly, Large Language Models(LLMs) that receive text perform better than VLMs using images. This suggests visual data complicates VLM's decision-making. This paper aims to enhance VLMs' reasoning without costly paired data.\n\nThis paper introduces text-only training, using model-synthesized textual data to enhance VLM decision-making, applied to multimodal inputs during inference. This method bolsters VLMs' language components and transfers learned skills to multimodal inference, negating the need for expensive paired data. VLMs achieve gains through self-improvement, using training data from their LLM counterparts rather than larger models. This establishes an efficient approach to enhance VLM capabilities.", "affiliation": "Hong Kong Polytechnic University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.16965/podcast.wav"}