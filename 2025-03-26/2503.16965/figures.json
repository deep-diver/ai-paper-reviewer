[{"figure_path": "https://arxiv.org/html/2503.16965/x1.png", "caption": "Figure 1: \nOur text-only training using model synthesized textual data enhances VLM decision-making abilities, which are then applied to multimodal inputs in inference.\nThis enables model improvement without image-text paired training data. Complete data samples are shown in \u00a0\u00a7A.5.", "description": "This figure illustrates the proposed text-only training method for enhancing Visual Language Model (VLM) decision-making abilities.  The process starts with model-synthesized textual data, which focuses on human-centered decision-making scenarios.  This textual data is used to train the VLM, strengthening its language understanding and reasoning capabilities.  Crucially, this training approach doesn't rely on expensive image-text paired data. After the text-only training phase, the improved VLM is then applied to multimodal inputs (including images) during inference, demonstrating the transferability of the learned abilities.  This method effectively improves VLM performance without requiring the typical large-scale image-text datasets.", "section": "3 Enhancing VLM Decision-Making via Text-Only Training"}, {"figure_path": "https://arxiv.org/html/2503.16965/x2.png", "caption": "Figure 2: VLM results after text-only training.", "description": "This figure shows the accuracy improvement of three different Vision Language Models (VLMs) after undergoing text-only fine-tuning.  Before fine-tuning, the accuracy of each VLM (Mllama, Qwen2-VL, and LLaVA-OneVision) is shown by the light-colored bars. The dark-colored bars represent the accuracy of each VLM *after* text-only fine-tuning. The improvement in accuracy after the training is clearly visible, demonstrating the efficacy of the text-only training approach in enhancing the VLMs' decision-making abilities.", "section": "3 Enhancing VLM Decision-Making via Text-Only Training"}, {"figure_path": "https://arxiv.org/html/2503.16965/x3.png", "caption": "Figure 3: Prompts for training data generation.", "description": "This figure shows the prompts used to generate training data for the text-only training approach.  The prompts instruct GPT-4 and Llama-3.1-8B-Instruct models to create complex decision-making questions based on human-centered scenarios.  The prompts encourage the models to vary the complexity by adding contextual details, making the choices challenging, and changing the way questions are asked, using techniques like reverse reasoning or critical thinking. The generated data consists of a situation description, a multiple-choice question, the correct answer, and a rationale explaining why the selected answer is correct. The prompts are designed to generate diverse and high-quality data for fine-tuning the VLMs.", "section": "A.3 Text-Only Training Data Generation"}]