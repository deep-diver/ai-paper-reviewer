[{"heading_title": "Video LMMs Eval", "details": {"summary": "**Evaluating video-based Large Multimodal Models (LMMs) is a critical yet challenging task.** Unlike image-based LMMs, video LMMs must process temporal information, increasing complexity. Evaluation requires assessing not just object recognition, but also event understanding and action sequencing. Existing benchmarks often fall short by either adapting image-based metrics or simplifying video content. A robust evaluation should consider varying video durations, frame rates, and complexity levels. **Hallucination, a major concern in LMMs, manifests differently in video**. It's not just about misidentifying objects, but misinterpreting actions or fabricating events within the video sequence. A comprehensive evaluation needs to assess the model's ability to maintain temporal consistency, avoid event fabrication, and handle ambiguous situations. The assessment should be nuanced, incorporating varied question types (open-ended, multiple-choice) to probe different aspects of understanding. **Furthermore, evaluating reasoning skills in video LMMs is paramount.** Models should not only describe the video but also infer relationships, predict outcomes, and answer causal questions. This requires benchmarks that go beyond surface-level understanding and delve into deeper analytical capabilities. Developing such benchmarks needs careful consideration of potential biases and the ability to scale, and it also necessitates automatic and human evaluation to get the comprehensive results."}}, {"heading_title": "Thinking SRFT", "details": {"summary": "**SRFT (Supervised Reasoning Fine-Tuning)** is a process of improving the logic and reasoning capabilities of models through supervised fine-tuning. It enhances **LMM's (Large Multimodal Models)** comprehension and correlation of visual inputs (videos) and linguistic structures, facilitating more logical derivations. The **LORA-based SFT** approach is key; this allows for model parameter modification during fine-tuning without catastrophic forgetting or significant computational cost. **Reasoning data synthesis** becomes indispensable when the base LMM struggles with multi-image or visual processing, using external models to generate training data to compensate and further refine its analytical capacity. This synthesized data aims to emulate multi-step thinking. "}}, {"heading_title": "HAVEN: 3-Axis", "details": {"summary": "The 'HAVEN: 3-Axis' heading, while not explicitly present in the text, likely refers to the core design of the HAVEN benchmark for evaluating hallucination in video understanding. Considering the paper's focus, the three axes probably represent the key dimensions along which hallucinations are assessed. **These axes are likely Hallucination Causes, Hallucination Aspects, and Question Formats**.  The first cause relates to identifying conflict between prior knowledge, in-context conflict, and the inherent capability deficiencies of LMMs. The second is how hallucination is observed in a video, object, scene, and event. The last refers to how it is being identified or evaluated like binary, multiple choice, and short answer. **This 3-axis structure enables a granular analysis of where and why LMMs falter in video understanding**, moving beyond simple accuracy metrics to pinpoint specific vulnerabilities. The axes also make the framework more robust and systematic. "}}, {"heading_title": "CoT helps LMMS", "details": {"summary": "**Chain-of-Thought (CoT) prompting enhances Large Multimodal Models (LMMs) by improving reasoning.** CoT enables LMMs to break down complex tasks into sequential steps, mirroring human thought processes. This step-by-step approach improves accuracy by addressing capability deficiencies and contextual conflicts. **CoT reduces hallucinations in LMMs by providing a structured framework** to verify information and maintain consistency. This leads to more reliable and trustworthy outputs, vital for applications requiring factual precision. **CoT's benefits include enhanced reasoning ability and reduced errors.**"}}, {"heading_title": "Thinking vs DPO", "details": {"summary": "**Thinking** and **Direct Preference Optimization (DPO)** represent distinct yet complementary strategies for enhancing the performance of large language models. **Thinking-based approaches** focus on equipping models with enhanced reasoning capabilities, often achieved through techniques like chain-of-thought prompting or supervised reasoning fine-tuning. This aims to improve the model's ability to understand complex relationships and generate more accurate responses. On the other hand, **DPO** is a preference learning technique that directly optimizes the model based on human feedback. It avoids the need for explicit reward modeling, making it more efficient and stable. Ideally, models would undergo thinking-based training to fortify reasoning, followed by DPO to align the reasoning process with human preferences, mitigating hallucinations."}}]