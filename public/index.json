
[{"content":"","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/","section":"AI Paper Reviews by AI","summary":"","title":"AI Paper Reviews by AI","type":"page"},{"content":"","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/blog/","section":"Categories","summary":"","title":"Blog","type":"categories"},{"content":"","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/post/","section":"Categories","summary":"","title":"Post","type":"categories"},{"content":" TL;DR # blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u0026nbsp; read the paper on arXiv Why does this paper matter? # blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rVisual Insights # Figure 1. Observatioins about visual redundancy acoross layers. Left: TextVQA performance of LLaVA-1.5 with varying ratio of retained image tokens at different layer. The preserved image tokens are those that receive the highest attention from the text tokens. Right: Visualization of attention map in shallow and deep layers. ModelTrain \u0026amp; InferGPU hours#patchesInfer Flops(T)MMEMMBMMB CNSEEDIMM StarPOPEAvgLLaVA -NeXT-7Bvanilla366520.81534.168.760.571.141.186.167.4PDrop21859.461540.867.860.669.941.786.567.3vanilla483940.61544.767.460.069.540.086.366.7PDrop269918.11542.068.161.070.340.986.667.3LLaVA -1.5-7Bvanilla10413.821510.764.358.366.133.285.963.9PDrop7911.781467.366.158.565.534.086.063.9\nTable 1. LVLM w and w/o our method on 6 benchmarks. Benchmark names are abbreviated due to space limits. MMB: MMBenchmark (Liu et al., 2023); MMBCN : MMBench-Chinese (Liu et al.,2023); SEEDI: SEED-Bench (Image) (Li et al., 2023b). We denote PyramidDrop as PDrop. More visual insights Figure 2. Overview of PyramidDrop. We divide the forward pass of the LLM into multiple stages, and drop part of the image tokens at the end of each stage with a pre-defined ratio. The dropping is based on a lightweight attention calculation with a negligible time overhead, and according to this criterion, the LLM accurately selects important image tokens related to instruction. Due to the efficient redundancy reduction strategy, the average sequence length decreases rapidly. Figure 3. We compare the performance of the original LLaVA-1.5 and LLaVA-1.5 trained using PDrop, where we preserve different ratios of image tokens at layer 2, 8, 16, and 24, respectively. The horizontal axis represents the proportion of retained image tokens according to attention score. Section Summary # Introduction # blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid Ref. blah blah blah blah blah blah blah blah blah More references to follow-up \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah Related Work # blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid Ref. blah blah blah blah blah blah blah blah blah More references to follow-up \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah Method # blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid Ref. blah blah blah blah blah blah blah blah blah More references to follow-up \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah Experiments # blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid Ref. blah blah blah blah blah blah blah blah blah More references to follow-up \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e library-solid blah blah blah blah blah blah blah blah blah Paper Image # ","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/pyramiddrop/","section":"Posts","summary":"PyramidDrop selectively reduces redundant visual tokens in deeper layers of large vision-language models, accelerating training and inference while maintaining performance.","title":"PyramidDrop: Accelerating Large Vision-Language Models via Visual Redundancy Reduction","type":"posts"},{"content":"","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"22 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/vision-language-models/","section":"Tags","summary":"","title":"Vision-Language Models","type":"tags"},{"content":"","date":"13 June 2022","externalUrl":null,"permalink":"/ai-paper-reviewer/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","externalUrl":null,"permalink":"/ai-paper-reviewer/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/ai-paper-reviewer/series/","section":"Series","summary":"","title":"Series","type":"series"}]