<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ðŸ¤— 24-10-25 on AI Paper Reviews by AI</title>
    <link>http://localhost:1313/ai-paper-reviewer/tags/-24-10-25/</link>
    <description>Recent content in ðŸ¤— 24-10-25 on AI Paper Reviews by AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Â© 2024 AI Paper Reviews by AI</copyright>
    <lastBuildDate>Thu, 24 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/ai-paper-reviewer/tags/-24-10-25/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CAMEL-Bench: A Comprehensive Arabic LMM Benchmark</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18976/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18976/</guid>
      <description>CAMEL-Bench: a new Arabic LMM benchmark with 29K+ questions across 8 diverse domains, revealing significant room for improvement even in top models.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18976/cover.png" />
    </item>
    
    <item>
      <title>CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18505/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18505/</guid>
      <description>CCI3.0-HQ: A new, high-quality 500GB Chinese dataset boosts large language model performance by leveraging a novel two-stage filtering pipeline, exceeding existing datasets in benchmark evaluations.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18505/cover.png" />
    </item>
    
    <item>
      <title>Data Scaling Laws in Imitation Learning for Robotic Manipulation</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18647/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18647/</guid>
      <description>Robotic manipulation policies achieve near-human success rates in unseen environments using a novel data-scaling approach, enabling efficient data collection and zero-shot generalization.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18647/cover.png" />
    </item>
    
    <item>
      <title>DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18860/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18860/</guid>
      <description>DeCoRe: A novel LLM decoding strategy dynamically contrasts base and masked LLM outputs using conditional entropy, significantly reducing hallucinations and boosting contextual accuracy.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18860/cover.png" />
    </item>
    
    <item>
      <title>Distill Visual Chart Reasoning Ability from LLMs to MLLMs</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18798/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18798/</guid>
      <description>Boosting visual chart reasoning in MLLMs via Code-as-Intermediary Translation (CIT):  efficiently generating high-quality training data by leveraging LLMs.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18798/cover.png" />
    </item>
    
    <item>
      <title>Framer: Interactive Frame Interpolation</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18978/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18978/</guid>
      <description>Framer: an interactive frame interpolation method lets users customize video transitions via keypoint manipulation, producing smoother, more creative results.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18978/cover.png" />
    </item>
    
    <item>
      <title>LOGO -- Long cOntext aliGnment via efficient preference Optimization</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18533/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18533/</guid>
      <description>LOGO: a novel training strategy enhances long-context models&amp;rsquo; generation by efficiently optimizing preferences, achieving performance comparable to GPT-4 with limited data.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18533/cover.png" />
    </item>
    
    <item>
      <title>MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18977/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18977/</guid>
      <description>MotionCLR: Training-free motion editing via attention mechanism manipulation.  Versatile editing, good generation, and explainability.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18977/cover.png" />
    </item>
    
    <item>
      <title>Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18775/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18775/</guid>
      <description>VINE, a novel watermarking method, significantly improves robustness against image editing by leveraging blurring distortions and a pretrained diffusion model for imperceptible embedding.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18775/cover.png" />
    </item>
    
    <item>
      <title>Should We Really Edit Language Models? On the Evaluation of Edited Language Models</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18785/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18785/</guid>
      <description>Current LLM editing methods are great for small updates, but scaling them negatively impacts performance and safety.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18785/cover.png" />
    </item>
    
    <item>
      <title>Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18451/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18451/</guid>
      <description>Skywork-Reward achieves state-of-the-art reward modeling for LLMs using novel data-centric techniques, producing a top-performing model with only 80K preference pairs.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18451/cover.png" />
    </item>
    
    <item>
      <title>SMITE: Segment Me In TimE</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18538/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18538/</guid>
      <description>SMITE: a novel video segmentation technique that uses few reference images to generate accurate, temporally consistent segmentations with varying granularities, outperforming existing methods.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18538/cover.png" />
    </item>
    
    <item>
      <title>Stable Consistency Tuning: Understanding and Improving Consistency Models</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18958/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18958/</guid>
      <description>Stable Consistency Tuning (SCT) boosts image generation speed and quality in consistency models, reaching new state-of-the-art performance.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18958/cover.png" />
    </item>
    
    <item>
      <title>Taipan: Efficient and Expressive State Space Language Models with Selective Attention</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18572/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18572/</guid>
      <description>Taipan: A novel hybrid language model efficiently handles long sequences via selective attention and SSMs, achieving superior performance across various tasks.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18572/cover.png" />
    </item>
    
    <item>
      <title>The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18441/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18441/</guid>
      <description>This paper enhances Transformer models by optimizing sub-word encoding, hyperparameters, and attention, improving generative AI&amp;rsquo;s efficiency and quality.</description>
      
    </item>
    
    <item>
      <title>Unbounded: A Generative Infinite Game of Character Life Simulation</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18975/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18975/</guid>
      <description>UNBOUNDED: A generative AI creates an infinite video game where players interact with a virtual character via natural language, generating open-ended storylines and visuals in real-time.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18975/cover.png" />
    </item>
    
    <item>
      <title>Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18693/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18693/</guid>
      <description>ScaleQuest revolutionizes LLM reasoning by efficiently generating a massive, high-quality math dataset from scratch using open-source models, significantly enhancing their performance.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18693/cover.png" />
    </item>
    
    <item>
      <title>WAFFLE: Multi-Modal Model for Automated Front-End Development</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18362/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18362/</guid>
      <description>WAFFLE: A new fine-tuning strategy boosts AI&amp;rsquo;s ability to turn UI designs into HTML code, achieving significant accuracy improvements.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18362/cover.png" />
    </item>
    
    <item>
      <title>Why Does the Effective Context Length of LLMs Fall Short?</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18745/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18745/</guid>
      <description>Boosting LLMs&amp;rsquo; long-context abilities: STRING shifts trained positions to enhance distant information gathering, significantly outperforming existing methods.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18745/cover.png" />
    </item>
    
    <item>
      <title>ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.17779/</link>
      <pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.17779/</guid>
      <description>ADEM-VL boosts vision-language model efficiency by using a parameter-free cross-attention mechanism, achieving superior accuracy with reduced computational cost.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.17779/cover.png" />
    </item>
    
    <item>
      <title>Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18252/</link>
      <pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18252/</guid>
      <description>Async RLHF trains LLMs 40% faster than sync methods by separating generation and training, enabling concurrent learning and sample production.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18252/cover.png" />
    </item>
    
    <item>
      <title>Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18234/</link>
      <pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18234/</guid>
      <description>Boosting LLM inference speed, this paper introduces multi-draft speculative sampling, optimizing token selection via importance and speculative sampling for improved efficiency.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18234/cover.png" />
    </item>
    
    <item>
      <title>Value Residual Learning For Alleviating Attention Concentration In Transformers</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.17897/</link>
      <pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.17897/</guid>
      <description>ResFormer &amp;amp; SVFormer: Novel transformers tackling attention concentration &amp;amp; reducing KV cache for efficient training &amp;amp; inference.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.17897/cover.png" />
    </item>
    
    <item>
      <title>ZIP-FIT: Embedding-Free Data Selection via Compression-Based Alignment</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18194/</link>
      <pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18194/</guid>
      <description>ZIP-FIT uses gzip compression to efficiently select task-relevant training data, significantly boosting language model performance and reducing training time.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18194/cover.png" />
    </item>
    
    <item>
      <title>Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.17243/</link>
      <pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.17243/</guid>
      <description>Inf-CL breaks the memory barrier in contrastive learning, enabling near-infinite batch size scaling without sacrificing accuracy, thus achieving unprecedented performance improvements.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.17243/cover.png" />
    </item>
    
    <item>
      <title>Can Knowledge Editing Really Correct Hallucinations?</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.16251/</link>
      <pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.16251/</guid>
      <description>HalluEditBench: A new benchmark reveals knowledge editing&amp;rsquo;s limitations in truly fixing LLM hallucinations, offering valuable insights for future improvements.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.16251/cover.png" />
    </item>
    
    <item>
      <title>Language Models are Symbolic Learners in Arithmetic</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.15580/</link>
      <pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.15580/</guid>
      <description>LLMs don&amp;rsquo;t calculate arithmetic like humans; they&amp;rsquo;re symbolic pattern-matchers, learning by identifying subgroups and their complexity.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.15580/cover.png" />
    </item>
    
    <item>
      <title>Pantograph: A Machine-to-Machine Interaction Interface for Advanced Theorem Proving, High Level Reasoning, and Data Extraction in Lean 4</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.16429/</link>
      <pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.16429/</guid>
      <description>Pantograph: A new Lean 4 interface boosts machine-assisted theorem proving via efficient search and high-level reasoning, opening avenues for advanced ML models.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.16429/cover.png" />
    </item>
    
    <item>
      <title>Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.15999/</link>
      <pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.15999/</guid>
      <description>SPARE, a novel training-free method, uses sparse autoencoders to precisely control LLMs&amp;rsquo; knowledge selection, significantly improving accuracy in resolving knowledge conflicts.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.15999/cover.png" />
    </item>
    
  </channel>
</rss>
