[{"figure_path": "https://arxiv.org/html/2501.09019/x1.png", "caption": "Figure 1: Illustration of FIFO-Diffusion\u00a0(Kim et\u00a0al. 2024) (top) and our Ouroboros-Diffusion (bottom) for tuning-free long video generation.", "description": "This figure illustrates the core difference between the existing FIFO-Diffusion method and the proposed Ouroboros-Diffusion method for generating long videos without requiring model fine-tuning.  FIFO-Diffusion (top) uses a queue to sequentially generate video frames.  It adds noisy frames to the tail of the queue and removes clean frames from the head. This approach often leads to inconsistency because newly added frames are independent of existing frames. In contrast, Ouroboros-Diffusion (bottom) enhances consistency by incorporating several strategies to improve structural and content consistency.  It generates the noisy frame to be added to the tail in a manner that takes into account existing frames, allowing for better temporal coherence.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.09019/x2.png", "caption": "Figure 2: \nAn overview of our Ouroboros-Diffusion. The whole framework (a) contains three key components: coherent tail latent sampling in queue manager , (b) Subject-Aware Cross-frame Attention (SACFA), and (c) self-recurrent guidance.\nThe coherent tail latent sampling in queue manager derives the enqueued frame latents at the queue tail to improve structural consistency.\nThe Subject-Aware Cross-frame Attention (SACFA) aligns subjects across frames within short segments for better visual coherence.\nThe self-recurrent guidance leverages information from all historical cleaner frames to guide the denoising of noisier frames, fostering rich and contextual global information interaction.", "description": "Ouroboros-Diffusion is composed of three key parts working together to generate long, consistent videos.  The queue manager uses coherent tail latent sampling, creating new frames by basing them on the existing frames to maintain structural consistency. The Subject-Aware Cross-Frame Attention (SACFA) mechanism enhances visual coherence by ensuring consistent subjects over short video segments. Finally, self-recurrence guidance uses previously generated, clean frames to inform the generation of noisier frames, enriching the overall content consistency and context.", "section": "4 Methodology"}, {"figure_path": "https://arxiv.org/html/2501.09019/x3.png", "caption": "Figure 3: The detailed illustration of coherent tail latent sampling in the queue manager.", "description": "This figure details the process of coherent tail latent sampling within the Ouroboros-Diffusion queue manager.  It illustrates how, instead of simply adding random Gaussian noise to the tail of the queue (as in standard FIFO-Diffusion), the model leverages the low-frequency component of the second-to-last frame in the queue. This low-frequency component preserves the overall structure and layout information, ensuring smoother transitions between frames and enhanced structural consistency. High-frequency components of random noise are then added to maintain video dynamics.  The figure visually depicts this process by showing the flow of information from the existing queue to the creation of a new coherent tail latent which maintains visual coherence yet introduces movement.", "section": "4.2 Coherent Tail Latent Sampling"}, {"figure_path": "https://arxiv.org/html/2501.09019/x4.png", "caption": "Figure 4: Visual examples of single-scene long video generation by different approaches. The text prompt is \u201cA cat wearing sunglasses and working as a lifeguard at a pool.\u201d", "description": "This figure displays visual examples of long-video generation (128 frames) from a single text prompt: \"A cat wearing sunglasses and working as a lifeguard at a pool.\"  It compares the results of five different methods: StreamingT2V, StreamingT2V-VideoTetris, FIFO-Diffusion, FreeNoise, and Ouroboros-Diffusion (the authors' proposed method).  The image shows a series of frames from each generated video, allowing a visual comparison of the quality, consistency, and motion smoothness across the various approaches. The purpose is to highlight the advantages of Ouroboros-Diffusion in terms of producing more coherent and visually consistent videos.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.09019/x5.png", "caption": "Figure 5: Visual examples of multi-scene long video generation by different approaches. The multi-scene prompts are: 1). an astronaut is riding a horse in space; 2). an astronaut is riding a dragon in space; 3). an astronaut is riding a motorcycle in space.", "description": "Figure 5 presents a comparison of multi-scene long video generation results from four different methods: Ouroboros-Diffusion, FreeNoise, and FIFO-Diffusion.  Each method was tasked with generating a long video based on three sequential prompts: an astronaut riding a horse in space, an astronaut riding a dragon in space, and an astronaut riding a motorcycle in space. The figure visually demonstrates the differences in the resulting videos concerning subject consistency, background consistency, motion smoothness, and overall visual quality. This allows for an assessment of each method's ability to maintain coherent content across multiple scenes within a longer video sequence. ", "section": "5 Experiments"}]