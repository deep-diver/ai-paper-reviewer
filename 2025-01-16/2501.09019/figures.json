[{"figure_path": "https://arxiv.org/html/2501.09019/x1.png", "caption": "Figure 1: Illustration of FIFO-Diffusion\u00a0(Kim et\u00a0al. 2024) (top) and our Ouroboros-Diffusion (bottom) for tuning-free long video generation.", "description": "This figure compares the FIFO-Diffusion and Ouroboros-Diffusion methods for generating long videos without fine-tuning.  FIFO-Diffusion uses a queue of video frames with increasing noise levels, adding new noisy frames at the tail and removing clean frames from the head.  This method, however, can suffer from inconsistencies due to independent noise introduction and lack of long-range temporal coherence. Ouroboros-Diffusion improves upon FIFO-Diffusion by introducing techniques to improve both structural and content consistency.  The bottom panel shows Ouroboros-Diffusion's enhanced approach to content consistency, including coherent noise sampling at the queue's tail and cross-frame attention across short segments to maintain visual coherence, using the past cleaner frames to guide the denoising of future, noisier frames.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.09019/x2.png", "caption": "Figure 2: \nAn overview of our Ouroboros-Diffusion. The whole framework (a) contains three key components: coherent tail latent sampling in queue manager , (b) Subject-Aware Cross-frame Attention (SACFA), and (c) self-recurrent guidance.\nThe coherent tail latent sampling in queue manager derives the enqueued frame latents at the queue tail to improve structural consistency.\nThe Subject-Aware Cross-frame Attention (SACFA) aligns subjects across frames within short segments for better visual coherence.\nThe self-recurrent guidance leverages information from all historical cleaner frames to guide the denoising of noisier frames, fostering rich and contextual global information interaction.", "description": "Ouroboros-Diffusion is composed of three key components working together to improve the consistency of generated long videos.  (a) shows the Queue Manager which uses coherent tail latent sampling to create smooth transitions between frames by considering the low-frequency components of the previous frame. (b) illustrates the Subject-Aware Cross-Frame Attention (SACFA) mechanism that improves subject consistency by enhancing visual coherence between frames within short segments. It does this by aligning subjects across these frames. (c) displays the self-recursive guidance that enhances global content consistency using information from previously generated clean frames to guide the denoising of the noisier frames currently being processed.", "section": "4 Methodology"}, {"figure_path": "https://arxiv.org/html/2501.09019/x3.png", "caption": "Figure 3: The detailed illustration of coherent tail latent sampling in the queue manager.", "description": "This figure details the process of coherent tail latent sampling within the queue manager of the Ouroboros-Diffusion model.  It shows how, instead of using purely random Gaussian noise, the model leverages the low-frequency components of the second-to-last frame's latent to construct the new tail latent. This ensures structural similarity between consecutive frames, promoting smooth visual transitions while still allowing for dynamic changes. The process involves applying a low-pass filter to the second-to-last frame's latent, preserving its structure, and combining it with the high-frequency components of random Gaussian noise.", "section": "4.2 Coherent Tail Latent Sampling"}, {"figure_path": "https://arxiv.org/html/2501.09019/x4.png", "caption": "Figure 4: Visual examples of single-scene long video generation by different approaches. The text prompt is \u201cA cat wearing sunglasses and working as a lifeguard at a pool.\u201d", "description": "Figure 4 presents a comparison of single-scene long video generation results from different methods using the prompt: \"A cat wearing sunglasses and working as a lifeguard at a pool.\"  Each approach's output is shown as a sequence of frames illustrating the video's progression. This allows for a visual assessment of each model's ability to generate temporally consistent and visually coherent long-form videos. Differences in subject consistency, background stability, motion smoothness, and overall visual quality are readily apparent.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.09019/x5.png", "caption": "Figure 5: Visual examples of multi-scene long video generation by different approaches. The multi-scene prompts are: 1). an astronaut is riding a horse in space; 2). an astronaut is riding a dragon in space; 3). an astronaut is riding a motorcycle in space.", "description": "Figure 5 presents a comparison of multi-scene long video generation results from four different methods: Ouroboros-Diffusion, FreeNoise, and FIFO-Diffusion.  Each method was tasked with generating a video based on three sequential prompts: an astronaut riding a horse in space, an astronaut riding a dragon in space, and an astronaut riding a motorcycle in space. The figure visually demonstrates the differences in subject and background consistency, motion smoothness, and overall visual quality between the generated videos from each approach. The differences in the quality of generation highlight the impact of different algorithms.", "section": "5. Experiments"}]