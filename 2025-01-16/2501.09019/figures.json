[{"figure_path": "https://arxiv.org/html/2501.09019/x1.png", "caption": "Figure 1: Illustration of FIFO-Diffusion\u00a0(Kim et\u00a0al. 2024) (top) and our Ouroboros-Diffusion (bottom) for tuning-free long video generation.", "description": "This figure illustrates the difference between FIFO-Diffusion and the proposed Ouroboros-Diffusion methods for generating long videos without fine-tuning.  FIFO-Diffusion (top) uses a queue where frames are progressively noisier towards the tail and cleaner towards the head.  New noisy frames are added to the tail, and clean frames are removed from the head. However, this approach struggles with long-range temporal consistency. Ouroboros-Diffusion (bottom) improves upon this by introducing new techniques to enhance both structural and content consistency, resulting in smoother transitions and more consistent content across the video.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.09019/x2.png", "caption": "Figure 2: \nAn overview of our Ouroboros-Diffusion. The whole framework (a) contains three key components: coherent tail latent sampling in queue manager , (b) Subject-Aware Cross-frame Attention (SACFA), and (c) self-recurrent guidance.\nThe coherent tail latent sampling in queue manager derives the enqueued frame latents at the queue tail to improve structural consistency.\nThe Subject-Aware Cross-frame Attention (SACFA) aligns subjects across frames within short segments for better visual coherence.\nThe self-recurrent guidance leverages information from all historical cleaner frames to guide the denoising of noisier frames, fostering rich and contextual global information interaction.", "description": "Ouroboros-Diffusion is composed of three key components: (a) a queue manager that utilizes coherent tail latent sampling to improve structural consistency by generating the next frame using low-frequency components of the previous frame; (b) Subject-Aware Cross-Frame Attention (SACFA) to improve subject consistency by aligning subjects across short video segments; and (c) self-recurrent guidance that uses previously generated frames to guide the denoising of noisier frames and improve global contextual information.", "section": "4 Methodology"}, {"figure_path": "https://arxiv.org/html/2501.09019/x3.png", "caption": "Figure 3: The detailed illustration of coherent tail latent sampling in the queue manager.", "description": "This figure details the process of \"coherent tail latent sampling\" within the Ouroboros-Diffusion framework.  Instead of simply adding random noise to the end of the queue (as in FIFO-Diffusion), Ouroboros-Diffusion uses the low-frequency components of the second-to-last frame in the queue to create the new tail frame.  This ensures that the new tail frame maintains structural similarity with the preceding frame, leading to smoother transitions and improved visual coherence in the generated video. The figure shows how the low-frequency component is extracted from the second-to-last frame using a low-pass filter, combined with high-frequency random noise, and then enqueued at the tail of the queue. This process is crucial for ensuring structural consistency in long video generation.", "section": "4.2 Coherent Tail Latent Sampling"}, {"figure_path": "https://arxiv.org/html/2501.09019/x4.png", "caption": "Figure 4: Visual examples of single-scene long video generation by different approaches. The text prompt is \u201cA cat wearing sunglasses and working as a lifeguard at a pool.\u201d", "description": "This figure displays visual comparisons of single-scene long video generation results from several different approaches, all using the same text prompt: \u201cA cat wearing sunglasses and working as a lifeguard at a pool.\u201d  The purpose is to showcase the relative strengths and weaknesses of each method in terms of visual quality, consistency of subject and background, motion smoothness, temporal coherence, and the presence of flickering artifacts. By comparing the generated videos across different frames, the reader can assess the effectiveness of each model in producing visually compelling and realistic long videos.", "section": "5. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.09019/x5.png", "caption": "Figure 5: Visual examples of multi-scene long video generation by different approaches. The multi-scene prompts are: 1). an astronaut is riding a horse in space; 2). an astronaut is riding a dragon in space; 3). an astronaut is riding a motorcycle in space.", "description": "Figure 5 presents a comparison of multi-scene long video generation results from four different approaches: Ouroboros-Diffusion, FreeNoise, and FIFO-Diffusion.  Each method was tasked with generating a video from three sequential prompts: an astronaut riding a horse in space, an astronaut riding a dragon in space, and an astronaut riding a motorcycle in space. The figure visually demonstrates the differences in video quality, consistency of the subject (astronaut) and background, and the smoothness of the transitions between the scenes. The generated videos highlight the strengths and weaknesses of each method in maintaining coherent narratives and consistent visuals across multiple scenes, showcasing the superiority of Ouroboros-Diffusion in maintaining visual coherence and smoothness.", "section": "5. Experiments"}]