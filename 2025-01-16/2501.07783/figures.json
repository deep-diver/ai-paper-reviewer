[{"figure_path": "https://arxiv.org/html/2501.07783/x1.png", "caption": "Figure 1: Different multi-resolution designs in visual perception and multimodal understanding.\n(a)(e) Plain network without multi-scale features.\n(b)(c)(f) Inefficient image pyramid networks using equivalently large models for all scales, either with shared weights or with separate weights and interactions.\n(d) Parameter-direct image pyramid network which processes high-resolution images with large models, leading to high computational cost.\n(g) Multi-resolution approaches on multimodal tasks based on grid partition.\n(h) Our efficient and effective parameter-inverted image pyramid network (PIIP), which pairs models of increasing parameter sizes inversely with images of decreasing resolution. It achieves better performance with much lower computational cost.", "description": "Figure 1 illustrates various multi-resolution approaches used in visual perception and multimodal understanding tasks, highlighting their efficiency and effectiveness.  (a) and (e) depict 'plain' networks lacking multi-scale feature processing. (b), (c), and (f) show traditional image pyramid methods that use the same large model across all scales, leading to high computational costs.  (d) presents a 'parameter-direct' approach using large models for high-resolution images, again inefficient. (g) shows a grid-partition method for high-resolution images common in multimodal tasks. Finally, (h) introduces the authors' proposed Parameter-Inverted Image Pyramid Network (PIIP), which employs smaller models for high-resolution images and larger models for lower-resolution images, significantly improving efficiency while maintaining performance. ", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.07783/x2.png", "caption": "Figure 2: Overall architecture of PIIP. We use multi-resolution branches to process images of different resolutions, where larger images are handled by smaller models. Each branch leverages pretrained ViTs or CNNs. Interaction units build connections between adjacent branches. Branch merging is inserted after all the blocks or within certain intermediate blocks to combine the features of all branches.", "description": "The figure illustrates the architecture of Parameter-Inverted Image Pyramid Networks (PIIP).  PIIP uses multiple branches to process images at different resolutions.  Larger images are processed by smaller model branches to improve computational efficiency. Each branch uses a pre-trained Vision Transformer (ViT) or Convolutional Neural Network (CNN). Interaction units connect adjacent branches, allowing features from different scales to be integrated. Finally, branch merging combines features across all branches, either at the end or at intermediate points within the network.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/x3.png", "caption": "Figure 3: Illustration of PIIP-LLaVA for multimodal understanding.\nWe use one projector after each branch to align the visual features with the language embedding space of the LLM, and combine the features to obtain the visual features.", "description": "PIIP-LLaVA, a multimodal large language model, uses a parameter-inverted image pyramid network to efficiently process images at multiple resolutions. Each resolution level is fed into a separate branch, initialized with a pretrained vision model (ViT or CNN). A projector aligns the visual features from each branch to the language embedding space of the LLM. Finally, a feature fusion module combines these aligned features to produce comprehensive visual representations for multimodal tasks.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/x4.png", "caption": "Figure 4: Detailed structure of the interaction unit. It consists of two deformable attentions with fully-connect layers and feed-forward networks.", "description": "This figure details the architecture of the interaction unit used within the Parameter-Inverted Image Pyramid Network (PIIP).  The interaction unit facilitates the fusion of features from different resolution branches of the network.  It achieves this fusion through two deformable attention mechanisms. Each deformable attention mechanism is preceded by a fully connected (FC) layer that projects the features to the correct dimension.  Following each deformable attention, a feed-forward network (FFN) further refines the fused features.  This design allows effective integration of multi-scale feature information with relatively low computational cost.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/x5.png", "caption": "Figure 5: Detailed design of branch merging in different tasks. For detection, segmentation and multimodal understanding, output features from all branches are fused together with projection and upsampling, and fed into the subsequent FPN or LLM. For classification, we employ the original classification heads to compute logits, and average them as the final prediction.", "description": "This figure details how the model combines features from different branches of the image pyramid for various tasks.  For object detection, segmentation, and multimodal understanding, features from each branch undergo projection and upsampling to match dimensions. Then, they are fused and passed to either a Feature Pyramid Network (FPN) for detection and segmentation or a Large Language Model (LLM) for multimodal understanding. In contrast, for image classification, the model uses the original classification heads from each branch to produce logits. These logits are then averaged to get the final prediction.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/interaction_types/inter_type_v4.png", "caption": "(a) Object detection", "description": "This figure shows qualitative results of object detection using the proposed Parameter-Inverted Image Pyramid Network (PIIP).  The images showcase the model's ability to accurately detect objects of varying sizes, including small objects that are often missed by traditional methods. The bounding boxes accurately outline the detected objects, demonstrating the precision of the PIIP model.  The results highlight the method's improved capabilities, especially for discerning smaller objects within complex scenes. ", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.07783/x6.png", "caption": "(b) Instance segmentation", "description": "This figure shows the qualitative results of instance segmentation using the proposed PIIP method. It showcases the model's ability to accurately segment various objects in diverse scenes, including a city street with cars and people, a savannah landscape with animals, and indoor scenes with furniture and household items.  The detailed segmentation masks highlight the precision and effectiveness of the model across different object scales and complexities.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.07783/x7.png", "caption": "Figure 6: Performance of different PIIP variants by adjusting input resolutions on object detection and instance segmentation.", "description": "This figure displays the performance of various PIIP (Parameter-Inverted Image Pyramid) network configurations on object detection and instance segmentation tasks.  Different PIIP variants are compared, each using different input image resolutions. The x-axis represents the computational cost (GFLOPS), and the y-axis shows the performance (AP or mIoU).  The graph demonstrates the impact of varying input image resolution (and the corresponding model size used to process that resolution) on overall performance and computational cost. The different lines represent different PIIP versions with varying numbers of branches and models of different scales. The figure helps to analyze the trade-off between computational efficiency and accuracy achieved by the PIIP architecture. ", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.07783/x8.png", "caption": "TABLE V: Experiments of initializing with different pre-trained weights on COCO val2017 with PIIP-SBL 1568/1120/672.", "description": "Table V presents the results of experiments conducted to assess the impact of using various pre-trained weights on the performance of the PIIP-SBL model. The experiments were performed on the COCO val2017 dataset, utilizing the PIIP-SBL model with a specific resolution of 1568/1120/672.  Different pre-trained weights were used to initialize the model, and the resulting performance in terms of APb and APm is reported. This table helps to understand how the choice of pre-trained weights affects the overall performance of the model, providing valuable insights for model selection and optimization.", "section": "IV. Experiments"}]