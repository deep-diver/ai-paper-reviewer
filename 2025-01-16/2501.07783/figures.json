[{"figure_path": "https://arxiv.org/html/2501.07783/x1.png", "caption": "Figure 1: Different multi-resolution designs in visual perception and multimodal understanding.\n(a)(e) Plain network without multi-scale features.\n(b)(c)(f) Inefficient image pyramid networks using equivalently large models for all scales, either with shared weights or with separate weights and interactions.\n(d) Parameter-direct image pyramid network which processes high-resolution images with large models, leading to high computational cost.\n(g) Multi-resolution approaches on multimodal tasks based on grid partition.\n(h) Our efficient and effective parameter-inverted image pyramid network (PIIP), which pairs models of increasing parameter sizes inversely with images of decreasing resolution. It achieves better performance with much lower computational cost.", "description": "Figure 1 illustrates various approaches to handling multi-resolution images in visual perception and multimodal understanding tasks.  (a) and (e) show a baseline 'plain' network without any multi-scale processing. (b), (c), and (f) depict traditional image pyramid methods, where the same large model processes images at multiple scales, leading to high computational costs.  This is inefficient, whether using a single, shared model or multiple, separate models interacting with each other.  (d) illustrates a 'parameter-direct' approach that uses large models for high-resolution images only, and also suffers from high computational costs. (g) presents a multi-resolution method specific to multimodal tasks that partitions high-resolution images into grids before processing. Finally, (h) shows the proposed Parameter-Inverted Image Pyramid Network (PIIP), which uses smaller models for higher-resolution images and larger models for lower-resolution images, achieving a more efficient balance between performance and computational cost.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.07783/x2.png", "caption": "Figure 2: Overall architecture of PIIP. We use multi-resolution branches to process images of different resolutions, where larger images are handled by smaller models. Each branch leverages pretrained ViTs or CNNs. Interaction units build connections between adjacent branches. Branch merging is inserted after all the blocks or within certain intermediate blocks to combine the features of all branches.", "description": "The figure illustrates the architecture of Parameter-Inverted Image Pyramid Networks (PIIP).  PIIP uses multiple branches to process images at different resolutions.  Higher-resolution images are processed by smaller models to balance computational cost and performance. Each branch employs a pre-trained Vision Transformer (ViT) or Convolutional Neural Network (CNN).  Interaction units connect adjacent branches to allow information flow between different resolutions.  Finally, a branch merging module combines features from all branches at the end or at intermediate stages to produce the final output.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/x3.png", "caption": "Figure 3: Illustration of PIIP-LLaVA for multimodal understanding.\nWe use one projector after each branch to align the visual features with the language embedding space of the LLM, and combine the features to obtain the visual features.", "description": "The figure illustrates the architecture of PIIP-LLaVA, a multimodal large language model.  It shows how PIIP's multi-resolution branches process images at different scales. Each branch uses a pretrained vision model (ViT or CNN) to extract visual features.  A projector then maps the visual features from each branch into the language embedding space of the LLM. Finally, these aligned features are combined to create a unified visual representation for multimodal understanding tasks.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/x4.png", "caption": "Figure 4: Detailed structure of the interaction unit. It consists of two deformable attentions with fully-connect layers and feed-forward networks.", "description": "This figure details the architecture of the interaction unit within the Parameter-Inverted Image Pyramid Network (PIIP).  The interaction unit is crucial for integrating features from different resolution branches. As illustrated, it employs two deformable attention mechanisms. Each mechanism involves a fully connected layer (FC) to project the features, followed by the deformable attention itself.  After each deformable attention, a feed-forward network (FFN) with a hidden dimension ratio of 0.25 further refines and fuses the features before passing them to the next layer. This design ensures efficient and effective feature interaction between branches.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/x5.png", "caption": "Figure 5: Detailed design of branch merging in different tasks. For detection, segmentation and multimodal understanding, output features from all branches are fused together with projection and upsampling, and fed into the subsequent FPN or LLM. For classification, we employ the original classification heads to compute logits, and average them as the final prediction.", "description": "Figure 5 illustrates how the output features from multiple branches of the Parameter-Inverted Image Pyramid Network (PIIP) are combined for different downstream tasks.  For object detection and segmentation, and also for multimodal understanding tasks, a projection and upsampling step is applied to the feature maps from each branch. This ensures they are all the same spatial dimensions before being fused together. The combined features are then fed into either a Feature Pyramid Network (FPN) or a large language model (LLM). In contrast, for image classification tasks, the original classification heads are used for each branch individually. Finally, the individual classification results are averaged to obtain the final classification prediction. This figure showcases the flexibility of PIIP's multi-branch architecture in adapting to various computer vision tasks.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/interaction_types/inter_type_v4.png", "caption": "(a) Object detection", "description": "This figure shows qualitative results of object detection on various images.  The images span diverse scenes, including landscapes, city streets, and indoor settings.  Bounding boxes are overlaid on the images to indicate the objects detected by the model. The visual demonstrates the model's ability to accurately identify and localize objects across varied scales and complexities. High-resolution image processing capabilities enable accurate detection of small objects which would be missed by other models.", "section": "IV. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.07783/x6.png", "caption": "(b) Instance segmentation", "description": "This figure shows the results of instance segmentation on several images.  Instance segmentation is a computer vision task that goes beyond simply identifying objects within an image; it precisely outlines the boundaries of each object detected.  The images demonstrate the model's ability to accurately segment various objects, even those that are small or partially occluded, highlighting the model's high-resolution processing capabilities.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.07783/x7.png", "caption": "Figure 6: Performance of different PIIP variants by adjusting input resolutions on object detection and instance segmentation.", "description": "This figure shows the performance of different PIIP (Parameter-Inverted Image Pyramid) network variants on object detection and instance segmentation tasks.  The x-axis represents the GFLOPs (floating point operations), which indicates the computational cost. The y-axis shows the performance in terms of Average Precision (AP) for both box detection (APb) and mask-based segmentation (APm). Different colored lines represent different PIIP variants, indicating how performance changes with increasing computational cost for each model. Each variant is configured with varying input image resolution.  The plot demonstrates the performance trade-off and scaling behavior of PIIP architectures under different computational budgets.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.07783/x8.png", "caption": "TABLE V: Experiments of initializing with different pre-trained weights on COCO val2017 with PIIP-SBL 1568/1120/672.", "description": "Table V presents an ablation study on the impact of different pre-trained weights on the performance of the PIIP-SBL model.  The PIIP-SBL model, a parameter-inverted image pyramid network with a specific resolution configuration (1568/1120/672), was evaluated on the COCO val2017 dataset for object detection.  The table compares the performance (measured as APb and APm) achieved when initializing the model using various pre-trained weights from different vision transformer models (ViT). This experiment helps to determine the impact of the initial weights of the network branches on final performance. The results demonstrate the effectiveness of specific pre-trained weights for improving the model's performance.", "section": "IV. Experiments"}]