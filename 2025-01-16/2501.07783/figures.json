[{"figure_path": "https://arxiv.org/html/2501.07783/x1.png", "caption": "Figure 1: Different multi-resolution designs in visual perception and multimodal understanding.\n(a)(e) Plain network without multi-scale features.\n(b)(c)(f) Inefficient image pyramid networks using equivalently large models for all scales, either with shared weights or with separate weights and interactions.\n(d) Parameter-direct image pyramid network which processes high-resolution images with large models, leading to high computational cost.\n(g) Multi-resolution approaches on multimodal tasks based on grid partition.\n(h) Our efficient and effective parameter-inverted image pyramid network (PIIP), which pairs models of increasing parameter sizes inversely with images of decreasing resolution. It achieves better performance with much lower computational cost.", "description": "Figure 1 illustrates various approaches to handling multi-resolution images in visual perception and multimodal understanding tasks.  (a) and (e) show a basic network without any multi-scale feature processing. (b), (c), and (f) depict traditional image pyramids where a single, large model processes images at all scales \u2013 a computationally expensive method.  (d) showcases a parameter-direct approach, using large models for high-resolution images which, while effective, also demands significant resources.  (g) presents a multi-resolution strategy used in multimodal tasks, partitioning high-resolution images into smaller sections for processing. Finally, (h) introduces the authors' proposed Parameter-Inverted Image Pyramid Network (PIIP), a novel design that employs smaller models for higher-resolution images and larger models for lower-resolution images; achieving a balance between computational cost and performance.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.07783/x2.png", "caption": "Figure 2: Overall architecture of PIIP. We use multi-resolution branches to process images of different resolutions, where larger images are handled by smaller models. Each branch leverages pretrained ViTs or CNNs. Interaction units build connections between adjacent branches. Branch merging is inserted after all the blocks or within certain intermediate blocks to combine the features of all branches.", "description": "Figure 2 illustrates the architecture of Parameter-Inverted Image Pyramid Networks (PIIP).  It uses multiple branches to process images at different resolutions, with higher-resolution images processed by smaller models to optimize computational efficiency. Each branch employs a pre-trained Vision Transformer (ViT) or Convolutional Neural Network (CNN).  Interaction units connect adjacent branches to integrate information across scales.  Finally, a branch merging component combines the features from all branches, either after all processing blocks or at intermediate stages, to produce the final output. This design balances computational cost and performance.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/x3.png", "caption": "Figure 3: Illustration of PIIP-LLaVA for multimodal understanding.\nWe use one projector after each branch to align the visual features with the language embedding space of the LLM, and combine the features to obtain the visual features.", "description": "The figure illustrates the architecture of PIIP-LLaVA, a multimodal large language model designed for efficient high-resolution image understanding.  It utilizes a parameter-inverted image pyramid, where smaller models process high-resolution images and larger models process lower-resolution images.  Each branch of the pyramid (processing a different resolution) feeds into a projector that aligns its visual feature representation with the language embedding space of the underlying Large Language Model (LLM). These projected features are then combined to generate a comprehensive visual representation fed into the LLM for multimodal understanding. This design is more efficient than using a single large model for all resolutions.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/x4.png", "caption": "Figure 4: Detailed structure of the interaction unit. It consists of two deformable attentions with fully-connect layers and feed-forward networks.", "description": "This figure details the architecture of the interaction unit used in the Parameter-Inverted Image Pyramid Networks (PIIP).  The interaction unit facilitates communication between features extracted from different resolution branches of the network. It uses two deformable attention mechanisms. Each deformable attention mechanism consists of a fully connected layer to project features, the deformable attention itself, and a feedforward network for channel-wise fusion. This design allows for the effective integration of multi-scale features, leveraging the complementary information present in different resolution levels for improved visual understanding.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/x5.png", "caption": "Figure 5: Detailed design of branch merging in different tasks. For detection, segmentation and multimodal understanding, output features from all branches are fused together with projection and upsampling, and fed into the subsequent FPN or LLM. For classification, we employ the original classification heads to compute logits, and average them as the final prediction.", "description": "Figure 5 illustrates how the output features from multiple branches of the Parameter-Inverted Image Pyramid Networks (PIIP) are integrated for various downstream tasks.  For object detection, instance segmentation, and multimodal understanding, a branch merging module is used. This module first projects the features from each branch to a consistent feature dimension, then upsamples them to the same spatial resolution, and finally combines them using learnable weights. The combined features are then fed into either a Feature Pyramid Network (FPN) for detection and segmentation or a Large Language Model (LLM) for multimodal understanding. In contrast, for image classification, the original classification heads of each branch are used independently to compute classification logits.  These logits are then averaged to produce the final classification result.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/interaction_types/inter_type_v4.png", "caption": "(a) Object detection", "description": "This figure shows qualitative results of object detection on several example images.  The model successfully identifies and localizes various objects across different scenes, including the Tokyo Tower, cars, and animals in various settings. Note the model's capability to detect both large and small objects.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.07783/x6.png", "caption": "(b) Instance segmentation", "description": "The figure displays qualitative results of instance segmentation from the PIIP-SBL model.  It presents three example images, each showing the model's ability to precisely segment various objects within complex scenes, highlighting the model's accuracy in identifying object boundaries, even for smaller and more intricate objects.", "section": "IV. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.07783/x7.png", "caption": "Figure 6: Performance of different PIIP variants by adjusting input resolutions on object detection and instance segmentation.", "description": "This figure displays the performance of various PIIP (Parameter-Inverted Image Pyramid) network configurations in object detection and instance segmentation tasks. Different PIIP variants are compared, each utilizing a distinct combination of input image resolutions and model sizes.  The x-axis represents the computational cost (GFLOPS), and the y-axis shows the performance (APb for object detection and APm for instance segmentation). The plot visualizes how the balance between computational cost and performance varies with changes in the input image resolution and the selection of PIIP variant. Each point on the graph corresponds to a different PIIP configuration, allowing for a direct comparison of their efficiency and efficacy across the two tasks.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.07783/x8.png", "caption": "TABLE V: Experiments of initializing with different pre-trained weights on COCO val2017 with PIIP-SBL 1568/1120/672.", "description": "Table V presents the results of an experiment evaluating the effect of using different pre-trained weights on the performance of a Parameter-Inverted Image Pyramid Network (PIIP) model specifically designed for object detection. The model used in this experiment was PIIP-SBL with a resolution of 1568/1120/672.  Different pre-trained weights from various vision transformer models (ViTs) were used to initialize the PIIP-SBL model, and the resulting performance, in terms of Average Precision (AP) for both bounding boxes (APb) and masks (APm) on the COCO val2017 dataset, is shown in the table.  The table allows a comparison of model performance across different pre-training strategies, showing how the choice of pre-trained weights affects the final object detection results on this benchmark.", "section": "IV. Experiments"}]