[{"figure_path": "https://arxiv.org/html/2501.07783/x1.png", "caption": "Figure 1: Different multi-resolution designs in visual perception and multimodal understanding.\n(a)(e) Plain network without multi-scale features.\n(b)(c)(f) Inefficient image pyramid networks using equivalently large models for all scales, either with shared weights or with separate weights and interactions.\n(d) Parameter-direct image pyramid network which processes high-resolution images with large models, leading to high computational cost.\n(g) Multi-resolution approaches on multimodal tasks based on grid partition.\n(h) Our efficient and effective parameter-inverted image pyramid network (PIIP), which pairs models of increasing parameter sizes inversely with images of decreasing resolution. It achieves better performance with much lower computational cost.", "description": "Figure 1 illustrates various multi-resolution approaches used in visual perception and multimodal understanding tasks.  (a) and (e) show baseline network architectures without multi-scale features.  (b), (c), and (f) depict traditional image pyramid methods, which use the same large model at all resolutions, leading to high computational costs. This can be done by sharing parameters across scales (b) or with separate parameterizations for different scales (c,f). (d) shows a parameter-direct approach, where high-resolution images are processed by large models, further increasing computational burden. (g) presents a multi-resolution strategy for multimodal tasks that divides the image into grids before processing. Finally, (h) introduces the proposed Parameter-Inverted Image Pyramid Network (PIIP), an efficient method that utilizes smaller models for higher resolution images and larger models for lower resolution images, resulting in improved performance with lower computational costs.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.07783/x2.png", "caption": "Figure 2: Overall architecture of PIIP. We use multi-resolution branches to process images of different resolutions, where larger images are handled by smaller models. Each branch leverages pretrained ViTs or CNNs. Interaction units build connections between adjacent branches. Branch merging is inserted after all the blocks or within certain intermediate blocks to combine the features of all branches.", "description": "The figure illustrates the architecture of Parameter-Inverted Image Pyramid Networks (PIIP).  PIIP uses multiple branches to process images at different resolutions.  Higher-resolution images are handled by smaller model branches, while lower-resolution images are processed by larger branches, balancing computational cost and performance. Each branch employs pre-trained Vision Transformers (ViTs) or Convolutional Neural Networks (CNNs).  Interaction units connect adjacent branches to integrate information across scales.  Finally, a branch merging mechanism combines features from all branches, either at the end of the network or at intermediate stages.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/x3.png", "caption": "Figure 3: Illustration of PIIP-LLaVA for multimodal understanding.\nWe use one projector after each branch to align the visual features with the language embedding space of the LLM, and combine the features to obtain the visual features.", "description": "PIIP-LLaVA, a multimodal large language model, uses a parameter-inverted image pyramid network (PIIP) for efficient and effective high-resolution understanding.  The figure illustrates how PIIP-LLaVA processes images.  Multiple branches, each with a different-sized pretrained vision model, process the image at different resolutions (high-resolution branches use smaller models, low-resolution branches use larger models).  After each branch processes the image, a projector aligns its visual features with the language model's embedding space. These aligned features are then combined to create a unified visual representation that is fed into the language model for multimodal understanding tasks. The parameter-inverted structure balances computational cost and performance. This process efficiently utilizes visual models of varying capacities to extract rich multi-scale features.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/x4.png", "caption": "Figure 4: Detailed structure of the interaction unit. It consists of two deformable attentions with fully-connect layers and feed-forward networks.", "description": "This figure details the architecture of the interaction unit, a key component within the Parameter-Inverted Image Pyramid Network (PIIP).  The interaction unit facilitates the fusion of features from different resolution branches of the network. It achieves this through two deformable attention mechanisms. Each deformable attention module processes features from adjacent branches to capture relationships between different spatial scales and semantic levels. Fully-connected layers project the features to compatible dimensions, and feed-forward networks further integrate the information from both attention modules before passing the refined features to the subsequent layers. This design allows for effective information exchange between branches, enhancing the overall network's multi-scale understanding capability.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/x5.png", "caption": "Figure 5: Detailed design of branch merging in different tasks. For detection, segmentation and multimodal understanding, output features from all branches are fused together with projection and upsampling, and fed into the subsequent FPN or LLM. For classification, we employ the original classification heads to compute logits, and average them as the final prediction.", "description": "Figure 5 illustrates how the output features from each branch of the Parameter-Inverted Image Pyramid Network (PIIP) are combined for different downstream tasks.  For object detection and segmentation tasks, features from all branches undergo a projection to a unified feature dimension and then upsampling to match the spatial resolution of the highest-resolution branch.  These unified features are then fed into a Feature Pyramid Network (FPN) for further processing. In contrast, for multimodal understanding, the projected and upsampled features are passed to a Large Language Model (LLM) for final prediction.  Finally, for the image classification task, the original classification heads of the pre-trained models are used, one for each branch, and the final prediction is the average of logits from all branches.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/interaction_types/inter_type_v4.png", "caption": "(a) Object detection", "description": "This figure shows qualitative results of object detection.  The images demonstrate the model's ability to detect objects of varying sizes and in complex scenes. High-resolution processing allows for accurate detection even of small objects.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.07783/x6.png", "caption": "(b) Instance segmentation", "description": "This figure shows qualitative results of instance segmentation on various images.  The model successfully identifies and segments different objects within complex scenes, including cars, buildings, furniture, and plants.  The segmentation masks accurately outline the boundaries of each object, demonstrating the model's ability to perform fine-grained visual perception.", "section": "IV. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.07783/x7.png", "caption": "Figure 6: Performance of different PIIP variants by adjusting input resolutions on object detection and instance segmentation.", "description": "This figure displays the performance of various PIIP (Parameter-Inverted Image Pyramid) network configurations on object detection and instance segmentation tasks.  Different lines represent different PIIP variants, each tested across a range of input image resolutions. The x-axis shows the GFLOPS (floating point operations per second), a measure of computational cost, and the y-axis presents the performance metric (either Average Precision for bounding boxes (APb) or Average Precision for masks (APm)).  The plot illustrates how the model's performance and computational cost change as the input resolution varies for each PIIP variant. This allows one to assess the trade-off between performance and computational resources across different models and resolutions.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.07783/x8.png", "caption": "TABLE V: Experiments of initializing with different pre-trained weights on COCO val2017 with PIIP-SBL 1568/1120/672.", "description": "Table V presents the results of an experiment evaluating the performance of the Parameter-Inverted Image Pyramid Network (PIIP) on the COCO val2017 dataset.  Specifically, it shows how the choice of pre-trained weights for the PIIP-SBL model (a variant of PIIP with a specific configuration of 1568/1120/672 resolution) affects performance. Different pre-trained ViT weights (ViT-S and ViT-L)  from various sources (AugReg, DeiT III, MAE, Uni-Perceiver, DINOv2, and BEiTv2) were used to initialize the model. The table displays the resulting Average Precision (AP) scores for both bounding boxes (APb) and masks (APm) to compare the effectiveness of different pre-trained weights in the context of the PIIP architecture.", "section": "IV. Experiments"}]