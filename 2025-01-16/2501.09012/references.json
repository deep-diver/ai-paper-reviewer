{"references": [{"fullname_first_author": "Yingying Deng", "paper_title": "StyTr2: Image style transfer with transformers", "publication_date": "2022-XX-XX", "reason": "This paper introduces a novel transformer-based approach to image style transfer, which is directly relevant to the core theme of the main paper and is cited multiple times, suggesting its significant influence."}, {"fullname_first_author": "Nisha Huang", "paper_title": "DiffStyler: Controllable dual diffusion for text-driven image stylization", "publication_date": "2024-XX-XX", "reason": "This paper proposes a method for text-driven image stylization using dual diffusion models, providing a strong comparative benchmark for the proposed MLLM-based approach in the main paper."}, {"fullname_first_author": "Christoph Schuhmann", "paper_title": "LAION-5B: An open large-scale dataset for training next generation image-text models", "publication_date": "2022-XX-XX", "reason": "This paper introduces a large-scale dataset crucial for training and evaluating the multimodal LLMs used in the main paper's experiments, highlighting its importance as a foundational dataset."}, {"fullname_first_author": "Ruixiang Jiang", "paper_title": "Artist: Aesthetically controllable text-driven stylization without training", "publication_date": "2024-XX-XX", "reason": "This paper presents a novel text-driven stylization method that does not require training, serving as a strong comparative method to the MLLM-based method in the main paper."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-XX-XX", "reason": "This paper is foundational for multimodal learning and is cited multiple times, highlighting its significant contribution to the field and its influence on the methods employed in the main paper."}]}