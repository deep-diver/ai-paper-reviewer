[{"figure_path": "https://arxiv.org/html/2501.08983/x1.png", "caption": "Figure 1: Overview of CityDreamer4D. 4D city generation is divided into static and dynamic scene generation, conditioned on \ud835\udc0b\ud835\udc0b\\mathbf{L}bold_L and \ud835\udc13tsubscript\ud835\udc13\ud835\udc61\\mathbf{T}_{t}bold_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, produced by Unbounded Layout Generator and Traffic Scenario Generator, respectively. City Background Generator uses \ud835\udc0b\ud835\udc0b\\mathbf{L}bold_L to create background images \ud835\udc08^Gsubscript^\ud835\udc08\ud835\udc3a\\mathbf{\\hat{I}}_{G}over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT for stuff like roads, vegetation, and the sky, while Building Instance Generator renders the buildings {\ud835\udc08^Bi}subscript^\ud835\udc08subscript\ud835\udc35\ud835\udc56\\{\\mathbf{\\hat{I}}_{B_{i}}\\}{ over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT } within the city. Using \ud835\udc13tsubscript\ud835\udc13\ud835\udc61\\mathbf{T}_{t}bold_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, Vehicle Instance Generator generates vehicles {\ud835\udc08^Vit}superscriptsubscript^\ud835\udc08subscript\ud835\udc49\ud835\udc56\ud835\udc61\\{\\mathbf{\\hat{I}}_{V_{i}}^{t}\\}{ over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT } at time step t\ud835\udc61titalic_t. Finally, Compositor combines the rendered background, buildings, and vehicles into a unified and coherent image \ud835\udc08^Ctsuperscriptsubscript^\ud835\udc08\ud835\udc36\ud835\udc61\\mathbf{\\hat{I}}_{C}^{t}over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. \u201cGen.\u201d, \u201cMod.\u201c, \u201cCond.\u201d, \u201cBG.\u201d, \u201cBLDG.\u201d, and \u201cVEH.\u201d denote \u201cGeneration\u201d, \u201cModulation\u201d, \u201cCondition\u201d, \u201cBackground\u201d, \u201cBuilding\u201d, and \u201cVehicle\u201d, respectively.", "description": "CityDreamer4D is a framework for generating unbounded 4D city scenes by separating static and dynamic components.  Static elements (roads, buildings, etc.) are generated using the Unbounded Layout Generator from a city layout \ud835\udc0b. Dynamic elements (vehicles) are generated using the Traffic Scenario Generator at each time step t, based on a traffic scenario \ud835\udc13\u209c.  Building Instance Generator creates building images, and Vehicle Instance Generator creates vehicle images.  The City Background Generator creates background images (sky, vegetation). Finally, the Compositor combines all generated elements into a single, coherent image \ud835\udc08^C\u209c.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08983/x2.png", "caption": "Figure 2: Overview of the OSM and GoogleEarth Datasets. (a) Examples of the 2D and 3D annotations in the GoogleEarth dataset, which can be automatically generated using the OSM dataset. (b) The automatic annotation pipeline can be readily adapted for worldwide cities. (c) The dataset statistics highlight the diverse perspectives in the GoogleEarth dataset.", "description": "Figure 2 provides a detailed look into the OSM and Google Earth datasets used in the CityDreamer4D research.  Part (a) showcases examples of the 2D and 3D annotations within the Google Earth dataset, emphasizing that these annotations can be automatically generated using data from the OSM dataset. This highlights the efficiency and scalability of the annotation process. Part (b) demonstrates the adaptability of the automated annotation pipeline, showing its potential for use with cities worldwide.  Finally, part (c) presents a statistical overview of the Google Earth dataset, illustrating the diversity of perspectives captured within this dataset (e.g., varying viewpoints, altitudes, and elevation angles).", "section": "4 DATASETS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x3.png", "caption": "Figure 3: Overview of the CityTopia Dataset. (a) The virtual city generation pipeline. \u201cPro.Inst.\u201d, \u201cSur.Spl\u201d, and \u201c3D Inst. Anno.\u201d denote \u201cPrototype Instantiation\u201d, \u201cSurface Sampling\u201d, and \u201c3D Instance Annotation\u201d, respectively. (b) Examples of 2D and 3D annotations in the CityTopia dataset are shown from both daytime and nighttime street-view and aerial-view perspectives, automatically generated during virtual city generation. (c) The dataset statistics highlight the diverse perspectives in both street and aerial views.", "description": "Figure 3 provides a comprehensive overview of the CityTopia dataset, a key contribution of the CityDreamer4D paper.  Panel (a) details the dataset's creation pipeline: starting with a prototype city instantiation, followed by surface sampling to generate 3D instance annotations, and finally, the generation of both daytime and nighttime virtual city images from street-view and aerial perspectives. Panel (b) showcases examples of the resulting 2D and 3D annotations, emphasizing the rich detail and diversity captured. Panel (c) presents statistical summaries of the dataset, illustrating the range of viewpoints (elevation and altitude) included, ensuring a diverse and comprehensive representation of city scenes.", "section": "4 DATASETS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x4.png", "caption": "Figure 4: Qualitative Comparison on Google Earth. For SceneDreamer\u00a0[7] and CityDreamer4D, vehicles are generated using models trained on CityTopia due to the lack of semantic annotations for vehicles in Google Earth. For DimensionX\u00a0[107], the initial frame is provided by CityDreamer4D. The visual results of InfiniCity\u00a0[26], provided by the authors, have been zoomed in for better viewing. \u201cPers.Nature\u201d stands for \u201cPersistentNature\u201d\u00a0[105].", "description": "This figure presents a qualitative comparison of different methods for generating 4D city scenes, specifically focusing on the Google Earth dataset.  Because the Google Earth dataset lacks semantic annotations for vehicles, SceneDreamer and CityDreamer4D used models trained on the CityTopia dataset for vehicle generation.  The initial frame for the DimensionX results was generated by CityDreamer4D.  The InfiniCity results (provided by the original authors) are zoomed in for better visualization.  'Pers.Nature' is an abbreviation for PersistentNature.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08983/x5.png", "caption": "Figure 5: Qualitative Comparison on CityTopia. The initial frame for DimensionX and the input frames for DreamScene4D are chosen from the dataset. \u201cPers.Nature\u201d refers to \u201cPersistentNature\u201d\u00a0[105].", "description": "This figure provides a qualitative comparison of CityDreamer4D with several other state-of-the-art methods on the CityTopia dataset.  The comparison showcases the visual results of generating 4D city scenes.  It highlights the differences in realism, detail, and consistency of the generated scenes between CityDreamer4D and the baselines (SGAM, PersistentNature, SceneDreamer, DreamScene4D, and DimensionX).  Note that the initial frame for DimensionX and input frames for DreamScene4D were taken from the CityTopia dataset itself.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08983/x6.png", "caption": "Figure 6: User Study on 4D City Generation. All scores are in the range of 5, with 5 indicating the best. \u201cPers.Nature\u201d refers to \u201cPersistentNature\u201d\u00a0[105].", "description": "This figure presents the results of a user study evaluating the quality of 4D city generation.  Participants rated different methods across three key aspects: perceptual quality, 4D realism, and view consistency. Each aspect was scored on a scale of 1 to 5, with 5 being the highest score. The results show a comparison of CityDreamer4D against several other state-of-the-art methods, including SGAM, PersistentNature, InfiniCity, SceneDreamer, DreamScene4D, and DimensionX.  The labels on the chart indicate the methods used for generation, with \"Pers.Nature\" as a shorthand for PersistentNature.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08983/x7.png", "caption": "Figure 7: Qualitative Comparison of City Layout Generators. The height map values are normalized to a range of [0,1]01[0,1][ 0 , 1 ] by dividing each value by the maximum value within the map.", "description": "Figure 7 presents a qualitative comparison of three different city layout generators: the proposed Unbounded Layout Generator (ULG), InfinityGAN, and IPSM.  The comparison focuses on the generated height maps, which are visualized. Before comparison, the height map values are normalized to a range of 0 to 1 for consistency.  The figure allows for a visual assessment of the quality and characteristics of the height maps produced by each method, aiding in understanding their relative strengths and weaknesses in generating realistic and detailed city layouts.", "section": "3.1 Unbounded Layout Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x8.png", "caption": "Figure 8: Qualitative Comparison of Building Instance Generator (BIG) Variants. (a) and (b) illustrate the effects of removing BIG and instance labels, respectively. (c)\u2013(f) present the results of various scene parameterizations. Note that \u201cEnc.\u201d is an abbreviation for \u201cEncoder\u201d.", "description": "This figure presents an ablation study on the Building Instance Generator (BIG) within the CityDreamer4D model.  The first two subfigures (a) and (b) show the impact of removing the BIG module entirely and the effect of removing instance labels from the input, respectively, highlighting the importance of these components for generating realistic building instances.  The remaining subfigures (c) through (f) explore the impact of different scene parameterization techniques (global encoder with hash grids, global encoder with sinusoidal positional encoding, local encoder with hash grids, and local encoder with sinusoidal positional encoding), demonstrating how the choice of parameterization affects the quality and detail of the generated buildings.", "section": "3.4 Building Instance Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x9.png", "caption": "Figure 9: Qualitative Comparison of Vehicle Instance Generator (VIG) Variants. (a) and (b) illustrate the effects of removing VIG and canonicalization, respectively. (c)\u2013(f) present the results of various scene parameterizations. Note that \u201cEnc.\u201d is an abbreviation for \u201cEncoder\u201d.", "description": "This figure compares different variations of the Vehicle Instance Generator (VIG) to showcase their impact on vehicle generation within the CityDreamer4D model.  (a) and (b) demonstrate the effects of completely removing the VIG module and removing the canonicalization process, respectively, highlighting the importance of these components.  The remaining images (c) through (f) illustrate the results obtained using various scene parameterization techniques, specifically examining the combination of global/local encoders with different types of positional encoding (Hash, SinCos). This allows for a comparison of how different architectural choices affect the quality and detail of the generated vehicles.", "section": "3.5 Vehical Instance Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x10.png", "caption": "Figure 10: Localized Editing on the Generated Cities. (a) and (c) show vehicle editing results, while (b) and (d) present building editing results.", "description": "Figure 10 showcases the capabilities of CityDreamer4D for localized editing within generated city scenes.  Panels (a) and (c) demonstrate modifications made to vehicles, illustrating changes in their appearance and placement.  Panels (b) and (d) display similar edits applied to buildings, showing alterations to building height and style. This figure highlights the model's ability to perform fine-grained adjustments to individual elements within the complex 4D city environment without affecting the overall coherence of the scene.", "section": "5. Applications"}, {"figure_path": "https://arxiv.org/html/2501.08983/x11.png", "caption": "Figure 11: Text-driven City Stylization with ControlNet. The multi-view consistency is preserved in stylized Minecraft and Cyberpunk cities.", "description": "This figure demonstrates the application of ControlNet to stylize generated city scenes.  ControlNet, a technique that adds conditional control to text-to-image diffusion models, is used here to transform the generated cities into distinct styles, specifically mimicking the visual aesthetics of Minecraft and Cyberpunk.  Importantly, the stylization process preserves the multi-view consistency of the original 3D city model, ensuring that the stylized scenes maintain their structural integrity and visual coherence from different viewpoints.", "section": "5.5 Applications"}, {"figure_path": "https://arxiv.org/html/2501.08983/x12.png", "caption": "Figure 12: COLMAP Reconstruction of 600-frame Orbital Videos. The red ring shows the camera positions, and the clear point clouds demonstrate CityDreamer4D\u2019s consistent rendering. Note that \u201dRecon.\u201d stands for \u201dReconstruction.\u201d", "description": "This figure showcases the results of applying COLMAP, a Structure-from-Motion (SfM) and Multi-View Stereo (MVS) pipeline, to a sequence of 600 orbital videos generated by CityDreamer4D.  The videos simulate a camera orbiting a city scene, capturing a comprehensive view from different angles.  The red ring in the figure highlights the estimated camera positions determined by COLMAP. The clear point cloud visualization demonstrates the high quality and consistency of the 3D scene geometry produced by CityDreamer4D across multiple viewpoints.  This validates the model's ability to generate temporally consistent and geometrically accurate 4D urban scenes.", "section": "5.6 Discussions"}, {"figure_path": "https://arxiv.org/html/2501.08983/x13.png", "caption": "Figure 13: Directional Light Relighting Effect. (a) and (b) show the lighting intensity. (c) illustrates the relighting effect. Note that \u201cS.M.\u201d denotes \u201cShadow Mapping\u201d.", "description": "This figure demonstrates the relighting effects applied to a 3D scene generated by CityDreamer4D. It highlights the two-stage process of relighting: Lambertian shading and shadow mapping.  Panel (a) shows the lighting intensity based on Lambertian shading, resulting in uniform illumination across all directions. Panel (b) presents the lighting intensity from the shadow mapping, showcasing how shadowing and occlusion affect the final illumination. Finally, Panel (c) combines both effects to illustrate the final relighting result, demonstrating the interplay of direct light, shadows, and occlusion.", "section": "3 METHOD"}]