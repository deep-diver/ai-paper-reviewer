[{"figure_path": "https://arxiv.org/html/2501.08983/x1.png", "caption": "Figure 1: Overview of CityDreamer4D. 4D city generation is divided into static and dynamic scene generation, conditioned on \ud835\udc0b\ud835\udc0b\\mathbf{L}bold_L and \ud835\udc13tsubscript\ud835\udc13\ud835\udc61\\mathbf{T}_{t}bold_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, produced by Unbounded Layout Generator and Traffic Scenario Generator, respectively. City Background Generator uses \ud835\udc0b\ud835\udc0b\\mathbf{L}bold_L to create background images \ud835\udc08^Gsubscript^\ud835\udc08\ud835\udc3a\\mathbf{\\hat{I}}_{G}over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT for stuff like roads, vegetation, and the sky, while Building Instance Generator renders the buildings {\ud835\udc08^Bi}subscript^\ud835\udc08subscript\ud835\udc35\ud835\udc56\\{\\mathbf{\\hat{I}}_{B_{i}}\\}{ over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT } within the city. Using \ud835\udc13tsubscript\ud835\udc13\ud835\udc61\\mathbf{T}_{t}bold_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, Vehicle Instance Generator generates vehicles {\ud835\udc08^Vit}superscriptsubscript^\ud835\udc08subscript\ud835\udc49\ud835\udc56\ud835\udc61\\{\\mathbf{\\hat{I}}_{V_{i}}^{t}\\}{ over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT } at time step t\ud835\udc61titalic_t. Finally, Compositor combines the rendered background, buildings, and vehicles into a unified and coherent image \ud835\udc08^Ctsuperscriptsubscript^\ud835\udc08\ud835\udc36\ud835\udc61\\mathbf{\\hat{I}}_{C}^{t}over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. \u201cGen.\u201d, \u201cMod.\u201c, \u201cCond.\u201d, \u201cBG.\u201d, \u201cBLDG.\u201d, and \u201cVEH.\u201d denote \u201cGeneration\u201d, \u201cModulation\u201d, \u201cCondition\u201d, \u201cBackground\u201d, \u201cBuilding\u201d, and \u201cVehicle\u201d, respectively.", "description": "CityDreamer4D is a framework for generating 4D city scenes by separating static and dynamic elements.  Static elements (city layout, buildings, background) are generated first using the Unbounded Layout Generator and City Background Generator.  Dynamic elements (vehicle traffic) are added using the Traffic Scenario Generator and Vehicle Instance Generator. Finally, a Compositor module combines these elements into a single coherent image. The system uses a Bird's Eye View (BEV) representation for efficient scene handling and allows for various downstream tasks like instance editing and stylization.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08983/x2.png", "caption": "Figure 2: Overview of the OSM and GoogleEarth Datasets. (a) Examples of the 2D and 3D annotations in the GoogleEarth dataset, which can be automatically generated using the OSM dataset. (b) The automatic annotation pipeline can be readily adapted for worldwide cities. (c) The dataset statistics highlight the diverse perspectives in the GoogleEarth dataset.", "description": "This figure provides a detailed overview of the OSM and Google Earth datasets used in the CityDreamer4D model.  Part (a) showcases examples of 2D and 3D annotations within the Google Earth dataset, emphasizing that these annotations are automatically generated using the OSM dataset as a base. Part (b) demonstrates the adaptability of the automatic annotation pipeline, highlighting its potential for use with cities worldwide. Lastly, part (c) presents statistical information about the Google Earth dataset, illustrating the variety of perspectives (view angles and altitudes) captured within the dataset.", "section": "4 DATASETS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x3.png", "caption": "Figure 3: Overview of the CityTopia Dataset. (a) The virtual city generation pipeline. \u201cPro.Inst.\u201d, \u201cSur.Spl\u201d, and \u201c3D Inst. Anno.\u201d denote \u201cPrototype Instantiation\u201d, \u201cSurface Sampling\u201d, and \u201c3D Instance Annotation\u201d, respectively. (b) Examples of 2D and 3D annotations in the CityTopia dataset are shown from both daytime and nighttime street-view and aerial-view perspectives, automatically generated during virtual city generation. (c) The dataset statistics highlight the diverse perspectives in both street and aerial views.", "description": "Figure 3 provides a comprehensive overview of the CityTopia dataset, a key contribution of the paper.  Panel (a) details the virtual city generation pipeline, outlining the steps involved: Prototype Instantiation (creation of a base city model), Surface Sampling (generating dense 3D annotations from the model), and 3D Instance Annotation (assigning labels to individual 3D objects within the scene). Panel (b) showcases examples of both 2D and 3D annotations from various viewpoints\u2014daytime and nighttime street views and aerial views\u2014to illustrate the dataset's richness. Importantly, this panel shows the automatic generation of these annotations as part of the CityTopia creation process. Finally, panel (c) presents statistical summaries of the dataset, highlighting the diversity of viewpoints represented in the dataset's images (street-view vs. aerial-view; varying altitudes and elevation angles).", "section": "4 DATASETS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x4.png", "caption": "Figure 4: Qualitative Comparison on Google Earth. For SceneDreamer\u00a0[7] and CityDreamer4D, vehicles are generated using models trained on CityTopia due to the lack of semantic annotations for vehicles in Google Earth. For DimensionX\u00a0[107], the initial frame is provided by CityDreamer4D. The visual results of InfiniCity\u00a0[26], provided by the authors, have been zoomed in for better viewing. \u201cPers.Nature\u201d stands for \u201cPersistentNature\u201d\u00a0[105].", "description": "This figure displays a qualitative comparison of several 3D scene generation models on the Google Earth dataset.  The comparison focuses on the visual quality of generated scenes, particularly regarding the realism of vehicles.  Because the Google Earth dataset lacks semantic annotations for vehicles, the vehicles in the SceneDreamer and CityDreamer4D results were generated using models trained on the CityTopia dataset, which contains such annotations. For DimensionX, the first frame was generated by CityDreamer4D. The InfiniCity results are zoomed in for better viewing as provided by the original authors. PersistentNature is shortened to Pers.Nature in the legend.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08983/x5.png", "caption": "Figure 5: Qualitative Comparison on CityTopia. The initial frame for DimensionX and the input frames for DreamScene4D are chosen from the dataset. \u201cPers.Nature\u201d refers to \u201cPersistentNature\u201d\u00a0[105].", "description": "This figure presents a qualitative comparison of CityDreamer4D's 4D city generation capabilities against several state-of-the-art methods on the CityTopia dataset.  It showcases example image sequences from each method, highlighting differences in visual realism, detail, and consistency.  Note that for methods like DimensionX and DreamScene4D, the initial frames are taken from the CityTopia dataset itself.  'Pers.Nature' is an abbreviation for the PersistentNature method.", "section": "5 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x6.png", "caption": "Figure 6: User Study on 4D City Generation. All scores are in the range of 5, with 5 indicating the best. \u201cPers.Nature\u201d refers to \u201cPersistentNature\u201d\u00a0[105].", "description": "This figure presents the results of a user study comparing the performance of different methods in generating 4D cities.  Users rated each method across three dimensions: perceptual quality, 4D realism, and view consistency, with scores ranging from 1 to 5 (5 being the best).  The results show CityDreamer4D outperforming other state-of-the-art methods on all three dimensions.  'Pers.Nature' in the chart refers to the PersistentNature method from reference [105].", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08983/x7.png", "caption": "Figure 7: Qualitative Comparison of City Layout Generators. The height map values are normalized to a range of [0,1]01[0,1][ 0 , 1 ] by dividing each value by the maximum value within the map.", "description": "This figure compares the performance of three different city layout generators:  the proposed method (Ours), InfinityGAN, and IPSM.  The generated height maps from each generator are visualized. The color intensity of each pixel represents the height, and it is normalized to a range between 0 and 1 for better comparison across the methods. This allows for a visual assessment of the detail and realism captured by each generator in creating the foundational height data of a city.", "section": "3.1 Unbounded Layout Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x8.png", "caption": "Figure 8: Qualitative Comparison of Building Instance Generator (BIG) Variants. (a) and (b) illustrate the effects of removing BIG and instance labels, respectively. (c)\u2013(f) present the results of various scene parameterizations. Note that \u201cEnc.\u201d is an abbreviation for \u201cEncoder\u201d.", "description": "This figure compares different variations of the Building Instance Generator (BIG) within the CityDreamer4D model.  The first row shows the impact of removing BIG entirely (a) and removing instance labels while keeping BIG (b). The second row showcases the results obtained using various scene parameterization methods,  specifically using different combinations of global and local encoders with either generative hash grids or sinusoidal positional encoding. This helps to analyze how different choices in the model architecture and its parameterization strategy affect the final generated images.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08983/x9.png", "caption": "Figure 9: Qualitative Comparison of Vehicle Instance Generator (VIG) Variants. (a) and (b) illustrate the effects of removing VIG and canonicalization, respectively. (c)\u2013(f) present the results of various scene parameterizations. Note that \u201cEnc.\u201d is an abbreviation for \u201cEncoder\u201d.", "description": "This figure compares different variants of the Vehicle Instance Generator (VIG) used in the CityDreamer4D model.  Specifically, it shows the impact of removing VIG entirely (a), removing canonicalization (b), and using different scene parameterization methods (c-f).  The scene parameterization variations involve combinations of global vs. local encoders and different positional encoding schemes (hash grids vs. sinusoidal). The figure visually demonstrates how these choices affect the quality and realism of the generated vehicle instances within the 4D city scenes.", "section": "3.5 Vehical Instance Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x10.png", "caption": "Figure 10: Localized Editing on the Generated Cities. (a) and (c) show vehicle editing results, while (b) and (d) present building editing results.", "description": "This figure showcases the localized editing capabilities of the CityDreamer4D model.  Panels (a) and (c) demonstrate how individual vehicles within the generated city scenes can be modified, changing their properties such as style and position.  Panels (b) and (d) illustrate the same localized editing applied to buildings, modifying building heights and architectural styles.  These edits are performed without affecting the surrounding environment. The results highlight the compositional nature of CityDreamer4D, allowing for fine-grained control over individual objects in a large-scale city setting.", "section": "5.5 Applications"}, {"figure_path": "https://arxiv.org/html/2501.08983/x11.png", "caption": "Figure 11: Text-driven City Stylization with ControlNet. The multi-view consistency is preserved in stylized Minecraft and Cyberpunk cities.", "description": "This figure showcases the results of applying ControlNet to stylize generated city scenes.  ControlNet, a neural network extension, allows for precise control over the image generation process, in this case, influencing the overall style. The images demonstrate that CityDreamer4D successfully maintains multi-view consistency even when applying drastically different styles like Minecraft and Cyberpunk.  The consistent appearance across different viewpoints highlights the robustness and quality of the 3D city model generated.", "section": "5.5 Applications"}, {"figure_path": "https://arxiv.org/html/2501.08983/x12.png", "caption": "Figure 12: COLMAP Reconstruction of 600-frame Orbital Videos. The red ring shows the camera positions, and the clear point clouds demonstrate CityDreamer4D\u2019s consistent rendering. Note that \u201dRecon.\u201d stands for \u201dReconstruction.\u201d", "description": "This figure showcases the results of a 3D reconstruction performed using COLMAP on a sequence of 600 orbital videos generated by CityDreamer4D.  The red ring visually represents the trajectory of the camera, indicating its movement during the video capture.  The clarity and completeness of the generated point cloud serve as evidence of CityDreamer4D's ability to produce temporally consistent and visually realistic 3D scenes.  This demonstrates the model's effectiveness in creating high-quality, coherent 4D city representations.", "section": "5.6 Discussions"}, {"figure_path": "https://arxiv.org/html/2501.08983/x13.png", "caption": "Figure 13: Directional Light Relighting Effect. (a) and (b) show the lighting intensity. (c) illustrates the relighting effect. Note that \u201cS.M.\u201d denotes \u201cShadow Mapping\u201d.", "description": "This figure demonstrates the relighting effect achieved by separating Lambertian shading and shadow mapping.  (a) shows the lighting intensity calculated using Lambertian shading, which results in uniform lighting across all directions. (b) shows the lighting intensity generated by shadow mapping, taking into account light visibility and occlusions from other objects. (c) combines both (a) and (b) to produce a realistic relighting effect with shadows and highlights. This highlights the effectiveness of the approach in generating realistic images of 3D scenes.", "section": "3 METHOD"}]