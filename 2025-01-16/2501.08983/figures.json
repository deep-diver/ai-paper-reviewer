[{"figure_path": "https://arxiv.org/html/2501.08983/x1.png", "caption": "Figure 1: Overview of CityDreamer4D. 4D city generation is divided into static and dynamic scene generation, conditioned on \ud835\udc0b\ud835\udc0b\\mathbf{L}bold_L and \ud835\udc13tsubscript\ud835\udc13\ud835\udc61\\mathbf{T}_{t}bold_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, produced by Unbounded Layout Generator and Traffic Scenario Generator, respectively. City Background Generator uses \ud835\udc0b\ud835\udc0b\\mathbf{L}bold_L to create background images \ud835\udc08^Gsubscript^\ud835\udc08\ud835\udc3a\\mathbf{\\hat{I}}_{G}over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT for stuff like roads, vegetation, and the sky, while Building Instance Generator renders the buildings {\ud835\udc08^Bi}subscript^\ud835\udc08subscript\ud835\udc35\ud835\udc56\\{\\mathbf{\\hat{I}}_{B_{i}}\\}{ over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT } within the city. Using \ud835\udc13tsubscript\ud835\udc13\ud835\udc61\\mathbf{T}_{t}bold_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, Vehicle Instance Generator generates vehicles {\ud835\udc08^Vit}superscriptsubscript^\ud835\udc08subscript\ud835\udc49\ud835\udc56\ud835\udc61\\{\\mathbf{\\hat{I}}_{V_{i}}^{t}\\}{ over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT } at time step t\ud835\udc61titalic_t. Finally, Compositor combines the rendered background, buildings, and vehicles into a unified and coherent image \ud835\udc08^Ctsuperscriptsubscript^\ud835\udc08\ud835\udc36\ud835\udc61\\mathbf{\\hat{I}}_{C}^{t}over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. \u201cGen.\u201d, \u201cMod.\u201c, \u201cCond.\u201d, \u201cBG.\u201d, \u201cBLDG.\u201d, and \u201cVEH.\u201d denote \u201cGeneration\u201d, \u201cModulation\u201d, \u201cCondition\u201d, \u201cBackground\u201d, \u201cBuilding\u201d, and \u201cVehicle\u201d, respectively.", "description": "CityDreamer4D is a framework for generating unbounded 4D cities by separating static and dynamic components.  The static scene (buildings, roads) is generated from a city layout using the Unbounded Layout Generator.  The dynamic component (vehicles) is generated from a traffic scenario using the Traffic Scenario Generator.  Three neural fields generate background elements, buildings, and vehicles separately.  A compositor combines these elements to create a single, coherent image. The entire process is conditioned on the layout and the traffic scenario at each time step.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08983/x2.png", "caption": "Figure 2: Overview of the OSM and GoogleEarth Datasets. (a) Examples of the 2D and 3D annotations in the GoogleEarth dataset, which can be automatically generated using the OSM dataset. (b) The automatic annotation pipeline can be readily adapted for worldwide cities. (c) The dataset statistics highlight the diverse perspectives in the GoogleEarth dataset.", "description": "Figure 2 presents a detailed overview of the OSM and Google Earth datasets used in the CityDreamer4D research.  Part (a) showcases examples of 2D and 3D annotations within the Google Earth dataset, illustrating how these annotations are automatically derived using data from the OSM dataset.  Part (b) demonstrates the generalizability of the automated annotation pipeline, indicating its adaptability to a wide range of global cities.  Finally, part (c) provides statistical summaries of the Google Earth dataset, highlighting the variety of perspectives captured in the data (e.g., varying camera altitudes and angles).", "section": "4 DATASETS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x3.png", "caption": "Figure 3: Overview of the CityTopia Dataset. (a) The virtual city generation pipeline. \u201cPro.Inst.\u201d, \u201cSur.Spl\u201d, and \u201c3D Inst. Anno.\u201d denote \u201cPrototype Instantiation\u201d, \u201cSurface Sampling\u201d, and \u201c3D Instance Annotation\u201d, respectively. (b) Examples of 2D and 3D annotations in the CityTopia dataset are shown from both daytime and nighttime street-view and aerial-view perspectives, automatically generated during virtual city generation. (c) The dataset statistics highlight the diverse perspectives in both street and aerial views.", "description": "Figure 3 provides a comprehensive overview of the CityTopia dataset, a key contribution of the paper.  Panel (a) details the virtual city generation pipeline, illustrating the stages of Prototype Instantiation, Surface Sampling, and 3D Instance Annotation.  Panel (b) showcases examples of the 2D and 3D annotations present in the dataset. These annotations are automatically generated and cover both daytime and nighttime scenarios from street-view and aerial-view perspectives, offering varied viewpoints.  Finally, panel (c) presents a statistical summary of the dataset, highlighting the diverse range of perspectives\u2014elevation and altitude\u2014captured in the CityTopia imagery.", "section": "4 DATASETS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x4.png", "caption": "Figure 4: Qualitative Comparison on Google Earth. For SceneDreamer\u00a0[7] and CityDreamer4D, vehicles are generated using models trained on CityTopia due to the lack of semantic annotations for vehicles in Google Earth. For DimensionX\u00a0[107], the initial frame is provided by CityDreamer4D. The visual results of InfiniCity\u00a0[26], provided by the authors, have been zoomed in for better viewing. \u201cPers.Nature\u201d stands for \u201cPersistentNature\u201d\u00a0[105].", "description": "This figure shows a qualitative comparison of different methods for generating 4D city scenes, specifically focusing on Google Earth data.  Because the Google Earth dataset lacks semantic annotations for vehicles, SceneDreamer and CityDreamer4D use models trained on the CityTopia dataset to generate vehicles.  For DimensionX, the initial frame is generated by CityDreamer4D. The InfiniCity results, provided by their authors, are zoomed in for clarity.  PersistentNature is abbreviated as Pers.Nature.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08983/x5.png", "caption": "Figure 5: Qualitative Comparison on CityTopia. The initial frame for DimensionX and the input frames for DreamScene4D are chosen from the dataset. \u201cPers.Nature\u201d refers to \u201cPersistentNature\u201d\u00a0[105].", "description": "Figure 5 presents a qualitative comparison of CityDreamer4D's city generation capabilities against several state-of-the-art methods on the CityTopia dataset.  The comparison includes SGAM, PersistentNature, SceneDreamer, DreamScene4D, and DimensionX.  It visually demonstrates the differences in the realism, detail, and consistency of generated cities across these methods.  Note that for DimensionX and DreamScene4D, initial or input frames were selected from the CityTopia dataset.", "section": "5 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x6.png", "caption": "Figure 6: User Study on 4D City Generation. All scores are in the range of 5, with 5 indicating the best. \u201cPers.Nature\u201d refers to \u201cPersistentNature\u201d\u00a0[105].", "description": "This figure presents the results of a user study evaluating the quality of 4D city generation across different methods.  Participants rated each method on a scale of 1 to 5 (5 being the best) for three key aspects: perceptual quality, 4D realism, and view consistency.  The bar chart visually compares the scores for each method, including CityDreamer4D and several baseline methods.  The abbreviation \"Pers.Nature\" represents the method \"PersistentNature\" which is referenced elsewhere in the paper.", "section": "5 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x7.png", "caption": "Figure 7: Qualitative Comparison of City Layout Generators. The height map values are normalized to a range of [0,1]01[0,1][ 0 , 1 ] by dividing each value by the maximum value within the map.", "description": "Figure 7 presents a qualitative comparison of three different city layout generators: the proposed Unbounded Layout Generator (ULG), InfinityGAN, and IPSM.  The figure visually demonstrates the generated height maps for each generator. To facilitate comparison, the height map values have been normalized to a range of 0 to 1 by dividing each value by the maximum value found in the respective height map. This normalization ensures that differences in the overall height scales of the generated layouts do not affect the visual comparison of the spatial distribution of heights generated by the different methods.", "section": "3.1 Unbounded Layout Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x8.png", "caption": "Figure 8: Qualitative Comparison of Building Instance Generator (BIG) Variants. (a) and (b) illustrate the effects of removing BIG and instance labels, respectively. (c)\u2013(f) present the results of various scene parameterizations. Note that \u201cEnc.\u201d is an abbreviation for \u201cEncoder\u201d.", "description": "This figure compares different variations of the Building Instance Generator (BIG), a key component in the CityDreamer4D model.  The first two subfigures (a, b) show the impact of removing BIG entirely and the effect of removing instance labels from the input.  The remaining subfigures (c-f) explore different scene parameterization strategies within BIG, specifically varying the encoder type (global or local) and the positional encoding method (generative hash grid or sinusoidal). This allows for a visual assessment of how these design choices affect the quality and realism of the generated building instances within the overall 4D city scene.", "section": "3.4 Building Instance Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x9.png", "caption": "Figure 9: Qualitative Comparison of Vehicle Instance Generator (VIG) Variants. (a) and (b) illustrate the effects of removing VIG and canonicalization, respectively. (c)\u2013(f) present the results of various scene parameterizations. Note that \u201cEnc.\u201d is an abbreviation for \u201cEncoder\u201d.", "description": "This figure compares different versions of the Vehicle Instance Generator (VIG) to show how design choices affect the quality of generated vehicle instances.  (a) and (b) show results when VIG is removed or canonicalization is not used, highlighting the importance of these components. (c) through (f) explore different scene parameterization approaches (using global or local encoders, and different positional encoding methods\u2014hash grids or sinusoidal functions), demonstrating the impact of these choices on vehicle generation.", "section": "3.5 Vehical Instance Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x10.png", "caption": "Figure 10: Localized Editing on the Generated Cities. (a) and (c) show vehicle editing results, while (b) and (d) present building editing results.", "description": "This figure demonstrates the localized editing capabilities of CityDreamer4D.  Panels (a) and (c) showcase modifications made to vehicles within the generated city scenes, highlighting changes in vehicle styles and placement. Panels (b) and (d) illustrate similar editing operations applied to buildings, specifically altering building heights and architectural styles.  The edits are localized, meaning only the targeted elements are modified, while the overall city structure and surrounding elements remain largely consistent. This illustrates CityDreamer4D's ability to perform fine-grained instance-level edits within large-scale, complex city environments.", "section": "5.5 Applications"}, {"figure_path": "https://arxiv.org/html/2501.08983/x11.png", "caption": "Figure 11: Text-driven City Stylization with ControlNet. The multi-view consistency is preserved in stylized Minecraft and Cyberpunk cities.", "description": "This figure demonstrates the results of applying ControlNet, a technique for adding conditional control to text-to-image diffusion models, to stylize generated city scenes.  The results show that the model can successfully generate stylized versions of the city, mimicking the aesthetics of Minecraft and Cyberpunk. Importantly, multi-view consistency is maintained in these stylized versions, ensuring that the generated scenes remain coherent and realistic from multiple viewpoints.", "section": "5.5 Applications"}, {"figure_path": "https://arxiv.org/html/2501.08983/x12.png", "caption": "Figure 12: COLMAP Reconstruction of 600-frame Orbital Videos. The red ring shows the camera positions, and the clear point clouds demonstrate CityDreamer4D\u2019s consistent rendering. Note that \u201dRecon.\u201d stands for \u201dReconstruction.\u201d", "description": "This figure displays the results of 3D reconstruction performed using COLMAP on a sequence of 600 orbital video frames generated by CityDreamer4D.  The red ring highlights the camera positions during the video capture. The dense and clearly defined point cloud demonstrates the high consistency and quality of the 3D scene generated by CityDreamer4D, indicating its ability to produce temporally consistent and realistic 4D city models.", "section": "5.6 Discussions"}, {"figure_path": "https://arxiv.org/html/2501.08983/x13.png", "caption": "Figure 13: Directional Light Relighting Effect. (a) and (b) show the lighting intensity. (c) illustrates the relighting effect. Note that \u201cS.M.\u201d denotes \u201cShadow Mapping\u201d.", "description": "Figure 13 demonstrates the relighting effects applied to a 3D scene generated by CityDreamer4D.  It showcases how the model incorporates both Lambertian shading (a) and shadow mapping (b) to achieve realistic lighting in the final rendered image (c). Lambertian shading provides uniform lighting across all surfaces, while shadow mapping simulates shadows and occlusions due to the presence of objects. Combining these techniques results in the realistic relighting effect in (c), where the light source is positioned to the left of the scene.", "section": "3 METHOD"}]