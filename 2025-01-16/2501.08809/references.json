{"references": [{"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-XX-XX", "reason": "This paper introduced the Transformer architecture, which is fundamental to many modern sequence-to-sequence models, including the XMusic framework's Generator and Selector."}, {"fullname_first_author": "Z. Dai", "paper_title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "publication_date": "2019-XX-XX", "reason": "This work enhanced the Transformer architecture to handle longer sequences, addressing a key challenge in music generation where long-range dependencies are crucial."}, {"fullname_first_author": "C.-Z. A. Huang", "paper_title": "Music Transformer: Generating Music with Long-Term Structure", "publication_date": "2021-XX-XX", "reason": "This paper pioneered the application of Transformers to symbolic music generation, demonstrating their potential for creating high-quality and coherent musical pieces."}, {"fullname_first_author": "W.-Y. Hsiao", "paper_title": "Compound Word Transformer: Learning to Compose Full-Song Music Over Dynamic Directed Hypergraphs", "publication_date": "2021-XX-XX", "reason": "This work introduced a novel tokenization technique for symbolic music, improving the efficiency and effectiveness of music generation models by reducing the length of token sequences."}, {"fullname_first_author": "H.-T. Hung", "paper_title": "EMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation", "publication_date": "2021-XX-XX", "reason": "This paper presented a large-scale dataset annotated with emotion labels, which is crucial for training emotion-aware music generation models like XMusic."}]}