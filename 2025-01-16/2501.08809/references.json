{"references": [{"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-MM-DD", "reason": "This paper introduced the Transformer architecture, a crucial component in many modern sequence-to-sequence models, including the XMusic framework's Generator and Selector."}, {"fullname_first_author": "Z. Dai", "paper_title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "publication_date": "2019-MM-DD", "reason": "This work addressed the limitations of standard Transformers in handling long sequences, which is essential for processing long musical sequences like those generated by XMusic."}, {"fullname_first_author": "C.-Z. A. Huang", "paper_title": "Music Transformer: Generating Music with Long-Term Structure", "publication_date": "2021-MM-DD", "reason": "This paper pioneered the successful application of Transformers to symbolic music generation, which forms a basis for many subsequent music generation models, including XMusic."}, {"fullname_first_author": "W.-Y. Hsiao", "paper_title": "Compound Word Transformer: Learning to Compose Full-Song Music Over Dynamic Directed Hypergraphs", "publication_date": "2021-MM-DD", "reason": "This work introduced a novel tokenization technique that improves the efficiency and quality of symbolic music generation, directly influencing XMusic's music representation."}, {"fullname_first_author": "H.-T. Hung", "paper_title": "EMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation", "publication_date": "2021-MM-DD", "reason": "This paper introduced a large-scale dataset annotated with emotion labels, which is crucial for training emotion-aware music generation models, such as XMusic."}]}