{"references": [{"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-MM-DD", "reason": "This paper introduced the Transformer architecture, a crucial component of the XMusic framework, demonstrating its effectiveness in handling long-range dependencies in sequential data like symbolic music."}, {"fullname_first_author": "Z. Dai", "paper_title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "publication_date": "2019-MM-DD", "reason": "This paper extended the Transformer architecture to handle longer sequences, which is particularly important for generating longer pieces of music."}, {"fullname_first_author": "C.-Z. A. Huang", "paper_title": "Music Transformer: Generating Music with Long-Term Structure", "publication_date": "2021-MM-DD", "reason": "This paper demonstrated the successful application of the Transformer architecture for generating long symbolic music sequences, showcasing the model's ability to capture temporal dependencies effectively."}, {"fullname_first_author": "W.-Y. Hsiao", "paper_title": "Compound Word Transformer: Learning to Compose Full-Song Music Over Dynamic Directed Hypergraphs", "publication_date": "2021-MM-DD", "reason": "This paper proposed a novel tokenization technique for symbolic music, enhancing the efficiency and performance of music generation models."}, {"fullname_first_author": "H.-T. Hung", "paper_title": "EMOPIA: A Multi-modal Pop Piano Dataset for Emotion Recognition and Emotion-based Music Generation", "publication_date": "2021-MM-DD", "reason": "This paper introduced the EMOPIA dataset, a valuable resource for training and evaluating emotion-based music generation models, which is relevant to the XMusic framework's focus on generating emotionally controllable music."}]}