[{"figure_path": "https://arxiv.org/html/2501.08809/x1.png", "caption": "Figure 1: The architectural overview of our XMusic framework. It contains two essential components: XProjector and XComposer. XProjector parses various input prompts into specific symbolic music elements. These elements then serve as control signals, guiding the music generation process within the Generator of XComposer. Additionally, XComposer includes a Selector that evaluates and identifies high-quality generated music. The Generator is trained on our large-scale dataset, XMIDI, which includes precise emotion and genre labels.", "description": "The XMusic framework consists of two main components: XProjector and XComposer.  XProjector acts as a multi-modal parser, converting various input types (images, videos, text, tags, humming) into symbolic music elements like emotion, genre, rhythm, and notes. These elements serve as control signals for XComposer. XComposer contains a Generator that produces music based on these control signals, and a Selector that filters the output, selecting only high-quality music using a multi-task learning approach encompassing quality assessment, emotion recognition, and genre recognition. The Generator is trained using the XMIDI dataset, a large-scale symbolic music dataset with precise emotion and genre labels.", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x2.png", "caption": "Figure 2: Illustration of the proposed XMusic, which supports flexible (a) X-Prompts to guide the generation of high-quality symbolic music. The XProjector analyzes these prompts, mapping them to symbolic music elements within the (b) Projection Space. Subsequently, the (c) Generator of XComposer transforms these symbolic music elements into token sequences based on our enhanced representation. It employs a Transformer Decoder as the generative model to predict successive events iteratively, thereby creating complete musical compositions. Finally, the (d) Selector of XComposer utilizes a Transformer Encoder to encode the complete token sequences and employs a multi-task learning scheme to evaluate the quality of the generated music.", "description": "This figure illustrates the XMusic framework's architecture, which consists of two main components: XProjector and XComposer.  XProjector handles multi-modal inputs (images, videos, text, tags, humming) transforming them into symbolic musical elements (emotions, genres, rhythm, notes) in a projection space. XComposer comprises a Generator (using a Transformer Decoder to create music from the elements) and a Selector (using a Transformer Encoder and multi-task learning to assess and select high-quality music).", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x3.png", "caption": "Figure 3: Comparison between our representation and Compound Word (CP) \u00a0[10] representation. The dotted boxes represent our new tokens in comparison with those of the CP representation.", "description": "Figure 3 illustrates the enhanced symbolic music representation used in the proposed XMusic framework.  It compares the authors' representation to the Compound Word (CP) representation from a previous work [10]. The key difference is highlighted by the dotted boxes, which showcase the new tokens (Tag, Instrument, and enhanced Rhythm elements) introduced in the XMusic representation. These additions enable finer control over emotional expression, instrument selection, and rhythmic structure during music generation, providing greater versatility and precision.", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x4.png", "caption": "Figure 4: Data statistics of our XMIDI dataset.", "description": "Figure 4 presents a comprehensive overview of the XMIDI dataset's statistical properties, visualized across three subfigures. (a) illustrates the distribution of emotion categories within the dataset, revealing the prevalence of certain emotions like \"exciting\", \"warm\", and \"happy\" while showcasing less frequent emotions such as \"fear\" and \"magnificent\". (b) displays the distribution of genre categories, demonstrating a more balanced representation of genres, with \"rock\" and \"pop\" being slightly more dominant. Finally, (c) presents the distribution of music length, highlighting a concentration of music pieces in the 120-180 second and 180-240 second ranges and fewer longer and shorter pieces.", "section": "IV. Experiments"}]