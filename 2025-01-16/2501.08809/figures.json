[{"figure_path": "https://arxiv.org/html/2501.08809/x1.png", "caption": "Figure 1: The architectural overview of our XMusic framework. It contains two essential components: XProjector and XComposer. XProjector parses various input prompts into specific symbolic music elements. These elements then serve as control signals, guiding the music generation process within the Generator of XComposer. Additionally, XComposer includes a Selector that evaluates and identifies high-quality generated music. The Generator is trained on our large-scale dataset, XMIDI, which includes precise emotion and genre labels.", "description": "The XMusic framework consists of two main components: XProjector and XComposer.  XProjector takes various input prompts (images, videos, text, tags, humming) and translates them into symbolic musical elements like emotion, genre, rhythm, and notes. These elements act as control signals for XComposer. XComposer comprises a Generator, which produces music based on these signals using the XMIDI dataset for training, and a Selector, which assesses the generated music for quality, emotion, and genre to filter for high-quality outputs.  The XMIDI dataset is a large-scale dataset of MIDI files labeled with emotions and genres.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.08809/x2.png", "caption": "Figure 2: Illustration of the proposed XMusic, which supports flexible (a) X-Prompts to guide the generation of high-quality symbolic music. The XProjector analyzes these prompts, mapping them to symbolic music elements within the (b) Projection Space. Subsequently, the (c) Generator of XComposer transforms these symbolic music elements into token sequences based on our enhanced representation. It employs a Transformer Decoder as the generative model to predict successive events iteratively, thereby creating complete musical compositions. Finally, the (d) Selector of XComposer utilizes a Transformer Encoder to encode the complete token sequences and employs a multi-task learning scheme to evaluate the quality of the generated music.", "description": "XMusic is a framework for generating high-quality symbolic music from various input prompts (images, videos, text, tags, humming).  The XProjector component parses these prompts into symbolic music elements (emotions, genres, rhythms, notes). The XComposer component then uses a Transformer-based Generator to create music from these elements, and a Selector to evaluate and filter the generated music using a multi-task learning approach assessing quality, emotion, and genre. The figure illustrates this process showing the flow of information from input prompts to generated music. ", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x3.png", "caption": "Figure 3: Comparison between our representation and Compound Word (CP) \u00a0[10] representation. The dotted boxes represent our new tokens in comparison with those of the CP representation.", "description": "Figure 3 illustrates the differences between the proposed symbolic music representation and the Compound Word (CP) representation from prior work.  The figure highlights that the proposed representation expands upon the CP representation by adding new token types to better control the generation process. These new tokens include instrument, emotion, genre, and additional tokens to enhance rhythm representation and control. The added tokens and their groupings are visually emphasized in the figure using dotted boxes.", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x4.png", "caption": "Figure 4: Data statistics of our XMIDI dataset.", "description": "Figure 4 presents a comprehensive statistical overview of the XMIDI dataset, visualizing the distribution of emotion and genre labels, as well as music length.  The emotion distribution shows an uneven spread, with certain emotions (such as exciting, warm, happy, romantic, funny, sad, and angry) being significantly more frequent than others (such as quiet, lazy, and fear).  The genre distribution is more balanced, though rock music is the most common genre. The length of music in the dataset spans a considerable range, however, most pieces fall within 1-5 minutes.", "section": "IV. Experiments"}]