[{"figure_path": "https://arxiv.org/html/2501.08809/x1.png", "caption": "Figure 1: The architectural overview of our XMusic framework. It contains two essential components: XProjector and XComposer. XProjector parses various input prompts into specific symbolic music elements. These elements then serve as control signals, guiding the music generation process within the Generator of XComposer. Additionally, XComposer includes a Selector that evaluates and identifies high-quality generated music. The Generator is trained on our large-scale dataset, XMIDI, which includes precise emotion and genre labels.", "description": "The XMusic framework consists of two main components: XProjector and XComposer.  XProjector takes multi-modal inputs (images, video, text, tags, humming) and translates them into symbolic music elements such as emotion, genre, rhythm, and notes. These elements act as control signals for XComposer. XComposer contains a Generator that produces music based on the control signals from XProjector, and a Selector that evaluates the generated music based on quality, emotion, and genre, selecting only the high-quality outputs.  The Generator is trained on the large-scale XMIDI dataset, which is annotated with emotion and genre labels. ", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x2.png", "caption": "Figure 2: Illustration of the proposed XMusic, which supports flexible (a) X-Prompts to guide the generation of high-quality symbolic music. The XProjector analyzes these prompts, mapping them to symbolic music elements within the (b) Projection Space. Subsequently, the (c) Generator of XComposer transforms these symbolic music elements into token sequences based on our enhanced representation. It employs a Transformer Decoder as the generative model to predict successive events iteratively, thereby creating complete musical compositions. Finally, the (d) Selector of XComposer utilizes a Transformer Encoder to encode the complete token sequences and employs a multi-task learning scheme to evaluate the quality of the generated music.", "description": "The figure illustrates the XMusic framework, which consists of two main components: XProjector and XComposer.  XProjector takes in multi-modal prompts (images, videos, text, tags, humming) and translates them into symbolic music elements (emotions, genres, rhythms, notes) within a projection space. XComposer then uses a Generator (Transformer Decoder) to produce a musical composition from these elements, iteratively predicting musical events. Finally, a Selector (Transformer Encoder) evaluates the generated music's quality using a multi-task learning approach that assesses quality, emotion, and genre, selecting only the high-quality outputs.", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x3.png", "caption": "Figure 3: Comparison between our representation and Compound Word (CP) \u00a0[10] representation. The dotted boxes represent our new tokens in comparison with those of the CP representation.", "description": "Figure 3 illustrates the differences between the proposed symbolic music representation and the Compound Word (CP) representation from a prior work.  The core of the comparison focuses on the novel tokens introduced in the proposed model.  The dotted boxes highlight these new tokens, which include those for instrument, emotion, and genre information, alongside enhancements to rhythm representation, such as density and strength parameters for bars and beats. The new tokens allow for greater controllability and expressiveness in generated music compared to the original CP representation.", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x4.png", "caption": "Figure 4: Data statistics of our XMIDI dataset.", "description": "Figure 4 presents a visual summary of the XMIDI dataset's key statistical properties, broken down into three subfigures. (a) displays the distribution of emotion labels across the dataset, revealing the prevalence of certain emotions (e.g., exciting, warm, happy) over others (e.g., quiet, fear, magnificent). (b) shows the distribution of genre labels, indicating a relatively balanced representation of different music genres (e.g., rock, pop, country, jazz, classical, folk). Finally, (c) illustrates the distribution of music length, demonstrating the concentration of music pieces within specific durations (primarily between 120 seconds and 300 seconds).  Overall, the figure offers a quick overview of the dataset's composition with respect to emotional expression, musical style, and temporal extent.", "section": "IV. Experiments"}]