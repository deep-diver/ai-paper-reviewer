{"references": [{"fullname_first_author": "P. Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "publication_date": "2021-00-00", "reason": "This paper is foundational for transformer-based diffusion models, a key approach in the paper\u2019s methodology."}, {"fullname_first_author": "A. Ramesh", "paper_title": "Zero-shot text-to-image generation", "publication_date": "2021-00-00", "reason": "This is a seminal work on text-to-image generation using diffusion models, providing a basis for extending to text-to-video generation."}, {"fullname_first_author": "J. Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-00-00", "reason": "This paper introduces denoising diffusion probabilistic models (DDPMs), the core technology behind many modern generative models, including those used in the RepVideo approach."}, {"fullname_first_author": "R. Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-00-00", "reason": "This paper introduces latent diffusion models (LDMs), a crucial improvement over previous diffusion models, which enhances efficiency and quality by operating on latent representations of images."}, {"fullname_first_author": "U. Singer", "paper_title": "Make-a-video: Text-to-video generation without text-video data", "publication_date": "2022-09-14", "reason": "This paper is a significant advancement in text-to-video generation using diffusion models, demonstrating high-quality results without relying on large text-video datasets, setting the stage for RepVideo's improvements."}]}