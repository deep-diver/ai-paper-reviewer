[{"figure_path": "https://arxiv.org/html/2501.08994/x1.png", "caption": "Figure 1: The examples generated by RepVideo. RepVideo effectively generates diverse videos with enhanced temporal coherence and fine-grained spatial details.", "description": "This figure showcases example videos generated using the RepVideo model.  The videos demonstrate RepVideo's ability to produce diverse video content with high quality.  Specifically, the examples highlight the model's improved generation of fine details within each frame (spatial details) and the smoother transitions between frames, improving temporal coherence.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.08994/x2.png", "caption": "Figure 2: The architecture of recent transformer-based video diffusion models. These methods typically consist of three core components: a 3D VAE, the text encoder, and a transformer network.", "description": "The figure illustrates the typical architecture of modern transformer-based video diffusion models.  It highlights the three main components: a 3D Variational Autoencoder (VAE) for compressing video input into a lower-dimensional latent space, a text encoder to process textual descriptions and convert them into a numerical format suitable for the model, and a transformer network that processes both the encoded text and the compressed video representation to generate the final video output. The VAE handles the spatial and temporal compression of the video, the text encoder manages the semantic understanding of the text prompt, and the transformer network learns the complex mapping between the textual information and the video's visual representation, generating a coherent and contextually relevant video.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x3.png", "caption": "Figure 3: The visualization of the attention distribution of each frame\u2019s token across the entire token sequence. The results highlight significant variations in attention distributions across layers, with deeper layers focusing more on tokens from the same frame and exhibiting weaker attention to tokens from other frames.", "description": "This figure visualizes how attention mechanisms in a transformer network distribute attention across different frames within a video. Each subplot represents a frame from the video, and each line within each subplot shows the attention distribution across various layers of the network. The key takeaway is that deeper layers of the network focus more attention on tokens from the same frame, while shallower layers exhibit a wider distribution of attention across multiple frames. This indicates that deeper layers are better at capturing the spatial context within individual frames.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x4.png", "caption": "Figure 4: The visualization of attention maps across transformer layers. Each layer attends to distinct regions, capturing diverse spatial features. However, the lack of coordination across layers results in fragmented feature representations, weakening the model\u2019s ability to establish coherent spatial semantics within individual frames.", "description": "This figure visualizes attention maps from different layers of a transformer network within a video diffusion model. Each layer's attention map shows where the model focuses its attention when processing video frames.  Different layers attend to different spatial regions within the frame, indicating that each layer captures specific aspects of the scene. However, the lack of coordination between these layers results in fragmented and inconsistent representations of the overall scene. This inconsistency ultimately hinders the model's capacity to create a cohesive and coherent understanding of spatial semantics within each frame of the generated video.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x5.png", "caption": "Figure 5: The average similarity between adjacent frame features across diffusion layers and denoising steps. The similarity decreases as layer depth increases for a given denoising step, indicating greater differentiation in deeper layers. Additionally, similarity between adjacent frames declines as the denoising process progresses.", "description": "This figure shows the average similarity between features of adjacent frames within a video generation model.  The x-axis represents the layer depth of the transformer network, while the y-axis represents the average cosine similarity between consecutive frames.  Multiple lines represent different stages of the denoising process.  The figure illustrates two key findings: 1. For any given denoising step, the similarity between adjacent frames decreases as the model processes the data through deeper layers. This indicates increasing differentiation of features across layers. 2. For a given layer, the similarity decreases as the denoising process progresses from early to late steps, which means that temporal consistency is reduced as the model refines the generation.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x6.png", "caption": "Figure 6: The comparison of the original feature maps from a standard transformer layer with those obtained after aggregation in the Feature Cache Module. The aggregated features demonstrate more comprehensive semantic information and clearer structural details.", "description": "This figure compares feature maps from a standard transformer layer against those produced after aggregation within the Feature Cache Module (a component of the RepVideo model).  The key takeaway is that feature aggregation leads to feature maps that are richer semantically and exhibit more clearly defined structural details. In essence, the combined features from multiple layers provide a more comprehensive and refined representation of the input compared to individual layers alone.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x7.png", "caption": "Figure 7: The comparison of adjacent frame similarity between original and aggregated features. The aggregated features from the Feature Cache Module exhibit higher similarity between adjacent frames compared to the original transformer layers, indicating improved temporal coherence.", "description": "Figure 7 presents a quantitative comparison illustrating the impact of feature aggregation on temporal coherence in video generation.  Two sets of results are shown: one using the original features directly from the transformer layers, and the other using features aggregated by the Feature Cache Module.  The graph displays the average similarity between adjacent frames for both sets of features.  The key finding is that the aggregated features exhibit significantly higher similarity scores, indicating that the Feature Cache Module effectively enhances temporal coherence by smoothing transitions between frames and reducing inconsistencies.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x8.png", "caption": "Figure 8: The architecture of the enhanced cross-layer\nrepresentation framework.", "description": "The figure illustrates the RepVideo architecture, an enhanced cross-layer representation framework designed to improve video generation. It enhances video representations in text-to-video diffusion models by using a feature cache module that aggregates features from multiple adjacent transformer layers. This aggregated representation is then combined with the original transformer input via a gating mechanism. This approach aims to maintain consistency and enhance semantic information across the frames, improving both temporal and spatial quality.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x9.png", "caption": "Figure 9: The qualitative comparison between our method and the baseline CogVideoX-2B\u00a0[31]. The first row shows results from the baseline CogVideoX-2B\u00a0[31], while the second row presents the results generated by RepVideo, demonstrating significant improvements in quality and coherence.", "description": "Figure 9 presents a qualitative comparison of video generation results between the proposed RepVideo model and the baseline CogVideoX-2B model.  The figure displays examples of videos generated from the same text prompts. Each row shows a sequence of frames from a single video. The first row showcases videos generated by the CogVideoX-2B baseline, illustrating common artifacts in video generation, such as inconsistencies and blurriness. The second row displays the results obtained from RepVideo, highlighting its superior performance by generating videos with improved temporal coherence, higher visual fidelity and much more coherent spatial details.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08994/x10.png", "caption": "Figure 10: The Layer-wise comparison of feature maps between CogVideoX-2B and RepVideo. The comparison shows that RepVideo consistently captures richer semantic information and maintains more coherent spatial details across layers compared to CogVideoX-2B\u00a0[31].", "description": "Figure 10 presents a layer-by-layer comparison of feature maps generated by two different models: RepVideo and CogVideoX-2B.  The visualization reveals that RepVideo consistently produces feature maps with richer semantic information and more coherent spatial details across all layers. In contrast, CogVideoX-2B shows a decline in the quality of feature maps at deeper layers, indicating a loss of semantic detail and coherence.  Two example scenes are shown: one depicting a person in a spacesuit examining plants, and the other showing a man in a dark suit walking towards a room.  The superior quality and clarity of RepVideo's feature maps demonstrate the model's ability to maintain spatial fidelity and context across different levels of processing.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08994/x11.png", "caption": "Figure 11: The comparison of attention maps between CogVideoX-2B and RepVideo. The comparison demonstrates that RepVideo could maintain more consistent semantic relationship compared to CogVideoX-2B\u00a0[31].", "description": "Figure 11 visualizes attention maps from CogVideoX-2B and RepVideo, highlighting RepVideo's improved ability to maintain consistent semantic relationships across different parts of the video.  The attention maps show which parts of the video the models focus on when generating video from a text prompt. In RepVideo, attention remains focused on relevant objects and areas throughout the video generation process, resulting in more coherent and semantically meaningful videos. CogVideoX-2B, on the other hand, exhibits less consistent attention, leading to potential inconsistencies and disruptions in the generated output. The comparison underscores RepVideo\u2019s strength in preserving coherent meaning and continuity during video generation.", "section": "V. DISCUSSION"}, {"figure_path": "https://arxiv.org/html/2501.08994/x12.png", "caption": "Figure 12: The cosine similarity between consecutive frames across layers.", "description": "This figure visualizes the temporal consistency of video generation across different layers of a transformer network.  The x-axis represents the frame number, indicating consecutive frames in a generated video sequence.  The y-axis shows the cosine similarity between the features of adjacent frames. Multiple lines are plotted, each representing a different layer of the transformer model.  The graph illustrates how the similarity between consecutive frames changes as the model processes the video through its different layers.  A decrease in cosine similarity suggests reduced temporal coherence at that layer. This allows us to analyze the effect of each layer on temporal consistency within the generated video.", "section": "IV. EXPERIMENTS"}]