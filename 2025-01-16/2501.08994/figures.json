[{"figure_path": "https://arxiv.org/html/2501.08994/x1.png", "caption": "Figure 1: The examples generated by RepVideo. RepVideo effectively generates diverse videos with enhanced temporal coherence and fine-grained spatial details.", "description": "This figure showcases various video examples generated using the RepVideo model. Each video clip demonstrates RepVideo's capability to generate diverse and high-quality videos with fine details and improved temporal coherence. The enhanced temporal coherence ensures smooth and realistic transitions between frames, while the fine-grained spatial details result in sharp, clear, and visually appealing videos.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.08994/x2.png", "caption": "Figure 2: The architecture of recent transformer-based video diffusion models. These methods typically consist of three core components: a 3D VAE, the text encoder, and a transformer network.", "description": "The figure illustrates the architecture of modern transformer-based video diffusion models.  It highlights the three main components: a 3D Variational Autoencoder (VAE) that compresses video data into a lower-dimensional latent representation, a text encoder that processes textual input and converts it into embeddings, and a transformer network that combines the textual embeddings with the video latent representation to generate video frames. The VAE handles the compression of the video, the text encoder translates text into machine-understandable information, and the transformer is the core generative part that uses the combined data to generate the actual video. ", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x3.png", "caption": "Figure 3: The visualization of the attention distribution of each frame\u2019s token across the entire token sequence. The results highlight significant variations in attention distributions across layers, with deeper layers focusing more on tokens from the same frame and exhibiting weaker attention to tokens from other frames.", "description": "This figure visualizes how attention mechanisms in a transformer network for video generation distribute attention across different frames within a video sequence.  Each subplot represents a different frame (Frame 1, Frame 2, Frame 3). Within each subplot, the y-axis shows the attention weight, and the x-axis shows the tokens in the sequence (representing various aspects of the scene and the relationship between elements). Multiple lines within each subplot indicate the attention distribution for different layers of the transformer network (Layer 1, Layer 2, Layer 3, Layer 4, Layer 5). The visualization shows that in the lower layers (shallow), attention is distributed more evenly across various tokens from different frames.  As the depth of the layers increases, the focus of attention becomes more concentrated on the tokens corresponding to the current frame, while less attention is given to tokens of other frames. This shift in attention distribution implies that deeper layers refine the video's spatial representation focusing more intensely on details in a single frame.  It suggests that there is a tradeoff between the broad contextual understanding in shallower layers and the frame-level detail in deeper layers.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x4.png", "caption": "Figure 4: The visualization of attention maps across transformer layers. Each layer attends to distinct regions, capturing diverse spatial features. However, the lack of coordination across layers results in fragmented feature representations, weakening the model\u2019s ability to establish coherent spatial semantics within individual frames.", "description": "Figure 4 visualizes attention maps from different layers of a transformer network used in a video generation model. Each layer focuses on different spatial aspects of the input, showing that deeper layers focus less on the relationships between frames.  The lack of coordination between layers' attention leads to fragmented feature representations, hindering the model's ability to create coherent spatial relationships within individual frames.  This fragmentation ultimately impacts the model's ability to generate videos with consistent and accurate spatial details across the entire scene.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x5.png", "caption": "Figure 5: The average similarity between adjacent frame features across diffusion layers and denoising steps. The similarity decreases as layer depth increases for a given denoising step, indicating greater differentiation in deeper layers. Additionally, similarity between adjacent frames declines as the denoising process progresses.", "description": "This figure visualizes the average similarity between features of consecutive frames across different layers of a diffusion model during the denoising process.  The x-axis represents the layer number, and different lines represent different denoising steps. The y-axis shows the similarity score. The figure demonstrates two key findings: 1. For a given denoising step, the similarity between adjacent frames decreases as the layer depth increases, indicating that deeper layers produce features with greater differences between frames.  2. As the denoising process progresses (moving from earlier to later steps), the similarity between adjacent frames generally declines across all layers, indicating a reduction in temporal coherence as the model refines the generated video.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x6.png", "caption": "Figure 6: The comparison of the original feature maps from a standard transformer layer with those obtained after aggregation in the Feature Cache Module. The aggregated features demonstrate more comprehensive semantic information and clearer structural details.", "description": "This figure compares feature maps from a standard transformer layer with those generated by the Feature Cache Module in RepVideo.  The standard feature map shows less defined features and less comprehensive semantic details, indicating fragmentation. In contrast, the aggregated feature map from the Feature Cache Module shows enhanced structural details and more comprehensive semantic information, suggesting that combining features across multiple layers improves representation quality. This is crucial for generating videos with higher quality and better coherence.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x7.png", "caption": "Figure 7: The comparison of adjacent frame similarity between original and aggregated features. The aggregated features from the Feature Cache Module exhibit higher similarity between adjacent frames compared to the original transformer layers, indicating improved temporal coherence.", "description": "Figure 7 presents a comparative analysis of the similarity between consecutive frames' features. It contrasts the similarity scores obtained from the original transformer layers against those derived from features aggregated by the Feature Cache Module (FCM). The results demonstrate that the FCM significantly enhances the similarity between adjacent frames' features. This improved similarity is indicative of better temporal coherence in the generated video sequences, implying that the FCM effectively mitigates inconsistencies or abrupt transitions between consecutive frames, leading to smoother and more natural-looking video generation.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x8.png", "caption": "Figure 8: The architecture of the enhanced cross-layer\nrepresentation framework.", "description": "This figure illustrates the architecture of RepVideo, an enhanced cross-layer representation framework. RepVideo modifies the standard transformer-based video diffusion model by adding a Feature Cache Module. This module aggregates features from multiple adjacent transformer layers to form a more comprehensive and temporally consistent representation. The aggregated features are then integrated into the original transformer inputs via a gating mechanism, allowing for the dynamic balancing of enriched features and preserving layer-specific information. This approach improves both the spatial and temporal consistency in generated videos, leading to better quality.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x9.png", "caption": "Figure 9: The qualitative comparison between our method and the baseline CogVideoX-2B\u00a0[31]. The first row shows results from the baseline CogVideoX-2B\u00a0[31], while the second row presents the results generated by RepVideo, demonstrating significant improvements in quality and coherence.", "description": "Figure 9 presents a qualitative comparison of video generation results between the baseline model, CogVideoX-2B, and the proposed RepVideo model.  The figure displays four video generation scenarios, each shown in two rows. The top row shows the results produced by CogVideoX-2B, while the bottom row shows the corresponding results generated by RepVideo. The examples showcase the differences in visual quality, coherence, and attention to detail between the two models. RepVideo is shown to generate videos with improved coherence and quality.", "section": "IV. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08994/x10.png", "caption": "Figure 10: The Layer-wise comparison of feature maps between CogVideoX-2B and RepVideo. The comparison shows that RepVideo consistently captures richer semantic information and maintains more coherent spatial details across layers compared to CogVideoX-2B\u00a0[31].", "description": "Figure 10 presents a layer-by-layer comparison of feature maps generated by two different video generation models: RepVideo and CogVideoX-2B.  The comparison focuses on how well each model maintains spatial coherence and semantic detail as the model processes the video data through its layers.  The visual representation shows that RepVideo (the model proposed in this paper) consistently generates feature maps with clearer spatial details and richer semantic information across multiple layers compared to CogVideoX-2B. This highlights RepVideo's improved ability to maintain coherence and accuracy in its representation of the video content.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08994/x11.png", "caption": "Figure 11: The comparison of attention maps between CogVideoX-2B and RepVideo. The comparison demonstrates that RepVideo could maintain more consistent semantic relationship compared to CogVideoX-2B\u00a0[31].", "description": "Figure 11 visualizes and compares attention maps from CogVideoX-2B and RepVideo models, focusing on how each model's attention mechanism handles semantic relationships between different elements within a generated video frame.  CogVideoX-2B shows significant variations and inconsistencies in attention patterns across layers.  In contrast, RepVideo demonstrates more consistent and stable attention distribution across layers, indicating its ability to maintain stronger semantic coherence within the generated video frames. This demonstrates improved semantic relationships in RepVideo compared to CogVideoX-2B.", "section": "V. DISCUSSION"}, {"figure_path": "https://arxiv.org/html/2501.08994/x12.png", "caption": "Figure 12: The cosine similarity between consecutive frames across layers.", "description": "This figure visualizes the temporal consistency of video generation across different layers of a transformer network.  The x-axis represents the frame number, showing the similarity between consecutive frames. The y-axis represents the cosine similarity, a measure of how similar two frames are.  Separate lines are plotted for each layer in the network. The plots show how the cosine similarity changes as the model progresses through the layers of the network. This helps to understand whether the network is maintaining temporal coherence as the generation process continues.  A higher cosine similarity indicates better temporal consistency (smoother transitions between frames).", "section": "IV. EXPERIMENTS"}]