[{"figure_path": "https://arxiv.org/html/2501.08994/x1.png", "caption": "Figure 1: The examples generated by RepVideo. RepVideo effectively generates diverse videos with enhanced temporal coherence and fine-grained spatial details.", "description": "This figure showcases example videos generated using the RepVideo model.  The videos demonstrate the model's ability to produce diverse video content with high quality.  Specifically, it highlights RepVideo's improvements in temporal coherence (smooth and natural transitions between frames) and fine-grained spatial detail (sharp and clear visuals). Each video depicts a different scene, showcasing the model's versatility and capability to generate a range of video styles and subject matters.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.08994/x2.png", "caption": "Figure 2: The architecture of recent transformer-based video diffusion models. These methods typically consist of three core components: a 3D VAE, the text encoder, and a transformer network.", "description": "This figure illustrates the typical architecture of modern transformer-based video diffusion models.  It breaks down the process into three main stages. First, a 3D Variational Autoencoder (VAE) compresses the input video into a lower-dimensional latent representation. This latent representation, along with text embeddings generated from a text encoder that processes a textual description of the desired video, is then fed into a transformer network. This network processes the latent video and text data, capturing complex spatial and temporal relationships to generate a new video sequence. Finally, a video decoder reconstructs the video from the transformer's output.  The entire pipeline shows how textual input can be used to control the generation of coherent and high-quality video content using the power of transformers and latent space encoding.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x3.png", "caption": "Figure 3: The visualization of the attention distribution of each frame\u2019s token across the entire token sequence. The results highlight significant variations in attention distributions across layers, with deeper layers focusing more on tokens from the same frame and exhibiting weaker attention to tokens from other frames.", "description": "This figure visualizes how attention mechanisms in a transformer network for video generation distribute attention across different frames within a video sequence.  The visualization shows the attention distribution for each frame's tokens across all tokens in the sequence, separately for different layers of the transformer.  Key observations are: significant variation in attention patterns across layers, and a trend towards deeper layers focusing more strongly on tokens from the same frame (higher self-attention), with reduced attention to tokens from other frames. This illustrates that different layers of the network capture different aspects of the visual information, but also highlights a potential limitation in how the model integrates temporal information between frames.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x4.png", "caption": "Figure 4: The visualization of attention maps across transformer layers. Each layer attends to distinct regions, capturing diverse spatial features. However, the lack of coordination across layers results in fragmented feature representations, weakening the model\u2019s ability to establish coherent spatial semantics within individual frames.", "description": "This figure visualizes attention maps across multiple layers of a transformer network used in a video generation model.  Each layer's attention map highlights different spatial regions within the input frame, indicating a focus on diverse features.  However, a lack of coordination or consistency in attention patterns across layers leads to fragmented representations of the spatial scene. This fragmentation results in difficulties for the model in generating coherent and semantically meaningful spatial relationships between objects and elements within the frames.  In essence, the attention mechanisms in different layers aren't effectively working together, which negatively impacts the overall spatial coherence of the generated video frames.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x5.png", "caption": "Figure 5: The average similarity between adjacent frame features across diffusion layers and denoising steps. The similarity decreases as layer depth increases for a given denoising step, indicating greater differentiation in deeper layers. Additionally, similarity between adjacent frames declines as the denoising process progresses.", "description": "This figure displays the average similarity between features of adjacent frames across different layers of a diffusion model during its denoising process.  The x-axis represents the layer number, while the y-axis shows the average similarity. Multiple lines represent different denoising steps.  The key observation is the decreasing similarity both as the layer depth increases (within a given denoising step) and as the denoising process progresses (across different steps). This decline suggests increased feature differentiation at deeper layers, and reduced temporal coherence between adjacent frames due to the denoising process.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x6.png", "caption": "Figure 6: The comparison of the original feature maps from a standard transformer layer with those obtained after aggregation in the Feature Cache Module. The aggregated features demonstrate more comprehensive semantic information and clearer structural details.", "description": "This figure shows a comparison of feature maps from a standard transformer layer and a modified transformer layer with a Feature Cache Module. The standard layer's feature map is less coherent and detailed. In contrast, the Feature Cache Module aggregates features from multiple layers, resulting in a feature map that shows clearer structural details and richer semantic information. This demonstrates the effectiveness of the Feature Cache Module in generating better video representations. ", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x7.png", "caption": "Figure 7: The comparison of adjacent frame similarity between original and aggregated features. The aggregated features from the Feature Cache Module exhibit higher similarity between adjacent frames compared to the original transformer layers, indicating improved temporal coherence.", "description": "Figure 7 displays a comparison of the similarity between adjacent frames' features, both before and after aggregation within the Feature Cache Module.  The original features, directly from the transformer layers, show a decrease in similarity between consecutive frames.  However, the aggregated features demonstrate significantly higher similarity, illustrating the effectiveness of the Feature Cache Module in enhancing temporal coherence within generated videos.  This improvement in temporal coherence is crucial for generating smoother and more realistic video sequences.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x8.png", "caption": "Figure 8: The architecture of the enhanced cross-layer\nrepresentation framework.", "description": "RepVideo enhances the video representation in text-to-video diffusion models by leveraging features from multiple adjacent transformer layers.  The architecture introduces a Feature Cache Module that aggregates features from these layers and a gating mechanism to combine the aggregated features with the original transformer input, creating enriched feature inputs for each transformer layer. This approach leads to more stable semantic information and improved temporal consistency.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x9.png", "caption": "Figure 9: The qualitative comparison between our method and the baseline CogVideoX-2B\u00a0[31]. The first row shows results from the baseline CogVideoX-2B\u00a0[31], while the second row presents the results generated by RepVideo, demonstrating significant improvements in quality and coherence.", "description": "This figure presents a qualitative comparison of video generation results between the baseline model, CogVideoX-2B, and the proposed RepVideo model.  Each row displays a sequence of frames generated from the same text prompt. The top row shows the output of CogVideoX-2B, illustrating issues such as blurriness, inconsistencies in object appearance, and jerky or unnatural movements. The bottom row shows the output from RepVideo, showcasing improved video quality, smoother transitions between frames, and more accurate rendering of the details described in the prompt. The examples illustrate the improved temporal coherence and spatial detail achieved by RepVideo.", "section": "IV. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08994/x10.png", "caption": "Figure 10: The Layer-wise comparison of feature maps between CogVideoX-2B and RepVideo. The comparison shows that RepVideo consistently captures richer semantic information and maintains more coherent spatial details across layers compared to CogVideoX-2B\u00a0[31].", "description": "Figure 10 presents a layer-by-layer comparison of feature maps generated by RepVideo and CogVideoX-2B, highlighting RepVideo's superior ability to capture detailed semantic information and maintain spatial consistency across different layers.  The images showcase how RepVideo's feature maps retain clear object structures and contours even at deeper layers, unlike CogVideoX-2B, where details blur and become less defined. This demonstrates RepVideo's enhanced ability to generate visually coherent and semantically rich videos.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08994/x11.png", "caption": "Figure 11: The comparison of attention maps between CogVideoX-2B and RepVideo. The comparison demonstrates that RepVideo could maintain more consistent semantic relationship compared to CogVideoX-2B\u00a0[31].", "description": "Figure 11 visually compares attention maps generated by RepVideo and CogVideoX-2B, highlighting RepVideo\u2019s enhanced ability to maintain consistent semantic relationships across different parts of the generated videos.  The attention maps illustrate how the models focus on different aspects of the scene, with RepVideo showing more coherent and less fragmented attention patterns.", "section": "V. DISCUSSION"}, {"figure_path": "https://arxiv.org/html/2501.08994/x12.png", "caption": "Figure 12: The cosine similarity between consecutive frames across layers.", "description": "This figure visualizes the temporal consistency of video generation across different layers of a transformer network.  It plots the cosine similarity between consecutive frames for various layers of the network.  Higher cosine similarity indicates stronger temporal coherence (smoother transitions between frames), while lower similarity suggests weaker coherence (more abrupt changes or inconsistencies).  The x-axis represents the frame index, and the y-axis shows the cosine similarity.  The plot allows comparison of temporal consistency across different layers and helps assess how the model's ability to maintain temporal coherence changes as the generated video progresses through the network.", "section": "IV. EXPERIMENTS"}]