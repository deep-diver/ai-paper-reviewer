{"importance": "This paper is crucial for researchers in information retrieval and multimodal learning.  **It introduces MMDocIR, a novel benchmark dataset specifically designed for evaluating multi-modal document retrieval systems.** This addresses a significant gap in the field, offering a robust and comprehensive evaluation tool. The findings, particularly the superior performance of visual retrievers, **will significantly impact the design and development of future multi-modal retrieval systems.**  The benchmark's release also opens new avenues for research into long document understanding and fine-grained layout retrieval.", "summary": "MMDocIR benchmark paves the way for better multi-modal document retrieval by offering a large-scale, expertly-annotated dataset that outperforms text-only methods.", "takeaways": ["MMDocIR benchmark dataset significantly improves multi-modal document retrieval evaluation.", "Visual-based retrieval methods consistently outperform text-based counterparts.", "The dual-task framework (page and layout retrieval) offers more granular evaluation of systems performance. "], "tldr": "Current benchmarks for multi-modal document retrieval lack sufficient document diversity, high-quality questions, and granular evaluation.  This limits the reliable assessment of retrieval system performance across various document types.  The existing benchmarks also primarily use questions repurposed from Visual Question Answering (VQA) tasks, which are not always suitable for evaluating retrieval methods.\nTo address these issues, this paper introduces MMDocIR, a new benchmark dataset with two tasks: page-level and layout-level retrieval.  **MMDocIR comprises 313 lengthy documents, 1,685 expertly-annotated questions, and bootstrapped labels for 173,843 additional questions.**  The experiments show that visual retrievers vastly outperform text-based counterparts, highlighting the importance of visual information in document retrieval.  **The dataset includes both page-level and layout-level annotations, allowing for more fine-grained analysis.**  The findings provide valuable insights into the strengths and weaknesses of various retrieval approaches and advance the field considerably.", "affiliation": "Noah's Ark Lab, Huawei", "categories": {"main_category": "Multimodal Learning", "sub_category": "Cross-Modal Retrieval"}, "podcast_path": "2501.08828/podcast.wav"}