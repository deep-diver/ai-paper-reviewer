{"importance": "This paper is crucial because **it addresses the critical need for robust benchmarks in multi-modal document retrieval**.  Existing benchmarks are insufficient, hindering effective evaluation of systems.  MMDocIR provides a comprehensive, expertly annotated dataset, advancing the field and enabling more accurate comparisons of retrieval methods. This opens avenues for future research in multi-modal understanding and retrieval techniques, impacting various applications.", "summary": "MMDocIR, a new benchmark dataset, enables better evaluation of multi-modal document retrieval systems by providing page-level and layout-level annotations for diverse long documents and questions.", "takeaways": ["MMDocIR offers a new benchmark dataset for evaluating multi-modal document retrieval systems.", "Visual-based retrieval methods significantly outperform text-based ones in the benchmark.", "The benchmark includes both page-level and layout-level retrieval tasks, providing finer-grained evaluation."], "tldr": "Current multi-modal document retrieval benchmarks suffer from issues like poor question quality, incomplete documents, and limited retrieval granularity.  These shortcomings hinder accurate evaluation and progress in this important area. \n\nTo address this, the researchers introduce MMDocIR, a new benchmark dataset with two key tasks: page-level and layout-level retrieval.  **MMDocIR features 313 long documents across 10 diverse domains, along with 1685 expertly-annotated questions**.  The results show that visual-based methods generally outperform text-based methods.  This benchmark significantly improves multi-modal document retrieval evaluation and guides future research directions.", "affiliation": "Noah's Ark Lab, Huawei", "categories": {"main_category": "Multimodal Learning", "sub_category": "Cross-Modal Retrieval"}, "podcast_path": "2501.08828/podcast.wav"}