{"importance": "This paper is crucial for researchers in information retrieval and computer vision.  **It introduces a novel benchmark, MMDocIR, addressing the limitations of existing datasets for multi-modal document retrieval**. This benchmark will significantly advance research by enabling more robust evaluation of retrieval systems, facilitating better model development and comparison. The findings regarding the superiority of visual-based retrievers also open exciting new avenues in multi-modal learning.", "summary": "MMDocIR: A new benchmark dataset revolutionizes multi-modal document retrieval by offering a dual-task framework (page and layout-level retrieval) and extensive, expertly annotated data.", "takeaways": ["MMDocIR benchmark dataset facilitates better evaluation of multi-modal document retrieval systems.", "Visual-based retrieval methods significantly outperform text-based methods.", "The dual-task framework (page and layout-level retrieval) provides more granular evaluation."], "tldr": "Current benchmarks for evaluating multi-modal document retrieval systems suffer from issues such as poor question quality, incomplete documents, and insufficient retrieval granularity.  These limitations hinder accurate assessment of system performance and limit the applicability of research findings to real-world scenarios.  \nTo address these issues, the researchers introduce MMDocIR, a novel benchmark dataset that features a dual-task framework encompassing both page-level and layout-level retrieval.  **MMDocIR includes a large dataset of lengthy documents, expertly-annotated questions, and page/layout-level annotations.**  Through rigorous experiments, the researchers demonstrate the effectiveness of their proposed benchmark, highlighting the superior performance of visual-based retrievers over their text-based counterparts and emphasizing the importance of incorporating visual elements into multi-modal document retrieval.", "affiliation": "Noah's Ark Lab, Huawei", "categories": {"main_category": "Multimodal Learning", "sub_category": "Cross-Modal Retrieval"}, "podcast_path": "2501.08828/podcast.wav"}