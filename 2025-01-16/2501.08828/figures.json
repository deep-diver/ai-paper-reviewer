[{"figure_path": "https://arxiv.org/html/2501.08828/x1.png", "caption": "Figure 1. MMDocIR comprises 313 lengthy documents across 10 different domains, along with 1,685 questions. For each question, page-level annotations are provided via selected screenshots. Red boundary boxes represent layout-level annotations.", "description": "This figure illustrates the MMDocIR dataset, a benchmark for multi-modal document retrieval.  The dataset contains 313 long documents spanning 10 diverse domains.  Each document is paired with multiple questions (1685 in total).  Crucially, the dataset provides annotations at two levels: page-level, indicated by selected screenshots showing the relevant page for each question, and a finer-grained layout-level, where red boxes highlight specific layouts (like text blocks, figures, or tables) containing the answer within the relevant page.  This dual annotation scheme allows for more nuanced evaluation of multi-modal retrieval systems.", "section": "3 MMDocIR: Evaluation Set"}, {"figure_path": "https://arxiv.org/html/2501.08828/x2.png", "caption": "Figure 2. Area ratio of different modalities (1) in overall and (2) by domains in MMLongBench-Doc benchmark\u00a0(Ma et\u00a0al., 2024b). Note that the white spaces, headers, and footers are removed from the area counting.", "description": "This figure shows a comparison of the area ratios occupied by different modalities (text, images, tables) within documents from the MMLongBench-Doc benchmark. The analysis is presented both as an overall average across all domains and broken down by individual domains.  The visualization helps to understand the relative proportion of textual versus visual information in different types of documents, illustrating the multi-modal nature of the benchmark dataset and highlighting the need for multi-modal retrieval methods. Note that the calculations exclude whitespace, headers, and footers.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.08828/x3.png", "caption": "(a) Avg word length", "description": "Figure 3 shows a comparison of the average word length and the distribution of word length between OCR-extracted text and VLM-generated text from tables and figures within the MMDocIR dataset.  The subfigure (a) displays the average word length for each text type (OCR and VLM) across different visual elements (tables and images) within documents. Subfigure (b) presents the distribution of word lengths for both OCR and VLM text types. This figure visually demonstrates the significant difference in word length between OCR and VLM-generated text, particularly evident in the analysis of images.", "section": "Analysis of OCR and VLM Text"}, {"figure_path": "https://arxiv.org/html/2501.08828/x4.png", "caption": "(b) Distribution density of word length", "description": "The figure shows the distribution density of word length for OCR-text and VLM-text extracted from tables and figures in the MMDocIR dataset.  The x-axis represents the word length, and the y-axis shows the distribution density (the proportion of words having a given length).  The figure helps illustrate the difference in word length between text extracted directly by OCR and text generated by a vision-language model (VLM) describing the visual content of tables and figures.", "section": "Analysis of OCR and VLM Text"}]