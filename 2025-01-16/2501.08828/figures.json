[{"figure_path": "https://arxiv.org/html/2501.08828/x1.png", "caption": "Figure 1. MMDocIR comprises 313 lengthy documents across 10 different domains, along with 1,685 questions. For each question, page-level annotations are provided via selected screenshots. Red boundary boxes represent layout-level annotations.", "description": "MMDocIR is a benchmark dataset for multi-modal document retrieval.  It contains 313 long documents spanning 10 diverse domains.  For each document, 1,685 questions were curated, and each question is annotated at two levels: page-level and layout-level. Page-level annotations indicate the relevant pages containing the answer, shown via screenshots in the figure. Layout-level annotations further pinpoint the specific layout elements (like paragraphs, tables, charts, or figures) within those pages, precisely identified by red bounding boxes in the figure. The dataset aims to facilitate the evaluation of retrieval systems that leverage both textual and visual information within documents.", "section": "3 MMDocIR: Evaluation Set"}, {"figure_path": "https://arxiv.org/html/2501.08828/x2.png", "caption": "Figure 2. Area ratio of different modalities (1) in overall and (2) by domains in MMLongBench-Doc benchmark\u00a0(Ma et\u00a0al., 2024b). Note that the white spaces, headers, and footers are removed from the area counting.", "description": "Figure 2 presents a visual representation of the area occupied by different data modalities within documents from the MMLongBench-Doc benchmark.  The figure is divided into two parts. (1) shows the overall distribution of text, images, tables, and other elements across all documents in the benchmark. (2) breaks down this distribution by specific document domains, such as research reports, administrative documents, and brochures. This allows for a comparison of modality ratios across different document types. Importantly, whitespace, headers, and footers have been excluded from the area calculations to provide a more accurate reflection of the content's composition.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2501.08828/x3.png", "caption": "(a) Avg word length", "description": "This figure shows a comparison of the average word length and word length distribution between OCR-extracted text and VLM-generated text for tables and images within the MMDocIR dataset.  The bar chart (a) presents the average word length for each text type (OCR and VLM) across tables and images. The histogram (b) displays the distribution of word lengths for both OCR and VLM-derived text, revealing differences in the frequency of various word lengths.", "section": "6.5 Analysis of OCR and VLM Text"}, {"figure_path": "https://arxiv.org/html/2501.08828/x4.png", "caption": "(b) Distribution density of word length", "description": "The figure shows the distribution density of word length in OCR and VLM text. It helps to visualize the difference in word length between text extracted by OCR and text generated by a vision-language model (VLM). The x-axis represents the word length, and the y-axis represents the distribution density. The figure indicates that VLM text generally has a longer word length than OCR text. This is possibly because VLM can generate more descriptive and complex sentences than OCR, which only extracts text from images.", "section": "Analysis of OCR and VLM Text"}]