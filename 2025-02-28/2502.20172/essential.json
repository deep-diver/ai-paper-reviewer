{"importance": "This research is pivotal as it **simplifies complex image manipulation tasks** by leveraging LMMs. The DREAM ENGINE framework introduces new methods for creative image generation, **improving efficiency and customization**, opening doors for further exploration in multimodal AI research.", "summary": "DREAM ENGINE: Text-image interleaved control made easy, unifying text and visual cues for creative image generation.", "takeaways": ["Large Multimodal Models can be easily adapted into the text encoder of text-to-image diffusion models without updating parameters.", "Achieve object-driven generation, combining object detection and captioning for compositional generation.", "Enable complex interwoven guidance from both text and images, resulting in highly customized outputs and state-of-the-art quality."], "tldr": "Current text-to-image generation models struggle with complex instructions that weave text and visuals. Though methods exist to add control signals like edges or depth maps, they lack flexibility for complex text-image instructions, like merging visual elements from multiple images using natural language. To solve this issue, the paper explores utilizing large multimodal models (LMMs) to use their capabilities for more flexible control.\n\nThe paper introduces DREAM ENGINE, a framework for text-image interleaved control in image generation models. It replaces text encoders with versatile multimodal encoders like QwenVL, and uses a two-stage training: text-image alignment and interleaved instruction tuning. This enables the model to generate images guided by text and image, designs a new object driven generation task to enable composition. Achieves a 0.69 score on GenEval.", "affiliation": "Peking University", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}, "podcast_path": "2502.20172/podcast.wav"}