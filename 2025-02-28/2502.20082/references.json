{"references": [{"fullname_first_author": "Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "publication_date": "2021-04-09", "reason": "This paper introduced the Rotary Position Embedding (RoPE), which is fundamental to the context window extension approach discussed in LongRoPE2."}, {"fullname_first_author": "Peng", "paper_title": "Yarn: Efficient context window extension of large language models", "publication_date": "2023-09-01", "reason": "YaRN is a prior ROPE rescaling method for context window extension that LongRoPE2 compares itself against and seeks to improve upon."}, {"fullname_first_author": "Jacot", "paper_title": "Neural tangent kernel: Convergence and generalization in neural networks", "publication_date": "2018-01-01", "reason": "This paper introduces the Neural Tangent Kernel (NTK) theory, which is used in NTK-based RoPE rescaling methods that LongRoPE2 uses and compares against."}, {"fullname_first_author": "Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-06-12", "reason": "This seminal work on Transformers is a fundamental building block for LLMs, the technology that LongRoPE2 improves upon."}, {"fullname_first_author": "Ding", "paper_title": "Longrope: Extending llm context window beyond 2 million tokens", "publication_date": "2024-02-13", "reason": "LongRoPE is the prior method by some of the same authors, which LongRoPE2 improves upon through a novel perspective and technical advancements."}]}