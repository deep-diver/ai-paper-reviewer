[{"figure_path": "https://arxiv.org/html/2502.16944/x1.png", "caption": "Figure 1: Overview of Decoupled Value Policy Optimization (DVPO) and PPO in RLHF. DVPO eliminates the need for a reward model and decouples policy and value learning during policy optimization. In contrast, PPO requires training a reward model before policy optimization. DVPO instead trains a global value model using the same offline data as the reward model. During policy training, no additional ground-truth rewards are obtained.", "description": "This figure compares the architectures of Decoupled Value Policy Optimization (DVPO) and Proximal Policy Optimization (PPO) in the context of Reinforcement Learning from Human Feedback (RLHF).  It highlights that DVPO replaces the need for a separate reward model with a pre-trained global value model, decoupling the policy and value learning phases.  In contrast, PPO requires joint training of both a policy and a value function, guided by a separately trained reward model.  Both methods utilize offline data, but DVPO doesn't require any additional ground truth reward signals during policy optimization.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.16944/x4.png", "caption": "Figure 2: Results of the model on the Ultrafeedback held-out testset. We employed GPT4o as a judge to assess the quality of model-generated responses. Performance is measured using the win rate, where Left represents DVPO, and Right represents the baseline model for comparison.", "description": "This figure shows the results of the Decoupled Value Policy Optimization (DVPO) model and a baseline model on the Ultrafeedback held-out test set.  GPT-4 was used to judge the quality of the generated responses. The performance is evaluated by comparing the win rates of DVPO against the baseline model.  A win is recorded if the DVPO-generated response is judged to be better than the baseline response. The figure visually represents the win rates, clearly indicating the superiority of the DVPO model.", "section": "5. Experiment results"}]