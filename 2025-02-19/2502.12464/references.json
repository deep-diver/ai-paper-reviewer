{"references": [{"fullname_first_author": "Seanie Lee", "paper_title": "HarmAug: Effective data augmentation for knowledge distillation of safety guard models", "publication_date": "2025-00-00", "reason": "This paper proposes HarmAug, a data augmentation method specifically designed to improve the knowledge distillation of safety guard models, which is directly related to the core methodology of the main paper."}, {"fullname_first_author": "Patrick Chao", "paper_title": "Jailbreakbench: An open robustness benchmark for jailbreaking large language models", "publication_date": "2024-00-00", "reason": "This paper introduces Jailbreakbench, a benchmark dataset crucial for evaluating the robustness of safety guardrails against malicious attacks, providing essential context for the main paper's focus on safety and efficiency."}, {"fullname_first_author": "Seungju Han", "paper_title": "WildGuard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of LLMs", "publication_date": "2024-00-00", "reason": "This paper presents WildGuard, a comprehensive dataset and tooling for safety risk assessment in LLMs, forming the foundation for the experimental evaluation of the main paper."}, {"fullname_first_author": "AI @ Meta Llama Team", "paper_title": "The llama 3 family of models", "publication_date": "2024-00-00", "reason": "This paper introduces the Llama 3 family of models, which are central to the main paper's model selection methodology and experimental results."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This work details the reinforcement learning from human feedback (RLHF) approach used to train many LLMs, explaining the foundational training technique of the models used in this paper."}]}