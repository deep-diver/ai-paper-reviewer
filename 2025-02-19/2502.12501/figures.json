[{"figure_path": "https://arxiv.org/html/2502.12501/x1.png", "caption": "Figure 1: An overview of our method. By evaluating the candidate responses A/B alongside the crowd responses, the resulting crowd judgment can be used as context to enrich the evaluation of A/B responses, leading to a more comprehensive CoT judgment.", "description": "The figure illustrates the Crowd-based Comparative Evaluation (CCE) method.  Candidate responses (A and B) are evaluated not in isolation, but in comparison to additional crowd responses (C, D, etc.).  The judgments derived from comparing the candidate responses against the crowd responses provide richer contextual information. This information is then fed back into the LLM-as-a-Judge to produce a more comprehensive and detailed chain-of-thought (CoT) judgment for the original candidate responses.  Essentially, the crowd responses act as anchors, exposing deeper details and nuances within the candidate responses that the LLM may have otherwise missed.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.12501/x2.png", "caption": "Figure 2: Pipeline of our proposed crowd-based comparative evaluation. For a given instance (x,yA,yB)\ud835\udc65superscript\ud835\udc66\ud835\udc34superscript\ud835\udc66\ud835\udc35(x,y^{A},y^{B})( italic_x , italic_y start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ), we first use the LLM to generate crowd responses {yi|i\u2208{C,D,E,\u2026}}conditional-setsuperscript\ud835\udc66\ud835\udc56\ud835\udc56\ud835\udc36\ud835\udc37\ud835\udc38\u2026\\left\\{y^{i}|i\\in\\{C,D,E,...\\}\\right\\}{ italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | italic_i \u2208 { italic_C , italic_D , italic_E , \u2026 } } based on x\ud835\udc65xitalic_x. These responses are then compared with yAsuperscript\ud835\udc66\ud835\udc34y^{A}italic_y start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT and yBsuperscript\ud835\udc66\ud835\udc35y^{B}italic_y start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT to produce initial crowd judgments \ud835\udca5\ud835\udca5\\mathcal{J}caligraphic_J, which are subsequently refined into \ud835\udca5^^\ud835\udca5\\hat{\\mathcal{J}}over^ start_ARG caligraphic_J end_ARG after selection and processing. Finally, \ud835\udca5^^\ud835\udca5\\hat{\\mathcal{J}}over^ start_ARG caligraphic_J end_ARG are used as contextual input to evaluate the instance (x,yA,yB)\ud835\udc65superscript\ud835\udc66\ud835\udc34superscript\ud835\udc66\ud835\udc35(x,y^{A},y^{B})( italic_x , italic_y start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ).", "description": "This figure illustrates the Crowd-based Comparative Evaluation (CCE) process.  For a given task (x) and two candidate responses (yA and yB), the system first generates additional responses (crowd responses, yi) using an LLM. These crowd responses are then compared with the original candidate responses to create initial judgments (J). These judgments are refined (J^) through selection and processing steps that prioritize informative comparisons. Finally, the refined judgments are used as context for the LLM to produce a more comprehensive evaluation of the original candidates (yA, yB).", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.12501/x3.png", "caption": "Figure 3: Evaluation performance under scaling crowd judgments in the context. As the number of crowd judgments grows, both accuracy and CoT length generally increase.", "description": "This figure displays the impact of increasing the number of crowd judgments on the performance of the proposed Crowd-based Comparative Evaluation (CCE) method.  The x-axis represents the number of crowd judgments included, while the y-axis shows two key metrics: accuracy of the LLM-as-a-judge and the length of the chain-of-thought (CoT) reasoning produced by the LLM. The graphs illustrate that as more crowd judgments are considered (increasing context), both the accuracy of the evaluation and the detail level of the CoT reasoning generally improve.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.12501/x4.png", "caption": "Figure 4: CoT Comparison. CCE\u2019s CoT consistently yields a higher average number of key points and a higher coverage rate across all benchmarks.", "description": "Figure 4 presents a comparison of Chain-of-Thought (CoT) reasoning quality between the proposed Crowd-based Comparative Evaluation (CCE) method and the standard approach (Vanilla).  It uses two key metrics: the average number of key points identified in the CoT and the coverage rate.  The key points metric reflects how many distinct aspects or details the model considered when forming its judgment. The coverage rate indicates the proportion of relevant information in the input response that the model's CoT addressed.  Across all five benchmarks used for evaluation, CCE consistently outperformed the vanilla method in both of these metrics, demonstrating that CCE generates more comprehensive and in-depth CoTs.", "section": "4.3 Analysis Experiments"}, {"figure_path": "https://arxiv.org/html/2502.12501/x5.png", "caption": "Figure 5: Prompt of Our Method.", "description": "This figure displays the prompt used in the Crowd-based Comparative Evaluation (CCE) method.  The prompt instructs the LLM to act as an impartial judge, evaluating two AI-generated responses. It emphasizes holistic assessment, considering various criteria like helpfulness, relevance, accuracy, and completeness.  The prompt also instructs the LLM to implicitly consider characteristics of other responses (crowd responses) as a background context, further refining its judgment. Crucially, the prompt explicitly forbids the LLM from referencing these background responses directly in its final verdict. The prompt concludes by specifying the format for the final verdict: '[[A]]' or '[[B]]', indicating which assistant performed better.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.12501/x6.png", "caption": "Figure 6: Prompt of Vanilla LLM-as-a-Judge.", "description": "This figure displays the prompt used in the vanilla LLM-as-a-Judge method.  Unlike the more complex prompt used in the Crowd Comparative Evaluation method (CCE), the vanilla prompt is simpler and focuses on the basic comparison of the two responses based on general criteria such as helpfulness, relevance, accuracy, depth, creativity, and level of detail.  It instructs the LLM to choose the response that better answers the user's question, emphasizing objectivity and avoiding bias based on factors such as length, style, order of presentation, or AI assistant names.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.12501/x7.png", "caption": "Figure 7: Prompt of 16-Criteria LLM-as-a-Judge.", "description": "This figure displays the prompt used for the 16-criteria LLM-as-a-Judge evaluation method.  The prompt instructs the LLM to act as an impartial judge, evaluating two AI-generated responses based on sixteen specified criteria, which are listed explicitly in the prompt, encompassing aspects such as helpfulness, relevance, accuracy, depth, creativity, detail level, overall quality, readability, coherence, fluency, grammaticality, simplicity, adequacy, faithfulness, non-hallucination and complexity.  The LLM is directed to compare the responses, provide a short explanation, and render a final verdict indicating which response is superior, using a pre-defined format.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.12501/x8.png", "caption": "Figure 8: Prompt of LongPrompt LLM-as-a-Judge.", "description": "This figure shows the prompt used for the LongPrompt LLM-as-a-Judge baseline.  The prompt instructs the LLM to act as an impartial judge, comparing two AI-generated responses to a given question. It emphasizes a detailed explanation of the comparison, aiming for a comprehensive evaluation of aspects like helpfulness, accuracy, depth, and creativity.  The LLM is explicitly instructed to provide a long explanation.  The final verdict must follow the format \"[[A]]\" or \"[[B]]\", indicating the better response.", "section": "3 Methodology"}]