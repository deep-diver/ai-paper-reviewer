{"references": [{"fullname_first_author": "Agrawal, A.", "paper_title": "Taming throughput-latency tradeoff in LLM inference with sarathi-serve", "publication_date": "2024-00-00", "reason": "This paper introduces a novel serving system for LLMs that tackles the tradeoff between throughput and latency, which is highly relevant to the context of memory-efficient LLM inference."}, {"fullname_first_author": "Aminabadi, R. Y.", "paper_title": "Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale", "publication_date": "2022-00-00", "reason": "This paper presents Deepspeed-inference, a system that enables efficient inference of large transformer models, which addresses the memory challenges of large-scale model inference."}, {"fullname_first_author": "Bai, Y.", "paper_title": "Longbench: A bilingual, multitask benchmark for long context understanding", "publication_date": "2023-08-00", "reason": "This paper introduces LongBench, a benchmark specifically designed for evaluating the performance of LLMs on long context understanding tasks, providing a valuable tool for assessing memory-efficient LLM inference strategies."}, {"fullname_first_author": "Bai, Y.", "paper_title": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks", "publication_date": "2024-12-00", "reason": "This paper extends LongBench with more challenging tasks and longer contexts, offering a more comprehensive evaluation of LLMs' abilities to handle long sequences."}, {"fullname_first_author": "Sheng, Y.", "paper_title": "Flexgen: High-throughput generative inference of large language models with a single GPU", "publication_date": "2023-00-00", "reason": "This paper focuses on achieving high throughput during LLM inference, a crucial aspect of real-world deployment, making the efficient use of GPU resources especially important."}]}