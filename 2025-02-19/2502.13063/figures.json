[{"figure_path": "https://arxiv.org/html/2502.13063/x1.png", "caption": "Figure 1: \nHow many tokens fit into a single input vector? We estimate maximum number of tokens that can be decoded from a single input vector across various language models.", "description": "This figure shows the maximum number of tokens that different language models can process within a single input vector.  The x-axis represents the number of tokens in the input text, and the y-axis represents the model's capacity to handle those tokens.  Each bar represents a different language model, showing how many tokens each model can effectively process before encountering performance limitations. This illustrates the varying capacities of different language models in compressing text information into a single vector, highlighting the limitations and potential for improvement.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13063/extracted/6214616/imgs/compression_schema.drawio.png", "caption": "Figure 2: Compressing text into a [mem] vector. The pre-trained LLM is frozen, and we only finetune one or multiple [mem] vectors to decode the sequence of tokens [t1,t2,\u2026,tN]subscript\ud835\udc611subscript\ud835\udc612\u2026subscript\ud835\udc61\ud835\udc41[t_{1},t_{2},\\ldots,t_{N}][ italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_t start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]. [mem] vectors are trained for each text separately.", "description": "This figure illustrates the process of compressing a sequence of tokens into a smaller set of memory vectors using a pre-trained language model.  The pre-trained language model's weights are frozen; only the memory vectors are trainable parameters. These memory vectors are prepended to the input token sequence and serve as a compact representation to be processed by the frozen language model. The model is trained to predict the original token sequence from these memory vectors. Importantly, the memory vectors are trained independently for each text sequence, emphasizing the per-sample optimization approach used in this study.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2502.13063/x2.png", "caption": "Figure 3: \nInformation gain of text compression to [mem] vector doesn\u2019t depend on language understanding capabilities of models. Compression results for various language models show the relationship between the cross-entropy (CE) of the original and decompressed texts. If the text CE falls below a model-specific threshold (red line), the text is losslessly compressed. This value is a input vector capacity in terms of entropy (Information Gain, CHsubscript\ud835\udc36\ud835\udc3bC_{H}italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT).\nFor texts that are not perfectly compressed, the compression process reduces their CE to a consistent, model-specific value (bias of the black dashed line).\nLarger models (e.g., Llama-3.1-8B) can handle longer texts before reaching the compression threshold, due to their greater capacity compared to smaller models (e.g., Pythia-160M). This behavior holds for both natural texts (PG-19) and unnatural random texts consisting of random word sequences.", "description": "Figure 3 examines the relationship between the cross-entropy of original and compressed texts across various language models, demonstrating that compression capacity is independent of the model's language understanding capabilities.  The x-axis shows the original text's cross-entropy, and the y-axis represents the cross-entropy after compression.  Lossless compression occurs when the compressed text's cross-entropy falls below a model-specific threshold (red line), representing the model's information gain.  Texts above this line show a consistent cross-entropy reduction to a model-specific value (black dashed line). Larger models demonstrate a higher compression capacity than smaller models.  The same pattern holds true for both natural language texts and synthetic random text sequences.", "section": "4 Experiments and Results"}, {"figure_path": "https://arxiv.org/html/2502.13063/x3.png", "caption": "Figure 4: \nCompression scales linearly with the number of trainable [mem] vectors.\nDashed lines represent ideal linear scaling, and shaded regions indicate \u00b11plus-or-minus1\\pm 1\u00b1 1 std.\nPythia-160m reaches its maximum input context length of 2048 tokens and can successfully encode texts of up to 2016 tokens into 32 [mem] input vectors. Llama-3.2-1B can perfectly decode texts of 7168 tokens from just 16 input vectors.", "description": "This figure displays the linear relationship between the number of trainable memory vectors ([mem] vectors) used and the model's ability to compress and decompress text.  The dashed lines show the ideal linear scaling. Shaded areas represent the standard deviation (\u00b11 std).  For the Pythia-160M model, the maximum input context is 2048 tokens; it successfully encoded up to 2016 tokens with 32 [mem] vectors.  The Llama-3.2-1B model perfectly decoded texts of 7168 tokens using just 16 [mem] vectors, demonstrating that the compression capacity scales effectively with the number of trainable memory vectors.", "section": "4.4 Scaling Compression with More Trainable Vectors"}, {"figure_path": "https://arxiv.org/html/2502.13063/x4.png", "caption": "Figure 5: \nOnly fraction of learned input embedding information capacity can be utilized. Top. Maximum token capacity (see\u00a0Eq.\u00a01) against gain in correctly decoded tokens shows differences in utilization of learned memory embedding for studied models.\nBottom. Capacity utilization for natural and random texts.", "description": "Figure 5 analyzes the efficiency of different language models in using their input embedding capacity for text compression.  The top panel plots the theoretical maximum number of tokens that can be encoded in the input embedding (calculated using Equation 1) against the actual gain in correctly decoded tokens achieved through compression. This comparison reveals variations in how effectively different models utilize their embedding space. The bottom panel further examines capacity utilization by contrasting performance on natural language text with that of randomly generated text, providing insights into how model architecture and training data affect the capacity to represent text information in the input embeddings.", "section": "4 Experiments and Results"}, {"figure_path": "https://arxiv.org/html/2502.13063/x5.png", "caption": "Figure 6: \nInformation gain of text compression to [mem] vector doesn\u2019t depend on language understanding capabilities of models. Compression results for various language models show the relationship between the cross-entropy (CE) of the original and decompressed texts. If the text CE falls below a model-specific threshold (red line), the text is losslessly compressed. This value is a input vector capacity in terms of entropy (Information Gain, CHsubscript\ud835\udc36\ud835\udc3bC_{H}italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT). For texts that are not perfectly compressed, the compression process reduces their CE to a consistent, model-specific value (bias of the black dashed line).\nLarger models (e.g., Llama-3.1-8B) can handle longer texts before reaching the compression threshold, due to their greater capacity compared to smaller models (e.g., Pythia-160M). This behavior holds for both natural texts (PG-19) and unnatural random texts consisting of random word sequences.", "description": "Figure 6 illustrates the relationship between the cross-entropy of original and compressed texts for various language models.  The x-axis represents the cross-entropy of the original text, and the y-axis represents the cross-entropy after compression using a trainable memory vector.  The red line indicates a model-specific threshold: if the compressed text's cross-entropy falls below this line, the compression is lossless. The black dashed lines show the consistent, model-specific cross-entropy reduction for texts not perfectly compressed. Larger models demonstrate a greater capacity to handle longer texts before reaching this lossless compression threshold. This consistent behavior is observed for both naturally occurring and randomly generated texts.", "section": "Experiments and Results"}, {"figure_path": "https://arxiv.org/html/2502.13063/x6.png", "caption": "Figure 7: Intra/inter-sample embeddings cosine similarity. Empirical\nprobability densities of cosine similarity between intra-sample and inter-sample\nembeddings. Intra-sample similarities are measured between\nof the same sequence of tokens, while inter-sample between different\nones. Measured on GovReport Huang et\u00a0al. (2021) and Sheared-Llama-1.3B Xia et\u00a0al. (2024).", "description": "This figure displays the distribution of cosine similarity scores between different embeddings of the same text sequence (intra-sample) and between embeddings of different text sequences (inter-sample).  The cosine similarity measures how similar two vector representations are, with a score of 1 indicating identical vectors and 0 indicating orthogonal vectors. The distributions show that intra-sample similarities are not strongly clustered around a high similarity value (e.g. 1), indicating the model can generate diverse vector representations for the same input. Furthermore, the overlap between the intra-sample and inter-sample distributions suggests that the uniqueness of the embedding for a given input is not guaranteed and that there is a significant amount of variation in the representation of different input sequences.", "section": "D Understanding the Structure of Compressed Vectors"}, {"figure_path": "https://arxiv.org/html/2502.13063/x7.png", "caption": "Figure 8: Intra-sample Interpolation Accuracies. Interpolation lines\nare provided for all pairs between 32 embeddings of the same input sequence.\nAll interpolation lines are printed with high transparency\nto show denser regions. Grey lines depict minimums and maximums of the\naccuracy for a given interpolation parameter \u03b8\ud835\udf03\\thetaitalic_\u03b8.", "description": "Figure 8 visualizes the accuracy of reconstructing text sequences from linear interpolations between different embeddings of the same input sequence.  For each input sequence, 32 different embeddings were generated using the method described in the paper. Then, linear interpolations were performed between all pairs of these embeddings.  The plot shows the accuracy of reconstructing the original text at various points along these interpolation lines. The higher density of lines in certain areas indicates a higher concentration of interpolation points with similar reconstruction accuracy. The grey lines represent the minimum and maximum accuracies observed across all interpolations for a given interpolation parameter (\u03b8). This helps illustrate the variability in reconstruction accuracy across different interpolations between embeddings of the same sequence.", "section": "D Understanding the Structure of Compressed Vectors"}]