<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-02-19s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/</link><description>Recent content in 2025-02-19s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Tue, 18 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/index.xml" rel="self" type="application/rss+xml"/><item><title>Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13063/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13063/</guid><description>LLMs can losslessly compress 1568 tokens into a single vector, surpassing prior methods by two orders of magnitude.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13063/cover.png"/></item><item><title>Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12501/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12501/</guid><description>Crowd-based comparative evaluation significantly boosts LLM-as-a-judge accuracy by using crowd responses to expose deeper details, resulting in more reliable and efficient auto-evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12501/cover.png"/></item><item><title>Eager Updates For Overlapped Communication and Computation in DiLoCo</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12996/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12996/</guid><description>Eager updates drastically speed up training massive language models by cleverly overlapping communication and computation in DiLoCo, achieving near-optimal performance even with low bandwidth.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12996/cover.png"/></item><item><title>HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12574/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12574/</guid><description>HEADINFER achieves memory-efficient LLM inference by cleverly offloading key-value cache to the CPU, enabling 4 million token inference on a single consumer GPU.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12574/cover.png"/></item><item><title>Magma: A Foundation Model for Multimodal AI Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13130/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13130/</guid><description>Magma: a new foundation model for multimodal AI agents excels at bridging verbal and spatial intelligence, achieving state-of-the-art performance across various tasks, including UI navigation and robo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13130/cover.png"/></item><item><title>Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13145/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13145/</guid><description>mmMamba: a novel framework creates linear-complexity multimodal models via distillation, drastically improving efficiency without sacrificing performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13145/cover.png"/></item><item><title>PAFT: Prompt-Agnostic Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12859/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12859/</guid><description>PAFT dynamically adjusts prompts during LLM fine-tuning, improving model robustness and generalization across diverse prompts without sacrificing performance or efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12859/cover.png"/></item><item><title>Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12669/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12669/</guid><description>Perovskite-LLM: a new knowledge-enhanced system boosts perovskite solar cell research by integrating a domain-specific knowledge graph, high-quality datasets, and specialized LLMs for superior knowled&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12669/cover.png"/></item><item><title>Pre-training Auto-regressive Robotic Models with 4D Representations</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13142/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13142/</guid><description>ARM4R pre-trains autoregressive robotic models using low-level 4D representations from human videos, achieving efficient transfer learning and improved task performance across various environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13142/cover.png"/></item><item><title>RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12513/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12513/</guid><description>RealSyn: A new, scalable multimodal dataset revolutionizes vision-language learning by effectively using interleaved image-text documents.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12513/cover.png"/></item><item><title>Rethinking Diverse Human Preference Learning through Principal Component Analysis</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13131/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13131/</guid><description>Decomposed Reward Models (DRMs) extract diverse human preferences from binary comparisons using PCA, enabling flexible and interpretable LLM alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13131/cover.png"/></item><item><title>SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12464/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12464/</guid><description>SafeRoute efficiently enhances LLM safety by adaptively using smaller and larger safety guard models, maximizing accuracy while minimizing costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12464/cover.png"/></item><item><title>Atom of Thoughts for Markov LLM Test-Time Scaling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12018/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12018/</guid><description>Atom of Thoughts (AOT) revolutionizes LLM test-time scaling by decomposing complex reasoning into independent sub-questions, drastically reducing computation while maintaining high accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12018/cover.png"/></item><item><title>Continuous Diffusion Model for Language Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.11564/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.11564/</guid><description>RDLM: A novel continuous diffusion model for language modeling leverages the geometry of categorical distributions, outperforming existing discrete approaches and approaching autoregressive model perf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.11564/cover.png"/></item><item><title>FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.11433/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.11433/</guid><description>FLAG-TRADER fuses LLMs &amp;amp; RL for enhanced financial trading, achieving superior performance compared to traditional methods by efficiently integrating multimodal data and adapting to market dynamics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.11433/cover.png"/></item><item><title>Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12215/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12215/</guid><description>Contrary to popular belief, longer reasoning chains don&amp;rsquo;t always boost Large Language Model (LLM) accuracy; this research reveals that parallel scaling with shorter solutions outperforms sequential sc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12215/cover.png"/></item><item><title>FinMTEB: Finance Massive Text Embedding Benchmark</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10990/</link><pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10990/</guid><description>FinMTEB: A new benchmark reveals that general-purpose embedding models struggle in the finance domain; domain-specific models excel, and surprisingly, simple BoW outperforms sophisticated models on ce&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10990/cover.png"/></item><item><title>Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10708/</link><pubDate>Sat, 15 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10708/</guid><description>This survey paper comprehensively analyzes methods for injecting domain-specific knowledge into LLMs, categorizing them into four key approaches and evaluating their trade-offs to enhance performance &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10708/cover.png"/></item><item><title>Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10852/</link><pubDate>Sat, 15 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10852/</guid><description>XLM-SWCM: A novel framework efficiently adapts multilingual encoders for text generation in extremely low-resource languages by cleverly sharing weights between encoder and decoder, achieving superior&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10852/cover.png"/></item><item><title>HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.09838/</link><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.09838/</guid><description>HealthGPT: A novel medical vision-language model unifying comprehension and generation via heterogeneous knowledge adaptation, achieving superior performance on various medical tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.09838/cover.png"/></item><item><title>MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12170/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12170/</guid><description>MUDDFormer boosts Transformer performance by dynamically generating connection weights, improving cross-layer information flow and surpassing models trained with significantly more compute.</description></item><item><title>You Do Not Fully Utilize Transformer's Representation Capacity</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.09245/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.09245/</guid><description>Boosting Transformer performance, Layer-Integrated Memory (LIMe) enhances representation capacity by enabling access to earlier layers&amp;rsquo; hidden states, significantly improving performance across variou&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.09245/cover.png"/></item></channel></rss>