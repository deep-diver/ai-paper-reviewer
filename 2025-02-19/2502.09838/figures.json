[{"figure_path": "https://arxiv.org/html/2502.09838/x2.png", "caption": "Figure 1: HealthGPT enables medical multi-modal comprehension and generation, outperforming both state-of-the-art unified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling complex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation.", "description": "This figure showcases HealthGPT's capabilities in handling medical multi-modal tasks, including both comprehension and generation.  It demonstrates superior performance compared to other state-of-the-art models, both general-purpose unified visual models and those specifically designed for medical applications.  The results displayed highlight HealthGPT's ability to effectively address complex medical tasks.  The metrics 'Comp. Perf.' and 'Gen. Perf.' represent the performance scores achieved in the comprehension and generation tasks, respectively.  Various example tasks are shown, emphasizing the wide range of capabilities.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.09838/extracted/6211206/fig/HealthGPT_Framework.png", "caption": "Figure 2: With a fixed amount of comprehension (generation) data, increasing the proportion of the other type leads to significant performance degradation.", "description": "This figure shows the impact of data imbalance on the performance of a unified vision-language model.  Two graphs are presented, one showing comprehension performance and the other showing generation performance. Both graphs show performance on the y-axis and the ratio of comprehension to generation data on the x-axis. As the ratio of one type of data increases (while the amount of the other type is held constant), the model's performance on the task with less training data decreases significantly. This highlights a challenge in training unified models that perform both comprehension and generation tasks, where an optimal balance of data is needed for both tasks to achieve good performance on both.", "section": "Conflicts between Comprehension and Generation"}, {"figure_path": "https://arxiv.org/html/2502.09838/x3.png", "caption": "Figure 3: The HealthGPT architecture integrates hierarchical visual perception and H-LoRA, employing a task-specific hard router to select visual features and H-LoRA plugins, ultimately generating outputs with an autoregressive manner.", "description": "This figure illustrates the architecture of HealthGPT, a medical large vision-language model.  HealthGPT uses a hierarchical visual perception approach, processing images through multiple levels of a Vision Transformer (ViT) to extract both concrete (detailed) and abstract (semantic) visual features. These features are then routed to task-specific H-LoRA (Heterogeneous Low-Rank Adaptation) plugins. The H-LoRA plugins, designed for efficient parameter-tuning, are responsible for adapting the pre-trained language model to both comprehension and generation tasks.  A hard router dynamically selects the appropriate visual features (concrete or abstract) based on the task (comprehension or generation). The model then generates outputs (text or images) in an autoregressive manner.", "section": "4 HealthGPT"}, {"figure_path": "https://arxiv.org/html/2502.09838/x4.png", "caption": "Figure 4: Data statistics of VL-Health.", "description": "This figure shows the statistical distribution of data within the VL-Health dataset, a key component of the HealthGPT model.  Panel (a) illustrates the number of images and text-image pairs used for different types of medical visual tasks including fundus images, OCT, MRI, ultrasound, X-ray, microscopy, digital photography, dermoscopy, and tomography.  Different generation tasks such as image reconstruction and modality conversion are also represented. Panel (b) provides a breakdown of the number of data samples used for each specific dataset in the VL-Health dataset including IXI, MIMIC-CHEST-XRAY, and LLaVA-558k among others. This breakdown further categorizes the data into different task types such as comprehension and generation.", "section": "4.4 Training Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.09838/x5.png", "caption": "Figure 5: Performance comparison of LoRA, MoELoRA, and H-LoRA under different rank settings.", "description": "This figure compares the performance of three parameter-efficient fine-tuning (PEFT) methods: LoRA, MoELORA, and H-LoRA, across different rank settings.  The methods were used to adapt large language models for medical visual comprehension and generation tasks. The x-axis represents the rank (dimensionality of the low-rank matrices), and the y-axis shows the performance.  It demonstrates how the performance of each method changes as the rank increases, showing the trade-off between model size and performance.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2502.09838/x6.png", "caption": "Figure 6:  The loss visualization (a) and performance comparison (b) with respect to different visual perceptions.", "description": "This figure shows the impact of different visual perception strategies on the performance of the HealthGPT model.  Part (a) visualizes the training loss curves for comprehension and generation tasks, separately using either abstract or concrete visual features. The curves illustrate how the choice of features impacts the training process and potential conflicts between comprehension and generation. Part (b) presents a bar chart comparing the final performance (measured by some unspecified metric) for both comprehension and generation tasks using either abstract or concrete features. This comparison highlights the effectiveness of the chosen hierarchical visual perception approach, which utilizes both abstract and concrete features depending on the task.", "section": "4.2 Hierarchical Visual Perception"}, {"figure_path": "https://arxiv.org/html/2502.09838/x7.png", "caption": "Figure 7: Case study of report-to-CXR under different instructions. (a) shows a normal CXR image for comparison. (b) and (c) illustrate generated cases with varying severity and affected regions. The graffiti areas indicate abnormal conditions.", "description": "This figure presents a case study demonstrating the capabilities of the HealthGPT model in generating chest X-ray (CXR) images based on textual descriptions.  Panel (a) provides a baseline with a normal CXR image for comparison. Panels (b) and (c) show generated CXR images illustrating the model's ability to produce images reflecting different levels of severity and different affected regions within the lungs, as indicated by varying levels of pulmonary edema and pleural effusion. The highlighted areas in (b) and (c) represent the regions described as having abnormal conditions in the text prompts.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.09838/x8.png", "caption": "Figure 8: VL-Health dataset collection distribution.", "description": "This figure shows the distribution of data used to create the VL-Health dataset.  Panel (a) presents a bar chart illustrating the number of question-answer pairs and images for each of the datasets used for medical visual comprehension tasks (VQA-RAD, SLAKE, PathVQA, MIMIC-CXR-VQA, LLaVA-Med, PubMedVision).  Panel (b) displays a bar chart showing the number of image pairs used for various medical image generation tasks. These tasks include super-resolution, modality conversion (CT to MRI, MRI to CT), and image reconstruction. The datasets used for generation tasks are IXI, MIMIC-CHEST-XRAY, LLaVA-558k, and SynthRAD2023.", "section": "A.3 VL-Health"}, {"figure_path": "https://arxiv.org/html/2502.09838/x9.png", "caption": "Figure 9: Performance changes before and after the stage-2.", "description": "This figure displays a bar chart visualizing the performance changes in medical visual comprehension and generation tasks before and after the second stage of training. The x-axis represents the task type (comprehension or generation), and the y-axis represents the performance metric.  The bars show a comparison of performance before and after the introduction of the heterogeneous low-rank adaptation (H-LORA) in stage 2 of the training process, highlighting the impact of this stage on the model's ability to handle both tasks effectively.", "section": "Supplemental Experimental Results"}, {"figure_path": "https://arxiv.org/html/2502.09838/extracted/6211206/fig/MT.png", "caption": "Figure 10: (a) Proportion of model responses selected as the best in human evaluation. (b) Human Evaluation Dataset.", "description": "Figure 10 presents the results of a human evaluation comparing HealthGPT's performance to other LLMs on medical visual question answering tasks.  Part (a) is a pie chart showing the proportion of times each model's response was ranked as the best among multiple responses to the same question by human evaluators (clinicians). Part (b) briefly describes the human evaluation dataset used, which includes questions from VQA-RAD, SLAKE, and PathVQA.", "section": "C.4 Human Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.09838/extracted/6211206/fig/SR_CASE.png", "caption": "Figure 11: Case of modality transfer.", "description": "This figure showcases examples of modality transfer, specifically converting between CT and MRI scans.  It presents pairs of images for four different scenarios: brain CT to brain MRI, pelvic CT to pelvic MRI, brain MRI to brain CT, and pelvic MRI to pelvic CT.  Within each scenario, the original image, the prediction generated by the HealthGPT model, and the ground truth image are shown side-by-side for comparison. This visual comparison demonstrates the model's ability to accurately transform medical images between modalities.", "section": "5 Experiments"}]