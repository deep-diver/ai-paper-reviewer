[{"figure_path": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/figure1_showv7.png", "caption": "Figure 1: VideoWorld explores learning knowledge from unlabeled videos, ranging from task-specific rules to high-level reasoning and planning capabilities. Compared to other learning methods: reinforcement learning (RL), supervised learning (SL) and text-based learning, it offers three advantages: 1) better generalization with unified visual representation for various tasks and interfaces, 2) lower manual annotation burden, and 3) learning richer real-world information than text description.", "description": "The figure illustrates VideoWorld, a novel approach to learning knowledge directly from unlabeled video data.  It contrasts VideoWorld with traditional methods like reinforcement learning (RL), supervised learning (SL), and text-based learning.  The core idea is that VideoWorld learns complex knowledge, including task-specific rules, reasoning, and planning abilities, solely by observing videos.  The figure highlights three key advantages of VideoWorld:  First, its unified visual representation enables better generalization across various tasks and interfaces compared to RL and SL. Second, it significantly reduces the need for manual annotation, a significant advantage over SL and text-based methods. Third, learning directly from video data allows VideoWorld to acquire richer real-world information than methods relying solely on text.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/rep_space_v3.png", "caption": "Figure 2: Comparison of prediction targets.\n\u201cState\u201d, \u201cVideo\u201d and \u201cVideo w/ LDM\u201d refer to three different prediction targets: a state sequence (e.g., labeled positions of moves in Go), a raw video sequence, and a video sequence augmented with latent codes representing future visual changes (this approach is adopted by VideoWorld). \u201cAction-Value\u201d denotes the score for each move in the game, with details provided in Sec.\u00a04.2. By combining rich video information with a compact representation of visual changes, VideoWorld enables more effective learning.", "description": "The figure compares three different approaches to predicting the next move in a game of Go: using only the game state, using raw video, and using video enhanced with latent codes to represent future visual changes.  The graph shows that using video with latent dynamics improves learning efficiency.  The 'Action-Value' metric represents the quality of each move prediction.", "section": "3.2 Learning with Latent Dynamic Model"}, {"figure_path": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/overview_v6.png", "caption": "Figure 3: Overview of the proposed VideoWorld model architecture. (Left) Overall architecture. (Right) The proposed latent dynamics model (LDM). First, LDM compresses the visual changes from each frame to its subsequent H\ud835\udc3bHitalic_H frames into a set of latent codes. Then, an auto-regressive transformer seamlessly integrates the output of LDM with the next token prediction paradigm.", "description": "Figure 3 illustrates the architecture of VideoWorld, a novel video generation model designed for knowledge learning from unlabeled videos. The left panel shows the overall architecture, which comprises a VQ-VAE (Vector Quantized Variational Autoencoder) for encoding video frames into discrete tokens, and an autoregressive transformer for predicting the next token (or next frame) based on the previous tokens. The right panel focuses on the Latent Dynamics Model (LDM), a key component of VideoWorld. The LDM efficiently handles long-range dependencies in video sequences by first compressing the visual changes from each frame to its subsequent H frames into a set of latent codes and then seamlessly integrating these codes with the next token prediction paradigm of the autoregressive transformer. This two-stage process enhances both efficiency and effectiveness of the video generation and knowledge acquisition in VideoWorld.", "section": "3. VideoWorld"}, {"figure_path": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/umap_v4.png", "caption": "Figure 4: UMAP projection\u00a0[34] of the learned latent code on the Go (Left) and CALVIN (right) training set. Each point represents\nthe continuous (pre-quantization) latent code generated by the LDM. In Go examples, odd steps represent white\u2019s moves, and even steps represent black\u2019s moves. We visualize the latent codes of black moves in steps 2/4/6. The legend shows examples of common patterns learned for new black moves. For clarity, these moves are highlighted on the board with added colors and lines to indicate new patterns. On the right, we visualize the latent codes of the robotic arm\u2019s movement along the X/Y/Z axes at intervals of 1, 5, and 10 frames. Points are color-coded by displacement range, with purple and red indicating the maximum displacement in opposite directions along each axis.", "description": "This figure visualizes the latent codes learned by the Latent Dynamics Model (LDM) during training on Go and robotic manipulation tasks.  The left panel shows UMAP projections of latent codes from the Go game, where each point represents a latent code generated by the LDM before quantization. Odd steps correspond to white player moves and even steps to black player moves. Black moves in steps 2, 4, and 6 are highlighted to demonstrate common patterns for new black moves. These patterns are further clarified with additional color and lines on the board. The right panel shows UMAP projections of latent codes from the robotic arm's movement in the CALVIN dataset. Here, each point represents a latent code, and the points are color-coded according to the magnitude of displacement along the X, Y, and Z axes at intervals of 1, 5, and 10 frames. Purple and red colors indicate the maximum displacement in opposite directions.", "section": "3. VideoWorld"}, {"figure_path": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/umap_test_v5.png", "caption": "Figure 5: Illustration of playing against KataGO and UMAP projection\u00a0[34] of the predicted latent code. Our model plays as black. The generated latent code is visualized through the LDM decoder and new stones in the visualization are marked with colors to match the legend. The visualization serves as a probe, indicating that the model shows signs of forward planning.", "description": "This figure demonstrates a game of Go between the VideoWorld model and KataGO.  The VideoWorld model plays as black. The visualization uses UMAP to project the latent codes generated by the model's Latent Dynamics Model (LDM). The colors of the new stones placed on the Go board correspond to the colors of the projected latent codes, showing how the model's internal representation (latent codes) relates to its actions in the game. The clustering of the latent codes suggests that the model is not just reacting to the immediate game state but also considering future possibilities, demonstrating forward planning capabilities.", "section": "4. Video-GoBench"}, {"figure_path": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/umap_calvin.png", "caption": "(a)", "description": "This figure shows the comparison of prediction targets. It compares three different prediction targets: a state sequence (e.g., labeled positions of moves in Go), a raw video sequence, and a video sequence augmented with latent codes representing future visual changes. The x-axis represents the number of seen samples and the y-axis represents the Action-Value. It demonstrates that VideoWorld, by combining rich video information with a compact representation of visual changes, achieves superior training efficiency compared to using only state or video information.", "section": "3. VideoWorld"}, {"figure_path": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/supp_vis_cap.png", "caption": "(b)", "description": "This figure shows the results of an ablation study on the compression length of the latent dynamics model (LDM) in the CALVIN environment.  The x-axis represents different compression lengths (H), indicating how many future frames are compressed into a latent code. The y-axis shows the task success rate for three robotic manipulation tasks: Push, Open/Close, and Turn On/Off. The baseline represents the performance without LDM. Different compression lengths were tested, revealing the optimal H value for each task that balances compression and information retention for effective learning.", "section": "3.2 Learning with Latent Dynamic Model"}, {"figure_path": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/supp_vis_sac.png", "caption": "(c)", "description": "The figure shows the results of intervening latent codes with different indices.  It demonstrates the impact of altering latent codes at different time steps on the model's performance. By replacing latent codes with random tokens, the experiment shows how altering earlier codes (those representing immediate next steps) has a greater effect than altering later codes. This highlights the importance of the causal relationships and temporal ordering of information within the latent code sequence for effective reasoning and task completion.", "section": "3.2 Learning with Latent Dynamic Model"}, {"figure_path": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/supp_vis_calvin_3.png", "caption": "(d)", "description": "The figure shows the ablation study of different codebook sizes in the latent dynamics model (LDM) on the performance of Go and CALVIN tasks.  The results demonstrate how different codebook sizes (729, 15625, 64000, and 262144) impact the model's ability to learn and achieve high accuracy in both Go and CALVIN tasks, showcasing the importance of selecting an appropriate codebook size for effective knowledge acquisition.", "section": "3.2. Learning with Latent Dynamic Model"}, {"figure_path": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/supp_vis_rlbench.png", "caption": "(e)", "description": "This ablation study investigates the impact of the data source on the performance of the VideoWorld model.  It compares the model's performance using only human-generated Go data, only KataGo-generated data, and a combination of both. The results demonstrate how different data sources affect the model's ability to learn and perform the game, highlighting the role of data quality and diversity in knowledge acquisition.", "section": "5.4. Results with LDM"}, {"figure_path": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/state_count.png", "caption": "Figure 6: Illustration of robotic manipulation and UMAP projection of the predicted latent code during inference. Latent codes are visualized through the LDM decoder. The UMAP projection illustrates the 9 predicted latent codes (i.e. H=9\ud835\udc3b9H=9italic_H = 9) across different tasks, with each point color-coded by task type. Visualizations with a yellow background show the model\u2019s actual robotic arm control during inference, while those with a green background represent the model\u2019s next-frame predictions during training.", "description": "This figure visualizes the latent codes generated by the Latent Dynamics Model (LDM) during inference for robotic manipulation tasks.  The UMAP projection shows how these latent codes (9 codes representing 9 future time steps, H=9) cluster based on the task being performed.  Each point in the UMAP plot represents a latent code, and the color indicates the specific task.  The images on the right side show the model's actions. Yellow-background images depict the actual robotic arm movements during inference. Green-background images show the model's predictions of the next frames while it was training, illustrating its planning ability.", "section": "5.5. Understanding Learned Knowledge with LDM"}, {"figure_path": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/rep.png", "caption": "(a)", "description": "This figure shows the UMAP projection of the learned latent codes on the Go (left) and CALVIN (right) training sets. Each point represents the continuous (pre-quantization) latent code generated by the LDM.  The Go visualizations show odd steps representing white's moves and even steps representing black's moves.  The legend shows examples of common patterns learned for new black moves; these are highlighted on the board with colors and lines. The CALVIN visualizations show the latent codes of robotic arm movements along the X, Y, and Z axes at different frame intervals. Points are color-coded by displacement range, with purple and red indicating maximum displacement in opposite directions.", "section": "3. VideoWorld"}]