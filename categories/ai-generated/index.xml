<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI Generated on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/categories/ai-generated/</link><description>Recent content in AI Generated on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Thu, 24 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/categories/ai-generated/index.xml" rel="self" type="application/rss+xml"/><item><title>CAMEL-Bench: A Comprehensive Arabic LMM Benchmark</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18976/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18976/</guid><description>CAMEL-Bench: A new Arabic LMM benchmark with 29K+ questions across 8 diverse domains, revealing significant room for improvement in Arabic LLM development.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18976/cover.png"/></item><item><title>CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18505/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18505/</guid><description>CCI3.0-HQ: A new, high-quality 500GB Chinese dataset significantly improves large language model performance, outperforming existing datasets on multiple benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18505/cover.png"/></item><item><title>Data Scaling Laws in Imitation Learning for Robotic Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18647/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18647/</guid><description>Robotic manipulation policies achieve near-human performance using surprisingly little data when training with diverse environments and objects, demonstrating power-law scaling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18647/cover.png"/></item><item><title>DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18860/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18860/</guid><description>DeCoRe: A novel, training-free decoding method significantly reduces LLM hallucinations by contrasting outputs from masked and unmasked retrieval heads, boosting accuracy on various tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18860/cover.png"/></item><item><title>Distill Visual Chart Reasoning Ability from LLMs to MLLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18798/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18798/</guid><description>Code-as-Intermediary Translation synthesizes chart Q&amp;amp;A data, boosting MLLM visual reasoning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18798/cover.png"/></item><item><title>Framer: Interactive Frame Interpolation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18978/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18978/</guid><description>FRAMER lets users creatively control video frame interpolation by adjusting keypoint trajectories, producing smooth transitions and handling complex scenarios with an &amp;lsquo;autopilot&amp;rsquo; mode for automated tr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18978/cover.png"/></item><item><title>LOGO -- Long cOntext aliGnment via efficient preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18533/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18533/</guid><description>LOGO optimizes long-context model alignment via efficient preference optimization, achieving comparable performance to GPT-4 on real-world tasks with only 0.3B training data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18533/cover.png"/></item><item><title>MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18977/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18977/</guid><description>MotionCLR enables training-free interactive human motion editing by leveraging attention mechanisms for versatile generation and explainable control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18977/cover.png"/></item><item><title>Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18775/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18775/</guid><description>VINE, a novel watermarking method, significantly improves robustness against image editing by leveraging a pretrained diffusion model and blurring distortions as surrogate attacks during training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18775/cover.png"/></item><item><title>Should We Really Edit Language Models? On the Evaluation of Edited Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18785/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18785/</guid><description>Model editing boosts LLMs, but scaling edits hurts performance and safety!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18785/cover.png"/></item><item><title>Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18451/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18451/</guid><description>Skywork-Reward boosts LLM reward modeling by curating a smaller, high-quality preference dataset and employing advanced training techniques, achieving top performance on RewardBench.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18451/cover.png"/></item><item><title>SMITE: Segment Me In TimE</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18538/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18538/</guid><description>SMITE: a novel video segmentation method using few reference images to achieve temporally consistent, flexible-granularity segmentations outperforming state-of-the-art.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18538/cover.png"/></item><item><title>Stable Consistency Tuning: Understanding and Improving Consistency Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18958/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18958/</guid><description>Stable Consistency Tuning (SCT) boosts image generation speed and quality in consistency models by reducing training variance and discretization errors, achieving new state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18958/cover.png"/></item><item><title>Taipan: Efficient and Expressive State Space Language Models with Selective Attention</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18572/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18572/</guid><description>Taipan: A new language model efficiently handles extremely long text sequences by cleverly combining a memory-efficient architecture with selective attention, outperforming current state-of-the-art mo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18572/cover.png"/></item><item><title>The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18441/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18441/</guid><description>This paper enhances generative AI&amp;rsquo;s Transformer models by applying probabilistic optimization to sub-word encoding, hyperparameter tuning, attention mechanisms, and quantization, improving efficiency &amp;hellip;</description></item><item><title>Unbounded: A Generative Infinite Game of Character Life Simulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18975/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18975/</guid><description>UNBOUNDED: a generative infinite game uses AI to create an open-ended character life simulation, transcending traditional game boundaries.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18975/cover.png"/></item><item><title>Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18693/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18693/</guid><description>ScaleQuest generates 1M high-quality math problems from scratch using small LLMs, boosting open-source model performance by 29.2%-46.4% on MATH.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18693/cover.png"/></item><item><title>WAFFLE: Multi-Modal Model for Automated Front-End Development</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18362/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18362/</guid><description>WAFFLE, a novel fine-tuning strategy, significantly improves UI design-to-HTML code generation by using structure-aware attention and contrastive learning, outperforming current methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18362/cover.png"/></item><item><title>Why Does the Effective Context Length of LLMs Fall Short?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18745/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18745/</guid><description>Boosting LLMs&amp;rsquo; long-context performance, STRING, a training-free method, shifts position embeddings to overcome undertraining on long-range dependencies, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18745/cover.png"/></item><item><title>ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17779/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17779/</guid><description>ADEM-VL boosts vision-language model efficiency by using parameter-free cross-attention and adaptive fusion, significantly improving accuracy while reducing computational cost.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17779/cover.png"/></item><item><title>Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18252/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18252/</guid><description>Boosting language model training speed by 40%, this research introduces asynchronous RLHF, separating sample generation and learning for greater efficiency and compute optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18252/cover.png"/></item><item><title>DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18084/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18084/</guid><description>DynamicCity generates high-quality, large-scale 4D LiDAR scenes from dynamic environments, enabling diverse downstream applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18084/cover.png"/></item><item><title>Lightweight Neural App Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17883/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17883/</guid><description>LiMAC, a novel mobile app control architecture, boosts action accuracy by up to 42% and speeds up execution 30 times using a lightweight transformer and fine-tuned VLM.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17883/cover.png"/></item><item><title>MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17637/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17637/</guid><description>MIA-DPO efficiently aligns large vision-language models with human preferences on multi-image tasks, significantly boosting performance while maintaining single-image capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17637/cover.png"/></item><item><title>Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18234/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18234/</guid><description>This paper proposes a novel two-step approach to multi-draft speculative sampling, improving large language model decoding efficiency and achieving higher token rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18234/cover.png"/></item><item><title>Scalable Ranked Preference Optimization for Text-to-Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18013/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18013/</guid><description>Researchers created a scalable method for training text-to-image models using synthetically generated ranked preferences, improving both image quality and prompt adherence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18013/cover.png"/></item><item><title>Scaling Diffusion Language Models via Adaptation from Autoregressive Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17891/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17891/</guid><description>Scaling text diffusion models is achieved via adapting pre-trained autoregressive models, resulting in competitive performance on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17891/cover.png"/></item><item><title>TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18071/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18071/</guid><description>TP-Eval enhances MLLM evaluation by customizing prompts for each model, reducing bias and revealing true capabilities previously hidden by prompt sensitivity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18071/cover.png"/></item><item><title>Value Residual Learning For Alleviating Attention Concentration In Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17897/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17897/</guid><description>ResFormer and SVFormer mitigate attention concentration in deep Transformers, improving performance and reducing KV cache by almost half.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17897/cover.png"/></item><item><title>WorldSimBench: Towards Video Generation Models as World Simulators</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18072/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18072/</guid><description>WorldSimBench: A dual evaluation framework for video generation models, assessing visual quality and action consistency in embodied scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18072/cover.png"/></item><item><title>ZIP-FIT: Embedding-Free Data Selection via Compression-Based Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18194/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18194/</guid><description>ZIP-FIT revolutionizes data selection for language models by using gzip compression to identify task-aligned data, drastically improving model performance and training efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18194/cover.png"/></item><item><title>Aligning Large Language Models via Self-Steering Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17131/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17131/</guid><description>Self-Steering Optimization (SSO) autonomously generates high-quality preference signals for aligning LLMs, eliminating manual annotation and significantly improving performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17131/cover.png"/></item><item><title>Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17243/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17243/</guid><description>Inf-CL breaks the memory barrier in contrastive learning, enabling near-infinite batch size training with linear memory scaling and maintaining accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17243/cover.png"/></item><item><title>Frontiers in Intelligent Colonoscopy</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17241/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17241/</guid><description>This study pioneers multimodal AI for colonoscopy, creating a large-scale dataset (ColonINST), a language model (ColonGPT), and a benchmark to improve colorectal cancer detection.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17241/cover.png"/></item><item><title>JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17250/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17250/</guid><description>JMMMU, a new benchmark, rigorously evaluates large multimodal models&amp;rsquo; Japanese language and cultural understanding, revealing significant performance gaps and highlighting the need for culturally dive&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17250/cover.png"/></item><item><title>LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17434/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17434/</guid><description>LongVU efficiently processes hour-long videos for video-language understanding by adaptively compressing spatiotemporal redundancies, surpassing existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17434/cover.png"/></item><item><title>LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17242/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17242/</guid><description>LVSM: A revolutionary transformer-based model for novel view synthesis that outperforms existing methods by minimizing 3D inductive bias, achieving superior quality and scalability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17242/cover.png"/></item><item><title>Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16930/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16930/</guid><description>Math Neurosurgery precisely targets LLMs&amp;rsquo; math reasoning parameters via forward passes, boosting performance without harming other abilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16930/cover.png"/></item><item><title>MiniPLM: Knowledge Distillation for Pre-Training Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17215/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17215/</guid><description>MINIPLM boosts student language models&amp;rsquo; performance by efficiently refining the training data distribution using a teacher model, improving downstream task accuracy and reducing pre-training computati&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17215/cover.png"/></item><item><title>PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17247/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17247/</guid><description>PyramidDrop boosts large vision-language models&amp;rsquo; efficiency by 40% in training and 55% in inference, via a novel visual redundancy reduction method that selectively drops image tokens in deeper layers&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17247/cover.png"/></item><item><title>SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17249/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17249/</guid><description>SpectroMotion: Dynamic 3D scene reconstruction mastering specular surfaces, even in motion, using enhanced 3D Gaussian Splatting.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17249/cover.png"/></item><item><title>3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16266/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16266/</guid><description>3DGS-Enhancer boosts the quality of 3D Gaussian splatting, especially with sparse input views, by cleverly using 2D video diffusion priors to ensure consistent views.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16266/cover.png"/></item><item><title>Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16259/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16259/</guid><description>Agent-to-Sim (ATS) learns realistic 3D agent behaviors from casual videos by reconstructing a persistent 4D representation and training a generative model, enabling real-to-sim transfer for interactiv&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16259/cover.png"/></item><item><title>Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15748/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15748/</guid><description>Alchemy: A novel framework synthesizes formal theorems via symbolic mutation, significantly enhancing large language model theorem-proving performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15748/cover.png"/></item><item><title>AutoTrain: No-code training for state-of-the-art models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15735/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15735/</guid><description>AutoTrain: a no-code open-source library simplifies state-of-the-art model training for various tasks, democratizing AI development.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15735/cover.png"/></item><item><title>Can Knowledge Editing Really Correct Hallucinations?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16251/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16251/</guid><description>HalluEditBench: A new benchmark reveals knowledge editing&amp;rsquo;s true potential in fixing LLM hallucinations, highlighting strengths and limitations across various methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16251/cover.png"/></item><item><title>CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16256/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16256/</guid><description>CompassJudger-1: The first all-in-one open-source judge LLM for versatile and robust evaluation of large language models, improving efficiency and reproducibility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16256/cover.png"/></item><item><title>FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16271/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16271/</guid><description>FrugalNeRF achieves fast, high-fidelity 3D scene reconstruction from limited views without relying on pre-trained models, significantly reducing training time.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16271/cover.png"/></item><item><title>Improve Vision Language Model Chain-of-thought Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16198/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16198/</guid><description>Boosting VLM reasoning: New training data &amp;amp; reinforcement learning unlock superior chain-of-thought capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16198/cover.png"/></item><item><title>Language Models are Symbolic Learners in Arithmetic</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15580/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15580/</guid><description>LLMs learn arithmetic symbolically, not through calculation; they identify patterns, challenging the assumption of numerical computation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15580/cover.png"/></item><item><title>LLM-based Optimization of Compound AI Systems: A Survey</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16392/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16392/</guid><description>LLMs are revolutionizing compound AI optimization by efficiently handling complex parameters without gradient calculations, enabling end-to-end system tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16392/cover.png"/></item><item><title>Mitigating Object Hallucination via Concentric Causal Attention</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15926/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15926/</guid><description>Concentric Causal Attention (CCA) combats LVLMs&amp;rsquo; object hallucination by cleverly rearranging visual tokens to reduce the impact of RoPE&amp;rsquo;s long-term decay, significantly improving accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15926/cover.png"/></item><item><title>Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16153/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16153/</guid><description>PANGEA: A fully open multilingual, multimodal LLM outperforms existing models across diverse languages and cultural contexts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16153/cover.png"/></item><item><title>Pantograph: A Machine-to-Machine Interaction Interface for Advanced Theorem Proving, High Level Reasoning, and Data Extraction in Lean 4</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16429/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16429/</guid><description>Pantograph boosts machine-assisted theorem proving by creating a seamless Lean 4 interface for efficient proof search and high-level reasoning, facilitating advanced AI model training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16429/cover.png"/></item><item><title>Pre-training Distillation for Large Language Models: A Design Space Exploration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16215/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16215/</guid><description>Boosting LLMs&amp;rsquo; performance, this study systematically explores pre-training distillation, optimizing key aspects and revealing that larger student models significantly benefit from this technique.</description></item><item><title>RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16184/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16184/</guid><description>RM-BENCH, a new benchmark, rigorously evaluates reward models&amp;rsquo; sensitivity to subtle content and style, revealing significant room for improvement in current models and highlighting the importance of &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16184/cover.png"/></item><item><title>SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16268/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16268/</guid><description>SAM2Long dramatically improves long-video object segmentation by using a training-free memory tree, resolving error accumulation and achieving state-of-the-art results on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16268/cover.png"/></item><item><title>Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15633/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15633/</guid><description>GATEAU: Improve LLM long-context alignment by cleverly selecting influential training samples using Homologous Models&amp;rsquo; Guidance and Contextual Awareness Measurement.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15633/cover.png"/></item><item><title>Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15999/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15999/</guid><description>SPARE, a novel training-free method, uses sparse autoencoders to precisely control LLMs&amp;rsquo; knowledge selection, improving accuracy in open-domain question answering.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15999/cover.png"/></item><item><title>xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16267/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16267/</guid><description>BLIP-3-Video efficiently represents videos using only 32 tokens, achieving state-of-the-art accuracy in video question answering and captioning tasks with a smaller model size.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16267/cover.png"/></item><item><title>Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15460/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15460/</guid><description>New training method, SeND, reduces large language model hallucinations by up to 40% by selectively dropping unreliable neurons during training, improving accuracy and efficiency.</description></item><item><title>Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15316/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15316/</guid><description>Ichigo: a real-time voice assistant fusing speech &amp;amp; text via a unified transformer, achieving state-of-the-art performance with 111ms latency!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15316/cover.png"/></item><item><title>M-RewardBench: Evaluating Reward Models in Multilingual Settings</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15522/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15522/</guid><description>M-REWARDBENCH, a new multilingual benchmark, reveals significant performance gaps in reward models across languages, highlighting the need for improved multilingual alignment in LLMs.</description></item><item><title>Baichuan Alignment Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14940/</link><pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14940/</guid><description>Baichuan Alignment significantly boosts large language model performance via refined training, superior data strategies, enhanced capabilities, and rigorous evaluation, achieving up to 28% user experi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14940/cover.png"/></item><item><title>DM-Codec: Distilling Multimodal Representations for Speech Tokenization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15017/</link><pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15017/</guid><description>DM-Codec, a novel speech tokenizer, uses distillation to integrate acoustic, semantic, and contextual information, significantly improving speech tokenization accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15017/cover.png"/></item><item><title>How Many Van Goghs Does It Take to Van Gogh? Finding the Imitation Threshold</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15002/</link><pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15002/</guid><description>Researchers discover text-to-image models start reliably imitating concepts from training data after seeing 200-600 examples, offering crucial insights for copyright and privacy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15002/cover.png"/></item><item><title>Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14677/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14677/</guid><description>AI-generated text detectors struggle in real-world scenarios due to flawed evaluation datasets; this study proposes robust methods to evaluate these datasets and improve detector reliability.</description></item><item><title>BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14672/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14672/</guid><description>BiGR: a novel image generation model unifying generation and discrimination using compact binary codes, achieving superior performance and enabling zero-shot generalization across various vision tasks&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14672/cover.png"/></item><item><title>EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14649/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14649/</guid><description>EvoPress: A novel evolutionary algorithm optimizes dynamic LLM compression, achieving state-of-the-art accuracy and efficiency across pruning, sparsity, and quantization.</description></item><item><title>How Do Training Methods Influence the Utilization of Vision Models?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14470/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14470/</guid><description>Training methods dramatically alter which neural network layers are crucial for decision-making, revealing how different training strategies impact model efficiency and functionality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14470/cover.png"/></item><item><title>Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14208/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14208/</guid><description>MONTESSORI-INSTRUCT: A novel data synthesis framework tailors synthetic training data to student learning preferences, significantly improving student language model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14208/cover.png"/></item><item><title>NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14669/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14669/</guid><description>NaturalBench: A new VLM benchmark reveals critical weaknesses and enables continuous evaluation using naturally-occurring adversarial samples.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14669/cover.png"/></item><item><title>Teaching Models to Balance Resisting and Accepting Persuasion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14596/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14596/</guid><description>LLMs can be taught to both resist harmful and accept helpful persuasion, improving accuracy and teamwork.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14596/cover.png"/></item><item><title>ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13924/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13924/</guid><description>ARKit LabelMaker creates the largest real-world 3D dataset with dense semantic annotations, boosting 3D scene understanding models&amp;rsquo; performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13924/cover.png"/></item><item><title>CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13218/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13218/</guid><description>New CBT-BENCH benchmark rigorously evaluates LLMs&amp;rsquo; potential in assisting Cognitive Behavioral Therapy, revealing strengths and limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13218/cover.png"/></item><item><title>Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13394/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13394/</guid><description>CIA Suite enables reliable multilingual LLM evaluation by training an evaluator LLM on English references, achieving human-level performance across diverse languages.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13394/cover.png"/></item><item><title>DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13726/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13726/</guid><description>DAWN generates realistic talking-head videos at high speed using a novel non-autoregressive diffusion model, exceeding prior methods in quality and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13726/cover.png"/></item><item><title>Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13674/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13674/</guid><description>Image-guided diffusion models create a synthetic-to-real data spectrum for curriculum learning, boosting long-tail classification and low-quality image learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13674/cover.png"/></item><item><title>DPLM-2: A Multimodal Diffusion Protein Language Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13782/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13782/</guid><description>DPLM-2, a new multimodal protein model, simultaneously generates protein sequences and 3D structures, improving upon previous separate-modality approaches.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13782/cover.png"/></item><item><title>FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13925/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13925/</guid><description>FiTv2, an enhanced vision transformer, generates high-resolution images with diverse aspect ratios, exceeding existing models in speed and resolution adaptability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13925/cover.png"/></item><item><title>In-context learning and Occam's razor</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14086/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14086/</guid><description>In-context learning implicitly minimizes model complexity and training error, aligning with Occam&amp;rsquo;s Razor, thus improving generalization, especially in data-scarce scenarios.</description></item><item><title>Looking Inward: Language Models Can Learn About Themselves by Introspection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13787/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13787/</guid><description>LLMs can learn about themselves through introspection, outperforming other models in predicting their own behavior, as demonstrated by experiments with GPT-4 and other LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13787/cover.png"/></item><item><title>MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13370/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13370/</guid><description>MagicTailor empowers text-to-image models with component-level control, enabling precise visual concept personalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13370/cover.png"/></item><item><title>MedINST: Meta Dataset of Biomedical Instructions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13458/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13458/</guid><description>MedINST, a massive biomedical instruction dataset with 133 tasks and 7M samples, boosts LLM cross-task generalization in medical analysis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13458/cover.png"/></item><item><title>PUMA: Empowering Unified MLLM with Multi-granular Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13861/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13861/</guid><description>PUMA: A unified MLLM mastering diverse image generation &amp;amp; understanding through multi-granular visual features, balancing diversity and controllability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13861/cover.png"/></item><item><title>Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13184/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13184/</guid><description>Router-Tuning and MindSkip revolutionize Transformers: dynamic depth is achieved via efficient router fine-tuning, boosting speed and cutting training costs without compromising accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13184/cover.png"/></item><item><title>SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13276/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13276/</guid><description>SeerAttention learns attention sparsity, boosting LLMs&amp;rsquo; efficiency and scalability via a learnable gate and customized FlashAttention, achieving near-lossless accuracy with high sparsity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13276/cover.png"/></item><item><title>SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14745/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14745/</guid><description>SEMIEVOL boosts LLM performance by cleverly combining labeled and unlabeled data using a two-stage knowledge propagation and selection approach, achieving significant improvements across diverse tasks&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14745/cover.png"/></item><item><title>Steering Your Generalists: Improving Robotic Foundation Models via Value Guidance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13816/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13816/</guid><description>Boosting robot performance, Value-Guided Policy Steering (V-GPS) re-ranks actions from generalist policies using a value function, significantly improving task success across multiple robots and polic&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13816/cover.png"/></item><item><title>UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14059/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14059/</guid><description>UCFE benchmark dynamically evaluates LLMs&amp;rsquo; financial expertise via human-aligned, multi-round interactions, revealing performance discrepancies and highlighting the potential of mid-sized models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14059/cover.png"/></item><item><title>Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13232/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13232/</guid><description>Boosting web agents&amp;rsquo; efficiency, new World-Model-Augmented agents simulate action outcomes, improving policy selection and outperforming tree-search methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13232/cover.png"/></item><item><title>Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/</guid><description>KeyNMF, a novel topic modeling approach using transformer embeddings, reveals how Chinese diaspora media&amp;rsquo;s information dynamics correlate with major Western political events, highlighting PRC narrativ&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/cover.png"/></item><item><title>Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12788/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12788/</guid><description>Meta-Chunking boosts RAG performance by intelligently segmenting text into logically coherent chunks using LLMs, improving efficiency and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12788/cover.png"/></item><item><title>Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11190/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11190/</guid><description>Mini-Omni2: Open-source multimodal model rivaling GPT-40&amp;rsquo;s vision, speech, text, and duplex interaction capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11190/cover.png"/></item><item><title>SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11331/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11331/</guid><description>Shakti: a resource-efficient 2.5B parameter language model excels in edge AI, enabling high-performance NLP on low-resource devices.</description></item><item><title>HART: Efficient Visual Generation with Hybrid Autoregressive Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10812/</link><pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10812/</guid><description>HART: A hybrid autoregressive transformer rivals diffusion models in 1024x1024 image generation quality while achieving significantly higher throughput and lower latency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10812/cover.png"/></item></channel></rss>