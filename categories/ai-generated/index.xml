<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI Generated on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/categories/ai-generated/</link><description>Recent content in AI Generated on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Tue, 29 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/categories/ai-generated/index.xml" rel="self" type="application/rss+xml"/><item><title>Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21647/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21647/</guid><description>REPOCOD benchmark exposes current LLMs&amp;rsquo; struggles with real-world, complex code generation tasks, pushing the field towards building stronger, more context-aware models.</description></item><item><title>Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22304/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22304/</guid><description>Flow-DPO: Online multi-agent learning boosts LLM mathematical reasoning by collaboratively generating detailed, high-quality reasoning traces, surpassing single-model approaches.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22304/cover.png"/></item><item><title>Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21845/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21845/</guid><description>Human-in-the-loop RL enables robots to master complex manipulation tasks with near-perfect success rates and superhuman speed, exceeding prior methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21845/cover.png"/></item><item><title>Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22325/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22325/</guid><description>Pre-trained robots achieve higher manipulation success rates using a novel manipulation-centric representation (MCR) learned from large-scale robot datasets, surpassing baselines significantly.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22325/cover.png"/></item><item><title>Task Vectors are Cross-Modal</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22330/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22330/</guid><description>Vision-language models surprisingly use similar internal representations for similar tasks regardless of input type (image or text) or specification (example or instruction).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22330/cover.png"/></item><item><title>ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21465/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21465/</guid><description>SHADOWKV boosts long-context LLM inference throughput by up to 3.04x by cleverly caching low-rank keys on the GPU and offloading value caches to the CPU, minimizing latency while maintaining accuracy.</description></item><item><title>SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/</guid><description>SocialGPT uses vision and language foundation models for zero-shot social relation reasoning, achieving state-of-the-art results with improved interpretability via Greedy Segment Prompt Optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/cover.png"/></item><item><title>Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21242/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21242/</guid><description>ReDE-RF revolutionizes zero-shot dense retrieval by using relevance feedback from LLMs to refine query embeddings, achieving state-of-the-art results with vastly improved efficiency.</description></item><item><title>Accelerating Direct Preference Optimization with Prefix Sharing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20305/</link><pubDate>Sun, 27 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20305/</guid><description>Boosting LLM training speed by 1.3-1.6x, this research introduces &amp;lsquo;prefix sharing&amp;rsquo; for preference optimization, processing chosen and rejected responses as one sequence to remove redundancy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20305/cover.png"/></item><item><title>AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20424/</link><pubDate>Sun, 27 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20424/</guid><description>AutoKaggle, a novel multi-agent framework, automates Kaggle data science competitions with high accuracy, integrating LLM reasoning, iterative debugging, and a custom machine learning tools library.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20424/cover.png"/></item><item><title>Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21333/</link><pubDate>Sun, 27 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21333/</guid><description>Chain-of-thought prompting can hurt LLMs&amp;rsquo; performance on tasks where human deliberation worsens accuracy; this research identifies those tasks and offers a new tool for prompt design.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21333/cover.png"/></item><item><title>RARe: Retrieval Augmented Retrieval with In-Context Examples</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20088/</link><pubDate>Sat, 26 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20088/</guid><description>RARE enhances retrieval model accuracy by effectively integrating in-context examples, achieving up to +2.72% nDCG improvement.</description></item><item><title>Measuring memorization through probabilistic discoverable extraction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19482/</link><pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19482/</guid><description>&lt;strong>Researchers introduce probabilistic discoverable extraction&lt;/strong>, a novel approach improving LLM memorization measurement by considering probabilistic sampling and multiple extraction attempts, reveali&amp;hellip;</description></item><item><title>OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19609/</link><pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19609/</guid><description>&lt;strong>OpenWebVoyager: A novel open-source framework enables building multimodal web agents that iteratively learn from real-world exploration and feedback, achieving strong performance.&lt;/strong></description></item><item><title>CLEAR: Character Unlearning in Textual and Visual Modalities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18057/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18057/</guid><description>CLEAR benchmark enables effective evaluation of multimodal unlearning methods by offering a new dataset with textual and visual data, highlighting challenges, and demonstrating mitigation of catastrop&amp;hellip;</description></item></channel></rss>