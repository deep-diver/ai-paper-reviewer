<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI Generated on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/categories/ai-generated/</link><description>Recent content in AI Generated on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Thu, 24 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/categories/ai-generated/index.xml" rel="self" type="application/rss+xml"/><item><title>CAMEL-Bench: A Comprehensive Arabic LMM Benchmark</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18976/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18976/</guid><description>CAMEL-Bench: a new Arabic LMM benchmark enabling comprehensive evaluation of large multimodal models across eight diverse domains, revealing significant room for improvement even in state-of-the-art m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18976/cover.png"/></item><item><title>CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18505/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18505/</guid><description>CCI3.0-HQ: A new 500GB high-quality Chinese dataset boosts Chinese LLM performance, outperforming existing datasets on key benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18505/cover.png"/></item><item><title>Data Scaling Laws in Imitation Learning for Robotic Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18647/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18647/</guid><description>Robotic manipulation policies achieve near 90% success in novel environments and with unseen objects using a data-efficient imitation learning approach guided by discovered power-law scaling laws.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18647/cover.png"/></item><item><title>DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18860/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18860/</guid><description>DeCoRe, a training-free decoding strategy, significantly reduces LLM hallucinations by contrasting outputs from masked and unmasked retrieval heads, improving contextual faithfulness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18860/cover.png"/></item><item><title>Distill Visual Chart Reasoning Ability from LLMs to MLLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18798/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18798/</guid><description>Researchers created REACHQA, a dataset improving visual reasoning in LLMs by using code as an intermediary to translate chart representations into text, enabling efficient and scalable data synthesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18798/cover.png"/></item><item><title>Framer: Interactive Frame Interpolation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18978/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18978/</guid><description>Framer: a novel interactive frame interpolation method allows users to customize video transitions by intuitively adjusting keypoints, resulting in seamless and creative video generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18978/cover.png"/></item><item><title>LOGO -- Long cOntext aliGnment via efficient preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18533/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18533/</guid><description>LOGO, a novel training strategy, significantly enhances long-context language model generation by efficiently optimizing preferences, achieving performance comparable to GPT-4 on real-world tasks with&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18533/cover.png"/></item><item><title>MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18977/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18977/</guid><description>MotionCLR: Training-free human motion editing via attention mechanism manipulation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18977/cover.png"/></item><item><title>Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18775/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18775/</guid><description>VINE: A novel watermarking method significantly enhances robustness against advanced image editing techniques while maintaining high image quality, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18775/cover.png"/></item><item><title>Should We Really Edit Language Models? On the Evaluation of Edited Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18785/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18785/</guid><description>Language model editing, while efficient for small updates, causes inevitable performance drops and safety issues when scaled, urging a reassessment of its practical applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18785/cover.png"/></item><item><title>Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18451/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18451/</guid><description>Skywork-Reward achieves state-of-the-art results on RewardBench using a smaller, high-quality preference dataset and refined training techniques, highlighting the importance of data curation in LLM re&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18451/cover.png"/></item><item><title>SMITE: Segment Me In TimE</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18538/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18538/</guid><description>SMITE: a novel method for flexible-granularity video segmentation using only a few reference images, achieving temporally consistent results with superior accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18538/cover.png"/></item><item><title>Stable Consistency Tuning: Understanding and Improving Consistency Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18958/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18958/</guid><description>Stable Consistency Tuning (SCT) boosts consistency model speed and quality by reducing training variance and discretization errors, achieving new state-of-the-art results on ImageNet-64.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18958/cover.png"/></item><item><title>Taipan: Efficient and Expressive State Space Language Models with Selective Attention</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18572/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18572/</guid><description>Taipan, a hybrid language model, efficiently handles long contexts (up to 1 million tokens) by selectively applying attention, outperforming existing models in both speed and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18572/cover.png"/></item><item><title>The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18441/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18441/</guid><description>This paper enhances Transformer models by applying probabilistic optimization, yielding efficient subword encoding, hyperparameter optimization, and novel attention mechanisms for improved generative &amp;hellip;</description></item><item><title>Unbounded: A Generative Infinite Game of Character Life Simulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18975/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18975/</guid><description>UNBOUNDED, a generative infinite game, uses AI to create an open-ended character life simulation where players interact using natural language, transcending traditional game design.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18975/cover.png"/></item><item><title>Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18693/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18693/</guid><description>ScaleQuest synthesizes a million high-quality mathematical reasoning problems using efficient open-source methods, substantially boosting LLM reasoning performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18693/cover.png"/></item><item><title>WAFFLE: Multi-Modal Model for Automated Front-End Development</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18362/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18362/</guid><description>WAFFLE, a novel multi-modal model, revolutionizes front-end development by accurately translating UI designs into HTML code using structure-aware attention and contrastive learning, significantly outp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18362/cover.png"/></item><item><title>Why Does the Effective Context Length of LLMs Fall Short?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18745/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18745/</guid><description>Researchers unveil why LLMs underperform with long contexts, attributing it to skewed position frequency distribution, and introduce STRING, a training-free method that dramatically enhances long-cont&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18745/cover.png"/></item><item><title>ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17779/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17779/</guid><description>ADEM-VL: A novel vision-language tuning framework achieves state-of-the-art accuracy with significantly reduced computational cost and parameters, using a parameter-free cross-attention mechanism and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17779/cover.png"/></item><item><title>Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18252/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18252/</guid><description>Asynchronous RLHF accelerates language model training by 40% with improved efficiency, matching the performance of synchronous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18252/cover.png"/></item><item><title>DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18084/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18084/</guid><description>DynamicCity generates large-scale, high-quality 4D LiDAR scenes capturing dynamic environments, improving autonomous driving system development.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18084/cover.png"/></item><item><title>Lightweight Neural App Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17883/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17883/</guid><description>LiMAC, a lightweight neural app control architecture, uses a hybrid approach combining a small Action Transformer with a fine-tuned vision-language model for precise, real-time mobile app control, out&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17883/cover.png"/></item><item><title>MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17637/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17637/</guid><description>MIA-DPO boosts large vision-language model performance on multi-image tasks by cleverly augmenting single-image data and using attention mechanisms to filter out inaccurate responses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17637/cover.png"/></item><item><title>Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18234/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18234/</guid><description>Researchers boosted Large Language Model inference speed by using multiple draft models and a novel token selection scheme, improving block efficiency and token rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18234/cover.png"/></item><item><title>Scalable Ranked Preference Optimization for Text-to-Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18013/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18013/</guid><description>Researchers created a scalable method for aligning text-to-image models using synthetic preference datasets and a novel ranking-based optimization, significantly improving image quality and prompt-fol&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18013/cover.png"/></item><item><title>Scaling Diffusion Language Models via Adaptation from Autoregressive Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17891/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17891/</guid><description>Researchers efficiently adapt large autoregressive language models into competitive diffusion language models, overcoming previous scalability challenges and demonstrating improved performance on vari&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17891/cover.png"/></item><item><title>TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18071/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18071/</guid><description>TP-Eval unveils the hidden potential of MLLMs by customizing prompts to mitigate evaluation bias caused by prompt sensitivity, leading to a more accurate assessment of model capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18071/cover.png"/></item><item><title>Value Residual Learning For Alleviating Attention Concentration In Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17897/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17897/</guid><description>ResFormer &amp;amp; SVFormer alleviate Transformer attention concentration, boosting performance and reducing memory needs, paving the way for more efficient large language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17897/cover.png"/></item><item><title>WorldSimBench: Towards Video Generation Models as World Simulators</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18072/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18072/</guid><description>WorldSimBench: A new benchmark rigorously evaluates video generation models&amp;rsquo; ability to simulate realistic, actionable videos, advancing embodied AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18072/cover.png"/></item><item><title>ZIP-FIT: Embedding-Free Data Selection via Compression-Based Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18194/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18194/</guid><description>ZIP-FIT: a novel data selection method uses gzip compression to efficiently identify task-relevant data, significantly boosting model performance and reducing training time.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18194/cover.png"/></item><item><title>Aligning Large Language Models via Self-Steering Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17131/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17131/</guid><description>Self-Steering Optimization (SSO) autonomously generates high-quality preference signals for aligning LLMs, eliminating manual annotation and improving model performance significantly.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17131/cover.png"/></item><item><title>Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17243/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17243/</guid><description>Inf-CL breaks the memory barrier in contrastive learning, enabling near-infinite batch size scaling and drastically reducing memory costs without sacrificing accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17243/cover.png"/></item><item><title>Frontiers in Intelligent Colonoscopy</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17241/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17241/</guid><description>Revolutionizing colonoscopy, this study introduces ColonINST, a massive multimodal dataset, and ColonGPT, a powerful language model, enabling interactive, AI-assisted colonoscopy and improving diagnos&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17241/cover.png"/></item><item><title>JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17250/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17250/</guid><description>JMMMU, a new Japanese benchmark, provides a comprehensive culture-aware evaluation for Large Multimodal Models, revealing significant performance gaps and highlighting the need for culturally diverse &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17250/cover.png"/></item><item><title>LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17434/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17434/</guid><description>LongVU: A novel spatiotemporal compression method enables efficient long-video understanding by selectively reducing redundant video frames and tokens, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17434/cover.png"/></item><item><title>LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17242/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17242/</guid><description>LVSM, a novel transformer-based model, achieves state-of-the-art novel view synthesis by eliminating 3D inductive biases, leading to superior quality, scalability, and zero-shot generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17242/cover.png"/></item><item><title>Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16930/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16930/</guid><description>Math Neurosurgery precisely isolates LLMs&amp;rsquo; math skills using only forward passes, boosting their math performance significantly without affecting other abilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16930/cover.png"/></item><item><title>MiniPLM: Knowledge Distillation for Pre-Training Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17215/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17215/</guid><description>MINIPLM: A novel knowledge distillation framework boosts smaller language models&amp;rsquo; performance during pre-training by efficiently refining training data distributions with a teacher model&amp;rsquo;s knowledge, &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17215/cover.png"/></item><item><title>PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17247/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17247/</guid><description>PyramidDrop accelerates large vision-language models by efficiently reducing visual redundancy in deeper layers, achieving significant speedups in training and inference without performance loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17247/cover.png"/></item><item><title>SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17249/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17249/</guid><description>SpectroMotion reconstructs dynamic specular scenes with photorealistic accuracy by combining 3D Gaussian Splatting, physically-based rendering, and deformation fields, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17249/cover.png"/></item><item><title>3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16266/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16266/</guid><description>3DGS-Enhancer boosts realistic 3D scene generation from limited viewpoints by cleverly using 2D video diffusion priors to improve 3D view consistency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16266/cover.png"/></item><item><title>Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16259/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16259/</guid><description>Agent-to-Sim (ATS) learns interactive 3D agent behaviors from casual longitudinal videos using a novel coarse-to-fine registration and generative modeling approach, enabling real-to-sim transfer for v&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16259/cover.png"/></item><item><title>Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15748/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15748/</guid><description>Alchemy: A novel framework synthesizes formal theorems via symbolic mutation, boosting neural theorem-proving performance by significantly expanding the training dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15748/cover.png"/></item><item><title>AutoTrain: No-code training for state-of-the-art models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15735/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15735/</guid><description>AutoTrain: a no-code, open-source library simplifies training state-of-the-art models for diverse tasks, democratizing access to advanced AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15735/cover.png"/></item><item><title>Can Knowledge Editing Really Correct Hallucinations?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16251/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16251/</guid><description>HalluEditBench: A new benchmark reveals whether knowledge editing truly fixes LLM hallucinations, offering insights into efficacy, generalization, and robustness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16251/cover.png"/></item><item><title>CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16256/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16256/</guid><description>CompassJudger-1: An open-source, all-in-one judge LLM offering robust generalization and diverse evaluation capabilities, enhanced by the new JudgerBench benchmark, propelling LLM evaluation forward.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16256/cover.png"/></item><item><title>FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16271/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16271/</guid><description>FrugalNeRF: a novel few-shot NeRF, achieves high-fidelity 3D scene reconstruction with significantly faster convergence, eliminating the need for external data or complex scheduling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16271/cover.png"/></item><item><title>Improve Vision Language Model Chain-of-thought Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16198/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16198/</guid><description>Researchers enhanced vision-language model reasoning by distilling rationales from GPT-4, fine-tuning models, and applying reinforcement learning, achieving significant improvements in complex reasoni&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16198/cover.png"/></item><item><title>Language Models are Symbolic Learners in Arithmetic</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15580/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15580/</guid><description>LLMs don&amp;rsquo;t calculate; they&amp;rsquo;re symbolic pattern-matchers in arithmetic, mastering easy patterns first, then tackling harder ones through subgroup selection, as shown by a novel experimental framework.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15580/cover.png"/></item><item><title>LLM-based Optimization of Compound AI Systems: A Survey</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16392/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16392/</guid><description>This survey explores using LLMs to optimize compound AI systems, offering a unified framework based on program analysis to understand and improve LLM-based optimization strategies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16392/cover.png"/></item><item><title>Mitigating Object Hallucination via Concentric Causal Attention</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15926/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15926/</guid><description>Concentric Causal Attention (CCA) significantly reduces object hallucination in Large Vision Language Models by mitigating the negative effects of long-term decay in Rotary Position Encoding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15926/cover.png"/></item><item><title>Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16153/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16153/</guid><description>PANGEA: A fully open multilingual, multimodal LLM for 39 languages, significantly outperforming existing models in diverse cultural contexts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16153/cover.png"/></item><item><title>Pantograph: A Machine-to-Machine Interaction Interface for Advanced Theorem Proving, High Level Reasoning, and Data Extraction in Lean 4</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16429/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16429/</guid><description>Pantograph, a new Lean 4 tool, revolutionizes machine-assisted theorem proving by offering a seamless interface for integrating machine learning models and proof assistants, enabling more efficient an&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16429/cover.png"/></item><item><title>Pre-training Distillation for Large Language Models: A Design Space Exploration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16215/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16215/</guid><description>Boosting LLMs: This study reveals how pre-training distillation significantly enhances large language models, exploring key design factors for optimal performance.</description></item><item><title>RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16184/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16184/</guid><description>RM-BENCH: a new benchmark reveals that current reward models struggle with subtle content and style, highlighting the need for improvement and better alignment of language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16184/cover.png"/></item><item><title>SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16268/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16268/</guid><description>SAM2Long enhances video object segmentation by using a training-free memory tree, significantly improving accuracy and handling of occlusions and reappearing objects in long videos.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16268/cover.png"/></item><item><title>Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15633/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15633/</guid><description>GATEAU, a novel framework, leverages Homologous Models&amp;rsquo; Guidance and Contextual Awareness Measurement to identify influential samples for enhanced long-context alignment in LLMs, boosting performance &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15633/cover.png"/></item><item><title>Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15999/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15999/</guid><description>SPARE, a training-free method, uses sparse autoencoders to precisely steer LLMs&amp;rsquo; knowledge selection, resolving conflicts between memory and context for improved accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15999/cover.png"/></item><item><title>xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16267/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16267/</guid><description>BLIP-3-Video achieves state-of-the-art video question answering with only 32 visual tokens, drastically reducing computational costs while maintaining high accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16267/cover.png"/></item><item><title>Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15460/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15460/</guid><description>New training method, Sensitive Neuron Dropout (SeND), reduces Large Language Model (LLM) hallucinations by up to 40%, improving factual accuracy and reliability.</description></item><item><title>Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15316/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15316/</guid><description>Ichigo, a novel mixed-modal voice assistant, achieves real-time speech understanding and generation by using a tokenized early-fusion approach, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15316/cover.png"/></item><item><title>M-RewardBench: Evaluating Reward Models in Multilingual Settings</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15522/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15522/</guid><description>M-REWARDBENCH: A new multilingual benchmark reveals significant performance gaps in reward models across languages, highlighting the need for improved cross-lingual alignment in LLMs.</description></item><item><title>Baichuan Alignment Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14940/</link><pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14940/</guid><description>Baichuan Alignment unveils novel techniques for aligning LLMs, boosting performance significantly across various benchmarks and user experience metrics, advancing the field towards AGI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14940/cover.png"/></item><item><title>DM-Codec: Distilling Multimodal Representations for Speech Tokenization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15017/</link><pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15017/</guid><description>DM-Codec, a novel speech tokenizer, leverages multimodal distillation to drastically improve speech quality and reduce transcription errors, outperforming state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15017/cover.png"/></item><item><title>How Many Van Goghs Does It Take to Van Gogh? Finding the Imitation Threshold</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15002/</link><pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15002/</guid><description>Researchers discover the &amp;lsquo;imitation threshold&amp;rsquo; in text-to-image models: around 200-600 training images are needed to reliably generate images of a specific person or art style, impacting copyright a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15002/cover.png"/></item><item><title>Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14677/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14677/</guid><description>AI-generated text detection models struggle in real-world scenarios due to flawed training datasets; this paper provides a systematic review and proposes robust evaluation methods for these datasets.</description></item><item><title>BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14672/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14672/</guid><description>BiGR: a novel conditional image generation model using compact binary codes, unifying generative and discriminative tasks for improved visual representation and zero-shot generalization across multipl&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14672/cover.png"/></item><item><title>EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14649/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14649/</guid><description>EvoPress uses evolutionary search to optimize dynamic LLM compression, proving optimality and surpassing existing methods in accuracy and efficiency.</description></item><item><title>How Do Training Methods Influence the Utilization of Vision Models?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14470/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14470/</guid><description>Training methods dramatically alter which parts of a vision model are actually used for decisions, revealing surprising variations in layer importance across different training techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14470/cover.png"/></item><item><title>Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14208/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14208/</guid><description>Montessori-Instruct optimizes synthetic data generation for LLMs by aligning teacher models with student learning preferences, significantly improving student model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14208/cover.png"/></item><item><title>NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14669/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14669/</guid><description>NaturalBench: a new benchmark reveals vision-language models struggle with simple, natural images and questions, highlighting biases and prompting development of more robust models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14669/cover.png"/></item><item><title>Teaching Models to Balance Resisting and Accepting Persuasion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14596/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14596/</guid><description>LLMs can be easily persuaded; this paper introduces Persuasion-Balanced Training (PBT) to make them both resistant to misinformation and receptive to helpful influence, leading to improved performance&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14596/cover.png"/></item><item><title>ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13924/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13924/</guid><description>ARKit LabelMaker creates a massive, real-world 3D dataset with dense semantic labels, automatically generated to boost indoor scene understanding model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13924/cover.png"/></item><item><title>CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13218/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13218/</guid><description>CBT-BENCH: A new benchmark systematically evaluates LLMs&amp;rsquo; potential for assisting Cognitive Behavioral Therapy (CBT), revealing strengths and weaknesses in various CBT tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13218/cover.png"/></item><item><title>Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13394/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13394/</guid><description>The CIA Suite, a novel extensible framework, enables cross-lingual evaluation of multilingual LLMs using evaluator LLMs and a new multilingual benchmark dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13394/cover.png"/></item><item><title>DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13726/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13726/</guid><description>DAWN: a novel non-autoregressive diffusion framework for all-at-once generation of dynamic talking head videos, achieving higher quality and speed than autoregressive methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13726/cover.png"/></item><item><title>Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13674/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13674/</guid><description>Diffusion Curriculum Learning (DisCL) generates high-quality synthetic data via image-guided diffusion, significantly boosting accuracy in long-tail and low-quality data classification.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13674/cover.png"/></item><item><title>DPLM-2: A Multimodal Diffusion Protein Language Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13782/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13782/</guid><description>DPLM-2: a new multimodal model revolutionizes protein design by simultaneously generating protein sequences and 3D structures, surpassing existing methods in accuracy and diversity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13782/cover.png"/></item><item><title>FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13925/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13925/</guid><description>FiTv2, an enhanced vision transformer, enables efficient and high-quality image generation at arbitrary resolutions and aspect ratios, surpassing existing diffusion models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13925/cover.png"/></item><item><title>In-context learning and Occam's razor</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14086/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14086/</guid><description>This study reveals that in-context learning implicitly minimizes model complexity alongside training error, providing a theoretical basis for Occam&amp;rsquo;s Razor in modern sequence models.</description></item><item><title>Looking Inward: Language Models Can Learn About Themselves by Introspection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13787/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13787/</guid><description>Language models can learn about themselves through introspection, outperforming other models in self-prediction tasks, showcasing a surprising new capability and challenging prevailing assumptions abo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13787/cover.png"/></item><item><title>MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13370/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13370/</guid><description>MagicTailor empowers text-to-image models with component-level control, enabling precise customization of generated images by modifying specific visual elements.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13370/cover.png"/></item><item><title>MedINST: Meta Dataset of Biomedical Instructions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13458/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13458/</guid><description>MEDINST: a massive biomedical instruction dataset (133 tasks, 7M samples) improves LLM cross-task generalization in medical analysis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13458/cover.png"/></item><item><title>PUMA: Empowering Unified MLLM with Multi-granular Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13861/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13861/</guid><description>PUMA: a unified multi-granular MLLM mastering diverse visual tasks by seamlessly integrating image generation and understanding, achieving both high diversity and precise controllability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13861/cover.png"/></item><item><title>Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13184/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13184/</guid><description>Router-Tuning and MindSkip boost Transformer efficiency by dynamically skipping less crucial layers, achieving a 21% speedup with minimal performance loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13184/cover.png"/></item><item><title>SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13276/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13276/</guid><description>SeerAttention learns to automatically identify and leverage inherent attention sparsity in LLMs, drastically boosting inference speed and scalability while maintaining accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13276/cover.png"/></item><item><title>SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14745/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14745/</guid><description>SEMIEVOL: A novel semi-supervised fine-tuning framework boosts LLM performance by cleverly integrating labeled and unlabeled data via knowledge propagation and adaptive selection, enabling efficient m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14745/cover.png"/></item><item><title>Steering Your Generalists: Improving Robotic Foundation Models via Value Guidance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13816/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13816/</guid><description>Value-Guided Policy Steering (V-GPS) significantly boosts the performance of generalist robotic policies by re-ranking actions via offline RL, without retraining, improving both precision and robustne&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13816/cover.png"/></item><item><title>UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14059/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14059/</guid><description>UCFE benchmark realistically evaluates LLMs&amp;rsquo; financial expertise via user-centric design and dynamic interactions, revealing performance gaps and highlighting the need for more robust, human-aligned m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14059/cover.png"/></item><item><title>Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13232/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13232/</guid><description>Boosting LLM-based web agents&amp;rsquo; performance, this study introduces World-Model-Augmented agents that simulate action outcomes for improved decision-making in complex web navigation tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13232/cover.png"/></item><item><title>Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/</guid><description>KeyNMF, a novel topic modeling method, reveals how Chinese diaspora media&amp;rsquo;s information dynamics correlate with major European political events, highlighting the PRC&amp;rsquo;s potential influence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/cover.png"/></item><item><title>Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12788/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12788/</guid><description>Meta-Chunking: A novel text segmentation method using LLMs improves RAG efficiency by 1.32 on 2WikiMultihopQA, consuming only 45.8% of the time compared to similarity chunking.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12788/cover.png"/></item><item><title>Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11190/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11190/</guid><description>Mini-Omni2: An open-source, multi-modal AI model closely replicating GPT-40&amp;rsquo;s vision, speech, and text capabilities, offering valuable insights for future research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11190/cover.png"/></item><item><title>SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11331/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11331/</guid><description>Shakti: a 2.5B parameter LLM optimized for edge AI, boasts high performance and efficiency on resource-constrained devices via novel VGQA, SwiGLU, and RoPE.</description></item><item><title>HART: Efficient Visual Generation with Hybrid Autoregressive Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10812/</link><pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10812/</guid><description>HART, a novel hybrid autoregressive transformer, generates high-quality 1024x1024 images efficiently, rivaling diffusion models while being significantly faster.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10812/cover.png"/></item></channel></rss>