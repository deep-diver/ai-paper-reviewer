{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to the field of large language models (LLMs), introducing the concept of few-shot learning that is central to many LLM applications, including reasoning."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-07-01", "reason": "This paper established the empirical relationship between model size, training data, and performance, which is crucial for understanding the scaling behavior of LLMs and for designing efficient large reasoning models."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper introduced the chain-of-thought prompting technique, a crucial innovation that significantly improves the reasoning capabilities of LLMs by guiding the model to generate step-by-step reasoning."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper introduced reinforcement learning from human feedback (RLHF), a technique widely used for aligning LLMs with human values and preferences, which is essential for building reliable and safe large reasoning models."}, {"fullname_first_author": "Hunter Lightman", "paper_title": "Let's verify step by step", "publication_date": "2023-05-01", "reason": "This paper introduced process reward models (PRMs), a key component of recent work on large reasoning models that provides step-by-step feedback during training, which enables the model to learn more effective reasoning processes."}]}