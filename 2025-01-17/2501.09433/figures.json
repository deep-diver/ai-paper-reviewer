[{"figure_path": "https://arxiv.org/html/2501.09433/x1.png", "caption": "Figure 1: CaPa pipeline.\nWe first generate 3D geometry using a 3D latent diffusion model.\nUsing the learned 3D latent space with ShapeVAE, we train a 3D Latent Diffusion Model that generates 3D geometries, guided by multi-view images to ensure alignment between the generated shape and texture.\nAfter the 3D geometry is created, we render four orthogonal views of the mesh, which serve as inputs for texture generation.\nTo produce a high-quality texture while preventing the Janus problem, we utilize a novel, model-agnostic spatially decoupled attention.\nFinally, we obtain a hyper-quality textured mesh through back projection and a 3D-aware occlusion inpainting algorithm.", "description": "CaPa, a novel framework for efficient 4K textured mesh generation, is depicted.  The pipeline begins with multi-view images as input, which guide a 3D latent diffusion model (trained using ShapeVAE) to generate a 3D geometry. Four orthogonal views of this geometry are then rendered and used as input for a texture synthesis stage employing spatially decoupled attention.  This attention mechanism ensures high-quality textures while preventing inconsistencies across different views (the 'Janus problem'). Finally, a 3D-aware occlusion inpainting algorithm is applied to fill in any missing or incomplete texture regions, resulting in a hyper-quality textured mesh.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.09433/x2.png", "caption": "Figure 2: Spatially Decoupled Cross Attention.\nTo produce high-quality multi-view images for a given geometry, we design a model-agnostic Spatially Decoupled Cross Attention.\nDuring cross-attention in denoising U-Net, we replicate hidden feature channels so that each duplicated channels focuses solely on the designated view.\nSince the design is model-agnostic, we can utilize an external ControlNet to guide the textures aligned with the input mesh.", "description": "This figure illustrates the Spatially Decoupled Cross Attention mechanism.  The core idea is to improve multi-view texture generation by decoupling the attention process for each view. Instead of a single attention mechanism processing all views simultaneously, this method replicates the hidden feature channels within the denoising U-Net. Each set of duplicated channels then focuses exclusively on a single view's information. This allows the model to generate more consistent and higher-quality textures across different perspectives without sacrificing efficiency. The model-agnostic nature means it works with existing models like ControlNet, further enhancing texture fidelity by aligning the generated textures with the input geometry.", "section": "3.2.1 Spatially Decoupled Cross Attention"}, {"figure_path": "https://arxiv.org/html/2501.09433/x3.png", "caption": "Figure 3: 3D-Aware Occlusion Inpainting.\nFirst, we cluster the normal and spatial coordinates of the occluded face.\nUsing clustered centers as viewpoints, we create specialized UV maps through projection mapping.\nThis approach captures surface locality, allowing 2D diffusion-based inpainting to effectively fill occluded regions. Note that this UV map is utilized solely for occlusion.", "description": "CaPa uses a novel 3D-aware occlusion inpainting algorithm to address the problem of incomplete textures caused by occlusions in multi-view 3D models.  First, it clusters the surface normals and spatial coordinates of occluded faces to identify distinct occluded regions.  Then, it uses the cluster centers as viewpoints to generate specialized UV maps that preserve surface locality.  Finally, these UV maps are used to guide a 2D diffusion-based inpainting process, effectively filling the occluded regions in the texture. Importantly, these specialized UV maps are only used for inpainting and not the final texture.", "section": "3.2.2 Occlusion Inpainting"}, {"figure_path": "https://arxiv.org/html/2501.09433/extracted/6135590/figs/other_texture.png", "caption": "Figure 4: Comparison of Texturing Method.\nUnlike prior works, CaPa effectively resolved the Janus problem with consistent ID.", "description": "This figure compares CaPa's texturing results with two other state-of-the-art methods (SyncMVD and FlashTex).  It highlights CaPa's ability to generate high-quality textures that are consistent across multiple views, unlike the other methods which suffer from the \"Janus problem\" (inconsistent textures across different viewpoints).  The Janus problem leads to visible artifacts and inconsistencies in the final 3D model.  CaPa effectively addresses the Janus problem by generating textures with consistent identity (ID), resulting in a more visually appealing and coherent 3D model.", "section": "4.2. Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2501.09433/x4.png", "caption": "Figure 5: Qualitative Comparison of Image-to-3D Generation.\nWe compare CaPa with state-of-the-art Image-to-3D methods.\nHere, all the assets are converted to polygonal mesh, using its official code.\nThe proposed CaPa significantly outperforms both geometry stability and texture quality, especially for the back and side view\u2019s visual fidelity and texture coherence.", "description": "This figure compares CaPa's image-to-3D generation results with several state-of-the-art methods.  All models' outputs were converted to polygonal meshes using the original code provided by the respective authors. The comparison highlights CaPa's superior performance in terms of both geometry stability and texture quality, particularly noticeable in the less-commonly-seen back and side views.  CaPa produces meshes with noticeably better visual fidelity and texture coherence.", "section": "4.2. Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2501.09433/x5.png", "caption": "Figure 6: Ablation Study.\n(a) demonstrates that using multi-view guidance significantly increases the geometry quality.\n(b) shows our Spatially Decoupled Attention effectively resolves the Janus problem, achieving high-fidelity texture coherence,\n(c) reveals our occlusion inpainting outperforms previous inpainting methods like UV-ControlNet, presented in Paint3D\u00a0[59].", "description": "This figure presents an ablation study evaluating the impact of three key components of the CaPa model on 3D asset generation: multi-view guidance for geometry generation, spatially decoupled attention for texture synthesis, and 3D-aware occlusion inpainting.  Subfigure (a) compares geometry quality with and without multi-view guidance, showcasing improved results with the guidance. Subfigure (b) compares texture quality with and without spatially decoupled attention, highlighting its effectiveness in preventing the Janus problem (texture inconsistencies across views). Finally, subfigure (c) compares the performance of the proposed occlusion inpainting method with a state-of-the-art technique, demonstrating superior results in terms of texture fidelity.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.09433/extracted/6135590/figs/scalability.png", "caption": "Figure 7: Scalability of CaPa.\n(a) Original result of CaPa.\n(b) 3D inpainting result using text-prompt (\u201corange sofa, orange pulp\u201d). CaPa\u2019s texture generation extends smoothly to 3D inpainting, stylizing the generated asset.\n(c) CaPa w/ LoRA\u00a0[14] adaptation. The model-agnostic approach allows CaPa to leverage pre-trained LoRA (balloon style) without additional 3D-specific retraining.", "description": "Figure 7 demonstrates CaPa's versatility and scalability by showcasing three examples. (a) shows a standard CaPa-generated output. (b) illustrates how CaPa seamlessly integrates with 3D inpainting by using a text prompt ('orange sofa, orange pulp') to modify the existing model, demonstrating its ability to adapt to new styles.  (c) highlights CaPa's model-agnostic nature by applying a pre-trained LoRA (Low-Rank Adaptation) for 'balloon style' without requiring additional 3D-specific training, thus showcasing its compatibility with external models and ease of customization.", "section": "4.5 Scalability of CaPa"}, {"figure_path": "https://arxiv.org/html/2501.09433/x6.png", "caption": "Figure 8: Result of the CaPa with PBR Understanding.\nWe demonstrate CaPa\u2019s capability for disentangling physically based rendering (PBR) materials.\nThe figure shows PBR-aware generation results under various lighting conditions: \u2018city,\u2019 \u2018studio,\u2019 and \u2018night,\u2019 using Blender\u2019s default environment settings\u00a0[7].\nAs shown, CaPa effectively adapts to different light environments, highlighting its potential for PBR-aware asset generation.", "description": "Figure 8 showcases the results of CaPa when integrated with Physically Based Rendering (PBR) material capabilities.  The images demonstrate CaPa's ability to generate high-quality textures that adapt to different lighting conditions. Three lighting scenarios are shown: 'city', 'studio', and 'night', all using Blender's default settings. The results highlight CaPa's potential for creating realistic 3D assets with accurate material properties and lighting interactions.", "section": "A. Discussions and Limitations"}, {"figure_path": "https://arxiv.org/html/2501.09433/x7.png", "caption": "Figure 9: Additional Image-to-3D Results of CaPa.\nCaPa can generate diverse objects from textual, and visual input.\nThe result demonstrates our diversity across the various categories, marking a significant advancement in practical 3D asset generation methodologies.", "description": "Figure 10 showcases the diverse range of 3D assets CaPa can generate from text and image inputs.  The examples illustrate CaPa's ability to produce high-quality, detailed models across a variety of object categories, including animals (tigers, dragons), accessories (handbags), characters (cartoon figures, superheroes), and household items. The figure highlights CaPa's flexibility and its potential for practical applications in 3D asset creation for games, films, and VR/AR experiences.", "section": "B. Additional Experiments"}, {"figure_path": "https://arxiv.org/html/2501.09433/x8.png", "caption": "Figure 10: Additional Comparison of Image-to-3D Generation.\nCaPa significantly outperforms both geometry stability and texture quality, especially for the back and side view\u2019s visual fidelity and texture coherence.", "description": "Figure 10 presents a detailed comparison of 3D model generation results from CaPa against several state-of-the-art methods.  The comparison highlights CaPa's superior performance in terms of both geometry stability and texture quality, particularly noticeable in the back and side views where other methods often exhibit significant degradation. The figure shows multiple views of several different 3D models, allowing for a visual assessment of the quality and consistency across various perspectives. This demonstrates CaPa's ability to produce high-fidelity 3D assets which are structurally sound and maintain texture detail from different viewpoints.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.09433/x9.png", "caption": "Figure 11: Impact of Spatially Decoupled Cross Attention on Janus Artifacts.\nIn this additional figure, We demonstrate the capability of Janus prevention in the proposed spatially decoupled cross-attention mechanism.\nEach row depicts, (a) with spatially decoupled cross attention, (b) without spatially decoupled cross attention, and (c) a mesh rendering of the current view, respectively.", "description": "This figure demonstrates the effectiveness of the proposed spatially decoupled cross-attention mechanism in preventing the Janus artifact, a common issue in multi-view 3D texture generation.  Each row shows a comparison of three different approaches for generating textures: (a) using the spatially decoupled cross-attention, (b) without using the spatially decoupled cross-attention (resulting in the Janus artifact), and (c) a mesh rendering of the current view for better visualization.  The comparison highlights how the proposed method improves multi-view consistency and eliminates texture discrepancies.", "section": "B.2 Impact of Spatially Decoupled Cross Attention on Janus Artifacts"}, {"figure_path": "https://arxiv.org/html/2501.09433/x10.png", "caption": "Figure 12: Qualitative results for different occlusion inpainting methods.\n(a) shows results from our 3D-aware occlusion inpainting method, (b) uses automatic view selection, and (c) employs UV ControlNet.", "description": "This figure compares three different occlusion inpainting methods: the authors' proposed 3D-aware approach, automatic view selection, and UV ControlNet. Each method's results are shown for various objects with challenging occlusions, demonstrating the strengths and limitations of each approach in terms of texture fidelity, seam visibility, and overall visual coherence.", "section": "B.3. Analysis of the Occlusion"}, {"figure_path": "https://arxiv.org/html/2501.09433/x11.png", "caption": "Figure 13: Text-to-3D Results of CaPa.\nCaPa can generate diverse objects from textual, and visual input.\nThe result underscores CaPa\u2019s strengths in generating high-resolution textures that align with well-defined geometries.", "description": "This figure showcases various 3D models generated by CaPa from text prompts.  It highlights CaPa's ability to create high-resolution textures that seamlessly integrate with well-defined 3D geometries.  The examples demonstrate CaPa's versatility in generating a wide range of objects, from skulls and anthropomorphic animals to furniture and fictional characters.  The detailed textures and sharp geometries demonstrate the system's effectiveness and its capability to produce high-quality 3D assets suitable for various applications.", "section": "B.4. Text-to-3D Results of CaPa"}, {"figure_path": "https://arxiv.org/html/2501.09433/extracted/6135590/figs/remeshing_draft.png", "caption": "Figure 14: Results of Our Remeshing Algorithm.\nWe employ a carefully designed remeshing scheme after geometry generation for better practical usage for broader applications.\n(a) shows the original polygonal mesh, (b) shows remeshed output of quadrilateral faces, and (c) shows remeshed output of triangular faces.", "description": "This figure demonstrates the remeshing process employed in the CaPa framework.  The original polygonal mesh (a) is shown, alongside the results after remeshing with quadrilateral faces (b) and triangular faces (c).  Remeshing is a crucial step in producing high-quality 3D models suitable for various applications, enhancing the mesh quality and resolving issues such as non-manifold geometry.  Quadrilateral remeshing offers a more structured and uniform mesh ideal for texturing and subsequent processing, while triangular remeshing is more flexible. The figure showcases the improved mesh quality and regularity achieved through these remeshing techniques.", "section": "C.1 Additional Details of CaPa"}]