[{"figure_path": "https://arxiv.org/html/2501.09756/x1.png", "caption": "Figure 1: SynthLight performs relighting on portraits using an environment map lighting. By learning to re-render synthetic human faces, our diffusion model produces realistic illumination effects on real portrait photographs, including distinct cast shadows on the neck and natural specular highlights on the skin. Despite being trained exclusively on synthetic headshot images for relighting, the model demonstrates remarkable generalization to diverse scenarios, successfully handling half-body portraits and even full-body figurines.", "description": "The figure showcases the capabilities of SynthLight, a diffusion model trained on synthetic human face headshots, to relight real portrait photographs realistically.  SynthLight uses environment map lighting to produce illumination effects, including subtle cast shadows and specular highlights. Notably, it generalizes well beyond its training data, effectively relighting not only headshots, but also half-body and full-body images.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.09756/x2.png", "caption": "Figure 2: Synthetic Faces: Subjects are rendered under various lighting conditions (details in \u00a0Sec.\u00a03.1). We show two examples, where each pair consists of a subject rendered using two different environment maps. The network is trained to re-render synthetic faces by transforming a subject rendered with one environment map into its counterpart rendered with the other environment map.", "description": "This figure shows pairs of synthetic human faces rendered under different lighting conditions. Each pair shows the same 3D face model illuminated by two different environment maps (HDR). The goal is to train a neural network to transform an image of a face rendered under one lighting condition to its counterpart image rendered under another lighting condition.  This demonstrates how the model learns to re-render faces under varying lighting scenarios.", "section": "3.1. Synthetic Data for Relighting"}, {"figure_path": "https://arxiv.org/html/2501.09756/x3.png", "caption": "Figure 3: Training pipeline of SynthLight. We first enable the relighting modeling by training the diffusion backbone with synthetic relighting tuples (Task 1, top row), detailed in Sec.\u00a03.2. To further alleviate the domain gap between synthetic and real image domain, we include a joint training of the text-to-image task (Task 2, bottom row), detailed in Sec.\u00a03.3. Our model is based on LDM [34] and is composed of a VAE and a UNet. For simplicity, VAE is omitted in the diagram.", "description": "This figure illustrates the training pipeline of the SynthLight model.  The pipeline uses a multi-task learning approach, combining synthetic relighting data (Task 1) with real-world text-to-image data (Task 2). Task 1 trains the model on synthetically generated portrait images, rendered under different lighting conditions, to learn to relight images. Task 2 uses real-world image data to minimize the domain gap between synthetic training data and real images and improve photorealism. The model is based on Latent Diffusion Models (LDM), comprising a Variational Autoencoder (VAE) and a U-Net.  For clarity, the VAE is omitted from the diagram.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.09756/x4.png", "caption": "Figure 4: We employ the image-conditioning classifier-free guidance during inference to proportionally balance between identity preservation, and relighting effects. The final score estimate is computed as per Eq.\u00a02.", "description": "This figure illustrates the inference-time adaptation method used in SynthLight.  It shows how classifier-free guidance is used to balance identity preservation and relighting effects.  The method proportionally adjusts the contribution of image-conditional and unconditional scores during the diffusion process, allowing for control over how strongly the input image's details are preserved while incorporating the desired relighting effects specified by the environment map. The equation (Eq. 2) that is referenced shows the mathematical formula used to compute the balanced score.", "section": "3.4 Inference Time Adaptation"}]