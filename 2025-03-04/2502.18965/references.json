{"references": [{"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model.", "publication_date": "2024-01-01", "reason": "This paper introduces Direct Preference Optimization (DPO), which is crucial for aligning language models with desired behaviors by directly optimizing from preference data."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback.", "publication_date": "2022-01-01", "reason": "This work is fundamental in the field of reinforcement learning from human feedback (RLHF), which is a primary method for aligning language models with human preferences."}, {"fullname_first_author": "Guorui Zhou", "paper_title": "Deep interest network for click-through rate prediction.", "publication_date": "2018-01-01", "reason": "This paper is influential in the realm of click-through rate (CTR) prediction and introduces a novel architecture for capturing user interests."}, {"fullname_first_author": "Yi Tay", "paper_title": "Transformer memory as a differentiable search index.", "publication_date": "2022-01-01", "reason": "This paper introduces the use of transformers for generative retrieval, which is a core inspiration for the OneRec model."}, {"fullname_first_author": "Shashank Rajput", "paper_title": "Recommender systems with generative retrieval.", "publication_date": "2023-01-01", "reason": "This paper represents one of the first implementations of a recommender system with generative retrieval, setting the stage for the current work."}]}