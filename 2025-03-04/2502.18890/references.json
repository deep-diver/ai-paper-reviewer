{"references": [{"fullname_first_author": "Leviathan, Y.", "paper_title": "Fast inference from transformers via speculative decoding", "publication_date": "2023-01-01", "reason": "This paper introduces speculative decoding, a method to accelerate transformer inference, which is fundamental to the approach presented in the main paper."}, {"fullname_first_author": "Holtzman, A.", "paper_title": "The curious case of neural text degeneration", "publication_date": "2020-01-01", "reason": "This paper discusses the issue of text degeneration in neural language models and introduces nucleus sampling, which is a problem this submission addresses."}, {"fullname_first_author": "Cai, T.", "paper_title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads", "publication_date": "2024-01-01", "reason": "This paper introduced Medusa, which enables multiple draft tokens by using multiple decoding heads, and the submisson's multi-token self-drafting is inspired by this."}, {"fullname_first_author": "Sun, H.", "paper_title": "Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding", "publication_date": "2024-01-01", "reason": "This paper introduces Triforce, which accelerates long sequence generation with hierarchical speculative decoding and serves as an important comparison baseline in this submission."}, {"fullname_first_author": "Chen, Z.", "paper_title": "Magicdec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding", "publication_date": "2024-01-01", "reason": "This paper presents MagicDec, which addresses the latency-throughput tradeoff for long-context generation with speculative decoding and is compared with the method in this paper."}]}