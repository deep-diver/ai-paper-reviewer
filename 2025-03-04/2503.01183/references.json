{"references": [{"fullname_first_author": "Andrea Agostinelli", "paper_title": "Musiclm: Generating music from text", "publication_date": "2023-01-01", "reason": "This paper is important because it introduced MusicLM, a model for generating music from text descriptions, influencing subsequent research in the field of music generation, and also serves as the baseline for other comparisons."}, {"fullname_first_author": "Jade Copet", "paper_title": "Simple and controllable music generation", "publication_date": "2023-01-01", "reason": "This paper is significant as it presents MusicGen, a simple and controllable model for music generation, which is a relevant and impactful method in the music generation domain, and also served as a comparison baseline."}, {"fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-01-01", "reason": "This paper is important as it introduces DiT, a scalable diffusion model with transformers, providing a strong foundation for various generation tasks, including audio and music, providing the backbone used in the main paper."}, {"fullname_first_author": "Zach Evans", "paper_title": "Long-form music generation with latent diffusion", "publication_date": "2024-04-01", "reason": "This paper's focus on long-form music generation with latent diffusion is highly relevant to DiffRhythm, making it a crucial reference for understanding challenges and approaches in generating longer musical pieces."}, {"fullname_first_author": "Zhiqing Hong", "paper_title": "Text-to-song: Towards controllable music generation incorporating vocals and accompaniment", "publication_date": "2024-04-01", "reason": "This paper is important as it deals directly with generating songs from text, integrating both vocals and accompaniment, a capability that aligns closely with DiffRhythm's objectives and directly inspired the model's structure."}]}