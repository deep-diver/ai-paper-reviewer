[{"figure_path": "https://arxiv.org/html/2503.01743/x1.png", "caption": "Figure 1: A overview of the Multimodal architecture for Phi-4-Multimodal", "description": "Phi-4-Multimodal is a unified multimodal model that processes multiple input modalities (text, vision, audio/speech) simultaneously.  The architecture uses a frozen language model (Phi-4-Mini) as its base. Modality-specific encoders (vision, audio) project their respective features to the language model's embedding space. LoRA adapters are applied to the language decoder to adapt to the different modalities, allowing seamless integration of modalities.  This approach ensures that the language model is only adapted and is not fine-tuned, thereby maintaining original language model performance.", "section": "2 Model architecture"}, {"figure_path": "https://arxiv.org/html/2503.01743/extracted/6234065/figures/phio_vision_demo_case.png", "caption": "Figure 2: One demo case to show the vision-language understanding and reasoning capability of Phi-4-Multimodal.", "description": "Figure 2 presents a demonstration of Phi-4-Multimodal's capabilities in understanding and reasoning with vision and language.  The example shows a chart depicting the percentage of people across various generations using AI tools at work, and the model correctly answers a question based on this chart. This showcases the model's ability to process visual data along with text inputs, demonstrating its multimodal understanding and reasoning skills.", "section": "2 Model architecture"}, {"figure_path": "https://arxiv.org/html/2503.01743/extracted/6234065/figures/phio_speech_demo.png", "caption": "Figure 3: An example to showcase the understanding capabilities for Phi-4-Multimodal, including audio understanding, summarization, ASR, and AST.", "description": "Figure 3 presents a comprehensive example illustrating the multimodal capabilities of Phi-4-Multimodal.  It showcases the model's ability to process audio input, perform automatic speech recognition (ASR) to transcribe the audio into text, and then automatic speech translation (AST) to translate the audio into another language. Further, the figure displays the model's capacity for summarization by generating a concise summary of the conversation contained within the audio clip.  This example highlights Phi-4-Multimodal's proficiency in handling multiple modalities simultaneously and delivering coherent, insightful responses.", "section": "4.1 Multimodal Benchmarks"}]