{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-06-01", "reason": "This paper introduces CLIP, which is used for evaluating the alignment between text descriptions and generated images in the text-to-3D task."}, {"fullname_first_author": "Lvmin Zhang", "paper_title": "Adding conditional control to text-to-image diffusion models", "publication_date": "2023-01-01", "reason": "This paper introduces ControlNet, which Kiss3DGen integrates to handle various 3D-related tasks such as enhancement, editing, and image-to-3D generation."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This paper presents latent diffusion models, which are widely used for image generation and are relevant as Kiss3DGen repurposes diffusion models."}, {"fullname_first_author": "Ben Poole", "paper_title": "Dreamfusion: Text-to-3d using 2d diffusion", "publication_date": "2022-01-01", "reason": "This paper uses a pre-trained 2D diffusion model to create 3D content via optimization and is relevant to 3D generation techniques."}, {"fullname_first_author": "Aditya Ramesh", "paper_title": "Hierarchical text-conditional image generation with clip latents", "publication_date": "2022-01-01", "reason": "This paper presents text-conditional image generation, which is important as Kiss3DGen generates 3D assets with text conditions."}]}