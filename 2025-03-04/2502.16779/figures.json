[{"figure_path": "https://arxiv.org/html/2502.16779/x1.png", "caption": "Figure 1: We present a novel method for estimating room layouts from a set of unconstrained indoor images. Our approach demonstrates robust generalization capabilities, performing well on both in-the-wild datasets (Zhou et\u00a0al., 2018) and out-of-domain cartoon (Weber et\u00a0al., 2024) data.", "description": "Figure 1 showcases the effectiveness of a novel room layout estimation method.  The figure visually demonstrates the method's ability to accurately reconstruct room layouts from multiple, unconstrained indoor images.  Importantly, it highlights the model's robustness and generalization capabilities by successfully processing both realistic images from in-the-wild datasets (as seen in the left image group) and significantly different, cartoon-style images from an out-of-domain dataset (as seen in the right image group).  This showcases the method's ability to work effectively across diverse image styles and data sources.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2502.16779/x2.png", "caption": "Figure 2: Our multi-view room layout estimation pipeline. It consists of three parts: 1) a 2D plane detector f1subscript\ud835\udc531f_{1}italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, 2) a 3D information prediction and correspondence establishment method Plane-DUSt3R f2subscript\ud835\udc532f_{2}italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and 3) a post-processing algorithm f3subscript\ud835\udc533f_{3}italic_f start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT.", "description": "This figure illustrates the three-stage pipeline for multi-view room layout estimation.  The first stage uses a 2D plane detector (f<sub>1</sub>) to identify planar regions in each input image.  The second stage employs Plane-DUSt3R (f<sub>2</sub>), a modified version of the DUSt3R 3D reconstruction model, to predict 3D information from the 2D detections and establish correspondences between planes across multiple views.  The final stage involves a post-processing algorithm (f<sub>3</sub>) that refines the 3D layout by merging corresponding planes and resolving inconsistencies.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2502.16779/extracted/6239959/figs/arch_2.png", "caption": "Figure 3: Plane-DUSt3R architecture remains identical to DUSt3R. The transformer decoder and regression head are further fine-tuned on the occlusion-free depth map (see Figure\u00a04).", "description": "The Plane-DUSt3R architecture is a modified version of the DUSt3R architecture.  The core components remain the same: a Vision Transformer (ViT) encoder, a transformer decoder, and two regression heads. However, a key difference is that Plane-DUSt3R is fine-tuned on a depth map that has been pre-processed to remove occlusions caused by objects within the room. This allows Plane-DUSt3R to focus solely on the structural planes, such as walls, floors, and ceilings, simplifying the layout prediction task.  The figure visually depicts the architecture of Plane-DUSt3R, emphasizing its similarity to DUSt3R while highlighting the modifications made for the specific purpose of room layout estimation. ", "section": "3.2 f2: PLANE-BASED DUST3R"}, {"figure_path": "https://arxiv.org/html/2502.16779/extracted/6239959/figs/depth_ori.png", "caption": "(a) The original DUSt3R depth map.", "description": "This figure shows a comparison of depth maps generated by two different methods. (a) displays the depth map produced by the original DUSt3R model, which contains information about all 3D elements within the scene, including both structural elements (walls, floors, ceilings) and non-structural objects (furniture). This comprehensive depth map can be complex and noisy.  The goal of Plane-DUSt3R is to simplify this for better room layout estimation.", "section": "3.2 f2: PLANE-BASED DUST3R"}, {"figure_path": "https://arxiv.org/html/2502.16779/extracted/6239959/figs/depth_plane.png", "caption": "(b) The Plane-DUSt3R depth map.", "description": "Figure 4(b) shows the depth map generated by the Plane-DUSt3R model.  Unlike the original DUSt3R depth map (shown in Figure 4(a)), this depth map focuses exclusively on structural planes (walls, floors, ceilings), effectively removing occlusions caused by objects or furniture. This processed depth map serves as crucial input for subsequent steps in the room layout estimation pipeline, leading to a more accurate and streamlined layout prediction.", "section": "3.2 f2: Plane-based DUSt3R"}, {"figure_path": "https://arxiv.org/html/2502.16779/extracted/6239959/figs/bird-view/ori.png", "caption": "Figure 4: The (a) original DUSt3R depth map and (b) occlusion removed depth map.", "description": "This figure compares the depth maps generated by the original DUSt3R model and the modified Plane-DUSt3R model.  (a) shows the depth map produced by DUSt3R which includes various objects and occlusions within the scene. (b) displays the depth map generated by Plane-DUSt3R after modifications.  The key difference is that Plane-DUSt3R's depth map focuses exclusively on structural planes (walls, floor, ceiling), effectively removing occlusions from non-structural elements like furniture to improve room layout estimation.", "section": "3.2 f2: PLANE-BASED DUST3R"}, {"figure_path": "https://arxiv.org/html/2502.16779/extracted/6239959/figs/bird-view/rotated.png", "caption": "(a) Projected Lines", "description": "Figure 5(a) shows the visualization of the plane projection step in the multi-view room layout estimation pipeline.  The input is a set of planes detected in multiple images of the same scene. Each plane is represented by a line segment in the image, and these line segments are then projected onto a common 2D plane (such as the xz plane). This projection simplifies the scene representation, making the subsequent merging process easier. The line segments shown in this figure form the basis for classifying lines as horizontal or vertical before the final merging step.  They will then be used in the steps described in (b), (c) and (d) to align and merge them into coherent planes for the final layout. ", "section": "3.3 f3: POST-PROCESSING"}, {"figure_path": "https://arxiv.org/html/2502.16779/extracted/6239959/figs/bird-view/calibrated.png", "caption": "(b) Rotated Lines", "description": "This figure shows the results of rotating the line segments representing planes projected onto the x-z plane so that they become approximately horizontal or vertical. This rotation step simplifies the subsequent classification and merging of line segments into complete planes.", "section": "3.3 f3: POST-PROCESSING"}, {"figure_path": "https://arxiv.org/html/2502.16779/extracted/6239959/figs/bird-view/layout.png", "caption": "(c) Aligned Lines", "description": "This figure shows the results of aligning line segments to be either horizontal or vertical after a rotation. This step is part of a multi-view room layout estimation process where the goal is to merge planes from different images to create a unified 3D room layout. The alignment simplifies the merging process by ensuring that line segments representing the same wall are parallel to each other, facilitating their identification and combination.", "section": "3.3 f3: POST-PROCESSING"}, {"figure_path": "https://arxiv.org/html/2502.16779/extracted/6239959/figs/out_domain/friends/chandler-apartment.png", "caption": "(d) Correspondance", "description": "This figure shows the results of merging plane segments to create a unified representation of the room layout.  Panel (a) displays the initial projection of plane segments onto the x-z plane. Panel (b) shows the rotation of these segments to be either horizontal or vertical. Panel (c) shows the classification and further alignment of segments. Panel (d) presents the final result of merged planes, with segments of the same plane highlighted with the same color and index. This illustrates the process of combining individual plane segments from multiple views into a consistent 3D room layout.", "section": "3.3 f3: POST-PROCESSING"}, {"figure_path": "https://arxiv.org/html/2502.16779/extracted/6239959/figs/out_domain/friends/friends-overnight-joey-chandler-apartment.jpeg", "caption": "Figure 5: (a) Planes are projected onto the x-z plane as 2D line segments. (b) The scene is rotated so that line segments are approximately horizontal or vertical. (c) Line segments are classified and aligned to be either horizontal or vertical. (d) Merged planes are shown, with segments belonging to the same plane indicated by the same color and index.", "description": "Figure 5 illustrates the multi-view plane merging process. (a) shows the projection of planes onto the x-z plane, represented as 2D line segments.  (b) depicts the scene's rotation to make these line segments roughly horizontal or vertical. (c) shows classification and alignment of these line segments as either horizontal or vertical. Finally, (d) presents the merged planes with segments of the same plane indicated using a consistent color and index, resolving plane correspondence across multiple images.", "section": "3.3 f3: POST-PROCESSING"}, {"figure_path": "https://arxiv.org/html/2502.16779/extracted/6239959/figs/out_domain/friends/result.png", "caption": "Figure 6: Qualitative results on Structure3D testing set. The first 3 columns are input views, the fourth and fifth columns are layout results of Noncuboid+MASt3R and our pipeline respectively. Due to space limitations, we refer reader to appendix for more complete results.", "description": "This figure showcases qualitative results of room layout reconstruction on the Structure3D dataset's testing set.  The first three columns display the input images from multiple viewpoints. The fourth column presents the results obtained using the Noncuboid+MASt3R method. The fifth column shows the results produced by the proposed Plane-DUSt3R pipeline in this paper.  Due to space constraints in the publication, additional results are available in the appendix for a more comprehensive evaluation.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.16779/extracted/6239959/figs/out_domain/burger/1.png", "caption": "Figure 7: Birdview of multi-view 3D planes aligned to the same coordinate. The first row shows 5 cases of our pipeline results after post-processing step. The second row is the results of Noncuboid+MASt3R. Line segments of the same color indicate that they belong to the same plane.", "description": "This figure displays a bird's-eye view of 3D room layouts generated from multiple images using two different methods. The top row presents five examples of 3D room layouts produced by the proposed pipeline (Plane-DUSt3R), illustrating the pipeline's ability to reconstruct the spatial structure of rooms from sparse, unconstrained viewpoints.  Each plane is represented by a unique color, making it easy to distinguish individual walls, floors, and ceilings in the reconstruction. The bottom row shows five comparative results obtained using a baseline method (Noncuboid+MASt3R), highlighting differences in accuracy and completeness of the room layout reconstruction. The consistent use of color-coding for corresponding planes in both rows enables a direct visual comparison of the two methods' performance.  This visualization effectively demonstrates the superior performance of the proposed Plane-DUSt3R method in handling multi-view scenarios with sparse image data.", "section": "4.2 MULTI-VIEW ROOM LAYOUT ESTIMATION RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.16779/extracted/6239959/figs/out_domain/burger/2.jpg", "caption": "Figure 8: Qualitative results on in-the-wild data (Zhou et\u00a0al., 2018). The first three columns are input views, the fourth column is the layout results of Noncuboid+MASt3R. The rightmost column shows the predicted plane pointmap with the extracted wireframe drawn in red.", "description": "Figure 8 displays a qualitative comparison of room layout estimations on the 'in-the-wild' dataset (Zhou et al., 2018).  The figure presents sets of three input images from various viewpoints for each room, followed by the room layout estimated using the Noncuboid+MASt3R method. The final column shows the predicted 3D plane point cloud generated by the proposed method, with the extracted wireframe structure overlaid in red, providing a clear visualization of the planes detected and their spatial relationships within each scene.", "section": "4.3 GENERALIZABILITY TO UNKNOWN AND OUT-OF-DOMAIN DATA"}, {"figure_path": "https://arxiv.org/html/2502.16779/extracted/6239959/figs/out_domain/burger/result.png", "caption": "Figure 9: Qualitative results on Structure3D testing set. The 5-th column is our result visualized with pointcloud, the last column is the result shown in pure wireframe", "description": "Figure 9 presents a qualitative comparison of the proposed multi-view room layout estimation method's performance on the Structure3D test set.  Each row shows a different room scene. The first four columns display the input views (multiple perspectives of the same scene).  The fifth column shows the 3D reconstruction results using point cloud visualization to represent the room's layout, demonstrating the method's ability to accurately estimate the positions of planes representing walls, floors, and ceilings. The final column displays the same results using only the wireframe, providing a simplified representation focusing solely on the geometric structure of the layout and omitting the detailed 3D point cloud.", "section": "4 Experiments"}]