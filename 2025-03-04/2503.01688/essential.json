{"importance": "This paper advances **LLM evaluation** by pinpointing the conditions where uncertainty estimates are reliable. It also emphasizes the need for enhanced datasets that consider reasoning complexity and topic balance, guiding the creation of **more robust and trustworthy LLMs**.", "summary": "This paper investigates when LLMs are apprehensive and when their uncertainty is justified.", "takeaways": ["Entropy is an effective indicator of question difficulty in knowledge-dependent domains.", "The Model-as-Judge approach needs refinement to accurately assess uncertainty.", "Existing MMLU-Pro samples are biased and require balancing the amount of reasoning for a fair assessment."], "tldr": "Large Language Models(LLMs) must be uncertainty aware, but current methods focus on specific uncertainties while ignoring others. This study explores token-wise entropy and model-as-judge to assess multiple-choice questions across different topics. The experiments use Phi-4, Mistral and Qwen with sizes from 1.5B to 72B and 14 topics to estimate uncertainty.\n\nThe paper finds that response entropy predicts errors in knowledge-dependent domains and indicates question difficulty.  The correlation vanishes for reasoning-dependent domains. Data-related entropy is important for uncertainty estimates while model-as-judge needs refining. Existing MMLU-Pro samples are biased and should balance reasoning for fair LLM assessment. Thus this paper facilitates a reliable deployment of LLMs.", "affiliation": "Skolkovo Institute of Science and Technology (Skoltech)", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.01688/podcast.wav"}