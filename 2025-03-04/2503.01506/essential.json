{"importance": "**SampleMix** offers a novel approach to pre-training data mixing by considering both quality and diversity, leading to more efficient and effective language models. It inspires future research in automatic data evaluation metrics and code data mixing.", "summary": "SampleMix: Sample-wise Pre-training Data Mixing by Coordinating Data Quality and Diversity", "takeaways": ["SampleMix, a novel data mixing approach, enhances pre-training by evaluating sample quality and diversity.", "SampleMix outperforms existing domain-based methods and reduces the training steps by 1.4x to 2.1x.", "The study analyzes inter-domain overlaps and sample distribution within domains, highlighting areas for improvement in pre-training data mixture."], "tldr": "Existing data mixing methods for LLMs often determine domain weights and then sample uniformly, neglecting inter-domain overlaps and sample quality. This leads to suboptimal data distribution, hindering effective pre-training due to a failure in controlling global diversity of the constructed training dataset and ignoring fine-grained sample-specific features.\n\nTo address these shortcomings, **SampleMix** employs a bottom-up approach, performing global cross-domain sampling by evaluating the quality and diversity of each sample. It dynamically determines the optimal domain distribution, surpassing existing methods and cutting training steps by 1.4x to 2.1x. **SampleMix** coordinates quality and diversity, capturing commonalities and optimizing sample distribution.", "affiliation": "Meituan Group", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.01506/podcast.wav"}