{"importance": "This work is important to researchers as it **evaluates data selection methods at scales mirroring real-world instruction tuning**. It highlights **critical scaling issues in existing techniques** and introduces a **robust, efficient alternative**, paving the way for more effective and scalable LM training pipelines.", "summary": "RDS+ is the unsung hero for scaling instruction tuning data selection!", "takeaways": ["Many data selection methods falter or decline in performance with larger data pools, while RDS+ consistently improves.", "RDS+ outperforms human-curated mixtures and baselines when selecting data for multiple tasks.", "RDS+ achieves better performance with less compute compared to other methods, especially at larger data scales."], "tldr": "Instruction tuning uses high-quality data to refine language models, yet automated data selection struggles with large datasets. Current methods, tested on small datasets, face scaling issues when applied to the millions of samples used in practice. This paper systematically studies how well existing data selection methods perform when scaling up the dataset. It selects up to 2.5M samples from pools of up to 5.8M samples and evaluates them on 7 diverse tasks.\n\nThe study reveals that many recent methods fall short of random selection, even declining in performance with larger pools, despite using more compute. However, a variant of representation-based data selection (**RDS+**), using weighted mean pooling of pretrained LM hidden states, consistently outperforms other complex methods across all settings. RDS+ is also more compute-efficient, highlighting the importance of examining the scaling properties of data selection.", "affiliation": "University of Washington", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.01807/podcast.wav"}