[{"figure_path": "https://arxiv.org/html/2503.01807/x1.png", "caption": "Figure 1: Performance against estimated compute cost of varied data selection methods when selecting 10k points from data pools consisting of 200k (left points) and 5.8M (right points) data points in the single-task setup described in \u00a74.1. We do not run LESS with 5.8M samples due to its high compute cost. Most data selection methods do not improve in performance with a larger pool, with the exception of RDS+ and Embed (GTR). We shade the Pareto frontier of efficiency and performance in red.", "description": "This figure compares the performance and computational cost of various data selection methods for instruction tuning.  It shows the average performance achieved when selecting 10,000 data points from two different sized data pools: one with 200,000 samples and another significantly larger, with 5.8 million samples. The x-axis represents the estimated computational cost (FLOPS), while the y-axis shows the average performance across seven different tasks.  The plot reveals that most methods either fail to improve or even decrease in performance when given access to a larger data pool, despite increased computation. Notably, RDS+ and Embed (GTR) show improvements with larger pools. The Pareto frontier (the best trade-off between performance and efficiency) is highlighted in red.", "section": "4.1 Single-Task Data Selection"}, {"figure_path": "https://arxiv.org/html/2503.01807/x2.png", "caption": "Figure 2: Size and makeup of data pools considered in this work (unfiltered Tulu 2, 3) and in past work\u00a0(Xia et\u00a0al., 2024; Chen et\u00a0al., 2024; Li et\u00a0al., 2024b). We provide the size of each pool on top of each bar. Each color represents a different dataset. See App.\u00a0B for more details on data pool composition.", "description": "This figure compares the size and composition of the datasets used in this research with those from prior studies.  The datasets in this paper, 'Unfiltered T\u00dcLU 2' and 'Unfiltered T\u00dcLU 3', are significantly larger and more diverse than those in previous works (Xia et al., 2024; Chen et al., 2024; Li et al., 2024b).  The figure uses a stacked bar chart to visualize the relative proportions of different data sources within each dataset pool. The total number of samples in each pool is shown above each bar.  More details about the composition of these datasets are available in Appendix B.", "section": "3.2 Data Pool"}, {"figure_path": "https://arxiv.org/html/2503.01807/x3.png", "caption": "Figure 3: Average multi-task performance against FLOPs cost (including selection) for balanced random and RDS+. We label points with the % of the total data pool used. RDS+ outperforms random selection significantly when selecting less data, and is more FLOPs efficient at larger selection sizes. See App.\u00a0E for details on FLOPs estimates.", "description": "This figure compares the average performance across multiple tasks of two data selection methods: balanced random sampling and RDS+.  The x-axis shows the estimated FLOPs (floating point operations) cost, encompassing both the data selection process and model training. The y-axis displays the resulting average performance.  Different points on the graph represent using varying percentages of the total data pool for selection.  The key finding illustrated is that RDS+ consistently outperforms balanced random sampling, particularly when fewer data points are selected, and becomes significantly more FLOP-efficient as the amount of selected data increases.", "section": "4 Results"}, {"figure_path": "https://arxiv.org/html/2503.01807/x4.png", "caption": "Figure 4: Average multi-task performance against number of samples selected. RDS+ consistently beats balanced random at all data sizes tested, up to using the entire data pool.", "description": "This figure displays the average performance across multiple tasks for different numbers of samples selected from the data pool.  The performance of the RDS+ method (a type of data selection method) is compared against balanced random sampling. The x-axis represents the number of samples used, ranging from a small subset to the entire data pool. The y-axis shows the average performance across the multiple tasks.  The key takeaway is that RDS+ consistently outperforms random selection across all sample sizes.", "section": "4.3 Scaling Multi-task Selection"}, {"figure_path": "https://arxiv.org/html/2503.01807/x5.png", "caption": "Figure 5: Histogram of RDS scores for the top 10,000 samples picked for GSM8k and AlpacaEval from the T\u00fclu\u00a02 unfiltered pool. We find that AlpacaEval instances have lower average similarity than GSM8k.", "description": "This histogram visualizes the distribution of similarity scores obtained using the Representation-based Data Selection (RDS) method.  The top 10,000 data points selected for the GSM8K and AlpacaEval tasks from the T\u00fclu 2 unfiltered dataset are analyzed. The x-axis represents the cosine similarity scores, and the y-axis shows the frequency of those scores.  The comparison reveals a key observation: AlpacaEval instances exhibit lower average similarity scores compared to GSM8K instances, suggesting a difference in data characteristics relevant to the two tasks.", "section": "4.1 Single-Task Data Selection"}, {"figure_path": "https://arxiv.org/html/2503.01807/x6.png", "caption": "Figure 6: Breakdown of what data gets selected when selecting 326,000 samples using RDS from the T\u00fclu\u00a02 unfiltered pool. \u2018Random\u2019 represents the samples chosen when randomly downsampling to 326,000 samples, and \u2018round-robin\u2019 refers to the samples selected by the multi-task round-robin selection.", "description": "Figure 6 shows the source distribution of the 326,000 data points selected using the RDS+ method from the unfiltered T\u00dcLU 2 dataset.  It compares this distribution to a random sample of the same size (326,000) from the same pool. The figure visually represents the proportion of data points selected from different sources (e.g., FLAN V2, ShareGPT, etc.) for both the RDS+ selection and the random selection.  The breakdown of the selected data points across various sources highlights the different data selection preferences between RDS+ and the random sampling method.  The 'round-robin' designation in the caption indicates that this is the data distribution obtained when selecting data for a multi-task scenario using a round-robin strategy.", "section": "4.2 Multi-task Selection"}, {"figure_path": "https://arxiv.org/html/2503.01807/x7.png", "caption": "Figure 7: Breakdown of what data gets selected when selecting 10,000 or 326,000 samples using RDS from the T\u00fclu\u00a02 unfiltered pool using various selection methods. Sample counts normalized to add to 1. \u2018Random\u2019 represents the samples chosen when randomly downsampling to 326,000 samples. IFD has a clear bias to ShareGPT data at both sizes, while PPL has a clear bias to FLAN data.", "description": "Figure 7 presents a comparative analysis of data selection methods applied to the T\u00fclu 2 unfiltered dataset.  It shows the proportions of data samples selected from different sources (FLAN, ShareGPT, etc.) by various methods including IFD, Top-PPL, RDS+, and random selection. Two sample sizes are compared: 10,000 and 326,000. The figure visually demonstrates that certain methods exhibit biases towards specific data sources, for example, IFD showing a preference for ShareGPT data and Top-PPL favoring FLAN data, regardless of the sample size.  The random selection serves as a baseline for comparison, illustrating the non-uniformity of the other methods' selections.", "section": "4.2 Multi-task Data Selection"}]