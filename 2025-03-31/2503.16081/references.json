{"references": [{"fullname_first_author": "Christopher JCH Watkins", "paper_title": "Q-learning", "publication_date": "1992-01-01", "reason": "This paper is important because it introduces Q-learning, a classical reinforcement learning method that inspires the Dynamic KL divergence strategy used in the proposed approach."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-01-01", "reason": "This is an important paper as it introduces LLaVA, a Visual Instruction Tuning method, that serves as one of the multimodal large language model (MLLM) foundations for the developments in this field."}, {"fullname_first_author": "Peng Wang", "paper_title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution", "publication_date": "2024-09-01", "reason": "This paper is significant as it introduces the Qwen2-VL model which is used as the backbone model for the experiments, which illustrates its importance for the validation and evaluation in the present work."}, {"fullname_first_author": "Daya Guo", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning", "publication_date": "2025-01-01", "reason": "This paper is crucial because it presents DeepSeek-R1, a model using reinforcement learning with verifiable rewards (RLVR) which inspires the approach and reward model used in OThink-MR1, influencing both the methodology and experimental setup."}, {"fullname_first_author": "Liang Chen", "paper_title": "RLVR in Vision Language Models: Findings, Questions and Directions", "publication_date": "2025-02-01", "reason": "This paper explores the use of Reinforcement Learning with Verifiable Rewards (RLVR) in Vision Language Models, a key concept that the current paper builds upon and extends for multimodal tasks, particularly in adapting the RLVR approach from Visual-RFT for geometry reasoning."}]}