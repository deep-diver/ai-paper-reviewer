[{"figure_path": "https://arxiv.org/html/2503.21614/x1.png", "caption": "Figure 1: \nTop:\nTo answer the elementary school-level math problem, LRM (QwQ-32B) consumes altogether 1248 tokens, while the instruct LLM counterpart (Qwen2.5-32B-Instruct) only needs 30 tokens.\nBottom: The distribution of generation length of two models on a mixed set of math problems sourced from GSM8K, MATH-500, and AIME 2024.", "description": "Figure 1 demonstrates the inefficiency of Large Reasoning Models (LRMs) compared to instruction-tuned LLMs.  The top panel shows a simple math problem where the LRM QwQ-32B generates an excessively long response (1248 tokens), while the instruction-tuned LLM Qwen2.5-32B-Instruct provides a concise answer (30 tokens). The bottom panel presents histograms illustrating the distribution of generated token lengths for both models across a more extensive set of math problems from three benchmark datasets (GSM8K, MATH-500, and AIME 2024). The significant difference in token length highlights the inefficiency of the LRM, underscoring the paper's focus on improving reasoning efficiency.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.21614/x2.png", "caption": "Figure 2: \nIn this paper, we comprehensively study methods for efficient reasoning from the stages of Per-training, Supervised Fine-tuning (SFT), Reinforcement Learning (RL), and Inference.", "description": "This figure illustrates the stages involved in achieving efficient reasoning in large reasoning models (LRMs).  It highlights the different methods used at each stage, starting from pre-training where the model's foundational knowledge is established. Then, the model undergoes supervised fine-tuning (SFT) to refine its reasoning capabilities, followed by reinforcement learning (RL) to further optimize its performance. Finally, the inference stage depicts how the model uses this learned knowledge to solve problems efficiently.  Each stage is visually represented and connected to show the flow of development from foundational training to efficient reasoning.", "section": "1.1 Structure of the Survey"}, {"figure_path": "https://arxiv.org/html/2503.21614/x3.png", "caption": "Figure 3: Taxonomy of efficient reasoning methods for LRMs and future directions.", "description": "This figure presents a taxonomy of methods designed to improve reasoning efficiency in Large Reasoning Models (LRMs).  It categorizes techniques based on when they are applied in the LRM lifecycle: pre-training, supervised fine-tuning (SFT), reinforcement learning (RL), and inference.  The taxonomy includes several categories of methods such as Length Budgeting, System Switching, Model Switching and Parallel Search. For each category, specific techniques are listed and referenced. The figure also highlights promising future research directions related to efficient reasoning in LRMs, including efficient multimodality and video reasoning, linearization, and the development of improved benchmarks for evaluation.", "section": "1.1 Structure of the Survey"}, {"figure_path": "https://arxiv.org/html/2503.21614/x4.png", "caption": "Figure 4: To translate a single Chinese sentence (\u201cHow to think efficiently?\u201d) into English, recent LRM consumes more than 2000 tokens to reason.", "description": "The figure illustrates the excessive token consumption by Large Reasoning Models (LRMs) during inference.  A simple task of translating the Chinese sentence \u201c\u5982\u4f55\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406?\u201d (How to think efficiently?) into English requires over 2000 tokens. This highlights the inefficiency of current LRMs in terms of token economy, where concise reasoning is crucial for both efficiency and scalability. The high token usage suggests the need for methods and techniques to make LRMs more efficient in their reasoning processes.", "section": "Efficient Reasoning during Inference"}, {"figure_path": "https://arxiv.org/html/2503.21614/x5.png", "caption": "Figure 5: Illustration of efficient reasoning during inference. (1) Length Budgeting limits intermediate tokens to reduce overhead; (2) System Switch dynamically alternates between fast, intuitive and slow, deliberate reasoning; (3) Model Switch directs queries to optimal models based on task difficulty; (4) Parallel Search generates and prunes candidate outputs concurrently to cut latency.", "description": "This figure illustrates four key strategies for efficient reasoning during inference.  Length budgeting controls the number of tokens used to limit computational overhead. System switching dynamically shifts between fast, intuitive System 1 reasoning and slower, more deliberate System 2 reasoning, optimizing for task difficulty. Model switching routes queries to models best suited for that task based on its complexity. Parallel search simultaneously explores multiple reasoning paths and discards less promising ones to reduce latency.", "section": "Efficient Reasoning during Inference"}, {"figure_path": "https://arxiv.org/html/2503.21614/x6.png", "caption": "Figure 6: \nIllustration of efficient reasoning during SFT. (a) Original SFT: Standard training with sequential token generation. (b) Reasoning Chain Compression: Training with token skipping to simplify reasoning. (c) Latent-Space SFT: Iterative training using continuous hidden states for more efficient reasoning.", "description": "Figure 6 illustrates three different approaches to efficient reasoning during supervised fine-tuning (SFT).  (a) shows the standard SFT process, where models are trained to generate a sequence of tokens representing a reasoning chain. (b) demonstrates reasoning chain compression.  Here, training data is modified to skip less important tokens, leading to shorter, more efficient reasoning chains.  (c) depicts latent-space SFT, where the training process iteratively uses continuous hidden states, thus avoiding the need to generate explicit reasoning steps, resulting in greater efficiency.", "section": "Efficient Reasoning with SFT"}, {"figure_path": "https://arxiv.org/html/2503.21614/x7.png", "caption": "Figure 7: \nIllustration of efficiency training during the RL phase. Sub-Figures (a) and (b) illustrate the representative approach using length reward and not using length reward, respectively.", "description": "Figure 7 illustrates two distinct reinforcement learning (RL) strategies for enhancing efficiency in large language models (LLMs).  Subfigure (a) depicts a method that incorporates a length reward into the training process.  This reward incentivizes the model to generate concise and efficient reasoning chains, balancing the need for brevity with the goal of accurate responses. Subfigure (b) presents an alternative RL approach that does not rely on explicit length rewards.  Instead, it focuses on optimizing other aspects of the model's behavior, possibly related to reasoning quality or computational cost, to achieve efficient performance without explicitly targeting chain length.  Both subfigures showcase how RL can be used to shape the model's behavior to maximize the intelligence per token.", "section": "Efficient Reasoning with Reinforcement Learning"}, {"figure_path": "https://arxiv.org/html/2503.21614/x8.png", "caption": "Figure 8: \nIllustration of efficient reasoning during pretraining: (a) Standard transformer pretraining utilizing text tokens; (b) Pretraining the transformer in latent space; (c) Employing linear models for pretraining instead of self-attention transformers; (d) Linearization methods that transform standard transformer models into linear models.", "description": "Figure 8 illustrates various approaches to efficient reasoning during the pretraining phase of large language models (LLMs).  Panel (a) shows standard transformer-based pretraining using text tokens, which incurs high computational costs due to the quadratic complexity of self-attention. Panel (b) presents an alternative approach where the transformer is pretrained in a latent space, aiming to reduce complexity by operating on lower-dimensional representations. Panel (c) explores the use of linear models in pretraining to further decrease computational burden by avoiding the quadratic complexity of self-attention mechanisms altogether. Lastly, Panel (d) shows linearization methods that transform standard transformer models into linear models, offering an additional path towards more efficient pretraining.", "section": "Efficient Reasoning during Pretraining"}]