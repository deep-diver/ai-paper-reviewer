{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-15", "reason": "This report details the technical specifications of OpenAI's GPT-4, a foundational LRM that set a benchmark for reasoning capabilities."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "publication_date": "2022-04-15", "reason": "This paper introduces the chain-of-thought method which allows large language models to generate intermediate reasoning steps."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is All You Need", "publication_date": "2017-01-01", "reason": "This paper introduces the Transformer architecture which forms the backbone of most large language models, including LRMs."}, {"fullname_first_author": "Daniel Kahneman", "paper_title": "Thinking, Fast and Slow", "publication_date": "2011-01-01", "reason": "This book introduces the dual process theory and has had a significant impact on the interpretation of large language models."}, {"fullname_first_author": "Wayne Xin Zhao", "paper_title": "A Survey of Large Language Models", "publication_date": "2023-03-31", "reason": "This paper provides a comprehensive overview of large language models including Large Reasoning Models."}]}