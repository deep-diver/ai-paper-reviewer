[{"figure_path": "https://arxiv.org/html/2503.21732/extracted/6315866/figs/pipeline.png", "caption": "Figure 1: SparseFlex VAE achieves high-fidelity reconstruction and generalization from point clouds. Benefiting from a sparse-structured differentiable isosurface surface representation and an efficient frustum-aware sectional voxel training strategy, our SparseFlex VAE demonstrates the state-of-the-art performance on complex geometries (left), open surfaces (top right), and even interior structures (bottom right), facilitating the high-quality image-to-3D generation with arbitrary topology.", "description": "SparseFlex VAE, a novel variational autoencoder, achieves high-fidelity 3D shape reconstruction and generation from point cloud inputs.  Its success stems from a sparse-structured, differentiable isosurface representation and an efficient training strategy (frustum-aware sectional voxel training). This allows it to surpass state-of-the-art performance on complex shapes with arbitrary topology, including intricate geometries, open surfaces, and even internal structures, paving the way for high-quality image-to-3D generation.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.21732/extracted/6315866/figs/strategy.png", "caption": "Figure 2: Overview of the SparseFlex VAE pipeline. SparseFlex VAE takes point clouds sampled from a mesh as input, voxelizes them, and aggregates their features into each voxel. A sparse transformer encoder-decoder compresses the structured feature into a more compact latent space, followed by a self-pruning upsampling for higher resolution. Finally, the structured features are decoded to SparseFlex through a linear layer. Using the frustum-aware section voxel training strategy, we can train the entire pipeline more efficiently by rendering loss.", "description": "This figure illustrates the SparseFlex VAE pipeline.  The process begins with point cloud data sampled from a 3D mesh.  These points are voxelized, meaning they're grouped into volumetric units (voxels), and their features are combined within each voxel. A sparse transformer network (encoder-decoder) then compresses these structured voxel features into a lower-dimensional latent space, which efficiently represents the 3D shape. The process then uses a self-pruning upsampling technique to increase the resolution of the representation. Finally, a linear layer decodes the latent space features back into the SparseFlex representation (a sparse collection of voxels representing the shape's surface). Importantly, the entire pipeline uses a 'frustum-aware sectional voxel training strategy,' significantly increasing training efficiency by rendering losses (only calculating the loss for voxels currently visible from the camera viewpoint).", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.21732/x2.png", "caption": "Figure 3: Frustum-aware sectional voxel training. The previous mesh-based rendering training strategy (left) requires activating the entire dense grid to extract the mesh surface, even though only a few voxels are necessary during rendering. In contrast, our approach (right) adaptively activates the relevant voxels and enables the reconstruction of mesh interiors only using rendering supervision.", "description": "This figure illustrates the core concept of frustum-aware sectional voxel training. The left panel depicts the conventional method of mesh-based rendering, which necessitates activating every voxel in the dense grid to extract the mesh surface. This method is highly inefficient, especially when only a few voxels are essential for rendering. In contrast, the right panel demonstrates the proposed approach.  This method selectively activates only the relevant voxels within the camera's viewing frustum, resulting in significant computational and memory savings.  Furthermore, this approach uniquely allows for the reconstruction of mesh interiors, using only rendering supervision, by strategically positioning the virtual camera. The figure highlights the superior efficiency and capabilities of the proposed method compared to the conventional dense grid approach.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.21732/x3.png", "caption": "Figure 4: Qualitative comparison of VAE reconstruction between ours and other state-of-the-art baselines. Our approach demonstrate superior performance in reconstructing complex shapes, open surfaces, and even interior structures.", "description": "Figure 4 presents a qualitative comparison of 3D shape reconstruction results from various state-of-the-art Variational Autoencoders (VAEs), including the proposed SparseFlex VAE.  The figure showcases the superior performance of SparseFlex in handling complex geometries, open surfaces (shapes with incomplete boundaries), and even interior structures (reconstructing the insides of 3D objects).  The comparison is visual, highlighting the detailed reconstruction capabilities of SparseFlex compared to other leading methods, demonstrating its ability to accurately capture fine details and complex topologies.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.21732/x4.png", "caption": "Figure 5: Qualitative comparison of VAE reconstruction quality between our method with different resolution and TRELLIS.", "description": "Figure 5 presents a qualitative comparison of 3D shape reconstruction results obtained using the SparseFlex VAE with different resolutions (256\u00b3, 512\u00b3, and 1024\u00b3) and the TRELLIS method.  The figure visually showcases the impact of increasing resolution on the fidelity of reconstructed 3D shapes. By comparing the output of SparseFlex VAE at various resolutions to the TRELLIS results, the improvements in accuracy and detail preservation with higher resolutions are highlighted.", "section": "4. Experiments"}]