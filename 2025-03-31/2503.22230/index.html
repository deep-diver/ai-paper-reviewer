<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback &#183; HF Daily Paper Reviews by AI"><meta name=description content="This paper enhances Reinforcement Learning from Human Feedback (RLHF) by tackling reward hacking and response diversity issues through improved data construction methods."><meta name=keywords content="Machine Learning,Reinforcement Learning,üè¢ ByteDance Seed,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/2025-03-31/2503.22230/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/2025-03-31/2503.22230/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback"><meta property="og:description" content="This paper enhances Reinforcement Learning from Human Feedback (RLHF) by tackling reward hacking and response diversity issues through improved data construction methods."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="2025-03-31"><meta property="article:published_time" content="2025-03-28T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-28T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="üè¢ ByteDance Seed"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/2025-03-31/2503.22230/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/2025-03-31/2503.22230/cover.png"><meta name=twitter:title content="Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback"><meta name=twitter:description content="This paper enhances Reinforcement Learning from Human Feedback (RLHF) by tackling reward hacking and response diversity issues through improved data construction methods."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"2025-03-31s","name":"Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback","headline":"Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback","abstract":"This paper enhances Reinforcement Learning from Human Feedback (RLHF) by tackling reward hacking and response diversity issues through improved data construction methods.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/2025-03-31\/2503.22230\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-03-28T00:00:00\u002b00:00","datePublished":"2025-03-28T00:00:00\u002b00:00","dateModified":"2025-03-28T00:00:00\u002b00:00","keywords":["Machine Learning","Reinforcement Learning","üè¢ ByteDance Seed"],"mainEntityOfPage":"true","wordCount":"3814"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-27</p></a><a href=/ai-paper-reviewer/2025-03-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-28</p></a><a href=/ai-paper-reviewer/2025-03-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-31</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-27</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-28</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-31</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/2025-03-31/2503.22230/cover_hu5784609719483315664.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/2025-03-31/>2025-03-31s</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/2025-03-31/2503.22230/>Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-03-28T00:00:00+00:00>28 March 2025</time><span class="px-2 text-primary-500">&#183;</span><span>3814 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">18 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_2025-03-31/2503.22230/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_2025-03-31/2503.22230/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/machine-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Machine Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/reinforcement-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Reinforcement Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-bytedance-seed/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ ByteDance Seed</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#rlhf-data-scaling>RLHF Data Scaling</a></li><li><a href=#hybrid-reward-model>Hybrid Reward Model</a></li><li><a href=#pre-ppo-strategy>Pre-PPO Strategy</a></li><li><a href=#coding-task-focus>Coding Task Focus</a></li><li><a href=#response-diversity>Response Diversity</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#rlhf-data-scaling>RLHF Data Scaling</a></li><li><a href=#hybrid-reward-model>Hybrid Reward Model</a></li><li><a href=#pre-ppo-strategy>Pre-PPO Strategy</a></li><li><a href=#coding-task-focus>Coding Task Focus</a></li><li><a href=#response-diversity>Response Diversity</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2503.22230</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Wei Shen et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-03-31</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2503.22230 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2503.22230 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2503.22230/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Recent RLHF research focuses on algorithmic advancements, with less attention to prompt-data construction and scalability. Addressing this gap, the paper explores data-driven bottlenecks that hinder RLHF performance scaling, focusing on reward hacking and decreasing response diversity. They introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. This approach not only exhibits enhanced resistance to reward hacking but also enables accurate assessment of responses against clearly defined ground-truth solutions.</p><p>To ensure response diversity and enhance learning effectiveness, the paper proposes a novel prompt-selection method named Pre-PPO, explicitly identifying training prompts that are inherently challenging and thus less prone to reward hacking. Prioritizing mathematical and coding tasks during the early phases of RLHF training significantly boosts performance, given that these tasks encode fine-grained response distinctions and possess clearly defined ground truths. Experiments across two model sizes validate the effectiveness and scalability of the proposed methods.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-a516bd05d5acac8a5c563319f90a7a60></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-a516bd05d5acac8a5c563319f90a7a60",{strings:[" A hybrid reward system (RTV + GenRM) enhances resistance to reward hacking. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-d275b7f26fca3b47b1b65a59469a0586></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-d275b7f26fca3b47b1b65a59469a0586",{strings:[" Prioritizing challenging prompts (Pre-PPO) improves response diversity and overall RLHF performance. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-e2b02f75169e3061b87dbfcdb9f71850></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-e2b02f75169e3061b87dbfcdb9f71850",{strings:[" Early-stage focus on mathematical and coding tasks boosts performance due to fine-grained response distinctions. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This work is crucial for <strong>improving RLHF, ensuring LLMs align with human values</strong> while being resistant to reward hacking. It provides practical methodologies for data construction, benefiting researchers working on LLM alignment and safety.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/x1.png alt></figure></p><blockquote><p>üîº This figure illustrates the RLHF training framework, which involves two main stages. The first stage is Reward Model Training, where three reward models are trained: the Bradley-Terry (BT) model (trained on pairwise comparisons of human preferences), the Generative Reward Model (GenRM) (assigns reward scores based on ground truth or BT model&rsquo;s best-of-N selections), and Reasoning Task Verifiers (RTV) (specialized validators for specific tasks, such as code execution for programming tasks). The second stage is Reinforcement Learning Optimization, where the language model is iteratively optimized using PPO, guided by GenRM and RTV. Pre-PPO prompt selection is employed to identify the most challenging prompts, leading to robust performance and alignment enhancements.</p><details><summary>read the caption</summary>Figure 1: Overview of the RLHF Training Framework. Our proposed pipeline consists of two sequential phases: (1) Reward Model Training, where we construct three complementary reward models‚Äînamely, the Bradley-Terry (BT) model, the Generative Reward Model (GenRM), and Reasoning Task Verifiers (RTV). Specifically, the BT model is trained on pairwise comparisons to capture human preferences, while the GenRM assigns explicit reward scores aligned with these preferences using either ground-truth solutions (for reasoning tasks) or the best-of-N selections identified by the BT model (for general tasks). The RTV component implements specialized validators tailored to specific task requirements, such as code-execution sandboxes for evaluating programming tasks; and (2) Reinforcement Learning Optimization, in which the language model is iteratively optimized using PPO under guidance from both GenRM and RTV. This stage leverages carefully selected training prompts identified through our Pre-PPO prompt-selection method and employs strategic optimization techniques to robustly enhance model performance and alignment.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S4.T1.1.1><thead class=ltx_thead><tr class=ltx_tr id=S4.T1.1.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=S4.T1.1.1.1.1.1><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.1.1.1.1>Method</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.1.1.1.1.2><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.1.1.2.1>Logical</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.1.1.1.1.3><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.1.1.3.1>IF</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.1.1.1.1.4><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.1.1.4.1>STEM</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.1.1.1.1.5><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.1.1.5.1>Coding</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.1.1.1.1.6><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.1.1.6.1>NLP</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.1.1.1.1.7><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.1.1.7.1>Knowledge</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.1.1.1.1.8><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.1.1.8.1>CU</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.1.1.1.1.9><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.1.1.9.1>OOD</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.1.1.1.1.10><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.1.1.10.1>Overall</span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S4.T1.1.1.2.1><th class="ltx_td ltx_th ltx_th_row" id=S4.T1.1.1.2.1.1></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column" id=S4.T1.1.1.2.1.2><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.2.1.2.1>Reasoning</span></th><th class="ltx_td ltx_th ltx_th_column" id=S4.T1.1.1.2.1.3></th><th class="ltx_td ltx_th ltx_th_column" id=S4.T1.1.1.2.1.4></th><th class="ltx_td ltx_th ltx_th_column" id=S4.T1.1.1.2.1.5></th><th class="ltx_td ltx_th ltx_th_column" id=S4.T1.1.1.2.1.6></th><th class="ltx_td ltx_th ltx_th_column" id=S4.T1.1.1.2.1.7></th><th class="ltx_td ltx_th ltx_th_column" id=S4.T1.1.1.2.1.8></th><th class="ltx_td ltx_th ltx_th_column" id=S4.T1.1.1.2.1.9></th><td class=ltx_td id=S4.T1.1.1.2.1.10></td></tr><tr class=ltx_tr id=S4.T1.1.1.3.2><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id=S4.T1.1.1.3.2.1>Initial-Run (V1.0)</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S4.T1.1.1.3.2.2>27.1</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S4.T1.1.1.3.2.3>34.8</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S4.T1.1.1.3.2.4>49.3</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S4.T1.1.1.3.2.5>51.6</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S4.T1.1.1.3.2.6>24.7</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S4.T1.1.1.3.2.7>37.0</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S4.T1.1.1.3.2.8>40.0</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S4.T1.1.1.3.2.9>39.0</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S4.T1.1.1.3.2.10>37.7</th></tr><tr class=ltx_tr id=S4.T1.1.1.4.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T1.1.1.4.3.1>Baseline-Small (V1.0)</th><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.4.3.2>26.4</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.4.3.3>35.1</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.4.3.4>48.8</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.4.3.5>50.9</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.4.3.6>24.8</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.4.3.7>36.1</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.4.3.8>40.6</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.4.3.9>40.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.4.3.10>37.7</td></tr><tr class=ltx_tr id=S4.T1.1.1.5.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.1.1.5.4.1>Data Scale-Small (V1.0)</th><td class="ltx_td ltx_align_center" id=S4.T1.1.1.5.4.2>28.7</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.5.4.3>36.1</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.5.4.4>50.4</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.5.4.5>53.3</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.5.4.6>24.2</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.5.4.7>36.6</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.5.4.8>39.7</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.5.4.9>43.6</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.5.4.10>38.8</td></tr><tr class=ltx_tr id=S4.T1.1.1.6.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.1.1.6.5.1>Improvement</th><td class="ltx_td ltx_align_center" id=S4.T1.1.1.6.5.2><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.6.5.2.1>+2.4</span></td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.6.5.3>+1.1</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.6.5.4><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.6.5.4.1>+1.6</span></td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.6.5.5><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.6.5.5.1>+2.4</span></td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.6.5.6>-0.6</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.6.5.7>+0.6</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.6.5.8>-0.9</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.6.5.9><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.6.5.9.1>+3.1</span></td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.6.5.10><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.6.5.10.1>+1.1</span></td></tr><tr class=ltx_tr id=S4.T1.1.1.7.6><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T1.1.1.7.6.1>Baseline-Large (V1.0)</th><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.7.6.2>37.3</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.7.6.3>46.3</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.7.6.4>55.6</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.7.6.5>55.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.7.6.6>45.7</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.7.6.7>46.8</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.7.6.8>58.4</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.7.6.9>54.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.7.6.10>49.7</td></tr><tr class=ltx_tr id=S4.T1.1.1.8.7><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.1.1.8.7.1>Data Scale-Large (V1.0)</th><td class="ltx_td ltx_align_center" id=S4.T1.1.1.8.7.2>39.6</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.8.7.3>46.0</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.8.7.4>56.5</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.8.7.5>58.7</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.8.7.6>44.9</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.8.7.7>47.9</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.8.7.8>59.6</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.8.7.9>55.6</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.8.7.10>50.8</td></tr><tr class=ltx_tr id=S4.T1.1.1.9.8><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.1.1.9.8.1>Improvement</th><td class="ltx_td ltx_align_center" id=S4.T1.1.1.9.8.2><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.9.8.2.1>+2.2</span></td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.9.8.3>-0.4</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.9.8.4>+0.9</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.9.8.5><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.9.8.5.1>+3.2</span></td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.9.8.6>-0.8</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.9.8.7>+1.1</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.9.8.8>+1.2</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.9.8.9><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.9.8.9.1>+1.2</span></td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.9.8.10><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.9.8.10.1>+1.1</span></td></tr><tr class=ltx_tr id=S4.T1.1.1.10.9><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T1.1.1.10.9.1>Baseline-Small (V2.0)</th><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.10.9.2>17.6</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.10.9.3>26.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.10.9.4>26.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.10.9.5>41.2</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.10.9.6>21.2</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.10.9.7>28.2</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.10.9.8>19.6</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.10.9.9>21.3</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.10.9.10>23.9</td></tr><tr class=ltx_tr id=S4.T1.1.1.11.10><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.1.1.11.10.1>Data Scale-Small (V2.0)</th><td class="ltx_td ltx_align_center" id=S4.T1.1.1.11.10.2>19.9</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.11.10.3>27.3</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.11.10.4>29.5</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.11.10.5>42.3</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.11.10.6>21.8</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.11.10.7>28.9</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.11.10.8>20.2</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.11.10.9>21.7</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.11.10.10>25.1</td></tr><tr class=ltx_tr id=S4.T1.1.1.12.11><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.1.1.12.11.1>Improvement</th><td class="ltx_td ltx_align_center" id=S4.T1.1.1.12.11.2><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.12.11.2.1>+2.3</span></td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.12.11.3>+0.8</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.12.11.4><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.12.11.4.1>+3.0</span></td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.12.11.5><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.12.11.5.1>+1.1</span></td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.12.11.6>+0.6</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.12.11.7>+0.7</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.12.11.8><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.12.11.8.1>+0.8</span></td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.12.11.9>+0.4</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.12.11.10><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.12.11.10.1>+1.2</span></td></tr><tr class=ltx_tr id=S4.T1.1.1.13.12><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T1.1.1.13.12.1>Baseline-Large (V2.0)</th><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.13.12.2>29.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.13.12.3>36.3</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.13.12.4>28.0</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.13.12.5>48.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.13.12.6>29.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.13.12.7>45.6</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.13.12.8>36.8</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.13.12.9>35.0</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.1.1.13.12.10>34.0</td></tr><tr class=ltx_tr id=S4.T1.1.1.14.13><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.1.1.14.13.1>Data Scale-Large (V2.0)</th><td class="ltx_td ltx_align_center" id=S4.T1.1.1.14.13.2>31.2</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.14.13.3>36.4</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.14.13.4>31.9</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.14.13.5>50.7</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.14.13.6>32.3</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.14.13.7>45.5</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.14.13.8>36.6</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.14.13.9>37.1</td><td class="ltx_td ltx_align_center" id=S4.T1.1.1.14.13.10>35.4</td></tr><tr class=ltx_tr id=S4.T1.1.1.15.14><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S4.T1.1.1.15.14.1>Improvement</th><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.1.1.15.14.2><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.15.14.2.1>+1.8</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.1.1.15.14.3>+0.1</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.1.1.15.14.4><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.15.14.4.1>+3.9</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.1.1.15.14.5><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.15.14.5.1>+2.1</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.1.1.15.14.6><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.15.14.6.1>+2.7</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.1.1.15.14.7>-0.1</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.1.1.15.14.8>-0.2</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.1.1.15.14.9><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.15.14.9.1>+2.1</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.1.1.15.14.10><span class="ltx_text ltx_font_bold" id=S4.T1.1.1.15.14.10.1>+1.4</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of two RLHF methods: the proposed &lsquo;Data Scale&rsquo; method (combining Pre-PPO and prioritizing math/coding tasks) and a baseline PPO-based RLHF method. The comparison is done using two evaluation datasets (V1.0 and V2.0) across eight different abilities: logical reasoning, instruction-following, STEM tasks, coding, natural language processing, knowledge, contextual understanding, and out-of-distribution generalization. Statistically significant improvements by the Data Scale method are highlighted in bold.</p><details><summary>read the caption</summary>Table 1: We present a performance comparison between our proposed method, termed ‚ÄôData Scale‚Äô (combining Pre-PPO and prioritizing mathematical and coding tasks first) and a baseline method (PPO-based RLHF) on evaluation datasets V1.0 and V2.0. Results are reported across various abilities, including logical reasoning, instruction-following (IF), STEM tasks, coding, natural language processing (NLP), knowledge, contextual understanding (CU), and out-of-distribution generalization (OOD). Results highlighted in bold indicate statistically significant improvements.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">RLHF Data Scaling<div id=rlhf-data-scaling class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#rlhf-data-scaling aria-label=Anchor>#</a></span></h4><p>While algorithmic advancements in RLHF are crucial, the significance of <strong>RLHF data construction and scaling</strong> is often understated. Data-driven bottlenecks, specifically <strong>reward hacking and diminishing response diversity</strong>, hinder performance scaling. Addressing reward hacking requires hybrid reward systems like combining reasoners and generative models which shows resistance. Furthermore, preemptive prompt selection, such as identifying inherently challenging prompts (Pre-PPO) for training, can ensure diversity and enhance learning effectiveness. Prioritizing tasks, like <strong>mathematics and coding</strong>, that encode fine-grained distinctions is a strategy to enhance RLHF performance.</p><h4 class="relative group">Hybrid Reward Model<div id=hybrid-reward-model class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#hybrid-reward-model aria-label=Anchor>#</a></span></h4><p>A hybrid reward model seems like a promising approach in reinforcement learning, particularly when dealing with complex tasks or environments. It involves <strong>combining multiple reward signals or models</strong> to provide a more comprehensive and nuanced assessment of an agent&rsquo;s behavior. This could involve integrating intrinsic and extrinsic rewards, shaping rewards, or even learning from demonstrations. One potential benefit is <strong>increased robustness to reward hacking</strong>, where agents exploit loopholes in a single reward function. The key challenge lies in <strong>effectively balancing and weighting these different reward components</strong>, ensuring that they align with the desired objectives and don&rsquo;t lead to unintended consequences. Careful design and experimentation are crucial for realizing the full potential of a hybrid reward model. By leveraging the strengths of diverse reward signals, hybrid reward model can improve learning efficiency, stability, and generalization capabilities, <strong>Ultimately achieving superior policy performance</strong>.</p><h4 class="relative group">Pre-PPO Strategy<div id=pre-ppo-strategy class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#pre-ppo-strategy aria-label=Anchor>#</a></span></h4><p>The <strong>Pre-PPO strategy</strong> represents a critical component of enhancing Reinforcement Learning from Human Feedback (RLHF) performance. It involves selecting training prompts based on their reward model scores before PPO, aiming to address the <strong>reward hacking</strong> phenomenon where models generate syntactically correct but semantically flawed responses. The initial experiment revealed that simply increasing the number of prompts doesn&rsquo;t improve RLHF performance. Analysis of reward scores of new prompts indicated a high proportion of scores, but <strong>manual inspection</strong> uncovered reward hacking behavior. The core idea is to utilize prompts with lower reward model scores, posing greater learning challenges and reducing susceptibility to reward hacking. These are then combined with the original dataset to retrain the RL model. Recognizing differing score distributions across task domains, the strategy involves normalizing these scores within each domain before selection.</p><h4 class="relative group">Coding Task Focus<div id=coding-task-focus class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#coding-task-focus aria-label=Anchor>#</a></span></h4><p>The paper underscores a strategic emphasis on coding tasks within RLHF, noting their resistance to reward hacking due to grounded evaluation metrics like code execution sandboxes. <strong>Prioritizing these tasks early accelerates fine-grained distinction learning, boosting overall performance</strong>. Unlike tasks reliant on subjective human feedback, coding&rsquo;s objective validation creates a stable learning signal. The &lsquo;Data Scale&rsquo; method further enhances coding performance, demonstrating the value of carefully curated data and robust evaluation. <strong>Early exposure to coding&rsquo;s inherent structure equips the model to discern subtle patterns</strong>, impacting mathematical reasoning. This focus combats overfitting and ensures more resilient skill acquisition, enabling sustained improvements throughout training. By strategically balancing diverse task types, the RLHF model achieves superior coding prowess and foundational knowledge that benefits the broader skill set.</p><h4 class="relative group">Response Diversity<div id=response-diversity class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#response-diversity aria-label=Anchor>#</a></span></h4><p><strong>Response diversity is a critical aspect of language model performance</strong>, particularly in RLHF. The paper acknowledges a decline in entropy during training, indicating a loss of variety in generated outputs, which can limit the model&rsquo;s ability to handle diverse tasks and contexts. While RLHF effectively aligns models, it risks over-optimization towards specific reward signals, sacrificing the breadth of possible responses. <strong>The challenge is to balance alignment with creativity and adaptability.</strong> The authors observe that different reward models (RTV, GenRM) influence response diversity differently, highlighting the need for careful reward design. Prioritizing tasks that inherently encourage diverse responses, such as creative writing, and mitigating reward hacking are potential strategies to maintain diversity without compromising alignment. A lack of diversity constrains the model and may lead to less useful outputs, with the model learning coarse differences in response only.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/train_all.png alt></figure></p><blockquote><p>üîº This figure displays the overall test scores obtained during the initial RLHF training experiment. Two datasets were used: an original dataset of one million prompts and a newly collected dataset of six million prompts, combined for a total of seven million prompts. Despite the substantial increase in data size, the results show that RLHF training did not lead to improved performance as measured by the overall test score. In fact, the model achieved its peak performance around training step 3500, after which the performance gradually decreased. This suggests that simply increasing the amount of training data may not guarantee improved performance in RLHF, implying the importance of data quality over quantity.</p><details><summary>read the caption</summary>Figure 2: Overall test scores from the initial run using an expanded dataset combining newly collected data (six million prompts) with the original dataset (one million prompts). Despite increasing dataset size substantially, RLHF did not yield improvements in performance. Additionally, the best performance was observed at around the 3,500-step mark, after which test scores gradually declined.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/QRM.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of reward scores obtained from a newly collected dataset of prompts. The x-axis displays the percentage of prompts, while the y-axis represents the reward score, ranging from 0 to 1. A score of 0.5 indicates that the model&rsquo;s output is comparable to the reference output. The figure reveals that approximately 90% of the prompts received scores exceeding 0.5 for both small and large language models, seemingly outperforming the reference outputs. However, a closer manual inspection uncovered that a significant number of high-scoring outputs exhibited reward hacking behaviors and were qualitatively inferior to the initially selected best outputs.</p><details><summary>read the caption</summary>Figure 3: Distribution of reward scores for newly collected prompts. The x-axis shows the percentage of prompts. The y-axis represents the reward score range from 0 to 1, with 0.5 indicating parity with the reference. Approximately 90% of prompts received scores above 0.5 for both small-size and large-size models, suggesting apparent superiority over reference outputs. However, manual inspection revealed that many high-scoring outputs exhibited reward hacking behavior and were qualitatively inferior to the original best-selected outcomes.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/FRAC.png alt></figure></p><blockquote><p>üîº This figure shows the proportion of math and coding prompts used during each training step of the Reinforcement Learning from Human Feedback (RLHF) process. It visually represents how the focus on math and coding tasks changes over the course of training, illustrating the strategy of prioritizing these task types early in the training pipeline before incorporating other kinds of prompts. The x-axis represents the training step, and the y-axis represents the fraction of prompts dedicated to either math or coding tasks.</p><details><summary>read the caption</summary>Figure 4: The distribution of prompts across both math and coding task during the training phases</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/AblationStudy.png alt></figure></p><blockquote><p>üîº This ablation study uses a small-sized language model to evaluate the individual contributions of three different RLHF training strategies: 1) Pre-PPO prompt selection, which prioritizes more challenging prompts, 2) Early Training Emphasis, which focuses on mathematical and coding tasks in the initial training phase, and 3) a combination of both Pre-PPO and Early Training Emphasis. The graph likely shows the overall performance of the model across various tasks as a function of training steps, allowing comparison of the performance achieved using each strategy against the baseline (no additional strategies). This visualization helps determine the effectiveness and potential synergy between the proposed strategies in improving RLHF performance.</p><details><summary>read the caption</summary>Figure 5: Ablation study on small-size model. We do the ablation study to demonstrate the effectiveness of each strategy. Early Training Emphasis refers to early training emphasis on mathematical and coding tasks</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/codemath_first.png alt></figure></p><blockquote><p>üîº This figure displays the results of an ablation study that demonstrates the impact of prioritizing mathematical and coding tasks during the early stages of Reinforcement Learning from Human Feedback (RLHF) training. Two line graphs show the performance of a model trained with this early emphasis strategy versus a baseline model across different training steps, with performance measured in terms of test scores on coding and STEM tasks. The graph clearly shows that the model that prioritizes these tasks outperforms the baseline, achieving comparable results much faster. In particular, the model that prioritizes coding and math tasks significantly surpasses the baseline model in coding performance within 1000 training steps.</p><details><summary>read the caption</summary>Figure 6: Early emphasis on mathematical and coding tasks significantly improves RLHF performance in both coding and STEM areas on Testset-V1.0. Notably, the coding performance with this approach surpasses the baseline within just 1000 training steps.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/RM_CMP.png alt></figure></p><blockquote><p>üîº This figure displays the reward hacking susceptibility and performance trends observed during Reinforcement Learning from Human Feedback (RLHF) training for three different reward models: Reasoning Task Verifiers (RTV), Generative Reward Model (GenRM), and Bradley-Terry Reward Model (BT). It illustrates how each model&rsquo;s performance and resistance to reward hacking changes over the course of RLHF training. This allows for a comparison of the effectiveness and robustness of the various reward models in guiding the model&rsquo;s learning process and avoiding reward hacking behavior.</p><details><summary>read the caption</summary>Figure 7: Comparison of Reward Hacking Susceptibility and Performance Trends for RTV, GenRM, and BT Reward Models During RLHF Training</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/datascale_preppo.png alt></figure></p><blockquote><p>üîº This figure displays the results of an experiment investigating the effect of increasing the amount of training data on the performance of the RLHF model using the Pre-PPO prompt selection strategy. The x-axis represents the percentage of newly collected prompts added to the original dataset (10%, 20%, and 50%), while the y-axis shows the overall RLHF performance. The results show a counter-intuitive trend: increasing the size of the training dataset, even with the Pre-PPO strategy, does not improve performance. Instead, it leads to a decrease in performance. This suggests that the quality of the training prompts is more important than the sheer quantity. In other words, simply adding more data may not improve the performance, and it may even hurt the performance if the additional data are low quality. This highlights the scarcity of high-quality training prompts in real-world scenarios and the need for carefully curated datasets.</p><details><summary>read the caption</summary>Figure 8: Impact of data scaling on Pre-PPO strategy performance. The graph shows the overall RLHF performance as the percentage of newly collected training data increases from 10% to 20% and 50%. Counter-intuitively, increasing the amount of training data leads to a noticeable degradation in performance, suggesting that high-quality training prompts are scarce in real-world settings and that simply scaling data quantity does not guarantee improvement.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Pre-PPO-BOOST.png alt></figure></p><blockquote><p>üîº Figure 9 presents a comparison of the performance of coding and mathematical tasks between the baseline RLHF approach and the proposed &lsquo;Data Scale&rsquo; method. The graph shows that the &lsquo;Data Scale&rsquo; method, which combines Pre-PPO prompt selection and early emphasis on mathematical and coding tasks, leads to significantly improved performance on both task types compared to the baseline. Specifically, the &lsquo;Data Scale&rsquo; method demonstrates faster and more substantial gains in performance over the training steps.</p><details><summary>read the caption</summary>Figure 9: Data Scale method boost both math and code performance.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/QRM_Score.png alt></figure></p><blockquote><p>üîº This figure shows the relationship between reward model scores and the diversity of model responses. It compares the performance of two versions of the Generative Reward Model (GenRM): one trained with ground truth and one without. The x-axis represents the maximum edit distance among five model responses for a given prompt. A higher edit distance indicates less diversity in the responses. The y-axis displays the average normalized reward model score for each edit distance bin. The figure demonstrates that the GenRM trained with ground truth is more sensitive to fine-grained response variations (lower edit distances) and assigns higher scores when responses show higher diversity. In contrast, the GenRM without ground truth shows a less pronounced relationship between diversity and score, reflecting a lower sensitivity to finer distinctions between responses.</p><details><summary>read the caption</summary>Figure 10: Comparison of Reward Model Scores across Different Edit Distance Bins for GenRM with and without Ground Truth.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Score_Diff.png alt></figure></p><blockquote><p>üîº This figure compares the differences in reward scores assigned by three different reward models (GenRM with ground truth, GenRM without ground truth, and RTV) across various levels of response diversity. The x-axis represents bins of maximum edit distances between responses, indicating the granularity of response differences (smaller distances mean finer-grained distinctions). The y-axis shows the normalized score differences within each bin. The figure aims to illustrate how each reward model&rsquo;s sensitivity to fine-grained response variations differs and whether that impacts the effectiveness of RLHF.</p><details><summary>read the caption</summary>Figure 11: Comparison of Score Difference across Different Edit Distance Bins for GenRM with and without Ground Truth, and RTV.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/entropy.png alt></figure></p><blockquote><p>üîº This figure illustrates the change in response entropy throughout the reinforcement learning from human feedback (RLHF) training process. Response entropy is a measure of the diversity of the model&rsquo;s generated responses. A higher entropy indicates greater response diversity, while a lower entropy suggests less diversity, with the model tending to produce more similar responses. The x-axis represents the training steps, and the y-axis shows the response entropy at each step. The plot reveals how the diversity of the model&rsquo;s responses changes over the course of RLHF training. This is a key metric to assess whether the training process is successfully improving the model&rsquo;s ability to generate a wide range of responses or is narrowing its output over time.</p><details><summary>read the caption</summary>(a) Response entropy change during the RLHF training process</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Entropy_Creation.png alt></figure></p><blockquote><p>üîº Figure 12(b) displays the change in response entropy during RLHF training for models using GenRM without ground truth. It compares the entropy of responses generated by the baseline RLHF model and the model trained using the improved method (DataScale). The x-axis represents training steps, while the y-axis represents the mean entropy across various responses. The plot visually demonstrates the impact of the proposed methods on maintaining response diversity throughout the training process.</p><details><summary>read the caption</summary>(b) The comparison of response entropy change during the RLHF training process</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Entropy_Math.png alt></figure></p><blockquote><p>üîº This figure compares the response entropy change during RLHF training between the baseline method and the proposed method. The response entropy is calculated separately for three different types of reward models: GenRM with ground truth, GenRM without ground truth, and RTV. The figure helps to analyze the impact of the proposed method on the response diversity of the model during training and how this method affects different types of reward models differently.</p><details><summary>read the caption</summary>(c) The comparison of response entropy change during the RLHF training process</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Entropy_RTV.png alt></figure></p><blockquote><p>üîº Figure 12(d) displays the change in response entropy over the course of RLHF training, specifically focusing on the performance of the RTV reward model. The graph visually represents how the diversity of model responses evolves as training progresses. It allows for a comparison between the baseline model&rsquo;s response diversity and the response diversity achieved by the model using the improved strategy. This comparison helps in evaluating the effectiveness of the proposed techniques in maintaining response diversity throughout the RLHF training process. The x-axis of the graph represents the training steps, while the y-axis corresponds to the mean response entropy.</p><details><summary>read the caption</summary>(d) The comparison of response entropy change during the RLHF training process</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Big_Dist.png alt></figure></p><blockquote><p>üîº Figure 12 presents a detailed analysis of how response diversity changes during the Reinforcement Learning from Human Feedback (RLHF) training process. It visualizes the entropy of model responses across various task categories, offering insights into the impact of different reward models and training strategies. Subfigures (a) to (d) show the change in response entropy over training steps for tasks supervised by distinct reward methods: Generative Reward Model (GenRM) with ground truth, GenRM without ground truth, and Reasoning Task Verifiers (RTV). These subfigures provide a granular view of how response diversity evolves under different reward mechanisms and highlight the effects of the proposed data selection and prioritization strategies on response diversity across different task types. The figure illustrates a decline in overall response diversity during RLHF training but highlights the importance of careful data curation and strategic training to maintain diversity.</p><details><summary>read the caption</summary>Figure 12: The comparison of response entropy change during the RLHF training process</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S4.T2.1><thead class=ltx_thead><tr class=ltx_tr id=S4.T2.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=S4.T2.1.1.1.1>Method</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T2.1.1.1.2>Knowledge</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T2.1.1.1.3>STEM</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T2.1.1.1.4>IF</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T2.1.1.1.5>Creation</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T2.1.1.1.6>Coding</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T2.1.1.1.7>Overall</th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S4.T2.1.2.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T2.1.2.1.1>Baseline-Large</th><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T2.1.2.1.2>63.3</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T2.1.2.1.3>76.7</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T2.1.2.1.4>46.7</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T2.1.2.1.5>52.1</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T2.1.2.1.6>24.8</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T2.1.2.1.7>67.2</td></tr><tr class=ltx_tr id=S4.T2.1.3.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T2.1.3.2.1>Data Scale-Large</th><td class="ltx_td ltx_align_center" id=S4.T2.1.3.2.2>66.1</td><td class="ltx_td ltx_align_center" id=S4.T2.1.3.2.3>80.6</td><td class="ltx_td ltx_align_center" id=S4.T2.1.3.2.4>48.3</td><td class="ltx_td ltx_align_center" id=S4.T2.1.3.2.5>54.6</td><td class="ltx_td ltx_align_center" id=S4.T2.1.3.2.6>53.3</td><td class="ltx_td ltx_align_center" id=S4.T2.1.3.2.7>71.0</td></tr><tr class=ltx_tr id=S4.T2.1.4.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T2.1.4.3.1>Improvement</th><td class="ltx_td ltx_align_center" id=S4.T2.1.4.3.2>+1.1</td><td class="ltx_td ltx_align_center" id=S4.T2.1.4.3.3><span class="ltx_text ltx_font_bold" id=S4.T2.1.4.3.3.1>+6.1</span></td><td class="ltx_td ltx_align_center" id=S4.T2.1.4.3.4>+1.7</td><td class="ltx_td ltx_align_center" id=S4.T2.1.4.3.5><span class="ltx_text ltx_font_bold" id=S4.T2.1.4.3.5.1>+5.8</span></td><td class="ltx_td ltx_align_center" id=S4.T2.1.4.3.6>+4.4</td><td class="ltx_td ltx_align_center" id=S4.T2.1.4.3.7><span class="ltx_text ltx_font_bold" id=S4.T2.1.4.3.7.1>+4.4</span></td></tr><tr class=ltx_tr id=S4.T2.1.5.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S4.T2.1.5.4.1>p-value</th><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T2.1.5.4.2>0.01</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T2.1.5.4.3>0.41</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T2.1.5.4.4>0.04</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T2.1.5.4.5>0.39</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T2.1.5.4.6>0.09</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T2.1.5.4.7>0.12</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a human evaluation comparing the performance of two RLHF methods: the proposed method (Pre-PPO with prioritized math and coding tasks) and a baseline PPO method. Evaluations were conducted across six abilities: Knowledge, STEM, Instruction-Following (IF), Creation, Coding, and an overall assessment. Statistically significant improvements (p&lt;0.05) from the proposed method are highlighted in bold. All scores represent aggregated results from human raters.</p><details><summary>read the caption</summary>Table 2: Performance comparison based on comprehensive human evaluations between our proposed method (combining Pre-PPO and prioritizing mathematical and coding tasks first) and the baseline method (PPO-based RLHF). Results are shown across multiple abilities, including Knowledge, STEM, Instruction-Following (IF), Creation, Coding, and Overall performance. Improvements highlighted in bold indicate statistically significant differences (p < 0.05). All metrics represent aggregated scores from human assessments.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S4.T3.1.1><tbody class=ltx_tbody><tr class=ltx_tr id=S4.T3.1.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=S4.T3.1.1.1.1.1><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.1.1.1.1>Method</span></th><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T3.1.1.1.1.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.1.1.2.1>Logical</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T3.1.1.1.1.3><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.1.1.3.1>IF</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T3.1.1.1.1.4><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.1.1.4.1>STEM</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T3.1.1.1.1.5><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.1.1.5.1>Coding</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T3.1.1.1.1.6><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.1.1.6.1>NLP</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T3.1.1.1.1.7><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.1.1.7.1>Knowledge</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T3.1.1.1.1.8><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.1.1.8.1>CU</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T3.1.1.1.1.9><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.1.1.9.1>OOD</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T3.1.1.1.1.10><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.1.1.10.1>Overall</span></td></tr><tr class=ltx_tr id=S4.T3.1.1.2.2><th class="ltx_td ltx_th ltx_th_row" id=S4.T3.1.1.2.2.1></th><td class="ltx_td ltx_align_center" id=S4.T3.1.1.2.2.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.2.2.2.1>Reasoning</span></td><td class=ltx_td id=S4.T3.1.1.2.2.3></td><td class=ltx_td id=S4.T3.1.1.2.2.4></td><td class=ltx_td id=S4.T3.1.1.2.2.5></td><td class=ltx_td id=S4.T3.1.1.2.2.6></td><td class=ltx_td id=S4.T3.1.1.2.2.7></td><td class=ltx_td id=S4.T3.1.1.2.2.8></td><td class=ltx_td id=S4.T3.1.1.2.2.9></td><td class=ltx_td id=S4.T3.1.1.2.2.10></td></tr><tr class=ltx_tr id=S4.T3.1.1.3.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T3.1.1.3.3.1>Baseline-Large (V2.0)</th><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.3.3.2>29.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.3.3.3>36.3</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.3.3.4>28.0</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.3.3.5>48.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.3.3.6>29.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.3.3.7>45.6</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.3.3.8>36.8</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.3.3.9>35.0</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.3.3.10>34.0</td></tr><tr class=ltx_tr id=S4.T3.1.1.4.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.4.4.1>Pre-PPO-Large (V2.0)</th><td class="ltx_td ltx_align_center" id=S4.T3.1.1.4.4.2>31.3</td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.4.4.3>35.9</td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.4.4.4>30.8</td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.4.4.5>49.5</td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.4.4.6>32.3</td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.4.4.7>45.7</td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.4.4.8>36.1</td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.4.4.9>37.9</td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.4.4.10>35.1</td></tr><tr class=ltx_tr id=S4.T3.1.1.5.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T3.1.1.5.5.1>Improvement</th><td class="ltx_td ltx_align_center" id=S4.T3.1.1.5.5.2><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.5.5.2.1>+1.8</span></td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.5.5.3>-0.4</td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.5.5.4><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.5.5.4.1>+2.5</span></td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.5.5.5><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.5.5.5.1>+1.0</span></td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.5.5.6><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.5.5.6.1>+1.8</span></td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.5.5.7>+1.1</td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.5.5.8>-0.7</td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.5.5.9><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.5.5.9.1>+2.9</span></td><td class="ltx_td ltx_align_center" id=S4.T3.1.1.5.5.10><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.5.5.10.1>+1.1</span></td></tr><tr class=ltx_tr id=S4.T3.1.1.6.6><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T3.1.1.6.6.1>Data Scale-Large (V2.0)</th><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.6.6.2>31.2</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.6.6.3>36.4</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.6.6.4>31.9</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.6.6.5>50.7</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.6.6.6>32.3</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.6.6.7>45.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.6.6.8>36.6</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.6.6.9>37.1</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T3.1.1.6.6.10>35.4</td></tr><tr class=ltx_tr id=S4.T3.1.1.7.7><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S4.T3.1.1.7.7.1>Improvement on Pre-PPO</th><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T3.1.1.7.7.2>-0.2</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T3.1.1.7.7.3>+0.5</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T3.1.1.7.7.4><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.7.7.4.1>+1.1</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T3.1.1.7.7.5><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.7.7.5.1>+1.2</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T3.1.1.7.7.6>+0.0</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T3.1.1.7.7.7>-0.2</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T3.1.1.7.7.8>+0.5</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T3.1.1.7.7.9><span class="ltx_text ltx_font_bold" id=S4.T3.1.1.7.7.9.1>-0.8</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T3.1.1.7.7.10>+0.3</td></tr></tbody></table></table></figure><blockquote><p>üîº This ablation study investigates the individual contributions of two proposed strategies to the performance scaling of Reinforcement Learning from Human Feedback (RLHF) in large language models. The strategies are Pre-PPO, a prompt selection method, and an early training emphasis on mathematical and coding tasks. The table compares the performance across different evaluation metrics (Logical Reasoning, IF, STEM, Coding, NLP, Knowledge, CU, OOD, and Overall) of three different model configurations: the baseline model (no additional techniques), the model with Pre-PPO only, the model with early training emphasis only, and the model incorporating both Pre-PPO and early training emphasis. This allows for isolating and quantifying the effect of each strategy on various aspects of the model‚Äôs capabilities and their synergistic effect when combined.</p><details><summary>read the caption</summary>Table 3: Ablation Study: Performance Scaling of Pre-PPO and Early Training Emphasis in Large Language Models</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-1e484872e97d8652037d851da9f3bd99 class=gallery><img src=https://ai-paper-reviewer.com/2503.22230/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.22230/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/2025-03-31/2503.22230/&amp;title=Exploring%20Data%20Scaling%20Trends%20and%20Effects%20in%20Reinforcement%20Learning%20from%20Human%20Feedback" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/2025-03-31/2503.22230/&amp;text=Exploring%20Data%20Scaling%20Trends%20and%20Effects%20in%20Reinforcement%20Learning%20from%20Human%20Feedback" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/2025-03-31/2503.22230/&amp;subject=Exploring%20Data%20Scaling%20Trends%20and%20Effects%20in%20Reinforcement%20Learning%20from%20Human%20Feedback" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_2025-03-31/2503.22230/index.md",oid_likes="likes_2025-03-31/2503.22230/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/2025-03-31/2503.22236/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal Bridging</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-03-28T00:00:00+00:00>28 March 2025</time>
</span></span></a></span><span></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>