[{"figure_path": "https://arxiv.org/html/2503.17827/x1.png", "caption": "Figure 1: An example demonstrating the challenges of 4D object understanding involves multi-view spatial-temporal reasoning. Given the 4D object, the robot\u2019s right hand seems ambiguous in some views at first and eventually disappears over time.\nHence, answering the question needs to (1) address multi-view ambiguity and choose proper views and time that the right hand is visible, (2) localize the right hand, (3) and track its evolutions along the time dimension.", "description": "This figure illustrates the complexities of 4D object understanding by presenting a multi-view video sequence of a robot.  The robot's right hand is central to the task.  The challenge is that the hand's appearance varies significantly across different viewpoints and across time, creating ambiguity. To answer a simple question about what the hand transforms into requires the system to solve three subproblems: 1) resolve multi-view ambiguity by selecting the right viewpoint and time frame where the hand is clearly visible; 2) precisely locate the hand within those selected frames; and 3) track the hand's changes across the selected time-series to determine the final state.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.17827/x2.png", "caption": "Figure 2: Illustration of the 4D-Bench. 4D-Bench consists of two critical tasks (a) 4D object QA and (b) 4D object captioning. 4D object QA provides one question and four choices per QA to evaluate MLLMs. 4D object captioning provides five human captions per 4D object.", "description": "This figure illustrates the 4D-Bench benchmark, which evaluates the abilities of Multimodal Large Language Models (MLLMs) to understand 4D objects.  It highlights the two main tasks included: 4D object question answering (QA) and 4D object captioning.  The 4D object QA task presents a question with four answer choices, testing the MLLM's reasoning ability. The 4D object captioning task requires the MLLM to generate a caption for a 4D object (dynamic 3D object), and its performance is evaluated against five human-generated captions for the same object. The visual representation shows example 4D objects displayed as multi-view videos over time, demonstrating the complexity inherent in understanding dynamic 3D scenes.", "section": "3. A New Benchmark: 4D-Bench"}, {"figure_path": "https://arxiv.org/html/2503.17827/x3.png", "caption": "Figure 3: Pipeline for constructing the 4D-Bench dataset. The pipeline includes rendering multi-view videos for 4D objects from Objaverse-XL, motion filtering, visual quality filtering, and multi-stage annotations for QA pairs and captions. Captions are purely human-annotated, while QA pairs are generated through a hybrid approach using MLLMs and human validation.", "description": "This figure illustrates the process of creating the 4D-Bench dataset.  It starts with 3D objects sourced from Objaverse-XL, which are then rendered into multi-view videos. These videos undergo two filtering stages: motion filtering (removing static objects) and visual quality filtering (removing poorly rendered objects). The data then passes to annotation, creating both 4D object captions (purely human-annotated) and 4D object question-answer pairs (created using a combination of human annotation and MLLMs).", "section": "3.3 Data Collection and Annotation"}, {"figure_path": "https://arxiv.org/html/2503.17827/x6.png", "caption": "Figure 4: Subtask and category distributions in 4D object QA and captioning. Left: Distribution of five subtasks in the 4D object QA task, 751 question-answering pairs in total. Right: Distribution of 4D object categories in 4D object captioning task, 580 4D objects in total.", "description": "This figure presents a detailed breakdown of the data distribution within the 4D-Bench benchmark. The left panel displays the distribution of question-answer pairs across five subtasks in the 4D object question answering section of the benchmark.  It shows that a total of 751 question-answer pairs were created. The right panel shows the distribution of 4D objects across six categories in the 4D object captioning section of the benchmark, which includes 580 4D objects in total. This visualization helps to understand the composition and balance of different task types and object categories within the benchmark dataset.", "section": "3. A New Benchmark: 4D-Bench"}, {"figure_path": "https://arxiv.org/html/2503.17827/x7.png", "caption": "Figure 5: An example from Object Counting subtask. Answering this question requires integrating multi-view information and capturing cross-view correspondences to count the presents, necessitating multi-view reasoning abilities. If relying solely on a single view (e.g. the middle row), it would lead to wrong answers (e.g. four), since some boxes are occluded and invisible in this view.", "description": "This figure showcases a sample question from the Object Counting subtask within the 4D-Bench benchmark.  The task's complexity highlights the need for multi-view spatial reasoning.  A single viewpoint (like the middle row in the image) might incorrectly suggest four presents because some are partially or fully occluded, leading to an inaccurate count. Correctly answering requires integrating information from multiple perspectives to identify all presents.", "section": "4.3. Evaluation Results on 4D Object QA"}, {"figure_path": "https://arxiv.org/html/2503.17827/x8.png", "caption": "Figure 6: Effect of view number and temporal sampling on the 4D object QA performance. Tested on Gemini 1.5 Flash. Left: Accuracies across different numbers of views with fixed 6 frames. Right: Accuracies across different temporal frequencies with fixed 3 views.", "description": "This figure displays the impact of varying the number of viewpoints and the frequency of sampled frames on the accuracy of 4D object question answering.  The experiment uses the Gemini 1.5 Flash model. The left panel shows how accuracy changes as the number of views increases while keeping the number of frames per view constant at 6. The right panel shows the effect of altering the sampling frequency (frames per view) while maintaining a fixed number of 3 views.  The results illustrate the interplay between the richness of visual input (number of views) and the temporal detail (sampling frequency) in influencing the model's performance.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17827/x9.png", "caption": "Figure 7: Effect of view number and temporal sampling on the 4D object captioning performance. Tested on Qwen2-VL 7B. Left: GPT-Eval scores across different numbers of views with fixed 6 frames. Right: GPT-Eval scores across different temporal frequencies with fixed 3 views.", "description": "This figure demonstrates the impact of the number of views and temporal sampling frequency on the performance of the Qwen2-VL 7B model in 4D object captioning.  The left panel shows how the GPT-Eval score changes as the number of views increases, while keeping the temporal sampling frequency constant at 6 frames. The right panel shows the effect of altering the temporal sampling frequency while using a fixed number of 3 views. The results illustrate the model's sensitivity to both the spatial and temporal information presented.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17827/x10.png", "caption": "Figure 8: A counterfactual example from 4D object QA task. A synthetic spider with six legs, illustrating a counterfactual scenario for testing model understanding, as real spiders typically have eight legs.", "description": "This figure shows a synthetic spider with six legs, which is not a naturally occurring characteristic of spiders (they typically have eight legs).  The image is a still from a multi-view video used in the 4D Object Question Answering (QA) task of the 4D-Bench benchmark. This example tests if the models can identify and reason about counterfactual scenarios, where objects have properties or traits that contradict real-world knowledge.  By presenting an impossible feature, the researchers aim to evaluate whether the model truly understands the object or simply relies on its training data and pre-existing knowledge.", "section": "3. A New Benchmark: 4D-Bench"}, {"figure_path": "https://arxiv.org/html/2503.17827/x11.png", "caption": "Figure 9: A counterfactual example from 4D object QA task. A ball rolling into a downward-facing hole and then rolling back out, depicting a counterfactual scenario that violates physical laws, as a ball would normally stay trapped in the hole.", "description": "Figure 9 showcases a counterfactual scenario within the 4D object question answering task of the 4D-Bench benchmark.  A ball is shown rolling into a downward-facing hole and then inexplicably rolling back out. This defies the laws of physics, as a ball would typically remain trapped in the hole.  The figure highlights the benchmark's ability to test the robustness of Multimodal Large Language Models (MLLMs) by presenting situations that violate real-world expectations.", "section": "3. A New Benchmark: 4D-Bench"}, {"figure_path": "https://arxiv.org/html/2503.17827/x12.png", "caption": "Figure I: The frame-length distribution of multi-view videos used in the 4D object captioning task", "description": "This figure displays the distribution of video lengths in the 4D object captioning task dataset of the 4D-Bench benchmark.  It shows the number of videos across different lengths, measured in the number of frames.  The statistics, including mean, median, standard deviation, minimum, and maximum video length, are also provided to summarize the data.", "section": "3.4 Statistics of 4D-Bench"}, {"figure_path": "https://arxiv.org/html/2503.17827/x13.png", "caption": "Figure II: The length distribution of ground-truth captions used in the 4D object captioning task", "description": "This histogram displays the distribution of caption lengths (number of words) in the 4D object captioning dataset. It shows the frequency of captions with different word counts, providing insights into the overall length and variability of the human-generated captions used in the benchmark.", "section": "3.4. Statistics of 4D-Bench"}, {"figure_path": "https://arxiv.org/html/2503.17827/x14.png", "caption": "Figure III: The frame-length distribution of multi-view videos used in the 4D object question answering task", "description": "This figure shows the distribution of video lengths used in the 4D object question answering task of the 4D-Bench benchmark.  It presents a histogram and kernel density estimate (KDE) plot illustrating the number of videos with varying lengths (number of frames). The mean, median, standard deviation, minimum, and maximum video lengths are also provided, giving a comprehensive view of the data distribution.", "section": "3.4 Statistics of 4D-Bench"}, {"figure_path": "https://arxiv.org/html/2503.17827/x15.png", "caption": "Figure IV: The prompt provided to the evaluated MLLMs in the 4D object captioning task. In this prompt, we describe the video information, caption requirement, and output format. We also provide several caption examples to guide the style of captions generated by MLLMs.", "description": "Figure IV illustrates the prompt used for the 4D object captioning task in the 4D-Bench benchmark.  The prompt instructs the evaluated multi-modal large language models (MLLMs) to generate a caption for a provided 4D object (represented as a multi-view video).  The prompt details the video content, specifies the required elements of the caption (appearance and actions), dictates the desired output format, and includes example captions to guide the MLLMs' style and level of detail. This structure ensures consistency and facilitates a more objective evaluation of the MLLMs' captioning capabilities.", "section": "3.3 Data Collection and Annotation"}, {"figure_path": "https://arxiv.org/html/2503.17827/x16.png", "caption": "Figure V: The truncated length distribution of correct answers and decoys used in 4D object question answering dataset", "description": "Figure V illustrates the distribution of the lengths of correct answers and incorrect options (decoys) in the 4D object question answering dataset.  It shows histograms for each, revealing the frequency of different answer lengths. This helps to assess if there are biases in the dataset towards certain lengths, which could impact the evaluation of models.", "section": "3.3.2 4D Object Question Answering Annotation"}, {"figure_path": "https://arxiv.org/html/2503.17827/x17.png", "caption": "Figure VI: The prompt provided to the evaluated MLLMs in the 4D object QA task. In this prompt, we detailed the video information, questions and options, and the output format.", "description": "Figure VI shows the prompt used in the 4D object question answering task of the 4D-Bench benchmark.  The prompt instructs the evaluated Multimodal Large Language Models (MLLMs) to analyze a multi-view video (18 frames from multiple viewpoints) and select the correct answer from four given options. The prompt emphasizes the need for accurate responses and specifies that only the letter corresponding to the chosen answer (A, B, C, or D) should be returned.", "section": "3.3.2 4D Object Question Answering Annotation"}, {"figure_path": "https://arxiv.org/html/2503.17827/x18.png", "caption": "Figure VII: Prompt used in GPT-Appearance metric", "description": "This figure displays the prompt template used for the GPT-Appearance metric in evaluating the quality of video captions generated by multimodal large language models (MLLMs).  The prompt instructs the evaluator (GPT-4) to assess the predicted caption based on its accuracy in describing the visual appearance and shape of the objects in the video, comparing it to a human-annotated reference caption.  A scoring rubric from 0 to 5 is provided, with detailed descriptions for each score level to guide the evaluation. Several example pairs of human and predicted captions with their corresponding scores are also given to illustrate how the scoring should be applied.", "section": "4.1 Evaluation Metrics"}, {"figure_path": "https://arxiv.org/html/2503.17827/x19.png", "caption": "Figure VIII: Prompt used in GPT-Action metric", "description": "This figure shows the prompt template used for evaluating the quality of video captions generated by large language models (LLMs) using the GPT-Action metric.  The prompt guides the evaluator to focus specifically on the accuracy and completeness of the action descriptions in the captions, ignoring other aspects like appearance or context.  A scoring rubric (0-5) is provided, with detailed descriptions for each score level, illustrating expected qualities like precision, recall, and handling of synonyms/paraphrases. Several examples of human-generated captions paired with predicted captions and their corresponding scores are included to help the evaluator understand the scoring criteria.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17827/x20.png", "caption": "Figure IX: Qualitative results of different MLLMs on the 4D object captioning task of 4D-Bench", "description": "This figure showcases a qualitative comparison of how various multi-modal large language models (MLLMs) perform on the 4D object captioning task within the 4D-Bench benchmark.  It presents several example 4D objects (dynamic 3D objects) and displays the captions generated by different MLLMs alongside a human-generated caption for comparison.  This allows for a visual assessment of each model's ability to accurately and comprehensively describe both the visual appearance and the actions depicted in the 4D object sequences.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17827/x21.png", "caption": "Figure X: Qualitative results of different MLLMs on the 4D object captioning task of 4D-Bench", "description": "This figure presents a qualitative comparison of how different Multi-modal Large Language Models (MLLMs) perform the 4D object captioning task within the 4D-Bench benchmark.  It shows example 4D objects and the captions generated by several MLLMs, alongside a human-generated caption for each object, allowing for a visual assessment of each model's ability to understand both visual appearance and temporal aspects of dynamic 3D objects. The GPT-Appearance and GPT-Action scores, which evaluate the quality of appearance and action description, respectively, are included for each model's caption to provide a quantitative comparison.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17827/x22.png", "caption": "Figure XI: Qualitative results of different MLLMs on the 4D object captioning task of 4D-Bench", "description": "This figure presents a qualitative comparison of how different large multimodal language models (MLLMs) perform on the 4D object captioning task within the 4D-Bench benchmark.  It shows examples of 4D objects and captions generated by various MLLMs, including MiniGPT4-Video, VideoChat2-Mistral, Qwen2-VL 7B, and Gemini 1.5 Pro, alongside a human-generated caption for comparison.  The goal is to illustrate the strengths and weaknesses of each MLLM in terms of visual and motion understanding and the ability to generate accurate and descriptive captions for dynamic 3D objects.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17827/x23.png", "caption": "Figure XII: Qualitative results of different MLLMs on the 4D object captioning task of 4D-Bench", "description": "Figure XII presents a qualitative comparison of how different Multi-modal Large Language Models (MLLMs) perform on the 4D object captioning task within the 4D-Bench benchmark.  It shows example 4D objects (dynamic 3D objects over time) and the captions generated by several MLLMs, alongside a human-generated caption for comparison.  This allows for a visual assessment of each model's ability to understand and describe both the visual appearance and the temporal evolution of the objects in the videos.", "section": "4. Evaluation Results on 4D Object Captioning"}, {"figure_path": "https://arxiv.org/html/2503.17827/x24.png", "caption": "Figure XIII: Qualitative results of different MLLMs on the 4D object question answering task of 4D-Bench", "description": "This figure showcases the performance of various Multimodal Large Language Models (MLLMs) on a 4D object question answering task within the 4D-Bench benchmark.  It presents a specific question and four possible answers, revealing which answer each MLLM selected. This allows for a direct comparison of the models' abilities to understand and reason about spatiotemporal information in a 4D context, highlighting strengths and weaknesses in their 4D object understanding capabilities.", "section": "4.3 Evaluation Results on 4D Object QA"}, {"figure_path": "https://arxiv.org/html/2503.17827/x25.png", "caption": "Figure XIV: Qualitative results of different MLLMs on the 4D object question answering task of 4D-Bench", "description": "This figure showcases the performance of various large language models (LLMs) on a 4D object question answering task within the 4D-Bench benchmark.  The task involves a video of a pink flamingo performing an action, and the LLMs must correctly identify the sequence of actions from the given choices.  The results demonstrate the varying abilities of different LLMs to comprehend temporal dynamics and spatial reasoning in 4D object understanding.", "section": "4.3 Evaluation Results on 4D Object QA"}, {"figure_path": "https://arxiv.org/html/2503.17827/x26.png", "caption": "Figure XV: Qualitative results of different MLLMs on the 4D object question answering task of 4D-Bench", "description": "This figure showcases a qualitative comparison of how various large language models (LLMs) perform on a 4D object question answering task within the 4D-Bench benchmark.  It presents a sample question and the responses generated by different LLMs, highlighting their strengths and weaknesses in understanding temporal and spatial aspects of 4D objects (3D objects changing over time). The models' answers are contrasted with the correct answer to illustrate performance differences.", "section": "4.3 Evaluation Results on 4D Object QA"}]