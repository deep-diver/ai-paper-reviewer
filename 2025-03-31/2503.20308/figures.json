[{"figure_path": "https://arxiv.org/html/2503.20308/x3.png", "caption": "Figure 1: What defines perceptually accurate lip movement for a speech signal? In this work, we define three criteria to assess perceptual alignment between speech and lip movements of 3D talking heads: Temporal Synchronization, Lip Readability, and Expressiveness (a).\nThe motivational hypothesis is the existence of a desirable representation space that models and complies well with the three criteria between diverse speech characteristics and 3D facial movements, as illustrated in (b); where\nrepresentations with the same phonemes\nare clustered, are sensitive to temporal synchronization, and follow a certain pattern\nas the speech intensity increases. Consequently, we build a rich speech-mesh synchronized representation space that exhibits the desirable properties.", "description": "Figure 1 illustrates the key aspects of perceptually accurate 3D talking head generation.  Panel (a) introduces three essential criteria for evaluating the perceptual quality of lip synchronization: Temporal Synchronization (the timing alignment between audio and visual), Lip Readability (how easily the lip movements can be understood), and Expressiveness (the naturalness and intensity of lip movements). Panel (b) presents a conceptual diagram showing a desirable representation space where speech and lip movements are closely linked. This space groups together lip movements corresponding to the same phonemes, is sensitive to the timing of speech, and clearly shows how lip movements change with varying speech intensity. The authors aim to create a rich speech-mesh synchronized representation space reflecting these properties.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.20308/x4.png", "caption": "Figure 2: Pipeline of speech-mesh synchronized representation learning. We train our speech-mesh representation space in a two-stage manner. In the first stage, we learn a rich audio-visual\nrepresentation in 2D domain to capture the synchronization between lip movement and speech.\nIn the second stage, we train the 3D mesh encoder to align the 3D mesh space with the frozen speech space. As an application of our speech-mesh representation space, we propose a plug-in perceptual loss to 3D talking head models to enhance the quality of lip movements.", "description": "This figure illustrates the two-stage training process for learning a speech-mesh synchronized representation. The first stage involves training an audio-visual model on 2D data to capture the intricate correspondence between speech and lip movements. This learned representation is then used in the second stage as an anchor for training a 3D mesh encoder, aligning the 3D mesh space with the speech space.  The resulting speech-mesh representation is then used as a plug-in perceptual loss in 3D talking head models, enhancing the quality of lip movements.", "section": "4. Speech-Mesh Synchronized Representation"}, {"figure_path": "https://arxiv.org/html/2503.20308/x5.png", "caption": "Figure 3: Qualitative results of the effectiveness of our perceptual loss for lip readability. Our perceptual loss guides baselines\u00a0[11, 46, 21] to generate perceptually accurate lip movements.", "description": "This figure showcases the impact of the proposed perceptual loss on lip readability in 3D talking head generation.  Three different baseline models ([11, 46, 21]) are compared: their outputs before and after incorporating the perceptual loss are shown. The improved lip movements demonstrate a more natural and accurate lip synchronization, highlighting the effectiveness of the perceptual loss in enhancing the perceptual quality of the generated videos.", "section": "4. Speech-Mesh Synchronized Representation"}, {"figure_path": "https://arxiv.org/html/2503.20308/x6.png", "caption": "Figure 4: t-SNE plot of ablation study.\nWe plot the t-SNE graph for each perceptual critic model.\nWe represent the features with same phoneme as same color.\nSquared and circled points denote mesh and speech features from each representation, respectively.", "description": "This figure presents a t-SNE visualization of the feature embeddings generated by different models in the ablation study. Each point represents a feature vector extracted from either the speech or mesh data.  Points of the same color indicate features corresponding to the same phoneme. Square and circular shapes distinguish between speech and mesh features, respectively.  By visualizing the data in this way, we can observe the degree of clustering and separation between different phonemes in the feature space generated by each model, thus illustrating the effectiveness of each model's ability to capture the relationships between speech and lip movements.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2503.20308/x7.png", "caption": "Figure 5: Behaviors of our representation in temporal and expressiveness sensitivity.\nWe demonstrate the effectiveness of our representation in temporal synchronization and expressiveness using a cosine similarity graph and speech feature plots, respectively. We color the point as low, medium, and high intensity.", "description": "This figure displays two graphs illustrating the properties of the speech-mesh representation, focusing on temporal synchronization and expressiveness. The first graph shows the relationship between cosine similarity and temporal offset, revealing the model's sensitivity to temporal alignment. As the temporal offset increases, the cosine similarity decreases, indicating that the model prioritizes accurate synchronization. The second graph, a t-SNE plot, visualizes the speech features colored by intensity level.  The plot demonstrates the ability of the representation to capture varying intensities of speech, where similar phonemes cluster together, and points move along a direction vector as intensity increases. These observations demonstrate the representation's ability to capture critical aspects of perceptually accurate lip movements.", "section": "4. Speech-Mesh Synchronized Representation"}, {"figure_path": "https://arxiv.org/html/2503.20308/x8.png", "caption": "Figure 6: Qualitative results for the expressiveness. Given high and low intensity levels of speech, models trained on both MEAD-3D and VOCASET show more expressive lip movements compared to those trained on VOCASET alone, and even better with our perceptual loss.", "description": "This figure displays a qualitative comparison of lip expressiveness in 3D talking head generation models.  Three different models (FaceFormer, CodeTalker, SelfTalk) were trained using two datasets: VOCASET and MEAD-3D.  The images show the generated lip movements for high and low intensity speech. Models trained on both datasets (MEAD-3D and VOCASET) show more expressive lip movements, particularly when further enhanced by the incorporation of a perceptual loss function.  The perceptual loss leads to even better expressiveness in lip movements.", "section": "6. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.20308/x9.png", "caption": "Figure S1: Emergent properties of 2D speech representation.\nWe visualize a cosine similarity versus temporal offset graph and a t-SNE visualization of the 2D audio-visual speech representation.\nThe 2D speech representation already possesses desired properties we pursue, which motivates us to transfer the emergent properties to the speech-mesh representation space.", "description": "This figure demonstrates the emergent properties of a 2D audio-visual speech representation learned before training the main speech-mesh representation.  The left panel shows a graph of cosine similarity plotted against temporal offset, illustrating the model's sensitivity to temporal synchronization. The right panel displays a t-SNE visualization of the learned representation, showing how the model groups speech features with similar phonetic properties together. This observation supports the hypothesis that transferring the learned representation's characteristics to the 3D speech-mesh representation would improve the accuracy of lip synchronization.", "section": "B. Emergent Properties of 2D Speech Representation"}, {"figure_path": "https://arxiv.org/html/2503.20308/x10.png", "caption": "Figure S2: Speech and lip intensity distributions across datasets.\nWe present speech and lip intensity distributions and corresponding standard deviation values across datasets.", "description": "This figure displays the distributions of speech and lip intensity across four different datasets: VOCASET, BIWI, LRS3-3D, and MEAD-3D.  It uses histograms to show the frequency of various intensity levels for both speech and lip movements within each dataset.  The standard deviation (\u03c3) for each distribution is also provided, indicating the variability or spread of intensity values within each dataset.  The visual comparison helps illustrate the differences in the range and distribution of speech and lip intensities across these datasets.", "section": "C.3 Dataset statistics"}, {"figure_path": "https://arxiv.org/html/2503.20308/extracted/6313747/source/Figure_DDTW2.png", "caption": "Figure S3: Central vertices of the lower and upper lips.\nWe select two specific vertices that correspond to the center of the upper and lower lips to extract the lip vertex displacement sequences.", "description": "This figure shows how the lip vertex displacement sequences are extracted.  Specifically, it highlights the selection of two central vertices, one from the upper lip and one from the lower lip. These vertices are chosen because they represent the central movement of the lips during speech and provide a reliable measurement of lip movements when calculating displacement sequences for evaluation metrics.", "section": "Supplementary Material"}, {"figure_path": "https://arxiv.org/html/2503.20308/x11.png", "caption": "Figure S4: An example of DDTW matching results between ground truth and predicted lip distance sequences.\nWe present an example of the DDTW local extrema correspondences\nof the ground truth and predicted lip vertex displacement sequences.\nWe represent matched local extrema using green lines.", "description": "This figure displays an example of Dynamic Time Warping (DDTW) applied to lip movement data.  The DDTW algorithm finds the optimal alignment between two time series, in this case, the ground truth lip distances (from a reference video) and the predicted lip distances (from a generated video). The plot shows the ground truth lip distance and the predicted lip distance over time.  The algorithm identifies local extrema (peaks and valleys) in both curves. Green lines connect matched extrema (peaks to peaks, valleys to valleys) demonstrating the correspondences between the two time series, and thus, how well the generated video's lip movements match the reference video.  The differences in timing between matched extrema represent the temporal misalignment between ground truth and prediction. A smaller difference indicates better lip synchronization.", "section": "E. Evaluation Metrics"}, {"figure_path": "https://arxiv.org/html/2503.20308/x12.png", "caption": "Figure S5: Physical accuracy of Mean Temporal Misalignment.\nWe introduce temporal mismatch to the ground truth mesh sequences of VOCASET\u00a0[8] by shifting the speech to lead the mesh sequences by 0 to 10 frames (where 0 represents no mismatch).\nFor each temporal offset, we calculate the average MTM and plot a graph showing the relationship between the temporal offset and the corresponding MTM values.", "description": "This figure demonstrates the accuracy of the Mean Temporal Misalignment (MTM) metric in capturing temporal discrepancies between speech and lip movements.  The experiment systematically introduced varying degrees of asynchrony (0 to 10 frames, equivalent to 0 to 333 milliseconds in VOCASET) by shifting the timing of speech relative to the ground truth lip movements.  For each level of asynchrony, the average MTM was calculated.  The resulting graph plots the calculated MTM values against the corresponding temporal offset (in frames), clearly showing a direct and linear relationship: as the temporal offset increases, so does the MTM, demonstrating the metric's effectiveness in quantifying the degree of asynchrony.", "section": "Supplementary Material, E. Evaluation Metrics, E.1 Definition and implementation details"}, {"figure_path": "https://arxiv.org/html/2503.20308/x13.png", "caption": "Figure S6: Perceptual loss stability. We visualize the perceptual loss between GT speech-mesh pairs on VOCASET samples. Our representation demonstrates strong generalization capability and provides a stable training signal compared to 3D SyncNet and our representation without 2D prior.", "description": "This figure displays the stability of the perceptual loss function during the training process of three different 3D talking head generation models: 3D SyncNet, a model without a 2D prior, and the proposed model with a 2D prior.  The perceptual loss is visualized for both training and test sets using box plots.  The y-axis represents the perceptual loss, and the x-axis shows the three models. The box plots illustrate the mean, median, and interquartile range of the perceptual loss, revealing how consistent and stable the loss is across different datasets. The model with the 2D prior exhibits substantially less variability in the perceptual loss, indicating better generalization and stability compared to 3D SyncNet and the model without the 2D prior.", "section": "G.4 Stability comparison on loss and cosine similarity"}, {"figure_path": "https://arxiv.org/html/2503.20308/x14.png", "caption": "Figure S7: Cosine similarity stability. We visualize the cosine similarity between GT speech-mesh pairs on VOCASET samples. Our representation demonstrates strong generalization capability compared to 3D SyncNet and our representation without 2D prior.", "description": "Figure S7 presents a comparison of the stability of cosine similarity scores across three different speech-mesh representations: the proposed model, the proposed model without the 2D prior, and the 3D SyncNet model.  The cosine similarity, a measure of the similarity between the ground truth (GT) speech-mesh pairs and the model's generated representations, was computed for both training and test samples from the VOCASET dataset. The results, presented as box plots, show that the proposed model exhibits significantly lower variability in cosine similarity scores across the training and testing sets than the other two models. This indicates that the proposed representation generalizes better to unseen data, highlighting its superior robustness and reliability in capturing the intricate relationships between speech and 3D facial mesh.", "section": "G.4 Stability comparison on loss and cosine similarity"}]