[{"content": "| Feature Engineering | Data Engineering | Verifier Engineering |\n|---|---|---|\n| **Representative Models** | Machine Learning Models | Deep Learning Models | Foundation Models |\n| e.g. | SVM, XGBoost | CNN, LSTM | LLMs, VLMs |\n| **Supervision** | Manual Features | Human Annotations | Verifier Feedback |\n| **Scope** | Task-Specific | Multiple Related Tasks | General Intelligence |\n| **Generalization** | Limited | Relatively high | High |\n| **Scalability** | Limited | Moderate | High |", "caption": "Table 1: Comparison of feature engineering, data engineering, and verifier engineering", "description": "This table compares three different paradigms in machine learning: feature engineering, data engineering, and verifier engineering.  It highlights key differences across several dimensions, including the types of machine learning models used, the nature of supervision signals (manual features, human annotations, verifier feedback), the scope of tasks addressed (task-specific versus multiple related tasks or general intelligence), and the relative levels of generalization and scalability achieved by each approach.  It illustrates the evolution of machine learning from handcrafted features to data-driven approaches and finally to a new paradigm using verifiers for providing feedback.", "section": "1. Introduction"}, {"content": "| Verifier Type | Verification Form | Verify Granularity | Verifier Source | Extra Training |\n|---|---|---|---|---|\n| Golden Annotation | Binary/Text | Thought Step/Full Trajectory | Program Based | No |\n| Rule-based | Binary/Text | Thought Step/Full Trajectory | Program Based | No |\n| Code Interpreter | Binary/Score/Text | Token/Thought Step/Full Trajectory | Program Based | No |\n| ORM | Binary/Score/Rank/Text | Full Trajectory | Model Based | Yes |\n| Language Model | Binary/Score/Rank/Text | Thought Step/Full Trajectory | Model Based | Yes |\n| Tool | Binary/Score/Rank/Text | Token/Thought Step/Full Trajectory | Program Based | No |\n| Search Engine | Text | Thought Step/Full Trajectory | Program Based | No |\n| PRM | Score | Token/Thought Step | Model Based | Yes |\n| Knowledge Graph | Text | Thought Step/Full Trajectory | Program Based | No |", "caption": "Table 2: A comprehensive taxonomy of verifiers across four dimensions:  verification form, verify granularity, verifier source, and the need for extra training.", "description": "This table categorizes verifiers based on four key characteristics: the format of their output (binary, score, ranking, or text), the level of detail they examine (token, thought, or trajectory), whether they are program-based or model-based, and whether they require additional training.  This provides a structured overview of the diverse types of verifiers used in verifier engineering, highlighting the trade-offs between different approaches.", "section": "4. Verify"}, {"content": "| Method | Search | Verify | Feedback | Task |\n|---|---|---|---|---|\n| STar (Zelikman et al., 2022a), RFT (Yuan et al., 2023c) | Linear | Golden Annotation | Imitation Learning | Math |\n| CAG (Pan et al., 2024) | Linear | Golden Annotation | Imitation Learning | RAG |\n| Self-Instruct (Wang et al., 2023e) | Linear | Rule-based | Imitation Learning | General |\n| Code Alpaca (Chaudhary, 2023), WizardCoder (Luo et al., 2024d) | Linear | Rule-based | Imitation Learning | Code |\n| ILF-Code (Chen et al., 2024a) | Linear | Rule-based & Code interpreter | Imitation Learning | Code |\n| RAFT (Dong et al., 2023), RRHF (Yuan et al., 2023a) | Linear | ORM | Imitation Learning | General |\n| SSO (Xiang et al., 2024) | Linear | Rule-based | Preference Learning | Alignment |\n| CodeUltraFeedback (Weyssow et al., 2024) | Linear | Language Model | Preference Learning | Code |\n| Self-Rewarding (Yuan et al., 2024) | Linear | Language Model | Preference Learning | Alignment |\n| StructRAG (Li et al., 2024b) | Linear | Language Model | Preference Learning | RAG |\n| LLAMA-BERRY (Zhang et al., 2024a) | Tree | ORM | Preference Learning | Reasoning |\n| Math-Shepherd (Wang et al., 2024b) | Linear | Golden Annotation & Rule-based | Reinforcement Learning | Math |\n| RLTF (Liu et al., 2023b), PPOCoder (Shojaee et al., 2023b) | Linear | Code Interpreter | Reinforcement Learning | Code |\n| RLAIF (Lee et al., 2023) | Linear | Language Model | Reinforcement Learning | General |\n| SIRLC (Pang et al., 2023) | Linear | Language Model | Reinforcement Learning | Reasoning |\n| RLFH (Wen et al., 2024d) | Linear | Language Model | Reinforcement Learning | Knowledge |\n| RLHF (Ouyang et al., 2022a) | Linear | ORM | Reinforcement Learning | Alignment |\n| Quark (Lu et al., 2022) | Linear | Tool | Reinforcement Learning | Alignment |\n| ReST-MCTS (Zhang et al., 2024b) | Tree | Language Model | Reinforcement Learning | Math |\n| CRITIC (Gou et al., 2024) | Linear | Code Interpreter & Tool & Search Engine | Verifier-Aware | Math, Code & Knowledge & General |\n| Self-Debug (Chen et al., 2023c) | Linear | Code Interpreter | Verifier-Aware | Code |\n| Self-Refine (Madaan et al., 2023) | Linear | Language Model | Verifier-Aware | Alignment |\n| ReAct (Yao et al., 2022) | Linear | Search Engine | Verifier-Aware | Knowledge |\n| Constrative decoding (Li et al., 2023a) | Linear | Language Model | Verifier-Guided | General |\n| Chain-of-verfication (Dhuliawala et al., 2023) | Linear | Language Model | Verifier-Guided | Knowledge |\n| Inverse Value Learning (Lu et al., 2024) | Linear | Language Model | Verifier-Guided | General |\n| PRM (Lightman et al., 2023b) | Linear | PRM | Verifier-Guided | Math |\n| KGR (Guan et al., 2023) | Linear | Knowledge Graph | Verifier-Guided | Knowledge |\n| UoT (Hu et al., 2024) | Tree | Language Model | Verifier-Guided | General |\n| ToT (Yao et al., 2024) | Tree | Language Model | Verifier-Guided | Reasoning |", "caption": "Table 3: This paper provides a comprehensive exploration of the verifier engineering landscape, breaking it down into three core stages: search, verify, and feedback.", "description": "This table provides a comprehensive overview of various methods used in verifier engineering, categorized into three core stages: search, verification, and feedback.  Each row represents a different approach or technique, detailing the search strategy employed (linear or tree-based), the type of verifier used (e.g., golden annotation, reward model), the feedback mechanism (e.g., imitation, reinforcement, preference learning), and the specific task the method is applied to (e.g., math, code, reasoning). The table aims to illustrate the diversity of techniques within each stage of verifier engineering and their applications to different tasks.", "section": "5. Feedback"}]