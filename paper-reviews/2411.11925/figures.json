[{"figure_path": "https://arxiv.org/html/2411.11925/x2.png", "caption": "Figure 1: \nContinuous speculative decoding accelerates the inference speed while maintaining the original generation quality.", "description": "This figure displays a comparison of image generation speeds using different methods. Three sets of images are shown, each with a default autoregressive model and the proposed continuous speculative decoding method. The latter shows a significant speed-up (2.15x, 2.32x, and 2.26x faster) while preserving the original image quality. This demonstrates the effectiveness of the proposed approach for accelerating inference without sacrificing the quality of autoregressive image generation.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.11925/x3.png", "caption": "Figure 2: \nComparison between discrete- and continuous-valued speculative decoding. Discrete models can conveniently compute output probabilities and be sampled from modified distributions. In contrast, continuous models require determining how to compute probabilities, and sampling from modified distributions via draft and target output distributions is often more challenging.", "description": "This figure compares discrete and continuous speculative decoding methods.  Discrete methods easily calculate output probabilities and resample from adjusted distributions. However, continuous methods face the challenge of calculating probabilities in a continuous space and then sampling from modified distributions, which requires more complex calculations involving both draft and target model outputs.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.11925/x4.png", "caption": "Figure 3: \nThe overview of our proposed continuous speculative decoding. Continuous speculative decoding leverages the diffusion model component of continuous AR models. Tokens 1\u223c3similar-to131\\sim 31 \u223c 3 are prefix tokens, and token x\ud835\udc65xitalic_x is to be verified. Upon obtaining and comparing the probability density values from the draft and target model, if q\u2062(x)<p\u2062(x)\ud835\udc5e\ud835\udc65\ud835\udc5d\ud835\udc65q(x)<p(x)italic_q ( italic_x ) < italic_p ( italic_x ), x\ud835\udc65xitalic_x is accepted. Otherwise, x\ud835\udc65xitalic_x is rejected with probability 1\u2212p\u2062(x)q\u2062(x)1\ud835\udc5d\ud835\udc65\ud835\udc5e\ud835\udc651-\\frac{p(x)}{q(x)}1 - divide start_ARG italic_p ( italic_x ) end_ARG start_ARG italic_q ( italic_x ) end_ARG, followed by sampling from the modified distribution via acceptance-rejection sampling to obtain x\u2032superscript\ud835\udc65\u2032x^{\\prime}italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT.", "description": "This figure illustrates the continuous speculative decoding process. It uses a draft model to generate a sequence of tokens (1, 2, 3 being prefix tokens, and x being the token to verify). The target model then compares the probability densities of the draft and target models for token x. If the draft model's density is less than the target model's, the token is accepted; otherwise, it's rejected with a probability determined by the ratio of the densities. If rejected, a new token is sampled from a modified distribution using acceptance-rejection sampling, and the process continues until a token is accepted.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.11925/x5.png", "caption": "Figure 4: \nIllustration of denoising trajectory alignment. The denoising process maps the noise distribution to data distribution through gradual denoising. These denoising steps generate a trajectory. Aligned trajectories lead to similar output distribution, while unaligned one produces a more distinct distribution.", "description": "The figure illustrates the concept of denoising trajectory alignment in the context of continuous autoregressive image generation.  The denoising process starts with a noise distribution and gradually refines it through a series of steps to generate the final data distribution. Each step in this process is represented as a point in a trajectory. The figure contrasts two scenarios: one where the trajectories generated by the draft and target models are aligned, resulting in similar output distributions; and another where the trajectories are not aligned, leading to different output distributions. This alignment is crucial for ensuring consistency between the draft and target models in speculative decoding, which improves the acceptance rate and efficiency of the method.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.11925/x6.png", "caption": "Figure 5: \nIllustration of the modified distribution (unnormalized), where the dashed lines represent the output distributions of the draft and target models, and the red area denotes the modified distribution. The integral of this area is hard to compute, and there is no analytical expression available, which complicates sampling.", "description": "This figure illustrates the challenge of sampling from the modified distribution in continuous speculative decoding. The dashed lines represent the probability density functions (PDFs) of the draft and target models. The red area represents the modified distribution, obtained by taking the positive difference between the target and draft model PDFs.  The caption highlights that calculating the integral of this area is computationally complex due to the lack of an analytical solution, creating difficulty in directly sampling from this modified distribution.", "section": "3.3. How to resample a new token from the modified distribution after x is rejected?"}, {"figure_path": "https://arxiv.org/html/2411.11925/x7.png", "caption": "Figure 6: \nQualitative Results. We show the images generated under continuous speculative decoding with MAR.", "description": "This figure displays a set of images generated using the continuous speculative decoding method in conjunction with the MAR model.  It visually demonstrates the quality of images produced by this accelerated inference technique. The images cover a variety of subjects and styles to showcase the model's capabilities.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2411.11925/x8.png", "caption": "Figure 7: \nQualitative Comparison Results. We show the generated images using the algorithm at various draft length \u03b3\ud835\udefe\\gammaitalic_\u03b3.", "description": "This figure displays a qualitative comparison of images generated using the continuous speculative decoding method described in the paper. It shows the impact of varying the draft length (represented by the Greek letter gamma, \u03b3) on the quality of the generated images.  By comparing images generated with different \u03b3 values, the figure visually demonstrates how the length of the draft sequence affects the final output.  Different rows show different classes of images, and multiple columns within each row illustrate variations in the generated images with increasing draft length. This allows the reader to assess the trade-off between speed and quality achieved by using speculative decoding with varying lengths of draft sequences.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.11925/x9.png", "caption": "Figure 8: \nAcceptance ratio under different number of drafts. Larger number of drafts leads to the decay of acceptance ratio.", "description": "This figure shows the relationship between the number of draft tokens used in speculative decoding and the acceptance rate.  As the number of drafts increases, the acceptance rate decreases. This is because using more drafts increases the chance of generating tokens that differ significantly from the target model's predictions, which leads to more rejections during the verification step.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.11925/x10.png", "caption": "Figure 9: \nComparison on pure draft (left) and verified (right) generation results. Regions of rejected tokens are roughly marked out.", "description": "This figure compares image generation results using only a draft model (left) versus results after verification by a target model (right). The regions where the draft model's tokens were rejected and replaced by the target model are highlighted.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.11925/x11.png", "caption": "Figure 10: \nThe examples without (upper) and with (lower) denoising trajectory alignment. After alignment, the generated images exhibit a reduction in deformations and artifacts, thereby achieving higher quality.", "description": "This figure demonstrates the impact of denoising trajectory alignment on image generation quality. The top row shows images generated without alignment, exhibiting noticeable artifacts and deformations.  The bottom row presents images generated with the proposed denoising trajectory alignment technique. A comparison reveals that the aligned images display a significant reduction in artifacts and improved overall visual quality, highlighting the effectiveness of the alignment method in enhancing the consistency of the output distributions from the draft and target models.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.11925/x12.png", "caption": "Figure 11: \nPer-step acceptance \u03b1\ud835\udefc\\alphaitalic_\u03b1 under different pre-filling ratios. Acceptance rate per step is averaged on 1000 samples.", "description": "This figure visualizes the acceptance rate (\u03b1) at each step of the autoregressive generation process using continuous speculative decoding.  The x-axis represents the step number within the generation sequence. The y-axis represents the acceptance rate, which is the probability that a token generated by the draft model will be accepted by the target model.  Multiple lines are shown, each corresponding to a different pre-filling ratio (0%, 5%, 15%). Pre-filling refers to the process of using tokens from the target model to prime the initial stages of the draft model's generation. The acceptance rate is averaged over 1000 samples for each data point to ensure statistical significance.  The graph shows how the acceptance rate changes over the course of generation for various levels of pre-filling, demonstrating the impact of pre-filling on the overall efficiency of the continuous speculative decoding method.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.11925/x13.png", "caption": "Figure 12: \nComparing image generation quality under different token pre-filling portions.", "description": "This figure displays a comparison of image generation quality using different percentages of pre-filled tokens.  Pre-filling involves using tokens from the target model at the start of generation before the draft model begins. The images show how different levels of pre-filling (0%, 5%, and 15%) impact the final generated image's quality, demonstrating the effect of this technique on the overall visual fidelity and detail of the output.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.11925/x14.png", "caption": "Figure 13: \nCFG scale has has a significant impact on the acceptance rate under different number of drafts.", "description": "This figure shows how classifier-free guidance (CFG) scale affects the acceptance rate in continuous speculative decoding.  The experiment varies both the CFG scale and the number of draft tokens used in the process.  The results illustrate a general trend: as the CFG scale increases, the acceptance rate decreases, regardless of the number of drafts. This suggests that stronger class guidance might introduce more inconsistencies between the draft and target models, leading to lower acceptance rates.", "section": "D. Additional Experiments"}, {"figure_path": "https://arxiv.org/html/2411.11925/x15.png", "caption": "Figure 14: \nTemperature influence on the acceptance rate. Left: without CFG. Right: with CFG.", "description": "This figure shows how temperature, a hyperparameter in the diffusion process of the MAR model, affects the acceptance rate during the token generation process. The left side shows the results without classifier-free guidance (CFG), and the right side shows the results with CFG.  The number of drafts used was 8.  The graphs illustrate the relationship between temperature and acceptance rate, revealing how different temperatures influence the probability distribution of the final output and consequently, the likelihood of a token being accepted during speculative decoding.", "section": "D.2. Temperature"}, {"figure_path": "https://arxiv.org/html/2411.11925/extracted/6006952/figures/more_accept.png", "caption": "Figure 15: \nEmpirical rejection times in acceptance-rejection sampling algorithm of the rejection phase.", "description": "This figure shows the number of times the rejection step is repeated during the acceptance-rejection sampling in the rejection phase of the algorithm. The x-axis represents the number of draft tokens used, while the y-axis represents the number of times the rejection step is executed.  The plot shows the relationship between the number of draft tokens and the number of rejections in the algorithm, illustrating how the efficiency of the sampling process changes as the number of draft tokens increases.", "section": "D.3. Acceptance-Rejection Sampling"}, {"figure_path": "https://arxiv.org/html/2411.11925/x16.png", "caption": "Figure 16: \nVisualizations of accepted token heatmap. Dark green: accepted. Light green: rejected.", "description": "This figure visualizes the acceptance and rejection of tokens during the continuous speculative decoding process.  The heatmap shows which tokens were accepted (dark green) and rejected (light green) by the target model during inference. This provides a visual representation of the model's decision-making process in the speculative decoding framework, highlighting which parts of the image were more easily or difficultly generated.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.11925/extracted/6006952/figures/fox.png", "caption": "Figure 17: \nVisual quality with increasing draft length \u03b3\ud835\udefe\\gammaitalic_\u03b3 compared with vanilla target model only generation. Best viewed zoom-in.", "description": "This figure compares image generation quality using the proposed continuous speculative decoding method with varying draft lengths (\u03b3) against the baseline method of using only the target model.  Each row shows a category of images generated with different \u03b3 values.  The leftmost column represents the target model (vanilla) output, providing a reference for comparison. Subsequent columns illustrate the results using the continuous speculative decoding method with increasing values of \u03b3, demonstrating the progression in image quality as more draft tokens are considered.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2411.11925/extracted/6006952/figures/balloon.png", "caption": "Figure 18: \nVisualization examples under \u03b3=4\ud835\udefe4\\gamma=4italic_\u03b3 = 4. Class label: arctic fox (297).", "description": "This figure visualizes sample images generated by the Continuous Speculative Decoding method with a draft length (\u03b3) of 4.  The generated images are all classified as arctic foxes (class label 297 from the ImageNet dataset). The figure showcases the visual quality of images produced using this method with a short draft sequence, offering insight into the model's performance at different draft lengths and its ability to generate coherent and relevant images.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2411.11925/extracted/6006952/figures/ice_cream.png", "caption": "Figure 19: \nVisualization examples under \u03b3=8\ud835\udefe8\\gamma=8italic_\u03b3 = 8. Class label: balloon (417).", "description": "This figure visualizes the results of image generation using the proposed continuous speculative decoding method.  Specifically, it shows a grid of images generated with a draft length (\u03b3) of 8, all belonging to the 'balloon' class (class ID 417 from the ImageNet dataset). Each image represents a sample generated by the model, demonstrating the variety and quality of images produced under these settings. The purpose is to show the visual results of the continuous speculative decoding approach for image generation and to demonstrate that the generated images maintain quality despite the speedup gained from speculative decoding.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2411.11925/extracted/6006952/figures/volcano.png", "caption": "Figure 20: \nVisualization examples under \u03b3=16\ud835\udefe16\\gamma=16italic_\u03b3 = 16. Class label: ice cream (928).", "description": "This figure displays a set of images generated by the model, showcasing the variety and quality of ice cream images produced.  Each image is a different rendition of ice cream, demonstrating the model's capacity to generate diverse examples within a given class label. The images illustrate various types, presentations, and toppings of ice cream, highlighting the detailed and realistic generation capabilities of the model. The images were generated using a specific parameter setting (draft length (\u03b3)=16), indicating that this parameter might affect the quality or diversity of the generated images.", "section": "More Qualitative Results"}]