{"references": [{"fullname_first_author": "Tianle Cai", "paper_title": "Medusa: Simple Ilm inference acceleration framework with multiple decoding heads", "publication_date": "2024-01-10", "reason": "This paper proposes a novel inference acceleration framework for LLMs that significantly improves efficiency by employing multiple decoding heads, which is directly relevant to the paper's focus on speculative decoding for faster inference."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-12-01", "reason": "This paper introduces denoising diffusion probabilistic models (DDPMs), a foundation for many continuous-valued autoregressive image generation models, which are central to the proposed continuous speculative decoding method."}, {"fullname_first_author": "Charlie Chen", "paper_title": "Accelerating large language model decoding with speculative sampling", "publication_date": "2023-02-01", "reason": "This paper introduces speculative decoding for LLMs, which is directly extended to continuous visual autoregressive models in the current paper, forming its core idea."}, {"fullname_first_author": "Tianhong Li", "paper_title": "Autoregressive image generation without vector quantization", "publication_date": "2024-06-11", "reason": "This paper introduces the MAR model, a key autoregressive image generation model using continuous-valued tokens, which is used for experimentation and forms the basis of the proposed method."}, {"fullname_first_author": "Lijie Fan", "paper_title": "Fluid: Scaling autoregressive text-to-image generation with continuous tokens", "publication_date": "2024-10-13", "reason": "This paper is highly relevant as it also focuses on continuous-valued autoregressive models for image generation, providing a benchmark for comparison and demonstrating the state-of-the-art in this area."}]}