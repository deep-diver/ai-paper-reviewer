[{"content": "| $M_p$ | $M_q$ | $\\gamma$ | $\\alpha$ | Speedup ratio |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|\n|  |  |  |  | bs=1 | bs=8 | bs=128 | bs=256 |\n| MAR-L | MAR-B | 32 | 0.26 | **1.18 \u00d7** | **1.21 \u00d7** | **1.44 \u00d7** | **1.49 \u00d7** |\n| MAR-L | MAR-B | 16 | 0.31 | 1.10 \u00d7 | 1.17 \u00d7 | 1.39 \u00d7 | 1.42 \u00d7 |\n| MAR-L | MAR-B | 8 | 0.36 | 1.05 \u00d7 | 1.12 \u00d7 | 1.29 \u00d7 | 1.32 \u00d7 |\n| MAR-L | MAR-B | 4 | 0.39 | 1.01 \u00d7 | 1.00 \u00d7 | 1.13 \u00d7 | 1.15 \u00d7 |\n| MAR-H | MAR-B | 32 | 0.19 | **1.44 \u00d7** | **1.61 \u00d7** | **2.17 \u00d7** | **2.33 \u00d7** |\n| MAR-H | MAR-L | 32 | 0.18 | 1.26 \u00d7 | 1.34 \u00d7 | 1.47 \u00d7 | 1.53 \u00d7 |\n| MAR-H | MAR-B | 16 | 0.26 | 1.37 \u00d7 | 1.51 \u00d7 | 2.07 \u00d7 | 2.20 \u00d7 |\n| MAR-H | MAR-L | 16 | 0.24 | 1.24 \u00d7 | 1.29 \u00d7 | 1.41 \u00d7 | 1.46 \u00d7 |\n| MAR-H | MAR-B | 8 | 0.27 | 1.26 \u00d7 | 1.44 \u00d7 | 1.88 \u00d7 | 1.96 \u00d7 |\n| MAR-H | MAR-L | 8 | 0.28 | 1.11 \u00d7 | 1.21 \u00d7 | 1.32 \u00d7 | 1.33 \u00d7 |\n| MAR-H | MAR-B | 4 | 0.30 | 1.11 \u00d7 | 1.20 \u00d7 | 1.56 \u00d7 | 1.62 \u00d7 |\n| MAR-H | MAR-L | 4 | 0.30 | 1.00 \u00d7 | 1.03 \u00d7 | 1.15 \u00d7 | 1.18 \u00d7 |", "caption": "Table 1: Results of speedup ratio on MAR\u00a0[21] under different model size, draft number and batch size. The bs refers to batch size. The acceptance rate \u03b1\ud835\udefc\\alphaitalic_\u03b1 of each setting is also represented.", "description": "This table presents the speedup achieved by the proposed continuous speculative decoding method compared to the original MAR model [21] under various experimental settings. It shows the speedup ratio for different combinations of model sizes (Mq and Mp), draft numbers (\u03b3), and batch sizes (bs). The acceptance rate (\u03b1) is also given for each setting, indicating the proportion of draft tokens accepted by the target model.  The results demonstrate how the speedup varies across different model sizes and the balance between computation cost and accuracy.", "section": "4.2 Main Results"}, {"content": "| $M_p$ | $M_q$ | w/o CFG |  | w/ CFG |  | \n|---|---|---|---|---|---| \n|  |  | FID \u2193 | IS \u2191 | FID \u2193 | IS \u2191 | \n| MAR-L | MAR-L | 2.60 | 221.4 | 1.78 | 296.0 | \n| MAR-L | MAR-B | 2.59 \u00b1 0.04 | 218.4 \u00b1 3.4 | 1.81 \u00b1 0.05 | 303.7 \u00b1 4.3 | \n| MAR-H | MAR-L | 2.35 | 227.8 | 1.55 | 303.7 | \n| MAR-H | MAR-B | 2.36 \u00b1 0.05 | 228.5 \u00b1 2.2 | 1.60 \u00b1 0.05 | 301.6 \u00b1 2.6 | \n| MAR-H | MAR-L | 2.34 \u00b1 0.04 | 228.9 \u00b1 2.8 | 1.57 \u00b1 0.04 | 301.4 \u00b1 2.5 |", "caption": "Table 2: Evaluation of FID and IS comparison on ImageNet 256\u00d7256256256256\\times 256256 \u00d7 256 unconditional and conditional generation. Continuous speculative decoding achieves acceleration while maintaining performance within a reasonable interval.", "description": "This table presents a comparison of Fr\u00e9chet Inception Distance (FID) and Inception Score (IS) metrics for image generation on the ImageNet 256x256 dataset.  It compares the performance of the original autoregressive model (MAR) with the proposed continuous speculative decoding method. The comparison is done for both unconditional and conditional image generation, demonstrating the impact of speculative decoding on both generation quality and speed. The results show that continuous speculative decoding achieves significant speedup without significantly sacrificing generation quality, remaining within a reasonable performance range of the baseline MAR model.", "section": "4. Main Results"}, {"content": "| $M_p$ | $M_q$ | $\\gamma$ | $\\alpha$ (w/o align) | $\\alpha$ (w/ align) |\n|---|---|---|---|---|\n| MAR-L | MAR-B | 32 | 0.10 | **0.34** |\n| MAR-L | MAR-B | 16 | 0.12 | **0.37** |\n| MAR-L | MAR-B | 8 | 0.12 | **0.39** |\n| MAR-L | MAR-B | 4 | 0.13 | **0.37** |\n| MAR-H | MAR-B | 32 | 0.07 | **0.30** |\n| MAR-H | MAR-L | 32 | 0.06 | **0.33** |\n| MAR-H | MAR-B | 16 | 0.07 | **0.33** |\n| MAR-H | MAR-L | 16 | 0.08 | **0.35** |\n| MAR-H | MAR-B | 8 | 0.13 | **0.31** |\n| MAR-H | MAR-L | 8 | 0.12 | **0.34** |\n| MAR-H | MAR-B | 4 | 0.14 | **0.32** |\n| MAR-H | MAR-L | 4 | 0.12 | **0.34** |", "caption": "Table 3: Ablation study on the impact of acceptance rate with and without denoising trajectory alignment.", "description": "This table presents an ablation study analyzing the effect of denoising trajectory alignment on the acceptance rate in continuous speculative decoding. It compares the acceptance rates achieved with and without trajectory alignment across various model configurations, differing in draft and target model sizes and the number of draft tokens.", "section": "4.3 Ablation Study"}, {"content": "| $M_p$ | $M_q$ | $\\gamma$ | $\\alpha$/Speed |  |  |\n|---|---|---|---|---|---| \n|  |  |  | $\\alpha$/Speed | 0% | 5% | 15% |\n| 0% | 5% | 15% |  MAR-L | MAR-B | 32 |\n| MAR-L | MAR-B | 32 | 0.27/1.24 \u00d7 | 0.34/1.22 \u00d7 | **0.37**/1.21 \u00d7 |\n| MAR-L | MAR-B | 16 | 0.35/1.19 \u00d7 | 0.37/1.19 \u00d7 | **0.38**/1.17 \u00d7 |\n| MAR-L | MAR-B | 8 | 0.35/1.14 \u00d7 | **0.39**/1.13 \u00d7 | **0.39**/1.12 \u00d7 |\n| MAR-L | MAR-B | 4 | 0.32/1.04 \u00d7 | 0.37/1.02 \u00d7 | **0.39**/1.00 \u00d7 |\n| MAR-H | MAR-B | 32 | 0.25/1.63 \u00d7 | 0.30/1.63 \u00d7 | **0.33**/1.61 \u00d7 |\n| MAR-H | MAR-L | 32 | 0.24/1.36 \u00d7 | **0.33**/1.35 \u00d7 | 0.32/1.34 \u00d7 |\n| MAR-H | MAR-B | 16 | 0.32/1.53 \u00d7 | 0.33/1.52 \u00d7 | **0.34**/1.51 \u00d7 |\n| MAR-H | MAR-L | 16 | 0.34/1.32 \u00d7 | **0.35**/1.29 \u00d7 | **0.35**/1.29 \u00d7 |\n| MAR-H | MAR-B | 8 | 0.33/1.47 \u00d7 | 0.31/1.47 \u00d7 | **0.34**/1.44 \u00d7 |\n| MAR-H | MAR-L | 8 | 0.34/1.21 \u00d7 | 0.34/1.21 \u00d7 | **0.35**/1.21 \u00d7 |\n| MAR-H | MAR-B | 4 | 0.31/1.21 \u00d7 | 0.32/1.21 \u00d7 | **0.34**/1.20 \u00d7 |\n| MAR-H | MAR-L | 4 | 0.31/1.05 \u00d7 | **0.34**/1.03 \u00d7 | **0.34**/1.03 \u00d7 |", "caption": "Table 4: Ablation study on pre-filling ratio. The experimental configuration remains the same as Table\u00a01. Underline indicates the highest speedup. Bold means the highest \u03b1\ud835\udefc\\alphaitalic_\u03b1.", "description": "This table presents the results of an ablation study investigating the impact of the pre-filling ratio on the performance of continuous speculative decoding.  The study varies the percentage of tokens pre-filled from the target model (0%, 5%, and 15%) while keeping other experimental settings consistent with Table 1.  The results show the speedup achieved compared to the baseline and the acceptance rate (\u03b1).  The highest speedup for each setting is underlined and the highest acceptance rate is shown in bold. This allows for a direct comparison of speed and accuracy across different pre-filling strategies.", "section": "4.3 Ablation Study"}]