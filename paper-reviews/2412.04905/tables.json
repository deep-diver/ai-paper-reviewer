[{"content": "| Data | Goal | Scene | Persona | Utterance | Dialogue | Analysis | Generation | Multilingual |\n|---|---|---|---|---|---|---|---|---|\n| **DialyDialog** [Li et al., 2017] | \u2717 | \u2717 | \u2717 | \u2717 | \u2713 | \u2717 | \u2713 | \u2717 |\n| **DialogSum** [Chen et al., 2021] | \u2717 | \u2717 | \u2717 | \u2717 | \u2713 | \u2713 | \u2717 | \u2717 |\n| **SODA** [Kim et al., 2023a] | \u2717 | \u2717 | \u2717 | \u2717 | \u2713 | \u2717 | \u2713 | \u2717 |\n| **CharacterGLM** [Zhou et al., 2023a] | \u2717 | \u2713 | \u2713 | \u2717 | \u2713 | \u2717 | \u2713 | \u2717 |\n| **Persona-Chat** [Jandaghi et al., 2023] | \u2717 | \u2717 | \u2713 | \u2717 | \u2713 | \u2717 | \u2713 | \u2717 |\n| **SOTOPIA** [Zhou et al., 2024c] | \u2713 | \u2713 | \u2713 | \u2717 | \u2717 | \u2717 | \u2713 | \u2717 |\n| **Ditto** [Lu et al., 2024] | \u2717 | \u2717 | \u2713 | \u2717 | \u2713 | \u2717 | \u2713 | \u2713 |\n| **DEMO (Ours)** | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 |", "caption": "Table 1: The overview of our DEMO\u2019s characteristics in comparison to those in the related work. DEMO encompasses comprehensive dialogue elements and tasks applicable to both English and Chinese.", "description": "This table compares the characteristics of the proposed DEMO benchmark with existing dialogue datasets.  It highlights DEMO's comprehensiveness in covering various dialogue elements (goal, scene, persona, utterance) and tasks (dialogue analysis, generation, multilingual support).  Unlike many existing datasets that focus on a subset of these aspects, DEMO offers a more holistic approach, including both English and Chinese languages.", "section": "1 Introduction"}, {"content": "| Goal |\n|---|", "caption": "Table 2: The results of various LLMs on the DEMO. The highest score among different LLMs is highlighted in bold, and the second highest is underlined. And \u0394\u0394\\Deltaroman_\u0394 values represent the improvement over the baseline. Element Awareness has four tasks:(1)\u00a0Goa: Goal Recognition, (2)\u00a0Per: Persona Modeling, (3)\u00a0Sce: Scene\nReconstruction, and (4)\u00a0Utt: Utterance Mining. Dialogue Agent Interaction assesses the performance from four dimensions: (1)\u00a0Goa: Goal Achievement, (2)\u00a0Bel: Believability,(3)\u00a0Ski: Skillfulness,(4)\u00a0Rea: Realistic. Overall: The overall score is the average of the element awareness and dialogue agent interaction.", "description": "Table 2 presents a comprehensive evaluation of various Large Language Models (LLMs) on the Dialogue Element Modeling benchmark (DEMO).  The table shows each model's performance across four Element Awareness subtasks (Goal Recognition, Persona Modeling, Scene Reconstruction, and Utterance Mining), and four Dialogue Agent Interaction dimensions (Goal Achievement, Believability, Skillfulness, and Realism).  The highest score for each task/dimension is shown in bold, the second-highest is underlined.  Improvements over a baseline model are indicated using \u0394 values. The overall score is the average of all eight tasks/dimensions, providing a holistic view of each LLM's performance on the DEMO benchmark.", "section": "3 DEMO Benchmark"}, {"content": "| Scene |\n|---|---|", "caption": "Table 3: The Kappa consistency results between LLM annotations and two human raters on different elements", "description": "This table presents the inter-annotator agreement results, calculated using Cohen's Kappa statistic, to evaluate the consistency between LLM-generated annotations and human annotations for various dialogue elements.  The elements assessed include Goal, Persona, Scene, and Utterance, reflecting different aspects of dialogue understanding.  The Kappa values quantify the level of agreement between the automated LLM assessment and manual human evaluation for each element, providing a measure of the reliability of the LLM-generated annotations.", "section": "3.2 Construction Framework for Benchmark"}, {"content": "| Persona |\n|---|---|", "caption": "Table 4: Evaluation results on SOTOPIA hard episodes, which scored from seven social dimensions: believability (BEL), relationship (REL), knowledge (KNO), secret (SEC), social rules (SOC), financial and material benefits (FIN), and goal completion (GOA). The overall score is the average of the seven social dimensions reflecting the overall social intelligence. GPT-4o rates each dimension.", "description": "Table 4 presents the performance of different models on SOTOPIA's challenging scenarios.  SOTOPIA assesses social intelligence across seven dimensions: believability, relationship management, knowledge application, handling secrets, adherence to social rules, understanding financial and material benefits, and goal achievement.  Each model's performance is evaluated on each of these seven dimensions, with GPT-4o providing the scores. The overall score represents the average performance across all dimensions, providing a comprehensive measure of social intelligence. Higher scores indicate better performance in socially intelligent interactions.", "section": "4 Experiments"}, {"content": "| Utterance |\n|---|---|", "caption": "Table 5: The evaluation results of the LLM\u2019s general capability and alignment performance, using the accuracy score.", "description": "This table presents the results of evaluating the general capabilities and alignment of several large language models (LLMs).  The evaluation uses accuracy scores derived from the MMLU and HHH benchmarks, which test the models' performance across various tasks and their adherence to safety guidelines, respectively. This allows for comparison of the models' overall capabilities beyond the specific dialogue modeling tasks explored elsewhere in the paper.  Higher accuracy scores indicate better performance on these general language understanding and alignment assessments.", "section": "4 Experiments"}, {"content": "| Dialogue |\n|---|---|", "caption": "Table 6: The detailed versions of our used LLMs.", "description": "This table lists the specific versions of the large language models (LLMs) used in the experiments.  For each model (e.g., GPT-4, Claude, Llama, Qwen), the exact version number and whether it was accessed via an API or a local version (VLLM) are provided. This detail is crucial for reproducibility of the experiments.", "section": "4 Experiments"}]