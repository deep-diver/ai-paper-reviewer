{"references": [{"fullname_first_author": "Vaswani, A.", "paper_title": "Attention is all you need.", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is the foundation of the model studied in this paper."}, {"fullname_first_author": "Srivastava, R. K.", "paper_title": "Highway networks.", "publication_date": "2015-05-01", "reason": "This paper introduced highway networks, a key element of modern deep neural networks and relevant to the design choices discussed in the paper."}, {"fullname_first_author": "He, K.", "paper_title": "Deep residual learning for image recognition.", "publication_date": "2015-12-01", "reason": "This paper introduced residual networks, another crucial element of modern deep neural networks and relevant to the design choices discussed in the paper."}, {"fullname_first_author": "Voita, E.", "paper_title": "The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives.", "publication_date": "2019-11-01", "reason": "This paper is cited for analysis on representation collapse, a key concept in understanding the limitations and improvements of the model discussed in this paper."}, {"fullname_first_author": "Arefin, M. R.", "paper_title": "Seq-vcr: Preventing collapse in intermediate transformer representations for enhanced reasoning.", "publication_date": "2024-11-01", "reason": "This paper is directly cited for its analysis of representation collapse and the methods proposed to address this issue, making it highly relevant to the current paper."}]}