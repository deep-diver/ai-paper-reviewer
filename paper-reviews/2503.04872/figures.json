[{"figure_path": "https://arxiv.org/html/2503.04872/x1.png", "caption": "Figure 1: (A) A simplified diagram of our Branch-Merge distillation approach. (1) In the Branch phase, each copy of the Initial Model (backbone) is trained on knowledge from a different domain; (2) In the Merge phase, models are merged based on Arcee Fusion rules. (B) Performance Comparison of different LLM models\u00a0Mustar (2025). TinyR1-32B-Preview outperforms distilled models of the same size in science, math, and coding and achieves comparable results to Deepseek R1. LiveCodeBench here refers to the 24.08-25.02 subset of full LiveCodeBench.", "description": "Figure 1(A) illustrates the two-phase Branch-Merge distillation method.  First, the 'Branch Phase' involves creating specialized student models by fine-tuning a base model on different domains (math, coding, science). Then, the 'Merge Phase' combines these specialized models into a single unified model using Arcee Fusion. Figure 1(B) shows a performance comparison of various LLMs, demonstrating that TinyR1-32B-Preview (the result of the Branch-Merge method) surpasses other distilled models of similar size in math, coding, and science benchmarks, while achieving performance comparable to DeepSeek R1.", "section": "2 The Branch-Merge Distillation Approach"}, {"figure_path": "https://arxiv.org/html/2503.04872/extracted/6258368/figure/gpqa.jpg", "caption": "Figure 2: Performance Comparison of merged models on the GPQA-Diamond benchmark.", "description": "This figure shows the performance comparison of different model merging methods on the GPQA-Diamond benchmark.  The methods compared include Arcee, Model Soup, using only the Math model, using only the Science model, Dare Linear, and Dare Tiles.  The graph displays the GPQA scores achieved by each method, demonstrating that the Arcee merging method significantly outperforms other approaches.", "section": "3 Experiment"}]