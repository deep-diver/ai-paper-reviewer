[{"figure_path": "https://arxiv.org/html/2501.12202/extracted/6144353/figures/teaser.png", "caption": "Figure 1: An overall of Hunyuan3D 2.0 system.", "description": "This figure provides a high-level overview of the Hunyuan3D 2.0 system architecture.  It shows the three main components: Hunyuan3D-DiT (for generating the 3D mesh), Hunyuan3D-Paint (for generating the textures), and Hunyuan3D-Studio (a user-friendly platform that integrates the mesh and texture generation components for easier 3D asset creation and manipulation).  Examples of generated outputs from each component are visually displayed to showcase the system's capabilities in creating realistic and detailed 3D assets.", "section": "2 Hunyuan3D 2.0 Architecture"}, {"figure_path": "https://arxiv.org/html/2501.12202/x1.png", "caption": "Figure 2: \nAn overall of Hunyuan3D 2.0 architecture for 3D generation. It consists of two main components: Hunyuan3D-DiT for generating bare mesh from a given input image and Hunyuan3D-Paint for generating a textured map for the generated bare mesh. Hunyuan3D-Paint takes geometry conditions \u2013 normal maps and position maps of generated mesh as inputs and generates multi-view images for texture baking.", "description": "Hunyuan3D 2.0 is a two-stage 3D generation system.  First, Hunyuan3D-DiT, a diffusion model, creates a 3D mesh from an input image. Then, Hunyuan3D-Paint, another diffusion model, generates a texture map. Hunyuan3D-Paint uses the normal and position maps from the generated mesh to create multi-view images, which are then baked into a final texture.", "section": "2 Hunyuan3D 2.0 Architecture"}, {"figure_path": "https://arxiv.org/html/2501.12202/x2.png", "caption": "Figure 3: The overall architecture of Hunyuan3D-ShapeVAE.\nInstead of only using uniform sampling on mesh surface, We have developed an importance sampling strategy to extract high-frequency detail information from the input mesh surface, such as edges and corners. This allows the model to better capture and represent the intricate details of 3D shapes.\nNote that during the point query construction, the Farthest Point Sampling (FPS) operation is performed separately for the uniform point cloud and the importance sampling point cloud.", "description": "Hunyuan3D-ShapeVAE is an autoencoder that converts a 3D mesh into a sequence of tokens, which are then used by the diffusion model to generate new meshes.  The architecture uses an encoder that incorporates both uniform and importance sampling of the input mesh's point cloud. Importance sampling focuses on high-frequency details like edges and corners, providing more information for representing complex shapes and enhancing reconstruction accuracy.  The encoder then uses cross-attention to create a set of tokens. The decoder reconstructs the 3D mesh from these tokens via a marching cubes algorithm. Farthest Point Sampling (FPS) is used separately on both the uniform and importance sampled point clouds during point query construction. This two-pronged sampling approach is crucial to capturing the full spectrum of details in the input 3D mesh.", "section": "3.1 Hunyuan3D-ShapeVAE"}, {"figure_path": "https://arxiv.org/html/2501.12202/x3.png", "caption": "Figure 4: Overview of Hunyuan3D-DiT. It adopts a transformer architecture with both double- and single-stream blocks. This design benefits the interaction between modalities of shape and image, helping our model to generate bare meshes with exceptional quality. (Note that the orange blocks have no learnable parameters, the blue blocks contain trainable parameters, and the gray blocks indicate a module composed of more details.)", "description": "Hunyuan3D-DiT uses a transformer architecture with double- and single-stream blocks to process latent tokens (shape information) and condition tokens (image information).  The double-stream blocks allow interaction between the shape and image modalities, improving mesh generation quality.  Single-stream blocks process the information separately before combining them. Orange blocks are non-trainable, blue blocks are trainable, and gray blocks represent modules containing additional details not shown in the main diagram.  This architecture allows the model to generate high-fidelity bare meshes from input image prompts.", "section": "3 Generative 3D Shape Generation"}, {"figure_path": "https://arxiv.org/html/2501.12202/x4.png", "caption": "Figure 5: Overview of Hunyuan3D-Paint. We leverage an image delighting module to convert the input image to an unlit state to produce light-invariant texture maps.\nThe system features a double-stream image conditioning reference-net, which provides faithfully conditional image features to the model. Furthermore, it facilitates the production of texture maps that conform closely to the input image.\nThe multi-task attention module ensures that the model synthesizes multi-view consistent images. This module maintains the coherence of all generated images while adhering to the input.", "description": "Hunyuan3D-Paint uses a three-stage pipeline. First, an image delighting module converts the input image into a light-invariant representation.  Second, a double-stream image conditioning reference-net feeds conditional image features to the model. This network is designed to produce texture maps closely matching the input image. Finally, a multi-task attention module ensures that the generated images are multi-view consistent, maintaining coherence and alignment with the input across all viewpoints.", "section": "4 Generative Texture Map Synthesis"}, {"figure_path": "https://arxiv.org/html/2501.12202/x5.png", "caption": "Figure 6: Visual comparisons.\nWe illustrate the reconstructed mesh (blue paint aims to show more details) in the figure, which showcases that only Hunyuan3D-ShapeVAE reconstructs mesh with fine-grained surface details and neat space. (Better viewed by zooming in.)", "description": "This figure presents a visual comparison of 3D mesh reconstruction results between Hunyuan3D-ShapeVAE and other methods (Michelangelo, 3DShape2VecSet, Direct3D).  Each row shows the ground truth mesh and the reconstruction attempts by the various models for the same object.  The visualization highlights that Hunyuan3D-ShapeVAE excels at reconstructing meshes with more fine-grained surface details and cleaner geometry, lacking the artifacts or omissions seen in the other models' outputs.", "section": "5.1 3D Shape Generation"}, {"figure_path": "https://arxiv.org/html/2501.12202/x6.png", "caption": "Figure 7: Visual comparisons.\nWe display the input image and the generated bare mesh (blue paint aims to show more details) from all methods in the figure. The human faces and piano keys show that Hunyuan3D-DiT could synthesize detailed surface bumps, maintaining completeness. Several scenes or logos demonstrate that Hunyuan3D-DiT could generate intricate details. (Better viewed by zooming in.)", "description": "Figure 7 presents a visual comparison of 3D shape generation results from various methods, including Hunyuan3D-DiT and several baselines.  For each example, the input image and the generated bare mesh (highlighted with blue paint to emphasize details) are shown. The results for Hunyuan3D-DiT showcase the model's ability to accurately reproduce fine surface details, such as the textures of human faces and piano keys, while maintaining the overall completeness of the generated shape.  In addition, the figure displays several objects with complex details and intricate features, further illustrating Hunyuan3D-DiT's ability to create high-fidelity 3D assets from diverse image prompts.", "section": "5.1 3D Shape Generation"}, {"figure_path": "https://arxiv.org/html/2501.12202/x7.png", "caption": "Figure 8: Visual comparisons.\nWe demonstrate several generated texture maps on different bare meshes. The fish and rabbit texture map showcases that Hunyuan3D-Paint produces the most text-conforming results.\nThe football indicates that our model could synthesize seamless and clean texture maps.\nMoreover, Hunyuan3D-Paint could generate complex texture maps, like the castle and bear.\n(Better viewed by zooming in.)", "description": "Figure 8 presents a visual comparison of texture maps generated by Hunyuan3D-Paint and other methods on various 3D models (fish, rabbit, soccer ball, castle, bear).  The results highlight Hunyuan3D-Paint's superior performance in generating text-conforming texture maps that are both seamless and detailed.  The fish and rabbit examples particularly demonstrate the model's ability to closely match the textual description, while the football showcases its ability to produce clean and seamless textures.  The castle and bear further illustrate the model's capability to handle complex textures.", "section": "5.2 Texture Map Synthesis"}, {"figure_path": "https://arxiv.org/html/2501.12202/x8.png", "caption": "Figure 9: Visual results.\nWe generate different texture maps for two meshes, and the results validate the performance of Hunyuan3D-Paint on texture reskinning. (Better viewed by zooming in.)", "description": "This figure displays the results of applying the Hunyuan3D-Paint model to generate different textures for the same 3D model.  Two example 3D models (a backpack and a teapot) are shown, each rendered with multiple distinct textures. This showcases the model's ability to perform texture reskinning \u2013 changing the surface texture of a model without modifying its geometry. The various textures demonstrate the diverse range of results achievable by the model in response to different inputs or conditions.", "section": "5.2 Texture Map Synthesis"}, {"figure_path": "https://arxiv.org/html/2501.12202/extracted/6144353/figures/texture/skinning.png", "caption": "Figure 10: The results of user study.", "description": "The figure presents a bar chart summarizing the results of a user study comparing Hunyuan3D 2.0 to several other methods across three criteria: overall user satisfaction, 3D asset quality, and adherence to image conditions.  Each bar represents the average percentage score for a given metric and method.  This allows for a direct visual comparison of the different models' performance from a user perspective.", "section": "5 Evaluations"}, {"figure_path": "https://arxiv.org/html/2501.12202/x9.png", "caption": "Figure 11: Visual comparisons.\nThe first case reflects that Hunyuan3D 2.0 could synthesize detailed surface bumps and correct texture maps.\nThe second penguin showcases our model\u2019s ability to handle complex actions.\nThe last mountain demonstrates that Hunyuan3D-DiT could produce intricate structures, and Hunyuan3D-Paint can synthesize vivid texture maps.\n(Better viewed by zooming in.)", "description": "Figure 11 presents a visual comparison of 3D model generation results from Hunyuan3D 2.0 and other methods.  The top row shows penguins, highlighting the ability of Hunyuan3D 2.0 to generate realistic surface details and textures. The middle row features a small animated character, showcasing the system's capacity to produce models capable of complex poses.  The bottom row depicts mountain ranges, illustrating the generation of intricate details in geometry and textures by Hunyuan3D-DiT and Hunyuan3D-Paint.", "section": "5.3 Textured 3D Assets Generation"}]