[{"figure_path": "2410.13785/tables/table_6_0.html", "caption": "Table 1: Main Experiments. GPT evaluation is used to compare each baseline with PopAlign on two alignment tasks (i.e., Harmless-Base, and Helpful-Base). The scores under 50.0 refers to the inferior performance compared with PopAlign. For the other three leaderboards (i.e., MT-Bench, AlpacaEval 2.0, and Arena Hard), we use their official GPT evaluation pipeline to obtain a point-wise score. Note that we report length-controlled win rate on AlpacaEval 2.0 tasks due to its higher agreement with human annotation. Higher scores indicate better performance. The best result is in bold, and the second-best result is underlined (with Label-DPO excluded).", "description": "Table 1 shows the performance of PopAlign and baselines on two alignment tasks and three leaderboards, measured by win rates against GPT.", "section": "3.2 Experimental Results"}, {"figure_path": "2410.13785/tables/table_6_1.html", "caption": "Table 2: Contrast Accuracy of the Synthesized Responses. We evaluate the percentage of synthesized chosen responses preferred by an oracle model over the rejected ones. We utilized GPT-4 and PairRM (Jiang et al., 2023) as the oracle models due to their well-recognized abilities in preference labeling. The best result is highlighted in bold.", "description": "The table shows the contrast accuracy of different contrasting strategies, measured by the percentage of synthesized response pairs where the oracle model correctly identifies and prefers the chosen response over the rejected one.", "section": "3.2 Experimental Results"}, {"figure_path": "2410.13785/tables/table_7_0.html", "caption": "Table 3: Evaluating the preference modeling, Label-RM and Label-DPO are trained on the original label responses in the training dataset. We report reward accuracies and reward margins (i.e., the mean difference between the chosen and corresponding rejected rewards) on the test split of UltraFeedback. The best scores are highlighted in bold.", "description": "This table shows a comparison of the reward accuracy and margins for several methods including PopAlign on the UltraFeedback dataset.", "section": "3.2 Experimental Results"}, {"figure_path": "2410.13785/tables/table_8_0.html", "caption": "Table 4: The effect of different models to be aligned. Both PopAlign-Yi and PopAlign-LLaMA3 is trained on the same data synthesized by Yi series (AI et al., 2024) as detailed in \u00a73.1.3.", "description": "The table shows the performance of different models (Yi-6B-Chat, PopAlign-Yi, LLaMA3-8B-Instruct, PopAlign-LLaMA3) on the MT-Bench leaderboard, comparing their performance when aligned using different methods.", "section": "3.2 Experimental Results"}, {"figure_path": "2410.13785/tables/table_8_1.html", "caption": "Table 1: Main Experiments. GPT evaluation is used to compare each baseline with PopAlign on two alignment tasks (i.e., Harmless-Base, and Helpful-Base). The scores under 50.0 refers to the inferior performance compared with PopAlign. For the other three leaderboards (i.e., MT-Bench, AlpacaEval 2.0, and Arena Hard), we use their official GPT evaluation pipeline to obtain a point-wise score. Note that we report length-controlled win rate on AlpacaEval 2.0 tasks due to its higher agreement with human annotation. Higher scores indicate better performance. The best result is in bold, and the second-best result is underlined (with Label-DPO excluded).", "description": "Table 1 shows the performance comparison of PopAlign against several baselines on two alignment tasks and three leaderboards using GPT evaluation.", "section": "3.2 Experimental Results"}, {"figure_path": "2410.13785/tables/table_12_0.html", "caption": "Table 1: Main Experiments. GPT evaluation is used to compare each baseline with PopAlign on two alignment tasks (i.e., Harmless-Base, and Helpful-Base). The scores under 50.0 refers to the inferior performance compared with PopAlign. For the other three leaderboards (i.e., MT-Bench, AlpacaEval 2.0, and Arena Hard), we use their official GPT evaluation pipeline to obtain a point-wise score. Note that we report length-controlled win rate on AlpacaEval 2.0 tasks due to its higher agreement with human annotation. Higher scores indicate better performance. The best result is in bold, and the second-best result is underlined (with Label-DPO excluded).", "description": "Table 1 presents the performance comparison of PopAlign against baselines on two alignment tasks and three leaderboards, using GPT evaluation to determine win rates.", "section": "3.2 Experimental Results"}, {"figure_path": "2410.13785/tables/table_13_0.html", "caption": "Table 1: Main Experiments. GPT evaluation is used to compare each baseline with PopAlign on two alignment tasks (i.e., Harmless-Base, and Helpful-Base). The scores under 50.0 refers to the inferior performance compared with PopAlign. For the other three leaderboards (i.e., MT-Bench, AlpacaEval 2.0, and Arena Hard), we use their official GPT evaluation pipeline to obtain a point-wise score. Note that we report length-controlled win rate on AlpacaEval 2.0 tasks due to its higher agreement with human annotation. Higher scores indicate better performance. The best result is in bold, and the second-best result is underlined (with Label-DPO excluded).", "description": "Table 1 presents the quantitative results comparing PopAlign's performance against various baselines across two alignment tasks and three leaderboards, using GPT evaluation for win rates.", "section": "3.2 Experimental Results"}]