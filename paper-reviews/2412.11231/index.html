<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Smaller Language Models Are Better Instruction Evolvers &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="Smaller Language Models Are Better Instruction Evolvers &#183; HF Daily Paper Reviews by AI"><meta name=description content="Smaller is better: SLMs outperform LLMs in evolving complex & diverse instructions for AI training."><meta name=keywords content="Natural Language Processing,Large Language Models,üè¢ Beijing University of Posts and Telecommunications,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="Smaller Language Models Are Better Instruction Evolvers"><meta property="og:description" content="Smaller is better: SLMs outperform LLMs in evolving complex & diverse instructions for AI training."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-15T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-15T00:00:00+00:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="üè¢ Beijing University of Posts and Telecommunications"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/cover.png"><meta name=twitter:title content="Smaller Language Models Are Better Instruction Evolvers"><meta name=twitter:description content="Smaller is better: SLMs outperform LLMs in evolving complex & diverse instructions for AI training."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Smaller Language Models Are Better Instruction Evolvers","headline":"Smaller Language Models Are Better Instruction Evolvers","abstract":"Smaller is better: SLMs outperform LLMs in evolving complex \u0026amp; diverse instructions for AI training.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.11231\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2024","dateCreated":"2024-12-15T00:00:00\u002b00:00","datePublished":"2024-12-15T00:00:00\u002b00:00","dateModified":"2024-12-15T00:00:00\u002b00:00","keywords":["Natural Language Processing","Large Language Models","üè¢ Beijing University of Posts and Telecommunications"],"mainEntityOfPage":"true","wordCount":"5507"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-02-17/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-02-17</p></a><a href=/ai-paper-reviewer/2025-02-18/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-02-18</p></a><a href=/ai-paper-reviewer/2025-02-19/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-02-19</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-17/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-02-17</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-18/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-02-18</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-19/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-02-19</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.11231/cover_hu1280330465895777431.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.11231/>Smaller Language Models Are Better Instruction Evolvers</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Smaller Language Models Are Better Instruction Evolvers</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-15T00:00:00+00:00>15 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>5507 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">26 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.11231/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.11231/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/natural-language-processing/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Natural Language Processing
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/large-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-beijing-university-of-posts-and-telecommunications/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Beijing University of Posts and Telecommunications</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#slm-instruction-evolution>SLM Instruction Evolution</a></li><li><a href=#beyond-model-scale>Beyond Model Scale</a></li><li><a href=#output-space--overconfidence>Output Space & Overconfidence</a></li><li><a href=#ic-ifd-complexity-matters>IC-IFD: Complexity Matters</a></li><li><a href=#future-of-slm-synthesis>Future of SLM Synthesis</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#slm-instruction-evolution>SLM Instruction Evolution</a></li><li><a href=#beyond-model-scale>Beyond Model Scale</a></li><li><a href=#output-space--overconfidence>Output Space & Overconfidence</a></li><li><a href=#ic-ifd-complexity-matters>IC-IFD: Complexity Matters</a></li><li><a href=#future-of-slm-synthesis>Future of SLM Synthesis</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.11231</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Tingfeng Hui et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2024-12-17</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.11231 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.11231 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/smaller-language-models-are-better target=_self role=button>‚Üó Papers with Code</a></p><audio controls><source src=https://ai-paper-reviewer.com/2412.11231/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Large language models (LLMs) are revolutionizing NLP applications. Creating complex and diverse instructions is crucial for effective LLM training, but generating them is challenging. Current approaches typically assume larger models are better at generating these instructions, leading to heavy reliance on resource-intensive models like GPT-4. This study challenges that assumption.</p><p>This paper investigates the potential of <strong>smaller language models (SLMs) for instruction evolution</strong>. It finds that <strong>SLMs outperform LLMs</strong> across various scenarios. <strong>SLMs produce more complex and diverse instructions</strong>, attributed to their broader output space and less tendency towards overconfidence. The paper also introduces <strong>IC-IFD</strong>, a new metric for assessing instruction data effectiveness <strong>without needing instruction tuning</strong>.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-37b4c89b7193b5fb8dc42c6e57fd6b5e></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-37b4c89b7193b5fb8dc42c6e57fd6b5e",{strings:[" Smaller language models (SLMs) are more effective than larger language models (LLMs) at evolving complex and diverse instructions. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-20e63e609f0b224f23a466f5722c5ac4></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-20e63e609f0b224f23a466f5722c5ac4",{strings:[" SLMs possess a broader output space during instruction evolution, avoiding overconfidence in token generation observed in LLMs. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-a0caad597337e7e39d6e47303aef0f44></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-a0caad597337e7e39d6e47303aef0f44",{strings:[" The Instruction Complex-Aware IFD (IC-IFD) metric provides a more accurate evaluation of instruction data quality without requiring instruction tuning resources "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p><strong>This research challenges the prevailing assumption that larger language models (LLMs) are inherently superior for evolving instructions.</strong> It demonstrates that <strong>smaller language models (SLMs) can actually generate more complex and diverse instructions</strong>, requiring fewer resources and leading to more efficient instruction tuning. This finding opens new avenues for <strong>optimizing instruction data creation</strong>, focusing on SLM capabilities and potentially reducing computational costs in AI research. Moreover, the introduction of the IC-IFD metric provides a valuable tool for <strong>assessing instruction data quality</strong> without the need for resource-intensive instruction tuning.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x1.png alt></figure></p><blockquote><p>üîº This figure compares the performance of a smaller language model (Llama-3.1-8B-Instruct) and a larger language model (Llama-3.1-70B-Instruct) in evolving instructions for a smaller backbone model (Llama-3-8B) across three iterations. The performance is evaluated on four downstream tasks: instruction following (IFEval with prompt-based and instance-based scores), math reasoning (GSM8K and MATH), and code generation (HumanEval and MBPP). The x-axis represents the iteration number (0-3, where 0 represents the seed data). The y-axis represents the performance metric for each task. The lines indicate the performance trend across iterations. This visualization aims to demonstrate whether smaller or larger language models are more effective at evolving instructions for improved downstream task performance.</p><details><summary>read the caption</summary>Figure 1: Comparison of performance on Llama-3-8B during three iterations of instruction evolution, using Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models for each round under Evol-Instruct scenario.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Instruction Following (IFEval)</th><th></th><th></th><th>Math Reasoning</th><th></th><th>Code Generation</th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>Pr.(S)</td><td>In.(S)</td><td>Pr.(L)</td><td>In.(L)</td><td></td><td>GSM8K</td><td>MATH</td><td></td><td>HumanEval</td></tr><tr><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td></tr><tr><td><em>Supervised Model: Llama-3.1-70B-Instruct</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mistral-7B-v0.3</td><td>19.59</td><td>31.77</td><td>22.74</td><td>34.65</td><td></td><td>33.89</td><td><strong>3.16</strong></td><td></td><td>24.39</td></tr><tr><td>DeepSeek-7B</td><td>36.23</td><td><strong>48.20</strong></td><td>41.04</td><td>52.52</td><td></td><td><strong>48.07</strong></td><td>2.96</td><td></td><td>28.66</td></tr><tr><td>Llama-3.2-3B</td><td>40.11</td><td>50.84</td><td>43.81</td><td>54.43</td><td></td><td>53.75</td><td>6.60</td><td></td><td>35.98</td></tr><tr><td>Llama-3-8B</td><td>33.83</td><td>46.28</td><td>36.41</td><td>49.28</td><td></td><td>63.00</td><td>7.62</td><td></td><td>43.90</td></tr><tr><td>Llama-3.1-8B</td><td>34.57</td><td>46.04</td><td>38.81</td><td>50.48</td><td></td><td>64.22</td><td>11.32</td><td></td><td><strong>51.22</strong></td></tr><tr><td>InternLM-2-7B</td><td>40.85</td><td>53.48</td><td>44.54</td><td>56.95</td><td></td><td><strong>68.31</strong></td><td>19.50</td><td></td><td>56.10</td></tr><tr><td><em>Supervised Model: Llama-3.1-8B-Instruct</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mistral-7B-v0.3</td><td><strong>24.40</strong></td><td><strong>35.01</strong></td><td><strong>26.25</strong></td><td><strong>37.53</strong></td><td></td><td><strong>40.18</strong></td><td>2.84</td><td></td><td><strong>29.27</strong></td></tr><tr><td>DeepSeek-7B</td><td><strong>36.60</strong></td><td>48.08</td><td><strong>41.77</strong></td><td><strong>53.12</strong></td><td></td><td>47.92</td><td><strong>3.56</strong></td><td></td><td><strong>34.76</strong></td></tr><tr><td>Llama-3.2-3B</td><td><strong>41.59</strong></td><td><strong>53.48</strong></td><td><strong>45.66</strong></td><td><strong>57.07</strong></td><td></td><td><strong>55.12</strong></td><td><strong>7.32</strong></td><td></td><td><strong>39.02</strong></td></tr><tr><td>Llama-3-8B</td><td><strong>35.49</strong></td><td><strong>47.00</strong></td><td><strong>39.56</strong></td><td><strong>50.72</strong></td><td></td><td><strong>63.38</strong></td><td><strong>11.44</strong></td><td></td><td><strong>48.17</strong></td></tr><tr><td>Llama-3.1-8B</td><td><strong>38.45</strong></td><td><strong>50.96</strong></td><td><strong>43.81</strong></td><td><strong>55.28</strong></td><td></td><td><strong>67.10</strong></td><td><strong>13.12</strong></td><td></td><td>48.78</td></tr><tr><td>InternLM-2-7B</td><td><strong>43.07</strong></td><td><strong>54.80</strong></td><td><strong>47.32</strong></td><td><strong>58.39</strong></td><td></td><td>68.08</td><td><strong>20.32</strong></td><td></td><td><strong>57.93</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of various 7B-scale language models (Mistral, DeepSeek, Llama, InternLM) after being fine-tuned on instruction data generated by either a smaller Llama-3.1-8B-Instruct model or a larger Llama-3.1-70B-Instruct model, within the Evol-Instruct scenario. The evaluation metrics span Instruction Following (IFEval with both prompt and instruction level scores), Math Reasoning (GSM8K and MATH), and Code Generation (HumanEval and MBPP). The table aims to demonstrate whether instructions evolved by smaller or larger language models lead to better performance in downstream tasks after fine-tuning.</p><details><summary>read the caption</summary>Table 1: Comparison of performance with Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models under Evol-Instruct scenario.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">SLM Instruction Evolution<div id=slm-instruction-evolution class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#slm-instruction-evolution aria-label=Anchor>#</a></span></h4><p><strong>SLM Instruction Evolution</strong> explores the novel concept of using smaller language models (SLMs) to generate and refine instructions for other AI models. This challenges the prevailing assumption that larger models are inherently better for this task. The research suggests that <strong>SLMs, due to a broader output space in token generation,</strong> produce more diverse and complex instructions, ultimately leading to improved performance in downstream tasks. This could be a <strong>significant shift in how we approach instruction tuning,</strong> potentially saving computational resources while boosting effectiveness. It opens exciting possibilities for aligning models with complex tasks by <strong>leveraging the unique strengths of SLMs in instruction data creation.</strong> Further investigation into the intricacies of SLM-driven evolution across different domains could uncover even more valuable insights.</p><h4 class="relative group">Beyond Model Scale<div id=beyond-model-scale class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#beyond-model-scale aria-label=Anchor>#</a></span></h4><p><strong>Scaling model size alone isn&rsquo;t the key to improved performance.</strong> While larger models possess greater capacity, factors like <strong>data quality, instruction design, and training methodologies</strong> play crucial roles. Smaller models, strategically trained, can outperform larger counterparts. Future research should explore <strong>efficient training techniques for smaller models</strong>, optimizing data usage and exploring novel architectures to maximize their potential. This shift towards efficiency could democratize access to powerful AI, reducing computational barriers and enabling wider adoption.</p><h4 class="relative group">Output Space & Overconfidence<div id=output-space--overconfidence class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#output-space--overconfidence aria-label=Anchor>#</a></span></h4><p><strong>Smaller language models (SLMs)</strong> exhibit a <strong>broader output space</strong> compared to <strong>larger language models (LLMs)</strong>. This broader output space contributes to the generation of <strong>more diverse and complex instructions</strong>, which are crucial for effective instruction tuning. LLMs, while generally more proficient in following instructions, tend to over-rely on high-probability tokens during instruction generation. This <strong>overconfidence</strong> narrows their output space, limiting the diversity of the generated instructions. Consequently, SLMs, with their less constrained token generation, emerge as <strong>better instruction evolvers</strong> despite their lower instruction-following capabilities. This suggests that encouraging exploration over exploitation in instruction generation is beneficial.</p><h4 class="relative group">IC-IFD: Complexity Matters<div id=ic-ifd-complexity-matters class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#ic-ifd-complexity-matters aria-label=Anchor>#</a></span></h4><p><strong>IC-IFD</strong>, or Instruction Complex-Aware IFD, introduces a crucial shift in evaluating instruction data quality. It underscores that <strong>instruction complexity significantly influences model performance</strong>, moving beyond simply assessing responses. Traditional metrics like IFD often overlook how complex instructions, even with higher IFD scores, can hinder performance. IC-IFD addresses this by incorporating <strong>instruction perplexity as a penalty</strong>, offering a more nuanced evaluation. This encourages generating instructions that are both effective and comprehensible, avoiding overly complex phrasing that can confuse models. IC-IFD promotes a balance between <strong>instruction difficulty and clarity</strong>, ultimately improving the effectiveness of instruction tuning. This shift has important implications for aligning language models with downstream tasks, especially complex ones. By considering complexity, we move towards generating instruction data that truly unlocks model potential.</p><h4 class="relative group">Future of SLM Synthesis<div id=future-of-slm-synthesis class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-of-slm-synthesis aria-label=Anchor>#</a></span></h4><p>The <strong>future of Smaller Language Model (SLM) synthesis</strong> lies in exploring their unique advantages. While this paper demonstrates SLMs&rsquo; superior instruction evolution capabilities compared to LLMs, further research should explore their potential beyond instruction tuning. <strong>Key areas include:</strong> 1) Expanding applications to broader domains like dialogue generation or creative writing. 2) Investigating the full SLM instruction data synthesis pipeline, not just evolution, optimizing for diverse dataset creation. 3) Refining evaluation metrics like the proposed IC-IFD to better assess complex instructions without relying on resource-intensive tuning. <strong>SLM&rsquo;s efficiency and broader output space suggest potential for novel applications,</strong> requiring further investigation into architecture, training methods, and efficient deployment strategies to maximize impact. This research opens exciting avenues for <strong>democratizing access to powerful language models.</strong></p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x2.png alt></figure></p><blockquote><p>üîº This figure presents the distribution of difficulty levels for instructions generated by a smaller language model (Llama-3.1-8B-Instruct) and a larger language model (Llama-3.1-70B-Instruct) across three iterations of instruction evolution. The difficulty levels are categorized as Very Easy, Easy, Medium, Hard, and Very Hard. The x-axis represents the difficulty levels, and the y-axis represents the percentage of instructions falling into each category. Each bar group represents one round of evolution (Iter1, Iter2, and Iter3), and within each group, the blue and orange bars represent the distributions from the smaller and larger models, respectively. The figure aims to demonstrate whether smaller or larger language models generate more complex instructions during the evolution process across different datasets: Alpaca (instruction following), GSM8K (mathematical reasoning), and HumanEval (code generation).</p><details><summary>read the caption</summary>Figure 2: Distribution of difficulty levels for instructions evolved during three iterations, using Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models for each round under Evol-Instruct scenario.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x3.png alt></figure></p><blockquote><p>üîº This figure compares the performance of different sizes of models (0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B) within the Qwen-2.5 series on four downstream tasks after instruction tuning with instruction data generated by both small language models (SLMs) and large language models (LLMs). The tasks include instruction following (IFEval, Pr.(S) and In.(S)), mathematical reasoning (GSM8K and MATH), and code generation (HumanEval and MBPP). For larger models (14B, 32B, and 72B), LORA was used for fine-tuning due to limited computational resources. The metrics used for evaluation are: Pr.(S) and In.(S) (strict and inclusive accuracy on IFEval), GSM8K and MATH (accuracy scores), HumanEval and MBPP (pass@1). The graph visually represents how SLM-generated and LLM-generated instruction data impact the performance of the models of different sizes for each of the four tasks. The complete numerical results of this comparison can be found in Table 11 of the paper.</p><details><summary>read the caption</summary>Figure 3: Comparison of performance among Qwen-2.5 series models. Detailed results can be found in Table¬†11.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x4.png alt></figure></p><blockquote><p>üîº This histogram displays the distribution of minimum neighbor distances (MND) for instructions generated by two models, Llama-3.1-8B-Instruct (SLM) and Llama-3.1-70B-Instruct (LLM), within the AutoIF instruction generation scenario. The x-axis represents the MND, a measure of similarity between instructions, calculated in the embedding space using all-mpnet-base-v2. The y-axis represents the frequency density of instructions at each MND. A higher MND suggests greater dissimilarity between instructions, implying better diversity. The figure aims to visually compare the diversity of instructions generated by the smaller and larger language models.</p><details><summary>read the caption</summary>Figure 4: Distribution of Minimum Neighbor Distance for instructions generated by Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct in the AutoIF scenario.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x5.png alt></figure></p><blockquote><p>üîº This figure presents a comparison of probability distributions for the top-1 output tokens generated by smaller language models (SLMs) and larger language models (LLMs) during instruction evolution in the Evol-Instruct scenario. The x-axis represents the probability, and the y-axis is the density. SLMs show a broader distribution of probabilities, with a lower peak and a longer tail, suggesting that their output space is more diverse than LLMs, which have a sharper peak around higher probabilities.</p><details><summary>read the caption</summary>Figure 5: Comparison of output token probability distributions in the Evol-Instruct scenario.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x6.png alt></figure></p><blockquote><p>üîº This figure compares the performance of instruction-tuned language models using different data filtering strategies. IC-IFD and IFD, two metrics for evaluating the quality of instructions, are used to filter the Alpaca dataset, a collection of instruction-following data. Three different filtering ratios (5%, 10%, and 15%) are applied. The filtered data is then used to fine-tune two language models: Llama-3-8B and Llama-3.2-3B. Performance is evaluated using AlpacaFarm, a benchmark for evaluating instruction following models. The comparison is shown in terms of win-tie-lose ratios, derived from assessments by GPT-4 on whether a model&rsquo;s response to an instruction is better, worse, or equal to another model&rsquo;s response. The results show that IC-IFD consistently outperforms IFD across all filtering ratios for both models, suggesting that IC-IFD is a more effective metric for filtering and selecting high-quality instruction data.</p><details><summary>read the caption</summary>Figure 6: Performance comparison of three data selection ratios on Alpaca dataset between IC-IFD and IFD.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x7.png alt></figure></p><blockquote><p>üîº This figure compares the performance of a Llama-3.2-3B model fine-tuned on different subsets of the Alpaca dataset, created using the Instruction Complex-Aware Instruction Following Difficulty (IC-IFD) metric. Three different selection ratios (5%, 10%, and 15%) are used to filter the full Alpaca dataset and create smaller training sets. The performance of the models trained on these filtered datasets is then compared to a model fine-tuned on the complete Alpaca dataset. The results are visualized to demonstrate the effectiveness of IC-IFD for instruction data selection.</p><details><summary>read the caption</summary>Figure 7: Performance comparison of three data selection ratios on Alpaca dataset between IC-IFD and full dataset.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x8.png alt></figure></p><blockquote><p>üîº This figure presents two examples of how large language models (LLMs) and smaller language models (SLMs) evolve instructions when given the constraint of a busy schedule and restrictive diet. The original instruction is to give three tips for staying healthy. The LLM adds the constraint of a &lsquo;moderate lifestyle&rsquo; and requests an explanation of how to incorporate the tips into a daily routine. The SLM adds the constraints of &rsquo;limited time for exercise&rsquo; and &lsquo;restrictive diet,&rsquo; and requests &rsquo;evidence-based&rsquo; tips. This demonstrates how SLMs are capable of generating more complex and challenging instructions compared to LLMs, by incorporating more constraints into the evolved prompt.</p><details><summary>read the caption</summary>Figure 8: Comparison of cases between LLMs and SLMs under adding constraints strategy.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x9.png alt></figure></p><blockquote><p>üîº This figure presents two examples of evolved instructions under the &lsquo;deepening&rsquo; strategy in the Evol-Instruct scenario, comparing the outputs of a Smaller Language Model (SLM) and a Large Language Model (LLM). The original instruction is a simple math word problem. The LLM adds a single additional condition about prorating the hourly wage, while the SLM adds several significantly more complex conditions regarding bonuses, weekday/weekend rates, and timeliness. This illustrates how SLMs can evolve more complex instructions than LLMs, leading to potentially more effective instruction tuning data.</p><details><summary>read the caption</summary>Figure 9: Comparison of cases between LLMs and SLMs under deepening strategy.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x10.png alt></figure></p><blockquote><p>üîº This figure presents the prompt template used for in-depth evolution of instructions within the Evol-Instruct scenario. The template instructs an LLM to act as a prompt rewriter, with the objective of increasing the complexity of a given prompt to challenge other large language models (e.g. ChatGPT, GPT-4). The rewritten prompt is expected to remain comprehensible and answerable by humans while only adding 10-20 words. Placeholders <code>{METHOD}</code> and <code>{INSTRUCTION}</code> within the template are to be replaced with the chosen evolution method (e.g., &lsquo;Adding Constraints&rsquo;, &lsquo;Deepening&rsquo;, etc.) and the given instruction to be evolved, respectively.</p><details><summary>read the caption</summary>Figure 10: In-depth evolution prompt template utilized in Evol-Instruct scenario.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x11.png alt></figure></p><blockquote><p>üîº This figure lists the four in-depth methods utilized in the Evol-Instruct scenario: 1. <strong>Adding Constraints:</strong> This method involves adding one or more constraints or requirements to the original prompt. 2. <strong>Deepening:</strong> If the given prompt contains inquiries about certain issues, this method increases the depth and breadth of the inquiry to make it more complex. 3. <strong>Concretizing:</strong> This method involves replacing general concepts in the prompt with more specific concepts. 4. <strong>Adding Reasoning Steps:</strong> If the original prompt can be solved with a few simple thinking processes, this method rewrites the prompt to explicitly request multiple-step reasoning, making it more challenging for the language model.</p><details><summary>read the caption</summary>Figure 11: Four in-depth methods utilized in Evol-Instruct scenario.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x12.png alt></figure></p><blockquote><p>üîº This figure presents the prompt template employed for in-breadth evolution within the Evol-Instruct scenario. In-breadth evolution aims to generate entirely new prompts inspired by a given prompt, but within the same domain, while exhibiting increased rarity. The generated prompt should maintain a similar length and complexity as the original. Crucially, the output should solely consist of the new prompt without any additional explanations or symbols. The template includes placeholders for the original prompt and the newly created prompt.</p><details><summary>read the caption</summary>Figure 12: In-breadth evolution prompt template utilized in Evol-Instruct scenario.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x13.png alt></figure></p><blockquote><p>üîº This figure presents the prompt template employed for generating seed instructions within the AutoIF scenario. It directs the language model to generate 50 distinct instructions, emphasizing that these instructions should focus on the format rather than the style of the response. It also highlights the importance of verifiability, stating that adherence to the instructions should be easily assessable by a Python function. Example instructions are provided for both desired and undesired instruction types, and format specifications for the generated output are clearly articulated. The prompt leverages a few seed examples within AutoIF to create verifiable instructions.</p><details><summary>read the caption</summary>Figure 13: Prompt template of Self-Instruct Seed Instructions in AutoIF scenario.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x14.png alt></figure></p><blockquote><p>üîº This figure presents the prompt template utilized for the &lsquo;Verification Funcs and Cases Generation&rsquo; phase within the AutoIF process. This stage focuses on generating Python code to evaluate whether responses adhere to given instructions. The prompt instructs the language model to create both an evaluation function and three test cases (input and expected output). The provided JSON example illustrates the format. In essence, this prompt guides the model to create an automated verification process for newly generated instructions in the AutoIF pipeline by providing sample JSON of the output function and the cases.</p><details><summary>read the caption</summary>Figure 14: Prompt template of Verification Funcs and Cases Generation in AutoIF scenario.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x15.png alt></figure></p><blockquote><p>üîº This figure presents the prompt template employed in the Auto Evol-Instruct scenario. The user acts as an instruction rewriter, tasked with transforming a given instruction into a more complex form. The process involves four steps: 1) Listing potential methods to increase instruction complexity, 2) Formulating a plan to implement these methods, 3) Rewriting the instruction based on the plan, and 4) Reviewing and refining the rewritten instruction for clarity and conciseness. The objective is to make instructions challenging for large language models, while still maintaining their understandability for humans. The template specifies the format strictly.</p><details><summary>read the caption</summary>Figure 15: Prompt template of Auto Evol-Instruct scenario.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x16.png alt></figure></p><blockquote><p>üîº This figure presents the prompt template used for response generation in the experiments. It shows two variations of the prompt, one for when an input is provided along with the instruction, and another for when only an instruction is given. In both cases, the model is instructed to provide a comprehensive and accurate response. This ensures consistent prompting across different experimental setups.</p><details><summary>read the caption</summary>Figure 16: Prompt template of response generation.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x17.png alt></figure></p><blockquote><p>üîº This figure presents the prompt template used for evaluating the difficulty levels of instructions. The prompt asks an LLM to assess an instruction&rsquo;s difficulty (&lsquo;very easy&rsquo;, &rsquo;easy&rsquo;, &lsquo;medium&rsquo;, &lsquo;hard&rsquo;, or &lsquo;very hard&rsquo;) based on the user&rsquo;s intent and knowledge needed to address it. The output should be just the difficulty level, without any additional text or symbols.</p><details><summary>read the caption</summary>Figure 17: Prompt template of evaluating the difficulty levels.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x18.png alt></figure></p><blockquote><p>üîº This figure presents the prompt template employed to extract keywords from the trajectories generated during the instruction evolution process. This process helps analyze the strategies employed by different language models, providing insights into how they modify instructions during the evolution.</p><details><summary>read the caption</summary>Figure 18: Prompt template of extracting the keywords from evolution trajectories.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x19.png alt></figure></p><blockquote><p>üîº This figure presents the prompt template utilized for evaluating the difficulty scores of instructions. The prompt instructs an LLM to assess an instruction&rsquo;s difficulty based on user intent and required knowledge. It requests the LLM to output a numerical score from 0 to 100, reflecting the estimated difficulty, without any additional text or symbols. This prompt is used to analyze the complexity of instructions in datasets.</p><details><summary>read the caption</summary>Figure 19: Prompt template of evaluating the difficulty scores.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11231/x20.png alt></figure></p><blockquote><p>üîº This figure presents the prompt template used for evaluating win/tie/lose rates between two AI assistants, which is used in the AlpacaFarm evaluation. The prompt consists of a user query and responses from two AI assistants. The prompt instructs an evaluator (likely a stronger LLM like GPT-4) to compare the quality of the two AI assistants‚Äô responses based on criteria including alignment with user needs, conciseness, comprehensiveness, logical flow, use of technical terms, and factual accuracy. The evaluator is then asked to output a single label indicating whether ‚ÄòAssistant 1 is better than Assistant 2‚Äô, ‚ÄòAssistant 1 is worse than Assistant 2‚Äô, or ‚ÄòAssistant 1 is equal to Assistant 2‚Äô.</p><details><summary>read the caption</summary>Figure 20: Prompt template of evaluating the win-tie-lose rates.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Instruction Following (IFEval)</th><th></th><th></th><th>Math Reasoning</th><th></th><th>Code Generation</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>Pr.(S)</td><td>In.(S)</td><td>Pr.(L)</td><td>In.(L)</td><td></td><td>GSM8K</td><td>MATH</td><td></td><td>HumanEval</td><td>MBPP</td></tr><tr><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td></tr><tr><td><strong>Supervised Model: Qwen-2-72B-Instruct</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mistral-7B-v0.3</td><td>20.15</td><td>30.94</td><td>23.84</td><td>34.41</td><td></td><td>46.93</td><td><strong>3.26</strong></td><td></td><td>32.32</td><td>1.80</td></tr><tr><td>DeepSeek-7B</td><td>35.67</td><td>47.12</td><td><strong>39.56</strong></td><td>50.84</td><td></td><td>44.81</td><td>2.76</td><td></td><td><strong>36.59</strong></td><td><strong>34.00</strong></td></tr><tr><td>Llama-3.2-3B</td><td>39.74</td><td>51.44</td><td>43.99</td><td>55.40</td><td></td><td>53.83</td><td><strong>7.40</strong></td><td></td><td>38.41</td><td>31.00</td></tr><tr><td>Llama-3-8B</td><td>34.75</td><td>45.80</td><td>37.71</td><td>48.92</td><td></td><td>63.76</td><td><strong>10.06</strong></td><td></td><td>43.90</td><td>35.40</td></tr><tr><td>Llama-3.1-8B</td><td><strong>36.41</strong></td><td><strong>47.60</strong></td><td>39.00</td><td>50.60</td><td></td><td>65.43</td><td>10.84</td><td></td><td><strong>48.17</strong></td><td>38.40</td></tr><tr><td>InternLM-2-7B</td><td>41.96</td><td>53.60</td><td>43.99</td><td>55.64</td><td></td><td>65.28</td><td>17.96</td><td></td><td>56.71</td><td>40.60</td></tr><tr><td><strong>Supervised Model: Qwen-2-7B-Instruct</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mistral-7B-v0.3</td><td><strong>25.32</strong></td><td><strong>37.17</strong></td><td><strong>29.76</strong></td><td><strong>41.01</strong></td><td></td><td><strong>47.31</strong></td><td>2.20</td><td></td><td><strong>32.93</strong></td><td><strong>12.00</strong></td></tr><tr><td>DeepSeek-7B</td><td><strong>36.41</strong></td><td><strong>48.56</strong></td><td>39.37</td><td><strong>51.32</strong></td><td></td><td><strong>48.07</strong></td><td><strong>3.82</strong></td><td></td><td>35.37</td><td>33.20</td></tr><tr><td>Llama-3.2-3B</td><td><strong>43.81</strong></td><td><strong>55.16</strong></td><td><strong>47.87</strong></td><td><strong>58.27</strong></td><td></td><td><strong>56.56</strong></td><td>7.18</td><td></td><td><strong>39.63</strong></td><td><strong>31.40</strong></td></tr><tr><td>Llama-3-8B</td><td><strong>38.92</strong></td><td><strong>48.33</strong></td><td><strong>43.81</strong></td><td><strong>52.19</strong></td><td></td><td><strong>63.91</strong></td><td>8.66</td><td></td><td><strong>45.73</strong></td><td><strong>38.40</strong></td></tr><tr><td>Llama-3.1-8B</td><td>34.75</td><td>45.80</td><td><strong>39.93</strong></td><td><strong>51.08</strong></td><td></td><td><strong>68.76</strong></td><td><strong>14.02</strong></td><td></td><td>46.34</td><td><strong>38.60</strong></td></tr><tr><td>InternLM-2-7B</td><td><strong>44.12</strong></td><td><strong>55.16</strong></td><td><strong>48.62</strong></td><td><strong>58.73</strong></td><td></td><td><strong>66.87</strong></td><td><strong>19.60</strong></td><td></td><td><strong>58.54</strong></td><td><strong>41.40</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of various 7B and 70B parameter models from the Mistral, DeepSeek, LLaMA, and InternLM families on instruction following, mathematical reasoning, and code generation tasks under the Evol-Instruct method. Two different supervised models are used: Qwen-2-7B-Instruct and Qwen-2-72B-Instruct. The metrics reported are Pr.(S), In.(S), Pr.(L), In.(L) for IFEval (instruction following); GSM8K and MATH for mathematical reasoning; and HumanEval and MBPP for code generation.</p><details><summary>read the caption</summary>Table 2: Comparison of performance with Qwen-2-7B-Instruct and Qwen-2-72B-Instruct as supervised models under Evol-Instruct scenario.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>IFEval</th><th></th><th></th><th>FollowBench (HSR)</th><th></th><th>Common Abilities</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>Pr.(S)</td><td>In.(S)</td><td>Pr.(L)</td><td>In.(L)</td><td></td><td>Level 1</td><td>Level 2</td><td>Level 3</td><td>Level 4</td><td>Level 5</td><td>Avg.</td><td></td><td>C-Eval</td></tr><tr><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td></tr><tr><td><strong>Supervision Model: Llama-3.1-70B-Instruct</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Llama-3.2-3B</td><td>40.85</td><td>51.92</td><td>42.33</td><td>53.84</td><td></td><td><strong>61.17</strong></td><td>57.59</td><td><strong>50.55</strong></td><td>33.09</td><td>26.74</td><td>45.83</td><td></td><td><strong>41.37</strong></td></tr><tr><td>Llama-3-8B</td><td>37.71</td><td>50.00</td><td>39.19</td><td>52.04</td><td></td><td>49.64</td><td>46.60</td><td>41.56</td><td>27.05</td><td>22.37</td><td>37.44</td><td></td><td>41.87</td></tr><tr><td>Llama-3.1-8B</td><td>41.96</td><td>53.36</td><td>42.70</td><td>54.20</td><td></td><td>51.77</td><td>45.60</td><td>45.04</td><td>34.85</td><td>26.61</td><td>40.78</td><td></td><td><strong>44.50</strong></td></tr><tr><td>Qwen-2-7B</td><td>41.96</td><td>53.60</td><td>43.62</td><td>55.64</td><td></td><td>72.18</td><td>62.45</td><td><strong>56.43</strong></td><td>41.31</td><td>35.42</td><td>53.56</td><td></td><td><strong>81.08</strong></td></tr><tr><td>Qwen-2.5-7B</td><td>49.17</td><td><strong>60.31</strong></td><td>50.46</td><td>61.51</td><td></td><td><strong>78.88</strong></td><td><strong>73.78</strong></td><td><strong>61.50</strong></td><td>51.99</td><td>45.42</td><td><strong>62.31</strong></td><td></td><td><strong>80.46</strong></td></tr><tr><td>InternLM-2-7B</td><td>46.21</td><td>56.71</td><td>48.06</td><td>58.63</td><td></td><td>68.89</td><td>62.23</td><td>54.17</td><td>44.27</td><td>42.06</td><td>54.33</td><td></td><td>60.11</td></tr><tr><td><strong>Supervision Model: Llama-3.1-8B-Instruct</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Llama-3.2-3B</td><td><strong>43.62</strong></td><td><strong>54.20</strong></td><td><strong>46.95</strong></td><td><strong>57.07</strong></td><td></td><td>56.95</td><td><strong>61.46</strong></td><td>50.20</td><td><strong>37.65</strong></td><td><strong>34.16</strong></td><td><strong>48.08</strong></td><td></td><td>40.56</td></tr><tr><td>Llama-3-8B</td><td><strong>41.04</strong></td><td><strong>51.32</strong></td><td><strong>42.88</strong></td><td><strong>53.11</strong></td><td></td><td><strong>62.99</strong></td><td><strong>54.38</strong></td><td><strong>49.29</strong></td><td><strong>32.21</strong></td><td><strong>32.21</strong></td><td><strong>46.21</strong></td><td></td><td><strong>43.49</strong></td></tr><tr><td>Llama-3.1-8B</td><td><strong>42.51</strong></td><td><strong>54.92</strong></td><td><strong>44.73</strong></td><td><strong>56.71</strong></td><td></td><td><strong>63.99</strong></td><td><strong>58.15</strong></td><td><strong>53.29</strong></td><td><strong>39.49</strong></td><td><strong>36.02</strong></td><td><strong>50.19</strong></td><td></td><td>43.77</td></tr><tr><td>Qwen-2-7B</td><td><strong>44.92</strong></td><td><strong>55.76</strong></td><td><strong>47.50</strong></td><td><strong>58.39</strong></td><td></td><td><strong>78.75</strong></td><td><strong>63.30</strong></td><td>52.31</td><td><strong>50.28</strong></td><td><strong>43.08</strong></td><td><strong>57.54</strong></td><td></td><td>80.11</td></tr><tr><td>Qwen-2.5-7B</td><td><strong>50.09</strong></td><td>59.59</td><td><strong>52.50</strong></td><td><strong>61.75</strong></td><td></td><td>77.86</td><td>70.22</td><td>59.86</td><td><strong>53.35</strong></td><td><strong>47.18</strong></td><td>61.69</td><td></td><td>79.74</td></tr><tr><td>InternLM-2-7B</td><td><strong>47.50</strong></td><td><strong>57.67</strong></td><td><strong>50.83</strong></td><td><strong>61.15</strong></td><td></td><td><strong>74.73</strong></td><td><strong>66.16</strong></td><td><strong>61.94</strong></td><td><strong>54.10</strong></td><td><strong>46.28</strong></td><td><strong>60.64</strong></td><td></td><td><strong>63.03</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of various 7B-scale language models fine-tuned with instructions generated by smaller (8B) and larger (70B) Llama-3.1 models in the AutoIF instruction generation framework. The table uses several benchmark datasets, including IFEval, FollowBench, C-Eval, MMLU, HumanEval, and GSM8K, to assess instruction following capabilities, common abilities, and reasoning skills. The results aim to demonstrate whether smaller language models are more effective at evolving good instructions for fine-tuning.</p><details><summary>read the caption</summary>Table 3: Comparison of performance with Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models under AutoIF scenario.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Instruction Following (IFEval)</th><th></th><th>Math Reasoning</th><th></th><th>Code Generation</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>Pr.(S)</td><td>In.(S)</td><td>Pr.(L)</td><td>In.(L)</td><td></td><td>GSM8K</td><td>MATH</td><td></td><td>HumanEval</td></tr><tr><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td></tr><tr><td><em>Supervised Model: Llama-3.1-70B-Instruct</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Llama-3.2-3B</td><td>36.60</td><td>48.68</td><td>39.00</td><td>51.08</td><td></td><td>53.60</td><td>7.56</td><td></td><td>35.37</td></tr><tr><td>Llama-3-8B</td><td>35.86</td><td>47.60</td><td>38.63</td><td>50.24</td><td></td><td>63.91</td><td>9.18</td><td></td><td>38.41</td></tr><tr><td>Llama-3.1-8B</td><td>36.97</td><td>47.60</td><td>40.30</td><td>51.08</td><td></td><td>66.11</td><td>11.68</td><td></td><td>40.85</td></tr><tr><td><em>Supervised Model: Llama-3.1-8B-Instruct</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Llama-3.2-3B</td><td><strong>45.47</strong></td><td><strong>57.43</strong></td><td><strong>50.28</strong></td><td><strong>61.27</strong></td><td></td><td><strong>56.48</strong></td><td><strong>8.42</strong></td><td></td><td><strong>38.41</strong></td></tr><tr><td>Llama-3-8B</td><td><strong>37.34</strong></td><td><strong>49.64</strong></td><td><strong>39.74</strong></td><td><strong>51.56</strong></td><td></td><td><strong>67.40</strong></td><td><strong>12.26</strong></td><td></td><td><strong>43.90</strong></td></tr><tr><td>Llama-3.1-8B</td><td><strong>38.08</strong></td><td><strong>49.76</strong></td><td><strong>40.48</strong></td><td><strong>52.40</strong></td><td></td><td><strong>69.52</strong></td><td><strong>15.62</strong></td><td></td><td><strong>51.22</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of the performance of different Llama models in an Auto Evol-Instruct scenario, where existing instructions are automatically refined and evolved to create more complex variants. Two different supervised models are used: Llama-3.1-8B-Instruct (a smaller language model) and Llama-3.1-70B-Instruct (a larger language model). The table reports performance on instruction following (IFEval), mathematical reasoning (GSM8K and MATH), and code generation (HumanEval and MBPP). The goal is to understand how the size of the supervised model impacts the effectiveness of the evolved instructions.</p><details><summary>read the caption</summary>Table 4: Comparison of performance with Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models under Auto Evol-Instruct scenario.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Metrics</th><th>IFEval</th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>Pr.(S)</td><td>In.(S)</td><td>Pr.(L)</td><td>In.(L)</td></tr><tr><td>Original</td><td>33.09</td><td>44.72</td><td>36.41</td><td>48.32</td></tr><tr><td>Instruction Len.</td><td>29.94</td><td>39.69</td><td>33.83</td><td>43.53</td></tr><tr><td>Instruction PPL</td><td>27.91</td><td>39.69</td><td>32.35</td><td>44.36</td></tr><tr><td>IFD</td><td>30.87</td><td>43.53</td><td>36.04</td><td>47.60</td></tr><tr><td>IC-IFD</td><td><strong>34.01</strong></td><td><strong>46.16</strong></td><td><strong>38.82</strong></td><td><strong>50.72</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of different filtering metrics (Instruction Length, Instruction Perplexity, IFD, and IC-IFD) on a subset (25%) of the Alpaca dataset&rsquo;s third iteration evolved by smaller language models (SLMs). The evaluation is conducted on the Llama-3-8B model and uses IFEval metrics (Pr.(S), In.(S), Pr.(L), In.(L)) to assess instruction-following capabilities. This analysis is part of an investigation into whether proposed metrics like IC-IFD can mitigate performance degradation observed when using instruction data evolved by SLMs.</p><details><summary>read the caption</summary>Table 5: Comparison of different metrics under 25% of Alpaca-iter3 evolved by SLMs on Llama-3-8B.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Hyperparameter</th><th>Value</th></tr></thead><tbody><tr><td>Learning Rate</td><td>2√ó10‚Åª‚Åµ</td></tr><tr><td>Number of Epochs</td><td>3</td></tr><tr><td>Number of Devices</td><td>8</td></tr><tr><td>Per-device Batch Size</td><td>1</td></tr><tr><td>Gradient Accumulation Steps</td><td>8</td></tr><tr><td>Learning Rate Scheduler</td><td>cosine</td></tr><tr><td>Warmup Ratio</td><td>0.03</td></tr><tr><td>Max Sequence Length</td><td>2048</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the hyperparameters used for the instruction evolution across three scenarios: Evol-Instruct, AutoIF, and Auto Evol-Instruct. It lists general hyperparameters such as the number of epochs, devices, batch size, learning rate scheduler, warmup ratio, and max sequence length. Additionally, it includes LoRA hyperparameters: rank, alpha, target modules, and dropout. The table distinguishes between the hyperparameters used for models Qwen-2.5-0.5B & 1.5B, Qwen-2.5-3B & 7B, and Qwen-2.5-14B, 32B, & 72B.</p><details><summary>read the caption</summary>Table 6: Hyperparameters utilized in Evol-Instruct, AutoIF and Auto Evol-Instruct scenarios.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Hyperparameter</th><th>Value</th></tr></thead><tbody><tr><td><strong>General Hyperparameters</strong></td><td></td></tr><tr><td>Number of Epochs</td><td>2</td></tr><tr><td>Number of Devices</td><td>8</td></tr><tr><td>Per-device Batch Size</td><td>1</td></tr><tr><td>Gradient Accumulation Steps</td><td>8</td></tr><tr><td>Learning Rate Scheduler</td><td>cosine</td></tr><tr><td>Warmup Ratio</td><td>0.03</td></tr><tr><td>Max Sequence Length</td><td>2048</td></tr><tr><td><strong>LoRA Hyperparameters</strong></td><td></td></tr><tr><td>LoRA Rank</td><td>8</td></tr><tr><td>LoRA Alpha</td><td>8</td></tr><tr><td>LoRA Target</td><td>all module</td></tr><tr><td>LoRA Dropout</td><td>0.0</td></tr><tr><td><strong>Qwen-2.5-0.5B and 1.5B</strong></td><td></td></tr><tr><td>Learning Rate</td><td>1e-5</td></tr><tr><td><strong>Qwen-2.5-3B and 7B</strong></td><td></td></tr><tr><td>Learning Rate</td><td>7e-6</td></tr><tr><td><strong>Qwen-2.5-14B, 32B and 72B</strong></td><td></td></tr><tr><td>Learning Rate</td><td>5e-5</td></tr></tbody></table></table></figure><blockquote><p>üîº This table details the fine-tuning hyperparameters used for different sizes of models within the Qwen-2.5 series. It shows distinct learning rates for models ranging from 0.5B to 72B parameters. Specifically, it presents learning rates for (1) Qwen-2.5-0.5B and 1.5B, (2) Qwen-2.5-3B and 7B, (3) Qwen-2.5-14B, 32B and 72B.</p><details><summary>read the caption</summary>Table 7: Hyperparameters utilized for fine-tuning Qwen-2.5 series models.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>Seed Data</th><th>Seed Data</th></tr></thead><tbody><tr><td></td><td><strong>Dataset</strong></td><td><strong>Datasize</strong></td></tr><tr><td>Instruction Following</td><td>Alpaca</td><td>51,983</td></tr><tr><td>Mathematical Reasoning</td><td>GSM8K Train</td><td>7,473</td></tr><tr><td>Code Generation</td><td>Code Alpaca</td><td>20,022</td></tr></tbody></table></table></figure><blockquote><p>üîº This table provides statistics on the size of the seed instruction datasets used for instruction following, mathematical reasoning, and code generation in the Evol-Instruct and Auto Evol-Instruct experimental scenarios. It lists the dataset name (Alpaca, GSM8K Train, Code Alpaca) along with the corresponding number of data points in each dataset.</p><details><summary>read the caption</summary>Table 8: Statistics of seed instruction data used in the Evol-Instruct and Auto-Evol-Instruct scenarios.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Instruction Following (IFEval)</th><th></th><th></th><th></th><th>Math Reasoning</th><th></th><th>Code Generation</th><th></th></tr></thead><tbody><tr><td></td><td>Pr.(S)</td><td>In.(S)</td><td>Pr.(L)</td><td>In.(L)</td><td></td><td>GSM8K</td><td>MATH</td><td></td></tr><tr><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td></tr><tr><td><strong>Seed instruction data</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mistral-7B-v0.3</td><td>17.01</td><td>26.86</td><td>19.04</td><td>29.14</td><td></td><td>27.07</td><td>0.12</td><td></td></tr><tr><td>DeepSeek-7B</td><td>22.00</td><td>34.05</td><td>23.48</td><td>35.73</td><td></td><td>44.05</td><td>0.56</td><td></td></tr><tr><td>Llama-3.2-3B</td><td>22.55</td><td>34.17</td><td>25.88</td><td>37.65</td><td></td><td>46.40</td><td>0.56</td><td></td></tr><tr><td>Llama-3-8B</td><td>23.11</td><td>32.97</td><td>24.77</td><td>35.13</td><td></td><td>53.68</td><td>0.22</td><td></td></tr><tr><td>Llama-3.1-8B</td><td>27.54</td><td>38.13</td><td>28.65</td><td>39.21</td><td></td><td>56.41</td><td>7.56</td><td></td></tr><tr><td>InternLM-2-7B</td><td>32.72</td><td>45.08</td><td>35.30</td><td>48.08</td><td></td><td>61.87</td><td>10.28</td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº This table (Table 9) presents the evaluation results of various base models on the original seed instruction datasets (without any instruction evolution) used in the instruction evolution experiments. The base models include Mistral-7B-v0.3, DeepSeek-7B, Llama-3.2-3B, Llama-3-8B, Llama-3.1-8B, and InternLM-2-7B. The evaluation is conducted on three different types of downstream tasks: instruction following, mathematical reasoning, and code generation. Instruction following is evaluated using IFEval (Pr.(S) and In.(S) represent the strict and loose accuracy on the prompt level, while Pr.(L) and In.(L) represent the strict and loose accuracy on the instruction level), mathematical reasoning using GSM8K and MATH (accuracy scores are reported), and code generation using HumanEval and MBPP (pass@1 metrics are reported). The purpose of this table is to establish a baseline performance before instruction evolution and to demonstrate that even powerful base models perform suboptimally on the initial seed data, highlighting the need for improved instruction datasets.</p><details><summary>read the caption</summary>Table 9: Results of seed instruction data.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Instruction Following (IFEval)</th><th></th><th>Math Reasoning</th><th></th><th>Code Generation</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>Pr.(S)</td><td>In.(S)</td><td>Pr.(L)</td><td>In.(L)</td><td></td><td>GSM8K</td><td>MATH</td><td></td><td>HumanEval</td></tr><tr><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td></tr><tr><td><em>Supervised Model: Llama-3.1-70B-Instruct</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Iteration 1</td><td>33.83</td><td>46.28</td><td>36.41</td><td>49.28</td><td></td><td>63.00</td><td>7.62</td><td></td><td>43.90</td></tr><tr><td>Iteration 2</td><td>32.53</td><td>43.76</td><td>34.20</td><td>46.16</td><td></td><td>64.59</td><td>10.04</td><td></td><td>42.07</td></tr><tr><td>Iteration 3</td><td>35.12</td><td>47.36</td><td>36.97</td><td>49.28</td><td></td><td>64.75</td><td>11.82</td><td></td><td>43.29</td></tr><tr><td><em>Supervised Model: Llama-3.1-8B-Instruct</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Iteration 1</td><td>35.49</td><td>47.00</td><td>39.56</td><td>50.72</td><td></td><td>63.38</td><td>11.44</td><td></td><td>48.17</td></tr><tr><td>Iteration 2</td><td>36.78</td><td>48.20</td><td>40.30</td><td>50.84</td><td></td><td>64.82</td><td>11.48</td><td></td><td>48.78</td></tr><tr><td>Iteration 3</td><td>33.09</td><td>44.72</td><td>36.41</td><td>48.32</td><td></td><td>65.88</td><td>14.12</td><td></td><td>44.51</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a detailed breakdown of the performance of a Llama-3-8B model after undergoing three iterations of instruction evolution using two different supervised models: Llama-3.1-70B-Instruct and Llama-3.1-8B-Instruct. The evaluation metrics used are IFEval (instruction following), GSM8K and MATH (mathematical reasoning), and HumanEval and MBPP (code generation). This table complements Figure 1, which visually represents the same data.</p><details><summary>read the caption</summary>Table 10: Detailed performance of different evolved iterations on Llama-3-8B refer to Figure¬†1.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Instruction Following (IFEval)</th><th></th><th></th><th>Math Reasoning</th><th></th><th>Code Generation</th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>Pr.(S)</td><td>In.(S)</td><td>Pr.(L)</td><td>In.(L)</td><td></td><td>GSM8K</td><td>MATH</td><td></td><td>HumanEval</td></tr><tr><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td></tr><tr><td><em>Supervised Model: Llama-3.1-70B-Instruct</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Qwen-2.5-0.5B</td><td>18.48</td><td>32.73</td><td>22.00</td><td>35.85</td><td></td><td>40.26</td><td>16.32</td><td></td><td>30.49</td></tr><tr><td>Qwen-2.5-1.5B</td><td>28.84</td><td>42.67</td><td>31.98</td><td>46.04</td><td></td><td>62.32</td><td>24.06</td><td></td><td>50.00</td></tr><tr><td>Qwen-2.5-3B</td><td>37.89</td><td>48.56</td><td>42.70</td><td>53.60</td><td></td><td>76.12</td><td>26.44</td><td></td><td>63.41</td></tr><tr><td>Qwen-2.5-7B</td><td>46.21</td><td>56.83</td><td>50.64</td><td>60.79</td><td></td><td>76.12</td><td>38.14</td><td></td><td>70.73</td></tr><tr><td>Qwen-2.5-14B (LoRA)</td><td>40.11</td><td>54.43</td><td>48.24</td><td>61.99</td><td></td><td>87.79</td><td>49.94</td><td></td><td>75.00</td></tr><tr><td>Qwen-2.5-32B (LoRA)</td><td>42.88</td><td>57.31</td><td>51.20</td><td>64.15</td><td></td><td>87.79</td><td>55.02</td><td></td><td>80.49</td></tr><tr><td>Qwen-2.5-72B (LoRA)</td><td>50.63</td><td>68.43</td><td>57.12</td><td>70.98</td><td></td><td>91.05</td><td>58.83</td><td></td><td>82.93</td></tr><tr><td><em>Supervised Model: Llama-3.1-8B-Instruct</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Qwen-2.5-0.5B</td><td>17.38</td><td>29.38</td><td>19.78</td><td>32.01</td><td></td><td>40.71</td><td>16.26</td><td></td><td>34.76</td></tr><tr><td>Qwen-2.5-1.5B</td><td>28.47</td><td>41.73</td><td>31.98</td><td>44.96</td><td></td><td>65.35</td><td>27.84</td><td></td><td>52.44</td></tr><tr><td>Qwen-2.5-3B</td><td>38.82</td><td>49.76</td><td>42.51</td><td>53.96</td><td></td><td>76.57</td><td>30.92</td><td></td><td>64.02</td></tr><tr><td>Qwen-2.5-7B</td><td>47.32</td><td>58.39</td><td>51.39</td><td>62.35</td><td></td><td>82.03</td><td>43.78</td><td></td><td>71.95</td></tr><tr><td>Qwen-2.5-14B (LoRA)</td><td>42.51</td><td>55.16</td><td>51.02</td><td>62.47</td><td></td><td>88.17</td><td>52.22</td><td></td><td>75.61</td></tr><tr><td>Qwen-2.5-32B (LoRA)</td><td>45.84</td><td>58.75</td><td>54.71</td><td>66.31</td><td></td><td>89.61</td><td>55.28</td><td></td><td>81.71</td></tr><tr><td>Qwen-2.5-72B (LoRA)</td><td>52.79</td><td>72.56</td><td>61.25</td><td>73.27</td><td></td><td>91.36</td><td>60.75</td><td></td><td>84.67</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comprehensive comparison of the performance achieved by various Qwen-2.5 series models, ranging in size from 0.5B to 72B parameters, across different evaluation benchmarks. The models were fine-tuned using instruction data generated by both smaller (SLM) and larger (LLM) language models, and their performance was evaluated using instruction following, mathematical reasoning, and code generation tasks. The goal is to assess the impact of model size and instruction data source on the effectiveness of instruction following capabilities.</p><details><summary>read the caption</summary>Table 11: Detailed performance among Qwen-2.5 series models refer to Figure¬†3.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Temperature</th><th>HumanEval</th><th>MBPP</th><th>HumanEval</th><th>MBPP</th></tr></thead><tbody><tr><td></td><td><em>Supervised Model: Llama-3.1-70B-Instruct</em></td><td></td><td><em>Supervised Model: Llama-3.1-8B-Instruct</em></td><td></td></tr><tr><td>greedy</td><td>37.20</td><td>33.40</td><td><strong>39.63</strong></td><td><strong>36.40</strong></td></tr><tr><td>0.1</td><td>36.59</td><td>36.40</td><td><strong>37.80</strong></td><td><strong>37.60</strong></td></tr><tr><td>0.3</td><td>38.41</td><td>35.20</td><td><strong>39.63</strong></td><td><strong>37.80</strong></td></tr><tr><td>0.5</td><td>35.98</td><td>33.40</td><td><strong>37.80</strong></td><td><strong>35.80</strong></td></tr><tr><td>0.7</td><td>35.98</td><td><strong>36.00</strong></td><td><strong>39.02</strong></td><td>32.80</td></tr><tr><td>0.9</td><td>34.76</td><td>33.00</td><td><strong>40.24</strong></td><td><strong>35.80</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the performance of a smaller language model (Llama-3.2-3B) on code generation tasks across different decoding temperatures (greedy decoding, 0.1, 0.3, 0.5, 0.7, and 0.9) during instruction evolution, using two different larger language models (Llama-3.1-70B-Instruct and Llama-3.1-8B-Instruct) as supervised models. The metrics used are HumanEval and MBPP, and the results are compared under each scenario.</p><details><summary>read the caption</summary>Table 12: Performance among different temperatures on Llama-3.2-3B under code generation scenario.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:center>Alpaca</th><th style=text-align:center>GSM8K Train</th><th style=text-align:center>Code Alpaca</th></tr></thead><tbody><tr><td style=text-align:left>Seed Instruction</td><td style=text-align:center>27.63</td><td style=text-align:center>34.05</td><td style=text-align:center>26.01</td></tr><tr><td style=text-align:left>LLM-Inst Iter1</td><td style=text-align:center>52.89</td><td style=text-align:center>39.88</td><td style=text-align:center>46.75</td></tr><tr><td style=text-align:left>SLM-Inst Iter1</td><td style=text-align:center><strong>66.35</strong></td><td style=text-align:center><strong>48.85</strong></td><td style=text-align:center><strong>58.86</strong></td></tr><tr><td style=text-align:left>LLM-Inst Iter2</td><td style=text-align:center>68.16</td><td style=text-align:center>47.14</td><td style=text-align:center>65.02</td></tr><tr><td style=text-align:left>SLM-Inst Iter2</td><td style=text-align:center><strong>77.62</strong></td><td style=text-align:center><strong>63.48</strong></td><td style=text-align:center><strong>73.37</strong></td></tr><tr><td style=text-align:left>LLM-Inst Iter3</td><td style=text-align:center>75.73</td><td style=text-align:center>54.00</td><td style=text-align:center>72.85</td></tr><tr><td style=text-align:left>SLM-Inst Iter3</td><td style=text-align:center><strong>82.44</strong></td><td style=text-align:center><strong>72.12</strong></td><td style=text-align:center><strong>79.19</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the difficulty scores of instructions generated by smaller and larger language models (Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct) during three iterations of instruction evolution using the Evol-Instruct method. The scores are calculated for three datasets: Alpaca, GSM8K Train, and Code Alpaca. The difficulty levels help analyze the complexity of generated instructions and their impact on model performance in downstream tasks.</p><details><summary>read the caption</summary>Table 13: Scores of difficulty levels for instructions evolved during three iterations, using Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models for each round under Evol-Instruct scenario.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left>Iteration</th><th style=text-align:center>Average Reward</th><th style=text-align:center>Average Reward</th><th style=text-align:center>Average Reward</th></tr></thead><tbody><tr><td style=text-align:left></td><td style=text-align:center>Alpaca</td><td style=text-align:center>GSM8K</td><td style=text-align:center>Code Alpaca</td></tr><tr><td style=text-align:left>&mdash;</td><td style=text-align:center>&mdash;</td><td style=text-align:center>&mdash;</td><td style=text-align:center>&mdash;</td></tr><tr><td style=text-align:left><em>Supervised Model: Llama-3.1-70B-Instruct</em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Iteration 1</td><td style=text-align:center>1.54</td><td style=text-align:center>0.74</td><td style=text-align:center>1.10</td></tr><tr><td style=text-align:left>Iteration 2</td><td style=text-align:center><strong>1.68</strong></td><td style=text-align:center>0.73</td><td style=text-align:center><strong>1.19</strong></td></tr><tr><td style=text-align:left>Iteration 3</td><td style=text-align:center><strong>1.56</strong></td><td style=text-align:center>0.69</td><td style=text-align:center><strong>1.14</strong></td></tr><tr><td style=text-align:left><em>Supervised Model: Llama-3.1-8B-Instruct</em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Iteration 1</td><td style=text-align:center><strong>1.59</strong></td><td style=text-align:center><strong>1.01</strong></td><td style=text-align:center><strong>1.23</strong></td></tr><tr><td style=text-align:left>Iteration 2</td><td style=text-align:center>1.54</td><td style=text-align:center><strong>0.79</strong></td><td style=text-align:center>0.96</td></tr><tr><td style=text-align:left>Iteration 3</td><td style=text-align:center>1.42</td><td style=text-align:center><strong>0.97</strong></td><td style=text-align:center>1.03</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of the average reward scores assigned by the InternLM-2-7B-Reward model to instructions generated by SLMs (Smaller Language Models) and LLMs (Larger Language Models) across three iterations of the Evol-Instruct process, using three different instruction datasets: Alpaca, GSM8K, and Code Alpaca. The reward model evaluates the quality of the generated instructions based on certain criteria, and the average reward score reflects the overall quality of the instruction set produced by each model type in each iteration. This comparison helps to understand if one model type consistently generates higher-quality instructions than the other across different iterations and datasets.</p><details><summary>read the caption</summary>Table 14: Comparison of average rewards among different iteration evolution instruction data.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Datasets</th><th>IFD (%)</th><th>IC-IFD (%)</th><th>Performance</th></tr></thead><tbody><tr><td>SLMs (Alpaca iter 3)</td><td><strong>83.04</strong></td><td>35.89</td><td>40.64</td></tr><tr><td>LLMs (Alpaca iter 3)</td><td>82.03</td><td><strong>37.05</strong></td><td><strong>42.18</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the Instruction Following Difficulty (IFD) and Instruction Complex-Aware IFD (IC-IFD) scores on the third round of Alpaca datasets evolved by smaller language models (SLMs) and larger language models (LLMs). It also includes the performance of these datasets on IFEval benchmark using Llama-3-8B model.</p><details><summary>read the caption</summary>Table 15: Comparison of IFD and IC-IFD on third-round evolved Alpaca datasets from SLMs and LLMs.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-ad49491c199101a7dcf445c39768bac6 class=gallery><img src=https://ai-paper-reviewer.com/2412.11231/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.11231/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/&amp;title=Smaller%20Language%20Models%20Are%20Better%20Instruction%20Evolvers" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/&amp;text=Smaller%20Language%20Models%20Are%20Better%20Instruction%20Evolvers" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/&amp;subject=Smaller%20Language%20Models%20Are%20Better%20Instruction%20Evolvers" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.11231/index.md",oid_likes="likes_paper-reviews/2412.11231/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.10360/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Apollo: An Exploration of Video Understanding in Large Multimodal Models</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-13T00:00:00+00:00>13 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.11258/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-15T00:00:00+00:00>15 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>