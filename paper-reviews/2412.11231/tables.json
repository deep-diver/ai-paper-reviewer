[{"content": "| Model | Instruction Following (IFEval) | | | Math Reasoning | | Code Generation | |\n|---|---|---|---|---|---|---|---|---|---|\n| | Pr.(S) | In.(S) | Pr.(L) | In.(L) | | GSM8K | MATH | | HumanEval | MBPP |\n|---|---|---|---|---|---|---|---|---|---|---|\n| *Supervised Model: Llama-3.1-70B-Instruct* | | | | | | | | | | |\n| Mistral-7B-v0.3 | 19.59 | 31.77 | 22.74 | 34.65 | | 33.89 | **3.16** | | 24.39 | 6.00 |\n| DeepSeek-7B | 36.23 | **48.20** | 41.04 | 52.52 | | **48.07** | 2.96 | | 28.66 | 33.00 |\n| Llama-3.2-3B | 40.11 | 50.84 | 43.81 | 54.43 | | 53.75 | 6.60 | | 35.98 | **36.00** |\n| Llama-3-8B | 33.83 | 46.28 | 36.41 | 49.28 | | 63.00 | 7.62 | | 43.90 | 36.20 |\n| Llama-3.1-8B | 34.57 | 46.04 | 38.81 | 50.48 | | 64.22 | 11.32 | | **51.22** | 40.60 |\n| InternLM-2-7B | 40.85 | 53.48 | 44.54 | 56.95 | | **68.31** | 19.50 | | 56.10 | 40.40 |\n| *Supervised Model: Llama-3.1-8B-Instruct* | | | | | | | | | | |\n| Mistral-7B-v0.3 | **24.40** | **35.01** | **26.25** | **37.53** | | **40.18** | 2.84 | | **29.27** | **19.60** |\n| DeepSeek-7B | **36.60** | 48.08 | **41.77** | **53.12** | | 47.92 | **3.56** | | **34.76** | **33.80** |\n| Llama-3.2-3B | **41.59** | **53.48** | **45.66** | **57.07** | | **55.12** | **7.32** | | **39.02** | 32.80 |\n| Llama-3-8B | **35.49** | **47.00** | **39.56** | **50.72** | | **63.38** | **11.44** | | **48.17** | **37.60** |\n| Llama-3.1-8B | **38.45** | **50.96** | **43.81** | **55.28** | | **67.10** | **13.12** | | 48.78 | **41.60** |\n| InternLM-2-7B | **43.07** | **54.80** | **47.32** | **58.39** | | 68.08 | **20.32** | | **57.93** | **40.80** |", "caption": "Table 1: Comparison of performance with Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models under Evol-Instruct scenario.", "description": "This table compares the performance of various 7B-scale language models (Mistral, DeepSeek, Llama, InternLM) after being fine-tuned on instruction data generated by either a smaller Llama-3.1-8B-Instruct model or a larger Llama-3.1-70B-Instruct model, within the Evol-Instruct scenario.  The evaluation metrics span Instruction Following (IFEval with both prompt and instruction level scores), Math Reasoning (GSM8K and MATH), and Code Generation (HumanEval and MBPP). The table aims to demonstrate whether instructions evolved by smaller or larger language models lead to better performance in downstream tasks after fine-tuning.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"content": "| Model | Instruction Following (IFEval) | | | Math Reasoning | | Code Generation | |\n|---|---|---|---|---|---|---|---|---|---|---| \n| | Pr.(S) | In.(S) | Pr.(L) | In.(L) | | GSM8K | MATH | | HumanEval | MBPP |\n|---|---|---|---|---|---|---|---|---|---|---| \n| **Supervised Model: Qwen-2-72B-Instruct** | | | | | | | | | | |\n| Mistral-7B-v0.3 | 20.15 | 30.94 | 23.84 | 34.41 | | 46.93 | **3.26** | | 32.32 | 1.80 |\n| DeepSeek-7B | 35.67 | 47.12 | **39.56** | 50.84 | | 44.81 | 2.76 | | **36.59** | **34.00** |\n| Llama-3.2-3B | 39.74 | 51.44 | 43.99 | 55.40 | | 53.83 | **7.40** | | 38.41 | 31.00 |\n| Llama-3-8B | 34.75 | 45.80 | 37.71 | 48.92 | | 63.76 | **10.06** | | 43.90 | 35.40 |\n| Llama-3.1-8B | **36.41** | **47.60** | 39.00 | 50.60 | | 65.43 | 10.84 | | **48.17** | 38.40 |\n| InternLM-2-7B | 41.96 | 53.60 | 43.99 | 55.64 | | 65.28 | 17.96 | | 56.71 | 40.60 |\n| **Supervised Model: Qwen-2-7B-Instruct** | | | | | | | | | | |\n| Mistral-7B-v0.3 | **25.32** | **37.17** | **29.76** | **41.01** | | **47.31** | 2.20 | | **32.93** | **12.00** |\n| DeepSeek-7B | **36.41** | **48.56** | 39.37 | **51.32** | | **48.07** | **3.82** | | 35.37 | 33.20 |\n| Llama-3.2-3B | **43.81** | **55.16** | **47.87** | **58.27** | | **56.56** | 7.18 | | **39.63** | **31.40** |\n| Llama-3-8B | **38.92** | **48.33** | **43.81** | **52.19** | | **63.91** | 8.66 | | **45.73** | **38.40** |\n| Llama-3.1-8B | 34.75 | 45.80 | **39.93** | **51.08** | | **68.76** | **14.02** | | 46.34 | **38.60** |\n| InternLM-2-7B | **44.12** | **55.16** | **48.62** | **58.73** | | **66.87** | **19.60** | | **58.54** | **41.40** |", "caption": "Table 2: Comparison of performance with Qwen-2-7B-Instruct and Qwen-2-72B-Instruct as supervised models under Evol-Instruct scenario.", "description": "This table compares the performance of various 7B and 70B parameter models from the Mistral, DeepSeek, LLaMA, and InternLM families on instruction following, mathematical reasoning, and code generation tasks under the Evol-Instruct method.  Two different supervised models are used: Qwen-2-7B-Instruct and Qwen-2-72B-Instruct. The metrics reported are Pr.(S), In.(S), Pr.(L), In.(L) for IFEval (instruction following); GSM8K and MATH for mathematical reasoning; and HumanEval and MBPP for code generation.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"content": "| Model | IFEval | | | FollowBench (HSR) | | Common Abilities | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | Pr.(S) | In.(S) | Pr.(L) | In.(L) | | Level 1 | Level 2 | Level 3 | Level 4 | Level 5 | Avg. | | C-Eval | MMLU | HumanEval | GSM8K |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Supervision Model: Llama-3.1-70B-Instruct** | | | | | | | | | | | | | | | |\n| Llama-3.2-3B | 40.85 | 51.92 | 42.33 | 53.84 | | **61.17** | 57.59 | **50.55** | 33.09 | 26.74 | 45.83 | | **41.37** | **52.65** | **29.88** | 27.07 |\n| Llama-3-8B | 37.71 | 50.00 | 39.19 | 52.04 | | 49.64 | 46.60 | 41.56 | 27.05 | 22.37 | 37.44 | | 41.87 | 51.14 | 26.83 | 37.76 |\n| Llama-3.1-8B | 41.96 | 53.36 | 42.70 | 54.20 | | 51.77 | 45.60 | 45.04 | 34.85 | 26.61 | 40.78 | | **44.50** | 56.39 | 31.10 | 38.21 |\n| Qwen-2-7B | 41.96 | 53.60 | 43.62 | 55.64 | | 72.18 | 62.45 | **56.43** | 41.31 | 35.42 | 53.56 | | **81.08** | 55.71 | 57.32 | **79.68** |\n| Qwen-2.5-7B | 49.17 | **60.31** | 50.46 | 61.51 | | **78.88** | **73.78** | **61.50** | 51.99 | 45.42 | **62.31** | | **80.46** | 58.39 | 67.68 | **85.90** |\n| InternLM-2-7B | 46.21 | 56.71 | 48.06 | 58.63 | | 68.89 | 62.23 | 54.17 | 44.27 | 42.06 | 54.33 | | 60.11 | 60.59 | 65.35 | 50.00 |\n| **Supervision Model: Llama-3.1-8B-Instruct** | | | | | | | | | | | | | | | |\n| Llama-3.2-3B | **43.62** | **54.20** | **46.95** | **57.07** | | 56.95 | **61.46** | 50.20 | **37.65** | **34.16** | **48.08** | | 40.56 | 49.08 | 25.00 | **29.87** |\n| Llama-3-8B | **41.04** | **51.32** | **42.88** | **53.11** | | **62.99** | **54.38** | **49.29** | **32.21** | **32.21** | **46.21** | | **43.49** | **55.63** | **37.20** | **45.26** |\n| Llama-3.1-8B | **42.51** | **54.92** | **44.73** | **56.71** | | **63.99** | **58.15** | **53.29** | **39.49** | **36.02** | **50.19** | | 43.77 | **58.32** | **32.32** | **47.92** |\n| Qwen-2-7B | **44.92** | **55.76** | **47.50** | **58.39** | | **78.75** | **63.30** | 52.31 | **50.28** | **43.08** | **57.54** | | 80.11 | **56.84** | **65.24** | 79.53 |\n| Qwen-2.5-7B | **50.09** | 59.59 | **52.50** | **61.75** | | 77.86 | 70.22 | 59.86 | **53.35** | **47.18** | 61.69 | | 79.74 | **60.17** | **72.56** | 84.69 |\n| InternLM-2-7B | **47.50** | **57.67** | **50.83** | **61.15** | | **74.73** | **66.16** | **61.94** | **54.10** | **46.28** | **60.64** | | **63.03** | **63.16** | **70.96** | **54.27** |", "caption": "Table 3: Comparison of performance with Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models under AutoIF scenario.", "description": "This table compares the performance of various 7B-scale language models fine-tuned with instructions generated by smaller (8B) and larger (70B) Llama-3.1 models in the AutoIF instruction generation framework.  The table uses several benchmark datasets, including IFEval, FollowBench, C-Eval, MMLU, HumanEval, and GSM8K, to assess instruction following capabilities, common abilities, and reasoning skills.  The results aim to demonstrate whether smaller language models are more effective at evolving good instructions for fine-tuning.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"content": "| Model | Instruction Following (IFEval) |   | Math Reasoning |   | Code Generation | |\n|---|---|---|---|---|---|---|---|---|---| \n| | Pr.(S) | In.(S) | Pr.(L) | In.(L) | | GSM8K | MATH | | HumanEval | MBPP |\n|---|---|---|---|---|---|---|---|---|---| \n| _Supervised Model: Llama-3.1-70B-Instruct_ | | | | | | | | | | |\n| Llama-3.2-3B | 36.60 | 48.68 | 39.00 | 51.08 | | 53.60 | 7.56 | | 35.37 | 33.00 |\n| Llama-3-8B | 35.86 | 47.60 | 38.63 | 50.24 | | 63.91 | 9.18 | | 38.41 | 32.40 |\n| Llama-3.1-8B | 36.97 | 47.60 | 40.30 | 51.08 | | 66.11 | 11.68 | | 40.85 | **40.40** |\n| _Supervised Model: Llama-3.1-8B-Instruct_ | | | | | | | | | | |\n| Llama-3.2-3B | **45.47** | **57.43** | **50.28** | **61.27** | | **56.48** | **8.42** | | **38.41** | **34.40** |\n| Llama-3-8B | **37.34** | **49.64** | **39.74** | **51.56** | | **67.40** | **12.26** | | **43.90** | **34.80** |\n| Llama-3.1-8B | **38.08** | **49.76** | **40.48** | **52.40** | | **69.52** | **15.62** | | **51.22** | 38.80 |", "caption": "Table 4: Comparison of performance with Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models under Auto Evol-Instruct scenario.", "description": "This table presents a comparison of the performance of different Llama models in an Auto Evol-Instruct scenario, where existing instructions are automatically refined and evolved to create more complex variants. Two different supervised models are used: Llama-3.1-8B-Instruct (a smaller language model) and Llama-3.1-70B-Instruct (a larger language model). The table reports performance on instruction following (IFEval), mathematical reasoning (GSM8K and MATH), and code generation (HumanEval and MBPP).  The goal is to understand how the size of the supervised model impacts the effectiveness of the evolved instructions.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"content": "| Metrics | IFEval | | | |\n|---|---|---|---|---| \n| | Pr.(S) | In.(S) | Pr.(L) | In.(L) |\n| Original | 33.09 | 44.72 | 36.41 | 48.32 |\n| Instruction Len. | 29.94 | 39.69 | 33.83 | 43.53 |\n| Instruction PPL | 27.91 | 39.69 | 32.35 | 44.36 |\n| IFD | 30.87 | 43.53 | 36.04 | 47.60 |\n| IC-IFD | **34.01** | **46.16** | **38.82** | **50.72** |", "caption": "Table 5: Comparison of different metrics under 25% of Alpaca-iter3 evolved by SLMs on Llama-3-8B.", "description": "This table compares the performance of different filtering metrics (Instruction Length, Instruction Perplexity, IFD, and IC-IFD) on a subset (25%) of the Alpaca dataset's third iteration evolved by smaller language models (SLMs).  The evaluation is conducted on the Llama-3-8B model and uses IFEval metrics (Pr.(S), In.(S), Pr.(L), In.(L)) to assess instruction-following capabilities.  This analysis is part of an investigation into whether proposed metrics like IC-IFD can mitigate performance degradation observed when using instruction data evolved by SLMs.", "section": "RQ3: How Do We Determine Whether An Instruction is Effective without Instruction Tuning?"}, {"content": "| Hyperparameter | Value |\n|---|---| \n| Learning Rate | 2\u00d710\u207b\u2075 |\n| Number of Epochs | 3 |\n| Number of Devices | 8 |\n| Per-device Batch Size | 1 |\n| Gradient Accumulation Steps | 8 |\n| Learning Rate Scheduler | cosine |\n| Warmup Ratio | 0.03 |\n| Max Sequence Length | 2048 |", "caption": "Table 6: Hyperparameters utilized in Evol-Instruct, AutoIF and Auto Evol-Instruct scenarios.", "description": "This table presents the hyperparameters used for the instruction evolution across three scenarios: Evol-Instruct, AutoIF, and Auto Evol-Instruct.  It lists general hyperparameters such as the number of epochs, devices, batch size, learning rate scheduler, warmup ratio, and max sequence length. Additionally, it includes LoRA hyperparameters: rank, alpha, target modules, and dropout. The table distinguishes between the hyperparameters used for models Qwen-2.5-0.5B & 1.5B, Qwen-2.5-3B & 7B, and Qwen-2.5-14B, 32B, & 72B.", "section": "A.1 Experimental Details"}, {"content": "| Hyperparameter | Value | \n|---|---| \n| **General Hyperparameters** | | \n| Number of Epochs | 2 | \n| Number of Devices | 8 | \n| Per-device Batch Size | 1 | \n| Gradient Accumulation Steps | 8 | \n| Learning Rate Scheduler | cosine | \n| Warmup Ratio | 0.03 | \n| Max Sequence Length | 2048 | \n| **LoRA Hyperparameters** | | \n| LoRA Rank | 8 | \n| LoRA Alpha | 8 | \n| LoRA Target | all module | \n| LoRA Dropout | 0.0 | \n| **Qwen-2.5-0.5B and 1.5B** | | \n| Learning Rate | 1e-5 | \n| **Qwen-2.5-3B and 7B** | | \n| Learning Rate | 7e-6 | \n| **Qwen-2.5-14B, 32B and 72B** | | \n| Learning Rate | 5e-5 |", "caption": "Table 7: Hyperparameters utilized for fine-tuning Qwen-2.5 series models.", "description": "This table details the fine-tuning hyperparameters used for different sizes of models within the Qwen-2.5 series.  It shows distinct learning rates for models ranging from 0.5B to 72B parameters.  Specifically, it presents learning rates for (1) Qwen-2.5-0.5B and 1.5B, (2) Qwen-2.5-3B and 7B, (3) Qwen-2.5-14B, 32B and 72B.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"content": "| | Seed Data | Seed Data |\n|---|---|---| \n| | **Dataset** | **Datasize** |\n| Instruction Following | Alpaca | 51,983 |\n| Mathematical Reasoning | GSM8K Train | 7,473 |\n| Code Generation | Code Alpaca | 20,022 |", "caption": "Table 8: Statistics of seed instruction data used in the Evol-Instruct and Auto-Evol-Instruct scenarios.", "description": "This table provides statistics on the size of the seed instruction datasets used for instruction following, mathematical reasoning, and code generation in the Evol-Instruct and Auto Evol-Instruct experimental scenarios.  It lists the dataset name (Alpaca, GSM8K Train, Code Alpaca) along with the corresponding number of data points in each dataset.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"content": "| Model | Instruction Following (IFEval) | | | | Math Reasoning | | Code Generation | |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| | Pr.(S) | In.(S) | Pr.(L) | In.(L) | | GSM8K | MATH | | HumanEval | MBPP |\n|---|---|---|---|---|---|---|---|---|---|\n| **Seed instruction data** | | | | | | | | | |\n| Mistral-7B-v0.3 | 17.01 | 26.86 | 19.04 | 29.14 | | 27.07 | 0.12 | | 10.20 | 8.80 |\n| DeepSeek-7B | 22.00 | 34.05 | 23.48 | 35.73 | | 44.05 | 0.56 | | 25.61 | 33.80 |\n| Llama-3.2-3B | 22.55 | 34.17 | 25.88 | 37.65 | | 46.40 | 0.56 | | 28.05 | 32.20 |\n| Llama-3-8B | 23.11 | 32.97 | 24.77 | 35.13 | | 53.68 | 0.22 | | 25.00 | 28.60 |\n| Llama-3.1-8B | 27.54 | 38.13 | 28.65 | 39.21 | | 56.41 | 7.56 | | 29.88 | 31.80 |\n| InternLM-2-7B | 32.72 | 45.08 | 35.30 | 48.08 | | 61.87 | 10.28 | | 42.07 | 40.00 |", "caption": "Table 9: Results of seed instruction data.", "description": "This table (Table 9) presents the evaluation results of various base models on the original seed instruction datasets (without any instruction evolution) used in the instruction evolution experiments. The base models include Mistral-7B-v0.3, DeepSeek-7B, Llama-3.2-3B, Llama-3-8B, Llama-3.1-8B, and InternLM-2-7B. The evaluation is conducted on three different types of downstream tasks: instruction following, mathematical reasoning, and code generation.  Instruction following is evaluated using IFEval (Pr.(S) and In.(S) represent the strict and loose accuracy on the prompt level, while Pr.(L) and In.(L) represent the strict and loose accuracy on the instruction level), mathematical reasoning using GSM8K and MATH (accuracy scores are reported), and code generation using HumanEval and MBPP (pass@1 metrics are reported). The purpose of this table is to establish a baseline performance before instruction evolution and to demonstrate that even powerful base models perform suboptimally on the initial seed data, highlighting the need for improved instruction datasets.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"content": "| Model | Instruction Following (IFEval) |  | Math Reasoning |  | Code Generation | |\n|---|---|---|---|---|---|---|---|---|---| \n|  | Pr.(S) | In.(S) | Pr.(L) | In.(L) |  | GSM8K | MATH |  | HumanEval | MBPP |\n|---|---|---|---|---|---|---|---|---|---|---| \n| *Supervised Model: Llama-3.1-70B-Instruct* |  |  |  |  |  |  |  |  |  |  |\n| Iteration 1 | 33.83 | 46.28 | 36.41 | 49.28 |  | 63.00 | 7.62 |  | 43.90 | 36.20 |\n| Iteration 2 | 32.53 | 43.76 | 34.20 | 46.16 |  | 64.59 | 10.04 |  | 42.07 | 36.60 |\n| Iteration 3 | 35.12 | 47.36 | 36.97 | 49.28 |  | 64.75 | 11.82 |  | 43.29 | 37.20 |\n| *Supervised Model: Llama-3.1-8B-Instruct* |  |  |  |  |  |  |  |  |  |  |\n| Iteration 1 | 35.49 | 47.00 | 39.56 | 50.72 |  | 63.38 | 11.44 |  | 48.17 | 37.60 |\n| Iteration 2 | 36.78 | 48.20 | 40.30 | 50.84 |  | 64.82 | 11.48 |  | 48.78 | 39.40 |\n| Iteration 3 | 33.09 | 44.72 | 36.41 | 48.32 |  | 65.88 | 14.12 |  | 44.51 | 40.80 |", "caption": "Table 10: Detailed performance of different evolved iterations on Llama-3-8B refer to Figure\u00a01.", "description": "This table presents a detailed breakdown of the performance of a Llama-3-8B model after undergoing three iterations of instruction evolution using two different supervised models: Llama-3.1-70B-Instruct and Llama-3.1-8B-Instruct.  The evaluation metrics used are IFEval (instruction following), GSM8K and MATH (mathematical reasoning), and HumanEval and MBPP (code generation). This table complements Figure 1, which visually represents the same data.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"content": "| Model | Instruction Following (IFEval) |  |  | Math Reasoning |  | Code Generation | |\n|---|---|---|---|---|---|---|---|---|---| \n| | Pr.(S) | In.(S) | Pr.(L) | In.(L) |  | GSM8K | MATH |  | HumanEval | MBPP |\n|---|---|---|---|---|---|---|---|---|---|---| \n| *Supervised Model: Llama-3.1-70B-Instruct* | | | | | | | | | | |\n| Qwen-2.5-0.5B | 18.48 | 32.73 | 22.00 | 35.85 |  | 40.26 | 16.32 |  | 30.49 | 27.60 |\n| Qwen-2.5-1.5B | 28.84 | 42.67 | 31.98 | 46.04 |  | 62.32 | 24.06 |  | 50.00 | 43.20 |\n| Qwen-2.5-3B | 37.89 | 48.56 | 42.70 | 53.60 |  | 76.12 | 26.44 |  | 63.41 | 55.40 |\n| Qwen-2.5-7B | 46.21 | 56.83 | 50.64 | 60.79 |  | 76.12 | 38.14 |  | 70.73 | 61.60 |\n| Qwen-2.5-14B (LoRA) | 40.11 | 54.43 | 48.24 | 61.99 |  | 87.79 | 49.94 |  | 75.00 | 67.20 |\n| Qwen-2.5-32B (LoRA) | 42.88 | 57.31 | 51.20 | 64.15 |  | 87.79 | 55.02 |  | 80.49 | 71.20 |\n| Qwen-2.5-72B (LoRA) | 50.63 | 68.43 | 57.12 | 70.98 |  | 91.05 | 58.83 |  | 82.93 | 76.00 |\n| *Supervised Model: Llama-3.1-8B-Instruct* | | | | | | | | | | |\n| Qwen-2.5-0.5B | 17.38 | 29.38 | 19.78 | 32.01 |  | 40.71 | 16.26 |  | 34.76 | 28.00 |\n| Qwen-2.5-1.5B | 28.47 | 41.73 | 31.98 | 44.96 |  | 65.35 | 27.84 |  | 52.44 | 49.94 |\n| Qwen-2.5-3B | 38.82 | 49.76 | 42.51 | 53.96 |  | 76.57 | 30.92 |  | 64.02 | 55.80 |\n| Qwen-2.5-7B | 47.32 | 58.39 | 51.39 | 62.35 |  | 82.03 | 43.78 |  | 71.95 | 61.80 |\n| Qwen-2.5-14B (LoRA) | 42.51 | 55.16 | 51.02 | 62.47 |  | 88.17 | 52.22 |  | 75.61 | 67.20 |\n| Qwen-2.5-32B (LoRA) | 45.84 | 58.75 | 54.71 | 66.31 |  | 89.61 | 55.28 |  | 81.71 | 73.20 |\n| Qwen-2.5-72B (LoRA) | 52.79 | 72.56 | 61.25 | 73.27 |  | 91.36 | 60.75 |  | 84.67 | 76.80 |", "caption": "Table 11: Detailed performance among Qwen-2.5 series models refer to Figure\u00a03.", "description": "This table presents a comprehensive comparison of the performance achieved by various Qwen-2.5 series models, ranging in size from 0.5B to 72B parameters, across different evaluation benchmarks.  The models were fine-tuned using instruction data generated by both smaller (SLM) and larger (LLM) language models, and their performance was evaluated using instruction following, mathematical reasoning, and code generation tasks. The goal is to assess the impact of model size and instruction data source on the effectiveness of instruction following capabilities.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"content": "| Temperature | HumanEval | MBPP | HumanEval | MBPP |\n|---|---|---|---|---|\n| | *Supervised Model: Llama-3.1-70B-Instruct* | | *Supervised Model: Llama-3.1-8B-Instruct* | |\n| greedy | 37.20 | 33.40 | **39.63** | **36.40** |\n| 0.1 | 36.59 | 36.40 | **37.80** | **37.60** |\n| 0.3 | 38.41 | 35.20 | **39.63** | **37.80** |\n| 0.5 | 35.98 | 33.40 | **37.80** | **35.80** |\n| 0.7 | 35.98 | **36.00** | **39.02** | 32.80 |\n| 0.9 | 34.76 | 33.00 | **40.24** | **35.80** |", "caption": "Table 12: Performance among different temperatures on Llama-3.2-3B under code generation scenario.", "description": "This table presents the performance of a smaller language model (Llama-3.2-3B) on code generation tasks across different decoding temperatures (greedy decoding, 0.1, 0.3, 0.5, 0.7, and 0.9) during instruction evolution, using two different larger language models (Llama-3.1-70B-Instruct and Llama-3.1-8B-Instruct) as supervised models. The metrics used are HumanEval and MBPP, and the results are compared under each scenario.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"content": "|             | Alpaca | GSM8K Train | Code Alpaca |\n| :---------- | :----: | :--------: | :--------: |\n| Seed Instruction | 27.63 |    34.05   |    26.01   |\n| LLM-Inst Iter1 | 52.89 |    39.88   |    46.75   |\n| SLM-Inst Iter1 | **66.35** |    **48.85**   |    **58.86**   |\n| LLM-Inst Iter2 | 68.16 |    47.14   |    65.02   |\n| SLM-Inst Iter2 | **77.62** |    **63.48**   |    **73.37**   |\n| LLM-Inst Iter3 | 75.73 |    54.00   |    72.85   |\n| SLM-Inst Iter3 | **82.44** |    **72.12**   |    **79.19**   |", "caption": "Table 13: Scores of difficulty levels for instructions evolved during three iterations, using Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models for each round under Evol-Instruct scenario.", "description": "This table presents the difficulty scores of instructions generated by smaller and larger language models (Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct) during three iterations of instruction evolution using the Evol-Instruct method. The scores are calculated for three datasets: Alpaca, GSM8K Train, and Code Alpaca.  The difficulty levels help analyze the complexity of generated instructions and their impact on model performance in downstream tasks.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"content": "| Iteration | Average Reward | Average Reward | Average Reward |\n| :------- | :------------: | :------------: | :------------: |\n|         |    Alpaca     |     GSM8K     |  Code Alpaca   |\n|---|---|---|---| \n| *Supervised Model: Llama-3.1-70B-Instruct* | | | |\n| Iteration 1 | 1.54 | 0.74 | 1.10 |\n| Iteration 2 | **1.68** | 0.73 | **1.19** |\n| Iteration 3 | **1.56** | 0.69 | **1.14** |\n| *Supervised Model: Llama-3.1-8B-Instruct* | | | |\n| Iteration 1 | **1.59** | **1.01** | **1.23** |\n| Iteration 2 | 1.54 | **0.79** | 0.96 |\n| Iteration 3 | 1.42 | **0.97** | 1.03 |", "caption": "Table 14: Comparison of average rewards among different iteration evolution instruction data.", "description": "This table presents a comparison of the average reward scores assigned by the InternLM-2-7B-Reward model to instructions generated by SLMs (Smaller Language Models) and LLMs (Larger Language Models) across three iterations of the Evol-Instruct process, using three different instruction datasets: Alpaca, GSM8K, and Code Alpaca. The reward model evaluates the quality of the generated instructions based on certain criteria, and the average reward score reflects the overall quality of the instruction set produced by each model type in each iteration.  This comparison helps to understand if one model type consistently generates higher-quality instructions than the other across different iterations and datasets.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"content": "| Datasets | IFD (%) | IC-IFD (%) | Performance |\n|---|---|---|---| \n| SLMs (Alpaca iter 3) | **83.04** | 35.89 | 40.64 |\n| LLMs (Alpaca iter 3) | 82.03 | **37.05** | **42.18** |", "caption": "Table 15: Comparison of IFD and IC-IFD on third-round evolved Alpaca datasets from SLMs and LLMs.", "description": "This table compares the Instruction Following Difficulty (IFD) and Instruction Complex-Aware IFD (IC-IFD) scores on the third round of Alpaca datasets evolved by smaller language models (SLMs) and larger language models (LLMs). It also includes the performance of these datasets on IFEval benchmark using Llama-3-8B model.", "section": "RQ3: How Do We Determine Whether An Instruction is Effective without Instruction Tuning?"}]