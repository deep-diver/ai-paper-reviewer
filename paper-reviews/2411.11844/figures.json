[{"figure_path": "https://arxiv.org/html/2411.11844/x3.png", "caption": "Figure 1: We propose the Generative World Explorer Genex that allows an agent to imaginatively explore a large-scale 3D virtual world and acquire imagined observation to update its belief. In this example, agent 1 (sliver car) imaginatively explores to the perspective at the location of agent 2 (taxi) with Genex. The imagined observation at agent 2\u2019s position (including the explored ambulance) revises agent 1\u2019s belief and enables more informed decision making.", "description": "The Generative World Explorer (Genex) framework enables an agent to perform mental exploration of a large-scale 3D environment.  Instead of physically exploring, the agent uses Genex to imaginatively explore from different perspectives. The figure illustrates this with an example: Agent 1 (a silver car) is approaching an intersection.  It uses Genex to simulate the view from Agent 2's perspective (a taxi that has stopped unexpectedly). Through this simulated perspective, Agent 1 discovers an ambulance that is blocking Agent 2's view. This new information (the ambulance) modifies Agent 1's belief about the situation and thus allows it to make a more informed decision (e.g. clearing the way for the ambulance instead of simply proceeding through the intersection). This highlights Genex's ability to improve decision-making by enabling belief revision through imaginative exploration.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2411.11844/x4.png", "caption": "Figure 2: \nGenex is able to explore an imaginative world by generating imagined video outputs, given RGB observations, exploration direction, and distance as inputs (a).\nGenex, grounded in physical environment, can perform GPT-assisted imaginative exploration (b) and target-driven imaginative navigation (c).", "description": "Figure 2 illustrates the Generative World Explorer (Genex) framework. Panel (a) shows the overall architecture: Genex takes as input an RGB observation (a panoramic image), an exploration direction, and a distance.  It then generates a sequence of imagined video frames, simulating the agent's movement and allowing exploration of unseen parts of the environment.  Panel (b) demonstrates Genex performing goal-agnostic exploration, where the agent freely explores its surroundings to build a better understanding. This is guided by a large language model (LLM) providing high-level instructions. Panel (c) shows Genex executing goal-driven exploration, where the agent receives a specific goal (e.g., \"Move to the blue car's position\") and uses the LLM to plan and execute a series of actions to achieve it, again generating imagined video along the way.", "section": "3 GENERATIVE WORLD EXPLORATION"}, {"figure_path": "https://arxiv.org/html/2411.11844/x5.png", "caption": "Figure 3: (a) Diffuser in Genex, a spherical-consistent panoramic video generation model.\nDuring training, video x0subscript\ud835\udc650x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is encoded into latent z0subscript\ud835\udc670z_{0}italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and noised to ztsubscript\ud835\udc67\ud835\udc61z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. A conditioned UNet \u03f5\u03b8subscriptitalic-\u03f5\ud835\udf03\\epsilon_{\\theta}italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT predicts and removes noise, resulting in z0\u2032subscriptsuperscript\ud835\udc67\u20320z^{\\prime}_{0}italic_z start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT which is decoded to x0\u2032subscriptsuperscript\ud835\udc65\u20320x^{\\prime}_{0}italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT.", "description": "Figure 3 illustrates the architecture of the Genex video diffusion model.  Panel (a) shows the model's training process.  A video (x\u2080) is encoded into a latent representation (z\u2080).  Noise is then added to this latent representation (resulting in z\u209c). A conditional U-Net (\u03f5\u03b8) attempts to reverse this process by predicting and removing the added noise. The output of the U-Net (z\u2080\u2032) is then decoded back into a video (x\u2080\u2032). The training objective is to minimize the difference between the original video (x\u2080) and the reconstructed video (x\u2080\u2032).", "section": "3 GENERATIVE WORLD EXPLORATION"}]