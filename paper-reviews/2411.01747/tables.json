[{"content": "| Agent Pipeline |  | GPT-4o mini |  | GPT-4o | \n|---|---|---|---|---|---|---|---|---|---| \n|  |  | Level 1 | Level 2 | Level 3 | Avg. |  | Level 1 | Level 2 | Level 3 | Avg. | \n|---|---|---|---|---|---|---|---|---|---|---| \n| MMAC (rep.) |  | - | - | - | - |  | 45.16 | 20.75 | 6.12 | 25.91 | \n| AutoGen Multi-Agent (rep.) |  | - | - | - | - |  | 47.31 | 28.93 | 14.58 | 32.33 | \n| HF Agent (rep.) |  | - | - | - | - |  | 49.46 | 28.30 | 18.75 | 33.33 | \n| Sibyl (rep.) |  | - | - | - | - |  | 47.31 | 32.70 | 16.33 | 34.55 | \n| Trase Agent (rep.) |  | - | - | - | - |  | 50.54 | 33.33 | 14.29 | 35.55 | \n| No Pipeline |  | 7.53 | 4.40 | 0.00 | 4.65 |  | 13.98 | 8.81 | 2.04 | 9.30 | \n| Sibyl (repl.) |  | 21.51 | 15.72 | 4.08 | 15.61 |  | 38.71 | 24.53 | 10.20 | 26.58 | \n| HF Agent (repl.) |  | 32.26 | 21.38 | 8.33 | 22.67 |  | 39.78 | 27.04 | 14.58 | 29.00 | \n| DynaSaur |  | **45.16** | **22.01** | 8.16 | **26.91** |  | **51.61** | **36.48** | **18.37** | **38.21** |", "caption": "Table 1: Performance comparison between various baseline methods and our proposed approach on the GAIA benchmark, evaluated under two LLM backbones: gpt-4o-2024-08-06 and gpt-4o-mini-2024-07-18. \u201cNo Pipeline\u201d refers to the baseline where no agent pipeline is employed, and the raw LLM is used. Results marked with (rep.) are reported results, while (repl.) indicates replicated results. Each value represents the average exact match percentage between the predicted answers and the ground truth.", "description": "This table compares the performance of DynaSaur against several baseline methods on the GAIA benchmark.  Two different LLM backbones were used for evaluation: gpt-4o-2024-08-06 and gpt-4o-mini-2024-07-18.  The results show the average exact match percentage between the model's predictions and the ground truth.  The 'No Pipeline' row represents the performance of the raw LLM without any agent pipeline, providing a baseline for comparison.  Results marked with (rep.) are from previously reported studies, while (repl.) signifies that the experiments were replicated by the authors.", "section": "4 Experiments"}, {"content": "| # | AA | AI | IA |  | Level 1 | Level 2 | Level 3 | Avg. |\n|---|---|---|---|---|---|---|---|---|\n| 1 | \u2713 | \u2713 | \u2713 |  | 49.06 | 41.86 | 26.92 | 41.82 |\n| 2 | \u2717 | \u2713 | \u2713 |  | 47.17 | 40.70 | 15.38 | 38.79 |\n| 3 | \u2717 | \u2717 | \u2713 |  | 43.40 | 37.21 | 11.54 | 35.15 |\n| 4 | \u2713 | \u2713 | \u2717 |  | 35.85 | 19.77 | 7.69 | 23.03 |\n| 5 | \u2717 | \u2713 | \u2717 |  | 33.96 | 18.60 | 7.69 | 21.82 |", "caption": "Table 2: Ablation of three major components in our framework: action accumulation (denoted as AA), action implementation (denoted as AI), and the initial set of actions (denoted at IA). Each number is the average exact match percentage between the predicted answers and the ground truth.", "description": "This table presents the results of an ablation study conducted to analyze the impact of three key components on the performance of the DynaSaur framework. The components evaluated are action accumulation (AA), action implementation (AI), and the initial set of actions (IA).  Each row represents a different combination of these components, with '\u2713' indicating inclusion and '\u2717' indicating exclusion. The average exact match percentage between the model's predictions and ground truth across various difficulty levels of the GAIA benchmark is reported for each configuration.  This allows for a quantitative assessment of the relative contributions of AA, AI, and IA to the overall system's success in solving diverse tasks.", "section": "4.3 Ablation Study"}, {"content": "| # | Action Header | Description |\n|---|---|---|\n| 1 | `submit_final_answer` | Submits the final answer to the given problem. |\n| 2 | `get_relevant_actions` | Retrieve *k* most relevent generated actions given a query. |\n| 3 | `informational_web_search` | Perform an informational web search query then return the search results. |\n| 4 | `navigational_web_search` | Perform a navigational web search query then immediately navigate to the top result. |\n| 5 | `visit_page` | Visit a webpage at a given URL and return its text. |\n| 6 | `download_file` | Download a file at a given URL. |\n| 7 | `page_up` | Scroll the viewport up in the current webpage and return the new viewport content. |\n| 8 | `page_down` | Scroll the viewport down in the current webpage and return the new viewport content. |\n| 9 | `find_on_page_ctrl_f` | Scroll the viewport to the first occurrence of the search string. |\n| 10 | `find_next` | Scroll the viewport to next occurrence of the search string. |\n| 11 | `find_archived_url` | Given a url, searches the Wayback Machine and returns the archived version of the url that\u2019s closest in time to the desired date. |\n| 12 | `visualizer` | Answer question about a given image. |\n| 13 | `inspect_file_as_text` | Read a file and return its content as Markdown text. |", "caption": "Table 3: List of initial actions used in this project.", "description": "This table lists the initial actions provided to the DynaSaur agent at the beginning of each task.  These actions are pre-defined functions, mostly interacting with external resources like web pages or files, enabling the agent to perform basic operations in various domains.  They serve as the foundation upon which the agent can build and expand its capabilities dynamically by generating and executing its own functions.", "section": "3 Methodology"}]