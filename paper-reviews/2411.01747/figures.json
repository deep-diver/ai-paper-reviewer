[{"figure_path": "https://arxiv.org/html/2411.01747/x3.png", "caption": "Figure 1: Illustration of the DynaSaur\u2009agent framework. In the first step, the agent receives a list of human-designed actions \ud835\udc9cusuperscript\ud835\udc9c\ud835\udc62\\mathcal{A}^{u}caligraphic_A start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT and a task t\ud835\udc61titalic_t as input. It then proposes an action a\ud835\udc4eaitalic_a, implemented as a Python snippet. The function is executed by the environment, which internally contains an IPython kernel. Depending on the generated action a\ud835\udc4eaitalic_a, the kernel may interact with either the action retriever, to retrieve relevant generated actions in \ud835\udc9cgsuperscript\ud835\udc9c\ud835\udc54\\mathcal{A}^{g}caligraphic_A start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT; the internet, for information retrieval from the web; or the local operating system for any other tasks. We do not impose any constraints on which entities the agent can interact with, so the list shown in this figure is not exhaustive and is mainly for illustration purposes. After executing the action a\ud835\udc4eaitalic_a, the environment returns an observation o\ud835\udc5coitalic_o to the agent. The observation can either be the result of executing a\ud835\udc4eaitalic_a or an error message if the kernel fails to execute a\ud835\udc4eaitalic_a.", "description": "The DynaSaur agent framework is illustrated in this figure.  The agent starts by receiving a task and a set of predefined actions. It then generates an action as a Python code snippet, which is executed within an environment containing an IPython kernel. This kernel can interact with various resources depending on the action, including an action retriever for previously generated actions, the internet for web searches, and the local operating system for other tasks.  The agent is not limited in its interactions; this list is illustrative. After executing the action, the environment returns an observation to the agent, which may be the result of the action or an error message.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2411.01747/x4.png", "caption": "Figure 2: Impact of action accumulation on performance over time.", "description": "This figure shows how the model's performance improves over time as more actions are accumulated. The x-axis represents the number of accumulated actions, and the y-axis represents the percentage of exact matches between the model's predictions and ground truth.  The figure shows separate lines for different difficulty levels (Level 1, Level 2, Level 3) of the GAIA benchmark.  It demonstrates the positive impact of dynamic action creation and accumulation on the model's performance, especially for more complex tasks.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.01747/x5.png", "caption": "Figure 3: Distribution of error types in tasks where agent A (without action implementation) answers incorrectly, while agent B (with action implementation) answers correctly.", "description": "This figure shows a breakdown of the reasons why Agent A (without the ability to create new actions) failed on tasks where Agent B (with the ability to create new actions) succeeded. The error types are categorized as follows: 1. Insufficient tooling: Agent A lacked the necessary tools to solve the problem. 2. Failure to follow instructions: Agent A failed to correctly interpret or follow the instructions. 3. Other reasons: Agent A failed due to factors not directly related to the lack of action implementation.  The chart visually represents the proportion of errors falling under each category.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.01747/x6.png", "caption": "Figure 4: Mean coverage over the validation set as the number of actions increases. The red dashed line marks the point where human-designed actions are added to the action set. Subsequent data points reflect the accumulation of generated actions.", "description": "This figure illustrates the relationship between the number of actions available to the DynaSaur agent and its performance on the GAIA validation set.  The x-axis represents the number of actions, starting from a small initial set and increasing as the agent generates new actions during training. The y-axis shows the mean coverage, which measures how effectively the current set of actions allows the agent to solve tasks successfully. The red dashed line indicates the point at which human-designed actions are added to the initial action set; data points after this line demonstrate the agent's improved performance due to the accumulation of generated actions over time. The plot shows the general trend of increased coverage as the number of actions available to the agent grows, suggesting the benefit of dynamic action creation and accumulation within the DynaSaur framework.", "section": "4.4 Measuring Action Coverage"}, {"figure_path": "https://arxiv.org/html/2411.01747/x7.png", "caption": "Figure 5: A case study demonstrates the difference in problem-solving flexibility between Agent A (a variant of DynaSaur\u2009without action implementation) and Agent B (the proposed agent framework). Both agents begin with the same initial step, but only Agent B, equipped with the ability to implement its own actions, successfully completes the task. Due to space constraints, the first step taken by Agent B is not shown.", "description": "This figure showcases a comparative analysis of two agent models, Agent A and Agent B, tackling the same problem. Agent A represents a DynaSaur variant without the capability for dynamic action creation. Agent B, on the other hand, embodies the proposed DynaSaur framework, allowing it to generate and implement its own actions.  Both agents start with identical initial steps.  The figure highlights how Agent B's dynamic action generation capabilities enable it to overcome obstacles Agent A encounters, ultimately leading to a successful task completion.  Due to layout constraints, the image only displays Agent B's trajectory from a later stage.", "section": "4.5 Case Studies"}, {"figure_path": "https://arxiv.org/html/2411.01747/x8.png", "caption": "Figure 6: Prompt for OpenAI\u2019s o1 to perform qualitative evaluation.", "description": "This figure shows the prompt used for qualitative analysis with OpenAI's 01 model. The prompt provides the evaluator with the task, the correct answer, the ground truth trajectory from a human, agent A's predicted answer and trajectory, agent B's predicted answer and trajectory. It then asks the evaluator to write a report that includes a summary of the task, summaries of both agents' trajectories, which agent performed better and why, and whether agent B's ability to implement its own actions impacted its performance.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.01747/x9.png", "caption": "Figure 7: The system prompt of our DynaSaur\u2009agent framework.", "description": "This figure shows the system prompt used to instruct the DynaSaur LLM agent.  The prompt details the agent's role as a problem-solving assistant with access to a Python interpreter, internet, and operating system functionalities.  It outlines the step-by-step process for solving tasks, emphasizing the need for clear reasoning (Thought), well-structured Python code (Code) that leverages relevant libraries, and iterative refinement based on the results.  The prompt also provides guidelines for writing reusable, modular functions and for analyzing outputs, stressing real-world data usage and the importance of persistence until a solution is found or the iteration limit is reached.  Sections on available functions and guidelines are included to aid the agent's interaction and code generation.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2411.01747/x10.png", "caption": "Figure 8: A case study demonstrates the difference in problem-solving flexibility between Agent A (a variant of DynaSaur\u2009without action implementation) and Agent B (the proposed agent framework).", "description": "This figure showcases a comparative analysis of two agents: Agent A, representing a version of DynaSaur without dynamic action creation, and Agent B, embodying the proposed DynaSaur framework. Both agents tackle the same task\u2014identifying a counterexample to prove that a binary operation is not commutative.  Agent A relies solely on predefined actions, hindering its ability to solve the problem effectively.  In contrast, Agent B leverages its dynamic action generation capabilities, allowing it to create and execute a custom function to reach the solution. This directly demonstrates how the ability to create actions on-demand significantly enhances problem-solving flexibility and efficiency within the framework.", "section": "4.5 Case Studies"}]