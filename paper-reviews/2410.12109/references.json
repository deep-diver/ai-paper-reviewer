{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "This paper is foundational for the work presented here, as it introduces GPT-4, a crucial component of the evaluation methodology.  The model's capabilities are compared to those of GPT-4, highlighting its relevance to evaluating the performance of OMCAT.  Without GPT-4 as a benchmark, the overall significance and evaluation of the OMCAT model's capabilities would be significantly diminished.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Andrea Agostinelli", "paper_title": "Musiclm: Generating music from text", "reason": "This paper introduces MusicLM, a significant advancement in music generation using large language models. It serves as a relevant comparison point within the broader context of multimodal LLMs and their advancements, enriching the background of related works and allowing for a more nuanced discussion on OMCAT.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Huda Alamri", "paper_title": "Audio visual scene-aware dialog", "reason": "This paper introduces the AVSD dataset, which is a key benchmark used for evaluating the performance of the proposed OMCAT model.  The model's performance on this dataset is reported in the experiments section, and a comparison is made against other state-of-the-art models, making this dataset crucial for establishing the novelty and effectiveness of OMCAT.", "section_number": 5}, {" publication_date": "2017", "fullname_first_author": "Lisa Anne Hendricks", "paper_title": "Localizing moments in video with natural language", "reason": "This paper is important for the context of video understanding and temporal localization, directly relevant to the goals of OMCAT.  It provides a comparative framework for assessing the advancements made by the proposed model in achieving precise temporal grounding and cross-modal alignment.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Honglie Chen", "paper_title": "Vggsound: A large-scale audio-visual dataset", "reason": "This paper introduces the Vggsound dataset, a relevant dataset for audio-visual understanding.  This dataset is discussed in the related work section and serves to contextualize the work presented here, highlighting the limitations of existing datasets which OMCAT aims to address.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Sihan Chen", "paper_title": "Valor: Vision-audio-language omni-perception pretraining model and dataset", "reason": "This paper introduces VALOR, a multimodal model similar to OMCAT.  The discussion and comparison in the related works section demonstrates the model's capabilities, limitations, and the novelty of the OMCAT model.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Sihan Chen", "paper_title": "Vast: A vision-audio-subtitle-text omni-modality foundation model and dataset", "reason": "This paper introduces VAST, a large-scale multimodal dataset similar to OCTAV.  Its importance lies in its comparison and contrast with OCTAV in the related work section.  The comparison highlights the novelty and specific contributions of OCTAV in addressing fine-grained temporal understanding.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zesen Cheng", "paper_title": "Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms", "reason": "This paper introduces VideoLLaMA 2, which sets the state-of-the-art in video LLMs. It serves as a strong benchmark against which to measure OMCAT's performance and allows for a deeper analysis of OMCAT's strengths and weaknesses.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality", "reason": "This paper presents Vicuna, a key component of the OMCAT model.  Vicuna is the large language model used in the OMCAT architecture, forming a core component of its ability to process and generate textual output. The performance of OMCAT hinges critically on Vicuna's capabilities.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Honglie Chen", "paper_title": "Vggsound: A large-scale audio-visual dataset", "reason": "This paper is important because it provides a detailed description of the Vggsound dataset, which is a relevant dataset to contextualize the research and highlight the limitations of existing datasets.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Konstantinos Drossos", "paper_title": "Clotho: An audio captioning dataset", "reason": "This paper introduces the Clotho dataset which is used for the evaluation of OMCAT, specifically to assess its audio-visual understanding capabilities.  The dataset's inclusion enhances the comprehensive nature of the experiments and shows the wider applicability of OMCAT.", "section_number": 5}, {" publication_date": "2021", "fullname_first_author": "Eduardo Fonseca", "paper_title": "Fsd50k: an open dataset of human-labeled sound events", "reason": "This paper is crucial because it presents FSD50K, a key dataset used for the sound events incorporated into the OCTAV dataset. The detailed information about the sound events in the FSD50K dataset is vital for understanding and evaluating the construction and design of the OCTAV dataset.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Chaoyou Fu", "paper_title": "Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis", "reason": "This paper introduces the Video-MME benchmark, which is a critical evaluation metric used to assess the model's performance in video analysis. This benchmark helps to understand the context of the research, highlighting the importance of the proposed model and its contributions to the field.", "section_number": 5}, {" publication_date": "2017", "fullname_first_author": "Jiyang Gao", "paper_title": "Tall: Temporal activity localization via language query", "reason": "This paper focuses on temporal activity localization in videos which is closely related to the temporal reasoning capabilities of OMCAT.  It is included in the related work section as a relevant background paper, allowing the reader to better understand the contributions of OMCAT in the context of previous work on this topic.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Tiantian Geng", "paper_title": "Dense-localizing audio-visual events in untrimmed videos: A large-scale benchmark and baseline", "reason": "This paper is relevant because it introduces the UnAV-100 dataset which is a crucial component used for the construction of OCTAV.  The detailed characteristics of the dataset are described in the paper, which are used for the construction and evaluation of the OCTAV-MT dataset.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Rohit Girdhar", "paper_title": "Imagebind: One embedding space to bind them all", "reason": "This paper is crucial to understanding the architecture of the OMCAT model.  ImageBind is used as the pre-trained audio encoder in the OMCAT model, playing a key role in audio feature extraction.  Without this encoder, the OMCAT model would not function as intended.", "section_number": 5}, {" publication_date": "2021", "fullname_first_author": "Shawn Hershey", "paper_title": "The benefit of temporally-strong labels in audio event classification", "reason": "This paper discusses the importance of temporally strong labels in audio event classification, which is directly relevant to the design and evaluation of the OCTAV dataset, where temporal precision is a key requirement.  It strengthens the justification for OCTAV's design choices.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "Lisa Anne Hendricks", "paper_title": "Localizing moments in video with natural language", "reason": "This paper focuses on localizing moments in videos using natural language, directly relevant to the temporal understanding aspect of OMCAT.  It provides valuable context for the work by highlighting the challenges in precisely localizing moments in videos using natural language.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Junnan Li", "paper_title": "Videochat: Chat-centric video understanding", "reason": "This paper is highly relevant as it introduces VideoChat, a large-scale dataset for video understanding.  It is mentioned in the related works section and serves as a comparison point for highlighting the novelty and improvements of the proposed OCTAV dataset.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Guangyao Li", "paper_title": "Learning to answer questions in dynamic audio-visual scenarios", "reason": "This paper is significant because it introduces a dataset and approach for answering questions in dynamic audio-visual scenarios, directly addressing the problem that OMCAT aims to solve.  The techniques and approaches presented here are relevant to the development and evaluation of OMCAT.", "section_number": 2}]}