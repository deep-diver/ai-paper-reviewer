[{"figure_path": "2410.12109/tables/table_4_0.html", "caption": "Table 1: Statistics with number of videos and question-answer pairs for the OCTAV-ST dataset.", "description": "Table 1 presents the number of videos and question-answer pairs used for training and testing the OCTAV-ST dataset, broken down by the source dataset.", "section": "3 THE OCTAV DATASET"}, {"figure_path": "2410.12109/tables/table_4_1.html", "caption": "Table 2: Statistics with number of videos and question-answer pairs for the OCTAV-MT dataset.", "description": "Table 2 presents the number of videos and question-answer pairs used in the OCTAV-MT dataset for training and testing, broken down by source dataset.", "section": "3 THE OCTAV DATASET"}, {"figure_path": "2410.12109/tables/table_4_2.html", "caption": "Table 3: Comparison of our proposed OCTAV dataset with other datasets with respect to modalities (audio/video), caption availability, multi-turn setup and timestamp information.", "description": "Table 3 compares the proposed OCTAV dataset with other existing datasets based on the availability of audio and video modalities, detailed captions, multi-turn setup, and timestamp information.", "section": "3 THE OCTAV DATASET"}, {"figure_path": "2410.12109/tables/table_7_0.html", "caption": "Table 4: List of datasets used for training OMCAT. TS indicates if timestamps are available. ST refers to single-turn question answers. MT is the version with multi-turn dialogue.", "description": "Table 4 lists the datasets used for training the OMCAT model across its three training stages, indicating whether timestamps are available and if the dataset supports single-turn or multi-turn question-answer pairs.", "section": "4.4 TRAINING STRATEGY"}, {"figure_path": "2410.12109/tables/table_8_0.html", "caption": "Table 5: Evaluation results for OMCAT and other state-of-the-art models on AVQA tasks (Yang et al., 2022b; Alamri et al., 2019; Li et al., 2022), Charades-STA (Gao et al., 2017) and our proposed OCTAV-ST dataset. While \u2020 describes results from models fine-tuned on the training set of those datasets, results in parentheses are zero-shot.", "description": "Table 5 presents a quantitative comparison of OMCAT's performance against other state-of-the-art models on various audio-visual question answering tasks and a temporal understanding task, including zero-shot and fine-tuned results.", "section": "5 EXPERIMENTS"}, {"figure_path": "2410.12109/tables/table_9_0.html", "caption": "Table 6: Results of different variations of OMCAT (ROPE, ITT and ROTE) on the OCTAV-MT benchmark and the UnAV-100-MT dataset.", "description": "Table 6 presents the performance comparison of three different variations of OMCAT (using ROPE, ITT, and ROTE time embeddings) on the OCTAV-MT benchmark and the UnAV-100-MT dataset.", "section": "5 EXPERIMENTS"}, {"figure_path": "2410.12109/tables/table_9_1.html", "caption": "Table 7: Effect of applying various time embeddings-ROPE, ITT and ROTE to OMCAT on all benchmarks.", "description": "Table 7 presents the performance comparison of three different time embedding methods (ROPE, ITT, and ROTE) used in OMCAT across multiple evaluation benchmarks (AVSD, Music-AVQA, AVQA, Charades-STA, OCTAV-ST-YouCook2, and OCTAV-ST-ActivityNet).", "section": "5 EXPERIMENTS"}, {"figure_path": "2410.12109/tables/table_9_2.html", "caption": "Table 8: Effect of alignment tuning data on the overall performance. LP denotes LLaVA-Pretrain-595k (Liu et al., 2024), WC denotes WavCaps (Mei et al., 2024) and, V denotes Valley-703K (Luo et al., 2023).", "description": "Table 8 shows the ablation study results, demonstrating the impact of alignment tuning data (LLaVA-Pretrain, WavCaps, Valley) on the overall performance of the model across three different tasks (Music-AVQA, Charades-STA, OCTAV-ST-Youcook2).", "section": "5.2 ABLATION STUDY"}, {"figure_path": "2410.12109/tables/table_21_0.html", "caption": "Table 9: Performance comparison on video understanding benchmarks. \u2020 means specialized model and * means trained on a much larger dataset.", "description": "Table 9 compares the performance of OMCAT and other state-of-the-art models on three video understanding benchmark datasets: MSRVTT-QA, MSVD-QA, and ActivityNet-QA.", "section": "5 EXPERIMENTS"}]