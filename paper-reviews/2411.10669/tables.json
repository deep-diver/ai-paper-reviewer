[{"content": "| Model | Parameters | Institutions | Chinese Overall | Chinese Perception | Chinese Reasoning |\n|---|---|---|---|---|---| \n| **Awaker2.5-VL (ours)** | 10.8B | Metabrain AGI | **62.7** | **67.71** | **52.07** |\n| Qwen2-VL | 8B | Alibaba | 55.5 | 59.80 | 46.46 |\n| InternVL-2 | 7B | Shanghai AI Lab | 54.3 | 57.79 | 46.65 |\n| InternVL-Chat-V1.5 | 20B | Shanghai AI Lab | 47.9 | 49.90 | 43.74 |\n| Claude3.5 Sonnet | - | Anthropic | 47.0 | 48.25 | 44.31 |\n| Yi-VL-34B | 34B | 01.AI | 42.0 | 42.45 | 41.16 |\n| CogVLM2-Llama3-Chat | 8B | THU & Zhipu AI | 39.8 | 38.57 | 42.25 |\n| GPT-4o | - | OpenAI | 38.8 | 43.44 | 29.05 |\n| Mini-Gemini-34B-HD | 34B | CUHK | 38.5 | 38.31 | 38.75 |\n| Cambrian-1-8B | 8B | NYU | 33.6 | 32.44 | 35.97 |\n| LLaVA-NeXT-Qwen-72B | 72B | Bytedance | 30.6 | 30.02 | 31.67 |\n| Gemini-1.5-Pro | - | Google | 28.1 | 36.10 | 11.14 |\n| DeepSeek-VL | 7B | DeepSeek AI | 27.6 | 27.63 | 27.63 |\n| GPT-4o-mini | - | OpenAI | 25.9 | 26.32 | 25.16 |", "caption": "Table 1: Evaluation Results on MME-Realworld-CN Benchmark.", "description": "This table presents a comprehensive evaluation of various multimodal large language models (MLLMs) on the MME-Realworld-CN benchmark.  The benchmark focuses on real-world Chinese scenarios, encompassing diverse tasks.  The table shows the performance of each model across three key dimensions: Overall, Perception, and Reasoning.  Model parameters, the institution responsible for the model, and the specific scores for each dimension are provided for comparison.", "section": "4 Experiments"}, {"content": "| Model | Parameters | Institutions | English Overall | English Perception | English Reasoning |\n|---|---|---|---|---|---| \n| **Awaker2.5-VL (ours)** | 10.8B | Metabrain AGI | **60.8** | **63.14** | 43.74 |\n| LLaVA-OneVision | 8B | Bytedance | 57.4 | 59.59 | 41.17 |\n| Qwen2-VL | 8B | Alibaba | 56.5 | 58.96 | 40.39 |\n| InternVL-2 | 7B | Shanghai AI Lab | 53.5 | 55.82 | 38.74 |\n| Claude3.5 Sonnet | - | Anthropic | 51.6 | 52.90 | **44.12** |\n| InternVL-Chat-V1.5 | 20B | Shanghai AI Lab | 49.4 | 51.36 | 36.48 |\n| Mini-Gemini-34B-HD | 34B | CUHK | 45.9 | 48.05 | 31.73 |\n| GPT-4o | - | OpenAI | 45.2 | 46.43 | 37.61 |\n| CogVLM2-Llama3-Chat | 8B | THU & Zhipu AI | 44.6 | 45.84 | 37.25 |\n| Cambrian-1-8B | 8B | NYU | 42.7 | 43.82 | 36.16 |\n| Gemini-1.5-Pro | - | Google | 38.2 | 39.63 | 29.19 |\n| GPT-4o-mini | - | OpenAI | 36.4 | 37.12 | 32.48 |\n| DeepSeek-VL | 7B | DeepSeek AI | 32.4 | 33.14 | 27.98 |\n| Yi-VL-34B | 34B | 01.AI | 31.0 | 30.97 | 32.45 |\n| LLaVA-NeXT-Qwen-72B | 72B | Bytedance | 28.7 | 29.01 | 27.86 |", "caption": "Table 2: Evaluation Results on MME-Realworld Benchmark.", "description": "This table presents a comprehensive evaluation of various multimodal large language models (MLLMs) on the MME-Realworld benchmark.  The benchmark focuses on real-world image datasets and evaluates the models' performance across three key aspects: overall accuracy, perception capabilities, and reasoning skills.  The table includes the model name, its parameter count, the institution that developed it, and the quantitative results for each evaluation aspect.", "section": "4 Experiments"}, {"content": "| Model | Parameters | Institutions | Chinese Overall | Chinese MMBench_v1.1 | Chinese MMBench |\n|---|---|---|---|---|---| \n| Qwen2-VL-72B | 73.4B | Alibaba | **86.3** | **85.8** | **86.7** |\n| InternVL2-40B | 40B | Shanghai AI Lab | 85.7 | 84.9 | 86.4 |\n| InternVL2-Llama-76B | 76B | Shanghai AI Lab | 85.5 | 85.5 | - |\n| Taiyi | - | Megvii | 85.2 | 85.0 | 85.4 |\n| JT-VL-Chat-V3.0 | - | China Mobile | 84.7 | 83.5 | 85.8 |\n| LLaVA-OneVision-72B | 73B | ByteDance | 84.6 | 83.9 | 85.3 |\n| Step-1.5V | - | StepFun | 84.0 | 83.5 | 84.5 |\n| Claude3.5-Sonnet-20241022 | - | Anthropic | 83.0 | 82.5 | 83.5 |\n| **Awaker2.5-VL (ours)** | 10.8B | Metabrain AGI | 82.6 | 81.8 | 83.4 |\n| GPT-4o (0513, detail-low) | - | OpenAI | 82.3 | 82.5 | 82.1 |\n| LLaVA-OneVision-7B | 8B | ByteDance | 81.8 | 80.9 | 82.7 |\n| GPT-4o (0513, detail-high) | - | OpenAI | 81.8 | 81.5 | 82.1 |\n| InternVL2-26B | 26B | Shanghai AI Lab | 81.5 | 80.9 | 82.1 |\n| CongROng | - | CloudWalk | 81.2 | 80.4 | 81.9 |\n| MMAlaya2 | 26B | DataCanvas | 80.9 | 79.7 | 82.1 |\n| Ovis1.6-Gemma2-9B | 10.2B | Alibaba | 80.8 | 79.5 | 82.0 |\n| Qwen2-VL-7B | 8B | Alibaba | 80.5 | 80.3 | 80.6 |\n| LLaVA-OneVision-72B (SI) | 73B | ByteDance | 80.0 | 81.9 | 78.0 |\n| InternVL-Chat-V1.5 | 26B | Shanghai AI Lab | 79.9 | 79.1 | 80.7 |\n| InternLM-XComposer2.5 | 8B | Shanghai AI Lab | 79.9 | 78.8 | 80.9 |\n| GPT-4o (0806, detail-high) | - | OpenAI | 79.8 | 79.2 | 80.3 |\n| GPT-4V (0409, detail-high) | - | OpenAI | 79.2 | 78.2 | 80.2 |", "caption": "Table 3: Evaluation Results on MMBench-CN Benchmark.", "description": "This table presents a comprehensive evaluation of various multimodal large language models (MLLMs) on the MMBench-CN benchmark.  The benchmark focuses on evaluating the performance of these models across a range of visual and language understanding tasks within the Chinese language.  The table lists each model, its number of parameters, the institution that developed it, and its performance scores across the overall benchmark and on two sub-benchmarks: MMBench_v1.1 and MMBench.  The scores provide a comparative analysis of the models' abilities in various visual and language understanding tasks.", "section": "4.3 Main Results"}, {"content": "| Model | Parameters | Institutions | English Overall | English MMBench_v1.1 | English MMBench |\n|---|---|---|---|---|---| \n| Qwen2-VL-72B | 73.4B | Alibaba | **86.5** | **86.1** | **86.9** |\n| InternVL2-40B | 40B | Shanghai AI Lab | 86.0 | 85.1 | 86.8 |\n| Taiyi | - | Megvii | 85.7 | 84.7 | 86.7 |\n| InternVL2-Llama-76B | 76B | Shanghai AI Lab | 85.5 | 85.5 | - |\n| LLaVA-OneVision-72B | 73B | ByteDance | 85.4 | 85.0 | 85.8 |\n| JT-VL-Chat-V3.0 | - | China Mobile | 84.5 | 83.6 | 85.4 |\n| **Awaker2.5-VL (ours)** | 10.8B | Metabrain AGI | 83.7 | 82.5 | 84.9 |\n| GPT-4o (0513, detail-high) | - | OpenAI | 83.2 | 83.0 | 83.4 |\n| GPT-4o (0513, detail-low) | - | OpenAI | 83.2 | 83.1 | 83.3 |\n| Step-1.5V | - | StepFun | 82.9 | 80.4 | 85.3 |\n| InternVL2-26B | 26B | Shanghai AI Lab | 82.5 | 81.5 | 83.4 |\n| Ovis1.6-Gemma2-9B | 10.2B | Alibaba | 82.5 | 81.5 | 83.4 |\n| RBDash-v1.2-72B | 79B | DLUT | 82.5 | 81.7 | 83.2 |\n| Qwen2-VL-7B | 8B | Alibaba | 82.4 | 81.8 | 83.0 |\n| LLaVA-OneVision-7B | 8B | ByteDance | 82.1 | 80.9 | 83.2 |\n| GPT-4o (0806, detail-high) | - | OpenAI | 82.0 | 81.8 | 82.1 |\n| LLaVA-OneVision-72B (SI) | 73B | ByteDance | 81.9 | 83.3 | 80.5 |\n| Qwen-VL-Plus-0809 | - | Alibaba | 81.9 | 81.1 | 82.7 |\n| CongROng | - | CloudWalk | 81.9 | 80.9 | 82.8 |\n| Claude3.5-Sonnet-20241022 | - | Anthropic | 81.8 | 80.9 | 82.6 |\n| MMAlaya2 | 26B | DataCanvas | 81.6 | 80.6 | 82.5 |\n| InternVL-Chat-V1.5 | 26B | Shanghai AI Lab | 81.3 | 80.3 | 82.3 |\n| InternLM-XComposer2.5 | 8B | Shanghai AI Lab | 81.1 | 80.1 | 82.0 |\n| GPT-4V (0409, detail-high) | - | OpenAI | 80.5 | 80.0 | 81.0 |", "caption": "Table 4: Evaluation Results on MMBench Benchmark.", "description": "This table presents a comprehensive comparison of various multimodal large language models (MLLMs) on the MMBench benchmark.  The benchmark assesses performance across multiple dimensions of visual-language understanding, including overall performance, MMBench v1.1, and MMBench. It provides parameters, institutions responsible for the models, and the scores for each model on each of the three dimensions for a set of models, including the model proposed in the paper.", "section": "4 Experiments"}]