[{"figure_path": "https://arxiv.org/html/2411.10669/extracted/6003672/figs/1115_moe.png", "caption": "Figure 1: The Standard MoE Structure in Awaker2.5-VL.", "description": "This figure illustrates the architecture of the Mixture of Experts (MoE) model used in Awaker2.5-VL.  It shows the base model (with frozen parameters), multiple expert modules (each a LoRA structure), and a gating network that controls which experts are activated for each input. The input data (image and text) is processed by the base model, and the output is combined with outputs from the activated expert(s) to produce the final output. A global expert module is always active to ensure versatility and generalization. The gating network uses a softmax function and top-k selection to choose the most suitable experts. The figure also visually depicts the flow of information within the model.", "section": "3.1 MoE Structure"}, {"figure_path": "https://arxiv.org/html/2411.10669/extracted/6003672/figs/1115_nogate.png", "caption": "Figure 2: The Simplified MoE Structure in Awaker2.5-VL.", "description": "This figure shows a simplified version of the Mixture of Experts (MoE) architecture used in the Awaker2.5-VL model.  Unlike the standard MoE structure (shown in Figure 1), this simplified version removes the gate layer. Instead, it directly accepts the gate results (Gglobal and Gexperts) calculated in another MoE module for routing.  This simplifies the architecture and improves training stability. The figure highlights the input (x), the MoE module, the gate result (Gglobal and Gmax), and the final output (y).", "section": "3.1 MoE Structure"}, {"figure_path": "https://arxiv.org/html/2411.10669/extracted/6003672/figs/training1114.png", "caption": "Figure 3: The Traing Pipeline of Awaker2.5-VL. From Left to Right: Stage I, Stage II, and Stage III.", "description": "This figure illustrates the three-stage training pipeline for the Awaker2.5-VL model. Stage I involves initializing the model by training only the LoRA parameters while keeping the base model frozen.  Stage II trains the MoE module, replacing the LoRA module from Stage I and again freezing the base model.  The MoE module includes the gate layer and all experts. Finally, Stage III performs instruction fine-tuning, focusing on training only the experts within the MoE module while keeping the gate layer frozen.  Each stage builds upon the previous one, progressively enhancing the model's capabilities.", "section": "3.3 Training Process"}]