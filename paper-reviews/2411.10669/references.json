{"references": [{"fullname_first_author": "AI, Young, A.", "paper_title": "Yi: Open foundation models by 01.ai", "publication_date": "2024-03-00", "reason": "This paper introduces Yi, a large open-foundation model, which is a significant contribution to the field and likely used as a comparison model in the current research."}, {"fullname_first_author": "Bai, J.", "paper_title": "Qwen technical report", "publication_date": "2023-09-00", "reason": "This work presents Qwen, a large language model that serves as the foundation for the Awaker2.5-VL model, making it a crucial reference for understanding the base model's architecture and capabilities."}, {"fullname_first_author": "Bai, J.", "paper_title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond", "publication_date": "2023-08-00", "reason": "Qwen-VL, introduced in this paper, is a multimodal model closely related to Awaker2.5-VL, providing a strong baseline for comparison and informing design choices."}, {"fullname_first_author": "Chen, Z.", "paper_title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks", "publication_date": "2024-03-00", "reason": "InternVL, discussed in this reference, is another significant multimodal model that is likely a key comparison model for evaluating Awaker2.5-VL's performance."}, {"fullname_first_author": "Li, J.", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-01-00", "reason": "BLIP-2 is a highly influential multimodal model that is often used as a comparison model, making it important for contextualizing the contribution of the Awaker2.5-VL model."}]}