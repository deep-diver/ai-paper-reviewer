[{"figure_path": "2410.09019/charts/charts_4_0.png", "caption": "Figure 2. Overview of language models and their performance on USMLE-style questions, contextualized over time. Panel A) shows the progession of smallest language model that is able to pass the USMLE, based on the MedQA. Panel B) displays MedMobile (red) compared to Llama-3 Ultra-Medical 8B (purple), and a baseline phi-3-mini (green) model on the entire MultiMedQA. MedMobile achieves almost identical or superior performance across the entirety of the MultiMedQA compared to the SOTA of <10B parameter language models (UltraMedical 8B), with a fraction of the parameters. Panel C) presents the relative accuracy of MedMobile to other language models on the MedQA. Current models range vastly in parameter size; with closed-source models such as Med-Palm 2 requiring", "description": "Figure 2 shows the progression of language models over time and their performance on USMLE style questions, comparing MedMobile to other models across various metrics.", "section": "Main"}, {"figure_path": "2410.09019/charts/charts_11_0.png", "caption": "Supplemental Figure 1. Comparison of number of output tokens in a response and accuracy on MedQA questions. Each question of the MedQA test set is represented 5x in this figure due to the ensemble performed. Some questions are not included in the plots (<20) as model response exceeded maximum generation output and an accuracy could not be evaluated. Top panel is a CoT enhanced baseline phi-3-mini model, whereas the bottom panel is our fine-tuned model, MedMobile.", "description": "Supplemental Figure 1 compares the number of output tokens generated by two language models (a baseline model and a fine-tuned model) against their accuracy on the MedQA question set, highlighting the impact of model size and fine-tuning on performance.", "section": "Supplementary Material"}, {"figure_path": "2410.09019/charts/charts_12_0.png", "caption": "Supplemental Figure 1. Comparison of number of output tokens in a response and accuracy on MedQA questions. Each question of the MedQA test set is represented 5x in this figure due to the ensemble performed. Some questions are not included in the plots (<20) as model response exceeded maximum generation output and an accuracy could not be evaluated. Top panel is a CoT enhanced baseline phi-3-mini model, whereas the bottom panel is our fine-tuned model, MedMobile.", "description": "Supplemental Figure 1 shows the relationship between the number of output tokens generated by two different language models (a baseline phi-3-mini model and the fine-tuned MedMobile model) and their accuracy on the MedQA questions.", "section": "Supplementary Material"}, {"figure_path": "2410.09019/charts/charts_12_1.png", "caption": "Supplemental Figure 2. Comparison of number of input tokens in a response and accuracy on MedQA questions. Each question of the MedQA test set is represented 5x in this figure due to the ensemble performed. Some questions are not included in the plots (<20) as model response exceeded maximum generation output and an accuracy could not be evaluated. Top panel is a CoT enhanced baseline phi-3-mini model, whereas the bottom panel is our trained model, MedMobile.", "description": "Supplemental Figure 2 shows the relationship between the number of input tokens in a response and the accuracy of the model (MedMobile) on the MedQA questions.", "section": "Supplementary Material"}, {"figure_path": "2410.09019/charts/charts_13_0.png", "caption": "Supplemental Figure 3. Panel A) depicts the accuracy of MedMobile on the MedQA relative to the number of k-shot prompting (i.e., number of examples given to the model alongside the evaluation question). Panel B) shows different forms of retrieval for RAG and their resultant effects on the accuracy of MedMobile on the MedQA dataset. To conduct RAG based on vector embeddings, we compute cosine similarity based on MedCPT vectors generation between the question and paragraphs in the textbook. RAG built on BM-25 is developed through the lucine implementation, and selects the paragraph with the highest score for a particular question. While all forms of RAG achieve sub-optimal results, we note that BM-25 seemed to affect the model least negatively with the addition of context. The source of information for these evaluations is from Harrison\u2019s Principles of Internal Medicine, 21e [28].", "description": "Supplemental Figure 3 shows the effect of k-shot prompting and various retrieval methods on the accuracy of the MedMobile model on the MedQA dataset.", "section": "Supplementary Material"}, {"figure_path": "2410.09019/charts/charts_13_1.png", "caption": "Supplemental Figure 3. Panel A) depicts the accuracy of MedMobile on the MedQA relative to the number of k-shot prompting (i.e., number of examples given to the model alongside the evaluation question). Panel B) shows different forms of retrieval for RAG and their resultant effects on the accuracy of MedMobile on the MedQA dataset. To conduct RAG based on vector embeddings, we compute cosine similarity based on MedCPT vectors generation between the question and paragraphs in the textbook. RAG built on BM-25 is developed through the lucine implementation, and selects the paragraph with the highest score for a particular question. While all forms of RAG achieve sub-optimal results, we note that BM25 seemed to affect the model least negatively with the addition of context. The source of information for these evaluations is from Harrison's Principles of Internal Medicine, 21e [28].", "description": "Supplemental Figure 3 shows the impact of different retrieval augmented generation (RAG) methods and k-shot prompting on MedMobile's performance on the MedQA dataset.", "section": "Other Techniques"}]