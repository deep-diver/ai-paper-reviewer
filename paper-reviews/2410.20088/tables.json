[{"figure_path": "2410.20088/tables/table_3_0.html", "caption": "Table 1: Training from decoder-only (LLM) checkpoint. Performance is measured by nDCG@10. RARe shows up to +2.72% absolute gain on average over Promptriever, demonstrating that starting from an existing embedding model is not a requirement. We provide a breakdown of In-Domain (ID) and Out-of-Domain (OOD) performance.", "description": "Table 1 presents the performance of different models trained from decoder-only checkpoints (LLMs) and their variants on the BeIR and RAR-b benchmarks.", "section": "5.1 Training from LLM Checkpoints"}, {"figure_path": "2410.20088/tables/table_5_0.html", "caption": "Table 1: Training from decoder-only (LLM) checkpoint. Performance is measured by nDCG@10. RARe shows up to +2.72% absolute gain on average over Promptriever, demonstrating that starting from an existing embedding model is not a requirement. We provide a breakdown of In-Domain (ID) and Out-of-Domain (OOD) performance.", "description": "This table presents the performance of different models trained from decoder-only checkpoints on three retrieval benchmarks (BeIR, RAR-b, and MS-MARCO), showing the effectiveness of the proposed RARe method.", "section": "5.1 Training from LLM Checkpoints"}, {"figure_path": "2410.20088/tables/table_6_0.html", "caption": "Table 2: Training from retriever checkpoint. Performance (nDCG@10) on BeIR (Thakur et al., 2021) and RAR-b (Xiao et al., 2024) benchmarks when fine-tuning retriever model on E5 dataset. We report a breakdown of performance on In-Domain (ID) and Out-of-Domain (OOD) tasks on BeIR. We consider all RAR-b tasks as OOD.", "description": "Table 2 shows the performance of different retriever models on BeIR and RAR-b benchmarks after fine-tuning with and without in-context examples.", "section": "5.2 Training from retriever checkpoint"}, {"figure_path": "2410.20088/tables/table_8_0.html", "caption": "Table 3: Impact of the number of in-context examples (k) during training and evaluation. All results are on E5-Mistral-Instruct. In general, performance increases when increasing the number of examples, and the optimal number of examples depends on the task.", "description": "Table 3 shows the impact of varying the number of in-context examples (k) during both training and evaluation phases on the E5-Mistral-Instruct model's performance across six different datasets.", "section": "6.1 Choice of In-Context Examples"}, {"figure_path": "2410.20088/tables/table_8_1.html", "caption": "Table 4: In-Context Format Comparing variants of in-context example format on E5-Mistral-Instruct. Instruct refers to the baseline which does not use any in-context examples.", "description": "Table 4 compares the performance of different in-context example formats on the E5-Mistral-Instruct model for various downstream tasks.", "section": "6.1 CHOICE OF IN-CONTEXT EXAMPLES"}, {"figure_path": "2410.20088/tables/table_8_2.html", "caption": "Table 5: Impact of adding negative documents in the in-context prompt. All results are on E5-Mistral-Instruct. Negative documents (d\u00af) in the prompt do not enhance performance.", "description": "Table 5 shows the impact of adding negative document pairs to the in-context examples on the performance of the E5-Mistral-Instruct model, indicating no performance gains.", "section": "6.1 Choice of In-Context Examples"}, {"figure_path": "2410.20088/tables/table_9_0.html", "caption": "Table 6: Latency breakdown (in seconds) of each stage in the retrieval pipeline for qinst and qinst+ic evaluation settings. # Corpus denote the number of documents and Avg Q len. denote the average number of query tokens split by whitespace. Table 11 in the Appendix provides numbers on additional datasets.", "description": "Table 6 presents the latency breakdown of each stage in the retrieval pipeline for both baseline and in-context settings, showing the impact of in-context examples on processing time across datasets of varying sizes and query lengths.", "section": "6.2 EFFICIENCY ANALYSIS"}, {"figure_path": "2410.20088/tables/table_18_0.html", "caption": "Table 7: Performance (nDCG@10) on BeIR (Thakur et al., 2021) when fine-tuning retriever model on E5 dataset. We report a breakdown of performance on In-Domain (ID) and Out-of-Domain (OOD) tasks on BeIR.", "description": "Table 7 presents the performance of different retrieval methods (base, instruct, and RARE) on various datasets of the BeIR benchmark, categorized by in-domain and out-of-domain, showing the impact of RARE on both types of data.", "section": "5.2 Training from retriever checkpoint"}, {"figure_path": "2410.20088/tables/table_18_1.html", "caption": "Table 8: Performance on reasoning-focused IR benchmark RAR-b (Xiao et al., 2024) when fine-tuning existing retriever models.", "description": "Table 8 presents the performance of different retrieval models on the RAR-b benchmark, comparing the base model, the model with only instructions, and the model augmented with in-context examples.", "section": "5.2 Training from retriever checkpoint"}, {"figure_path": "2410.20088/tables/table_19_0.html", "caption": "Table 9: Performance (nDCG@10) on BeIR when training decoder-only models.", "description": "Table 9 presents the nDCG@10 scores on the BeIR benchmark for different methods when training is performed only on decoder-only models.", "section": "5.1 Training from LLM checkpoints"}, {"figure_path": "2410.20088/tables/table_19_1.html", "caption": "Table 10: Performance (nDCG@10) on datasets from RAR-b when training decoder-only models.", "description": "Table 10 presents the performance of different models on reasoning-focused retrieval tasks from the RAR-b benchmark when training from decoder-only model checkpoints.", "section": "5.1 Training from LLM checkpoints"}, {"figure_path": "2410.20088/tables/table_20_0.html", "caption": "Table 6: Latency breakdown (in seconds) of each stage in the retrieval pipeline for qinst and qinst+ic evaluation settings. # Corpus denote the number of documents and Avg Q len. denote the average number of query tokens split by whitespace. Table 11 in the Appendix provides numbers on additional datasets.", "description": "Table 6 presents a breakdown of the latency of each stage of the retrieval pipeline for both baseline and in-context settings, showing the time required for nearest-neighbor in-context examples, query embeddings, and search.", "section": "6.2 EFFICIENCY ANALYSIS"}, {"figure_path": "2410.20088/tables/table_20_1.html", "caption": "Table 3: Impact of the number of in-context examples (k) during training and evaluation. All results are on E5-Mistral-Instruct. In general, performance increases when increasing the number of examples, and the optimal number of examples depends on the task.", "description": "Table 3 shows the impact of varying the number of in-context examples used during both training and evaluation on the performance of the E5-Mistral-Instruct model across different datasets.", "section": "5.1 Training from LLM checkpoints"}, {"figure_path": "2410.20088/tables/table_20_2.html", "caption": "Table 13: Impact of the number of in-context examples (k) during training and inference. All results are on E5-Mistral-Instruct. In general, performance increases when increasing the number of examples, and the optimal number of in-context examples can vary by task.", "description": "Table 13 shows the impact of varying the number of in-context examples during training and inference on the performance of the E5-Mistral-Instruct model across multiple datasets.", "section": "5.1 Training from LLM checkpoints"}, {"figure_path": "2410.20088/tables/table_21_0.html", "caption": "Table 14: In-Context Format Comparing variants of in-context example format on E5-Mistral-Instruct during inference only. Training is done with the Regular format. Instruct refers to the baseline which does not use any in-context examples.", "description": "This table compares the performance of different in-context example formats during inference only on the E5-Mistral-Instruct model, showing the impact of various query augmentation strategies on retrieval tasks.", "section": "6.4 Choice of In-Context Examples"}, {"figure_path": "2410.20088/tables/table_21_1.html", "caption": "Table 14: In-Context Format Comparing variants of in-context example format on E5-Mistral-Instruct during inference only. Training is done with the Regular format. Instruct refers to the baseline which does not use any in-context examples.", "description": "Table 14 shows the impact of different in-context example formats on retrieval performance when only modifying the query at inference time, holding the training format constant.", "section": "6.2 Choice of In-Context Examples"}, {"figure_path": "2410.20088/tables/table_21_2.html", "caption": "Table 16: Performance (nDCG@10) on datasets from the BeIR benchmark Thakur et al., 2021 when training decoder-only model (Llama3). Applying RARE with only in-context examples can lead to degradation of performance in the zero-shot setting (qinst), but this is easily mitigated my including a mixture of qinst and qinst+ic data (30% and 70%) respectively.", "description": "The table shows the performance of different training methods (using various combinations of instruction-only queries and in-context example queries) on the BeIR benchmark when starting from decoder-only LLMs.", "section": "5.1 Training from LLM Checkpoints"}]