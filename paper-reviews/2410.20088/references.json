{"references": [{" publication_date": "2020", "fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "reason": "This foundational paper introduced the concept of in-context learning (ICL) in large language models (LLMs), which is the core concept that the current paper builds upon.  Understanding ICL in LLMs is crucial to applying it to retrieval models, which is the central theme of the current work. The paper's significant impact and widespread influence on the field make it a critical reference.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Vladimir Karpukhin", "paper_title": "Dense passage retrieval for open-domain question answering", "reason": "This paper describes the standard dense retrieval setup, which forms the foundation of the current paper's experimental setup and methodology.  It is essential because the current paper focuses on improving dense retrievers, so understanding the existing framework is critical to evaluating the proposed improvements. The paper's wide adoption in the community makes it a significant reference.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Gautier Izacard", "paper_title": "Unsupervised dense information retrieval with contrastive learning", "reason": "This paper is essential because it presents a state-of-the-art approach in dense retrieval, which the authors aim to improve by incorporating in-context examples. Understanding the existing techniques and their limitations helps to clarify the novel aspects of RARE. The paper's strong performance in retrieval makes it a key reference for comparison.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Akari Asai", "paper_title": "Task-aware retrieval with instructions", "reason": "This paper is highly relevant as it explores using task-specific instructions in query augmentation, a technique similar to the in-context example approach proposed in the current paper. By comparing and contrasting these two approaches, the current work can better highlight the unique aspects of incorporating in-context examples.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Parishad BehnamGhader", "paper_title": "LLM2Vec: Large language models are secretly powerful text encoders", "reason": "This paper presents a strong baseline retriever model that the authors use in their experiments.  The chosen model's performance is important to understand the effectiveness of the proposed RARE method. It is a key reference for demonstrating the improvements obtained using RARE on this model compared to its baseline performance.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Liang Wang", "paper_title": "Improving text embeddings with large language models", "reason": "This paper presents a high-performing retriever model which serves as a baseline and a comparison point for the authors' method. Understanding the performance of existing approaches is essential to show improvements or novel aspects of the introduced RARE method.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Nandan Thakur", "paper_title": "BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models", "reason": "This paper introduces the BEIR benchmark, which is used in the current paper for evaluating the performance of the proposed RARE method.  The BEIR benchmark's widespread use and comprehensive evaluation metrics make it a critical reference for comparing retrieval models.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Chenghao Xiao", "paper_title": "RAR-b: Reasoning as retrieval benchmark", "reason": "This paper introduces the RAR-b benchmark, a subset of which is used in the current study to evaluate the model's performance, particularly on more complex reasoning-oriented retrieval tasks. The RAR-b benchmark's focus on reasoning complements the BeIR benchmark used by the authors and contributes to a more comprehensive evaluation of the model.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Xueguang Ma", "paper_title": "Fine-tuning llama for multi-stage text retrieval", "reason": "This paper is important because it serves as a direct comparison to the approach used in the current work. Both papers involve fine-tuning LLMs for retrieval; however, the current paper focuses on leveraging in-context examples, creating an important comparison.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Orion Weller", "paper_title": "Promptriever: Instruction-trained retrievers can be prompted like language models", "reason": "This paper is highly relevant because it is a direct comparison point for the authors' work. Both papers focus on improving retrieval models, but Promptriever uses task-specific instructions, while the current paper utilizes in-context examples, highlighting the differences and improvements of the RARE method.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Rowan Zellers", "paper_title": "HellaSwag: Can a machine really finish your sentence?", "reason": "This paper introduces the HellaSwag dataset, which is part of the RAR-b benchmark used in the current paper for evaluation.  Understanding the nature and difficulty of this dataset is essential to interpreting the model's performance on complex reasoning-oriented retrieval tasks.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Sweta Agrawal", "paper_title": "In-context examples selection for machine translation", "reason": "This paper is highly relevant due to its focus on in-context example selection, a critical component of the RARE method.  The strategies for selecting effective examples are directly applicable and inform the methods used in the current study.", "section_number": 6}, {" publication_date": "2022", "fullname_first_author": "Ohad Rubin", "paper_title": "Learning to retrieve prompts for in-context learning", "reason": "This paper directly addresses the challenge of effectively using in-context examples, a central aspect of the RARE method.  It explores methods for selecting relevant examples, a crucial consideration for the current paper's proposed approach.", "section_number": 6}, {" publication_date": "2022", "fullname_first_author": "Yiming Zhang", "paper_title": "Active example selection for in-context learning", "reason": "This paper directly addresses the topic of in-context example selection, which is a critical part of the RARE method.  It provides insights into how to choose examples effectively, improving performance, which directly connects to the selection strategies employed in the current study.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Jon Saad-Falcon", "paper_title": "Benchmarking and building long-context retrieval models with loco and m2-bert", "reason": "This paper is highly relevant because it explores methods for handling long contexts, a challenge that is relevant to the RARE approach. The latency introduced by long in-context examples is addressed in this paper. It is a significant reference for future work, providing a direction to tackle the latency problems associated with long contexts.", "section_number": 7}, {" publication_date": "2020", "fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is highly influential in the field of large language models and in-context learning, serving as a cornerstone of the current work. It provides a fundamental understanding of ICL, and the current research builds upon its findings.  Its widespread citation reflects its importance and foundational role.", "section_number": 7}, {" publication_date": "2023", "fullname_first_author": "Orion Weller", "paper_title": "Followir: Evaluating and teaching information retrieval models to follow instructions", "reason": "This paper is highly relevant because it focuses on the problem of adapting retrieval models to different tasks, a core issue addressed by the RARE approach.  Understanding methods for tailoring retrieval models to specific tasks is essential to contextualizing the improvements achieved by the RARE method.", "section_number": 7}, {" publication_date": "2023", "fullname_first_author": "Benfeng Xu", "paper_title": "knn prompting: Beyond-context learning with calibration-free nearest neighbor inference", "reason": "This paper is important as it introduces a novel prompting technique that shares similarities with the in-context learning approach explored in the current paper. This method demonstrates that alternative techniques can boost retrieval performance, showcasing the importance of prompt engineering in information retrieval. This provides a valuable comparison point for the current paper's approach.", "section_number": 7}, {" publication_date": "2022", "fullname_first_author": "Hattie Zhou", "paper_title": "Teaching algorithmic reasoning via in-context learning", "reason": "This paper explores the use of in-context learning to teach algorithmic reasoning, which is a related but different application of the core concept of in-context learning. It provides insights into the potential of ICL beyond simple retrieval tasks and offers a broader perspective on the capabilities and limitations of this technique. ", "section_number": 7}, {" publication_date": "2024", "fullname_first_author": "Chenghao Xiao", "paper_title": "RAR-b: Reasoning as retrieval benchmark", "reason": "This paper is crucial because it provides the RAR-b benchmark dataset used for evaluation in the paper. This benchmark focuses on reasoning-intensive tasks, showcasing how the RARE method works in more challenging conditions and how it compares to standard baselines.", "section_number": 7}]}