[{"figure_path": "https://arxiv.org/html/2503.13661/extracted/6277701/figures/pensez_r1.png", "caption": "Figure 1: Performance on knowledge retrieval and reasoning skills in French and English. Despite fewer training samples, Pensez 7B performs comparably to DeepSeek R1 7B on reasoning tasks but outperforms it in knowledge retrieval.", "description": "This figure displays a radar chart comparing the performance of various large language models (LLMs) on a range of knowledge retrieval and reasoning tasks in both English and French.  The models compared include Pensez 7B, DeepSeek R1 7B, and others.  Each axis represents a specific task, and the distance from the center reflects the model's performance on that task (higher is better).  The key takeaway is that despite being trained on significantly fewer samples than other models (as indicated in the figure and explicitly stated in the caption), Pensez 7B demonstrates comparable performance on reasoning tasks, and even surpasses other models in knowledge retrieval tasks.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.13661/extracted/6277701/figures/tasks.png", "caption": "Figure 2: The Pensez-2k dataset is categorized into 10 prevalent task types reflecting typical user interactions.", "description": "Figure 2 is a bar chart that visualizes the distribution of task types within the Pensez-2k dataset.  The dataset is categorized into ten common interaction types, revealing the diversity of tasks. The chart illustrates the percentage of samples belonging to each category.  This provides insights into the balance of task types included in the dataset and shows how the data is designed to represent the variety of interactions a large language model might encounter. The most frequent types of tasks are analysis (9.1%), mathematical reasoning (60.6%), and problem-solving (8.5%), while less prevalent categories such as interactive simulations, critical evaluation, and synthesis represent lower percentages of data.", "section": "Multilingual reasoning data Pensez-2k"}, {"figure_path": "https://arxiv.org/html/2503.13661/extracted/6277701/figures/data_processing.png", "caption": "Figure 3: Data collection and cleaning pipeline for the Pensez-2k dataset. The pipeline consists of three stages: initial collection, data filtering, and data augmentation. The final dataset comprises 2,000 samples, evenly distributed across English and French, with a focus on diverse reasoning tasks.", "description": "The figure illustrates the process of creating the Pensez-2k dataset, which is divided into three stages: data collection, data filtering, and data augmentation. The initial stage involves gathering various datasets from publicly available resources, categorized into reasoning and daily conversation datasets.  The data filtering stage refines the initial dataset using several criteria, including length, language purity, and diversity, to ensure that all samples are of high quality and relevant to the target task. This stage aims to reduce redundancy and improve data quality. Finally, the data augmentation stage expands the dataset through various techniques, including bilingual (English-French) translation to enhance the model's multilingual capabilities and reasoning chain augmentation to provide detailed reasoning steps within the training samples. The end result is a final dataset of 2000 samples (1000 in English and 1000 in French), carefully curated to support the development of a strong multilingual reasoning model.  The process of selection emphasizes diversity across task types, including mathematical reasoning, to ensure the model's reasoning abilities are well-rounded.", "section": "Multilingual reasoning data Pensez-2k"}, {"figure_path": "https://arxiv.org/html/2503.13661/extracted/6277701/figures/performance_english.png", "caption": "(a) Performance on English tasks", "description": "This figure displays the performance comparison of various language models across different English-language tasks.  Specifically, it shows how the performance scores of Pensez 7B, Qwen2.5 7B, OpenThinker 7B, DeepSeek R1 7B, and OpenR1 7B vary depending on the size of the training dataset used. The x-axis represents the number of training samples, while the y-axis shows the reasoning performance scores.  This visualization helps demonstrate the data efficiency of the Pensez model.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2503.13661/extracted/6277701/figures/performance_french.png", "caption": "(b) Performance on French tasks", "description": "This figure displays the performance of different language models on French-language tasks.  It shows the relationship between the number of training samples used and the performance score achieved.  The models compared include Pensez 7B, Qwen2.5 7B, OpenThinker 7B, OpenR1 7B, and DeepSeek R1 7B.  The x-axis represents the number of training samples, while the y-axis represents the performance score.  The plot helps visualize the data efficiency of Pensez 7B in comparison to other models trained on significantly larger datasets.", "section": "4 Results"}, {"figure_path": "https://arxiv.org/html/2503.13661/extracted/6277701/figures/model_comparison_mmlu.png", "caption": "Figure 4: Comparative performance on (a) AIME25 and MMLU (English), and (b) MATH Hard lv5 and MMLU (French). All models are finetuned from the Qwen2.5 7B Instruct base model and undergo SFT with varying sample sizes. Pensez 7B, trained on only 2,000 samples distributed across diverse tasks, achieves competitive performance on reasoning tasks while exhibiting minimal degradation in knowledge comprehension compared to other 7B reasoning models. This demonstrates its data efficiency relative to models trained on larger datasets.", "description": "Figure 4 presents a comparative analysis of the performance of several language models, including Pensez 7B, on various benchmarks assessing reasoning and knowledge comprehension.  The benchmarks include AIME25 and MMLU (English) in subfigure (a) and MATH Hard lv5 and MMLU (French) in subfigure (b).  All models are fine-tuned from the same base model (Qwen2.5 7B Instruct) but using different training datasets of varying sizes.  The key takeaway is that Pensez 7B, despite being trained on a significantly smaller dataset (only 2,000 samples), achieves competitive performance in reasoning tasks with minimal reduction in knowledge comprehension abilities when compared to larger models.", "section": "4 Results"}, {"figure_path": "https://arxiv.org/html/2503.13661/extracted/6277701/figures/model_comparison_math.png", "caption": "Figure 5: Model Performance Comparison on MMLU-French.", "description": "This figure presents a comparison of the performance of four different language models on the MMLU-French benchmark.  The models compared are Pensez 7B, DeepSeek R1 7B, and Qwen2.5 7B Instruct. The chart uses a donut chart visualization to show the percentage of correct, incorrect, and incorrect (out of context) predictions for each model.  The 'incorrect (out of context)' category signifies answers which are factually wrong but not necessarily because of reasoning failures; rather, the model may have strayed from the task or engaged in excessive self-reflection.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2503.13661/extracted/6277701/figures/reflection_analysis_comparison.png", "caption": "Figure 6: Model Performance Comparison on MATH Hard lv5 (French).", "description": "This figure presents a comparison of model performance on the MATH Hard lv5 (French) benchmark.  It shows the proportion of correct, incorrect, and incorrect-out-of-context responses for three models: Pensez 7B, DeepSeek R1 7B, and Qwen2.5 7B Instruct.  The 'incorrect-out-of-context' category represents instances where the model's responses deviated significantly from the expected format or reasoning process, often due to excessive self-reflection or getting 'stuck' in a loop. The figure highlights the different strengths and weaknesses of each model in mathematical reasoning tasks, particularly revealing Pensez 7B's tendency toward overthinking.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2503.13661/extracted/6277701/figures/wandb_pensez.png", "caption": "Figure 7: Reflection counts for correct and incorrect predictions in Pensez 7B and DeepSeek R1 7B models. Both models show substantially higher average reflection counts for incorrect predictions compared to correct predictions.", "description": "Figure 7 presents a comparative analysis of reflection counts in correct versus incorrect predictions generated by two language models: Pensez 7B and DeepSeek R1 7B.  The x-axis represents individual predictions (prediction ID), while the y-axis displays the total count of reflection tokens.  Reflection tokens are words or phrases indicating self-doubt or a reconsideration of the model's reasoning process.  The figure visually demonstrates a significant trend: both models exhibit substantially more reflection tokens in their incorrect predictions compared to their correct ones. This suggests that excessive self-reflection may be correlated with an increased likelihood of producing incorrect answers.  The plot allows for a direct visual comparison of the two models' tendencies toward overthinking.", "section": "Analysis"}]