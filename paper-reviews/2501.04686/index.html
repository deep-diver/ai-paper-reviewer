<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics &#183; HF Daily Paper Reviews by AI"><meta name=description content="URSA-7B:  A new multimodal model significantly improves chain-of-thought reasoning in mathematics!"><meta name=keywords content="Natural Language Processing,Large Language Models,üè¢ Tsinghua University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04686/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04686/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics"><meta property="og:description" content="URSA-7B:  A new multimodal model significantly improves chain-of-thought reasoning in mathematics!"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-01-08T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-08T00:00:00+00:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="üè¢ Tsinghua University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04686/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04686/cover.png"><meta name=twitter:title content="URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics"><meta name=twitter:description content="URSA-7B:  A new multimodal model significantly improves chain-of-thought reasoning in mathematics!"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics","headline":"URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics","abstract":"URSA-7B:  A new multimodal model significantly improves chain-of-thought reasoning in mathematics!","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2501.04686\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-01-08T00:00:00\u002b00:00","datePublished":"2025-01-08T00:00:00\u002b00:00","dateModified":"2025-01-08T00:00:00\u002b00:00","keywords":["Natural Language Processing","Large Language Models","üè¢ Tsinghua University"],"mainEntityOfPage":"true","wordCount":"5517"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About This Project">About</p></a><a href=/ai-paper-reviewer/2025-01-30/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-01-30s>2025-01-30</p></a><a href=/ai-paper-reviewer/2025-01-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-01-31s>2025-01-31</p></a><a href=/ai-paper-reviewer/2025-02-04/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-02-04s>2025-02-04</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="Paper Reviews by AI">Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About This Project">About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-01-30/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-01-30s>2025-01-30</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-01-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-01-31s>2025-01-31</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-04/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-02-04s>2025-02-04</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="Paper Reviews by AI">Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2501.04686/cover_hu_a97cb767cce34905.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2501.04686/>URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-01-08T00:00:00+00:00>8 January 2025</time><span class="px-2 text-primary-500">&#183;</span><span>5517 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">26 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2501.04686/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2501.04686/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/natural-language-processing/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Natural Language Processing
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/large-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-tsinghua-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Tsinghua University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu_97e7d424fadd1c26.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#multimodal-cot>Multimodal CoT</a></li><li><a href=#ursa-7b-model>URSA-7B Model</a></li><li><a href=#dualmath-11m>DualMath-1.1M</a></li><li><a href=#test-time-scaling>Test-Time Scaling</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#multimodal-cot>Multimodal CoT</a></li><li><a href=#ursa-7b-model>URSA-7B Model</a></li><li><a href=#dualmath-11m>DualMath-1.1M</a></li><li><a href=#test-time-scaling>Test-Time Scaling</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2501.04686</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Ruilin Luo et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-01-09</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2501.04686 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2501.04686 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/ursa-understanding-and-verifying-chain-of target=_self role=button>‚Üó Papers with Code</a></p><audio controls><source src=https://ai-paper-reviewer.com/2501.04686/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Multimodal mathematical reasoning by large language models (LLMs) is challenging due to the scarcity of high-quality Chain-of-Thought (CoT) training data. Existing models struggle with high-precision CoT reasoning and limited potential during test time. This hinders real-world applications.</p><p>This paper introduces URSA, a three-module framework integrating CoT distillation, trajectory rewriting, and format unification. URSA generates a new dataset, MMathCoT-1M, and trains a robust model, URSA-7B, achieving state-of-the-art results. A data synthesis strategy, DualMath-1.1M, and a verifier model, URSA-RM-7B, further boost test-time scaling and out-of-distribution generalization capabilities. <strong>The open-sourced model and data are valuable resources for future research.</strong></p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-8261aabb30970a58e230832558a29319></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-8261aabb30970a58e230832558a29319",{strings:[" The paper proposes a three-module synthesis strategy for generating high-quality CoT reasoning data in multimodal mathematics. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-38fbdd7d5255a890be7c3b92eca1a5fe></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-38fbdd7d5255a890be7c3b92eca1a5fe",{strings:[" The URSA-7B model achieves state-of-the-art performance on various multimodal mathematical benchmarks. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-e520c0e0a79b455315cd5c4af1b360d0></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-e520c0e0a79b455315cd5c4af1b360d0",{strings:[" A novel dual-view process supervision strategy enhances test-time scaling and out-of-distribution generalization. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is important because it addresses the critical need for high-quality training data in multimodal mathematical reasoning. It introduces a novel data synthesis strategy and a robust model (URSA-7B) that significantly advances the state-of-the-art, opening new avenues for research in test-time scaling and out-of-distribution generalization. The open-sourcing of the model and data further accelerates progress in the field.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x1.png alt></figure></p><blockquote><p>üîº This figure presents a radar chart comparing the performance of the URSA-7B model against several other open-source large language models (LLMs) on two benchmark datasets: MathVista and MathVerse. Each axis represents a different category of mathematical reasoning problems (e.g., statistical reasoning, geometry reasoning). The lengths of the vectors emanating from the center of the chart represent the accuracy of each model on the corresponding task. The chart also shows how the models perform when the type of input data (text-only, text-lite, etc.) is varied to assess their robustness when the type and amount of modal information change. The overall goal of the figure is to demonstrate the superior reasoning ability and stability of the URSA-7B model across various mathematical problems and different data conditions.</p><details><summary>read the caption</summary>Figure 1: We compare the reasoning ability of URSA-7B and other open-source MLLMs across different topics on MathVista and MathVerse, as well as their reasoning stability when faced with changes in modal information content.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>#Params</th><th>MathVista ALL</th><th>MathVista GPS</th><th>MathVista MWP</th><th>MathVista FQA</th><th>MathVista TQA</th><th>MathVista VQA</th><th>MathVerse ALL</th><th>MathVerse TD</th><th>MathVerse TL</th><th>MathVerse TO</th><th>MathVerse VI</th><th>MathVerse VD</th><th>MathVerse VO</th></tr></thead><tbody><tr><td><em>Baselines</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Random</td><td>-</td><td>17.9</td><td>21.6</td><td>3.8</td><td>18.2</td><td>19.6</td><td>26.3</td><td>12.4</td><td>12.4</td><td>12.4</td><td>12.4</td><td>12.4</td><td>12.4</td><td>12.4</td></tr><tr><td>Human</td><td>-</td><td>60.3</td><td>48.4</td><td>73.0</td><td>59.7</td><td>63.2</td><td>55.9</td><td>64.9</td><td>71.2</td><td>70.9</td><td>41.7</td><td>61.4</td><td>68.3</td><td>66.7</td></tr><tr><td><em>Closed-Source MLLMs</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4o [2024]</td><td>-</td><td>63.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>GPT-4V [2023]</td><td>-</td><td>49.9</td><td>50.5</td><td>57.5</td><td>43.1</td><td>65.2</td><td>38.0</td><td>54.4</td><td>63.1</td><td>56.6</td><td>60.3</td><td>51.4</td><td>32.8</td><td>50.3</td></tr><tr><td>Gemini-1.5-002-Flash [2023]</td><td>-</td><td>58.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Gemini-1.5-Pro [2023]</td><td>-</td><td>63.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>35.3</td><td>39.8</td><td>34.7</td><td>44.5</td><td>32.0</td><td>36.8</td><td>33.3</td></tr><tr><td>Claude-3.5-Sonnet [2024]</td><td>-</td><td>67.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td></tr><tr><td>Qwen-VL-Plus [2023]</td><td>-</td><td>43.3</td><td>35.5</td><td>31.2</td><td>54.6</td><td>48.1</td><td>51.4</td><td>21.3</td><td>26.0</td><td>21.2</td><td>25.2</td><td>18.5</td><td>19.1</td><td>21.8</td></tr><tr><td><em>Open-Source General MLLMs</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>mPLUG-Owl2-7B [2023]</td><td>7B</td><td>22.2</td><td>23.6</td><td>10.2</td><td>22.7</td><td>27.2</td><td>27.9</td><td>10.3</td><td>11.6</td><td>11.4</td><td>13.8</td><td>11.1</td><td>9.4</td><td>8.0</td></tr><tr><td>MiniGPT4-7B [2023]</td><td>7B</td><td>23.1</td><td>26.0</td><td>13.4</td><td>18.6</td><td>30.4</td><td>30.2</td><td>12.2</td><td>12.3</td><td>12.9</td><td>13.4</td><td>12.5</td><td>14.8</td><td>8.7</td></tr><tr><td>LLaVA-1.5-13B [2024b]</td><td>13B</td><td>27.7</td><td>22.7</td><td>18.9</td><td>23.8</td><td>43.0</td><td>30.2</td><td>12.7</td><td>17.1</td><td>12.0</td><td>22.6</td><td>12.6</td><td>12.7</td><td>9.0</td></tr><tr><td>SPHINX-V2-13B [2023]</td><td>13B</td><td>36.7</td><td>16.4</td><td>23.1</td><td>54.6</td><td>41.8</td><td>43.0</td><td>16.1</td><td>20.8</td><td>14.1</td><td>14.0</td><td>16.4</td><td>15.6</td><td>16.2</td></tr><tr><td>LLaVA-NeXT-34B [2024c]</td><td>34B</td><td>46.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>34.6</td><td>49.0</td><td>37.6</td><td>30.1</td><td>35.2</td><td>28.9</td><td>22.4</td></tr><tr><td>InternLM-XComposer2-VL [2024a]</td><td>7B</td><td>57.6</td><td>63.0</td><td>73.7</td><td>55.0</td><td>56.3</td><td>39.7</td><td>25.9</td><td>36.9</td><td>28.3</td><td>42.5</td><td>20.1</td><td>24.4</td><td>19.8</td></tr><tr><td>Deepseek-VL [2024]</td><td>7B</td><td>34.9</td><td>28.4</td><td>55.9</td><td>26.8</td><td>32.9</td><td>34.6</td><td>19.3</td><td>23.0</td><td>23.2</td><td>23.1</td><td>20.2</td><td>18.4</td><td>11.8</td></tr><tr><td>InternVL2-8B [2024b]</td><td>8B</td><td>58.3</td><td>62.0</td><td>59.1</td><td>58.7</td><td>61.4</td><td>49.7</td><td>35.9</td><td>39.0</td><td>33.8</td><td>36.0</td><td>32.2</td><td>30.9</td><td>27.7</td></tr><tr><td>Qwen2-VL [2024a]</td><td>7B</td><td>58.9</td><td>40.9</td><td>64.0</td><td>69.1</td><td>60.1</td><td>58.1</td><td>33.6</td><td>37.4</td><td>33.5</td><td>35.0</td><td>31.3</td><td>30.3</td><td>28.1</td></tr><tr><td><em>Open-Source Math MLLMs</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>G-LLaVA-7B [2023]</td><td>7B</td><td>25.1</td><td>48.7</td><td>3.6</td><td>19.1</td><td>25.0</td><td>28.7</td><td>16.6</td><td>20.9</td><td>20.7</td><td>21.1</td><td>17.2</td><td>14.6</td><td>9.4</td></tr><tr><td>Math-LLaVA-13B [2024]</td><td>13B</td><td>46.6</td><td>57.7</td><td>56.5</td><td>37.2</td><td>51.3</td><td>33.5</td><td>22.9</td><td>27.3</td><td>24.9</td><td>27.0</td><td>24.5</td><td>21.7</td><td>16.1</td></tr><tr><td>Math-PUMA-Qwen2-7B [2024]</td><td>7B</td><td>47.9</td><td>48.1</td><td>68.3</td><td>46.5</td><td>46.2</td><td>30.2</td><td>33.6</td><td>42.1</td><td>35.0</td><td>39.8</td><td>33.4</td><td>31.6</td><td>26.0</td></tr><tr><td>Math-PUMA-DeepSeek-Math [2024]</td><td>7B</td><td>44.7</td><td>39.9</td><td>67.7</td><td>42.8</td><td>42.4</td><td>31.3</td><td>31.8</td><td>43.4</td><td>35.4</td><td>47.5</td><td>33.6</td><td>31.6</td><td>14.7</td></tr><tr><td>MAVIS-7B [2024d]</td><td>7B</td><td>-</td><td>64.1</td><td>-</td><td>-</td><td>-</td><td>-</td><td>35.2</td><td>43.2</td><td>37.2</td><td>-</td><td>34.1</td><td>29.7</td><td>31.8</td></tr><tr><td>InfiMM-Math [2024]</td><td>7B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>34.5</td><td>46.7</td><td>32.4</td><td>-</td><td>38.1</td><td>32.4</td><td>15.8</td></tr><tr><td>MultiMath-7B [2024]</td><td>7B</td><td>50.0</td><td>66.8</td><td>61.8</td><td>40.1</td><td>50.0</td><td>33.0</td><td>27.7</td><td>34.8</td><td>30.8</td><td>35.3</td><td>28.1</td><td>25.9</td><td>15.0</td></tr><tr><td>URSA-7B</td><td>7B</td><td>59.8</td><td>79.3</td><td>75.3</td><td>44.6</td><td>63.9</td><td>40.2</td><td>45.7</td><td>55.3</td><td>48.3</td><td>51.8</td><td>46.4</td><td>43.9</td><td>28.6</td></tr><tr><td>Œî over SOTA Open-Source Math MLLMs</td><td>-</td><td>+9.8</td><td>+12.5</td><td>+7.0</td><td>-1.9</td><td>+12.6</td><td>+6.7</td><td>+10.5</td><td>+8.6</td><td>+11.1</td><td>+4.3</td><td>+8.3</td><td>+11.5</td><td>-3.2</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of various large language models (LLMs) on two mathematical reasoning benchmarks: MATHVISTA and MATHVERSE. The models are categorized into closed-source and open-source, with open-source models further divided into general-purpose and those specifically designed for mathematical reasoning. Performance is measured across several sub-tasks within each benchmark and is indicated by numerical scores. The table highlights the best-performing model for each task in bold, and the second-best is underlined. To help distinguish performance levels, the best closed-source model results are shown in green, while the best and second-best open-source model results are shown in red and blue, respectively.</p><details><summary>read the caption</summary>Table 1: Comparison with closed-source MLLMs and open-source MLLMs on MATHVISTA testmini and MATHVERSE testmini. The best is bold, and the runner-up is underline. The best results of Closed-source MLLMs are highlighted in green. The best and second-best results of Open-source MLLMs are highlighted in red and blue respectively.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Multimodal CoT<div id=multimodal-cot class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multimodal-cot aria-label=Anchor>#</a></span></h4><p>Multimodal Chain-of-Thought (CoT) reasoning extends the capabilities of large language models (LLMs) by integrating visual information with textual reasoning. <strong>This multimodal approach presents unique challenges</strong>, such as handling modality-specific mismatches (e.g., discrepancies between visual and textual data) and generating high-quality training data. <strong>A key benefit of multimodal CoT</strong> is the ability to tackle complex problems requiring both visual interpretation and logical deduction, which are beyond the scope of unimodal LLMs. However, <strong>effective multimodal CoT relies on robust data synthesis strategies</strong> for both training and testing, due to the scarcity of real-world multimodal CoT data. Furthermore, <strong>robust evaluation metrics are needed</strong> to assess the true capabilities of multimodal CoT systems, as traditional metrics may not capture the nuances of this approach. Future research in this area should focus on developing innovative data synthesis techniques and refined evaluation methods to fully realize the potential of multimodal CoT in various fields.</p><h4 class="relative group">URSA-7B Model<div id=ursa-7b-model class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#ursa-7b-model aria-label=Anchor>#</a></span></h4><p>The URSA-7B model, as described in the research paper, represents a significant advancement in multimodal mathematical reasoning. <strong>Its core strength lies in its ability to perform chain-of-thought (CoT) reasoning</strong>, a crucial step toward solving complex problems by breaking them down into smaller, manageable steps. The model&rsquo;s effectiveness is significantly boosted by the <strong>MMathCoT-1M dataset</strong>, a meticulously curated instruction fine-tuning dataset specifically designed for multimodal mathematics. This dataset plays a vital role in enabling URSA-7B to achieve state-of-the-art performance across various benchmarks. Furthermore, <strong>the innovative dual-view process supervision</strong>, using DualMath-1.1M, elevates URSA-7B&rsquo;s reasoning capabilities by addressing both logical and visual accuracy, leading to superior performance and robustness, particularly in scenarios with complex visual inputs. The integration of URSA-RM-7B as a verifier further enhances the model&rsquo;s capabilities for test-time scaling, showcasing a comprehensive approach to addressing challenges within multimodal mathematical reasoning.</p><h4 class="relative group">DualMath-1.1M<div id=dualmath-11m class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#dualmath-11m aria-label=Anchor>#</a></span></h4><p>The proposed DualMath-1.1M dataset is a crucial contribution to advancing multimodal mathematical reasoning. <strong>Its focus on dual-view process supervision</strong>‚Äîincorporating both logical correctness and visual accuracy‚Äîaddresses limitations in existing datasets which primarily focus on logical accuracy alone. By synthesizing data with targeted hallucination injection and binary error localization, DualMath-1.1M offers a sophisticated training resource that enables models to not only produce correct answers, but also robust and reliable reasoning steps. This dual-view approach directly addresses the complexity introduced by multimodal data, which may contain inconsistencies between textual and visual information. The dataset&rsquo;s impact extends beyond immediate performance gains; it pushes the boundary of test-time scaling methods by offering a rigorous means to evaluate and enhance reasoning processes. The success of DualMath-1.1M in training a verifier model (URSA-RM-7B) showcases its potential in improving the efficiency and reliability of future multimodal mathematical reasoning systems.</p><h4 class="relative group">Test-Time Scaling<div id=test-time-scaling class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#test-time-scaling aria-label=Anchor>#</a></span></h4><p>Test-time scaling in large language models (LLMs) focuses on improving reasoning performance <strong>without retraining</strong> the model. This is crucial for deploying LLMs in resource-constrained settings or when rapid adaptation is needed. The paper explores this by introducing a data synthesis strategy that generates process annotation datasets. This approach moves beyond simply producing correct answers, <strong>emphasizing the generation of high-quality reasoning trajectories</strong>. By training a separate verifier model, the system enhances the selection of correct reasoning paths at inference time, leading to improved accuracy and robustness. This dual-pronged approach, combining data augmentation and model verification, represents a <strong>significant step forward</strong> in efficient LLM deployment and demonstrates the potential for achieving substantial performance gains without the need for additional training data or increased model size.</p><h4 class="relative group">Future Work<div id=future-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-work aria-label=Anchor>#</a></span></h4><p>Future research directions stemming from this work on multimodal mathematical reasoning could explore several promising avenues. <strong>Extending the model&rsquo;s capabilities to handle even more complex mathematical problems and diverse problem types</strong> is crucial. Investigating <strong>improved data synthesis techniques</strong> that generate more sophisticated and nuanced reasoning trajectories is key for improving model accuracy. This includes exploring methods to better capture the subtleties of visual information within the context of mathematical problems. Additionally, <strong>developing more robust and effective verification methods</strong> is essential to ensure accuracy and enhance test-time scaling. The current verification model could be enhanced by incorporating more sophisticated evaluation metrics and potentially incorporating external knowledge bases. Finally, <strong>exploring alternative architectures and training methodologies</strong> may reveal further performance improvements. This could include exploring the integration of other advanced techniques like reinforcement learning and self-supervised learning to further enhance the model&rsquo;s capabilities.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x2.png alt></figure></p><blockquote><p>üîº This figure shows the data sources used to train the URSA-7B model. It breaks down the composition of the URSA-alignment-960K dataset used in the vision-language alignment phase and the MMathCoT-1M dataset used in the subsequent supervised fine-tuning (SFT) phase. The figure visually represents the percentage contribution of each dataset to the overall training data. It highlights the diversity of sources, including datasets focusing on various mathematical problem types and formats (e.g., geometry, word problems, and tables).</p><details><summary>read the caption</summary>Figure 2: Data sources used by the URSA-7B model during the VL-alignment and SFT phases.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x3.png alt></figure></p><blockquote><p>üîº This figure illustrates the process of generating high-quality chain-of-thought (CoT) reasoning data for multimodal mathematics. It uses Gemini-1.5-Flash-002 to augment existing datasets in three ways: CoT distillation, trajectory rewriting, and format unification. The figure shows examples of how input data from three different sources is transformed using these techniques to create a more consistent and comprehensive CoT dataset for training and validating multimodal mathematical reasoning models. Each step demonstrates how Gemini-1.5-Flash-002 is used to enhance the reasoning process, leading to a unified, improved CoT format suitable for fine-tuning.</p><details><summary>read the caption</summary>Figure 3: CoT augmentation and verifying for multimodal mathematical data from three type of sources using Gemini-1.5-Flash-002.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x4.png alt></figure></p><blockquote><p>üîº This figure demonstrates the process of inserting misinterpretations into a reasoning chain. The example shows a chart about public opinion on labor unions, broken down by age, education level, and political leaning. The model initially extracts information correctly, identifying that certain groups have more favorable views toward unions. However, a misinterpretation is then introduced, such as incorrectly interpreting the data for one group. Following the misinterpretation, the model continues to reason and draw incorrect conclusions based on this flawed interpretation. This illustrates how the engine creates training data by highlighting logical errors stemming from visual misinterpretations.</p><details><summary>read the caption</summary>Figure 4: Demonstration of Misinterpretation Insertion Engine.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x5.png alt></figure></p><blockquote><p>üîº This figure illustrates the three-stage training process for the URSA-7B and URSA-RM-7B models. Stage 1 involves vision-language alignment using the URSA-alignment-960K dataset, focusing on aligning the vision encoder and language model. Stage 2 performs mathematical instruction fine-tuning on the MMathCoT-1M dataset, enhancing the model&rsquo;s chain-of-thought (CoT) reasoning capabilities. Finally, Stage 3 employs dual-view process supervision using the DualMath-1.1M dataset, training a verifier model (URSA-RM-7B) to enhance the reasoning at test time. The figure clearly shows which model components are frozen and which are trained during each stage.</p><details><summary>read the caption</summary>Figure 5: An illustration of the training process for URSA-7B and URSA-RM-7B, with data from the three stages coming from URSA-alignment-960K, MMathCoT-1M, and DualMath-1.1M, respectively. The modules that are frozen and those that need to be trained are distinguished in each stage.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x8.png alt></figure></p><blockquote><p>üîº This figure compares the performance of different methods for improving the accuracy of chain-of-thought reasoning in mathematical problem-solving, specifically focusing on two benchmarks: MathVerse and MathVista-GPS. It shows how the &lsquo;pass@N&rsquo; metric (the percentage of times a model correctly answers a question within N attempts) and the &lsquo;best-of-N&rsquo; metric (choosing the best answer among N attempts) change with the number of samples (N). The methods compared likely include the baseline model, a self-consistency approach, and the URSA-RM-7B verifier model, which are described in the paper. The results visualize how using multiple samples and the verifier can lead to significant improvements in accuracy compared to relying on a single answer.</p><details><summary>read the caption</summary>Figure 6: Pass@N and Best-of-N results comparison on MathVerse and MathVista-GPS.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x9.png alt></figure></p><blockquote><p>üîº This figure presents the results of an ablation study investigating the impact of chain-of-thought (CoT) augmentation during the mathematical instruction fine-tuning (Math SFT) stage on the MathVista testmini dataset. It shows how the performance of the model changes when CoT augmentation is removed, providing a quantitative analysis of the contribution of CoT reasoning to the model&rsquo;s capabilities on various subtasks within MathVista. This allows for a better understanding of the effectiveness of the CoT augmentation strategy.</p><details><summary>read the caption</summary>Figure 7: Ablation Study w.r.t CoT Augmentation during Math SFT Stage on the MathVista testmini Set.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x10.png alt></figure></p><blockquote><p>üîº This figure shows the ablation study with respect to chain-of-thought (CoT) augmentation during the mathematical instruction fine-tuning (Math SFT) stage on the MathVerse benchmark. It visually compares the performance of the model with and without CoT augmentation across various sub-tasks within the MathVerse benchmark. The graph likely shows accuracy or a similar performance metric on the y-axis, and different sub-tasks or aspects of the MathVerse benchmark on the x-axis. This allows readers to see how much the addition of CoT augmentation improves the model&rsquo;s performance on each MathVerse sub-task.</p><details><summary>read the caption</summary>Figure 8: Ablation Study w.r.t CoT Augmentation during Math SFT Stage on the MathVerse benchmark.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x11.png alt></figure></p><blockquote><p>üîº Prompt ùí´ùíû for CoT distillation is used to generate chain-of-thought (CoT) reasoning from answer-only data. The prompt instructs the model to produce a step-by-step solution that leads to the given answer, emphasizing the importance of maintaining consistency between the generated reasoning and the provided answer. The prompt also ensures that the model doesn&rsquo;t request additional information or express uncertainty, focusing instead on a clear and concise solution.</p><details><summary>read the caption</summary>Figure 9: Prompt ùí´ùíûsubscriptùí´ùíû\mathcal{P}_{\mathcal{C}}caligraphic_P start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT used for CoT distillation on answer-only source data.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x12.png alt></figure></p><blockquote><p>üîº This figure shows the prompt used in the paper for the CoT solution rewriting task on analysis-formatted data. The prompt instructs the model to rewrite a given solution while maintaining the correctness of both the process and the final answer. It emphasizes the need for semantically coherent transcription and forbids the model from making requests or altering parts of the given solution.</p><details><summary>read the caption</summary>Figure 10: Prompt ùí´‚Ñõsubscriptùí´‚Ñõ\mathcal{P}_{\mathcal{R}}caligraphic_P start_POSTSUBSCRIPT caligraphic_R end_POSTSUBSCRIPT used for CoT solution rewriting on analysis-formatted source data.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x13.png alt></figure></p><blockquote><p>üîº Prompt ùí´‚Ñ± used for unifying solution format across style-varied source data. This prompt is used in the data synthesis process for the instruction fine-tuning of the URSA-7B model. The prompt instructs the model to convert a formal mathematical solution into a natural language explanation, ensuring clarity, conciseness, and adherence to the original solution&rsquo;s steps and final answer, avoiding modifications or reinterpretations.</p><details><summary>read the caption</summary>Figure 11: Prompt ùí´‚Ñ±subscriptùí´‚Ñ±\mathcal{P}_{\mathcal{F}}caligraphic_P start_POSTSUBSCRIPT caligraphic_F end_POSTSUBSCRIPT used for unifying solution format across style-varied source data.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x14.png alt></figure></p><blockquote><p>üîº This figure shows the prompt used for evaluating the quality of chain-of-thought (CoT) augmentation responses. The prompt guides the evaluator to assess the response based on two key criteria: solution fidelity (whether the reasoning is sound and free of speculation, and whether the final conclusion matches the given standard answer) and solution consistency (whether the reasoning steps logically lead to the answer without discrepancies). The evaluator is instructed to provide a judgment of &lsquo;yes&rsquo; or &rsquo;no&rsquo; based on this evaluation.</p><details><summary>read the caption</summary>Figure 12: Prompt used for checking CoT augmentation response based on consistency and correction.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x15.png alt></figure></p><blockquote><p>üîº This figure presents the prompt used for the misinterpretation insertion engine in the Dual-view Process Supervised Data Synthesis stage. The prompt instructs the model to introduce errors into a geometry problem solution by misreading the diagram and producing an incorrect answer. The process involves three stages: analyzing the correct solution, introducing a misinterpretation, and continuing reasoning with the misinterpretation to arrive at an incorrect answer. The prompt emphasizes that the misreading should be integrated naturally into the solution without explicit statements about making a misinterpretation, and the final answer should be marked with &lsquo;‚Ä†Answer:&rsquo;.</p><details><summary>read the caption</summary>Figure 13: Prompt used for inserting interpretation into geometry-related samples.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x16.png alt></figure></p><blockquote><p>üîº This figure shows the prompt used in the misinterpretation insertion engine for function and statistics related samples. The prompt instructs the model to introduce errors into a solution by misinterpreting a coordinate axis or chart. The model must identify areas in the solution where diagram information is extracted, choose one area to introduce a misinterpretation, and continue the reasoning process to derive an incorrect answer. The response should be natural and avoid explicitly mentioning misinterpretations. The prompt includes tags to mark correct and incorrect steps and specifies that the final answer should not be tagged. The misinterpretation action must be consistent with the plan outlined in the prompt.</p><details><summary>read the caption</summary>Figure 14: Prompt used for inserting interpretation into function and statistics-related samples.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x17.png alt></figure></p><blockquote><p>üîº This figure shows a geometry problem from the MathVista-GPS dataset. The problem involves a quadrilateral ABCD where AD = 6, AB = 4, and DE bisects angle ADC, intersecting BC at point E. The question asks for the length of BE. The figure displays several different attempts to solve this problem using different methods. These attempts include a correct solution, and solutions produced by GPT-40, Gemini-1.5-Flash-002, MultiMath-7B, Math-LLaVA-13B, and URSA-7B. The figure highlights the different approaches and results obtained by each method, illustrating the variety of reasoning paths used to solve geometry problems and the different levels of accuracy achieved.</p><details><summary>read the caption</summary>Figure 15: Case on MathVista-GPS.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.04686/x18.png alt></figure></p><blockquote><p>üîº The figure displays a geometry problem from the MathVerse benchmark dataset. The problem involves a circle with a tangent line and several angles. The question asks for the length of a specific line segment (AP), given the length of another line segment (OP) and the measure of a particular angle (‚à†BOC). Different models&rsquo; solutions and their correctness are compared and illustrated.</p><details><summary>read the caption</summary>Figure 16: Case on MathVerse.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>#Params</th><th>Strict AVG ‚Üë</th><th>Strict IK ‚Üì</th><th>Strict IG ‚Üì</th><th>Strict CM ‚Üë</th><th>Strict RM ‚Üì</th><th>Loose AVG ‚Üë</th><th>Loose IK ‚Üì</th><th>Loose IG ‚Üì</th><th>Loose CM ‚Üë</th><th>Loose RM ‚Üì</th></tr></thead><tbody><tr><td><em>Closed-source MLLMs</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Qwen-VL-Max [2023]</td><td>-</td><td>10.5</td><td>65.1</td><td>7.6</td><td>6.7</td><td>75.5</td><td>25.5</td><td>65.1</td><td>7.6</td><td>21.7</td><td>20.3</td></tr><tr><td>Gemini-1.5-Pro [2023]</td><td>-</td><td>26.4</td><td>42.9</td><td>11.2</td><td>20.8</td><td>54.8</td><td>46.0</td><td>42.9</td><td>11.2</td><td>40.4</td><td>12.0</td></tr><tr><td>GPT-4V [2023]</td><td>-</td><td>31.1</td><td>39.8</td><td>14.5</td><td>23.8</td><td>47.9</td><td>51.4</td><td>39.8</td><td>14.5</td><td>44.2</td><td>3.3</td></tr><tr><td>GPT-4o [2024]</td><td>-</td><td>42.9</td><td>31.2</td><td>15.2</td><td>35.2</td><td>34.2</td><td>60.6</td><td>31.2</td><td>15.2</td><td>53.0</td><td>1.1</td></tr><tr><td><em>Open-source General MLLMs</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLaVA-1.6 [2024c]</td><td>7B</td><td>3.3</td><td>78.3</td><td>2.5</td><td>2.1</td><td>89.1</td><td>13.8</td><td>78.3</td><td>2.5</td><td>12.6</td><td>34.7</td></tr><tr><td>LLaVA-1.6 [2024c]</td><td>13B</td><td>5.2</td><td>69.1</td><td>3.2</td><td>3.6</td><td>86.9</td><td>22.0</td><td>69.1</td><td>3.2</td><td>20.4</td><td>26.2</td></tr><tr><td>InternVL-Chat-V1.5 [2024a]</td><td>26B</td><td>12.7</td><td>56.4</td><td>10.5</td><td>7.4</td><td>77.6</td><td>31.0</td><td>56.4</td><td>10.5</td><td>25.7</td><td>22.4</td></tr><tr><td>LLaVA-NeXT [2024c]</td><td>72B</td><td>13.4</td><td>58.9</td><td>7.1</td><td>9.9</td><td>71.0</td><td>31.5</td><td>58.9</td><td>7.1</td><td>28.0</td><td>17.9</td></tr><tr><td>DeepSeek-VL [2024]</td><td>7B</td><td>6.3</td><td>69.1</td><td>4.6</td><td>4.0</td><td>84.8</td><td>21.0</td><td>69.1</td><td>4.6</td><td>18.7</td><td>29.0</td></tr><tr><td>Phi3-Vision [2024]</td><td>4.2B</td><td>10.6</td><td>58.9</td><td>9.0</td><td>6.1</td><td>81.1</td><td>29.8</td><td>58.9</td><td>9.0</td><td>25.3</td><td>21.3</td></tr><tr><td>GLM-4V-9B [2024]</td><td>9B</td><td>14.9</td><td>53.0</td><td>9.5</td><td>10.1</td><td>73.1</td><td>35.1</td><td>53.0</td><td>9.5</td><td>30.3</td><td>19.3</td></tr><tr><td>InternLM-XComposer2-VL [2024a]</td><td>7B</td><td>12.7</td><td>56.4</td><td>10.5</td><td>7.4</td><td>77.6</td><td>31.0</td><td>56.4</td><td>10.5</td><td>25.7</td><td>22.4</td></tr><tr><td>InternVL2-8B [2024b]</td><td>8B</td><td>26.6</td><td>45.5</td><td>13.5</td><td>19.8</td><td>51.6</td><td>44.9</td><td>45.5</td><td>13.5</td><td>38.1</td><td>7.0</td></tr><tr><td>Qwen2-VL [2024a]</td><td>7B</td><td>25.6</td><td>47.1</td><td>14.7</td><td>18.3</td><td>52.2</td><td>43.0</td><td>47.1</td><td>14.7</td><td>35.6</td><td>7.0</td></tr><tr><td><em>Open-source Math MLLMs</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>G-LLaVA [2023]</td><td>13B</td><td>6.5</td><td>64.2</td><td>4.6</td><td>4.2</td><td>86.6</td><td>22.3</td><td>64.2</td><td>4.6</td><td>20.0</td><td>36.0</td></tr><tr><td>Math-LLaVA [2024]</td><td>13B</td><td>11.1</td><td>62.1</td><td>3.6</td><td>9.3</td><td>72.8</td><td>31.3</td><td>62.1</td><td>3.6</td><td>29.5</td><td>13.9</td></tr><tr><td>Math-PUMA-Qwen2-7B [2024]</td><td>7B</td><td>19.2</td><td>47.8</td><td>13.7</td><td>12.4</td><td>67.8</td><td>41.0</td><td>47.8</td><td>13.7</td><td>34.1</td><td>11.4</td></tr><tr><td>Math-PUMA-DeepSeek-Math-7B [2024]</td><td>7B</td><td>15.6</td><td>56.0</td><td>7.2</td><td>12.0</td><td>67.4</td><td>35.8</td><td>56.0</td><td>7.2</td><td>32.2</td><td>12.4</td></tr><tr><td>InfiMM-Math [2024]</td><td>7B</td><td>20.6</td><td>48.8</td><td>12.2</td><td>15.2</td><td>61.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>URSA-7B</td><td>7B</td><td>32.2</td><td>37.5</td><td>10.7</td><td>26.9</td><td>48.2</td><td>53.5</td><td>37.5</td><td>10.7</td><td>48.2</td><td>7.0</td></tr><tr><td>Œî over SOTA <em>Open-Source Math MLLMs</em></td><td></td><td>+11.6</td><td>+10.3</td><td>-7.1</td><td>+11.7</td><td>+13.5</td><td>+12.5</td><td>+10.3</td><td>-7.1</td><td>+14.1</td><td>+4.4</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 2 presents a comprehensive comparison of various Large Language Models (LLMs) on the WE-MATH testmini dataset, focusing on four key performance metrics: Average (AVG), Insufficient Knowledge (IK), Inadequate Generalization (IG), and Complete Mastery (CM). The models are categorized into closed-source and open-source LLMs, with the top-performing closed-source models highlighted in green and the top two open-source models highlighted in red and blue, respectively. This allows for a nuanced understanding of each model&rsquo;s strengths and weaknesses in different aspects of mathematical reasoning, providing valuable insights into the current state-of-the-art in multimodal mathematical reasoning.</p><details><summary>read the caption</summary>Table 2: The performance comparison with Closed-source MLLMs and Open-source MLLMs on four-dimensional metrics for WE-MATH testmini reasoning evaluation. The best results of Closed-source MLLMs are highlighted in green. The best and second-best results of Open-source MLLMs are highlighted in red and blue respectively.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>ALL</th><th>PG</th><th>SG</th><th>AG</th><th>AL</th><th>PT</th><th>GT</th><th>AR</th></tr></thead><tbody><tr><td><em>Closed-source MLLMs</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4o</td><td>63.7</td><td>56.8</td><td>52.0</td><td>61.0</td><td>76.9</td><td>51.8</td><td>58.1</td><td>61.5</td></tr><tr><td>Claude-3.5-Sonnet</td><td>64.8</td><td>49.9</td><td>49.3</td><td>55.3</td><td>81.0</td><td>44.1</td><td>69.4</td><td>61.2</td></tr><tr><td>Geimini-1.5-Pro</td><td>60.5</td><td>52.7</td><td>42.7</td><td>61.6</td><td>70.8</td><td>20.6</td><td>65.2</td><td>54.2</td></tr><tr><td><em>Open-source MLLMs</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Llava-v1.5-7B</td><td>16.6</td><td>10.5</td><td>7.3</td><td>19.5</td><td>6.5</td><td>8.2</td><td>32.3</td><td>10.8</td></tr><tr><td>Llava-v1.6-34B</td><td>27.1</td><td>21.4</td><td>25.3</td><td>27.6</td><td>14.9</td><td>7.6</td><td>32.7</td><td>23.1</td></tr><tr><td>Deepseek-VL-7B-chat</td><td>21.5</td><td>16.0</td><td>13.3</td><td>26.5</td><td>12.9</td><td>4.7</td><td>32.3</td><td>12.7</td></tr><tr><td>InternVL2-8B</td><td>39.7</td><td>33.9</td><td>37.3</td><td>32.5</td><td>46.9</td><td>15.9</td><td>42.1</td><td>37.3</td></tr><tr><td>Qwen2-VL</td><td>42.1</td><td>40.3</td><td>38.7</td><td>39.9</td><td>37.1</td><td>8.2</td><td>44.8</td><td>39.2</td></tr><tr><td>URSA-7B</td><td>44.7</td><td>48.1</td><td>38.0</td><td>33.7</td><td>66.9</td><td>24.7</td><td>39.2</td><td>38.5</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of URSA-7B against other open-source large language models (LLMs) specifically designed for mathematical reasoning on the DYNAMATH testmini dataset. DYNAMATH is a benchmark designed to evaluate the robustness of LLMs in multimodal mathematical reasoning across different mathematical skill areas, problem types and difficulty levels. The table presents performance scores across various subtests of the DYNAMATH dataset, allowing for a detailed comparison of the models&rsquo; abilities in different mathematical reasoning tasks.</p><details><summary>read the caption</summary>Table 3: Comparison with open-source MLLMs on DYNAMATH testmini dataset.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>N=4</th><th>N=8</th><th>N=16</th><th>N=32</th><th>N=64</th></tr></thead><tbody><tr><td><em>MathVista-GPS</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Self-Consistency</td><td>67.4</td><td>67.9</td><td>68.2</td><td>68.7</td><td>68.9</td></tr><tr><td>URSA-RM-7B</td><td><strong>68.8</strong></td><td><strong>69.7</strong></td><td><strong>70.4</strong></td><td><strong>70.7</strong></td><td><strong>70.8</strong></td></tr><tr><td><em>MathVerse</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Self-Consistency</td><td>29.1</td><td>29.7</td><td>30.1</td><td>30.2</td><td>30.2</td></tr><tr><td>URSA-RM-7B</td><td><strong>31.0</strong></td><td><strong>32.7</strong></td><td><strong>33.0</strong></td><td><strong>33.2</strong></td><td><strong>33.0</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the out-of-distribution (OOD) performance of the URSA-RM-7B model, which acts as a verifier, when evaluated on the Multimath-7B dataset. It compares the accuracy of URSA-RM-7B against a baseline method (Self-Consistency) for different sampling numbers (N). The results show how effectively URSA-RM-7B can enhance the reasoning capabilities of URSA-7B by identifying correct reasoning trajectories, particularly in OOD scenarios.</p><details><summary>read the caption</summary>Table 4: OOD performance when URSA-RM-7B works on Multimath-7B.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>N=4</th><th>N=8</th><th>N=16</th><th>N=32</th><th>N=64</th></tr></thead><tbody><tr><td><em>MathVista-GPS</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>URSA-RM-7B</td><td><strong>82.6</strong></td><td><strong>84.0</strong></td><td><strong>85.0</strong></td><td><strong>86.4</strong></td><td><strong>86.2</strong></td></tr><tr><td>URSA-RM-7B w/o ùíÆ<sub>BEL</sub></td><td>80.1</td><td>81.7</td><td>82.2</td><td>83.1</td><td>83.0</td></tr><tr><td>URSA-RM-7B w/o ùíÆ<sub>MIE</sub></td><td>81.8</td><td>83.3</td><td>84.1</td><td>85.6</td><td>85.6</td></tr><tr><td><em>MathVerse</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>URSA-RM-7B</td><td><strong>53.2</strong></td><td><strong>54.2</strong></td><td><strong>54.7</strong></td><td><strong>55.0</strong></td><td><strong>54.8</strong></td></tr><tr><td>URSA-RM-7B w/o ùíÆ<sub>BEL</sub></td><td>49.9</td><td>50.7</td><td>51.8</td><td>52.0</td><td>52.1</td></tr><tr><td>URSA-RM-7B w/o ùíÆ<sub>MIE</sub></td><td>52.8</td><td>53.7</td><td>53.8</td><td>53.9</td><td>53.8</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of an ablation study conducted on the URSA-RM-7B model. The study investigates the impact of removing specific components of the model&rsquo;s training process on its performance. Specifically, it compares the performance of the full URSA-RM-7B model against versions where either the BinaryErrorLocating (SBEL) engine or the Misinterpretation Insertion Engine (SMIE) are removed. Results are reported for two separate benchmarks, MathVista-GPS and MathVerse, indicating how the removal of each component affects overall accuracy.</p><details><summary>read the caption</summary>Table 5: Ablation study on URSA-RM-7B.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Hyperparameters & Cost</th><th>Stage 1</th><th>Stage 2</th><th>Stage 3</th></tr></thead><tbody><tr><td>Learning Rate</td><td>1e-4</td><td>1e-5</td><td>5e-6</td></tr><tr><td>Epoch</td><td>1</td><td>2</td><td>2</td></tr><tr><td>Warm-up Ratio</td><td>0.02</td><td>0.02</td><td>0.02</td></tr><tr><td>Weight Decay</td><td>0.02</td><td>0.01</td><td>0.02</td></tr><tr><td>Batch Size</td><td>64</td><td>128</td><td>128</td></tr><tr><td>Trainable Parts</td><td>Aligner</td><td>Vision Encoder, Aligner, Base LLM</td><td>Base LLM</td></tr><tr><td>Data Size</td><td>960K</td><td>1.0M</td><td>1.1M</td></tr><tr><td>Time Cost</td><td>~3.5h</td><td>~11h</td><td>~12h</td></tr></tbody></table></table></figure><blockquote><p>üîº This table details the hyperparameters used and the training time taken for each of the three stages in the training process of the URSA-7B model. It includes information such as learning rate, number of epochs, batch size, and which parts of the model were trained at each stage. The time cost is also provided for each stage.</p><details><summary>read the caption</summary>Table 6: Hyperparameter setting and training time cost.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>#Params</th><th>ALL</th><th>ALG</th><th>ARI</th><th>GEO</th><th>LOG</th><th>NUM</th><th>SCI</th><th>STA</th></tr></thead><tbody><tr><td><strong><em>Baselines</em></strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Random Choice</td><td>-</td><td>17.9</td><td>25.8</td><td>13.8</td><td>22.7</td><td>13.4</td><td>8.8</td><td>15.8</td><td>14.3</td></tr><tr><td>Human Performance</td><td>-</td><td>60.3</td><td>50.9</td><td>59.2</td><td>51.4</td><td>40.7</td><td>53.8</td><td>64.9</td><td>63.9</td></tr><tr><td><strong><em>Closed-source MLLMs</em></strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Qwen-VL-Plus (Bai et al., 2023)</td><td>-</td><td>43.3</td><td>39.1</td><td>32.0</td><td>39.3</td><td>18.9</td><td>26.4</td><td>59.0</td><td>56.1</td></tr><tr><td>GPT-4V (OpenAI, 2023)</td><td>-</td><td>49.9</td><td>53.0</td><td>49.0</td><td>51.0</td><td>21.6</td><td>20.1</td><td>63.1</td><td>55.8</td></tr><tr><td><strong><em>Open-source Genreral MLLMs</em></strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>mPLUG-Owl2-7B (Ye et al., 2023)</td><td>7B</td><td>22.2</td><td>23.6</td><td>19.2</td><td>23.9</td><td>13.5</td><td>12.7</td><td>26.3</td><td>21.4</td></tr><tr><td>LLaVA-1.5-13B (Liu et al., 2024c)</td><td>13B</td><td>25.7</td><td>19.6</td><td>28.6</td><td>17.6</td><td>10.8</td><td>27.8</td><td>33.6</td><td>22.9</td></tr><tr><td>MiniGPT-v2 (Chen et al., 2023)</td><td>7B</td><td>23.1</td><td>28.1</td><td>21.0</td><td>24.7</td><td>16.2</td><td>16.7</td><td>25.4</td><td>17.9</td></tr><tr><td>InternLM-XComposer2-VL-7B (Dong et al., 2024a)</td><td>7B</td><td>47.8</td><td>32.0</td><td>51.6</td><td>30.5</td><td>13.5</td><td>43.8</td><td>37.7</td><td>62.8</td></tr><tr><td>SPHINX-MoE (Lin et al., 2023)</td><td>8x7B</td><td>42.3</td><td>31.7</td><td>41.6</td><td>30.5</td><td>16.2</td><td>27.1</td><td>50.8</td><td>50.8</td></tr><tr><td>DeepSeek-VL (Lu et al., 2024)</td><td>7B</td><td>34.9</td><td>29.2</td><td>38.8</td><td>27.2</td><td>18.9</td><td>43.1</td><td>35.3</td><td>33.2</td></tr><tr><td>InternVL2-8B (Chen et al., 2024b)</td><td>8B</td><td>58.3</td><td>59.8</td><td>56.4</td><td>60.3</td><td>10.8</td><td>30.6</td><td>59.0</td><td>68.8</td></tr><tr><td>Qwen2-VL (Wang et al., 2024a)</td><td>7B</td><td>58.9</td><td>44.1</td><td>57.5</td><td>43.1</td><td>24.3</td><td>41.7</td><td>66.4</td><td>75.1</td></tr><tr><td><strong><em>Open-source Math MLLMs</em></strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>G-LLaVA (Gao et al., 2024)</td><td>7B</td><td>25.1</td><td>36.0</td><td>19.4</td><td>37.6</td><td>15.2</td><td>17.7</td><td>21.0</td><td>15.1</td></tr><tr><td>Math-LLaVA (Shi et al., 2024)</td><td>7B</td><td>46.6</td><td>51.5</td><td>40.7</td><td>56.2</td><td>23.3</td><td>34.7</td><td>47.7</td><td>42.3</td></tr><tr><td>Multimath-7B (Peng et al., 2024)</td><td>7B</td><td>50.0</td><td>61.9</td><td>42.2</td><td>64.9</td><td>23.3</td><td>32.6</td><td>42.6</td><td>49.2</td></tr><tr><td>Math-PUMA-Qwen2-7B (Zhuang et al., 2024)</td><td>7B</td><td>47.9</td><td>47.7</td><td>46.2</td><td>47.3</td><td>21.6</td><td>32.6</td><td>42.6</td><td>55.8</td></tr><tr><td>URSA-7B</td><td>7B</td><td>59.8</td><td>74.0</td><td>53.5</td><td>77.4</td><td>21.6</td><td>35.4</td><td>58.2</td><td>57.1</td></tr><tr><td>Œî over SOTA <em>Open-Source Math MLLMs</em></td><td>-</td><td>+9.8</td><td>+12.1</td><td>+7.3</td><td>+12.5</td><td>-1.7</td><td>+0.7</td><td>+10.5</td><td>+1.3</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 7 presents a comparison of the performance of various Large Language Models (LLMs) on the MATHVISTA testmini dataset, focusing on their mathematical reasoning abilities. It compares both closed-source and open-source LLMs, providing a detailed breakdown of their accuracy across different mathematical subtasks (Algebra, Arithmetic, Geometry, Logic, Number, Science, Statistics). The table helps in assessing the relative strengths and weaknesses of different models in various mathematical domains.</p><details><summary>read the caption</summary>Table 7: Comparison with close-source MLLMs open-source MLLMs on MATHVISTA testmini mathematics capabilities.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>#Params</th><th>S1</th><th>S2</th><th>S3</th><th>Mem UCU</th><th>Mem AL</th><th>PF CPF</th><th>PF UPF</th><th>SF CSF</th><th>SF USF</th><th>TMF BTF</th><th>TMF CCF</th><th>PD Dir</th><th>PD Pos</th><th>PD RoM</th><th>PD CCP</th></tr></thead><tbody><tr><td><em>Closed-source MLLMs</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4o (OpenAI, 2024)</td><td>-</td><td>72.8</td><td>58.1</td><td>43.6</td><td>86.6</td><td>39.1</td><td>77.4</td><td>71.6</td><td>84.5</td><td>62.3</td><td>58.7</td><td>69.4</td><td>93.1</td><td>72.7</td><td>47.5</td><td>73.3</td></tr><tr><td>GPT-4V (OpenAI, 2023)</td><td>-</td><td>65.5</td><td>49.2</td><td>38.2</td><td>82.5</td><td>38.4</td><td>70.7</td><td>60.2</td><td>76.6</td><td>56.3</td><td>57.8</td><td>67.7</td><td>79.3</td><td>57.5</td><td>47.8</td><td>63.3</td></tr><tr><td>Gemini-1.5-Pro (Team et al., 2023)</td><td>-</td><td>56.1</td><td>51.4</td><td>33.9</td><td>51.0</td><td>31.2</td><td>61.8</td><td>45.0</td><td>70.0</td><td>57.5</td><td>39.2</td><td>62.7</td><td>68.8</td><td>54.1</td><td>40.7</td><td>60.0</td></tr><tr><td>Qwen-VL-Max (Bai et al., 2023)</td><td>-</td><td>40.8</td><td>30.3</td><td>20.6</td><td>19.4</td><td>25.3</td><td>39.8</td><td>41.4</td><td>43.6</td><td>48.0</td><td>43.8</td><td>43.4</td><td>41.4</td><td>35.1</td><td>40.7</td><td>26.7</td></tr><tr><td><em>Open-source General MLLMs</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>InternVL-Chat-V1.5 (Chen et al., 2024a)</td><td>26B</td><td>49.4</td><td>30.6</td><td>28.5</td><td>44.0</td><td>29.8</td><td>52.2</td><td>52.1</td><td>44.2</td><td>48.2</td><td>47.1</td><td>65.7</td><td>50.5</td><td>36.5</td><td>36.7</td><td></td></tr><tr><td>LLaVA-1.6 (Liu et al., 2024c)</td><td>7B</td><td>23.0</td><td>20.8</td><td>15.8</td><td>18.5</td><td>20.5</td><td>16.9</td><td>29.6</td><td>15.6</td><td>18.6</td><td>42.7</td><td>24.1</td><td>17.6</td><td>43.3</td><td>28.9</td><td>26.7</td></tr><tr><td>LLaVA-1.6 (Liu et al., 2024c)</td><td>13B</td><td>29.4</td><td>25.3</td><td>32.7</td><td>21.7</td><td>23.2</td><td>23.4</td><td>34.7</td><td>25.3</td><td>26.4</td><td>37.5</td><td>41.7</td><td>26.9</td><td>28.9</td><td>37.1</td><td>30.0</td></tr><tr><td>GLM-4V-9B (GLM et al., 2024)</td><td>9B</td><td>47.3</td><td>37.2</td><td>38.2</td><td>53.4</td><td>37.0</td><td>51.3</td><td>46.5</td><td>50.6</td><td>38.2</td><td>44.1</td><td>45.2</td><td>41.0</td><td>49.3</td><td>36.8</td><td>53.3</td></tr><tr><td>MiniCPM-LLaMA3-V2.5 (Yao et al., 2024)</td><td>8B</td><td>39.8</td><td>31.1</td><td>29.7</td><td>28.6</td><td>37.0</td><td>40.8</td><td>39.8</td><td>41.0</td><td>38.6</td><td>32.0</td><td>42.7</td><td>41.0</td><td>42.7</td><td>44.0</td><td>43.3</td></tr><tr><td>LongVA (Zhang et al., 2024c)</td><td>7B</td><td>43.5</td><td>30.6</td><td>28.5</td><td>24.5</td><td>39.8</td><td>45.1</td><td>40.8</td><td>51.9</td><td>42.5</td><td>45.6</td><td>44.6</td><td>44.5</td><td>40.7</td><td>47.5</td><td>20.0</td></tr><tr><td>InternLM-XComposer2-VL (Dong et al., 2024a)</td><td>7B</td><td>47.0</td><td>33.1</td><td>33.3</td><td>31.3</td><td>46.5</td><td>47.7</td><td>42.6</td><td>51.4</td><td>43.9</td><td>41.1</td><td>50.6</td><td>65.5</td><td>53.9</td><td>55.2</td><td>40.0</td></tr><tr><td>Phi3-Vision (Abdin et al., 2024)</td><td>4.2B</td><td>42.1</td><td>34.2</td><td>27.9</td><td>28.7</td><td>16.0</td><td>47.2</td><td>38.8</td><td>50.0</td><td>44.4</td><td>28.8</td><td>31.2</td><td>48.6</td><td>49.2</td><td>26.4</td><td>50.0</td></tr><tr><td>DeepSeek-VL (Lu et al., 2024)</td><td>7B</td><td>32.6</td><td>26.7</td><td>25.5</td><td>16.6</td><td>35.1</td><td>27.3</td><td>38.0</td><td>24.2</td><td>38.7</td><td>50.0</td><td>23.3</td><td>24.5</td><td>41.0</td><td>51.7</td><td>23.3</td></tr><tr><td>InternVL2-8B (Chen et al., 2024b)</td><td>8B</td><td>59.4</td><td>43.6</td><td>35.2</td><td>71.4</td><td>20.5</td><td>62.0</td><td>55.5</td><td>67.1</td><td>57.3</td><td>54.0</td><td>60.5</td><td>58.6</td><td>63.6</td><td>44.5</td><td>50.0</td></tr><tr><td>Qwen2-VL (Wang et al., 2024a)</td><td>7B</td><td>59.1</td><td>43.6</td><td>26.7</td><td>62.7</td><td>37.2</td><td>62.6</td><td>60.8</td><td>65.7</td><td>49.2</td><td>52.5</td><td>49.2</td><td>48.1</td><td>68.2</td><td>55.0</td><td>56.7</td></tr><tr><td><em>Open-source Math MLLMs</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>G-LLaVA (Gao et al., 2023)</td><td>7B</td><td>32.4</td><td>30.6</td><td>32.7</td><td>33.3</td><td>29.1</td><td>32.0</td><td>37.9</td><td>19.6</td><td>33.5</td><td>37.1</td><td>32.8</td><td>31.2</td><td>33.2</td><td>25.6</td><td>40.0</td></tr><tr><td>Math-LLaVA (Shi et al., 2024)</td><td>13B</td><td>38.7</td><td>34.2</td><td>34.6</td><td>30.3</td><td>17.9</td><td>39.2</td><td>40.4</td><td>37.1</td><td>37.7</td><td>53.0</td><td>51.3</td><td>30.8</td><td>30.8</td><td>40.9</td><td>46.7</td></tr><tr><td>Math-PUMA-Qwen2-7B (Zhuang et al., 2024)</td><td>7B</td><td>53.3</td><td>39.4</td><td>36.4</td><td>63.5</td><td>42.5</td><td>60.2</td><td>45.9</td><td>66.2</td><td>48.6</td><td>42.3</td><td>53.5</td><td>31.2</td><td>37.7</td><td>40.4</td><td>46.7</td></tr><tr><td>MAVIS w/o DPO (Zhang et al., 2024d)</td><td>7B</td><td>56.9</td><td>37.1</td><td>33.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MAVIS (Zhang et al., 2024d)</td><td>7B</td><td>57.2</td><td>37.9</td><td>34.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>URSA-7B</td><td>7B</td><td>63.1</td><td>56.4</td><td>41.8</td><td>59.1</td><td>32.5</td><td>72.3</td><td>60.3</td><td>70.9</td><td>66.0</td><td>51.4</td><td>59.8</td><td>58.3</td><td>39.5</td><td>58.8</td><td>53.3</td></tr><tr><td>Œî over SOTA <em>Open-Source Math MLLMs</em></td><td>-</td><td>+5.9</td><td>+27.0</td><td>+5.4</td><td>-4.4</td><td>-10.0</td><td>+12.3</td><td>+14.4</td><td>+4.7</td><td>+17.4</td><td>-1.6</td><td>+6.3</td><td>+27.1</td><td>+1.8</td><td>+17.9</td><td>+6.6</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 8 presents a detailed comparison of the performance of various Large Language Models (LLMs) on the WE-MATH testmini subset. The table is organized to show the overall accuracy of each model across different problem complexities (one-step, two-step, and three-step problems). Additionally, it provides a more granular breakdown of performance based on different problem-solving strategies, offering insights into the strengths and weaknesses of each model in various mathematical reasoning approaches.</p><details><summary>read the caption</summary>Table 8: Accuracy comparison with close-source MLLMs and open-source MLLMs on WE-MATH testmini subset. First 3 columns show the overall performance on one-step, two-step and three-step problems. The other columns are used to demonstrate the performance in different problem strategies.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>#Params</th><th>ALL</th><th>Elementary School</th><th>High School</th><th>Undergraduate</th></tr></thead><tbody><tr><td><em>Closed-source MLLMs</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4o (OpenAI, 2024)</td><td>-</td><td>63.7</td><td>68.6</td><td>61.8</td><td>36.8</td></tr><tr><td>Claude-3.5-Sonnet (Anthropic, 2024)</td><td>-</td><td>64.8</td><td>66.7</td><td>62.6</td><td>33.3</td></tr><tr><td>Gemini-1.5-Pro (Team et al., 2023)</td><td>-</td><td>60.5</td><td>62.9</td><td>59.2</td><td>37.1</td></tr><tr><td><em>Open-sourced MLLMs</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Llava-v1.5-7B (Liu et al., 2024c)</td><td>7B</td><td>16.6</td><td>18.9</td><td>13.3</td><td>11.7</td></tr><tr><td>Llava-v1.6-34B (Liu et al., 2024c)</td><td>34B</td><td>27.1</td><td>35.9</td><td>23.8</td><td>16.6</td></tr><tr><td>Deepseek-VL-7B-Chat (Lu et al., 2024)</td><td>7B</td><td>21.5</td><td>28.3</td><td>19.0</td><td>16.0</td></tr><tr><td>InternVL2-8B (Chen et al., 2024b)</td><td>8B</td><td>39.7</td><td>51.1</td><td>37.4</td><td>19.6</td></tr><tr><td>Qwen2-VL (Wang et al., 2024a)</td><td>7B</td><td>42.1</td><td>47.6</td><td>42.2</td><td>24.4</td></tr><tr><td>URSA-7B</td><td>7B</td><td>44.7</td><td>53.5</td><td>44.3</td><td>41.8</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of various large language models (LLMs) on the DYNAMATH testmini dataset, categorized by knowledge level (elementary school, high school, undergraduate). It shows the overall accuracy of each model at these three knowledge levels, allowing for comparison of model performance across different levels of mathematical complexity.</p><details><summary>read the caption</summary>Table 9: Comparison with close-source MLLMs open-source MLLMs on DYNAMATH testmini based on knowledge level.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Size</th><th>Accuracy</th></tr></thead><tbody><tr><td><em>Baselines</em></td><td></td><td></td></tr><tr><td>Random Choice</td><td>-</td><td>17.1</td></tr><tr><td>Human</td><td>-</td><td>92.3</td></tr><tr><td>UniMath (Liang et al., 2023)</td><td>-</td><td>50.0</td></tr><tr><td><em>Closed-source MLLMs</em></td><td></td><td></td></tr><tr><td>GPT-4V (OpenAI, 2023)</td><td>-</td><td>45.2</td></tr><tr><td><em>Open-source MLLMs</em></td><td></td><td></td></tr><tr><td>LLaVA-1.5 (Liu et al., 2024b)</td><td>13B</td><td>20.3</td></tr><tr><td>G-LLaVA (Gao et al., 2023)</td><td>7B</td><td>64.2</td></tr><tr><td>G-LLaVA (Gao et al., 2023)</td><td>13B</td><td>67.0</td></tr><tr><td>Math-PUMA-DeepSeek-Math-7B (Zhuang et al., 2024)</td><td>7B</td><td>61.8</td></tr><tr><td>Math-PUMA-Qwen2-7B (Zhuang et al., 2024)</td><td>7B</td><td>63.6</td></tr><tr><td>Multimath (Peng et al., 2024)</td><td>7B</td><td>67.7</td></tr><tr><td>MAVIS-7B w/o DPO (Zhang et al., 2024d)</td><td>7B</td><td>66.7</td></tr><tr><td>MAVIS-7B (Zhang et al., 2024d)</td><td>7B</td><td>68.3</td></tr><tr><td>URSA-7B</td><td>7B</td><td>73.5</td></tr><tr><td>Œî over SOTA <em>Open-Source MLLMs</em></td><td>-</td><td>+5.2</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 10 presents a performance comparison of various Multimodal Large Language Models (MLLMs) on the GeoQA benchmark. GeoQA specifically tests the ability of MLLMs to solve geometric reasoning problems. The table compares the accuracy of different models, including both closed-source and open-source models, across varying model sizes (parameters). This allows for an assessment of the impact of model size on geometric reasoning capabilities.</p><details><summary>read the caption</summary>Table 10: Performance comparison of different MLLMs on GeoQA.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-0102eed3fe5da5bb91471a7b2dfd47e2 class=gallery><img src=https://ai-paper-reviewer.com/2501.04686/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.04686/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04686/&amp;title=URSA:%20Understanding%20and%20Verifying%20Chain-of-thought%20Reasoning%20in%20Multimodal%20Mathematics" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04686/&amp;text=URSA:%20Understanding%20and%20Verifying%20Chain-of-thought%20Reasoning%20in%20Multimodal%20Mathematics" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04686/&amp;subject=URSA:%20Understanding%20and%20Verifying%20Chain-of-thought%20Reasoning%20in%20Multimodal%20Mathematics" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2501.04686/index.md",oid_likes="likes_paper-reviews/2501.04686/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2501.04144/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Chirpy3D: Continuous Part Latents for Creative 3D Bird Generation</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-01-07T00:00:00+00:00>7 January 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2501.04689/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-01-08T00:00:00+00:00>8 January 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title=Tags>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>