[{"content": "| Model | #Params | MathVista ALL | MathVista GPS | MathVista MWP | MathVista FQA | MathVista TQA | MathVista VQA | MathVerse ALL | MathVerse TD | MathVerse TL | MathVerse TO | MathVerse VI | MathVerse VD | MathVerse VO |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| *Baselines* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Random | - | 17.9 | 21.6 | 3.8 | 18.2 | 19.6 | 26.3 | 12.4 | 12.4 | 12.4 | 12.4 | 12.4 | 12.4 | 12.4 |\n| Human | - | 60.3 | 48.4 | 73.0 | 59.7 | 63.2 | 55.9 | 64.9 | 71.2 | 70.9 | 41.7 | 61.4 | 68.3 | 66.7 |\n| *Closed-Source MLLMs* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o [2024] | - | 63.8 | - | - | - | - | - | - | - | - | - | - | - | - |\n| GPT-4V [2023] | - | 49.9 | 50.5 | 57.5 | 43.1 | 65.2 | 38.0 | 54.4 | 63.1 | 56.6 | 60.3 | 51.4 | 32.8 | 50.3 |\n| Gemini-1.5-002-Flash [2023] | - | 58.4 | - | - | - | - | - | - | - | - | - | - | - | - |\n| Gemini-1.5-Pro [2023] | - | 63.9 | - | - | - | - | - | 35.3 | 39.8 | 34.7 | 44.5 | 32.0 | 36.8 | 33.3 |\n| Claude-3.5-Sonnet [2024] | - | 67.7 | - | - | - | - | - | - | - | - | - | - | - |  |\n| Qwen-VL-Plus [2023] | - | 43.3 | 35.5 | 31.2 | 54.6 | 48.1 | 51.4 | 21.3 | 26.0 | 21.2 | 25.2 | 18.5 | 19.1 | 21.8 |\n| *Open-Source General MLLMs* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| mPLUG-Owl2-7B [2023] | 7B | 22.2 | 23.6 | 10.2 | 22.7 | 27.2 | 27.9 | 10.3 | 11.6 | 11.4 | 13.8 | 11.1 | 9.4 | 8.0 |\n| MiniGPT4-7B [2023] | 7B | 23.1 | 26.0 | 13.4 | 18.6 | 30.4 | 30.2 | 12.2 | 12.3 | 12.9 | 13.4 | 12.5 | 14.8 | 8.7 |\n| LLaVA-1.5-13B [2024b] | 13B | 27.7 | 22.7 | 18.9 | 23.8 | 43.0 | 30.2 | 12.7 | 17.1 | 12.0 | 22.6 | 12.6 | 12.7 | 9.0 |\n| SPHINX-V2-13B [2023] | 13B | 36.7 | 16.4 | 23.1 | 54.6 | 41.8 | 43.0 | 16.1 | 20.8 | 14.1 | 14.0 | 16.4 | 15.6 | 16.2 |\n| LLaVA-NeXT-34B [2024c] | 34B | 46.5 | - | - | - | - | - | 34.6 | 49.0 | 37.6 | 30.1 | 35.2 | 28.9 | 22.4 |\n| InternLM-XComposer2-VL [2024a] | 7B | 57.6 | 63.0 | 73.7 | 55.0 | 56.3 | 39.7 | 25.9 | 36.9 | 28.3 | 42.5 | 20.1 | 24.4 | 19.8 |\n| Deepseek-VL [2024] | 7B | 34.9 | 28.4 | 55.9 | 26.8 | 32.9 | 34.6 | 19.3 | 23.0 | 23.2 | 23.1 | 20.2 | 18.4 | 11.8 |\n| InternVL2-8B [2024b] | 8B | 58.3 | 62.0 | 59.1 | 58.7 | 61.4 | 49.7 | 35.9 | 39.0 | 33.8 | 36.0 | 32.2 | 30.9 | 27.7 |\n| Qwen2-VL [2024a] | 7B | 58.9 | 40.9 | 64.0 | 69.1 | 60.1 | 58.1 | 33.6 | 37.4 | 33.5 | 35.0 | 31.3 | 30.3 | 28.1 |\n| *Open-Source Math MLLMs* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| G-LLaVA-7B [2023] | 7B | 25.1 | 48.7 | 3.6 | 19.1 | 25.0 | 28.7 | 16.6 | 20.9 | 20.7 | 21.1 | 17.2 | 14.6 | 9.4 |\n| Math-LLaVA-13B [2024] | 13B | 46.6 | 57.7 | 56.5 | 37.2 | 51.3 | 33.5 | 22.9 | 27.3 | 24.9 | 27.0 | 24.5 | 21.7 | 16.1 |\n| Math-PUMA-Qwen2-7B [2024] | 7B | 47.9 | 48.1 | 68.3 | 46.5 | 46.2 | 30.2 | 33.6 | 42.1 | 35.0 | 39.8 | 33.4 | 31.6 | 26.0 |\n| Math-PUMA-DeepSeek-Math [2024] | 7B | 44.7 | 39.9 | 67.7 | 42.8 | 42.4 | 31.3 | 31.8 | 43.4 | 35.4 | 47.5 | 33.6 | 31.6 | 14.7 |\n| MAVIS-7B [2024d] | 7B | - | 64.1 | - | - | - | - | 35.2 | 43.2 | 37.2 | - | 34.1 | 29.7 | 31.8 |\n| InfiMM-Math [2024] | 7B | - | - | - | - | - | - | 34.5 | 46.7 | 32.4 | - | 38.1 | 32.4 | 15.8 |\n| MultiMath-7B [2024] | 7B | 50.0 | 66.8 | 61.8 | 40.1 | 50.0 | 33.0 | 27.7 | 34.8 | 30.8 | 35.3 | 28.1 | 25.9 | 15.0 |\n| URSA-7B | 7B | 59.8 | 79.3 | 75.3 | 44.6 | 63.9 | 40.2 | 45.7 | 55.3 | 48.3 | 51.8 | 46.4 | 43.9 | 28.6 |\n| \u0394 over SOTA Open-Source Math MLLMs | - | +9.8 | +12.5 | +7.0 | -1.9 | +12.6 | +6.7 | +10.5 | +8.6 | +11.1 | +4.3 | +8.3 | +11.5 | -3.2 |", "caption": "Table 1: Comparison with closed-source MLLMs and open-source MLLMs on MATHVISTA testmini and MATHVERSE testmini. The best is bold, and the runner-up is underline. The best results of Closed-source MLLMs are highlighted in green. The best and second-best results of Open-source MLLMs are highlighted in red and blue respectively.", "description": "This table compares the performance of various large language models (LLMs) on two mathematical reasoning benchmarks: MATHVISTA and MATHVERSE.  The models are categorized into closed-source and open-source, with open-source models further divided into general-purpose and those specifically designed for mathematical reasoning.  Performance is measured across several sub-tasks within each benchmark and is indicated by numerical scores.  The table highlights the best-performing model for each task in bold, and the second-best is underlined.  To help distinguish performance levels, the best closed-source model results are shown in green, while the best and second-best open-source model results are shown in red and blue, respectively.", "section": "4. Experiment"}, {"content": "| Model | #Params | Strict AVG \u2191 | Strict IK \u2193 | Strict IG \u2193 | Strict CM \u2191 | Strict RM \u2193 | Loose AVG \u2191 | Loose IK \u2193 | Loose IG \u2193 | Loose CM \u2191 | Loose RM \u2193 |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| *Closed-source MLLMs* |  |  |  |  |  |  |  |  |  |  |  |\n| Qwen-VL-Max [2023] | - | 10.5 | 65.1 | 7.6 | 6.7 | 75.5 | 25.5 | 65.1 | 7.6 | 21.7 | 20.3 |\n| Gemini-1.5-Pro [2023] | - | 26.4 | 42.9 | 11.2 | 20.8 | 54.8 | 46.0 | 42.9 | 11.2 | 40.4 | 12.0 |\n| GPT-4V [2023] | - | 31.1 | 39.8 | 14.5 | 23.8 | 47.9 | 51.4 | 39.8 | 14.5 | 44.2 | 3.3 |\n| GPT-4o [2024] | - | 42.9 | 31.2 | 15.2 | 35.2 | 34.2 | 60.6 | 31.2 | 15.2 | 53.0 | 1.1 |\n| *Open-source General MLLMs* |  |  |  |  |  |  |  |  |  |  |  |\n| LLaVA-1.6 [2024c] | 7B | 3.3 | 78.3 | 2.5 | 2.1 | 89.1 | 13.8 | 78.3 | 2.5 | 12.6 | 34.7 |\n| LLaVA-1.6 [2024c] | 13B | 5.2 | 69.1 | 3.2 | 3.6 | 86.9 | 22.0 | 69.1 | 3.2 | 20.4 | 26.2 |\n| InternVL-Chat-V1.5 [2024a] | 26B | 12.7 | 56.4 | 10.5 | 7.4 | 77.6 | 31.0 | 56.4 | 10.5 | 25.7 | 22.4 |\n| LLaVA-NeXT [2024c] | 72B | 13.4 | 58.9 | 7.1 | 9.9 | 71.0 | 31.5 | 58.9 | 7.1 | 28.0 | 17.9 |\n| DeepSeek-VL [2024] | 7B | 6.3 | 69.1 | 4.6 | 4.0 | 84.8 | 21.0 | 69.1 | 4.6 | 18.7 | 29.0 |\n| Phi3-Vision [2024] | 4.2B | 10.6 | 58.9 | 9.0 | 6.1 | 81.1 | 29.8 | 58.9 | 9.0 | 25.3 | 21.3 |\n| GLM-4V-9B [2024] | 9B | 14.9 | 53.0 | 9.5 | 10.1 | 73.1 | 35.1 | 53.0 | 9.5 | 30.3 | 19.3 |\n| InternLM-XComposer2-VL [2024a] | 7B | 12.7 | 56.4 | 10.5 | 7.4 | 77.6 | 31.0 | 56.4 | 10.5 | 25.7 | 22.4 |\n| InternVL2-8B [2024b] | 8B | 26.6 | 45.5 | 13.5 | 19.8 | 51.6 | 44.9 | 45.5 | 13.5 | 38.1 | 7.0 |\n| Qwen2-VL [2024a] | 7B | 25.6 | 47.1 | 14.7 | 18.3 | 52.2 | 43.0 | 47.1 | 14.7 | 35.6 | 7.0 |\n| *Open-source Math MLLMs* |  |  |  |  |  |  |  |  |  |  |  |\n| G-LLaVA [2023] | 13B | 6.5 | 64.2 | 4.6 | 4.2 | 86.6 | 22.3 | 64.2 | 4.6 | 20.0 | 36.0 |\n| Math-LLaVA [2024] | 13B | 11.1 | 62.1 | 3.6 | 9.3 | 72.8 | 31.3 | 62.1 | 3.6 | 29.5 | 13.9 |\n| Math-PUMA-Qwen2-7B [2024] | 7B | 19.2 | 47.8 | 13.7 | 12.4 | 67.8 | 41.0 | 47.8 | 13.7 | 34.1 | 11.4 |\n| Math-PUMA-DeepSeek-Math-7B [2024] | 7B | 15.6 | 56.0 | 7.2 | 12.0 | 67.4 | 35.8 | 56.0 | 7.2 | 32.2 | 12.4 |\n| InfiMM-Math [2024] | 7B | 20.6 | 48.8 | 12.2 | 15.2 | 61.7 | - | - | - | - | - |\n| URSA-7B | 7B | 32.2 | 37.5 | 10.7 | 26.9 | 48.2 | 53.5 | 37.5 | 10.7 | 48.2 | 7.0 |\n| \u0394 over SOTA *Open-Source Math MLLMs* |  | +11.6 | +10.3 | -7.1 | +11.7 | +13.5 | +12.5 | +10.3 | -7.1 | +14.1 | +4.4 |", "caption": "Table 2: The performance comparison with Closed-source MLLMs and Open-source MLLMs on four-dimensional metrics for WE-MATH testmini reasoning evaluation. The best results of Closed-source MLLMs are highlighted in green. The best and second-best results of Open-source MLLMs are highlighted in red and blue respectively.", "description": "Table 2 presents a comprehensive comparison of various Large Language Models (LLMs) on the WE-MATH testmini dataset, focusing on four key performance metrics: Average (AVG), Insufficient Knowledge (IK), Inadequate Generalization (IG), and Complete Mastery (CM).  The models are categorized into closed-source and open-source LLMs, with the top-performing closed-source models highlighted in green and the top two open-source models highlighted in red and blue, respectively. This allows for a nuanced understanding of each model's strengths and weaknesses in different aspects of mathematical reasoning, providing valuable insights into the current state-of-the-art in multimodal mathematical reasoning.", "section": "4. Experiment"}, {"content": "| Model | ALL | PG | SG | AG | AL | PT | GT | AR |\n|---|---|---|---|---|---|---|---|---|\n| _Closed-source MLLMs_ |  |  |  |  |  |  |  |  |\n| GPT-4o | 63.7 | 56.8 | 52.0 | 61.0 | 76.9 | 51.8 | 58.1 | 61.5 |\n| Claude-3.5-Sonnet | 64.8 | 49.9 | 49.3 | 55.3 | 81.0 | 44.1 | 69.4 | 61.2 |\n| Geimini-1.5-Pro | 60.5 | 52.7 | 42.7 | 61.6 | 70.8 | 20.6 | 65.2 | 54.2 |\n| _Open-source MLLMs_ |  |  |  |  |  |  |  |  |\n| Llava-v1.5-7B | 16.6 | 10.5 | 7.3 | 19.5 | 6.5 | 8.2 | 32.3 | 10.8 |\n| Llava-v1.6-34B | 27.1 | 21.4 | 25.3 | 27.6 | 14.9 | 7.6 | 32.7 | 23.1 |\n| Deepseek-VL-7B-chat | 21.5 | 16.0 | 13.3 | 26.5 | 12.9 | 4.7 | 32.3 | 12.7 |\n| InternVL2-8B | 39.7 | 33.9 | 37.3 | 32.5 | 46.9 | 15.9 | 42.1 | 37.3 |\n| Qwen2-VL | 42.1 | 40.3 | 38.7 | 39.9 | 37.1 | 8.2 | 44.8 | 39.2 |\n| URSA-7B | 44.7 | 48.1 | 38.0 | 33.7 | 66.9 | 24.7 | 39.2 | 38.5 |", "caption": "Table 3: Comparison with open-source MLLMs on DYNAMATH testmini dataset.", "description": "This table compares the performance of URSA-7B against other open-source large language models (LLMs) specifically designed for mathematical reasoning on the DYNAMATH testmini dataset.  DYNAMATH is a benchmark designed to evaluate the robustness of LLMs in multimodal mathematical reasoning across different mathematical skill areas, problem types and difficulty levels. The table presents performance scores across various subtests of the DYNAMATH dataset, allowing for a detailed comparison of the models' abilities in different mathematical reasoning tasks.", "section": "4. Experiment"}, {"content": "| Method | N=4 | N=8 | N=16 | N=32 | N=64 |\n|---|---|---|---|---|---| \n| *MathVista-GPS* |  |  |  |  |  |\n| Self-Consistency | 67.4 | 67.9 | 68.2 | 68.7 | 68.9 |\n| URSA-RM-7B | **68.8** | **69.7** | **70.4** | **70.7** | **70.8** |\n| *MathVerse* |  |  |  |  |  |\n| Self-Consistency | 29.1 | 29.7 | 30.1 | 30.2 | 30.2 |\n| URSA-RM-7B | **31.0** | **32.7** | **33.0** | **33.2** | **33.0** |", "caption": "Table 4: OOD performance when URSA-RM-7B works on Multimath-7B.", "description": "This table presents the out-of-distribution (OOD) performance of the URSA-RM-7B model, which acts as a verifier, when evaluated on the Multimath-7B dataset.  It compares the accuracy of URSA-RM-7B against a baseline method (Self-Consistency) for different sampling numbers (N). The results show how effectively URSA-RM-7B can enhance the reasoning capabilities of URSA-7B by identifying correct reasoning trajectories, particularly in OOD scenarios.", "section": "4. Experiment"}, {"content": "| Method | N=4 | N=8 | N=16 | N=32 | N=64 |\n|---|---|---|---|---|---| \n| *MathVista-GPS* |  |  |  |  |  |\n| URSA-RM-7B | **82.6** | **84.0** | **85.0** | **86.4** | **86.2** |\n| URSA-RM-7B w/o \ud835\udcae<sub>BEL</sub> | 80.1 | 81.7 | 82.2 | 83.1 | 83.0 |\n| URSA-RM-7B w/o \ud835\udcae<sub>MIE</sub> | 81.8 | 83.3 | 84.1 | 85.6 | 85.6 |\n| *MathVerse* |  |  |  |  |  |\n| URSA-RM-7B | **53.2** | **54.2** | **54.7** | **55.0** | **54.8** |\n| URSA-RM-7B w/o \ud835\udcae<sub>BEL</sub> | 49.9 | 50.7 | 51.8 | 52.0 | 52.1 |\n| URSA-RM-7B w/o \ud835\udcae<sub>MIE</sub> | 52.8 | 53.7 | 53.8 | 53.9 | 53.8 |", "caption": "Table 5: Ablation study on URSA-RM-7B.", "description": "This table presents the results of an ablation study conducted on the URSA-RM-7B model.  The study investigates the impact of removing specific components of the model's training process on its performance. Specifically, it compares the performance of the full URSA-RM-7B model against versions where either the BinaryErrorLocating (SBEL) engine or the Misinterpretation Insertion Engine (SMIE) are removed. Results are reported for two separate benchmarks, MathVista-GPS and MathVerse, indicating how the removal of each component affects overall accuracy.", "section": "5. Ablations"}, {"content": "| Hyperparameters & Cost | Stage 1 | Stage 2 | Stage 3 |\n|---|---|---|---|\n| Learning Rate | 1e-4 | 1e-5 | 5e-6 |\n| Epoch | 1 | 2 | 2 |\n| Warm-up Ratio | 0.02 | 0.02 | 0.02 |\n| Weight Decay | 0.02 | 0.01 | 0.02 |\n| Batch Size | 64 | 128 | 128 |\n| Trainable Parts | Aligner | Vision Encoder, Aligner, Base LLM | Base LLM |\n| Data Size | 960K | 1.0M | 1.1M |\n| Time Cost | ~3.5h | ~11h | ~12h |", "caption": "Table 6: Hyperparameter setting and training time cost.", "description": "This table details the hyperparameters used and the training time taken for each of the three stages in the training process of the URSA-7B model.  It includes information such as learning rate, number of epochs, batch size, and which parts of the model were trained at each stage. The time cost is also provided for each stage.", "section": "3. Model Training Process"}, {"content": "| Model | #Params | ALL | ALG | ARI | GEO | LOG | NUM | SCI | STA |\n|---|---|---|---|---|---|---|---|---|---| \n| **_Baselines_** |  |  |  |  |  |  |  |  |  |\n| Random Choice | - | 17.9 | 25.8 | 13.8 | 22.7 | 13.4 | 8.8 | 15.8 | 14.3 |\n| Human Performance | - | 60.3 | 50.9 | 59.2 | 51.4 | 40.7 | 53.8 | 64.9 | 63.9 |\n| **_Closed-source MLLMs_** |  |  |  |  |  |  |  |  |  |\n| Qwen-VL-Plus (Bai et al., 2023) | - | 43.3 | 39.1 | 32.0 | 39.3 | 18.9 | 26.4 | 59.0 | 56.1 |\n| GPT-4V (OpenAI, 2023) | - | 49.9 | 53.0 | 49.0 | 51.0 | 21.6 | 20.1 | 63.1 | 55.8 |\n| **_Open-source Genreral MLLMs_** |  |  |  |  |  |  |  |  |  |\n| mPLUG-Owl2-7B (Ye et al., 2023) | 7B | 22.2 | 23.6 | 19.2 | 23.9 | 13.5 | 12.7 | 26.3 | 21.4 |\n| LLaVA-1.5-13B (Liu et al., 2024c) | 13B | 25.7 | 19.6 | 28.6 | 17.6 | 10.8 | 27.8 | 33.6 | 22.9 |\n| MiniGPT-v2 (Chen et al., 2023) | 7B | 23.1 | 28.1 | 21.0 | 24.7 | 16.2 | 16.7 | 25.4 | 17.9 |\n| InternLM-XComposer2-VL-7B (Dong et al., 2024a) | 7B | 47.8 | 32.0 | 51.6 | 30.5 | 13.5 | 43.8 | 37.7 | 62.8 |\n| SPHINX-MoE (Lin et al., 2023) | 8x7B | 42.3 | 31.7 | 41.6 | 30.5 | 16.2 | 27.1 | 50.8 | 50.8 |\n| DeepSeek-VL (Lu et al., 2024) | 7B | 34.9 | 29.2 | 38.8 | 27.2 | 18.9 | 43.1 | 35.3 | 33.2 |\n| InternVL2-8B (Chen et al., 2024b) | 8B | 58.3 | 59.8 | 56.4 | 60.3 | 10.8 | 30.6 | 59.0 | 68.8 |\n| Qwen2-VL (Wang et al., 2024a) | 7B | 58.9 | 44.1 | 57.5 | 43.1 | 24.3 | 41.7 | 66.4 | 75.1 |\n| **_Open-source Math MLLMs_** |  |  |  |  |  |  |  |  |  |\n| G-LLaVA (Gao et al., 2024) | 7B | 25.1 | 36.0 | 19.4 | 37.6 | 15.2 | 17.7 | 21.0 | 15.1 |\n| Math-LLaVA (Shi et al., 2024) | 7B | 46.6 | 51.5 | 40.7 | 56.2 | 23.3 | 34.7 | 47.7 | 42.3 |\n| Multimath-7B (Peng et al., 2024) | 7B | 50.0 | 61.9 | 42.2 | 64.9 | 23.3 | 32.6 | 42.6 | 49.2 |\n| Math-PUMA-Qwen2-7B (Zhuang et al., 2024) | 7B | 47.9 | 47.7 | 46.2 | 47.3 | 21.6 | 32.6 | 42.6 | 55.8 |\n| URSA-7B | 7B | 59.8 | 74.0 | 53.5 | 77.4 | 21.6 | 35.4 | 58.2 | 57.1 |\n| \u0394 over SOTA _Open-Source Math MLLMs_ | - | +9.8 | +12.1 | +7.3 | +12.5 | -1.7 | +0.7 | +10.5 | +1.3 |", "caption": "Table 7: Comparison with close-source MLLMs open-source MLLMs on MATHVISTA testmini mathematics capabilities.", "description": "Table 7 presents a comparison of the performance of various Large Language Models (LLMs) on the MATHVISTA testmini dataset, focusing on their mathematical reasoning abilities. It compares both closed-source and open-source LLMs, providing a detailed breakdown of their accuracy across different mathematical subtasks (Algebra, Arithmetic, Geometry, Logic, Number, Science, Statistics). The table helps in assessing the relative strengths and weaknesses of different models in various mathematical domains.", "section": "4. Experiment"}, {"content": "| Model | #Params | S1 | S2 | S3 | Mem UCU | Mem AL | PF CPF | PF UPF | SF CSF | SF USF | TMF BTF | TMF CCF | PD Dir | PD Pos | PD RoM | PD CCP | \n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| *Closed-source MLLMs* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o (OpenAI, 2024) | - | 72.8 | 58.1 | 43.6 | 86.6 | 39.1 | 77.4 | 71.6 | 84.5 | 62.3 | 58.7 | 69.4 | 93.1 | 72.7 | 47.5 | 73.3 |\n| GPT-4V (OpenAI, 2023) | - | 65.5 | 49.2 | 38.2 | 82.5 | 38.4 | 70.7 | 60.2 | 76.6 | 56.3 | 57.8 | 67.7 | 79.3 | 57.5 | 47.8 | 63.3 |\n| Gemini-1.5-Pro (Team et al., 2023) | - | 56.1 | 51.4 | 33.9 | 51.0 | 31.2 | 61.8 | 45.0 | 70.0 | 57.5 | 39.2 | 62.7 | 68.8 | 54.1 | 40.7 | 60.0 |\n| Qwen-VL-Max (Bai et al., 2023) | - | 40.8 | 30.3 | 20.6 | 19.4 | 25.3 | 39.8 | 41.4 | 43.6 | 48.0 | 43.8 | 43.4 | 41.4 | 35.1 | 40.7 | 26.7 |\n| *Open-source General MLLMs* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| InternVL-Chat-V1.5 (Chen et al., 2024a) | 26B | 49.4 | 30.6 | 28.5 | 44.0 | 29.8 | 52.2 | 52.1 | 44.2 | 48.2 | 47.1 | 65.7 | 50.5 | 36.5 | 36.7 |\n| LLaVA-1.6 (Liu et al., 2024c) | 7B | 23.0 | 20.8 | 15.8 | 18.5 | 20.5 | 16.9 | 29.6 | 15.6 | 18.6 | 42.7 | 24.1 | 17.6 | 43.3 | 28.9 | 26.7 |\n| LLaVA-1.6 (Liu et al., 2024c) | 13B | 29.4 | 25.3 | 32.7 | 21.7 | 23.2 | 23.4 | 34.7 | 25.3 | 26.4 | 37.5 | 41.7 | 26.9 | 28.9 | 37.1 | 30.0 |\n| GLM-4V-9B (GLM et al., 2024) | 9B | 47.3 | 37.2 | 38.2 | 53.4 | 37.0 | 51.3 | 46.5 | 50.6 | 38.2 | 44.1 | 45.2 | 41.0 | 49.3 | 36.8 | 53.3 |\n| MiniCPM-LLaMA3-V2.5 (Yao et al., 2024) | 8B | 39.8 | 31.1 | 29.7 | 28.6 | 37.0 | 40.8 | 39.8 | 41.0 | 38.6 | 32.0 | 42.7 | 41.0 | 42.7 | 44.0 | 43.3 |\n| LongVA (Zhang et al., 2024c) | 7B | 43.5 | 30.6 | 28.5 | 24.5 | 39.8 | 45.1 | 40.8 | 51.9 | 42.5 | 45.6 | 44.6 | 44.5 | 40.7 | 47.5 | 20.0 |\n| InternLM-XComposer2-VL (Dong et al., 2024a) | 7B | 47.0 | 33.1 | 33.3 | 31.3 | 46.5 | 47.7 | 42.6 | 51.4 | 43.9 | 41.1 | 50.6 | 65.5 | 53.9 | 55.2 | 40.0 |\n| Phi3-Vision (Abdin et al., 2024) | 4.2B | 42.1 | 34.2 | 27.9 | 28.7 | 16.0 | 47.2 | 38.8 | 50.0 | 44.4 | 28.8 | 31.2 | 48.6 | 49.2 | 26.4 | 50.0 |\n| DeepSeek-VL (Lu et al., 2024) | 7B | 32.6 | 26.7 | 25.5 | 16.6 | 35.1 | 27.3 | 38.0 | 24.2 | 38.7 | 50.0 | 23.3 | 24.5 | 41.0 | 51.7 | 23.3 |\n| InternVL2-8B (Chen et al., 2024b) | 8B | 59.4 | 43.6 | 35.2 | 71.4 | 20.5 | 62.0 | 55.5 | 67.1 | 57.3 | 54.0 | 60.5 | 58.6 | 63.6 | 44.5 | 50.0 |\n| Qwen2-VL (Wang et al., 2024a) | 7B | 59.1 | 43.6 | 26.7 | 62.7 | 37.2 | 62.6 | 60.8 | 65.7 | 49.2 | 52.5 | 49.2 | 48.1 | 68.2 | 55.0 | 56.7 |\n| *Open-source Math MLLMs* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| G-LLaVA (Gao et al., 2023) | 7B | 32.4 | 30.6 | 32.7 | 33.3 | 29.1 | 32.0 | 37.9 | 19.6 | 33.5 | 37.1 | 32.8 | 31.2 | 33.2 | 25.6 | 40.0 |\n| Math-LLaVA (Shi et al., 2024) | 13B | 38.7 | 34.2 | 34.6 | 30.3 | 17.9 | 39.2 | 40.4 | 37.1 | 37.7 | 53.0 | 51.3 | 30.8 | 30.8 | 40.9 | 46.7 |\n| Math-PUMA-Qwen2-7B (Zhuang et al., 2024) | 7B | 53.3 | 39.4 | 36.4 | 63.5 | 42.5 | 60.2 | 45.9 | 66.2 | 48.6 | 42.3 | 53.5 | 31.2 | 37.7 | 40.4 | 46.7 |\n| MAVIS w/o DPO (Zhang et al., 2024d) | 7B | 56.9 | 37.1 | 33.2 | - | - | - | - | - | - | - | - | - | - | - | - |\n| MAVIS (Zhang et al., 2024d) | 7B | 57.2 | 37.9 | 34.6 | - | - | - | - | - | - | - | - | - | - | - | - |\n| URSA-7B | 7B | 63.1 | 56.4 | 41.8 | 59.1 | 32.5 | 72.3 | 60.3 | 70.9 | 66.0 | 51.4 | 59.8 | 58.3 | 39.5 | 58.8 | 53.3 |\n| \u0394 over SOTA *Open-Source Math MLLMs* | - | +5.9 | +27.0 | +5.4 | -4.4 | -10.0 | +12.3 | +14.4 | +4.7 | +17.4 | -1.6 | +6.3 | +27.1 | +1.8 | +17.9 | +6.6 |", "caption": "Table 8: Accuracy comparison with close-source MLLMs and open-source MLLMs on WE-MATH testmini subset. First 3 columns show the overall performance on one-step, two-step and three-step problems. The other columns are used to demonstrate the performance in different problem strategies.", "description": "Table 8 presents a detailed comparison of the performance of various Large Language Models (LLMs) on the WE-MATH testmini subset.  The table is organized to show the overall accuracy of each model across different problem complexities (one-step, two-step, and three-step problems). Additionally, it provides a more granular breakdown of performance based on different problem-solving strategies, offering insights into the strengths and weaknesses of each model in various mathematical reasoning approaches.", "section": "4.1. Experimental Setup"}, {"content": "| Model | #Params | ALL | Elementary School | High School | Undergraduate |\n|---|---|---|---|---|---| \n| *Closed-source MLLMs* |  |  |  |  |  |\n| GPT-4o (OpenAI, 2024) | - | 63.7 | 68.6 | 61.8 | 36.8 |\n| Claude-3.5-Sonnet (Anthropic, 2024) | - | 64.8 | 66.7 | 62.6 | 33.3 |\n| Gemini-1.5-Pro (Team et al., 2023) | - | 60.5 | 62.9 | 59.2 | 37.1 |\n| *Open-sourced MLLMs* |  |  |  |  |  |\n| Llava-v1.5-7B (Liu et al., 2024c) | 7B | 16.6 | 18.9 | 13.3 | 11.7 |\n| Llava-v1.6-34B (Liu et al., 2024c) | 34B | 27.1 | 35.9 | 23.8 | 16.6 |\n| Deepseek-VL-7B-Chat (Lu et al., 2024) | 7B | 21.5 | 28.3 | 19.0 | 16.0 |\n| InternVL2-8B (Chen et al., 2024b) | 8B | 39.7 | 51.1 | 37.4 | 19.6 |\n| Qwen2-VL (Wang et al., 2024a) | 7B | 42.1 | 47.6 | 42.2 | 24.4 |\n| URSA-7B | 7B | 44.7 | 53.5 | 44.3 | 41.8 |", "caption": "Table 9: Comparison with close-source MLLMs open-source MLLMs on DYNAMATH testmini based on knowledge level.", "description": "This table compares the performance of various large language models (LLMs) on the DYNAMATH testmini dataset, categorized by knowledge level (elementary school, high school, undergraduate).  It shows the overall accuracy of each model at these three knowledge levels, allowing for comparison of model performance across different levels of mathematical complexity.", "section": "4.1. Experimental Setup"}, {"content": "| Model | Size | Accuracy |\n|---|---|---|\n| *Baselines* |  |  |\n| Random Choice | - | 17.1 |\n| Human | - | 92.3 |\n| UniMath (Liang et al., 2023) | - | 50.0 |\n| *Closed-source MLLMs* |  |  |\n| GPT-4V (OpenAI, 2023) | - | 45.2 |\n| *Open-source MLLMs* |  |  |\n| LLaVA-1.5 (Liu et al., 2024b) | 13B | 20.3 |\n| G-LLaVA (Gao et al., 2023) | 7B | 64.2 |\n| G-LLaVA (Gao et al., 2023) | 13B | 67.0 |\n| Math-PUMA-DeepSeek-Math-7B (Zhuang et al., 2024) | 7B | 61.8 |\n| Math-PUMA-Qwen2-7B (Zhuang et al., 2024) | 7B | 63.6 |\n| Multimath (Peng et al., 2024) | 7B | 67.7 |\n| MAVIS-7B w/o DPO (Zhang et al., 2024d) | 7B | 66.7 |\n| MAVIS-7B (Zhang et al., 2024d) | 7B | 68.3 |\n| URSA-7B | 7B | 73.5 |\n| \u0394 over SOTA *Open-Source MLLMs* | - | +5.2 |", "caption": "Table 10: Performance comparison of different MLLMs on GeoQA.", "description": "Table 10 presents a performance comparison of various Multimodal Large Language Models (MLLMs) on the GeoQA benchmark.  GeoQA specifically tests the ability of MLLMs to solve geometric reasoning problems. The table compares the accuracy of different models, including both closed-source and open-source models, across varying model sizes (parameters). This allows for an assessment of the impact of model size on geometric reasoning capabilities.", "section": "4. Experiment"}]