{"references": [{"fullname_first_author": "Hu, E.J.", "paper_title": "Lora: Low-rank adaptation of large language models.", "publication_date": "2021-01-01", "reason": "This paper is the seminal work introducing LoRA, the foundational parameter-efficient fine-tuning technique upon which this paper builds."}, {"fullname_first_author": "Wang, A.", "paper_title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding.", "publication_date": "2019-01-01", "reason": "This paper presents the GLUE benchmark, a crucial dataset suite used extensively for evaluating NLU models, and used to evaluate GOAT."}, {"fullname_first_author": "Radford, A.", "paper_title": "Learning transferable visual models from natural language supervision.", "publication_date": "2021-01-01", "reason": "This paper details the original CLIP models; the core image model used in the GOAT experiments."}, {"fullname_first_author": "Touvron, H.", "paper_title": "Llama 2: Open foundation and fine-tuned chat models.", "publication_date": "2023-01-01", "reason": "This is the paper introducing LLaMA2, the open-source LLM architecture that GOAT experiments are conducted on."}, {"fullname_first_author": "Zellers, R.", "paper_title": "HellaSwag: Can a machine really finish your sentence?", "publication_date": "2019-01-01", "reason": "This paper introduces HellaSwag, a key task for evaluating commonsense reasoning, part of the CR experiments."}]}