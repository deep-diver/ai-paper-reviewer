[{"figure_path": "https://arxiv.org/html/2502.20321/x1.png", "caption": "Figure 1: (a): The unified tokenizer training paradigm. (b): Comparison with the unified tokenizer VILA-U in terms of ImageNet zero-shot accuracy and reconstruction FID.", "description": "Figure 1(a) illustrates the training process of a unified visual tokenizer.  It integrates both visual generation and understanding tasks by using a combined loss function: reconstruction loss to ensure accurate image reconstruction, and contrastive loss to align visual features with text captions. This unified training aims to bridge the gap between visual generation and understanding models, which often rely on separate tokenizers. Figure 1(b) presents a comparison of UniTok, the proposed unified tokenizer, against VILA-U, a state-of-the-art unified tokenizer, on two key metrics: ImageNet zero-shot accuracy (demonstrating understanding capabilities) and reconstruction FID (measuring generation quality). The results highlight UniTok's superior performance in both tasks.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.20321/x2.png", "caption": "Figure 2: An overview of UniTok. The tokenizer is trained to faithfully reconstruct the input image while aligning its discrete latent features with the text caption. For vector quantization, each visual token is split into multiple chunks, which then undergo code index lookup on corresponding sub-codebooks concurrently.", "description": "UniTok, a unified tokenizer, processes an image by first encoding it using a vision encoder. The resulting continuous token is then split into multiple chunks. Each chunk is independently quantized using a separate sub-codebook, resulting in a set of discrete tokens. This process is called multi-codebook quantization.  These discrete tokens are projected, creating a representation suitable for both image generation and understanding.  Simultaneously, the text caption undergoes processing through a text encoder. Finally, a contrastive loss is applied to align these discrete visual tokens with the text caption\u2019s representation, ensuring that the visual representation captures the semantic meaning described in the text.", "section": "3.1. Unified Supervision"}, {"figure_path": "https://arxiv.org/html/2502.20321/x3.png", "caption": "Figure 3: Roadmap to build UniTok. The blue bars illustrate the progressive changes in VQA performance from the CLIP tokenizer to the unified tokenizer, while the purple bars represent the proposed improvements in UniTok. The VQA score is measured using the average accuracy across the VQAv2, GQA, TextVQA, and POPE benchmarks. All models are trained from scratch on 512m image-text pairs from DataComp.", "description": "This figure illustrates the step-by-step improvement in Visual Question Answering (VQA) performance when developing the UniTok model. It starts with a CLIP tokenizer baseline and progressively adds components such as factorization, discretization, reconstruction loss, and multi-codebook quantization.  Each addition's effect on VQA performance, measured by averaging scores across four benchmark datasets (VQAv2, GQA, TextVQA, and POPE), is shown using blue bars. Purple bars represent the performance gains specifically due to UniTok's proposed improvements.  All models were trained on 512 million image-text pairs from the DataComp dataset.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.20321/x4.png", "caption": "Figure 4: Modified attention blocks for factorization. Modules in yellow indicate a change in the number of channels.", "description": "This figure illustrates the modified attention blocks used for factorization in UniTok. The original attention mechanism is adapted to improve the representation of factorized tokens.  The yellow highlighted modules signify alterations in the number of channels, showing how the channel dimensions change during the factorization process. This modification is crucial for maintaining compatibility with autoregressive generation, enabling the model to effectively handle sequential visual data.", "section": "3.3. UniTok"}, {"figure_path": "https://arxiv.org/html/2502.20321/extracted/6238271/fig/vis.png", "caption": "Figure 5: Images generated in a resolution of 256\u00d7256256256256\\times 256256 \u00d7 256 with our unified MLLM.", "description": "Figure 5 showcases various images generated using the UniTok unified multimodal large language model (MLLM).  These images, produced at a 256x256 pixel resolution, demonstrate the model's ability to create diverse and visually appealing outputs based on text prompts. The examples highlight a range of styles and subject matter, underscoring the MLLM's capabilities for both photorealistic imagery and creative, stylized generation.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.20321/extracted/6238271/fig/rec.png", "caption": "Figure 6: Qualitative results on image reconstruction in a resolution of 256\u00d7256256256256\\times 256256 \u00d7 256.", "description": "Figure 6 displays qualitative results demonstrating UniTok's image reconstruction capabilities at a resolution of 256x256 pixels.  The figure presents several example image pairs: the original image and its reconstruction by UniTok.  This visual comparison showcases UniTok's ability to accurately reconstruct images, including fine details such as text and facial features, even in complex scenes.", "section": "A. Image Reconstruction"}]