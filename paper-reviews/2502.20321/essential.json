{"importance": "This paper matters because it **bridges the gap between visual understanding and generation** for MLLMs. UniTok's multi-codebook quantization and attention factorization offer new directions for tokenizer design. It **achieves impressive performance** and contributes to stronger downstream task performance, which **opens avenues for better unified MLLMs**.", "summary": "UniTok: A unified tokenizer bridging the visual generation and understanding gap via multi-codebook quantization, achieving SOTA in MLLMs.", "takeaways": ["UniTok, a unified visual tokenizer, bridges the gap between visual generation and understanding.", "Multi-codebook quantization and attention factorization enhance the representational capacity of discrete tokens.", "UniTok achieves comparable or superior performance to domain-specific tokenizers and sets a new SOTA for unified autoregressive MLLMs."], "tldr": "The paper addresses the critical gap between visual generation and understanding in Multimodal Large Language Models(MLLMs). Current visual tokenizers struggle to balance fine-grained details for generation and high-level semantics for understanding, leading to performance limitations and the need for separate task-specific tokenizers. This disparity increases model complexity and hinders true integration. The research identifies that the bottleneck is from limited representational capacity of discrete tokens.\n\nTo address this, the paper introduces **UniTok**, a unified visual tokenizer employing **multi-codebook quantization** and attention factorization. This expands the latent feature space and enhances token expressiveness. UniTok achieves strong results, matching or surpassing domain-specific tokenizers in reconstruction and classification. It sets a new state-of-the-art for unified autoregressive MLLMs and is foundational for improved downstream task performance. ", "affiliation": "University of Hong Kong", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2502.20321/podcast.wav"}