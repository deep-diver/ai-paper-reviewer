[{"content": "| Methods | Face Sim. \u2191 (%) | CLIPScore \u2191 (%) | FID \u2193 | \n|---|---|---|---|\n| CogvideoX | 2.8 | 28.3 | - |\n| Inversion | 35.6 | 24.3 | 154.2 |\n| Ingredients | 77.1 | 26.7 | 106.3 |", "caption": "Table 1: Compare between our method and baseline approaches on multiple human video generation. Cogvideox served as the text-only baseline without any video conditioning while Inversion is tuning-based textual inversion.", "description": "Table 1 presents a quantitative comparison of the proposed 'Ingredients' method against two baseline approaches for multi-human video generation.  The first baseline, 'CogVideoX', serves as a text-only model, meaning it generates video based solely on text prompts without any visual conditioning or input. The second baseline, 'Inversion', represents a tuning-based textual inversion method, indicating that the model undergoes fine-tuning for each specific identity to enhance customization. The table compares the performance of these three methods across three key metrics: Face Similarity (measuring how similar generated faces are to the input faces), CLIPScore (evaluating the alignment between the generated video and the text prompt), and Fr\u00e9chet Inception Distance (FID, measuring the visual quality of the generated videos). Higher Face Similarity and CLIPScore values, along with a lower FID value, indicate superior performance.  This comparison allows for an objective assessment of the Ingredients framework's effectiveness in generating high-quality, identity-consistent videos compared to alternative methods.", "section": "4 Experiments"}, {"content": "| Initialize |  | Router Supervised |  | VAE Features |  | Metrics |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|\n| T2V | I2V | Box | Seg. | After | Before | Face Sim. \u2191 (%) | CLIP-T \u2191 (%) | FID \u2193 |  |\n| \u2713 |  |  | \u2713 |  | \u2713 | 58.1 | 26.5 | 122.5 |  |\n|  | \u2713 |  | \u2713 | \u2713 |  | 65.5 | 25.9 | 119.2 |  |\n|  | \u2713 | \u2713 |  |  | \u2713 | 74.3 | 26.7 | 110.4 |  |\n|  | \u2713 |  | \u2713 |  | \u2713 | 77.1 | 26.7 | 106.3 |  |", "caption": "Table 2: Ablation studies for components in ID embedding and control signals. Combine all of I2V initialization, segment supervision and spacal concatenation before VAE provide a best generative performance with multi-ID consistency.", "description": "This ablation study analyzes the impact of different components on the Ingredients framework's performance in multi-ID video generation. It examines the effects of different initialization methods (text-to-video vs. image-to-video), segment supervision (using bounding boxes and SAM segmentation masks), and the spatial concatenation of features before the Variational Autoencoder (VAE). The results show that combining image-to-video initialization, SAM-based segment supervision, and concatenating features before the VAE leads to the best performance in terms of multi-ID consistency, demonstrating the importance of these components for effective facial identity preservation.", "section": "4.3 Ablation Study"}, {"content": "| Methods | Face Sim. \u2191 (%) | CLIPScore \u2191 (%) | FID \u2193 | \n|---|---|---|---| \n| w/o $L_{route}$ | 62.2 | 26.9 | 112.3 | \n| w/ $L_{mse}$ | 72.5 | 26.1 | 109.5 | \n| w/ $L_{route}$ | 77.1 | 26.7 | 106.3 |", "caption": "Table 3: Effect of routing loss. Equipped with routing loss of ID classification helps to build a multi-ID consistent generation.", "description": "This table presents an ablation study on the impact of the routing loss component within the Ingredients framework.  It compares the performance of the model when the routing loss is omitted, replaced with a mean squared error (MSE) loss, and when the original ID classification-based routing loss is used. The results are measured by Face Similarity (higher is better), CLIPScore (higher is better), and Fr\u00e9chet Inception Distance (FID, lower is better) scores, which respectively quantify the similarity of generated faces to real faces, the alignment of generated videos with textual prompts, and the overall quality of the generated video. This demonstrates the effectiveness of the proposed routing loss in generating videos with multi-ID consistency.", "section": "4.3 Ablation Study"}]