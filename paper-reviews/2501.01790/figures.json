[{"figure_path": "https://arxiv.org/html/2501.01790/extracted/6109222/precase.png", "caption": "Figure 1: Examples of multi-ID customized video results from our proposed Ingredients. Given a reference with multiple human image set, our method can generate realistic and personalized videos while preserving specific human identity consistent.", "description": "This figure showcases the capabilities of the proposed 'Ingredients' framework for multi-identity video customization.  Multiple reference images of different individuals are used as input. The resulting videos demonstrate the ability of the model to generate realistic and personalized video content, successfully incorporating the distinct facial features of each individual while maintaining identity consistency throughout the video.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.01790/extracted/6109222/framework.png", "caption": "Figure 2: Overview of Ingredients framework. The proposed method consists of three key modules: a facial extractor, a q-former-based projector, and an ID router.\nThe facial extractor collects versatile editable facial features with a decoupling strategy for each ID.\nThe q-former projector map multi-scale facial embedding into different layers of video diffusion transformers.\nThe ID router combines and distributes ID embeddings to their respective locations adaptively without the intervention for prompts and layouts.\nThe entire training process of the framework is curated into two stages, i.e., the facial embedding alignment stage and the router fine-tuning stage.", "description": "The Ingredients framework uses three main modules to customize videos with multiple identities.  The facial extractor identifies and extracts distinct facial features for each person.  A q-former projector maps these features into the video diffusion transformer at multiple scales. Finally, the ID router strategically places these identities into the video based on their location and time, adapting to different regions of the video without needing extra prompts or predefined layouts. The system trains in two phases: aligning facial embeddings and then fine-tuning the router.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2501.01790/extracted/6109222/compare.png", "caption": "Figure 3: Qualitative comparison of different personalization methods on multi-ID video customization. It can been seen that compared with training-based customization, i.e., textual inversion, our method can clearly routing and attention the respect regions, benefits to ID consistency as well as strong prompt following.", "description": "Figure 3 presents a qualitative comparison of various multi-identity (multi-ID) video personalization methods.  It contrasts the results of the proposed 'Ingredients' method against a training-based approach (textual inversion). The figure showcases that Ingredients, unlike textual inversion, effectively routes and focuses attention on the relevant regions of the video, resulting in improved consistency in maintaining individual identities and strong adherence to the provided textual prompts.  The visualization highlights the superior performance of Ingredients in preserving distinct identities while accurately reflecting the desired content.", "section": "3.2 Key Modules"}, {"figure_path": "https://arxiv.org/html/2501.01790/extracted/6109222/badcase.png", "caption": "Figure 4: Additional bad examples of multi-human customization. Our Ingredients involves failures that generated characters appearing as though they were directly copied-pasted and out-painting, leading to an inconsistent video scenes.", "description": "Figure 4 showcases instances where the Ingredients model fails to produce satisfactory multi-human video customizations. The generated videos exhibit flaws such as characters that appear to be directly copied and pasted from the source images, leading to unnatural and jarring transitions.  Another issue is evident in instances where the background and elements surrounding the subjects are inconsistent, displaying signs of 'out-painting' where the AI attempts to generate areas outside the boundaries of the original image, leading to a mismatch between the figures and their environment.  The resulting videos lack a cohesive visual narrative and do not maintain the consistency aimed for.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.01790/extracted/6109222/routing.png", "caption": "Figure 5: Visualization of routing map within each cross-attention layer of video diffusion transformers. We can see that with the routing loss, the routing network can discern different human IDs at earlier timesetps and in a more pronounced manner.", "description": "Figure 5 visualizes how the routing network within the video diffusion transformer assigns different identities at various cross-attention layers.  The visualization uses a heatmap to show the assignment of two distinct human IDs (white and black pixels) across different time steps and layers.  It demonstrates that with the addition of the routing loss, the network successfully identifies the individual identities earlier in the generation process and with greater clarity.  The routing becomes more refined and accurate with deeper layers, highlighting the effectiveness of the routing loss in improving ID separation and consistency.", "section": "4.4 Visualization"}, {"figure_path": "https://arxiv.org/html/2501.01790/extracted/6109222/training.png", "caption": "Figure 6: The curve of different training loss in router fine-tuning stage. We can see that with training steps increases, routing loss significantly decreases, the router becomes more accurate, while the diffusion loss remains almost unchanged, maintaining the original generative performance.", "description": "This figure displays the training loss curves for two loss functions during the router fine-tuning stage of the Ingredients model. The x-axis represents the training steps, and the y-axis represents the loss value. The blue curve shows the routing loss, while the orange curve shows the diffusion loss.  As training progresses (increasing number of steps), the routing loss decreases significantly, indicating that the ID router is becoming more accurate in assigning identities to different regions of the video frames.  Conversely, the diffusion loss remains relatively constant throughout the training, demonstrating that the overall generative quality of the model is maintained.  The stable diffusion loss while the routing loss decreases significantly demonstrates that the model successfully refines the identity assignment without compromising the video generation quality.", "section": "3.3 Training"}, {"figure_path": "https://arxiv.org/html/2501.01790/extracted/6109222/sam.png", "caption": "Figure 7: Hyper-parameter settings for SAM segmentaion. We select -2.0 as threshold to build routing supervised labels.", "description": "This figure showcases the hyperparameter tuning process for the Segment Anything Model (SAM) used in the Ingredients framework.  Specifically, it illustrates how different threshold values (-2.0, -5.0, -10.0, and 0.0) impact the resulting segmentation masks. This is crucial because SAM's output is used to generate training labels for the 'router' component.  The goal is to optimize the threshold to precisely define the facial regions for multi-identity video generation, ensuring accurate assignment of identities to the corresponding spatial regions of the video. The image shows examples of how the segmentation mask changes depending on the threshold value chosen. The optimal threshold of -2.0 is selected for the experiment to generate the training supervision labels.", "section": "3.3 Training"}, {"figure_path": "https://arxiv.org/html/2501.01790/extracted/6109222/evaluate.png", "caption": "Figure 8: Domain distribution of evaluation images (left) and used prompt to generate text inputs (right). We consider multiple aspects for data collection to make evaluation more robust.", "description": "Figure 8 shows the variety of data used for evaluating the model's performance. The left side displays the diverse set of images used as references, highlighting differences in race, gender, clothing, and activities.  The right side presents examples of the detailed textual prompts that were used, illustrating that prompts incorporate multiple aspects (e.g., clothing, location, interactions) for generating the videos. By including such diversity in both image and text, the evaluation becomes more robust and less susceptible to bias from a limited range of inputs.", "section": "4 Experiments"}]