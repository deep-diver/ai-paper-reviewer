[{"content": "| Subset | Instruction Type | Video Source | #Videos | Avg. Duration | Avg. Resolution |\n|---|---|---|---|---|---| \n| Long Video Captioning | Video Captioning | Panda-70M [5] | 58,617 | 33.2s | 1277x720 |\n| Event Relationship QA | Freeform QA/MCQ | Panda-70M [5] | 56,854 | 33.4s | 1278x720 |\n| Temporal NIAH | Freeform QA/MCQ | Panda-70M [5] (N), MiraData [14] (H) | 59,751 | 67.6s | 640x358 |\n| Two Needle NIAH | Freeform QA | Panda-70M [5] (N), FineVideo [8] (H) | 52,349 | 112.4s | 591x382 |\n| Spatial NIAH | Freeform QA/MCQ | InternVid [50] (N), OpenVid-1M [33] (H) | 59,978 | 9.9s | 1726x971 |\n| Spatiotemporal NIAH | Freeform QA/MCQ | OpenVid-1M [33] (N), FineVideo [8] (H) | 56,494 | 89.9s | 591x383 |\n| HR Video Grid QA | Freeform QA/MCQ | InternVid [50] | 59,901 | 3s | 1920x1080 |\n| VISTA-400K | - | - | 403,944 | 48.6s | 1160x666 |", "caption": "Table 1: Statistics of our synthetic video instruction-following dataset. \u201c(N)\u201d and \u201c(H)\u201d corresponds to the \u201cneedle\u201d (short or low-res videos) and the \u201chaystack\u201d (long or high-res videos) in NIAH subsets.", "description": "This table presents a statistical summary of the VISTA-400K dataset, a synthetic video instruction-following dataset created using the VISTA framework.  It details the number of videos, average duration, and average resolution for each of the seven subsets of the dataset.  Each subset employs a different video augmentation technique to create synthetic videos of varying lengths and resolutions.  The 'Needle-in-a-Haystack' (NIAH) subsets combine short, low-resolution videos ('N') with longer, high-resolution videos ('H') to create more challenging training examples for video understanding models.  The table provides crucial information for understanding the characteristics and composition of the VISTA-400K dataset.", "section": "2. VISTA-400K Dataset"}, {"content": "| Long Video Understanding | Short Video Understanding |\n|---|---|---|\n| Video-MME w/o subtitles | MLVU | LVBench |\n|  | LongVideoBench | MVBench |\n|  | NExT-QA |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Models | Size | avg | short | medium | long | m-avg | test | val | test | mc |\n|---|---|---|---|---|---|---|---|---|---|---|\n| *Proprietary Models* |  |  |  |  |  |  |  |  |  |  |\n| GPT-4V [1] | - | 59.9 | 70.5 | 55.8 | 53.5 | 49.2 | - | 59.1 | 43.5 | - |\n| GPT-4o [35] | - | 71.9 | 80.0 | 70.3 | 65.3 | 64.6 | 34.7 | 66.7 | - | 76.0 |\n| Gemini-1.5-Pro [44] | - | 75.0 | 81.7 | 74.3 | 67.4 | - | 33.1 | 64.0 | - | - |\n| *Open-source Models* |  |  |  |  |  |  |  |  |  |  |\n| VideoChat2 [23] | 7B | 39.5 | 48.3 | 37.0 | 33.2 | 47.9 | - | 39.3 | 51.9 | **78.6** |\n| LLaMA-VID [25] | 7B | - | - | - | - | 33.2 | 23.9 | - | 41.3 | - |\n| ST-LLM [29] | 7B | 37.9 | 45.7 | 36.8 | 31.3 | - | - | - | 54.9 | - |\n| ShareGPT4Video [4] | 7B | 39.9 | 48.3 | 36.3 | 35.0 | 46.4 | - | 39.7 | 51.2 | - |\n| LongVILA [55] | 7B | 50.5 | 61.8 | 50.4 | 46.2 | - | - | - | - | - |\n| LongLLaVA [49] | 7B | 52.9 | 61.9 | 51.4 | 45.4 | - | - | - | 54.6 | - |\n| Video-XL [41] | 7B | **55.5** | 64.0 | **53.2** | **49.2** | **64.9** | - | 49.5 | **55.3** | 77.2 |\n| VideoLLaVA [26] | 7B | 39.9 | 45.3 | 38.0 | 36.2 | 47.3 | 29.3 | 39.9 | 43.8 | 61.8 |\n| VISTA-VideoLLaVA | 7B | 43.7 | 48.2 | 43.9 | 38.9 | 49.5 | 33.8 | 42.3 | 47.2 | 63.0 |\n| \u0394 - VideoLLaVA |  | **+3.8** | **+2.9** | **+5.9** | **+2.7** | **+2.2** | **+4.5** | **+2.4** | **+3.4** | **+1.2** |\n| Mantis-Idefics2 [13] | 8B | 45.4 | 55.9 | 43.0 | 37.2 | 49.4 | 35.0 | 45.8 | 51.4 | 75.8 |\n| VISTA-Mantis | 8B | 48.2 | 58.4 | 46.7 | 39.6 | 55.5 | 36.4 | 49.1 | 52.5 | 75.2 |\n| \u0394 - Mantis-Idefics2 |  | **+2.8** | **+2.5** | **+3.7** | **+2.4** | **+6.1** | **+1.4** | **+3.3** | **+1.1** | **-0.6** |\n| LongVA [63] | 7B | 52.4 | 61.4 | 50.9 | 45.0 | 56.3 | 35.9 | 51.8 | 49.2 | 68.3 |\n| VISTA-LongVA | 7B | **55.5** | **66.0** | 53.1 | 47.4 | 62.1 | **39.0** | **53.1** | 51.1 | 69.3 |\n| \u0394 - LongVA |  | **+3.1** | **+4.6** | **+2.2** | **+2.4** | **+5.8** | **+3.1** | **+1.3** | **+1.9** | **+1.0** |", "caption": "Table 2: Comparisons between baseline models and VISTA-finetuned models on long/short video understanding benchmarks. The best results among open-source models are bolded. \u0394\u0394\\Deltaroman_\u0394 denotes the performance differences before and after finetuning on VISTA-400K.", "description": "This table compares the performance of several baseline video language models (LLMs) against versions of those same models fine-tuned on the VISTA-400K dataset.  The comparison is made across multiple benchmarks designed to test both long and short video understanding capabilities.  The table shows the average performance across multiple categories, including \"short,\" \"medium,\" and \"long\" video lengths, as well as overall performance.  The best results achieved by open-source models are highlighted in bold.  The final column indicates the performance improvement (\u0394) after fine-tuning with VISTA-400K.", "section": "4. Experimental Results"}, {"content": "|                     | HRVideoBench | MSVD-QA | MSRVTT-QA | TGIF-QA | ActivityNet-QA |  |  |  |  |  |  |\n|---------------------|--------------|---------|-----------|---------|-----------------|---|----|----|----|----|----|\n| **High-Res Video Understanding** |              |         |           |         |                 | **Open-Ended Video QA** |  |  |  |  |  |\n| **Models**           | **avg**      | **object** | **action** | **acc.** | **score**       | **acc.** | **score** | **acc.** | **score** | **acc.** | **score** |\n| VideoLLaVA [26]     | 32.5         | 36.0     | 27.9       | 60.3     | 3.7              | 42.1     | 3.0       | 63.5     | 3.8       | 48.6     | 3.3       |\n| VISTA-VideoLLaVA     | 47.5         | 50.0     | 44.2       | 71.5     | 4.0              | 58.5     | 3.5       | 78.0     | 4.3       | 49.1     | 3.4       |\n| \u0394 - VideoLLaVA       | +15.0        | +14.0    | +16.3      | +11.2    | +0.3             | +16.4    | +0.5      | +14.5    | +0.5      | +0.5     | +0.1      |\n| Mantis-Idefics2 [13]| 48.5         | 50.9     | 45.4       | 57.4     | 3.5              | 34.9     | 2.7       | 65.7     | 3.8       | 46.5     | 3.1       |\n| VISTA-Mantis        | 51.0         | 53.5     | 47.7       | 65.2     | 3.8              | 46.4     | 3.1       | 71.4     | 4.0       | 48.8     | 3.3       |\n| \u0394 - Mantis          | +2.5         | +2.6     | +2.3       | +7.8     | +0.3             | +11.5    | +0.4      | +5.7     | +0.2      | +2.3     | +0.2      |\n| LongVA [63]         | 48.0         | 52.6     | 41.9       | 56.3     | 3.5              | 37.7     | 2.8       | 55.4     | 3.4       | 48.0     | 3.2       |\n| VISTA-LongVA        | 50.0         | 56.1     | 41.9       | 61.0     | 3.7              | 42.5     | 3.0       | 67.5     | 3.9       | 51.8     | 3.4       |\n| \u0394 - LongVA          | +2.0         | +3.5     | +0.0       | +4.7     | +0.2             | +4.8     | +0.2      | +12.1    | +0.5      | +3.8     | +0.2      |", "caption": "Table 3: Quantitative results on HRVideoBench and open-ended video QA benchmarks. \u201cacc.\u201d represents accuracy.", "description": "This table presents a quantitative comparison of different video language models' performance on high-resolution video understanding and open-ended video question answering tasks.  The HRVideoBench benchmark assesses the models' ability to understand high-resolution video details, while the open-ended benchmarks (MSVD-QA, MSRVTT-QA, TGIF-QA, and ActivityNet-QA) evaluate their performance on general video question-answering tasks.  The table shows the average accuracy (acc.) and scores achieved by each model on each benchmark.  Specifically for HRVideoBench, both object and action understanding accuracies are shown.", "section": "3. Evaluation: HRVideoBench"}, {"content": "| Models | Video-MME | HRVideoBench |\n|---|---|---|\n| Video-MME | w/o sub. avg | avg |\n| VISTA-Mantis | **48.2** | **51.0** |\n| w/o Long Video Captioning | 47.9 | 48.0 |\n| w/o Event Relationship QA | 47.7 | 49.5 |\n| w/o Temporal NIAH | 47.5 | 48.0 |\n| w/o Two Needle NIAH | 48.1 | 50.5 |\n| w/o Spatial NIAH | 47.2 | 47.5 |\n| w/o Spatiotemporal NIAH | 47.7 | 50.0 |\n| w/o HR Video Grid QA | 47.8 | 48.0 |\n| w/o Video Augmentation | 45.7 | 44.5 |", "caption": "Table 4: Ablation study results for VISTA-Mantis. Each \u201cw/o [Subset]\u201d denotes a Mantis-Idefics2 model finetuned on a modified VISTA-400K by replacing the corresponding subset with the same amount of training examples from VideoChat2-IT [23].", "description": "This ablation study investigates the impact of each video augmentation subset within VISTA-400K on the performance of the Mantis-Idefics2 model.  For each row, a modified version of VISTA-400K is created by replacing one of the seven subsets with an equal number of training examples from the VideoChat2-IT dataset. The table shows the average performance scores on the Video-MME and HRVideoBench benchmarks for the modified models, highlighting the contribution of each subset to the overall performance gains.", "section": "4. Experimental Results"}, {"content": "|                       | avg | short | medium | long | m-avg | test | val |\n|-----------------------|-----|-------|--------|------|-------|------|-----|\n| **Long Video Understanding** |     |       |        |      |       |      |     |\n| **Video-MME w/o subtitles** |     |       |        |      |       |      |     |\n| Models                 |     |       |        |      |       |      |     |\n| VideoLLaVA             | 39.9 | 45.3  | 38.0   | 36.2 | 45.0  | 29.3 | 39.1 |\n| VideoLLaVA (SFT on VISTA-400K) | 43.6 | 47.3  | 43.8   | 39.8 | 48.7  | 32.6 | 41.0 |\n| \u0394 - VideoLLaVA        | +3.7 | +2.0  | +5.8   | +3.6 | +3.7  | +3.3 | +1.9 |\n| VideoLLaVA (SFT on VISTA-400K + 300K VideoChat2-IT) | 43.7 | 48.2  | 43.9   | 38.9 | 49.5  | 33.8 | 42.3 |\n| \u0394 - VideoLLaVA (SFT on VISTA-400K) | +0.1 | +0.9  | +0.1   | -0.9 | +0.8  | +1.2 | +1.3 |", "caption": "Table 5: Comparison between the baseline VideoLLaVA model, VideoLLaVA finetuned on VISTA-400K and VideoLLaVA finetuned on VISTA-400K + 300K VideoChat2-IT data (VISTA-VideoLLaVA in the main paper) on long video understanding benchmarks. \u201cSFT\u201d indicates supervised finetuning.", "description": "This table presents a comparison of the performance of three different models on long video understanding benchmarks. The first model is the baseline VideoLLaVA model. The second model is VideoLLaVA finetuned on the VISTA-400K dataset. The third model is VideoLLaVA finetuned on both the VISTA-400K dataset and an additional 300K videos from the VideoChat2-IT dataset.  The benchmarks used are Video-MME, MLVU, LVBench, and LongVideoBench. The table shows the average performance across short, medium, and long video clips for each benchmark and model, as well as the improvement achieved by fine-tuning.  'SFT' denotes supervised finetuning.", "section": "4. Experimental Results"}, {"content": "| Models | avg | object | action |\n|---|---|---|---|\n| High-Resolution Video Understanding |  |  |  |\n| HRVideoBench |  |  |  |\n| VideoLLaVA | 32.5 | 36.0 | 27.9 |\n| VideoLLaVA (SFT on VISTA-400K) | 44.0 | 42.1 | 46.5 |\n| \u0394 - VideoLLaVA | +11.5 | +6.1 | +18.6 |\n| VideoLLaVA (SFT on VISTA-400K + 300K VideoChat2-IT) | 47.5 | 50 | 44.2 |\n| \u0394 - VideoLLaVA (SFT on VISTA-400K) | +3.5 | +7.9 | -2.3 |", "caption": "Table 6: Comparison between the baseline VideoLLaVA model, VideoLLaVA finetuned on VISTA-400K and VideoLLaVA finetuned on VISTA-400K + 300K VideoChat2-IT data (VISTA-VideoLLaVA in the main paper) on HRVideoBench. \u201cSFT\u201d indicates supervised finetuning.", "description": "This table compares the performance of three different models on the HRVideoBench benchmark: the baseline VideoLLaVA model, VideoLLaVA fine-tuned on the VISTA-400K dataset, and VideoLLaVA fine-tuned on both VISTA-400K and an additional 300K samples from the VideoChat2-IT dataset.  The results show the average performance, object recognition accuracy, and action recognition accuracy for each model on the benchmark.  The 'A' values represent the performance differences between the fine-tuned models and the baseline model.  'SFT' denotes that supervised fine-tuning was used for the models.", "section": "4. Experimental Results"}, {"content": "| Models | avg | short | medium | long |\n|---|---|---|---|---|\n| VideoLLaVA | 41.6 | 46.1 | 40.7 | 38.1 |\n| VISTA-VideoLLaVA | 45.1 | 50.2 | 45.7 | 39.3 |\n| \u0394 - VideoLLaVA | +3.5 | +4.1 | +5.0 | +1.2 |\n| Mantis-Idefics2 | 49.0 | 60.4 | 46.1 | 40.3 |\n| VISTA-Mantis | 50.9 | 61.8 | 48.6 | 42.3 |\n| \u0394 - Mantis-Idefics2 | +1.9 | +1.4 | +2.5 | +2.0 |\n| LongVA | 54.3 | 61.6 | 53.6 | 47.6 |\n| VISTA-LongVA | 59.3 | 70.0 | 57.6 | 50.3 |\n| \u0394 - LongVA | +5.0 | +8.4 | +4.0 | +2.7 |", "caption": "Table 7: Comparison between VISTA-finetuned models and baseline models on Video-MME w/ subtitle benchmark.", "description": "This table presents a comparison of the performance of baseline video language models and their VISTA-finetuned counterparts on the Video-MME benchmark, specifically using the \"with subtitles\" setting.  It shows the average accuracy scores, as well as scores for short, medium, and long video questions, demonstrating the improvement achieved by fine-tuning on the VISTA dataset.", "section": "4. Experimental Results"}]