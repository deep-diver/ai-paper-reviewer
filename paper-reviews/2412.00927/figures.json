[{"figure_path": "https://arxiv.org/html/2412.00927/x1.png", "caption": "Figure 1: VISTA is a simple but effective framework that generates high-quality video instruction data from existing video-caption pairs. Our VISTA-400K dataset enhances model performances on various long and high-resolution video benchmarks.", "description": "The figure illustrates the VISTA framework, which leverages existing video-caption datasets to produce high-quality video instruction-following data.  VISTA combines videos both spatially and temporally to create synthetic videos with longer durations and higher resolutions.  These videos are then paired with newly generated question-answer pairs, effectively augmenting the original dataset.  The figure also shows a bar graph comparing the average accuracy of baseline models versus models fine-tuned with the VISTA-400K dataset, highlighting the performance improvements achieved on various video understanding benchmarks (Long Video Bench, LVBench, HRVideoBench, Short Video Bench).  The improvement demonstrates VISTA's effectiveness in enhancing video understanding models' ability to handle long-duration and high-resolution video content.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.00927/x2.png", "caption": "Figure 2: Our proposed video augmentation and instruction-following data synthesis schemes for VISTA-400K. Given input videos, We perform spatiotemporal video combinations to produce augmented video samples with longer duration and higher resolution.", "description": "This figure illustrates the VISTA framework's seven video augmentation methods for generating synthetic video instruction-following data.  Starting with input short videos and their captions, VISTA spatially and temporally combines them to create longer, higher-resolution videos (e.g., by concatenating clips, inserting short clips into longer ones at different timepoints or locations, or arranging low-resolution videos in a grid). Then, using a large language model, VISTA synthesizes question-answer pairs about these new, augmented videos.  Each of the seven subsets demonstrates a different augmentation technique, including methods for creating long videos, long video captions, questions about event relationships, and various needle-in-a-haystack (NIAH) QA pairs for testing temporal and spatial video understanding.", "section": "2. VISTA-400K Dataset"}, {"figure_path": "https://arxiv.org/html/2412.00927/x3.png", "caption": "Figure 3: Qualitative comparisons between the baseline models and our VISTA-finetuned models. Red text indicates hallucinations or incorrect responses, while green text highlights the correct responses that correspond accurately to the video content.", "description": "Figure 3 presents a qualitative analysis comparing the performance of baseline video language models (VLMs) against VLMs fine-tuned using the VISTA dataset.  Two example scenarios are shown: 'helicopter' and 'table tennis'. Each scenario displays the question, followed by the responses generated by several models (baseline LongVA, baseline VideoLLaVA, VISTA-enhanced LongVA, and VISTA-enhanced VideoLLaVA for the 'helicopter' example; baseline Mantis-Idefics2, and VISTA-enhanced Mantis-Idefics2 for the 'table tennis' example). Incorrect or hallucinated responses are highlighted in red, while accurate responses are shown in green. This visual comparison highlights how VISTA improves the accuracy and reduces hallucinatory outputs of VLMs.", "section": "4.4. Case Study"}, {"figure_path": "https://arxiv.org/html/2412.00927/x4.png", "caption": "Figure 4: Example questions from our HRVideoBench. Zoom in for better visualizations.", "description": "This figure showcases two example questions from the HRVideoBench dataset, a high-resolution video understanding benchmark.  The examples highlight the dataset's focus on evaluating fine-grained object details and subtle actions within high-resolution videos. The first example requires identifying a car's color in a rearview mirror, demonstrating the need for precise object recognition at high resolution. The second example tasks the model with determining the directional movement of a person from a specific angle and at a specific location within the video, showcasing the benchmark's assessment of localized action recognition. The image emphasizes the high resolution and detailed nature of the videos used in the HRVideoBench.", "section": "7. HRVideoBench Details"}]