{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "It is a foundational large language model used extensively in the VISTA framework for instruction synthesis and QA pair generation."}, {"fullname_first_author": "Tsai-Shien Chen", "paper_title": "Panda-70M: Captioning 70m videos with multiple cross-modality teachers", "publication_date": "2024-00-00", "reason": "It is a major video-caption dataset leveraged by VISTA for synthesizing long video instruction-following data."}, {"fullname_first_author": "Chaoyou Fu", "paper_title": "Video-MME: The first-ever comprehensive evaluation benchmark of multi-modal LLMs in video analysis", "publication_date": "2024-00-00", "reason": "It is one of the primary benchmarks used for evaluating the effectiveness of VISTA in improving long video understanding."}, {"fullname_first_author": "Bin Lin", "paper_title": "Video-LLaVA: Learning united visual representations by alignment before projection", "publication_date": "2023-11-10", "reason": "It is a key video LLM used for finetuning experiments to demonstrate VISTA's effectiveness on enhancing video LMM performance."}, {"fullname_first_author": "Dongfu Jiang", "paper_title": "Mantis: Interleaved multi-image instruction tuning", "publication_date": "2024-05-01", "reason": "It is another key video LLM used for finetuning experiments to demonstrate VISTA's effectiveness on enhancing video LLM performance."}]}