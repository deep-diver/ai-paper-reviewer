[{"content": "| Training Data | R2R (Val Unseen) | REVERIE (Val Unseen) | ObjectNav-MP3D (Val) |\n|---|---|---|---|---|\n| R2R<sup>*</sup> | REVERIE<sup>*</sup> | MP3D<sup>*</sup> | TL | NE \u2193 | SR \u2191 | SPL \u2191 | TL | NE \u2193 | SR \u2191 | SPL \u2191 | TL | NE \u2193 | SR \u2191 | SPL \u2191 |\n| \u2713 |  |  | 14.33 | 3.82 | 67 | 55 | 19.61 | 7.55 | 39 | 28 | 15.30 | 4.69 | 55 | 24 |\n|  | \u2713 |  | 17.55 | 6.22 | 42 | 32 | 17.91 | 6.56 | 41 | 32 | 10.46 | 5.91 | 43 | 23 |\n|  |  | \u2713 | 20.76 | 8.55 | 16 | 9 | 20.00 | 10.11 | 13 | 9 | 22.17 | 3.67 | 68 | 29 |\n| \u2713 |  | \u2713 | 14.03 | 4.01 | 64 | 55 | 15.22 | 7.78 | 38 | 31 | 25.91 | 3.28 | 72 | 28 |\n|  | \u2713 | \u2713 | 19.17 | 7.13 | 34 | 26 | 19.46 | 6.24 | 35 | 26 | 21.50 | 3.29 | 70 | 33 |\n| \u2713 | \u2713 | \u2713 | 14.21 | 4.10 | 65 | 54 | 16.62 | 6.11 | 34 | 27 | 22.97 | 3.54 | 68 | 27 |", "caption": "Table 1: Comparison of single-run performance with a different mixture of training data for DuET\u00a0[16]. \u2217*\u2217 indicates utilizing Habitat-rendered images. Numbers in gray indicated zero-shot inference results on held-out datasets.", "description": "This table presents a comparison of the performance of the DuET model [16] when trained on different combinations of datasets.  It shows the impact of data diversity on the model's ability to generalize to unseen data.  The results are presented in terms of Trajectory Length (TL), Navigation Error (NE), Success Rate (SR), and SPL, which are common evaluation metrics for visual navigation tasks. The asterisk (*) indicates that Habitat-rendered images were used in the training process. The gray numbers represent the zero-shot performance of the model on held-out datasets, which were not used during training, highlighting the model's ability to generalize.  Different training data combinations were tested to understand how data mixing affects performance.", "section": "2.2 What are the Conflicts in Navigation Multi-tasks Learning?"}, {"content": "| Routing Condition | R2R (Val Unseen) |  |  |  | REVERIE (Val Unseen) |  |  |  | ObjectNav-MP3D (Val) |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  | TL | NE\u2193 | SR\u2191 | SPL\u2191 | TL | NE\u2193 | SR\u2191 | SPL\u2191 | TL | NE\u2193 | SR\u2191 | SPL\u2191 |\n| w/o MoE | 11.76 | 2.98 | 73.09 | 65.79 | 13.17 | 5.90 | 40.39 | 35.40 | 16.13 | 3.16 | 72.31 | 42.72 |\n| *Token-wised MoE* [93] |  |  |  |  |  |  |  |  |  |  |  |  |\n| Token Embedding | 13.28 | 2.98 | 73.99 | 64.81 | 16.78 | 5.60 | 43.45 | 35.40 | 15.69 | 3.04 | 74.38 | 44.97 |\n| *Task-wised MoE* |  |  |  |  |  |  |  |  |  |  |  |  |\n| Task Embedding | 12.98 | 3.13 | 72.84 | 64.81 | 14.90 | 5.71 | 43.71 | 36.58 | 14.96 | 3.17 | 71.13 | 43.88 |\n| Text `[CLS]` | 14.45 | 2.92 | 74.67 | 64.12 | 19.22 | 5.46 | 45.50 | 36.56 | 15.63 | 3.11 | 71.32 | 42.75 |\n|  w/ Task Embedding | 15.80 | 2.95 | 73.61 | 62.80 | 21.50 | 5.44 | 43.85 | 34.10 | 18.26 | 3.00 | 72.40 | 39.98 |\n| *State-Adaptive MoE (ours)* |  |  |  |  |  |  |  |  |  |  |  |  |\n| SAME | 13.51 | **2.90** | 73.69 | **64.92** | 16.32 | **5.38** | **45.67** | **37.95** | 15.60 | 3.10 | 71.43 | 43.39 |\n|  w/ Task Embedding | 14.00 | 2.89 | 74.50 | 64.66 | 17.65 | 5.66 | 42.32 | 33.61 | 17.20 | **2.86** | 73.39 | 42.07 |\n|  w/o Pretrain | 14.71 | 4.66 | 59.05 | 49.02 | 14.19 | 6.26 | 38.43 | 32.40 | 19.93 | 3.08 | 70.94 | 38.48 |", "caption": "Table 2: Comparison of single-run performance with different MoE routing conditions on R2R, REVERIE, and ObjectNav-MP3D.", "description": "This table presents a comparison of the performance of a single run of the model with different Mixture of Experts (MoE) routing methods.  The performance is evaluated across three different visual navigation tasks:  R2R (fine-grained instructions), REVERIE (coarse-grained instructions), and ObjectNav-MP3D (zero-grained instructions). The metrics used for comparison include Trajectory Length (TL), Navigation Error (NE), Success Rate (SR), and Success rate penalized by Path Length (SPL). Different MoE routing strategies are compared, such as no MoE, token-wise MoE, task-wise MoE and the proposed State-Adaptive MoE (SAME), with and without task embedding. This allows for assessment of how different routing mechanisms impact performance across tasks with varying levels of language instruction detail.", "section": "3. Mixture of Experts for Versatile Language-guided Visual Navigation"}, {"content": "| MoE Experts Position | R2R (Val Unseen) TL | R2R (Val Unseen) NE\u2193 | R2R (Val Unseen) SR\u2191 | R2R (Val Unseen) SPL\u2191 | REVERIE (Val Unseen) TL | REVERIE (Val Unseen) NE\u2193 | REVERIE (Val Unseen) SR\u2191 | REVERIE (Val Unseen) SPL\u2191 | ObjectNav-MP3D (Val) TL | ObjectNav-MP3D (Val) NE\u2193 | ObjectNav-MP3D (Val) SR\u2191 | ObjectNav-MP3D (Val) SPL\u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feed Forward | 13.18 | 3.08 | 73.44 | 64.86 | 16.02 | 5.62 | 42.77 | 35.28 | 16.02 | 3.09 | 71.37 | 42.24 |\n| Visual Query | 13.51 | 2.90 | 73.69 | 64.92 | 16.32 | 5.38 | 45.67 | 37.95 | 15.60 | 3.10 | 71.43 | 43.39 |\n| Textual Key & Value | 15.58 | 2.82 | 75.35 | 63.85 | 20.30 | 5.36 | 45.61 | 34.75 | 17.57 | 3.10 | 72.17 | 42.67 |", "caption": "Table 3: Comparison of single-run performance with different MoE experts\u2019 positions on R2R, REVERIE, and ObjectNav-MP3D.", "description": "This table presents a comparison of the performance of a model using different Mixture of Experts (MoE) configurations.  The model's performance is evaluated on three different visual navigation tasks: R2R (fine-grained instructions), REVERIE (coarse-grained instructions), and ObjectNav-MP3D (zero-grained instructions).  The different MoE configurations vary in where the MoE is applied within the model architecture (feed-forward network, visual query, textual key and value). The metrics used to evaluate performance are Trajectory Length (TL), Navigation Error (NE), Success Rate (SR), and SPL.", "section": "3.3 Comparison on MoE Routing"}, {"content": "| Methods | CVDN |  | RxR-EN |  | R2R |  | SOON |  | REVERIE |  | ObjectNav-MP3D |  | \n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  | Val | Test | Val unseen | Val unseen | Val unseen | Test unseen | Val unseen | Test unseen | Val unseen | Test unseen | Val | Test | \n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  | GP \u2191 | GP \u2191 | SR \u2191 | nDTW \u2191 | SR \u2191 | SPL \u2191 | SR \u2191 | SPL \u2191 | SR \u2191 | SPL \u2191 | SR \u2191 | SPL \u2191 | SR \u2191 | SPL \u2191 | SR \u2191 | SPL \u2191 | \n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| *Separate Model for Each Task*: |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| SF [28] | - | - | - | - | 36 | - | 35 | 28 | - | - | - | - | - | - | - | - | - | - |\n| RCM [103] | - | - | - | - | 43 | - | 43 | 38 | - | - | - | - | 9.3 | 7.0 | 7.8 | 6.7 | - | - |\n| EnvDrop [98] | - | - | - | - | 52 | 48 | 51 | 47 | - | - | - | - | - | - | - | - | - | - |\n| PREVALENT [34] | 3.15 | 2.44 | - | - | 58 | 53 | 54 | 51 | - | - | - | - | - | - | - | - | - | - |\n| VLN \u21bbBERT [36] | - | - | - | - | 63 | 57 | 63 | 57 | - | - | - | - | 25.5 | 21.1 | 24.6 | 19.5 | - | - |\n| HAMT [15] | 5.13 | 5.58 | 56.4 | 63.0 | 66 | 61 | 65 | 60 | - | - | - | - | 33.0 | 30.2 | 30.4 | 26.7 | - | - |\n| HOP+ [81] | - | - | - | - | 67 | 61 | 66 | 60 | - | - | - | - | 36.1 | 31.1 | 33.8 | 28.2 | - | - |\n| DUET [16] | - | - | - | - | 72 | 60 | 69 | 59 | 36.3 | 22.6 | 33.4 | 21.4 | 47.0 | 33.7 | 52.5 | 36.1 | - | - |\n| AutoVLN [17] | - | - | - | - | - | - | - | - | 41.0 | 30.7 | 40.4 | 27.9 | 55.9 | 40.9 | 55.2 | 38.9 | - | - |\n| BEVBert [3] | - | - | 66.7 | 69.6 | 75 | 64 | 73 | 62 | - | - | - | - | 51.8 | 36.4 | 52.8 | 36.4 | - | - |\n| GridMM [107] | - | - | - | - | 75 | 64 | 73 | 62 | - | - | - | - | - | - | - | - | - | - |\n| VER [67] | - |  | - | - | 76 | 65 | 76 | 66 | - | - | - | - | 56.0 | 39.7 | 56.8 | 38.8 | - | - |\n| GOAT [101] | - | - | 68.2 | 66.8 | 78 | 68 | 75 | 65 | 40.4 | 28.1 | 40.5 | 25.2 | 53.4 | 36.7 | 57.7 | 40.5 | - | - |\n| ScaleVLN [106] | 6.12 | 6.97 | - | - | 79 | 70 | 77 | 68 | - | - | - | - | 57.0 | 41.8 | 56.1 | 39.5 | - | - |\n| *Unified Model for All Tasks*: |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| MT-RCM [104] | 4.65 | 3.91 | - | - | 47 | 41 | 45 | 40 | - | - | - | - | - | - | - | - | - | - |\n| NaviLLM [121] | 6.16 | 7.90 | - | - | 67 | 59 | 68 | 60 | 38.3 | 29.2 | 35.0 | 26.3 | 42.2 | 35.7 | 39.8 | 32.3 | - | - |\n| ScaleVLN\u2020 | 5.93 | - | 46.7 | 49.7 | 76 | 67 | - | - | 33.2 | 25.4 | - | - | 41.9 | 34.4 | - | - | 72.3 | 43.4 |\n| SAME (ours) | 6.94 | 7.07 | 50.5 | 51.2 | 76 | 66 | 74 | 64 | 36.1 | 25.4 | 38.2 | 27.1 | 46.4 | 36.1 | 48.6 | 37.1 | 76.3 | 42.7 |", "caption": "Table 4: Agents performance across all tasks in the discrete environment\u00a0[6]. \u2020\u2020\\dagger\u2020 indicates our implementation of multi-task tuning. Note that existing methods tailored for ObjectNav-MP3D are proposed in continuous environments, which will be evaluated in Table\u00a05 below.", "description": "Table 4 presents a comparison of different methods' performance on various visual navigation tasks within a discrete environment.  The tasks vary in terms of the level of detail in the language instructions, ranging from fine-grained (requiring precise step-by-step instructions) to coarse-grained (providing higher-level guidance) to zero-grained (simply specifying an object goal).  The table shows results for both models designed for specific tasks and a single model trained for all tasks. The performance metrics used include success rate, trajectory length, navigation error, and SPL (Success weighted by Path Length), and nDTW (normalized Dynamic Time Warping). The table highlights the performance of the proposed SAME model compared to state-of-the-art methods.  Note that some models are only evaluated on a subset of the tasks, and some methods (ObjectNav-MP3D) designed for a continuous environment are not directly comparable to those evaluated in this discrete environment.", "section": "4. Experiments"}, {"content": "| Methods | R2R-CE (Val unseen) NE \u2193 | R2R-CE (Val unseen) SR \u2191 | R2R-CE (Val unseen) SPL \u2191 | MP3D (Val) SR \u2191 | MP3D (Val) SPL \u2191 |\n|---|---|---|---|---|---| \n| NaVid [117] | 5.47 | 37 | 36 | - | - |\n| ScaleVLN [106] | 4.80 | 55 | 51 | - | - |\n| ETPNav [4] | 4.71 | 57 | 49 | - | - |\n| BEVBert [3] | **4.57** | **59** | **50** | - | - |\n| SemExp [10] | - | - | - | 28 | 11 |\n| PONI [86] | - | - | - | 32 | 12 |\n| Habitat-Web [87] | - | - | - | 35 | 10 |\n| SAME (ours) | 5.31 | 47 | 38 | **43** | **21** |", "caption": "Table 5: Comparison with previous methods in the continuous environment\u00a0[92]. We report the zero-shot inference results of SAME using the same checkpoint from Table\u00a04.", "description": "This table compares the performance of the proposed SAME model with state-of-the-art (SOTA) methods on continuous environment datasets for vision-language navigation.  It specifically focuses on the zero-shot transfer performance.  The SAME model's results are taken from the same checkpoint used in Table 4 (discrete environment results). The table shows that even without further training on the continuous datasets, SAME performs competitively with or better than other existing methods.", "section": "4.1 Comparison with State-of-the-Art Models"}, {"content": "| Algorithm | Batch | R2R (Val Unseen) TL | R2R (Val Unseen) NE\u2193 | R2R (Val Unseen) SR\u2191 | R2R (Val Unseen) SPL\u2191 | REVERIE (Val Unseen) TL | REVERIE (Val Unseen) NE\u2193 | REVERIE (Val Unseen) SR\u2191 | REVERIE (Val Unseen) SPL\u2191 | ObjectNav-MP3D (Val) TL | ObjectNav-MP3D (Val) NE\u2193 | ObjectNav-MP3D (Val) SR\u2191 | ObjectNav-MP3D (Val) SPL\u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| Imitation | Mix | 13.77 | 3.82 | 65.51 | 56.57 | 19.02 | 5.79 | 35.64 | 28.15 | 31.31 | 2.64 | 75.83 | 26.77 |\n| Imitation | Sequential | 9.35 | 4.23 | 61.09 | 58.88 | 9.24 | 7.37 | 28.37 | 26.42 | 26.57 | 3.51 | 71.70 | 24.06 |\n| DAgger | Mix | 16.09 | 4.07 | 62.36 | 52.18 | 25.05 | 5.89 | 30.45 | 21.91 | 27.57 | 2.81 | 74.85 | 31.70 |\n| DAgger | Sequential | 13.51 | 2.90 | 73.69 | 64.92 | 16.32 | 5.38 | 45.67 | 37.95 | 15.60 | 3.10 | 71.43 | 43.39 |", "caption": "Table 6: Comparison of single-run performance with different training schema on R2R, REVERIE, and ObjectNav-MP3D.", "description": "This table presents a comparison of single-run performance across three different vision-language navigation (VLN) tasks (R2R, REVERIE, and ObjectNav-MP3D) under various training schemas.  Specifically, it contrasts performance when using imitation learning versus DAgger (an iterative training algorithm that refines the policy using feedback from simulated agent actions) and also compares the impact of mixing datasets in the same training batch versus processing them sequentially.", "section": "4. Experiments"}, {"content": "| \u03bb | R2R (Val Unseen) |  |  |  | REVERIE (Val Unseen) |  |  |  | ObjectNav-MP3D (Val) |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  | TL | NE\u2193 | SR\u2191 | SPL\u2191 | TL | NE\u2193 | SR\u2191 | SPL\u2191 | TL | NE\u2193 | SR\u2191 | SPL\u2191 |\n| 0.2 | 12.63 | 2.91 | 73.61 | 65.73 | 15.47 | 5.55 | 42.86 | 35.22 | 15.25 | 3.13 | 71.79 | 43.51 |\n| 0.5 | 13.77 | 2.89 | 74.29 | 65.00 | 19.45 | 5.56 | 44.70 | 34.89 | 18.10 | 3.05 | 72.26 | 40.50 |\n| 0.8 | 13.51 | 2.90 | 73.69 | 64.92 | 16.32 | 5.38 | 45.67 | 37.95 | 15.60 | 3.10 | 71.43 | 43.39 |\n| 1.0 | 13.56 | 3.02 | 73.56 | 64.52 | 16.19 | 5.24 | 45.81 | 37.89 | 18.11 | 2.67 | 74.66 | 44.81 |", "caption": "Table 7: Comparison of single-run performance with different MoE balance coefficients \u03bb\ud835\udf06\\lambdaitalic_\u03bb on R2R, REVERIE, and ObjectNav-MP3D.", "description": "This table presents a comparison of single-run performance metrics across three vision-and-language navigation datasets (R2R, REVERIE, and ObjectNav-MP3D) when varying the load balancing coefficient (\u03bb) in the Mixture of Experts (MoE) model.  The load balancing coefficient controls the distribution of inputs across different experts, impacting model performance. The table allows readers to observe the effect of this coefficient on various metrics such as Trajectory Length (TL), Navigation Error (NE), Success Rate (SR), and SPL. This analysis is crucial for optimizing the performance of the MoE model within a multi-task learning context.", "section": "3.3 Comparison on MoE Routing"}, {"content": "| Benchmark | TL | NE \u2193 | nDTW \u2191 | SR \u2191 | SPL \u2191 | TL | NE \u2193 | GP \u2191 | SR \u2191 | SPL \u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|\n| R2R [6] | 13.65 | 2.73 | 71.05 | 76.25 | 66.16 | 14.80 | 3.03 | \u2013 | 73.92 | 64.41 |\n| RxR-EN [52] | 22.69 | 6.53 | 51.20 | 50.52 | 42.19 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 |\n| REVERIE [80] | 18.87 | 5.18 | 48.54 | 46.35 | 36.12 | 19.47 | \u2013 | \u2013 | 48.60 | 37.10 |\n| SOON [125] | 34.42 | 8.12 | \u2013 | 36.11 | 25.42 | 37.99 | \u2013 | \u2013 | 38.18 | 27.11 |\n| CVDN [99] | 30.90 | 12.72 | \u2013 | 24.48 | 17.23 | \u2013 | \u2013 | 7.07 | 18.15 | 12.18 |", "caption": "Table 8: Full results of SAME\u00a0on all VLN benchmarks.", "description": "This table presents a comprehensive overview of the performance achieved by the SAME model across various Vision-and-Language Navigation (VLN) benchmarks.  It details the results for different metrics including Trajectory Length (TL), Navigation Error (NE), normalized Dynamic Time Warping (nDTW), Success Rate (SR), and Success Path Length (SPL).  The benchmarks cover a range of tasks with varying levels of language granularity and complexity. Results are shown for both validation and test sets where available, enabling a thorough assessment of SAME's generalization capabilities across different VLN tasks.", "section": "4. Experiments"}]