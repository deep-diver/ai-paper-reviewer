[{"figure_path": "https://arxiv.org/html/2412.17153/x1.png", "caption": "Figure 1: Qualitative comparisons between DD and vanilla LlamaGen Sun et\u00a0al. (2024) on ImageNet 256\u00d7\\times\u00d7256. We show that the generated images of DD have small quality loss compared to the pre-trained AR model, while achieving \u2265\\geq\u2265200\u00d7\\times\u00d7 speedup. More examples are in App.\u00a0F.", "description": "This figure presents a qualitative comparison of images generated by the proposed Distilled Decoding (DD) method and the original LlamaGen model.  The comparison focuses on ImageNet 256x256 images. The results demonstrate that DD achieves a significant speedup (at least 200 times faster) compared to LlamaGen, while maintaining comparable image quality.  The minimal quality loss suggests the effectiveness of DD in accelerating image generation without substantial compromise of visual fidelity. Additional examples are provided in Appendix F.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2412.17153/x2.png", "caption": "Figure 2: Qualitative results of DD-2step on text-to-image task. The model is distilled from LlamaGen model with prompts from LAION-COCO dataset. The speedup is around 93 \u00d7\\times\u00d7 compared to the teacher model. More examples are in App.\u00a0F.", "description": "Figure 2 showcases the performance of the Distilled Decoding (DD) method's two-step variant (DD-2step) on a text-to-image generation task.  The DD-2step model is a distilled version of the LlamaGen model, meaning its parameters have been optimized to mimic LlamaGen's behavior but with significantly improved speed. The input to the model consists of text prompts sourced from the LAION-COCO dataset. The figure displays four example image outputs generated by DD-2step, demonstrating the visual results obtained. Notably, the figure highlights the considerable speed enhancement achieved by DD-2step, achieving a 93x speedup over the original LlamaGen model. Additional examples illustrating the method's performance can be found in Appendix F.", "section": "5. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2412.17153/x3.png", "caption": "Figure 3: Comparison of DD models, pre-trained models, and other acceleration methods for pre-trained models. DD achieves significant speedup compared to pre-trained models with comparable performance. In contrast, other methods\u2019 performance degrades quickly as inference time decreases.", "description": "Figure 3 presents a comparison of the performance and inference speed of different methods for generating images using pretrained autoregressive models.  The methods include the proposed Distilled Decoding (DD) models, the original pretrained models, and other existing acceleration techniques.  The figure demonstrates that DD achieves a significant speedup over the pretrained models while maintaining comparable image quality (measured by FID). In contrast, the other acceleration techniques show a substantial decrease in image quality as their inference time is reduced.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2412.17153/x4.png", "caption": "Figure 4: High-level comparison between our Distilled Decoding (DD) and prior work. To generate a sequence of tokens qisubscript\ud835\udc5e\ud835\udc56q_{i}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT: (a) the vanilla AR model generates token-by-token, thus being slow; (b) parallel decoding generates multiple tokens in parallel (Sec.\u00a04.1), which fundamentally cannot match the generated distribution of the original AR model with one-step generation (see Sec.\u00a03.1); (c) our DD maps noise tokens \u03f5isubscriptitalic-\u03f5\ud835\udc56\\epsilon_{i}italic_\u03f5 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT from Gaussian distribution to the whole sequence of generated tokens directly in one step and it is guaranteed that (in the optimal case) the distribution of generated tokens matches that of the original AR model.", "description": "Figure 4 illustrates three approaches to generating a sequence of tokens using autoregressive (AR) models.  (a) shows the standard AR approach, where tokens are generated sequentially, one at a time. This is slow but accurately reflects the token dependencies. (b) demonstrates parallel decoding, a faster approach where multiple tokens are generated simultaneously. However, this method assumes independence between tokens, leading to inaccurate output distribution, especially when generating the entire sequence in a single step. (c) presents the proposed Distilled Decoding (DD) method. DD utilizes flow matching to deterministically map noise tokens from a Gaussian distribution to the target token distribution of the pre-trained AR model. This allows the generation of the entire token sequence in one step, matching the original model's distribution while being significantly faster.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2412.17153/x5.png", "caption": "Figure 5: AR flow matching. Given all previous tokens, the teacher AR model gives a probability vector for the next token, which defines a mixture of Dirac delta distributions over all tokens in the codebook. We then construct a deterministic mapping between the Gaussian distribution and the Dirac delta distribution with flow matching. The next noise token \u03f54subscriptitalic-\u03f54\\epsilon_{4}italic_\u03f5 start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT is sampled from the Gaussian distribution, and its corresponding token in the codebook becomes the next token q4subscript\ud835\udc5e4q_{4}italic_q start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT.", "description": "This figure illustrates the core concept of Distilled Decoding (DD).  The process begins with a pre-trained autoregressive (AR) model, which, given a sequence of previous tokens (q1, q2, q3...), provides a probability distribution for the next token. This distribution is a mixture of Dirac delta functions, where each function represents a token in the codebook and its weight is its probability. DD then leverages flow matching to create a deterministic mapping between a simple Gaussian distribution and this complex, discrete probability distribution from the AR model.  A sample from the Gaussian distribution (\u03f54) is transformed into a token (q4) using this deterministic mapping. This deterministic mapping is then learned by a neural network in the distillation phase. The result is that a simple noise input can be directly transformed into a valid output of the AR model, allowing for one-step or few-step sampling.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2412.17153/x6.png", "caption": "Figure 6: The training and generation workflow of DD. Given X1subscript\ud835\udc4b1X_{1}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT with noise tokens \u03f5isubscriptitalic-\u03f5\ud835\udc56\\epsilon_{i}italic_\u03f5 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, the whole trajectory X1,\u22ef,X5subscript\ud835\udc4b1\u22efsubscript\ud835\udc4b5X_{1},\\cdots,X_{5}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , italic_X start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT consists of data tokens qisubscript\ud835\udc5e\ud835\udc56q_{i}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and noise tokens \u03f5isubscriptitalic-\u03f5\ud835\udc56\\epsilon_{i}italic_\u03f5 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is uniquely determined (Sec.\u00a03.2). Assuming the timesteps are set to {t1=1,t2=3}formulae-sequencesubscript\ud835\udc6111subscript\ud835\udc6123\\{t_{1}=1,t_{2}=3\\}{ italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1 , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 3 }. During training (Sec.\u00a03.3), we train DD model to reconstruct X5subscript\ud835\udc4b5X_{5}italic_X start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT given X1subscript\ud835\udc4b1X_{1}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or X3subscript\ud835\udc4b3X_{3}italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT as input. The DD will then have the capability of jumping from X1subscript\ud835\udc4b1X_{1}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and X3subscript\ud835\udc4b3X_{3}italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT to any point in the later trajectory (e.g., X1subscript\ud835\udc4b1X_{1}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT to any of {X2,\u22ef,X5}subscript\ud835\udc4b2\u22efsubscript\ud835\udc4b5\\{X_{2},\\cdots,X_{5}\\}{ italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u22ef , italic_X start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT }).\nDuring generation (Sec.\u00a03.3), we can either do 1-step (X1\u2192X5\u2192subscript\ud835\udc4b1subscript\ud835\udc4b5X_{1}\\rightarrow X_{5}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2192 italic_X start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT) or 2-step generation (X1\u2192X3\u2192X5\u2192subscript\ud835\udc4b1subscript\ud835\udc4b3\u2192subscript\ud835\udc4b5X_{1}\\rightarrow X_{3}\\rightarrow X_{5}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2192 italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT \u2192 italic_X start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT). Additionally, we can do generation with more steps by incorporating the teacher AR model in part of the generation process, such as 3-step generation X1\u2192X2\u2192X3\u2192X5\u2192subscript\ud835\udc4b1subscript\ud835\udc4b2\u2192subscript\ud835\udc4b3\u2192subscript\ud835\udc4b5X_{1}\\rightarrow X_{2}\\rightarrow X_{3}\\rightarrow X_{5}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2192 italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2192 italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT \u2192 italic_X start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT where X2\u2192X3\u2192subscript\ud835\udc4b2subscript\ud835\udc4b3X_{2}\\rightarrow X_{3}italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2192 italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT utilizes the AR model and other steps use the DD model.", "description": "Figure 6 illustrates the training and generation workflow of the Distilled Decoding (DD) method.  Starting with a sequence of noise tokens (X<sub>1</sub>), a trajectory is generated using flow matching and the pre-trained autoregressive (AR) model. This trajectory (X<sub>1</sub>, ..., X<sub>5</sub>) consists of both noise and data tokens.  During training, the DD model learns to reconstruct the final state of the trajectory (X<sub>5</sub>) given intermediate states (X<sub>1</sub> or X<sub>3</sub>) as input.  This enables the DD model to \"jump\" forward in the trajectory, skipping intermediate steps.  During generation, the user can choose to generate the sequence in 1 step (directly from X<sub>1</sub> to X<sub>5</sub>), 2 steps (X<sub>1</sub> to X<sub>3</sub> then to X<sub>5</sub>), or more steps where parts of the trajectory leverage the pre-trained AR model for higher quality (e.g., a 3-step generation using DD for X<sub>1</sub> to X<sub>2</sub> and X<sub>3</sub> to X<sub>5</sub> and the AR model from X<sub>2</sub> to X<sub>3</sub>).", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/var1_update_samenoise.png", "caption": "Figure 7: The training curve of FID vs. epoch or iteration for different intermediate timesteps. FIDs are calculated with 5k generated sample.", "description": "This figure displays the training curves, showing the relationship between the Fr\u00e9chet Inception Distance (FID) score and the number of training epochs or iterations. Separate curves are shown for various choices of intermediate timesteps used during the training process of the Distilled Decoding (DD) model.  The FID score serves as an indicator of image generation quality, with lower scores representing better quality.  The plots illustrate how the FID changes as the model learns with different strategies for generating the tokens in the image. The FID scores are computed using 5000 generated samples to ensure statistical stability in evaluating the image quality at each training stage.", "section": "5.5 ABLATION STUDY"}, {"figure_path": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/var2_update_samenoise.png", "caption": "Figure 8: The training curve of FID vs. epoch for different dataset sizes. FIDs are calculated with 5k generated sample.", "description": "This figure displays the relationship between training epoch and FID scores for different dataset sizes.  Four lines represent training results using 0.6M, 0.9M, 1.2M, and 1.6M data-noise pairs, respectively.  Each FID score is an average calculated from 5,000 generated samples.  The plot shows how the FID score, a measure of image generation quality, changes over the course of training for each dataset size.  This helps evaluate the impact of data quantity on model performance.", "section": "5.5 ABLATION STUDY"}, {"figure_path": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/llamagen1_update_samenoise.png", "caption": "Figure 9: Generation results with VAR model (Tian et\u00a0al., 2024). From left to right: one-step DD model, two-step DD model, DD-pre-trained-4-6, and the pre-trained VAR model.", "description": "This figure displays a comparison of image generation results using different models.  The four images showcase the outputs of: (1) a one-step Distilled Decoding (DD) model; (2) a two-step DD model; (3) a DD model incorporating steps 4-6 of the original pre-trained VAR model; and (4) the original pre-trained VAR model (Tian et al., 2024). The comparison highlights the trade-off between the speed and quality of image generation achieved by reducing the number of steps in the autoregressive process.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/llamagen2_update_samenoise.png", "caption": "Figure 10: Generation results with VAR model (Tian et\u00a0al., 2024). From left to right: one-step DD model, two-step DD model, DD-pre-trained-4-6, and the pre-trained VAR model.", "description": "This figure showcases image generation results using various methods based on the VAR (Vector Quantized Auto-Regressive) model.  It compares outputs from four different approaches: a one-step Distilled Decoding (DD) model, a two-step DD model, a DD model incorporating steps 4-6 of the pre-trained VAR model, and the original, pre-trained VAR model. Each approach generates images for the same set of classes, allowing for a direct visual comparison of quality and speed across these methods. The image classes illustrate the diversity of the results and the model's ability to generate images across different visual categories.", "section": "5. Experiments"}]