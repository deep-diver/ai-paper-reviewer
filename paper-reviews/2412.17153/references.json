{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduces Llama, a foundational language model used for comparison in the experimental results, demonstrating the effectiveness of the proposed method on a state-of-the-art model."}, {"fullname_first_author": "Tianle Cai", "paper_title": "Medusa: Simple LLM inference acceleration framework with multiple decoding heads", "publication_date": "2024-01-10", "reason": "This paper is a relevant work that proposes an acceleration method for LLMs which is compared to the proposed approach."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper discusses instruction following in language models, which is related to the concept of few-step generation in autoregressive models and informs the discussion of the proposed method."}, {"fullname_first_author": "Yonglong Tian", "paper_title": "Visual autoregressive modeling: Scalable image generation via next-scale prediction", "publication_date": "2024-04-02", "reason": "This paper introduces VAR, a state-of-the-art image autoregressive model, which is used as a baseline model for comparison, showcasing the effectiveness of the proposed method."}, {"fullname_first_author": "Peize Sun", "paper_title": "Autoregressive model beats diffusion: Llama for scalable image generation", "publication_date": "2024-06-06", "reason": "This paper introduces LlamaGen, another state-of-the-art image autoregressive model used in the experiments, providing a strong benchmark for the proposed method."}]}