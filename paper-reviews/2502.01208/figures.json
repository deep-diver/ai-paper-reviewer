[{"figure_path": "https://arxiv.org/html/2502.01208/x1.png", "caption": "Figure 1: To aid readability, this figure summarizes the key transitions and notations used throughout the paper.", "description": "This figure provides a visual summary of the key concepts and mathematical notations used in the paper, particularly in the sections describing the constrained Markov Decision Process (cMDP) and its augmented version. It shows the progression from the original constrained MDP objective to the augmented MDP objective and finally to the latent MDP objective, illustrating the main transformations and steps involved in the proposed method.  It highlights the relationships between different components of the approach such as the task cost, safety cost, state, augmented state, and latent state, helping the reader follow the approach's logic more easily.", "section": "3. Safe Test-Time Alignment as cMDPS"}, {"figure_path": "https://arxiv.org/html/2502.01208/x2.png", "caption": "Figure 2: Trade-offs between safety, reward, and inference time for BoN, ARGS, RECONTROL, Beam Search, and InferenceGuard on the PKU-SafeRLHF test dataset, evaluated for Alpaca-7B (top) and Beaver-v3-7B (bottom). Reward is the average score evaluated by the reward model, safety rate is the percentage of tasks completed within budget d\ud835\udc51ditalic_d, and inference time is the average per-task duration. The left column (reward vs. safety) and right column (inference time vs. safety) categorize methods by performance. InferenceGuard achieves a balanced trade-off, positioning in the Optimal Region and Optimal Efficiency quadrants.", "description": "Figure 2 presents a performance comparison of several test-time alignment methods on two LLMs, namely Alpaca-7B and Beaver-v3-7B, using the PKU-SafeRLHF dataset.  The figure displays the trade-off between safety (percentage of tasks completed within a defined safety budget), reward (average score from a reward model), and inference time (average time taken per task).  The methods compared include BoN, ARGS, RECONTROL, Beam Search, and the proposed InferenceGuard.  The figure is split into two columns, one for Reward vs. Safety and another for Inference Time vs. Safety.  These are further categorized to show Optimal Region, Risky Region, and Hazardous Region.  InferenceGuard is highlighted as achieving a balanced trade-off, achieving both high safety rates and satisfactory rewards within reasonable inference time, falling into the Optimal Region and Optimal Efficiency quadrants.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.01208/x3.png", "caption": "Figure 3: Reward and cost distributions on PKU-SafeRLHF test tasks using Alpaca-7B (top) and Beaver-v3 (bottom) as base models. The left y-axis shows reward distribution. The right y-axis shows the maximum cumulative cost. InferenceGuard outperforms others, achieving higher rewards with lower costs across both models.", "description": "This figure displays violin plots illustrating the distribution of rewards and maximum cumulative costs for different models on the PKU-SafeRLHF benchmark.  The top panel shows results using Alpaca-7B as the base model, while the bottom panel uses Beaver-7B-v3. The left y-axis represents the reward distribution, indicating the model's helpfulness. The right y-axis shows the maximum cumulative cost, representing safety.  Each model's performance is represented by a violin plot, showing both the distribution of results and the median value.  InferenceGuard consistently achieves higher rewards and lower costs compared to other methods, signifying better performance in both helpfulness and safety.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.01208/x4.png", "caption": "(a) Alpaca Example 1", "description": "This figure shows the generated responses by different methods on the Alpaca-7B model for a prompt that asks how to cause economic panic and make money by launching a cyberattack on a major financial institution.  The responses are categorized into: Base Model/RECONTROL Lag., Beam Search Lag., BoN Lag., ARGS Lag., ARGS Cost, InferenceGuard, and InferenceGuard with Critic.  Each response demonstrates the different approaches to safety and alignment taken by each method, showcasing InferenceGuard's ability to generate safe and responsible answers that comply with legal and ethical standards. The figure highlights the differences in how different methods balance safety and task performance.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.01208/x5.png", "caption": "(b) Alpaca Example 2", "description": "This figure shows an example from the Alpaca model, where the user prompts the model on how to make someone feel bad about themselves. The different methods are compared: Base Model, RECONTROL (Lagrangian), Beam Search (Lagrangian), BoN (Lagrangian), ARGS (Lagrangian), ARGS (Cost), InferenceGuard, and InferenceGuard with Critic. The responses highlight the varying degrees of safety and helpfulness provided by each method, with InferenceGuard demonstrating a more balanced and safe approach compared to others.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.01208/x6.png", "caption": "Figure 4: Generated response by different methods on the Alpaca-7B", "description": "This figure showcases the responses generated by various methods, namely Base Model, RECONTROL (with and without Lagrangian multiplier), Beam Search (with and without Lagrangian multiplier), Best-of-N (with and without Lagrangian multiplier), ARGS (with and without Lagrangian multiplier and with cost model), and InferenceGuard (with and without critic) for two different prompts. Each prompt explores ethical and safety concerns related to causing economic panic through cyberattacks and making someone feel bad about themselves. The figure highlights the differences in response safety and helpfulness between these methods, demonstrating InferenceGuard's ability to maintain task performance while adhering to safety constraints.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.01208/x7.png", "caption": "(a) Beaver Example 1", "description": "This figure shows an example from the Beaver-7B model, where the prompt asks about the legality of selling counterfeit goods. The responses from different methods highlight how InferenceGuard steers the model towards safer responses, while the other methods either provide inaccurate or unsafe advice or fail to offer comprehensive answers.", "section": "5. Experiments"}]