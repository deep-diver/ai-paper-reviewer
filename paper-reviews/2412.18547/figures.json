[{"figure_path": "https://arxiv.org/html/2412.18547/x1.png", "caption": "(a) Direct answering (15 output tokens).", "description": "This figure shows a question and its answer generated by a language model without any intermediate reasoning steps.  The answer is concise and direct. The number of tokens in the model's response is specified, highlighting the brevity of the response.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.18547/x2.png", "caption": "(b) Vanilla CoT (258 output tokens).", "description": "This figure shows an example of a question being answered using the vanilla Chain-of-Thought (CoT) prompting method.  The question is a word problem about calculating the total time of someone's after-work activities. The response demonstrates the detailed, step-by-step reasoning process characteristic of vanilla CoT.  The caption indicates that this response used 258 tokens.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.18547/x3.png", "caption": "(c) CoT with an unreasonable budget (157 output tokens).", "description": "This figure shows an example of Chain-of-Thought (CoT) reasoning with an inappropriately small token budget (set to less than 10 tokens).  Despite the budget constraint, the LLM attempts to generate a detailed explanation, resulting in 157 output tokens. This highlights the 'Token Elasticity' phenomenon where a severely restrictive budget doesn't lead to proportionally shorter responses, instead often resulting in significantly longer outputs than with a more reasonable budget.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.18547/x4.png", "caption": "(d) CoT with an reasonable budget (86 output tokens).", "description": "This figure shows an example of Chain-of-Thought (CoT) reasoning with a reasonable token budget of 50.  The question is about calculating the total hours of Peyton's after-work activities.  Unlike the vanilla CoT example that uses many tokens for detailed explanations, this CoT with a budget forces the LLM to be concise in its reasoning.  The result is a much shorter chain of thought with 86 output tokens, which is significantly less than the 258 tokens in the vanilla CoT example, while still producing the correct answer.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.18547/x5.png", "caption": "Figure 1: Examples of different problem solving paradigms. The reasoning processes are highlighted.", "description": "This figure showcases various approaches to solving a math word problem using a large language model (LLM).  (a) shows a direct answer approach where the LLM attempts to directly answer the question without intermediate steps. (b) demonstrates the Chain-of-Thought (CoT) method where the LLM breaks down the problem into multiple steps, providing detailed reasoning for each step before arriving at a solution. (c) illustrates CoT with an unreasonable token budget, resulting in an overly long explanation despite the constraint. (d) exhibits CoT with a reasonable token budget; this approach leads to a more concise and effective solution while remaining within the specified token limit. This comparison emphasizes the trade-off between detail in reasoning and efficiency, highlighting how setting an appropriate token budget can improve LLM performance and resource usage.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.18547/x6.png", "caption": "(a) GPT-4o-mini budget search.", "description": "This figure shows the results of a binary search for the optimal token budget using the GPT-40-mini language model. The x-axis represents the iteration number of the search, and the y-axis represents the token budget being tested at each iteration. Different colored lines represent different samples, showing how the search process varies depending on the specific question. The figure demonstrates the concept of \"token elasticity\", where reducing the budget below a certain threshold can lead to an increase in the actual token usage.", "section": "4 Searching Optimal Token Budget"}, {"figure_path": "https://arxiv.org/html/2412.18547/x7.png", "caption": "(b) GPT-4o-mini token cost.", "description": "The figure shows the relationship between the searched token budget and the actual token cost during the budget search process using the GPT-40-mini language model.  The x-axis represents the iteration number of the binary search algorithm used to find the optimal token budget.  The y-axis represents the actual token cost incurred by the model for each searched budget.  Different colored lines represent different sample questions. The figure demonstrates that there exists a range of reasonable token budgets where the actual cost is significantly lower than when using budgets outside this range.  This illustrates the 'token elasticity' phenomenon observed by the authors, where excessively small budgets lead to unexpectedly high token usage.", "section": "Token Redundancy in LLM Reasoning"}, {"figure_path": "https://arxiv.org/html/2412.18547/x8.png", "caption": "(c) Yi-lightning budget search.", "description": "This figure shows the results of searching for the optimal token budget using the Yi-lightning language model. The x-axis represents the iteration number of the binary search algorithm used to find the optimal budget, and the y-axis shows the searched budget at each iteration. The plot visualizes the \"token elasticity\" phenomenon, where initially decreasing the budget leads to a reduction in token cost, but further reductions beyond a certain point result in increased costs. Different colored lines represent different samples, showing variability in the optimal budget and its relationship to the token cost across different samples.", "section": "4 Searching Optimal Token Budget"}, {"figure_path": "https://arxiv.org/html/2412.18547/x9.png", "caption": "(d) Yi-lightning token cost.", "description": "The figure shows the token costs for different searched budgets during the budget search process using the Yi-lightning large language model.  The x-axis represents the iteration number of the budget search, and the y-axis represents the actual number of tokens used by the model. Different colored lines represent different samples, showcasing the variability in token costs across various search instances.  The plot illustrates the 'token elasticity' phenomenon where using an unreasonably small budget can result in a higher token cost than using a more reasonable budget, demonstrating that optimal budget selection significantly influences the cost-effectiveness of the process.", "section": "Token Elasticity in LLM Reasoning"}, {"figure_path": "https://arxiv.org/html/2412.18547/x10.png", "caption": "Figure 2: Token elasticity phenomenon. The x-axis denotes the budget search iteration. The y-axis denotes the searched budget (Figure\u00a02a and Figure\u00a02c) or the real token costs for each searched budget (Figure\u00a02b and Figure\u00a02d). Different colors denote different samples.\nThe token cost is significantly lower in a reasonable token budget range. When the token budget is smaller than the reasonable range, the token cost gradually increases.", "description": "This figure illustrates the concept of \"token elasticity\" in large language models (LLMs).  The experiment systematically reduces the token budget provided to the LLM during a chain-of-thought (CoT) reasoning task. The x-axis represents the iterations of the budget reduction process, while the y-axis shows either the target budget (Figures 2a and 2c) or the actual number of tokens used by the LLM (Figures 2b and 2d).  Different colored lines represent different samples of the experiment. The figure shows that when the budget is set within a reasonable range, the LLM achieves a significant reduction in token usage. However, if the budget is set too low, the actual number of tokens used increases dramatically, exceeding the token usage when a larger budget was used. This demonstrates that there is an optimal range of token budgets for efficient LLM reasoning.", "section": "4 Searching Optimal Token Budget"}, {"figure_path": "https://arxiv.org/html/2412.18547/x11.png", "caption": "Figure 3: The effects of optimal searched budget. CoT with our optimal searched budget reduces the token costs significantly without influencing the accuracy.", "description": "This figure demonstrates the impact of using an optimal token budget in Chain-of-Thought (CoT) prompting.  It compares the token costs of CoT reasoning using different budget values. The results show that using a carefully searched optimal budget significantly reduces token costs while maintaining similar accuracy.  The figure likely includes a graph displaying token costs across different budget levels, showing a minimum point representing the optimal budget.", "section": "4 Searching Optimal Token Budget"}, {"figure_path": "https://arxiv.org/html/2412.18547/x12.png", "caption": "Figure 4: The workflow of TALE.\nGiven a question, TALE first estimates the token budget using a budget estimator. It then crafts a token-budget-aware prompt by combining the question with the estimated budget. Finally, the prompt is input to the LLM to generate the answer as the final output.", "description": "The figure illustrates the process of TALE (Token-Budget-Aware LLM Reasoning).  First, a question is inputted. TALE then uses a budget estimator to predict the optimal number of tokens for the LLM's response. This estimated token budget is incorporated into a modified prompt that includes both the original question and the budget constraint.  The augmented prompt is fed into the large language model (LLM), which generates a response that ideally stays within the specified token budget.", "section": "5 Methodology"}, {"figure_path": "https://arxiv.org/html/2412.18547/x13.png", "caption": "Figure 5: The prompt for zero-shot estimator.", "description": "This figure shows the prompt used for the zero-shot budget estimation method in the TALE framework.  The prompt instructs the large language model (LLM) to analyze a given question and estimate the minimum number of tokens needed to generate a complete and accurate response. The response format is specified as [[budget]], which ensures a consistent numerical output for processing.", "section": "5.2 Budget Estimation"}, {"figure_path": "https://arxiv.org/html/2412.18547/x14.png", "caption": "Figure 6: The instruction prompt used to format the LLM output on multiple-choice questions.", "description": "This figure shows the instruction prompt used to format the large language model's (LLM) output when answering multiple-choice questions.  The prompt ensures that the LLM's response is in a standardized format for easier evaluation, making the output directly comparable between different questions and models.  The standardized format requests a response in the format `[[choice]]`, where `choice` is replaced with the letter corresponding to the selected answer (e.g., [[A]]).", "section": "6. Evaluation"}, {"figure_path": "https://arxiv.org/html/2412.18547/x15.png", "caption": "(a) Direct answering (10 output tokens).", "description": "This figure shows an example of a question being answered using different methods. (a) demonstrates a concise, direct answer generated by the model, using only 10 output tokens.  This showcases a minimal response that directly answers the question without any intermediate reasoning steps. It highlights the trade-off between token usage and response detail.", "section": "1 Introduction"}]