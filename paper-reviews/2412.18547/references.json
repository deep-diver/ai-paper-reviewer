{"references": [{"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-XX-XX", "reason": "This paper introduces Chain-of-Thought (CoT) prompting, a foundational technique for enhancing LLM reasoning that is extensively analyzed and built upon in the target paper."}, {"fullname_first_author": "Sania Nayab", "paper_title": "Concise Thoughts: Impact of output length on LLM reasoning and cost", "publication_date": "2024-XX-XX", "reason": "This paper demonstrates the potential for compressing LLM reasoning by imposing length constraints, a key concept explored and extended in the target paper's token-budget-aware framework."}, {"fullname_first_author": "Alan Wake", "paper_title": "Yi-lightning technical report", "publication_date": "2024-XX-XX", "reason": "This paper introduces Yi-lightning, a state-of-the-art LLM used for evaluating the target paper's approach, providing a benchmark for comparison."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-XX-XX", "reason": "This paper introduces GSM8K, a benchmark dataset used in the target paper's evaluation, providing a standard for assessing the performance of different reasoning methods."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-XX-XX", "reason": "This paper describes RLHF, a crucial training technique for LLMs whose impact on the verbosity of LLM reasoning is analyzed in the target paper."}]}