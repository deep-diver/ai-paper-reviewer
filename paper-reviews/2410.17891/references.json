{"references": [{" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper introduces LLaMA, a family of open and efficient foundation language models that serves as the basis for the adaptation and scaling experiment in this study.  The authors' efficient design principles, particularly in architecture and training methods, directly impact this work's success in scaling diffusion models.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "reason": "This is a foundational paper in large language models. GPT-2, a model from this paper, is directly adapted into a diffusion model as part of the main experiment. Its impact is profound as a significant part of the methodology relies on this model as a base.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Ishaan Gulrajani", "paper_title": "Likelihood-based diffusion language models", "reason": "This paper is among the few that focus on training diffusion language models for text generation and directly inspires the experiment in this study. The paper's exploration of likelihood-based approaches and the challenges involved has a strong influence on the choice of methods and strategies in this work.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Xiang Lisa Li", "paper_title": "Diffusion-lm improves controllable text generation", "reason": "This paper presents continuous text diffusion models, which are a direct inspiration for the adaptation in this research. Its exploration of continuous text diffusion and various training objectives is highly relevant to the methods and challenges faced in this study.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Jacob Austin", "paper_title": "Structured denoising diffusion models in discrete state-spaces", "reason": "This paper introduces the framework of discrete diffusion models for text generation, which is crucial to the adaptation method in this work, as it provides a mechanism to bridge the gap between continuous and discrete representations of text, a key challenge in text-based diffusion models.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This paper is seminal in diffusion models, establishing the foundational concepts of diffusion processes. This paper is critical because it lays the theoretical groundwork that is later adapted and applied to text generation in this research.  The understanding of forward and reverse diffusion processes, crucial elements of diffusion models, is directly based on this work.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduces the Transformer architecture, a fundamental component of modern language models, including the ones utilized and adapted in this study.  The Transformer's architecture underlies the model's ability to process sequences effectively.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Gregor Bachmann", "paper_title": "The pitfalls of next-token prediction", "reason": "This paper discusses limitations of autoregressive language models, providing important context for the motivations behind exploring diffusion language models as an alternative.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "reason": "This work explores compute-optimal large language model training, relevant to the resource-intensive nature of diffusion models. The findings on optimal training strategies can inform this work's approach to scaling diffusion models.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "OpenAI", "paper_title": "Gpt-4 technical report", "reason": "This paper details GPT-4, a significant advancement in language models.  The model's capabilities highlight the current state-of-the-art in language model performance, providing a benchmark against which the newly developed diffusion models can be measured.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Daria Soboleva", "paper_title": "SlimPajama: A 627B token cleaned and deduplicated version of RedPajama", "reason": "This dataset was used to train the DiffuLLaMA model, making it a crucial component of the experimental design. This large-scale dataset provides crucial context for the paper's analysis of the performance of diffusion models compared to autoregressive models.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Guilherme Penedo", "paper_title": "The fineweb datasets: Decanting the web for the finest text data at scale", "reason": "This dataset is used for training DiffuGPT, forming a substantial part of the experimental methodology. The quality and characteristics of this dataset significantly impact the training results and subsequent performance evaluations of the model.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Alexander Quinn Nichol", "paper_title": "Improved denoising diffusion probabilistic models", "reason": "This is a highly influential work in diffusion models, introducing several important improvements. The advanced techniques in this paper are relevant to the techniques used in the study to build efficient diffusion models for text generation. The concepts are directly applied in this research.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Rowan Zellers", "paper_title": "HellaSwag: Can a machine really finish your sentence?", "reason": "HellaSwag is a benchmark dataset used in the evaluation of common sense reasoning capabilities. It helps to evaluate the ability of the newly developed diffusion models to handle this type of task effectively and provides another benchmark.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Diederik Kingma", "paper_title": "Variational diffusion models", "reason": "This paper is highly significant to diffusion models due to its presentation of the theoretical underpinning of variational diffusion models. The concepts and methods discussed are highly relevant to the fundamental theory behind text diffusion models and also the approach used in this research.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Mohammad Bavarian", "paper_title": "Efficient training of language models to fill in the middle", "reason": "The task of filling in the middle is a novel application of LLMs and directly addresses the limitations of autoregressive models in handling non-sequential text generation tasks. The methods and insights are applied in this paper.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Ari Holtzman", "paper_title": "The curious case of neural text degeneration", "reason": "This paper addresses a crucial issue in text generation, neural text degeneration, that is very relevant to evaluating the quality of text generated by diffusion models. The findings in this work inform the evaluation strategy and selection of metrics used to asses the quality of the newly developed models.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Edward Hu", "paper_title": "Amortizing intractable inference in large language models", "reason": "This work addresses the challenges of intractable inference in LLMs, specifically focusing on the computational cost. The insights are relevant to this work because they help to explain the computational demands of diffusion models and motivate the choice of adaptation methods.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Charlie Chen", "paper_title": "Accelerating large language model decoding with speculative sampling", "reason": "This work addresses the decoding speed of large language models, a critical aspect when comparing the efficiency of autoregressive and diffusion models. The decoding speed is often a major bottleneck in real-world applications.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Junxiong Wang", "paper_title": "The mamba in the llama: Distilling and accelerating hybrid models", "reason": "This research explores efficient methods for training and utilizing large language models, addressing the resource constraints often associated with diffusion models. Its findings on distillation and acceleration strategies are valuable in developing efficient diffusion models.", "section_number": 5}]}