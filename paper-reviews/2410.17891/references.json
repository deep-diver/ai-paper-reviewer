{"references": [{" publication_date": "2021", "fullname_first_author": "Jacob Austin", "paper_title": "Structured denoising diffusion models in discrete state-spaces", "reason": "This paper is foundational for understanding discrete diffusion models, a crucial aspect of the current research.  It introduces the concept of structured denoising in discrete state spaces, which is directly relevant to the adaptation of autoregressive models to diffusion models. The mathematical framework provided is essential for the theoretical underpinnings of the proposed approach. The method's effectiveness in bridging the gap between autoregressive and diffusion modeling objectives makes it a cornerstone in the field.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Mohammad Bavarian", "paper_title": "Efficient training of language models to fill in the middle", "reason": "This paper directly addresses the problem of infilling, a key application highlighted for diffusion models, which the current work also evaluates extensively. The techniques discussed in this paper for efficiently training language models to handle infilling are highly relevant and offer valuable insights into optimizing the proposed DLM adaptation method.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "reason": "This is a seminal paper in the field of large language models (LLMs), establishing the foundation for many subsequent works including the present study which leverages existing pre-trained LLMs for efficient adaptation. It presents a comprehensive analysis of the capabilities of large language models and their ability to perform various tasks with minimal fine-tuning.  This benchmark of few-shot learning capability is significant for comparing the performance of adapted diffusion language models against their autoregressive counterparts. ", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Charlie Chen", "paper_title": "Accelerating large language model decoding with speculative sampling", "reason": "This paper tackles the challenge of slow decoding speed in large language models (LLMs), an issue relevant to the evaluation of diffusion language models. The presented speculative sampling method is directly applicable to improving the inference speed of the adapted DLMs and is crucial for practical deployment of these models.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Huiwen Chang", "paper_title": "Maskgit: Masked generative image transformer", "reason": "This paper is highly relevant due to its introduction of masked image modeling techniques, which can offer valuable insights into improving masked modeling within the context of text diffusion. The success in image generation provides inspiration and transferable knowledge in adapting these techniques to the text domain.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Gregor Bachmann", "paper_title": "The pitfalls of next-token prediction", "reason": "This paper critically examines the limitations of next-token prediction, the core method employed by autoregressive language models. The limitations highlighted in this paper directly inform the rationale behind exploring alternative approaches such as diffusion language models and highlight the importance of addressing those limitations.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Mohammad Bavarian", "paper_title": "Efficient training of language models to fill in the middle", "reason": "This paper addresses a key challenge for LLMs - the ability to generate coherent text when filling in missing parts of a sequence. This capability is directly relevant to the evaluation of diffusion models, which are expected to excel in such tasks.  The techniques and findings in this paper would serve as a direct comparison to the performance of the proposed adapted diffusion models on infilling tasks, which is essential for demonstrating their capability improvement.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "reason": "This seminal work established the benchmark for few-shot learning in LLMs. The current work uses this benchmark to compare the few-shot learning capabilities of the adapted diffusion models against their autoregressive counterparts, directly demonstrating the effectiveness of the proposed adaptation technique in addressing the limitation of AR models on various tasks.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Jacob Austin", "paper_title": "Structured denoising diffusion models in discrete state-spaces", "reason": "This paper provides a crucial mathematical foundation for understanding discrete diffusion models, which are central to the proposed adaptation method. The theoretical framework and derivations in this paper directly underpin the mathematical formulations used in the current work and are essential for its theoretical rigor.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper introduces LLaMA, a powerful open-source large language model (LLM) that is used as a foundation for creating the DiffuLLaMA model in the current work. LLaMA\u2019s architecture and training methodology served as a basis for adapting an autoregressive model into a diffusion model, highlighting the efficiency of adapting pre-trained models.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This paper is a foundational work in diffusion models. The concepts and techniques introduced in this paper are directly relevant to understanding and developing text diffusion models, and the methodology provides a basis for comparison and evaluation of the proposed adaptation approach.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Diederik Kingma", "paper_title": "Variational diffusion models", "reason": "This paper is highly relevant because it provides the foundation for the variational lower bound (ELBO) used for training diffusion models, a critical aspect of the present work.  The ELBO is central to the optimization process in diffusion models, and the mathematical framework presented in this paper is directly relevant to the theoretical underpinnings and derivations of the current work\u2019s method.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "The release of the LLaMA model and its associated codebase has significantly accelerated research in LLMs. The current study directly leverages LLaMA-2 as the basis for creating DiffuLLaMA, demonstrating the practical applicability and efficiency gains from adapting existing, well-trained models. ", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Alexander Quinn Nichol", "paper_title": "Improved denoising diffusion probabilistic models", "reason": "This paper presents improvements to denoising diffusion probabilistic models (DDPMs), which are directly relevant to enhancing the performance of the text diffusion models presented in the current research. The techniques discussed are essential to improving the sampling efficiency and the quality of generated text, further enhancing the competitiveness of the proposed method.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduced the Transformer architecture, which is fundamental to many modern LLMs, including those used in the current work.  The transformer architecture is the backbone of many large language models, and understanding its workings is crucial for adapting pre-trained models to the diffusion model framework.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This paper introduced the foundational concepts and techniques of denoising diffusion probabilistic models (DDPMs), which are essential to the field of diffusion models and directly inform the understanding and development of text diffusion models discussed in this work. The original DDPM framework is crucial to understanding and improving the performance of diffusion-based text generation.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "reason": "This work introduces LLaMA-2, the direct foundation upon which the 7B parameter DiffuLLaMA model is built.  The detailed architecture and training methodology are essential aspects of this adaptation, and the results from LLaMA-2 serve as a key benchmark for evaluating the effectiveness of the proposed adaptation method.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Ishaan Gulrajani", "paper_title": "Likelihood-based diffusion language models", "reason": "This paper is highly influential due to its exploration of likelihood-based diffusion language models.  The insights into the training and optimization of diffusion models are critical for the proposed adaptation approach in the current work.  The methods presented for training and improving likelihood-based diffusion models are essential for replicating and advancing the research in this paper.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Aaron Lou", "paper_title": "Discrete diffusion language modeling by estimating the ratios of the data distribution", "reason": "This paper focuses on discrete diffusion language modeling, which is directly relevant to the current work\u2019s methodology that adapts autoregressive models into diffusion models. The techniques discussed for improving the modeling of discrete data distributions are particularly applicable to enhancing the performance of the proposed adapted models.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Emiel Hoogeboom", "paper_title": "Autoregressive diffusion models", "reason": "This paper explores autoregressive diffusion models, which is highly relevant to the current work due to its exploration of the relationship between autoregressive and diffusion models and its development of autoregressive diffusion models.  Understanding these models is crucial for adapting autoregressive language models to diffusion models.", "section_number": 3}]}