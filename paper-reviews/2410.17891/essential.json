{"reason": "This research paper explores the adaptation of pre-trained autoregressive language models into text diffusion models, achieving significant performance improvements and demonstrating the potential of diffusion models for various text generation tasks.", "summary": "Scaling text diffusion models is achieved via adapting pre-trained autoregressive models, resulting in competitive performance on various benchmarks.", "takeaways": ["A novel continual pre-training approach adapts autoregressive language models to build text diffusion models.", "The adapted diffusion models outperform existing smaller-scale models and are competitive with their autoregressive counterparts.", "The study highlights the potential of scaling text diffusion models via adaptation from existing large autoregressive models and opens new avenues for further research in text generation."], "tldr": "This paper introduces a new method for creating large text diffusion models (DLMs) by adapting existing, readily available autoregressive language models (AR). Unlike traditional DLM training from scratch, this approach leverages the extensive resources already invested in AR models.  The researchers demonstrate the efficacy of their method through a continual pre-training process, where an AR model is progressively modified to function as a DLM. This process involves strategically annealing the attention mask (gradually removing the causal masking) and adapting the shift operation from AR models. They develop models of varying sizes (127M, 355M, and 7B parameters), showing that the approach yields competitive results with the equivalent-sized AR models, even outperforming them on several benchmarks. Importantly, the generated text by these adapted DLMs is fluent, supports in-context learning, and allows for mid-text infilling without prompt rearrangement.  The improved performance across a suite of benchmarks shows the feasibility of building large DLMs through adaptation, an approach that addresses the challenge of training extremely large DLMs from scratch which can be exceptionally expensive and resource-intensive. The researchers release their models, code, and evaluation tools, furthering open-source contributions to the field."}