[{"figure_path": "2410.17891/charts/charts_6_0.png", "caption": "Figure 2: Training loss over tokens for different scales of our adapted diffusion models.", "description": "The chart displays the training loss curves for three different-sized diffusion language models (127M, 355M, and 7B parameters) across billions of training tokens.", "section": "4.1 ADAPTATION SETUP"}, {"figure_path": "2410.17891/charts/charts_8_0.png", "caption": "Figure 3: Quality evaluation for unconditional generation, with perplexity measured by GPT2 large and distinct 2-gram diversity.", "description": "The chart displays the relationship between decoding steps, generative perplexity, and distinct 2-gram diversity for various diffusion models in unconditional text generation.", "section": "4.3 LANGUAGE MODELING CAPACITIES"}, {"figure_path": "2410.17891/charts/charts_9_0.png", "caption": "Figure 4: Single batch decoding speed (seconds) for different models using flash-attention 2.", "description": "The chart compares the single-batch decoding speed of LLaMA2 and DiffuLLaMA models with varying sequence lengths and different numbers of diffusion timesteps (T).", "section": "4.5 DISCUSSIONS"}, {"figure_path": "2410.17891/charts/charts_22_0.png", "caption": "Figure 5: The unconditional generation quality for different diffusion time steps T and sampling algorithms. We annotate the temperature of top-k sampling and top-p sampling.", "description": "The chart shows the unconditional generation quality, measured by perplexity and distinct 2-gram diversity, for different diffusion time steps and sampling algorithms.", "section": "Unconditional Generation"}, {"figure_path": "2410.17891/charts/charts_22_1.png", "caption": "Figure 6: Finetune GSM8K data with discrete diffusion objectives, using a base model of either GPT2-S/M or DiffuGPT-S/M. DiffuGPT converges faster and attains a lower loss.", "description": "The chart shows that using DiffuGPT as the base model for finetuning GSM8K data with discrete diffusion objectives leads to faster convergence and lower training loss compared to using GPT2 as the base model.", "section": "4.5 DISCUSSIONS"}]