[{"figure_path": "2410.17891/charts/charts_6_0.png", "caption": "Figure 2: Training loss over tokens for different scales of our adapted diffusion models.", "description": "The chart displays the training loss curves for three different sizes of adapted diffusion language models (127M, 355M, and 7B parameters) across billions of training tokens.", "section": "4.1 ADAPTATION SETUP"}, {"figure_path": "2410.17891/charts/charts_8_0.png", "caption": "Figure 3: Quality evaluation for unconditional generation, with perplexity measured by GPT2 large and distinct 2-gram diversity.", "description": "The chart compares the generative perplexity and distinct 2-gram diversity of different diffusion models across various decoding steps, showing the trade-off between fluency and diversity.", "section": "4.3 LANGUAGE MODELING CAPACITIES"}, {"figure_path": "2410.17891/charts/charts_9_0.png", "caption": "Figure 4: Single batch decoding speed (seconds) for different models using flash-attention 2.", "description": "The chart compares the single-batch decoding time of LLaMA2 and DiffuLLaMA models with varying diffusion timesteps (T) across different generation lengths.", "section": "4.5 DISCUSSIONS"}, {"figure_path": "2410.17891/charts/charts_22_0.png", "caption": "Figure 5: The unconditional generation quality for different diffusion time steps T and sampling algorithms. We annotate the temperature of top-k sampling and top-p sampling.", "description": "The chart displays the relationship between the generative perplexity and distinct 2-gram diversity for different diffusion time steps (T) and sampling methods.", "section": "4.3 Language Modeling Capacities"}, {"figure_path": "2410.17891/charts/charts_22_1.png", "caption": "Figure 6: Finetune GSM8K data with discrete diffusion objectives, using a base model of either GPT2-S/M or DiffuGPT-S/M. DiffuGPT converges faster and attains a lower loss.", "description": "The chart displays the training loss curves for GPT2 and DiffuGPT models during fine-tuning on the GSM8K dataset, illustrating faster convergence and lower loss for DiffuGPT.", "section": "4.5 DISCUSSIONS"}]