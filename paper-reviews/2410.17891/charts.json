[{"figure_path": "2410.17891/charts/charts_6_0.png", "caption": "Figure 2: Training loss over tokens for different scales of our adapted diffusion models.", "description": "The chart displays the training loss curves for three different sized adapted diffusion language models (DiffuGPT-127M, DiffuGPT-355M, and DiffuLLaMA-7B) across various amounts of training tokens.", "section": "4.1 ADAPTATION SETUP"}, {"figure_path": "2410.17891/charts/charts_8_0.png", "caption": "Figure 3: Quality evaluation for unconditional generation, with perplexity measured by GPT2 large and distinct 2-gram diversity.", "description": "The chart displays the relationship between decoding steps, generative perplexity, and distinct 2-gram diversity for various diffusion models, comparing their performance in unconditional text generation.", "section": "4.3 LANGUAGE MODELING CAPACITIES"}, {"figure_path": "2410.17891/charts/charts_9_0.png", "caption": "Figure 4: Single batch decoding speed (seconds) for different models using flash-attention 2.", "description": "The chart compares the single batch decoding time of LLaMA2 and DiffuLLaMA models with different diffusion timesteps (T) across various generation lengths, using flash-attention 2.", "section": "4.5 DISCUSSIONS"}, {"figure_path": "2410.17891/charts/charts_22_0.png", "caption": "Figure 3: Quality evaluation for unconditional generation, with perplexity measured by GPT2 large and distinct 2-gram diversity.", "description": "The chart displays the perplexity and distinct 2-gram diversity of text generated by various diffusion models with different numbers of decoding steps.", "section": "4.3 LANGUAGE MODELING CAPACITIES"}, {"figure_path": "2410.17891/charts/charts_22_1.png", "caption": "Figure 6: Finetune GSM8K data with discrete diffusion objectives, using a base model of either GPT2-S/M or DiffuGPT-S/M. DiffuGPT converges faster and attains a lower loss.", "description": "The chart displays the training loss curves for GPT2 and DiffuGPT models during finetuning on the GSM8K dataset, showcasing DiffuGPT's faster convergence and lower loss.", "section": "4.5 DISCUSSIONS"}]