[{"figure_path": "2410.17891/figures/figures_3_0.png", "caption": "Figure 1: The overview of our approach to adapt autoregressive (AR) models to diffusion models. Left: The shift operation in AR models enables the output layer hi to approximate the distribution of next tokens Xi+1 in hidden representations through the cross entropy (CE) loss. Middle: We remove the causal mask gradually during training eventually making our model bi-directional. Right: inside the diffusion models we shift the logits to compute the loss with the next token (i.e., the loss on hi would be with respect to xi+1), while perceptually, the diffusion models are still functioning as recovering the original signals (since hi corresponds to xi+1 in AR loss).", "description": "The figure illustrates the adaptation process of converting autoregressive language models into diffusion language models by gradually removing causal masking, employing a shift operation, and using a time-embedding-free architecture.", "section": "3 MODEL"}]