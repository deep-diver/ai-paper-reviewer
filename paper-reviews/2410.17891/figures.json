[{"figure_path": "2410.17891/figures/figures_3_0.png", "caption": "Figure 1: The overview of our approach to adapt autoregressive (AR) models to diffusion models. Left: The shift operation in AR models enables the output layer hi to approximate the distribution of next tokens Xi+1 in hidden representations through the cross entropy (CE) loss. Middle: We remove the causal mask gradually during training eventually making our model bi-directional. Right: inside the diffusion models we shift the logits to compute the loss with the next token (i.e., the loss on hi would be with respect to xi+1), while perceptually, the diffusion models are still functioning as recovering the original signals (since hi corresponds to xi+1 in AR loss).", "description": "This figure illustrates the adaptation process from autoregressive language models to diffusion language models, highlighting the key steps of causal mask annealing, shift operation, and the resulting bi-directional attention mechanism.", "section": "3 MODEL"}]