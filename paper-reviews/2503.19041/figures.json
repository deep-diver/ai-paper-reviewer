[{"figure_path": "https://arxiv.org/html/2503.19041/x1.png", "caption": "Figure 1: \nOur method maintains safety alignment comparable to the seed model by rejecting harmful instructions, while achieving task performance improvements equivalent to vanilla fine-tuning.", "description": "This figure demonstrates the effectiveness of LookAhead Tuning in maintaining model safety while improving task performance.  It shows three examples: a harmful instruction (how to make a bomb), a harmless but challenging instruction (comparing two numbers), and a simple harmless instruction. The \"Seed Model\" represents the original, pre-trained model's response; \"Vanilla FT\" shows the response of a model fine-tuned using standard methods, which loses its safety in the first example. The \"Ours\" column shows that LookAhead Tuning successfully maintains safety, comparable to the seed model, even with improved performance on downstream tasks (as shown in the second example, where the number comparison was correct).", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.19041/x2.png", "caption": "Figure 2: Overview of Training Data Modification: Vanilla Fine-Tuning; LookAhead-Tuning with Real Answer (m=6\ud835\udc5a6m=6italic_m = 6); LookAhead-Tuning with Virtual Answer (P=\ud835\udc43absentP=italic_P = \u201cLet\u2019s solve this problem.\u201d). Inference data is unchanged.", "description": "This figure illustrates the differences in how training data is modified across three different methods: Vanilla Fine-Tuning, LookAhead Tuning with Real Answer, and LookAhead Tuning with Virtual Answer. Vanilla Fine-Tuning uses the original training data without any modification. LookAhead Tuning with Real Answer adds the first six tokens (m=6) of the actual answer to the input instruction. LookAhead Tuning with Virtual Answer adds a generic prefix, \u201cLet\u2019s solve this problem\u201d, to both the input instruction and the beginning of the answer.  The key takeaway is that LookAhead Tuning methods modify the training data to help preserve the model's safety by influencing the initial tokens generated. The inference data (the data the model sees during the inference stage after training) remains unchanged across all three methods.", "section": "3 Approach: LookAhead Tuning"}, {"figure_path": "https://arxiv.org/html/2503.19041/extracted/6213325/images/test_kl_last.png", "caption": "Figure 3: Per-token KL Divergence between the fine-tuned models and the original model on the Harmful HEx-PHI dataset.", "description": "This figure shows a bar chart illustrating the Kullback-Leibler (KL) divergence for each token position (1-8) between the original, pre-trained language model and several fine-tuned models.  These fine-tuned models include Vanilla Fine-Tuning, SDFT (Self-Distillation Fine-Tuning), Constrained SFT (Constrained Self-Fine-Tuning), and the two variants of LookAhead Tuning (real and virtual).  The Harmful HEx-PHI dataset was used, which contains prompts designed to elicit harmful or unsafe responses. The KL divergence quantifies the difference in the probability distributions of the first few tokens generated by the original model and the fine-tuned models when presented with harmful prompts.  Lower KL divergence indicates that the fine-tuned model's output probability distribution for initial tokens is more similar to the original model, suggesting better preservation of safety mechanisms.", "section": "4.3 Further Analysis"}, {"figure_path": "https://arxiv.org/html/2503.19041/extracted/6213325/images/test_num2.png", "caption": "Figure 4: \nLeft: Impact of increasing the number of previewed tokens on the GSM8K dataset. Right: Effectiveness of different prefixes using the virtual answer method on the GSM8K and SAMSum datasets. X-axis prefixes: p1 : \u201cLet\u2019s solve this problem.\u201d, p2 : \u201cLet\u2019s deal with this situation.\u201d, p3 : \u201cdadjalsasdfghkjzmnb\u201d. Suffix G denotes GSM8K, S denotes SAMSum. For instance, p1G refers to results on GSM8K with the prefix \u201cLet\u2019s solve this problem\u201d.", "description": "This figure shows the results of two experiments. The left panel displays how increasing the number of previewed tokens in the real answer method impacts the GSM8K dataset's performance.  The right panel demonstrates the effectiveness of using different prefixes in the virtual answer method on both the GSM8K and SAMSum datasets.  Three different prefixes were tested: \"Let's solve this problem\", \"Let's deal with this situation\", and a nonsensical string.  The results are presented using RSR, JSR, and UTILITY metrics for both datasets.", "section": "4 Experiments"}]