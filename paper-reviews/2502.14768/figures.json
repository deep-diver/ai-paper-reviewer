[{"figure_path": "https://arxiv.org/html/2502.14768/x1.png", "caption": "Figure 1: Validation accuracy and mean response length during RL training, illustrating how the model autonomously learns to allocate more thinking compute for improved performance. Remarkably, the model also demonstrates impressive generalization on completely unseen datasets (AIME, AMC).", "description": "This figure displays the validation accuracy and average response length of a language model during reinforcement learning (RL) training.  The x-axis represents the training step, showing progress over time. The y-axis on the left shows validation accuracy, indicating how well the model performs on unseen data. The y-axis on the right shows the mean response length, demonstrating the model's increasing use of 'thinking' steps.  The plot shows a clear correlation: as the model's performance improves, it allocates more resources to deliberation. The plot also includes the performance on two external, unseen benchmarks (AIME and AMC) that demonstrate the model's generalization ability after training.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2502.14768/extracted/6220745/figure/rq1.png", "caption": "Figure 2: Comparison of GRPO (Blue), REINFORCE++ (Red), and PPO (Green) performance (averaged by sliding window = 50) in terms of training speed, accuracy, and reward gain.", "description": "This figure compares the performance of three reinforcement learning algorithms: GRPO, REINFORCE++, and PPO.  The comparison is made across three key metrics: training speed (time taken to reach a certain number of training steps), accuracy (how well the model performs on a validation set), and reward gain (the amount of reward accumulated during training). The results are averaged using a sliding window of 50 steps to smooth out any noise and highlight the overall trends.  The graph visually represents how each algorithm performs over a set of training steps.", "section": "RQ 1: How Does GRPO Compare to Other RL Algorithms?"}, {"figure_path": "https://arxiv.org/html/2502.14768/extracted/6220745/figure/rq7.png", "caption": "Figure 3: Impact of complex reasoning behaviours and language mixing on reasoning performance. We analyzed the model\u2019s answer rewards for responses containing the tokens shown in the figure. Responses with \"verify\" and \"re-evaluate\" scored significantly higher than those without these words. Conversely, responses containing certain tokens from other languages generally received lower scores.", "description": "This figure examines the effects of complex reasoning behaviors and language mixing on the model's performance.  The analysis focuses on the impact of specific tokens, such as \"verify\" and \"re-evaluate,\" which are associated with higher scores when present in the model's responses. Conversely, the inclusion of tokens from languages other than English leads to lower scores, highlighting the model's sensitivity to language consistency and its preference for explicit verification during the reasoning process. The bar chart visually represents the average answer scores with and without these key tokens and language mixing, providing a clear comparison of their influence on the model's reasoning abilities.", "section": "RQ 2. Do certain thinking tokens and language-mixing phenomena improve reasoning?"}, {"figure_path": "https://arxiv.org/html/2502.14768/extracted/6220745/figure/word_frequency_charts/verify.png", "caption": "(a) Verify", "description": "The figure shows the frequency of the word \"verify\" appearing in the model's responses during the first 1800 training steps. The frequency increases gradually over time, indicating that the model increasingly uses the word \"verify\" as it learns to perform more complex reasoning, which involves self-verification. This is consistent with the model's development of emergent reasoning behaviors like reflection and verification discussed in the paper.", "section": "RQ 3: Does an 'Aha Moment' Emerge During Training?"}, {"figure_path": "https://arxiv.org/html/2502.14768/extracted/6220745/figure/word_frequency_charts/re-evaluate.png", "caption": "(b) Re-evaluate", "description": "The figure shows the impact of specific tokens on reasoning performance.  The word \"re-evaluate\" is highlighted because its presence in the model's reasoning process leads to significantly higher average answer scores compared to when it is absent. This suggests that the act of reconsidering and refining one's reasoning process is a key factor in improved performance. The graph visually demonstrates this effect by comparing average answer scores with and without the token \"re-evaluate\".", "section": "RQ 2. Do certain thinking tokens and language-mixing phenemona improve reasoning?"}, {"figure_path": "https://arxiv.org/html/2502.14768/extracted/6220745/figure/word_frequency_charts/check.png", "caption": "(c) Check", "description": "The figure shows the frequency of the word \"check\" appearing in the model's responses during training. The frequency increases gradually over time, suggesting that the model's self-verification behavior develops steadily, not suddenly.", "section": "RQ 3: Does an 'Aha Moment' Emerge During Training?"}, {"figure_path": "https://arxiv.org/html/2502.14768/extracted/6220745/figure/word_frequency_charts/yet.png", "caption": "(d) Yet", "description": "This figure displays the frequency of the word \"yet\" appearing in the model's responses during the first 1800 training steps of the reinforcement learning process.  The steady, gradual increase in frequency suggests there was no sudden \"aha moment\" or breakthrough in the model's reasoning abilities. Instead, the development of more complex reasoning behaviors, including reflection and self-verification, appears to have been incremental and continuous.", "section": "RQ 3: Does an 'Aha Moment' Emerge During Training?"}, {"figure_path": "https://arxiv.org/html/2502.14768/extracted/6220745/figure/word_frequency_charts/lets.png", "caption": "(e) Let\u2019s", "description": "The figure displays the frequency of specific words during the first 1800 steps of the RL training process. It visualizes how the frequency of words related to reflection ('verify', 're-evaluate', 'check'), cautiousness ('yet'), conversational phrases ('let's'), and even a Chinese word, changed over time.  The relatively gradual and consistent increase in frequency of these words, rather than a sudden spike, suggests a lack of a distinct 'aha moment' in the model's learning process.", "section": "RQ 3: Does an 'Aha Moment' Emerge During Training?"}, {"figure_path": "https://arxiv.org/html/2502.14768/extracted/6220745/figure/word_frequency_charts/chinese_yes.png", "caption": "(f) Chinese word", "description": "The figure shows the frequency of Chinese words appearing in the English responses generated by the model during the RL training process.  This unexpected appearance of Chinese words within responses primarily written in English suggests the model might be utilizing or accessing internal representations or mechanisms that involve Chinese vocabulary, even when the task and training data primarily use English.", "section": "RQ 4: Can the Model Generalize to Out-of-Distribution (OOD) Tasks?"}, {"figure_path": "https://arxiv.org/html/2502.14768/extracted/6220745/figure/limem_rl32.png", "caption": "Figure 4: Tracking the frequency of words in the first 1,800 training steps. 1. Reflective words like \"check\" and \"verify\" slowly increased (a)-(c). 2. Conversational phrases (e.g., \"Let\u2019s\") and cautious terms (e.g., \"yet\") became more frequent (d)-(e), 3. Chinese words began appearing in English responses (f). The frequency of all these words developed steadily without sudden jumps, suggesting that there may not be a distinct \"aha moment.\"", "description": "This figure displays the frequency of specific words over the initial 1800 training steps.  It visually demonstrates the gradual increase in the use of reflective words ('check', 'verify'), conversational phrases ('Let\u2019s'), and cautious terms ('yet').  Interestingly, the appearance of Chinese words in English responses is also tracked. The steady, non-abrupt increase in word frequency suggests a continuous learning process rather than a sudden 'aha moment' of understanding.", "section": "RQ 3: Does an 'Aha Moment' Emerge During Training?"}, {"figure_path": "https://arxiv.org/html/2502.14768/extracted/6220745/figure/limem_rft32.png", "caption": "Figure 5: Training Step vs. Accuracy on AIME (2021-2024) and AMC (2022-2023) Datasets.", "description": "This figure displays the model's accuracy on the American Invitational Mathematics Examination (AIME) from 2021-2024 and the American Mathematics Competitions (AMC) from 2022-2023, demonstrating its generalization capabilities to out-of-distribution datasets.  The x-axis represents the training step, showing the model's performance improvement over time. The y-axis shows the accuracy on each dataset.  The figure showcases the model's ability to generalize its learned reasoning skills beyond the specific training data.", "section": "RQ 4: Can the Model Generalize to Out-of-Distribution (OOD) Tasks?"}, {"figure_path": "https://arxiv.org/html/2502.14768/x4.png", "caption": "(a) RL", "description": "Figure 6 presents a comparison of the generalization capabilities of Reinforcement Learning (RL) and Reject Sampling Fine-Tuning (RFT).  The x-axis represents the Local Inconsistency-based Memorization Score (LiMem), which measures how sensitive the model is to changes in problem structure.  A high LiMem indicates memorization of training data, whereas a low LiMem signifies robust generalization.  The y-axis depicts test accuracy on unseen data.  The results demonstrate that RL achieves significantly higher accuracy on perturbed test instances (i.e., those with changes to wording or order) compared to RFT, indicating that RL learns true reasoning skills rather than memorizing superficial patterns. In contrast, RFT prioritizes superficial pattern matching, resulting in higher memorization, and diminished accuracy on unseen data.", "section": "RQ 5: Which Generalizes Better, SFT or RL?"}, {"figure_path": "https://arxiv.org/html/2502.14768/extracted/6220745/figure/rq3.png", "caption": "(b) RFT", "description": "Figure 6b presents the results of a Reject Sampling Fine-Tuning (RFT) experiment. It displays the relationship between the model's memorization level (LiMem) and its accuracy on unseen test data. The x-axis represents LiMem, measuring the extent to which the model has memorized the training data. The y-axis shows the accuracy on test data, indicating the model's ability to generalize to unseen examples. The figure showcases that as the model memorizes more training data (increasing LiMem), its ability to generalize to new data decreases (lower accuracy). This contrasts sharply with the results shown in Figure 6a, which illustrate the generalization capability of reinforcement learning, where the model's ability to generalize to new data increases along with its memorization of training data.", "section": "RQ 5: Which Generalizes Better, SFT or RL?"}, {"figure_path": "https://arxiv.org/html/2502.14768/extracted/6220745/figure/base_instruct/base_length.png", "caption": "Figure 6: RFT memorizes while RL generalizes. RFT (Reject sampling Fine-Tuning) slightly improves test accuracy at the expense of rapidly increasing L\u2062i\u2062M\u2062e\u2062m\u2062(f;T\u2062r)\ud835\udc3f\ud835\udc56\ud835\udc40\ud835\udc52\ud835\udc5a\ud835\udc53\ud835\udc47\ud835\udc5fLiMem(f;Tr)italic_L italic_i italic_M italic_e italic_m ( italic_f ; italic_T italic_r ), indicating it mainly learns superficial answer format than geniue reasoning. In contrast, RL achieves higher test accuracy with minimal or even negative increase in L\u2062i\u2062M\u2062e\u2062m\u2062(f;T\u2062r)\ud835\udc3f\ud835\udc56\ud835\udc40\ud835\udc52\ud835\udc5a\ud835\udc53\ud835\udc47\ud835\udc5fLiMem(f;Tr)italic_L italic_i italic_M italic_e italic_m ( italic_f ; italic_T italic_r ). Within the same L\u2062i\u2062M\u2062e\u2062m\ud835\udc3f\ud835\udc56\ud835\udc40\ud835\udc52\ud835\udc5aLiMemitalic_L italic_i italic_M italic_e italic_m interval, RL outperform RFT in test acc greatly, suggesting better generalization ability.", "description": "This figure compares the generalization capabilities of Reinforcement Learning (RL) and Reject sampling Fine-Tuning (RFT).  The x-axis represents the memorization score (LiMem(f;Tr)), measuring how much a model relies on memorizing the training data rather than learning underlying reasoning principles. The y-axis shows the test accuracy. RFT initially shows higher test accuracy but quickly increases its memorization score, demonstrating superficial learning focused on memorizing answer formats.  In contrast, RL achieves higher test accuracy with minimal or even negative increases in the memorization score, indicating a genuine understanding of reasoning principles and superior generalization ability.", "section": "RQ 5: Which Generalizes Better, SFT or RL?"}]