<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis &#183; HF Daily Paper Reviews by AI"><meta name=description content="Infinity, a novel bitwise autoregressive model, sets new records in high-resolution image synthesis, outperforming top diffusion models in speed and quality."><meta name=keywords content="Computer Vision,Image Generation,üè¢ ByteDance,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis"><meta property="og:description" content="Infinity, a novel bitwise autoregressive model, sets new records in high-resolution image synthesis, outperforming top diffusion models in speed and quality."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-05T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-05T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Image Generation"><meta property="article:tag" content="üè¢ ByteDance"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/cover.png"><meta name=twitter:title content="Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis"><meta name=twitter:description content="Infinity, a novel bitwise autoregressive model, sets new records in high-resolution image synthesis, outperforming top diffusion models in speed and quality."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis","headline":"Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis","abstract":"Infinity, a novel bitwise autoregressive model, sets new records in high-resolution image synthesis, outperforming top diffusion models in speed and quality.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.04431\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2024","dateCreated":"2024-12-05T00:00:00\u002b00:00","datePublished":"2024-12-05T00:00:00\u002b00:00","dateModified":"2024-12-05T00:00:00\u002b00:00","keywords":["Computer Vision","Image Generation","üè¢ ByteDance"],"mainEntityOfPage":"true","wordCount":"5538"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About This Project">About</p></a><a href=/ai-paper-reviewer/2025-02-04/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-02-04s>2025-02-04</p></a><a href=/ai-paper-reviewer/2025-02-05/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-02-05s>2025-02-05</p></a><a href=/ai-paper-reviewer/2025-02-06/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-02-06s>2025-02-06</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="Paper Reviews by AI">Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About This Project">About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-04/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-02-04s>2025-02-04</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-05/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-02-05s>2025-02-05</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-06/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-02-06s>2025-02-06</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="Paper Reviews by AI">Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.04431/cover_hu_1dd604e498cbfa65.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.04431/>Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-05T00:00:00+00:00>5 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>5538 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">26 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.04431/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.04431/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/image-generation/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Image Generation
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-bytedance/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ ByteDance</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu_97e7d424fadd1c26.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#bitwise-autoregressive>Bitwise Autoregressive</a></li><li><a href=#infinite-vocabulary>Infinite Vocabulary</a></li><li><a href=#self-correction-training>Self-Correction Training</a></li><li><a href=#scaling-laws-in-var>Scaling Laws in VAR</a></li><li><a href=#high-res-image-synth>High-Res Image Synth</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#bitwise-autoregressive>Bitwise Autoregressive</a></li><li><a href=#infinite-vocabulary>Infinite Vocabulary</a></li><li><a href=#self-correction-training>Self-Correction Training</a></li><li><a href=#scaling-laws-in-var>Scaling Laws in VAR</a></li><li><a href=#high-res-image-synth>High-Res Image Synth</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.04431</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Jian Han et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2024-12-06</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.04431 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.04431 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/infinity-scaling-bitwise-autoregressive target=_self role=button>‚Üó Papers with Code</a></p><audio controls><source src=https://ai-paper-reviewer.com/2412.04431/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current text-to-image models face challenges such as limited detail in high-resolution images and slow generation speeds, especially for autoregressive models. Existing models often rely on index-wise tokenization, which restricts scalability and can lead to quantization errors impacting image quality. Furthermore, training methods like teacher-forcing can introduce inconsistencies between training and inference.</p><p>The researchers introduced Infinity, a bitwise autoregressive model that tackles these issues. By utilizing a bitwise visual tokenizer, infinite-vocabulary classifier, and a bitwise self-correction mechanism, Infinity significantly improves generation capacity and detail. The model establishes new benchmarks for autoregressive text-to-image models, outperforming leading diffusion models in multiple evaluation metrics while achieving much faster inference speeds. The use of bitwise tokens offers a potential pathway to more efficient and scalable visual generation.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-f6a8223ef97f2fd2aa00ce5602ca0e37></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-f6a8223ef97f2fd2aa00ce5602ca0e37",{strings:[" Infinity achieves state-of-the-art results in high-resolution image synthesis, surpassing leading diffusion models. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-a8f4679952fe2872f97e73a7ff15dc59></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-a8f4679952fe2872f97e73a7ff15dc59",{strings:[" The novel bitwise modeling approach significantly improves the scalability and detail of autoregressive models. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-adb90667b041ecbc8e748232d1c6cdd8></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-adb90667b041ecbc8e748232d1c6cdd8",{strings:[" Infinity demonstrates impressive speed, generating high-quality 1024x1024 images in under a second. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial because it <strong>significantly advances high-resolution image synthesis</strong>, achieving state-of-the-art results and surpassing leading diffusion models in speed and quality. It introduces novel bitwise modeling techniques opening new avenues for visual generation research. This <strong>pushes the boundaries of autoregressive models</strong> and offers a potentially more efficient and scalable approach for future advancements in image generation.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/x1.png alt></figure></p><blockquote><p>üîº Figure 1 presents several high-resolution images generated by the Infinity model, demonstrating its ability to accurately interpret and generate images based on diverse textual prompts. The images showcase various capabilities of the model: precise adherence to the instructions within prompts (prompt following), creation of images with realistic spatial relationships between objects (spatial reasoning), accurate rendering of text with different styles and fonts (text rendering), and high aesthetic quality across different artistic styles and aspect ratios. This visual demonstration highlights Infinity&rsquo;s superior performance in advanced image synthesis.</p><details><summary>read the caption</summary>Figure 1: High-resolution image synthesis results from Infinity, showcasing its capabilities in precise prompt following, spatial reasoning, text rendering, and aesthetics across different styles and aspect ratios.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Methods</th><th># Params</th><th>GenEval Two Obj.</th><th>GenEval Position</th><th>GenEval Color Attri.</th><th>GenEval Overall</th><th>DPG Global</th><th>DPG Relation</th><th>DPG Overall</th></tr></thead><tbody><tr><td>Diffusion Models</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LDM [49]</td><td>1.4B</td><td>0.29</td><td>0.02</td><td>0.05</td><td>0.37</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SDv1.5 [49]</td><td>0.9B</td><td>0.38</td><td>0.04</td><td>0.06</td><td>0.43</td><td>74.63</td><td>73.49</td><td>63.18</td></tr><tr><td>PixArt-alpha [13]</td><td>0.6B</td><td>0.50</td><td>0.08</td><td>0.07</td><td>0.48</td><td>74.97</td><td>82.57</td><td>71.11</td></tr><tr><td>SDv2.1 [49]</td><td>0.9B</td><td>0.51</td><td>0.07</td><td>0.17</td><td>0.50</td><td>77.67</td><td>80.72</td><td>68.09</td></tr><tr><td>DALL-E 2 [45]</td><td>6.5B</td><td>0.66</td><td>0.10</td><td>0.19</td><td>0.52</td><td>-</td><td>-</td><td>-</td></tr><tr><td>DALL-E 3 [7]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.67‚Ä†</td><td>90.97</td><td>90.58</td><td>83.50</td></tr><tr><td>SDXL [43]</td><td>2.6B</td><td>0.74</td><td>0.15</td><td>0.23</td><td>0.55</td><td>83.27</td><td>86.76</td><td>74.65</td></tr><tr><td>PixArt-Sigma [12]</td><td>0.6B</td><td>0.62</td><td>0.14</td><td>0.27</td><td>0.55</td><td>86.89</td><td>86.59</td><td>80.54</td></tr><tr><td>SD3 (d=24) [21]</td><td>2B</td><td>0.74</td><td>0.34</td><td>0.36</td><td>0.62</td><td>-</td><td>-</td><td>84.08</td></tr><tr><td>SD3 (d=38) [21]</td><td>8B</td><td>0.89</td><td>0.34</td><td>0.47</td><td>0.71</td><td>-</td><td>-</td><td>-</td></tr><tr><td>AutoRegressive Models</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LlamaGen [55]</td><td>0.8B</td><td>0.34</td><td>0.07</td><td>0.04</td><td>0.32</td><td></td><td></td><td>65.16</td></tr><tr><td>Chameleon [59]</td><td>7B</td><td>-</td><td>-</td><td>-</td><td>0.39</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HART [58]</td><td>732M</td><td>-</td><td>-</td><td>-</td><td>0.56</td><td>-</td><td>-</td><td>80.89</td></tr><tr><td>Show-o [70]</td><td>1.3B</td><td>0.80</td><td>0.31</td><td>0.50</td><td>0.68</td><td>-</td><td>-</td><td>67.48</td></tr><tr><td>Emu3 [66]</td><td>8.5B</td><td>0.81‚Ä†</td><td>0.49‚Ä†</td><td>0.45‚Ä†</td><td>0.66‚Ä†</td><td>-</td><td>-</td><td>81.60</td></tr><tr><td>Infinity</td><td>2B</td><td>0.85‚Ä†</td><td>0.49‚Ä†</td><td>0.57‚Ä†</td><td>0.73‚Ä†</td><td>93.11</td><td>90.76</td><td>83.46</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a quantitative comparison of different text-to-image generation models on two benchmark datasets: GenEval and DPG. GenEval assesses the model&rsquo;s ability to generate images that accurately reflect the given prompt, focusing on aspects like object presence, attributes, and relationships. The DPG benchmark evaluates the overall quality of the generated images. The table shows each model&rsquo;s performance in terms of generating images with two objects, accurate positioning, correct color representation, and appropriate attributes. The overall scores for each dataset are displayed, along with a breakdown of the scores on global and relational aspects of the image generation task. A symbol ( ‚Ä† ) indicates that prompt rewriting was used for that particular model&rsquo;s evaluation.</p><details><summary>read the caption</summary>Table 1: Evaluation on the GenEval¬†[24] and DPG¬†[29] benchmark. ‚Ä†‚Ä†\dagger‚Ä† result is with prompt rewriting.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Bitwise Autoregressive<div id=bitwise-autoregressive class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#bitwise-autoregressive aria-label=Anchor>#</a></span></h4><p>The concept of &ldquo;Bitwise Autoregressive&rdquo; modeling presents a novel approach to image synthesis. It leverages the strengths of autoregressive models, particularly their scalability and generalizability, while addressing limitations in high-resolution image generation. <strong>The core innovation lies in replacing traditional index-wise tokenization with bitwise tokenization.</strong> This allows for a vastly expanded vocabulary size, effectively approaching an infinite vocabulary, leading to more precise and detailed image reconstructions. Furthermore, <strong>the bitwise framework enables the development of an infinite-vocabulary classifier</strong>, drastically reducing the number of parameters required compared to traditional classifiers and alleviating computational constraints associated with extremely large vocabularies. The incorporation of <strong>bitwise self-correction mechanisms enhances training stability and improves generation quality by mitigating the train-test discrepancy common in autoregressive models.</strong> This novel approach, therefore, represents a significant advancement in visual autoregressive modeling, offering improved scalability, efficiency, and image quality compared to previous methods.</p><h4 class="relative group">Infinite Vocabulary<div id=infinite-vocabulary class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#infinite-vocabulary aria-label=Anchor>#</a></span></h4><p>The concept of &ldquo;Infinite Vocabulary&rdquo; in the context of image generation models signifies a significant advancement in autoregressive modeling. Traditional methods relied on fixed-size vocabularies for representing image tokens, leading to limitations in capturing intricate details and diverse visual styles. <strong>An infinite vocabulary overcomes this constraint by allowing the model to represent an unbounded number of visual tokens</strong>, essentially handling any image feature without quantization limitations. This approach theoretically eliminates the information loss inherent in mapping continuous visual data to a discrete vocabulary, paving the way for <strong>higher-fidelity image generation with richer details and more nuanced aesthetics</strong>. <strong>The practical implementation likely involves techniques like bitwise modeling</strong>, which encodes visual features using a sequence of bits instead of fixed-length indices. This permits an exponentially larger vocabulary size while maintaining computational efficiency. <strong>This innovative method significantly enhances the model&rsquo;s expressive power, allowing it to handle highly detailed and complex images more effectively</strong>, achieving near-continuous representation similar to variational autoencoders but with the added advantage of causal autoregressive modeling, thus significantly improving generation quality and enabling more robust image synthesis.</p><h4 class="relative group">Self-Correction Training<div id=self-correction-training class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#self-correction-training aria-label=Anchor>#</a></span></h4><p>Self-correction training, in the context of visual autoregressive models, addresses the inherent limitations of traditional teacher-forcing methods. Teacher-forcing, while effective in initial training, often leads to a significant <strong>train-test discrepancy</strong>, hindering the model&rsquo;s ability to generalize to unseen data. The core idea behind self-correction is to introduce controlled noise or errors into the training process, forcing the model to learn to identify and correct these mistakes. This is achieved by deliberately perturbing the training data, either randomly or systematically, and allowing the model to recover from these perturbations. This approach is especially valuable in high-resolution image generation where cumulative errors during sequential prediction can severely impact the final result. By incorporating self-correction, the model learns to be more robust and less susceptible to cascading errors, leading to improved image quality and detail preservation. <strong>Bitwise self-correction</strong>, as presented in the research, offers a particularly efficient and effective mechanism by manipulating individual bits within the discrete token representations. This allows for fine-grained control over the introduced noise, enabling the model to learn more effectively from subtle errors. The resulting models demonstrate improved generation quality, achieving a higher degree of visual fidelity and adhering more precisely to the given textual prompts. The process also enhances the models&rsquo; capacity for generating images across diverse styles and aspect ratios.</p><h4 class="relative group">Scaling Laws in VAR<div id=scaling-laws-in-var class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#scaling-laws-in-var aria-label=Anchor>#</a></span></h4><p>The concept of &ldquo;Scaling Laws in VAR&rdquo; (Visual Autoregressive models) explores how performance changes as key model parameters, like the number of parameters, dataset size, and computational resources, increase. <strong>Understanding these scaling laws is crucial for efficient model development.</strong> A key insight is that simply increasing model size without considering other factors may not yield proportional improvements. Instead, <strong>optimal scaling involves a balanced increase across all relevant parameters</strong>, achieving a synergy where the combined impact exceeds the sum of individual effects. This often requires carefully curated datasets and efficient training strategies. <strong>Research into VAR scaling laws aims to identify optimal resource allocation for achieving target performance levels with maximal efficiency.</strong> The potential for substantial performance gains through strategic scaling makes understanding scaling laws a critical area for improving visual autoregressive model capabilities and reducing computational costs. <strong>Further research could focus on quantifying the relationships between different parameters and exploring novel training techniques to optimize scaling behavior.</strong> This improved understanding could lead to a more efficient design of future VAR models, ultimately enabling the generation of higher-quality images with less computational expense.</p><h4 class="relative group">High-Res Image Synth<div id=high-res-image-synth class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#high-res-image-synth aria-label=Anchor>#</a></span></h4><p>High-resolution image synthesis (High-Res Image Synth) is a challenging area of research, aiming to generate realistic and detailed images at high resolutions. The advancements discussed in the paper significantly impact this field. <strong>Bitwise autoregressive modeling</strong>, a novel approach, is presented as a key innovation, enabling the generation of high-quality 1024x1024 images efficiently. This method surpasses traditional index-wise tokenization, leading to better detail and faster generation. The <strong>Infinite-Vocabulary Classifier</strong> is another significant contribution, resolving computational limitations associated with very large vocabularies in autoregressive models. Furthermore, the technique of <strong>bitwise self-correction</strong> addresses training issues, enhancing the model&rsquo;s ability to generate consistent and accurate results. <strong>Dynamic aspect ratio handling</strong> expands the capabilities of the model, enabling it to generate images of various sizes, rather than being limited to square images. The paper&rsquo;s empirical results and benchmark comparisons showcase that this method achieves state-of-the-art performance, outperforming leading diffusion models in speed and quality, suggesting significant progress in the field of high-resolution image synthesis.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/x2.png alt></figure></p><blockquote><p>üîº Figure 2 illustrates the core concept of the visual tokenizer used in Infinity. The left side shows a conventional classifier that predicts indices from continuous features. A small change in the continuous features can result in a large change in the predicted index, leading to instability and requiring an exponentially growing number of parameters as the number of bits increases (d). The right side shows Infinity&rsquo;s Infinite-Vocabulary Classifier (IVC) which predicts individual bits instead of indices. This approach makes the classifier significantly more robust to small perturbations of continuous values, with only a small change in predicted bit values resulting from small changes in input values. Crucially, the number of parameters required by the IVC increases only linearly with the number of bits, making it highly scalable even with a massive vocabulary, in contrast to the exponentially growing needs of the conventional classifier. The example of d=32 and h=2048 illustrates the huge difference in parameter counts: 8.8 trillion for the conventional classifier versus 0.13 million for the IVC.</p><details><summary>read the caption</summary>Figure 2: Visual tokenizer quantizes continuous features and then gets index labels. Conventional classifier (left) predicts 2dsuperscript2ùëë2^{d}2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT indices. Infinite-Vocabulary Classifier (right) predicts dùëëditalic_d bits instead. Slight perturbations to near-zero values in continuous features cause a complete change of index labels. Bit labels (i.e. quantized features) change subtly and still provide steady supervision. Besides, parameters of conventional classifiers grow exponentially as dùëëditalic_d increases, while IVC grows linearly. If d=32ùëë32d=32italic_d = 32 and h=2048‚Ñé2048h=2048italic_h = 2048, the conventional classifier requires 8.8 trillion parameters, exceeding current compute limits. By contrast, IVC only requires 0.13M parameters.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/x3.png alt></figure></p><blockquote><p>üîº Figure 3 illustrates the architecture of the Infinity model, which uses bitwise modeling. The core components are: 1. <strong>Bitwise Multi-Scale Visual Tokenizer:</strong> This tokenizer encodes the input image into a sequence of residual feature maps (R1, R2, &mldr;, Rk) at different resolutions (scales). Each Rk represents the residual information at a particular scale. This is a departure from traditional tokenization methods that use a single, fixed vocabulary. 2. <strong>Infinite-Vocabulary Classifier (IVC):</strong> Instead of predicting indices from a fixed-size vocabulary (like in conventional models), the IVC predicts the bits that compose each token directly. This allows for a theoretically infinite vocabulary size, significantly increasing model capacity and enabling finer-grained control over image details. 3. <strong>Bitwise Self-Correction:</strong> This mechanism introduces noise or errors into the predictions of earlier residual maps (R1, R2, &mldr;, Rk-1). The model then attempts to correct these errors, leading to more robust and accurate image generation. This technique helps to mitigate cumulative errors from the autoregressive nature of the model. The process is autoregressive; each residual map Rk is predicted based on the preceding maps (R1, R2, &mldr;, Rk-1) and the text prompt, using a cross-attention mechanism. Importantly, the prediction is performed at the bit level instead of the index level, leading to a more efficient and powerful model.</p><details><summary>read the caption</summary>Figure 3: Framework of Infinity. Infinity introduces bitwise modeling, which incorporates a bitwise multi-scale visual tokenizer, Infinite-Vocabulary Classifier (IVC), and Bitwise Self-Correction. When predicting ùëπksubscriptùëπùëò\bm{R}_{k}bold_italic_R start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, the sequence (ùëπ1,ùëπ2,‚Ä¶,ùëπk‚àí1)subscriptùëπ1subscriptùëπ2‚Ä¶subscriptùëπùëò1(\bm{R}_{1},{\bm{R}}_{2},...,\bm{R}_{k-1})( bold_italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_R start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_R start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT ) serves as the prefixed context and the text condition guides the prediction through a cross attention mechanism. Different from VAR, Infinity performs next-scale prediction with bit labels.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/x4.png alt></figure></p><blockquote><p>üîº This figure showcases examples of high-resolution images generated by the Infinity model. Each image is accompanied by a short description of the prompt used to generate it. The images demonstrate Infinity&rsquo;s capabilities in various aspects of image synthesis including precise prompt following, accurate text rendering, diverse artistic styles and accurate depiction of spatial relationships. The variety of scenes and styles exemplifies the model&rsquo;s versatility and ability to generate photorealistic results.</p><details><summary>read the caption</summary>Figure 4: Qualitative results from Infinity.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/extracted/6046736/images/human_preference.png alt></figure></p><blockquote><p>üîº This figure displays the results of a human preference evaluation comparing the image generation quality of the Infinity model to several other open-source models. Participants were shown pairs of images, one generated by Infinity and one by a competing model, and asked to choose the image they preferred based on three criteria: overall quality, prompt following, and visual aesthetics. The bar chart visualizes the win rate (percentage of times each model was preferred) for each model across these three criteria, showing that Infinity was significantly preferred by the participants compared to the other models.</p><details><summary>read the caption</summary>Figure 5: Human Preference Evaluation. We ask users to select the better one in a side-by-side comparison in terms of Overall Quality, Prompt Following, and Visual Aesthetics. Infinity is more preferred by humans compared to other open-source models.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/x5.png alt></figure></p><blockquote><p>üîº This figure presents a qualitative comparison of prompt-following capabilities across six different text-to-image models, including Infinity-2B and four others. Each row shows the same prompt given to each model, and the generated images are displayed side-by-side. Key phrases from the prompt are highlighted in red to emphasize instances where Infinity-2B faithfully renders the text while other models fail to do so. This demonstrates Infinity-2B&rsquo;s superior ability to accurately follow detailed instructions provided in prompts.</p><details><summary>read the caption</summary>Figure 6: Prompt-following qualitative comparison. We highlight text in red that Infinity-2B consistently adheres to while the other four models fail to follow. Zoom in for better comparison.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/x6.png alt></figure></p><blockquote><p>üîº Figure 7 showcases the text rendering capabilities of the Infinity-2B model. It presents several examples of images generated from diverse prompts, highlighting the model&rsquo;s ability to accurately render text in various styles, fonts, and contexts. The prompts range from simple text phrases to more complex descriptions requiring specific styles, settings, and subject matter. The figure demonstrates the model&rsquo;s strong performance in generating text-consistent images regardless of the prompt&rsquo;s complexity or creative demands.</p><details><summary>read the caption</summary>Figure 7: Text rendering results from our Infinity-2B model. Infinity-2B could generate text-consistent images following user prompts across diverse categories.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/x7.png alt></figure></p><blockquote><p>üîº Figure 8 demonstrates the impact of using an Infinite-Vocabulary Classifier (IVC) for image generation. The left side shows results from a conventional classifier predicting index-wise labels, while the right side displays results from the IVC predicting bitwise labels. The comparison highlights that the IVC produces images with significantly richer details and finer visual qualities. This improvement is attributed to the ability of the IVC to handle a much larger vocabulary space effectively, leading to more precise and nuanced representation of image features during the generation process.</p><details><summary>read the caption</summary>Figure 8: Impact of Infinite-Vocabulary Classifier. Predicting bitwise labels with the Infinite-Vocabulary Classifier (Right) generates images with richer details compared to predicting index-wise labels using a conventional classifier (Left).</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/extracted/6046736/images/scaling_vae_bits_three_column.jpg alt></figure></p><blockquote><p>üîº Figure 9 illustrates the impact of increasing the vocabulary size on model performance in text-to-image generation. The experiment uses consistent training hyperparameters across different model sizes, focusing on the effect of changing the vocabulary size, represented as Vd (where d is the exponent of 2, indicating the number of bits used in the tokenizer). Smaller models (125M and 361M parameters) converge faster and achieve better results when the vocabulary size is Vd=2^16. However, when using a larger model (2.2B parameters), using Vd=2^32 outperforms Vd=2^16, demonstrating that the optimal vocabulary size is dependent on the model&rsquo;s complexity. The data used for this experiment comprises 5 million high-quality image-text pairs, and all images were 256x256 pixels.</p><details><summary>read the caption</summary>Figure 9: Effects of Scaling Up the Vocabulary. We analyze the impact of scaling the vocabulary size under consistent training hyperparameters throughout. Vocabulary size Vd=216subscriptùëâùëësuperscript216V_{d}=2^{16}italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT converges faster and achieves better results for small models (125M and 361M parameters). As we scale up the model size to 2.2B, Infinity with a vocabulary size Vd=232subscriptùëâùëësuperscript232V_{d}=2^{32}italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 32 end_POSTSUPERSCRIPT beats that one with Vd=216subscriptùëâùëësuperscript216V_{d}=2^{16}italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT. Experiment with 5M high-quality image-text pair data under 256√ó256256256256\times 256256 √ó 256 resolution.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/extracted/6046736/images/scaling_models.jpg alt></figure></p><blockquote><p>üîº This figure shows the impact of scaling the model size on the performance of visual autoregressive modeling. The experiment involved training models with 10 million pre-training data points and images at a 256x256 resolution. The key finding is that as the model size increases, the validation loss consistently decreases, indicating improved model performance. Furthermore, the validation loss serves as a strong indicator of overall model performance, exhibiting a high correlation with holistic image evaluation metrics. This supports the idea that larger models trained for longer durations lead to better results.</p><details><summary>read the caption</summary>Figure 10: Effects of Scaling Visual AutoRegressive Modeling. We analyze the impact of scaling model size under consistent training hyperparameters throughout (Experiment with 10M pre-training data and 256√ó256256256256\times 256256 √ó 256 resolution). Validation loss smoothly decreases as a function of the model size and training iterations. Besides, Validation loss is a strong predictor of overall model performance. There is a strong correlation between validation loss and holistic image evaluation metrics.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/x8.png alt></figure></p><blockquote><p>üîº Figure 11 visually demonstrates the positive impact of increased model size and training computational resources on the quality of image generation. As model size and training compute scale up, both the semantic accuracy (how well the generated image reflects the intended meaning) and the visual fidelity (sharpness, detail, and overall realism) show consistent and significant improvement. The figure showcases a series of images generated under various scaling conditions, enabling a clear visual comparison of the quality enhancements.</p><details><summary>read the caption</summary>Figure 11: Semantics and visual quality improve consistently with scaling up model size and training compute. Zoom in for better comparison.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/x9.png alt></figure></p><blockquote><p>üîº Figure 12 demonstrates the effectiveness of Bitwise Self-Correction in mitigating the train-test discrepancy inherent in teacher-forcing training. The left side shows the results of using teacher-forcing training, where the generated images suffer from significant discrepancies between training and inference. This leads to degraded performance during actual inference. In contrast, the right side illustrates how Bitwise Self-Correction effectively addresses this issue by automatically correcting mistakes during the generation process, thus resulting in significantly improved image quality. The generation parameters used here are œÑ=1 and cfg=3.</p><details><summary>read the caption</summary>Figure 12: Impact of Self-Correction. Teacher-forcing training introduces great train-test discrepancy which degrades performance during inference (left). Bitwise Self-Correction auto-corrects mistakes and thus generates better results (right). Decoding with œÑ=1ùúè1\tau=1italic_œÑ = 1 and c‚Å¢f‚Å¢g=3ùëêùëìùëî3cfg=3italic_c italic_f italic_g = 3.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/extracted/6046736/images/pe_ablation.jpg alt></figure></p><blockquote><p>üîº Figure 13 presents a comparison of training performance between two approaches to positional encoding in visual autoregressive models: learnable Absolute Position Embeddings (APE) as used in previous work, and the method proposed in the paper, which combines Rotary Position Embedding (RoPE2d) with learnable scale embeddings. The results show that the proposed method, applied to features at each scale, leads to faster convergence and higher training accuracy. This is likely because RoPE2d preserves the intrinsic 2D structure of images, while learnable scale embeddings efficiently adapt to varying sequence lengths caused by differing aspect ratios at various scales. The learnable APE approach, in contrast, can be less effective due to a much larger number of parameters that need to be optimized and the inherent difficulty of distinguishing features across different resolutions.</p><details><summary>read the caption</summary>Figure 13: Comparison between learnable APE and our positional embeddings. Our method, i.e., applying RoPE2d along with learnable scale embeddings on features of each scale, converges faster and reaches higher training accuracy.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/x10.png alt></figure></p><blockquote><p>üîº Figure 14 presents a comparison of image generation results using different sampling methods: Greedy Sampling, Normal Sampling, Pyramid CFG, and the authors&rsquo; proposed method. The figure visually demonstrates that the authors&rsquo; method produces images with significantly richer details and a stronger alignment between the generated image content and the input text prompt compared to the other three sampling techniques. The improved detail and alignment suggest that the authors&rsquo; sampling strategy is more effective at capturing the nuances of the text prompt and translating them into high-quality, faithful image generation.</p><details><summary>read the caption</summary>Figure 14: Comparison of different sampling methods. In contrast to Greedy Sample, Normal Sample and Pyramid Sample, our method could generate images with richer details and higher text-image alignments.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/extracted/6046736/images/Categories.png alt></figure></p><blockquote><p>üîº This figure shows a pie chart visualizing the distribution of prompt categories used in the paper&rsquo;s experiments. The categories represent different subject matters or themes of the image generation prompts, such as human, animal, foods, plants, architecture, products/artifacts, text rendering, landscape, and interior scenes. Each slice of the pie chart is proportionally sized to represent the percentage of prompts belonging to each category. This gives a reader a clear overview of the types of images the model was trained and evaluated on, highlighting the variety and balance in the dataset.</p><details><summary>read the caption</summary>Figure 15: Distribution of Prompt Categories</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/extracted/6046736/images/Prompts_Challenges.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of challenges present in the prompts used to evaluate the image generation models. The pie chart breaks down the percentage of prompts that fall into various categories of difficulty, such as simple prompts, complex prompts, those requiring nuanced understanding of semantics, specific color requests, precise positioning or perspective, particular artistic styles, detailed descriptions, or creative imagination. The analysis provides insight into the range and complexity of the prompts used for model evaluation, ensuring a comprehensive assessment of the models&rsquo; capabilities.</p><details><summary>read the caption</summary>Figure 16: Distribution of Prompts Challenges</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04431/x11.png alt></figure></p><blockquote><p>üîº Figure 17 presents a qualitative comparison of text-to-image (T2I) generation results. It compares the Infinity-2B model against four other open-source models: three diffusion models (Flux Schnell, SD3-Medium, and PixArt Sigma) and one autoregressive model (HART). The figure showcases several example prompts and their corresponding generated images from each model, highlighting the differences in image quality, detail, adherence to the prompt, and overall visual aesthetics. By visually examining the generated images for each prompt, one can assess the relative strengths and weaknesses of the different models in terms of their image generation capabilities. The differences in style, detail, and faithfulness to the prompt provide insights into the various techniques employed in each model. Zooming in on the images enhances the ability to compare fine details and subtle differences between the outputs.</p><details><summary>read the caption</summary>Figure 17: T2I qualitative comparison among our Infinity-2B model and the other four open-source models. Here we select three diffusion models (Flux Schnell, SD3-Medium and PixArt Sigma), one AR model (HART) for comparison. Zoom in for better comparsion.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Methods</th><th># Params</th><th>ImageReward ‚Üë</th><th></th><th></th><th>HPSv2.1 ‚Üë</th><th></th><th></th><th>Latency ‚Üì</th><th></th><th></th></tr></thead><tbody><tr><td>SD-XL [43]</td><td>2.6B</td><td>4</td><td>0.600</td><td>4</td><td>30.06</td><td>4</td><td>2.7s</td><td></td><td></td><td></td></tr><tr><td>SD3-Medium [21]</td><td>2B</td><td>3</td><td>0.871</td><td>3</td><td>30.91</td><td>3</td><td>2.1s</td><td></td><td></td><td></td></tr><tr><td>PixArt Sigma [12]</td><td>630M</td><td>2</td><td>0.872</td><td>2</td><td>31.47</td><td>2</td><td>1.1s</td><td></td><td></td><td></td></tr><tr><td><strong>Infinity</strong></td><td>2B</td><td>1</td><td><strong>0.962</strong></td><td>1</td><td><strong>32.25</strong></td><td>1</td><td><strong>0.8s</strong></td><td></td><td></td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of different text-to-image generation models, including Infinity, in terms of human preference metrics (ImageReward and HPSv2.1) and inference latency. Higher scores in ImageReward and HPSv2.1 indicate better human preference. Lower latency values indicate faster generation speed. The comparison focuses on state-of-the-art (SoTA) open-source models to benchmark Infinity&rsquo;s performance. The results demonstrate that Infinity achieves the highest scores in terms of human preference for both metrics and simultaneously boasts the fastest inference time.</p><details><summary>read the caption</summary>Table 2: Human Preference Metrics and Inference Latency. We compared our method with SoTA open-source models. Infinity achieved the best human preference results with the fastest speed.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Quantizer</th><th>d=16</th><th>d=18</th><th>d=20</th><th>d=32</th><th>d=64</th></tr></thead><tbody><tr><td>LFQ</td><td>37.6</td><td>53.7</td><td>OOM</td><td>OOM</td><td>OOM</td></tr><tr><td><strong>BSQ</strong></td><td>32.4</td><td>32.4</td><td>32.4</td><td>32.4</td><td>32.4</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the memory usage (in gigabytes) of two different quantization methods, LFQ and BSQ, during the training process of a visual autoregressive model. The comparison is made across various codebook dimensions (d), representing the number of bits used to represent each visual token. The results demonstrate that as the codebook dimension increases, the memory efficiency of BSQ (Bitwise Spherical Quantizer) significantly surpasses that of LFQ (Learned Fixed-Point Quantizer). This superior memory efficiency of BSQ enables the model to scale to an extremely large vocabulary size of 2<sup>64</sup>, which is crucial for generating high-resolution images.</p><details><summary>read the caption</summary>Table 3: Comparison of memory consumption (GB) between different quantizers during training. As codebook dimension dùëëditalic_d increases, MSR-BSQ shows significant advantages over MSR-LFQ, enabling nearly infinite vocabulary size of 264superscript2642^{64}2 start_POSTSUPERSCRIPT 64 end_POSTSUPERSCRIPT.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>VAE (stride=16)</th><th>TYPE</th><th>IN-256 rFID‚Üì</th><th>IN-512 rFID‚Üì</th></tr></thead><tbody><tr><td>$V_{d}=2^{16}$</td><td>Discrete</td><td>1.22</td><td>0.31</td></tr><tr><td>$V_{d}=2^{24}$</td><td>Discrete</td><td>0.75</td><td>0.30</td></tr><tr><td>$V_{d}=2^{32}$</td><td>Discrete</td><td>0.61</td><td>0.23</td></tr><tr><td>$V_{d}=2^{64}$</td><td>Discrete</td><td>0.33</td><td>0.15</td></tr><tr><td>SD VAE [49]</td><td>Contiguous</td><td>0.87</td><td>N/A</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of the ImageNet-rFID scores achieved by different visual tokenizers (discrete and continuous) in the context of VAEs (Variational Autoencoders). By increasing the vocabulary size of the discrete tokenizer, the results demonstrate that a discrete tokenizer can surpass the performance of a continuous VAE. The table showcases how scaling the vocabulary of the discrete tokenizer leads to improved results on ImageNet-rFID, suggesting the benefits of high-capacity discrete representations for image reconstruction.</p><details><summary>read the caption</summary>Table 4: By scaling up visual tokenizer‚Äôs vocabulary, discrete tokenizer surpasses continuous VAE of SD¬†[48] on ImageNet-rFID.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Classifier</th><th># Params</th><th>vRAM</th><th>Recons. Loss ‚Üì</th><th>FID ‚Üì</th><th>ImageReward ‚Üë</th><th>HPSv2.1 ‚Üë</th></tr></thead><tbody><tr><td>Convention</td><td>124M</td><td>2GB</td><td>0.184</td><td>4.49</td><td>0.79</td><td>31.95</td></tr><tr><td>IVC</td><td>0.65M</td><td>10MB</td><td>0.180</td><td>3.83</td><td>0.91</td><td>32.31</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 5 presents a comparison of the Infinite-Vocabulary Classifier (IVC) against a conventional classifier. Both classifiers are tasked with predicting visual tokens, but IVC uses a significantly more efficient architecture. The table shows that IVC reduces the number of parameters by 99.95% while achieving lower reconstruction loss and FID (Fr√©chet Inception Distance), a metric assessing the quality of generated images, and higher ImageReward and HPSv2.1 scores (measures of human preference). This demonstrates IVC&rsquo;s superior performance in terms of both efficiency and image generation quality, particularly when considering a large vocabulary size (2^16).</p><details><summary>read the caption</summary>Table 5: IVC saves 99.95% params and gets better performance to conventional classifier (Vd=216)V_{d}=2^{16})italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT )</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th># Params</th><th>GFLOPs</th><th>Hidden Dimension</th><th>Heads</th><th>Layers</th></tr></thead><tbody><tr><td>125M</td><td>30</td><td>768</td><td>8</td><td>12</td></tr><tr><td>361M</td><td>440</td><td>1152</td><td>12</td><td>16</td></tr><tr><td>940M</td><td>780</td><td>1536</td><td>16</td><td>24</td></tr><tr><td>2.2B</td><td>1500</td><td>2080</td><td>20</td><td>32</td></tr><tr><td>4.7B</td><td>2600</td><td>2688</td><td>24</td><td>40</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the model architectures used in the scaling experiments for visual autoregressive modeling. It shows how the number of parameters, GFLOPs (floating-point operations), hidden dimension, number of attention heads, and number of transformer layers were varied to explore the effects of model scaling on performance. Note that the GFLOPs are approximate because the computational cost is influenced by the length of the text prompt used in each image generation task.</p><details><summary>read the caption</summary>Table 6: Model architectures for scaling visual autoregressive modeling. Note that GFLOPs are rough values since they are affected by the length of the text prompt.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>FID ‚Üì</th><th>ImageReward ‚Üë</th><th>HPSv2.1 ‚Üë</th></tr></thead><tbody><tr><td>Baseline</td><td>9.76</td><td>0.52</td><td>29.53</td></tr><tr><td>Baseline + Random Flip</td><td>9.69</td><td>0.52</td><td>29.20</td></tr><tr><td>Baseline + Bitwise Self-Correction</td><td>3.48</td><td>0.76</td><td>30.71</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of an ablation study evaluating the impact of Bitwise Self-Correction (BSC) on the performance of the Infinity model. The experiment used 5 million high-quality images with a resolution of 512x512 pixels. The FID (Fr√©chet Inception Distance) metric was calculated on a validation set of 40,000 images. The decoding parameters used were œÑ (tau) = 1 and cfg (classifier-free guidance) = 3. The table compares the performance of the baseline model (without BSC), a model with random bit flips (simulating errors), and the model with BSC. The metrics reported are FID, ImageReward, and HPSv2.1, indicating the impact of BSC on image quality, adherence to prompts, and overall aesthetic appeal.</p><details><summary>read the caption</summary>Table 7: Bitwise Self-Correction makes significant improvements. Experiment with 5M high-quality data and 512√ó512512512512\times 512512 √ó 512 resolution. FID is measured on the validation set with 40K images. Decoding with œÑ=1ùúè1\tau=1italic_œÑ = 1 and c‚Å¢f‚Å¢g=3ùëêùëìùëî3cfg=3italic_c italic_f italic_g = 3.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>FID‚Üì</th><th>ImageReward‚Üë</th><th>HPSv2.1‚Üë</th></tr></thead><tbody><tr><td>w/o Bitwise Self-Correction</td><td>9.76</td><td>0.515</td><td>29.53</td></tr><tr><td>Bitwise Self-Correction (p=10%)</td><td>3.45</td><td>0.751</td><td>30.47</td></tr><tr><td>Bitwise Self-Correction (p=20%)</td><td>3.48</td><td>0.763</td><td>30.71</td></tr><tr><td>Bitwise Self-Correction (p=30%)</td><td><strong>3.33</strong></td><td><strong>0.775</strong></td><td><strong>31.05</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the ablation study results on the impact of different Bitwise Self-Correction (BSC) strengths on the model&rsquo;s performance. The experiment used 5 million high-quality images with a resolution of 512x512 pixels. The decoding parameters were set to œÑ (tau) = 1 and cfg = 3. The table shows how varying the probability of randomly flipping bits during BSC (10%, 20%, and 30%) affects the FID score, ImageReward, and HPSv2.1 scores, which are metrics evaluating image quality and human preference.</p><details><summary>read the caption</summary>Table 8: Comparison between different strengths of Bitwise Self-Correction. Experiment with 5M high-quality data and 512√ó512512512512\times 512512 √ó 512 resolution. Decoding with œÑ=1ùúè1\tau=1italic_œÑ = 1 and c‚Å¢f‚Å¢g=3ùëêùëìùëî3cfg=3italic_c italic_f italic_g = 3.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>Param</th><th>FID ‚Üì</th><th>ImageReward ‚Üë</th><th>HPSv2.1 ‚Üë</th></tr></thead><tbody><tr><td>Greedy Sampling</td><td>œÑ=0.01,cfg=1</td><td>9.97</td><td>0.397</td><td>30.98</td></tr><tr><td>Normal Sampling</td><td>œÑ=1.00,cfg=1</td><td>4.84</td><td>0.706</td><td>31.59</td></tr><tr><td>Pyramid CFG</td><td>œÑ=1.00,cfg=1‚Üí3</td><td>3.48</td><td>0.872</td><td><strong>32.48</strong></td></tr><tr><td>Pyramid CFG</td><td>œÑ=1.00,cfg=1‚Üí5</td><td>2.98</td><td>0.929</td><td>32.32</td></tr><tr><td>CFG on features</td><td>œÑ=1.00,cfg=3</td><td>3.00</td><td>0.953</td><td>32.13</td></tr><tr><td>CFG on logits</td><td>œÑ=1.00,cfg=3</td><td>2.91</td><td>0.952</td><td>32.31</td></tr><tr><td>CFG on logits (Ours)</td><td>œÑ=1.00,cfg=4</td><td><strong>2.82</strong></td><td><strong>0.962</strong></td><td>32.25</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares different decoding methods used in the Infinity model for text-to-image generation, showing their performance across various metrics such as FID (Fr√©chet Inception Distance), ImageReward, and HPSv2.1. The methods compared include Greedy Sampling, Normal Sampling, different configurations of Classifier-Free Guidance (CFG), and the authors&rsquo; proposed method. The results demonstrate the impact of different decoding strategies on the quality and fidelity of the generated images.</p><details><summary>read the caption</summary>Table 9: Comparison between different decoding methods.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Aspect Ratio</th><th>Resolution</th><th>Scale Schedule</th></tr></thead><tbody><tr><td>1.000 (1:1)</td><td>1024 √ó 1024</td><td>(1,1), (2,2), (4,4), (6,6), (8,8), (12,12), (16,16), (20,20), (24,24), (32,32), (40,40), (48,48), (64,64)</td></tr><tr><td>0.800 (4:5)</td><td>896 √ó 1120</td><td>(1,1), (2,2), (3,3), (4,5), (8,10), (12,15), (16,20), (20,25), (24,30), (28,35), (36,45), (44,55), (56,70)</td></tr><tr><td>1.250 (5:4)</td><td>1120 √ó 896</td><td>(1,1), (2,2), (3,3), (5,4), (10,8), (15,12), (20,16), (25,20), (30,24), (35,28), (45,36), (55,44), (70,56)</td></tr><tr><td>0.750 (3:4)</td><td>864 √ó 1152</td><td>(1,1), (2,2), (3,4), (6,8), (9,12), (12,16), (15,20), (18,24), (21,28), (27,36), (36,48), (45,60), (54,72)</td></tr><tr><td>1.333 (4:3)</td><td>1152 √ó 864</td><td>(1,1), (2,2), (4,3), (8,6), (12,9), (16,12), (20,15), (24,18), (28,21), (36,27), (48,36), (60,45), (72,54)</td></tr><tr><td>0.666 (2:3)</td><td>832 √ó 1248</td><td>(1,1), (2,2), (2,3), (4,6), (6,9), (10,15), (14,21), (18,27), (22,33), (26,39), (32,48), (42,63), (52,78)</td></tr><tr><td>1.500 (3:2)</td><td>1248 √ó 832</td><td>(1,1), (2,2), (3,2), (6,4), (9,6), (15,10), (21,14), (27,18), (33,22), (39,26), (48,32), (63,42), (78,52)</td></tr><tr><td>0.571 (4:7)</td><td>768 √ó 1344</td><td>(1,1), (2,2), (3,3), (4,7), (6,11), (8,14), (12,21), (16,28), (20,35), (24,42), (32,56), (40,70), (48,84)</td></tr><tr><td>1.750 (7:4)</td><td>1344 √ó 768</td><td>(1,1), (2,2), (3,3), (7,4), (11,6), (14,8), (21,12), (28,16), (35,20), (42,24), (56,32), (70,40), (84,48)</td></tr><tr><td>0.500 (1:2)</td><td>720 √ó 1440</td><td>(1,1), (2,2), (2,4), (3,6), (5,10), (8,16), (11,22), (15,30), (19,38), (23,46), (30,60), (37,74), (45,90)</td></tr><tr><td>2.000 (2:1)</td><td>1440 √ó 720</td><td>(1,1), (2,2), (4,2), (6,3), (10,5), (16,8), (22,11), (30,15), (38,19), (46,23), (60,30), (74,37), (90,45)</td></tr><tr><td>0.400 (2:5)</td><td>640 √ó 1600</td><td>(1,1), (2,2), (2,5), (4,10), (6,15), (8,20), (10,25), (12,30), (16,40), (20,50), (26,65), (32,80), (40,100)</td></tr><tr><td>2.500 (5:2)</td><td>1600 √ó 640</td><td>(1,1), (2,2), (5,2), (10,4), (15,6), (20,8), (25,10), (30,12), (40,16), (50,20), (65,26), (80,32), (100,40)</td></tr><tr><td>0.333 (1:3)</td><td>592 √ó 1776</td><td>(1,1), (2,2), (2,6), (3,9), (5,15), (7,21), (9,27), (12,36), (15,45), (18,54), (24,72), (30,90), (37,111)</td></tr><tr><td>3.000 (3:1)</td><td>1776 √ó 592</td><td>(1,1), (2,2), (6,2), (9,3), (15,5), (21,7), (27,9), (36,12), (45,15), (54,18), (72,24), (90,30), (111,37)</td></tr></tbody></table></table></figure><blockquote><p>üîº This table shows predefined scale schedules used by the Infinity model for generating images with various aspect ratios. Each row represents a different aspect ratio (e.g., 1:1, 4:3, 16:9), and the columns list the height and width resolutions (h, w) at each of the 13 scales (K=13) used in the model&rsquo;s next-scale prediction process. The model starts with small resolution images at the first scale and gradually upsamples them to 1024x1024 (or other specified resolutions) over 13 prediction steps. This pre-defined schedule ensures consistent training and efficient generation across different aspect ratios.</p><details><summary>read the caption</summary>Table 10: Predefined scale schedules {(h1r,w1r),‚Ä¶,(hKr,wKr)}subscriptsuperscript‚Ñéùëü1subscriptsuperscriptùë§ùëü1‚Ä¶subscriptsuperscript‚Ñéùëüùêæsubscriptsuperscriptùë§ùëüùêæ\{(h^{r}_{1},w^{r}_{1}),...,(h^{r}_{K},w^{r}_{K})\}{ ( italic_h start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_w start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ‚Ä¶ , ( italic_h start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT , italic_w start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ) } for different aspect ratios. Following the text guided next-scale prediction scheme, Infinity takes KùêæKitalic_K=13 scales to generate a 1024√ó1024102410241024\times 10241024 √ó 1024 (or other aspect ratio) image.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-6a0e9a3c1ec6f4844b7338113e87d3f3 class=gallery><img src=https://ai-paper-reviewer.com/2412.04431/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04431/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/&amp;title=Infinity:%20Scaling%20Bitwise%20AutoRegressive%20Modeling%20for%20High-Resolution%20Image%20Synthesis" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/&amp;text=Infinity:%20Scaling%20Bitwise%20AutoRegressive%20Modeling%20for%20High-Resolution%20Image%20Synthesis" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/&amp;subject=Infinity:%20Scaling%20Bitwise%20AutoRegressive%20Modeling%20for%20High-Resolution%20Image%20Synthesis" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.04431/index.md",oid_likes="likes_paper-reviews/2412.04431/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.04003/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-05T00:00:00+00:00>5 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.04280/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-05T00:00:00+00:00>5 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title=Tags>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>