[{"content": "| Methods | # Params | GenEval Two Obj. | GenEval Position | GenEval Color Attri. | GenEval Overall | DPG Global | DPG Relation | DPG Overall |\n|---|---|---|---|---|---|---|---|---|\n| Diffusion Models |  |  |  |  |  |  |  |  |\n| LDM [49] | 1.4B | 0.29 | 0.02 | 0.05 | 0.37 | - | - | - |\n| SDv1.5 [49] | 0.9B | 0.38 | 0.04 | 0.06 | 0.43 | 74.63 | 73.49 | 63.18 |\n| PixArt-alpha [13] | 0.6B | 0.50 | 0.08 | 0.07 | 0.48 | 74.97 | 82.57 | 71.11 |\n| SDv2.1 [49] | 0.9B | 0.51 | 0.07 | 0.17 | 0.50 | 77.67 | 80.72 | 68.09 |\n| DALL-E 2 [45] | 6.5B | 0.66 | 0.10 | 0.19 | 0.52 | - | - | - |\n| DALL-E 3 [7] | - | - | - | - | 0.67\u2020 | 90.97 | 90.58 | 83.50 |\n| SDXL [43] | 2.6B | 0.74 | 0.15 | 0.23 | 0.55 | 83.27 | 86.76 | 74.65 |\n| PixArt-Sigma [12] | 0.6B | 0.62 | 0.14 | 0.27 | 0.55 | 86.89 | 86.59 | 80.54 |\n| SD3 (d=24) [21] | 2B | 0.74 | 0.34 | 0.36 | 0.62 | - | - | 84.08 |\n| SD3 (d=38) [21] | 8B | 0.89 | 0.34 | 0.47 | 0.71 | - | - | - |\n| AutoRegressive Models |  |  |  |  |  |  |  |  |\n| LlamaGen [55] | 0.8B | 0.34 | 0.07 | 0.04 | 0.32 |  |  | 65.16 |\n| Chameleon [59] | 7B | - | - | - | 0.39 | - | - | - |\n| HART [58] | 732M | - | - | - | 0.56 | - | - | 80.89 |\n| Show-o [70] | 1.3B | 0.80 | 0.31 | 0.50 | 0.68 | - | - | 67.48 |\n| Emu3 [66] | 8.5B | 0.81\u2020 | 0.49\u2020 | 0.45\u2020 | 0.66\u2020 | - | - | 81.60 |\n| Infinity | 2B | 0.85\u2020 | 0.49\u2020 | 0.57\u2020 | 0.73\u2020 | 93.11 | 90.76 | 83.46 |", "caption": "Table 1: Evaluation on the GenEval\u00a0[24] and DPG\u00a0[29] benchmark. \u2020\u2020\\dagger\u2020 result is with prompt rewriting.", "description": "This table presents a quantitative comparison of different text-to-image generation models on two benchmark datasets: GenEval and DPG.  GenEval assesses the model's ability to generate images that accurately reflect the given prompt, focusing on aspects like object presence, attributes, and relationships.  The DPG benchmark evaluates the overall quality of the generated images.  The table shows each model's performance in terms of generating images with two objects, accurate positioning, correct color representation, and appropriate attributes.  The overall scores for each dataset are displayed, along with a breakdown of the scores on global and relational aspects of the image generation task.  A symbol ( \u2020 ) indicates that prompt rewriting was used for that particular model's evaluation.", "section": "Experiment"}, {"content": "| Methods | # Params | ImageReward \u2191 |  |  | HPSv2.1 \u2191 |  |  | Latency \u2193 |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| SD-XL [43] | 2.6B | 4 | 0.600 | 4 | 30.06 | 4 | 2.7s |  |  |  |\n| SD3-Medium [21] | 2B | 3 | 0.871 | 3 | 30.91 | 3 | 2.1s |  |  |  |\n| PixArt Sigma [12] | 630M | 2 | 0.872 | 2 | 31.47 | 2 | 1.1s |  |  |  |\n| **Infinity** | 2B | 1 | **0.962** | 1 | **32.25** | 1 | **0.8s** |  |  |  |", "caption": "Table 2:  Human Preference Metrics and Inference Latency. We compared our method with SoTA open-source models. Infinity achieved the best human preference results with the fastest speed.", "description": "This table presents a comparison of different text-to-image generation models, including Infinity, in terms of human preference metrics (ImageReward and HPSv2.1) and inference latency.  Higher scores in ImageReward and HPSv2.1 indicate better human preference. Lower latency values indicate faster generation speed.  The comparison focuses on state-of-the-art (SoTA) open-source models to benchmark Infinity's performance.  The results demonstrate that Infinity achieves the highest scores in terms of human preference for both metrics and simultaneously boasts the fastest inference time.", "section": "4.3 Text-to-Image Generation"}, {"content": "| Quantizer | d=16 | d=18 | d=20 | d=32 | d=64 |\n|---|---|---|---|---|---| \n| LFQ | 37.6 | 53.7 | OOM | OOM | OOM |\n| **BSQ** | 32.4 | 32.4 | 32.4 | 32.4 | 32.4 |", "caption": "Table 3:  Comparison of memory consumption (GB) between different quantizers during training. As codebook dimension d\ud835\udc51ditalic_d increases, MSR-BSQ shows significant advantages over MSR-LFQ, enabling nearly infinite vocabulary size of 264superscript2642^{64}2 start_POSTSUPERSCRIPT 64 end_POSTSUPERSCRIPT.", "description": "This table compares the memory usage (in gigabytes) of two different quantization methods, LFQ and BSQ, during the training process of a visual autoregressive model.  The comparison is made across various codebook dimensions (d), representing the number of bits used to represent each visual token. The results demonstrate that as the codebook dimension increases, the memory efficiency of BSQ (Bitwise Spherical Quantizer) significantly surpasses that of LFQ (Learned Fixed-Point Quantizer). This superior memory efficiency of BSQ enables the model to scale to an extremely large vocabulary size of 2<sup>64</sup>, which is crucial for generating high-resolution images.", "section": "3.2 Visual Tokenizer"}, {"content": "| VAE (stride=16) | TYPE | IN-256 rFID\u2193 | IN-512 rFID\u2193 |\n|---|---|---|---|\n| $V_{d}=2^{16}$ | Discrete | 1.22 | 0.31 |\n| $V_{d}=2^{24}$ | Discrete | 0.75 | 0.30 |\n| $V_{d}=2^{32}$ | Discrete | 0.61 | 0.23 |\n| $V_{d}=2^{64}$ | Discrete | 0.33 | 0.15 |\n| SD VAE [49] | Contiguous | 0.87 | N/A |", "caption": "Table 4:  By scaling up visual tokenizer\u2019s vocabulary, discrete tokenizer surpasses continuous VAE of SD\u00a0[48] on ImageNet-rFID.", "description": "This table presents a comparison of the ImageNet-rFID scores achieved by different visual tokenizers (discrete and continuous) in the context of VAEs (Variational Autoencoders).  By increasing the vocabulary size of the discrete tokenizer, the results demonstrate that a discrete tokenizer can surpass the performance of a continuous VAE.  The table showcases how scaling the vocabulary of the discrete tokenizer leads to improved results on ImageNet-rFID, suggesting the benefits of high-capacity discrete representations for image reconstruction.", "section": "4 Experiment"}, {"content": "| Classifier | # Params | vRAM | Recons. Loss \u2193 | FID \u2193 | ImageReward \u2191 | HPSv2.1 \u2191 |\n|---|---|---|---|---|---|---|\n| Convention | 124M | 2GB | 0.184 | 4.49 | 0.79 | 31.95 |\n| IVC | 0.65M | 10MB | 0.180 | 3.83 | 0.91 | 32.31 |", "caption": "Table 5:  IVC saves 99.95% params and gets better performance to conventional classifier (Vd=216)V_{d}=2^{16})italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT )", "description": "Table 5 presents a comparison of the Infinite-Vocabulary Classifier (IVC) against a conventional classifier.  Both classifiers are tasked with predicting visual tokens, but IVC uses a significantly more efficient architecture. The table shows that IVC reduces the number of parameters by 99.95% while achieving lower reconstruction loss and FID (Fr\u00e9chet Inception Distance), a metric assessing the quality of generated images, and higher ImageReward and HPSv2.1 scores (measures of human preference).  This demonstrates IVC's superior performance in terms of both efficiency and image generation quality, particularly when considering a large vocabulary size (2^16).", "section": "3.3 Infinite-Vocabulary Classifier"}, {"content": "| # Params | GFLOPs | Hidden Dimension | Heads | Layers |\n|---|---|---|---|---|\n| 125M | 30 | 768 | 8 | 12 |\n| 361M | 440 | 1152 | 12 | 16 |\n| 940M | 780 | 1536 | 16 | 24 |\n| 2.2B | 1500 | 2080 | 20 | 32 |\n| 4.7B | 2600 | 2688 | 24 | 40 |", "caption": "Table 6:  Model architectures for scaling visual autoregressive modeling. Note that GFLOPs are rough values since they are affected by the length of the text prompt.", "description": "This table presents the model architectures used in the scaling experiments for visual autoregressive modeling.  It shows how the number of parameters, GFLOPs (floating-point operations), hidden dimension, number of attention heads, and number of transformer layers were varied to explore the effects of model scaling on performance.  Note that the GFLOPs are approximate because the computational cost is influenced by the length of the text prompt used in each image generation task.", "section": "3 Infinity Architecture"}, {"content": "| Method | FID \u2193 | ImageReward \u2191 | HPSv2.1 \u2191 |\n|---|---|---|---|\n| Baseline | 9.76 | 0.52 | 29.53 |\n| Baseline + Random Flip | 9.69 | 0.52 | 29.20 |\n| Baseline + Bitwise Self-Correction | 3.48 | 0.76 | 30.71 |", "caption": "Table 7:  Bitwise Self-Correction makes significant improvements. Experiment with 5M high-quality data and 512\u00d7512512512512\\times 512512 \u00d7 512 resolution. FID is measured on the validation set with 40K images. Decoding with \u03c4=1\ud835\udf0f1\\tau=1italic_\u03c4 = 1 and c\u2062f\u2062g=3\ud835\udc50\ud835\udc53\ud835\udc543cfg=3italic_c italic_f italic_g = 3.", "description": "This table presents the results of an ablation study evaluating the impact of Bitwise Self-Correction (BSC) on the performance of the Infinity model.  The experiment used 5 million high-quality images with a resolution of 512x512 pixels. The FID (Fr\u00e9chet Inception Distance) metric was calculated on a validation set of 40,000 images.  The decoding parameters used were \u03c4 (tau) = 1 and cfg (classifier-free guidance) = 3.  The table compares the performance of the baseline model (without BSC), a model with random bit flips (simulating errors), and the model with BSC. The metrics reported are FID, ImageReward, and HPSv2.1, indicating the impact of BSC on image quality, adherence to prompts, and overall aesthetic appeal.", "section": "3.4 Bitwise Self-Correction"}, {"content": "| Method | FID\u2193 | ImageReward\u2191 | HPSv2.1\u2191 |\n|---|---|---|---|\n| w/o Bitwise Self-Correction | 9.76 | 0.515 | 29.53 |\n| Bitwise Self-Correction (p=10%) | 3.45 | 0.751 | 30.47 |\n| Bitwise Self-Correction (p=20%) | 3.48 | 0.763 | 30.71 |\n| Bitwise Self-Correction (p=30%) | **3.33** | **0.775** | **31.05** |", "caption": "Table 8:  Comparison between different strengths of Bitwise Self-Correction. Experiment with 5M high-quality data and 512\u00d7512512512512\\times 512512 \u00d7 512 resolution. Decoding with \u03c4=1\ud835\udf0f1\\tau=1italic_\u03c4 = 1 and c\u2062f\u2062g=3\ud835\udc50\ud835\udc53\ud835\udc543cfg=3italic_c italic_f italic_g = 3.", "description": "This table presents the ablation study results on the impact of different Bitwise Self-Correction (BSC) strengths on the model's performance.  The experiment used 5 million high-quality images with a resolution of 512x512 pixels. The decoding parameters were set to \u03c4 (tau) = 1 and cfg = 3. The table shows how varying the probability of randomly flipping bits during BSC (10%, 20%, and 30%) affects the FID score, ImageReward, and HPSv2.1 scores, which are metrics evaluating image quality and human preference.", "section": "4.7 Ablation Studies"}, {"content": "| Method | Param | FID \u2193 | ImageReward \u2191 | HPSv2.1 \u2191 |\n|---|---|---|---|---|\n| Greedy Sampling | \u03c4=0.01,cfg=1 | 9.97 | 0.397 | 30.98 |\n| Normal Sampling | \u03c4=1.00,cfg=1 | 4.84 | 0.706 | 31.59 |\n| Pyramid CFG | \u03c4=1.00,cfg=1\u21923 | 3.48 | 0.872 | **32.48** |\n| Pyramid CFG | \u03c4=1.00,cfg=1\u21925 | 2.98 | 0.929 | 32.32 |\n| CFG on features | \u03c4=1.00,cfg=3 | 3.00 | 0.953 | 32.13 |\n| CFG on logits | \u03c4=1.00,cfg=3 | 2.91 | 0.952 | 32.31 |\n| CFG on logits (Ours) | \u03c4=1.00,cfg=4 | **2.82** | **0.962** | 32.25 |", "caption": "Table 9:  Comparison between different decoding methods.", "description": "This table compares different decoding methods used in the Infinity model for text-to-image generation, showing their performance across various metrics such as FID (Fr\u00e9chet Inception Distance), ImageReward, and HPSv2.1.  The methods compared include Greedy Sampling, Normal Sampling, different configurations of Classifier-Free Guidance (CFG), and the authors' proposed method.  The results demonstrate the impact of different decoding strategies on the quality and fidelity of the generated images.", "section": "4.7 Ablation Studies"}, {"content": "| Aspect Ratio | Resolution | Scale Schedule |\n|---|---|---|\n| 1.000 (1:1) | 1024 \u00d7 1024 | (1,1), (2,2), (4,4), (6,6), (8,8), (12,12), (16,16), (20,20), (24,24), (32,32), (40,40), (48,48), (64,64) |\n| 0.800 (4:5) | 896 \u00d7 1120 | (1,1), (2,2), (3,3), (4,5), (8,10), (12,15), (16,20), (20,25), (24,30), (28,35), (36,45), (44,55), (56,70) |\n| 1.250 (5:4) | 1120 \u00d7 896 | (1,1), (2,2), (3,3), (5,4), (10,8), (15,12), (20,16), (25,20), (30,24), (35,28), (45,36), (55,44), (70,56) |\n| 0.750 (3:4) | 864 \u00d7 1152 | (1,1), (2,2), (3,4), (6,8), (9,12), (12,16), (15,20), (18,24), (21,28), (27,36), (36,48), (45,60), (54,72) |\n| 1.333 (4:3) | 1152 \u00d7 864 | (1,1), (2,2), (4,3), (8,6), (12,9), (16,12), (20,15), (24,18), (28,21), (36,27), (48,36), (60,45), (72,54) |\n| 0.666 (2:3) | 832 \u00d7 1248 | (1,1), (2,2), (2,3), (4,6), (6,9), (10,15), (14,21), (18,27), (22,33), (26,39), (32,48), (42,63), (52,78) |\n| 1.500 (3:2) | 1248 \u00d7 832 | (1,1), (2,2), (3,2), (6,4), (9,6), (15,10), (21,14), (27,18), (33,22), (39,26), (48,32), (63,42), (78,52) |\n| 0.571 (4:7) | 768 \u00d7 1344 | (1,1), (2,2), (3,3), (4,7), (6,11), (8,14), (12,21), (16,28), (20,35), (24,42), (32,56), (40,70), (48,84) |\n| 1.750 (7:4) | 1344 \u00d7 768 | (1,1), (2,2), (3,3), (7,4), (11,6), (14,8), (21,12), (28,16), (35,20), (42,24), (56,32), (70,40), (84,48) |\n| 0.500 (1:2) | 720 \u00d7 1440 | (1,1), (2,2), (2,4), (3,6), (5,10), (8,16), (11,22), (15,30), (19,38), (23,46), (30,60), (37,74), (45,90) |\n| 2.000 (2:1) | 1440 \u00d7 720 | (1,1), (2,2), (4,2), (6,3), (10,5), (16,8), (22,11), (30,15), (38,19), (46,23), (60,30), (74,37), (90,45) |\n| 0.400 (2:5) | 640 \u00d7 1600 | (1,1), (2,2), (2,5), (4,10), (6,15), (8,20), (10,25), (12,30), (16,40), (20,50), (26,65), (32,80), (40,100) |\n| 2.500 (5:2) | 1600 \u00d7 640 | (1,1), (2,2), (5,2), (10,4), (15,6), (20,8), (25,10), (30,12), (40,16), (50,20), (65,26), (80,32), (100,40) |\n| 0.333 (1:3) | 592 \u00d7 1776 | (1,1), (2,2), (2,6), (3,9), (5,15), (7,21), (9,27), (12,36), (15,45), (18,54), (24,72), (30,90), (37,111) |\n| 3.000 (3:1) | 1776 \u00d7 592 | (1,1), (2,2), (6,2), (9,3), (15,5), (21,7), (27,9), (36,12), (45,15), (54,18), (72,24), (90,30), (111,37) |", "caption": "Table 10: Predefined scale schedules {(h1r,w1r),\u2026,(hKr,wKr)}subscriptsuperscript\u210e\ud835\udc5f1subscriptsuperscript\ud835\udc64\ud835\udc5f1\u2026subscriptsuperscript\u210e\ud835\udc5f\ud835\udc3esubscriptsuperscript\ud835\udc64\ud835\udc5f\ud835\udc3e\\{(h^{r}_{1},w^{r}_{1}),...,(h^{r}_{K},w^{r}_{K})\\}{ ( italic_h start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_w start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , \u2026 , ( italic_h start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT , italic_w start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ) } for different aspect ratios. Following the text guided next-scale prediction scheme, Infinity takes K\ud835\udc3eKitalic_K=13 scales to generate a 1024\u00d71024102410241024\\times 10241024 \u00d7 1024 (or other aspect ratio) image.", "description": "This table shows predefined scale schedules used by the Infinity model for generating images with various aspect ratios.  Each row represents a different aspect ratio (e.g., 1:1, 4:3, 16:9), and the columns list the height and width resolutions (h, w) at each of the 13 scales (K=13) used in the model's next-scale prediction process.  The model starts with small resolution images at the first scale and gradually upsamples them to 1024x1024 (or other specified resolutions) over 13 prediction steps. This pre-defined schedule ensures consistent training and efficient generation across different aspect ratios.", "section": "3.5 Dynamic Aspect Ratios and Position Encoding"}]