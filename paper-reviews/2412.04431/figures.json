[{"figure_path": "https://arxiv.org/html/2412.04431/x1.png", "caption": "Figure 1: High-resolution image synthesis results from Infinity, showcasing its capabilities in precise prompt following, spatial reasoning, text rendering, and aesthetics across different styles and aspect ratios.", "description": "Figure 1 presents several high-resolution images generated by the Infinity model, demonstrating its ability to accurately interpret and generate images based on diverse textual prompts.  The images showcase various capabilities of the model: precise adherence to the instructions within prompts (prompt following), creation of images with realistic spatial relationships between objects (spatial reasoning), accurate rendering of text with different styles and fonts (text rendering), and high aesthetic quality across different artistic styles and aspect ratios. This visual demonstration highlights Infinity's superior performance in advanced image synthesis.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2412.04431/x2.png", "caption": "Figure 2: Visual tokenizer quantizes continuous features and then gets index labels. Conventional classifier (left) predicts 2dsuperscript2\ud835\udc512^{d}2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT indices. Infinite-Vocabulary Classifier (right) predicts d\ud835\udc51ditalic_d bits instead. Slight perturbations to near-zero values in continuous features cause a complete change of index labels. Bit labels (i.e. quantized features) change subtly and still provide steady supervision. Besides, parameters of conventional classifiers grow exponentially as d\ud835\udc51ditalic_d increases, while IVC grows linearly. If d=32\ud835\udc5132d=32italic_d = 32 and h=2048\u210e2048h=2048italic_h = 2048, the conventional classifier requires 8.8 trillion parameters, exceeding current compute limits. By contrast, IVC only requires 0.13M parameters.", "description": "Figure 2 illustrates the core concept of the visual tokenizer used in Infinity.  The left side shows a conventional classifier that predicts indices from continuous features.  A small change in the continuous features can result in a large change in the predicted index, leading to instability and requiring an exponentially growing number of parameters as the number of bits increases (d). The right side shows Infinity's Infinite-Vocabulary Classifier (IVC) which predicts individual bits instead of indices.  This approach makes the classifier significantly more robust to small perturbations of continuous values, with only a small change in predicted bit values resulting from small changes in input values.  Crucially, the number of parameters required by the IVC increases only linearly with the number of bits, making it highly scalable even with a massive vocabulary, in contrast to the exponentially growing needs of the conventional classifier. The example of d=32 and h=2048 illustrates the huge difference in parameter counts: 8.8 trillion for the conventional classifier versus 0.13 million for the IVC.", "section": "3 Infinity Architecture"}, {"figure_path": "https://arxiv.org/html/2412.04431/x3.png", "caption": "Figure 3: Framework of Infinity. Infinity introduces bitwise modeling, which incorporates a bitwise multi-scale visual tokenizer, Infinite-Vocabulary Classifier (IVC), and Bitwise Self-Correction. When predicting \ud835\udc79ksubscript\ud835\udc79\ud835\udc58\\bm{R}_{k}bold_italic_R start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, the sequence (\ud835\udc791,\ud835\udc792,\u2026,\ud835\udc79k\u22121)subscript\ud835\udc791subscript\ud835\udc792\u2026subscript\ud835\udc79\ud835\udc581(\\bm{R}_{1},{\\bm{R}}_{2},...,\\bm{R}_{k-1})( bold_italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_R start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , bold_italic_R start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT ) serves as the prefixed context and the text condition guides the prediction through a cross attention mechanism. Different from VAR, Infinity performs next-scale prediction with bit labels.", "description": "Figure 3 illustrates the architecture of the Infinity model, which uses bitwise modeling.  The core components are:\n\n1. **Bitwise Multi-Scale Visual Tokenizer:** This tokenizer encodes the input image into a sequence of residual feature maps (R1, R2, ..., Rk) at different resolutions (scales).  Each Rk represents the residual information at a particular scale. This is a departure from traditional tokenization methods that use a single, fixed vocabulary.\n\n2. **Infinite-Vocabulary Classifier (IVC):**  Instead of predicting indices from a fixed-size vocabulary (like in conventional models), the IVC predicts the bits that compose each token directly.  This allows for a theoretically infinite vocabulary size, significantly increasing model capacity and enabling finer-grained control over image details.\n\n3. **Bitwise Self-Correction:** This mechanism introduces noise or errors into the predictions of earlier residual maps (R1, R2, ..., Rk-1).  The model then attempts to correct these errors, leading to more robust and accurate image generation.  This technique helps to mitigate cumulative errors from the autoregressive nature of the model.\n\nThe process is autoregressive; each residual map Rk is predicted based on the preceding maps (R1, R2, ..., Rk-1) and the text prompt, using a cross-attention mechanism.  Importantly, the prediction is performed at the bit level instead of the index level, leading to a more efficient and powerful model.", "section": "3 Infinity Architecture"}, {"figure_path": "https://arxiv.org/html/2412.04431/x4.png", "caption": "Figure 4: Qualitative results from Infinity.", "description": "This figure showcases examples of high-resolution images generated by the Infinity model.  Each image is accompanied by a short description of the prompt used to generate it. The images demonstrate Infinity's capabilities in various aspects of image synthesis including precise prompt following, accurate text rendering, diverse artistic styles and accurate depiction of spatial relationships. The variety of scenes and styles exemplifies the model's versatility and ability to generate photorealistic results.", "section": "4.3 Text-to-Image Generation"}, {"figure_path": "https://arxiv.org/html/2412.04431/extracted/6046736/images/human_preference.png", "caption": "Figure 5: Human Preference Evaluation. We ask users to select the better one in a side-by-side comparison in terms of Overall Quality, Prompt Following, and Visual Aesthetics. Infinity is more preferred by humans compared to other open-source models.", "description": "This figure displays the results of a human preference evaluation comparing the image generation quality of the Infinity model to several other open-source models. Participants were shown pairs of images, one generated by Infinity and one by a competing model, and asked to choose the image they preferred based on three criteria: overall quality, prompt following, and visual aesthetics.  The bar chart visualizes the win rate (percentage of times each model was preferred) for each model across these three criteria, showing that Infinity was significantly preferred by the participants compared to the other models.", "section": "4.3 Text-to-Image Generation"}, {"figure_path": "https://arxiv.org/html/2412.04431/x5.png", "caption": "Figure 6: Prompt-following qualitative comparison. We highlight text in red that Infinity-2B consistently adheres to while the other four models fail to follow. Zoom in for better comparison.", "description": "This figure presents a qualitative comparison of prompt-following capabilities across six different text-to-image models, including Infinity-2B and four others.  Each row shows the same prompt given to each model, and the generated images are displayed side-by-side. Key phrases from the prompt are highlighted in red to emphasize instances where Infinity-2B faithfully renders the text while other models fail to do so. This demonstrates Infinity-2B's superior ability to accurately follow detailed instructions provided in prompts.", "section": "4.3 Text-to-Image Generation"}, {"figure_path": "https://arxiv.org/html/2412.04431/x6.png", "caption": "Figure 7: Text rendering results from our Infinity-2B model. Infinity-2B could generate text-consistent images following user prompts across diverse categories.", "description": "Figure 7 showcases the text rendering capabilities of the Infinity-2B model.  It presents several examples of images generated from diverse prompts, highlighting the model's ability to accurately render text in various styles, fonts, and contexts.  The prompts range from simple text phrases to more complex descriptions requiring specific styles, settings, and subject matter. The figure demonstrates the model's strong performance in generating text-consistent images regardless of the prompt's complexity or creative demands.", "section": "4.3 Text-to-Image Generation"}, {"figure_path": "https://arxiv.org/html/2412.04431/x7.png", "caption": "Figure 8: Impact of Infinite-Vocabulary Classifier. Predicting bitwise labels with the Infinite-Vocabulary Classifier (Right) generates images with richer details compared to predicting index-wise labels using a conventional classifier (Left).", "description": "Figure 8 demonstrates the impact of using an Infinite-Vocabulary Classifier (IVC) for image generation. The left side shows results from a conventional classifier predicting index-wise labels, while the right side displays results from the IVC predicting bitwise labels. The comparison highlights that the IVC produces images with significantly richer details and finer visual qualities. This improvement is attributed to the ability of the IVC to handle a much larger vocabulary space effectively, leading to more precise and nuanced representation of image features during the generation process.", "section": "3.3 Infinite-Vocabulary Classifier"}, {"figure_path": "https://arxiv.org/html/2412.04431/extracted/6046736/images/scaling_vae_bits_three_column.jpg", "caption": "Figure 9: Effects of Scaling Up the Vocabulary. We analyze the impact of scaling the vocabulary size under consistent training hyperparameters throughout. Vocabulary size Vd=216subscript\ud835\udc49\ud835\udc51superscript216V_{d}=2^{16}italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT converges faster and achieves better results for small models (125M and 361M parameters). As we scale up the model size to 2.2B, Infinity with a vocabulary size Vd=232subscript\ud835\udc49\ud835\udc51superscript232V_{d}=2^{32}italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 32 end_POSTSUPERSCRIPT beats that one with Vd=216subscript\ud835\udc49\ud835\udc51superscript216V_{d}=2^{16}italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT. Experiment with 5M high-quality image-text pair data under 256\u00d7256256256256\\times 256256 \u00d7 256 resolution.", "description": "Figure 9 illustrates the impact of increasing the vocabulary size on model performance in text-to-image generation.  The experiment uses consistent training hyperparameters across different model sizes, focusing on the effect of changing the vocabulary size, represented as Vd (where d is the exponent of 2, indicating the number of bits used in the tokenizer).  Smaller models (125M and 361M parameters) converge faster and achieve better results when the vocabulary size is Vd=2^16. However, when using a larger model (2.2B parameters), using Vd=2^32 outperforms Vd=2^16, demonstrating that the optimal vocabulary size is dependent on the model's complexity. The data used for this experiment comprises 5 million high-quality image-text pairs, and all images were 256x256 pixels.", "section": "4.4 Scaling Visual Tokenizer's Vocabulary"}, {"figure_path": "https://arxiv.org/html/2412.04431/extracted/6046736/images/scaling_models.jpg", "caption": "Figure 10: Effects of Scaling Visual AutoRegressive Modeling. We analyze the impact of scaling model size under consistent training hyperparameters throughout (Experiment with 10M pre-training data and 256\u00d7256256256256\\times 256256 \u00d7 256 resolution). Validation loss smoothly decreases as a function of the model size and training iterations. Besides, Validation loss is a strong predictor of overall model performance. There is a strong correlation between validation loss and holistic image evaluation metrics.", "description": "This figure shows the impact of scaling the model size on the performance of visual autoregressive modeling.  The experiment involved training models with 10 million pre-training data points and images at a 256x256 resolution.  The key finding is that as the model size increases, the validation loss consistently decreases, indicating improved model performance.  Furthermore, the validation loss serves as a strong indicator of overall model performance, exhibiting a high correlation with holistic image evaluation metrics.  This supports the idea that larger models trained for longer durations lead to better results.", "section": "3 Infinity Architecture"}, {"figure_path": "https://arxiv.org/html/2412.04431/x8.png", "caption": "Figure 11: Semantics and visual quality improve consistently with scaling up model size and training compute. Zoom in for better comparison.", "description": "Figure 11 visually demonstrates the positive impact of increased model size and training computational resources on the quality of image generation.  As model size and training compute scale up, both the semantic accuracy (how well the generated image reflects the intended meaning) and the visual fidelity (sharpness, detail, and overall realism) show consistent and significant improvement. The figure showcases a series of images generated under various scaling conditions, enabling a clear visual comparison of the quality enhancements.", "section": "Scaling Up Visual Autoregressive Modeling"}, {"figure_path": "https://arxiv.org/html/2412.04431/x9.png", "caption": "Figure 12: Impact of Self-Correction. Teacher-forcing training introduces great train-test discrepancy which degrades performance during inference (left). Bitwise Self-Correction auto-corrects mistakes and thus generates better results (right). Decoding with \u03c4=1\ud835\udf0f1\\tau=1italic_\u03c4 = 1 and c\u2062f\u2062g=3\ud835\udc50\ud835\udc53\ud835\udc543cfg=3italic_c italic_f italic_g = 3.", "description": "Figure 12 demonstrates the effectiveness of Bitwise Self-Correction in mitigating the train-test discrepancy inherent in teacher-forcing training. The left side shows the results of using teacher-forcing training, where the generated images suffer from significant discrepancies between training and inference. This leads to degraded performance during actual inference. In contrast, the right side illustrates how Bitwise Self-Correction effectively addresses this issue by automatically correcting mistakes during the generation process, thus resulting in significantly improved image quality. The generation parameters used here are \u03c4=1 and cfg=3.", "section": "3.4 Bitwise Self-Correction"}, {"figure_path": "https://arxiv.org/html/2412.04431/extracted/6046736/images/pe_ablation.jpg", "caption": "Figure 13: Comparison between learnable APE and our positional embeddings. Our method, i.e., applying RoPE2d along with learnable scale embeddings on features of each scale, converges faster and reaches higher training accuracy.", "description": "Figure 13 presents a comparison of training performance between two approaches to positional encoding in visual autoregressive models: learnable Absolute Position Embeddings (APE) as used in previous work, and the method proposed in the paper, which combines Rotary Position Embedding (RoPE2d) with learnable scale embeddings. The results show that the proposed method, applied to features at each scale, leads to faster convergence and higher training accuracy.  This is likely because RoPE2d preserves the intrinsic 2D structure of images, while learnable scale embeddings efficiently adapt to varying sequence lengths caused by differing aspect ratios at various scales. The learnable APE approach, in contrast, can be less effective due to a much larger number of parameters that need to be optimized and the inherent difficulty of distinguishing features across different resolutions.", "section": "4.7 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.04431/x10.png", "caption": "Figure 14: Comparison of different sampling methods. In contrast to Greedy Sample, Normal Sample and Pyramid Sample, our method could generate images with richer details and higher text-image alignments.", "description": "Figure 14 presents a comparison of image generation results using different sampling methods: Greedy Sampling, Normal Sampling, Pyramid CFG, and the authors' proposed method.  The figure visually demonstrates that the authors' method produces images with significantly richer details and a stronger alignment between the generated image content and the input text prompt compared to the other three sampling techniques.  The improved detail and alignment suggest that the authors' sampling strategy is more effective at capturing the nuances of the text prompt and translating them into high-quality, faithful image generation.", "section": "4.7 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.04431/extracted/6046736/images/Categories.png", "caption": "Figure 15: Distribution of Prompt Categories", "description": "This figure shows a pie chart visualizing the distribution of prompt categories used in the paper's experiments.  The categories represent different subject matters or themes of the image generation prompts, such as human, animal, foods, plants, architecture, products/artifacts, text rendering, landscape, and interior scenes. Each slice of the pie chart is proportionally sized to represent the percentage of prompts belonging to each category. This gives a reader a clear overview of the types of images the model was trained and evaluated on, highlighting the variety and balance in the dataset.", "section": "4 Experiment"}, {"figure_path": "https://arxiv.org/html/2412.04431/extracted/6046736/images/Prompts_Challenges.png", "caption": "Figure 16: Distribution of Prompts Challenges", "description": "This figure shows the distribution of challenges present in the prompts used to evaluate the image generation models.  The pie chart breaks down the percentage of prompts that fall into various categories of difficulty, such as simple prompts, complex prompts, those requiring nuanced understanding of semantics, specific color requests, precise positioning or perspective, particular artistic styles, detailed descriptions, or creative imagination. The analysis provides insight into the range and complexity of the prompts used for model evaluation, ensuring a comprehensive assessment of the models' capabilities.", "section": "4.3 Text-to-Image Generation"}, {"figure_path": "https://arxiv.org/html/2412.04431/x11.png", "caption": "Figure 17: T2I qualitative comparison among our Infinity-2B model and the other four open-source models. Here we select three diffusion models (Flux Schnell, SD3-Medium and PixArt Sigma), one AR model (HART) for comparison. Zoom in for better comparsion.", "description": "Figure 17 presents a qualitative comparison of text-to-image (T2I) generation results. It compares the Infinity-2B model against four other open-source models: three diffusion models (Flux Schnell, SD3-Medium, and PixArt Sigma) and one autoregressive model (HART).  The figure showcases several example prompts and their corresponding generated images from each model, highlighting the differences in image quality, detail, adherence to the prompt, and overall visual aesthetics.  By visually examining the generated images for each prompt, one can assess the relative strengths and weaknesses of the different models in terms of their image generation capabilities.  The differences in style, detail, and faithfulness to the prompt provide insights into the various techniques employed in each model. Zooming in on the images enhances the ability to compare fine details and subtle differences between the outputs.", "section": "4.3 Text-to-Image Generation"}]