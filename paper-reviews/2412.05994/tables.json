[{"content": "| Methods | Allen-Cahn | Helmholtz | Nonlinear Diffusion | Flow Mixing | Klein Gordon |\n|---|---|---|---|---|---| \n| PINN | - | 4.02e-1 | 9.50e-3 | - | 3.43e-2 |\n| LRA | - | 3.69e-3 | - | - | - |\n| PIXEL | 8.86e-3 | 8.63e-4 | - | - | - |\n| SPINN | - | - | 4.47e-2 | 2.90e-3 | 1.93e-2 |\n| JAX-PI | 5.37e-5 | - | - | - | - |\n| PirateNet | 2.24e-5 | - | - | - | - |\n| PIG (Ours) | 1.04e-4 | 4.13e-5 | 2.69e-3 | 4.51e-4 | 2.76e-3 |\n| \u00b1 1std | \u00b1 4.12e-5 | \u00b1 2.59e-05 | \u00b1 6.55e-4 | \u00b1 1.74e-4 | \u00b1 4.27e-4 |\n| best | 5.93e-5 | 2.22e-5 | 1.44e-3 | 2.67e-4 | 2.36e-3 |", "caption": "Table 1: Comparison of relative L2superscript\ud835\udc3f2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT errors across different methods. Three experiments were conducted using seeds 100, 200, and 300, with the mean and standard deviation presented in the table. The methods compared include PINN\u00a0(Raissi et\u00a0al., 2019), Learning Rate Annealing (LRA)\u00a0(Wang et\u00a0al., 2021), PIXEL\u00a0(Kang et\u00a0al., 2023), SPINN\u00a0(Cho et\u00a0al., 2024), JAX-PI\u00a0(Wang et\u00a0al., 2023), and Pirate-Net\u00a0(Wang et\u00a0al., 2024b). For fair comparisons, we included the reported values from the respective references and omitted results that were not provided in the original papers.", "description": "This table compares the performance of various methods for approximating solutions to partial differential equations (PDEs) by showing their relative L2 errors.  The comparison includes several state-of-the-art methods like PINN, Learning Rate Annealing (LRA), PIXEL, SPINN, JAX-PI, and Pirate-Net. The results are averaged over three separate experiments with different random seeds (100, 200, 300) to assess the robustness of the methods.  Mean and standard deviation of L2 errors are presented for a comprehensive evaluation.  Note that for fair comparisons, only results reported in the original papers for each method are included.", "section": "4 Experiments"}, {"content": "| # Gaussians | Flow-Mixing | Nonliner-Diffusion | Allen-cahn |\n|---|---|---|---|\n| 200 | 6.07e-03 | 2.33e-03 | 1.83e-02 |\n| 400 | 3.13e-03 | 2.22e-03 | 2.93e-03 |\n| 600 | 1.50e-03 | 2.23e-03 | 2.75e-03 |\n| 800 | 1.44e-03 | 1.95e-03 | 1.22e-03 |\n| 1000 | 1.31e-03 | 7.33e-03 | 4.81e-04 |\n| 1200 | 1.03e-03 | 3.96e-03 | 3.98e-04 |", "caption": "Table 2: The number of Gaussians and approximation accuracy (Flow-Mixing, Nonlinear Diffusion, and Allen-Cahn). The results indicate that increasing the number of Gaussians typically leads to a decrease in relative L2superscript\ud835\udc3f2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT error.", "description": "This table presents the impact of varying the number of Gaussian functions on the accuracy of the Physics-Informed Gaussian (PIG) model in approximating solutions for three different partial differential equations (PDEs): Flow-Mixing, Nonlinear Diffusion, and Allen-Cahn.  The relative L2 error, a common metric for assessing the accuracy of PDE solution approximations, is reported for each PDE and for different counts of Gaussian functions used in the PIG model.  The results illustrate the general trend that increasing the number of Gaussian functions generally leads to a reduction in the relative L2 error, suggesting improved approximation accuracy with more complex models.  This demonstrates the model's ability to converge to the true solution with improved resolution.", "section": "4.3 Hyperparameter Analysis and Ablation Study"}, {"content": "| (MLP, <math alttext=\"\\mu\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.1.1.1.m1.1\"><semantics id=\"S4.T3.1.1.1.m1.1a\"><mi id=\"S4.T3.1.1.1.m1.1.1\" xref=\"S4.T3.1.1.1.m1.1.1.cmml\">\\mu</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.1.1.1.m1.1b\"><ci id=\"S4.T3.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.1.1.1.m1.1.1\">\ud835\udf07</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.1.1.1.m1.1c\">\\mu</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T3.1.1.1.m1.1d\">italic_\u03bc</annotation></semantics></math>) | Allen-Cahn | Helmholtz | Nonlinear Diffusion | Flow-Mixing | Klein-Gordon |\n|---|---|---|---|---|---| \n| (X, Fixed) | 4.72e-03 | 3.97e-04 | 6.32e-03 | 4.33e-03 | 6.44e-02 |\n| (O, Fixed) | 1.82e-03 | 2.12e-04 | 2.10e-03 | 1.09e-03 | 2.69e-02 |\n| (X, Learn) | 7.29e-05 | 1.86e-04 | 5.26e-03 | 7.93e-04 | 8.51e-03 |\n| (O, Learn) | 7.27e-05 | 2.22e-05 | 1.44e-03 | 4.51e-04 | 2.76e-03 |", "caption": "Table 3: Ablation study results on MLP and \u03bc\ud835\udf07\\muitalic_\u03bc across various equations.", "description": "This table presents the results of an ablation study conducted to analyze the impact of two key components of the Physics-Informed Gaussian (PIG) model: the Multilayer Perceptron (MLP) and the Gaussian mean (\u03bc).  The study was performed across various Partial Differential Equations (PDEs).  It shows the effect of using either a fixed or learnable MLP, and whether the Gaussian means are fixed or dynamically adjusted during training.  The results are presented as relative L2 errors, showcasing how these design choices influence the model's overall performance in solving different PDEs.", "section": "3.2 Physics-Informed Gaussians"}, {"content": "| # Hidden units | MLP input dim (=k) |  |  |  | \n|---|---|---|---|---|\n|  | 1 | 2 | 3 | 4 |\n| 4 | 7.77e-03 | 9.60e-03 | 7.68e-03 | 9.60e-03 |\n| 8 | 8.55e-03 | 6.44e-03 | 1.06e-02 | 8.54e-03 |\n| 16 | 8.24e-03 | 1.06e-02 | 1.21e-02 | 6.90e-03 |\n| 32 | 7.14e-03 | 8.06e-03 | 1.22e-02 | 6.87e-03 |\n| 64 | 6.33e-03 | 7.50e-03 | 1.09e-02 | 9.48e-03 |\n| 128 | 6.38e-03 | 6.88e-03 | 8.48e-03 | 7.47e-03 |\n| 256 | 5.21e-03 | 6.60e-03 | 5.22e-03 | 5.40e-03 |", "caption": "Table 4: The performance of different MLP configurations for the Helmholtz equation, displaying L2superscript\ud835\udc3f2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT relative errors at iteration 1,000 across various configurations of hidden units and MLP input dimensions. Overall, the results highlight the robustness to the size of MLP, showing minimal variation in errors across different settings.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different Multi-Layer Perceptron (MLP) configurations on the accuracy of solving the Helmholtz equation using Physics-Informed Gaussians (PIGs). The study varied two key aspects of the MLP: the number of hidden units (8, 16, 32, 64, 128, 256) and the input dimension (1, 2, 3, 4). The table shows the relative L2 error achieved at iteration 1000 for each configuration. The results demonstrate that the PIG model is robust to changes in the MLP architecture, showing minimal variation in accuracy across different numbers of hidden units and input dimensions.", "section": "4.3.2 MLP impact and Adaptive Gaussian Positions"}, {"content": "|       | Helmholtz | Klein-Gordon | Flow-Mixing | Nonlinear Diffusion |\n| :---- | :------: | :----------: | :----------: | :----------------: |\n| Dense | 5.17e-05  | 1.81e-03     | 3.48e-04     | 3.86e-03           |\n| Diagonal | 2.22e-05  | 2.76e-03     | 4.51e-04     | 1.44e-03           |", "caption": "Table 5: Comparison of error levels between dense and diagonal covariance matrices in PIGs. For dense covariance matrix experiments, we first trained PIG using a diagonal covariance matrix and then fine-tuned full covariance matrix parameters initialized from the trained diagonal elements.", "description": "This table compares the performance of Physics-Informed Gaussians (PIGs) using two different types of covariance matrices: diagonal and dense.  For the dense matrix experiments, the PIG model was initially trained with a diagonal covariance matrix.  The weights obtained from this initial training were then used to initialize the parameters for a dense covariance matrix, which was then further fine-tuned. The results show the relative L2 errors achieved by both approaches on various partial differential equations.", "section": "4.3.3 COVARIANCE MATRICES"}, {"content": "|       | Equation 23 | Equation 24 |\n|---|---|---|\n| PIRBNs | 6.87e-03 \u00b1 3.70e-04 | 1.47e-02 \u00b1 9.16e-03 |\n| PIGs | 1.79e-05 \u00b1 3.80e-06 | 1.14e-04 \u00b1 1.19e-05 |", "caption": "Table 6: Results of the comparison study between PIGs and PIRBNs for Equations 23 and 24. PIGs achieve lower errors than PIRBNs, highlighting their superior performance in both equations.", "description": "This table presents a comparison of the performance of Physics-Informed Gaussians (PIGs) and Physics-Informed Radial Basis Networks (PIRBNs) on two different equations (Equations 23 and 24, as described in the paper's Appendix A.3). The results demonstrate that PIGs achieve significantly lower errors compared to PIRBNs, showcasing the superior performance of PIGs in approximating solutions for both equations.", "section": "4.3 Hyperparameter analysis and ablation study"}, {"content": "| FE\n+NN\n\u03b8 | Helmholtz | Flow-Mixing | Klein-Gordon |\n|---|---|---|---|\n| SIREN + Id | 1.68e-03 \u00b1 2.02e-03 | 1.22e-02 \u00b1 4.17e-03 | 1.18e-01 \u00b1 4.88e-02 |\n| SIREN + tanh | 1.31e-03 \u00b1 8.26e-04 | 2.80e-02 \u00b1 2.50e-02 | 1.04e-01 \u00b1 8.61e-02 |\n| PIG + SIREN | **1.37e-05** \u00b1 1.64e-06 | 1.28e-03 \u00b1 1.09e-04 | 2.37e-02 \u00b1 4.62e-03 |\n| PIG + tanh | 4.13e-05 \u00b1 2.59e-05 | **4.51e-04** \u00b1 1.74e-04 | **2.76e-03** \u00b1 4.27e-04 |", "caption": "Table 7: Comparison of PIG and SIREN performance. For all cases except the Helmholtz equation, the original PIG + tanh\\tanhroman_tanh formulation outperformed other methods. The improved performance of PIG + SIREN on the Helmholtz equation may be attributed to the functional form of its exact solution.", "description": "This table compares the performance of PIG (Physics-Informed Gaussians), using different neural network architectures, against SIREN (Implicit Neural Representations with Periodic Activation Functions).  The comparison is made across multiple partial differential equations (PDEs): Allen-Cahn, Helmholtz, Flow Mixing, and Klein-Gordon.  Results show that PIG with a tanh activation function generally outperforms SIREN across these PDEs, except for the Helmholtz equation. The better performance of PIG + SIREN on the Helmholtz equation is hypothesized to be due to the specific form of the Helmholtz equation's exact solution being better suited to SIREN's architecture.", "section": "4.3 MLP impact and Adaptive Gaussian Positions"}, {"content": "|               | Burgers\u2019 equation (1) | Burgers\u2019 equation (2) |\n| :------------- |:-------------:| :-------------: |\n| PIG           | 7.68 \u00d7 10\u207b\u2074 (0.28s/it) | 1.08 \u00d7 10\u207b\u00b3 (0.29s/it) |\n| PI-GS         | 1.62 \u00d7 10\u207b\u00b9 (1.5s/it)  | 2.61 \u00d7 10\u207b\u00b9 (1.68s/it) |", "caption": "Table 8: Performance comparison of PIG and PI-GS across 3 benchmark problems. Results include relative L2superscript\ud835\udc3f2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT errors and computation times per iteration (s/it). Benchmarks are conducted on two variations of the (2+1)D Burgers equation.", "description": "This table presents a comparison of the performance of Physics-Informed Gaussians (PIG) and Physics-Informed Gaussian Splatting (PI-GS) across three benchmark problems: two variations of the (2+1)D Burgers' equation and another unnamed problem. For each problem, the table shows the relative L2 error achieved by each method and the computation time per iteration in seconds (s/it).  This comparison highlights the relative efficiency and accuracy of the two approaches in solving these benchmark partial differential equations (PDEs).", "section": "4 Experiments"}]