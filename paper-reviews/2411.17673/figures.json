[{"figure_path": "https://arxiv.org/html/2411.17673/x2.png", "caption": "Figure 1: \nSketchAgent leverages an off-the-shelf multimodal LLM to facilitate language-driven, sequential sketch generation through an intuitive sketching language. It can sketch diverse concepts, engage in interactive sketching with humans, and edit content via chat.", "description": "This figure illustrates SketchAgent's capabilities.  It shows how the system uses a pre-trained multimodal large language model (LLM) and a custom sketching language to generate sketches sequentially based on textual descriptions.  The top row demonstrates the text-conditioned sequential generation, depicting how the system generates a sketch based on various textual prompts (such as \"butterfly\", \"Taj Mahal\"). The middle section highlights the human-agent collaborative sketching aspect, where the user and agent take turns sketching on a canvas, creating a sketch together. Finally, the rightmost section shows the system's chat-based sketch editing functionality, where the user can provide instructions for editing the sketch via natural language interactions.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.17673/x3.png", "caption": "Figure 2: Examples of sketches used across disciplines and goals. (A) Ideation and design: Process Elevation Sketches by the architect Frank Gehry, Guggenheim Museum. (B) Engineering: Alexander Bell\u2019s telephone drawing. (C) Expressing emotions: Children\u2019s sketches. (D) Visual communication: Planning and communicating game strategy in basketball.", "description": "This figure showcases diverse examples of sketches across various disciplines and purposes, highlighting the versatility of sketching as a communication tool.  Panel (A) illustrates architectural sketches by Frank Gehry, demonstrating the use of sketching in the ideation and design process. Panel (B) shows an engineering drawing by Alexander Graham Bell, highlighting sketching's role in technical communication. Panel (C) presents children's drawings, emphasizing the expressive and emotional nature of sketching. Finally, Panel (D) depicts a basketball game strategy sketch, illustrating the use of sketches for visual communication and planning.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.17673/x4.png", "caption": "Figure 3: Sketch appearance. (A) Text-to-image diffusion models operate in pixel space, lacking thesequential nature of sketches. (B) Prompting LLMs to produce visuals with SVG results in a uniform, mechanical appearance. (C) Sketches produced by our agent appear less mechanical, more closely resembling the nature of (D) Human sketches, which are often spontaneous and irregular.", "description": "Figure 3 demonstrates the differences in sketch generation methods. (A) shows that text-to-image diffusion models generate images in pixel space, which is a single-step process unlike the sequential, iterative process of human sketching. (B) illustrates that while LLMs can generate vector graphics (SVG) from text prompts, the results often appear uniform and machine-like. (C) shows the output of SketchAgent, our proposed method which, while using an LLM, generates sketches that are more natural and less mechanical.  Finally, (D) displays examples of human sketches for comparison, highlighting the spontaneous and irregular nature of human-generated artwork.", "section": "3. Preliminaries"}, {"figure_path": "https://arxiv.org/html/2411.17673/x5.png", "caption": "Figure 4: Cubic B\u00e9zier curve.", "description": "A cubic B\u00e9zier curve is a parametric curve defined by four control points:  a starting point (P0), an ending point (P3), and two control points (P1 and P2) that influence the curve's shape. The curve is smooth and can be described mathematically using a polynomial equation, where a parameter 't' (ranging from 0 to 1) determines the position along the curve.  When t=0, the point is at P0, and when t=1, the point is at P3. The image illustrates a cubic B\u00e9zier curve and its control points, showing how the control points affect the curve's curvature.", "section": "3. Preliminaries"}, {"figure_path": "https://arxiv.org/html/2411.17673/x6.png", "caption": "Figure 5: Method Overview. SketchAgent (blue) receives drawing instructions and generates a string representing the intended sketch. Inputs include: (1) a system prompt (orange) introducing the sketching language and canvas, (2) a user prompt (pink) specifying the task (e.g., \u201cdraw a shark\u201d), and (3) a numbered canvas. The agent\u2019s response outlines a sketching strategy (in thinking tags) and a sequence of strokes defined by coordinates, which are processed into B\u00e9zier curves and rendered onto the canvas.", "description": "SketchAgent, the core component of the system, receives three inputs: (1) a system prompt defining the sketching language and canvas; (2) a user prompt specifying the drawing task (e.g., \"draw a shark\"); and (3) a numbered canvas.  SketchAgent processes these inputs and outputs a textual representation of the intended sketch, including a \"thinking\" section outlining the drawing strategy and a sequence of strokes described by coordinates. These coordinates are then converted into B\u00e9zier curves and rendered on the canvas.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2411.17673/x31.png", "caption": "Figure 6: Although excelling in visual reasoning, multimodal LLMs often struggle to translate these abilities into spatial actions. In this example, GPT-4o [84] intends to draw a line between points 1 and 5 but fails to execute this with a draw_line function that accepts pixel coordinates.", "description": "The figure demonstrates the limitations of multimodal LLMs in performing spatial reasoning tasks.  Despite excelling at visual reasoning, the model struggles to translate this understanding into specific actions within a coordinate system.  The example shows a simple drawing with numbered points. The model correctly identifies the points needing to be connected to complete the drawing.  However, when instructed to draw a line using a `draw_line` function that requires pixel coordinates, the model fails to execute the command correctly, illustrating the disconnect between high-level visual understanding and precise low-level spatial control.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2411.17673/x37.png", "caption": "Figure 7: Methods for processing the agent\u2019s coordinate sequence (in red): (A) Polyline results in an unnatural appearance. (B) Directly using coordinates as B\u00e9zier control points is challenging as they do not lie on the curve. (C) Fitting a B\u00e9zier curve to sampled coordinates provides smoother results.", "description": "This figure compares three different approaches to rendering a sequence of coordinates generated by the SketchAgent model into a smooth curve.  Method (A) uses polylines, resulting in a jagged, unnatural-looking sketch. Method (B) attempts to use the coordinates directly as control points for a B\u00e9zier curve; however, this approach is flawed as B\u00e9zier control points do not typically lie on the curve itself, leading to inaccurate rendering. Method (C), the preferred method, samples points along a curve and then fits a B\u00e9zier curve to these points, achieving a much smoother and more natural-looking result.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2411.17673/extracted/6023764/appendix/figs/system_p_examples.png", "caption": "Figure 8: Sketches produced by SketchAgent for concepts beyond pre-defined categories. The textual input describing the desired concept shown below each image.", "description": "This figure showcases SketchAgent's ability to generate sketches of various concepts that go beyond pre-defined categories.  It displays examples of sketches generated for concepts such as landmarks (Golden Gate Bridge, Mount Fuji, Eiffel Tower), natural objects (DNA Double Helix), and physics concepts (Pendulum Motion, Double Slit Experiment), among others. Each sketch is accompanied by the text prompt used to generate it, highlighting the model's capability to interpret and visually represent diverse and abstract ideas.", "section": "5.1 Text-Conditioned Sketch Generation"}, {"figure_path": "https://arxiv.org/html/2411.17673/x38.png", "caption": "Figure 9: SketchAgent gradually draws stroke-by-stroke, each stroke is annotated by the agent with a semantic meaning.", "description": "This figure showcases SketchAgent's sequential sketching process.  The image shows multiple stages of a camel being drawn, with each stroke labeled by SketchAgent to indicate the specific part of the camel (e.g., head, body, leg) that each stroke represents. This highlights the model's ability to not only generate sketches stroke-by-stroke but also to provide meaningful semantic annotations for each stroke, demonstrating the model's understanding of the drawing process and the object being drawn.", "section": "5.2 Sequential Sketching"}, {"figure_path": "https://arxiv.org/html/2411.17673/x39.png", "caption": "Figure 10: Sequential sketching process. SVGDreamer [127] requires 2000 iterations (1.6 hours) with intermediate steps lacking semantic meaning. SketchRNN [48] operates in real-time with coherent steps but is limited to QuickDraw categories. SketchAgent creates sketches gradually with meaningful strokes and no category restrictions. Human sketches also evolve through gradual, meaningful steps.", "description": "Figure 10 compares the sketching processes of four different methods: SVGDreamer, SketchRNN, SketchAgent, and human sketching.  SVGDreamer, a purely optimization-based approach, is extremely slow (1.6 hours) and produces intermediate results that lack semantic meaning.  SketchRNN, a recurrent neural network trained on the QuickDraw dataset, operates in real time, but is limited to the categories present in that dataset.  SketchAgent, in contrast, generates sketches stroke by stroke in real time and demonstrates semantic meaning in each step, without being limited to predefined categories.  Finally, human sketches are also shown as an example of gradual, meaningful stroke generation.", "section": "5.2 Sequential Sketching"}, {"figure_path": "https://arxiv.org/html/2411.17673/x40.png", "caption": "Figure 11: Sequential sketching analysis of SketchAgent (blue) and Humans [54] (orange). Left: Histograms of stroke distribution per sketch, showing QuickDraw sketches are more abstract on average. Right: CLIPScore as a function of the accumulated number of strokes for sketches containing 4-7 strokes, showing a similar recognition pattern over time.", "description": "This figure compares SketchAgent's performance to human sketching in terms of sequential stroke generation. The left panel displays histograms showing the distribution of stroke counts in SketchAgent-generated sketches versus those from the QuickDraw dataset.  The QuickDraw sketches tend to have fewer strokes, suggesting a more abstract style. The right panel shows how the CLIP image recognition score (a measure of how well the sketch represents its intended subject) changes as more strokes are added to sketches containing between 4 and 7 strokes. This analysis reveals that SketchAgent's sketches show a similar increase in recognition score as more strokes are added, comparable to human drawings.", "section": "5.2 Sequential Sketching"}, {"figure_path": "https://arxiv.org/html/2411.17673/x41.png", "caption": "Figure 12: Collaborative sketching evaluation measured using CLIP classification.\nSketches created collaboratively (blue) approaching those made solely by users (dashed lines). In collaborative sketches, keeping agent-only strokes (pink) or user-only strokes (green) significantly reduces recognizability.", "description": "This figure displays the results of a collaborative sketching experiment comparing the performance of human-only sketching with human-agent collaborative sketching.  The x-axis represents different categories of sketches, and the y-axis shows the CLIP (Contrastive Language\u2013Image Pre-training) classification accuracy. The blue bars represent the accuracy of sketches produced collaboratively by humans and an AI agent.  The dashed lines show the accuracy of sketches made solely by human participants.  The pink and green bars highlight a key finding:  when only the AI agent's strokes (pink) or only the human's strokes (green) are considered from the collaborative sketches, the overall accuracy drops significantly, indicating that the combined contributions of both human and AI are necessary for high recognition accuracy.", "section": "5.3 Human-Agent Collaborative Sketching"}, {"figure_path": "https://arxiv.org/html/2411.17673/x42.png", "caption": "Figure 13: Chat-based sketch editing. We iteratively prompt SketchAgent to add objects to sketches through chat dialogues.", "description": "This figure demonstrates the iterative chat-based sketch editing capabilities of SketchAgent.  The user starts with an initial sketch. Through a chat interface, the user provides textual instructions to the SketchAgent, such as adding specific objects (e.g., 'Add glasses', 'Add a hat') or specifying spatial relationships ('Tree to the left', 'Sun on top right'). The agent then updates the sketch accordingly by adding the requested elements, resulting in a refined and progressively more complex drawing. This showcases the model's ability to understand language-based instructions and use them to modify an existing drawing, adding another layer to the overall process of sketch creation.", "section": "4.1. In-Chat Editing and Collaborative Sketching"}, {"figure_path": "https://arxiv.org/html/2411.17673/x43.png", "caption": "Figure 14: Limitations. Sketches of complex concepts (A) and human figures (B) appear too abstract and unrecognizable with non-professional style. (C) Fail to depict letters and numbers.", "description": "Figure 14 demonstrates limitations of SketchAgent in handling complex concepts and human figures. Panel A shows an example where the description of a unicorn is accurate, but the resulting sketch is unrecognizable due to its abstract and amateurish style. Panel B shows a similar issue with human figures, where distinctive features are captured in the text, but the sketch lacks detail and expressiveness. Panel C shows that the model struggles to represent letters and numbers.", "section": "7. Limitations and Future Work"}, {"figure_path": "https://arxiv.org/html/2411.17673/x44.png", "caption": "Figure 15: Visualization of single-stroke primitives used in the system prompt to introduce the grid and sketching language to the agent.", "description": "This figure shows examples of single-stroke primitives used to introduce the sketching language and grid-based canvas to the SketchAgent model.  The primitives include simple shapes like lines, ellipses, and rectangles, defined using coordinate pairs and B\u00e9zier curve parameters. This introduction allows the model to understand the sketching language which enables it to generate sketches sequentially. The visual examples are meant to teach the model how to utilize this language for creating sketches in a grid-based system. ", "section": "3. Preliminaries"}, {"figure_path": "https://arxiv.org/html/2411.17673/x45.png", "caption": "Figure 16: Visualization of the simple sketch of a house provided as an in-context example, represented with our sketching language through the user prompt.", "description": "This figure shows a simple sketch of a house, used as an in-context example within the SketchAgent model.  The sketch is represented using the model's intuitive sketching language, which consists of a numbered grid for spatial referencing and string-based actions that define strokes. The example is designed to illustrate how the model interprets this novel language for sequential sketch generation.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2411.17673/x46.png", "caption": "Figure 17: Sketch variability. Example of twelve different sketches produced for the concept \u201crabbit\u201d by SketchAgent, with the same settings.", "description": "This figure demonstrates the variability of SketchAgent's output when generating sketches for the same concept.  Twelve different sketches of a rabbit were produced using the same model settings. This illustrates the creativity and flexibility of the model, as it produces diverse variations rather than repetitive or identical drawings. The differences in the sketches highlight variations in style, pose, detail, and the overall level of abstraction.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2411.17673/x47.png", "caption": "Figure 18: Randomly selected sketches of scientific concepts. Ten textual concepts were randomly selected using GPT-4o. Five sketches were generated per concept, showcasing the variability and diversity of the outputs.", "description": "This figure displays ten scientific concepts, each represented by five different sketches generated using GPT-40.  The sketches illustrate the variability and diversity achievable by the model when prompted with the same textual description.", "section": "5.1. Text-Conditioned Sketch Generation"}, {"figure_path": "https://arxiv.org/html/2411.17673/x48.png", "caption": "Figure 19: Randomly selected sketches of diagrams across fields. Ten textual concepts were randomly selected using GPT-4o. Five sketches were generated per concept, showcasing the variability and diversity of the outputs.", "description": "This figure displays a collection of sketches generated by the SketchAgent model in response to prompts describing various types of diagrams.  Ten different diagram types were randomly chosen, and five sketches of each were created to illustrate the variability and diversity in the model's output. The range of diagram types demonstrates the model's ability to generate sketches across many different fields and styles.", "section": "5.1 Text-Conditioned Sketch Generation"}, {"figure_path": "https://arxiv.org/html/2411.17673/x49.png", "caption": "Figure 20: Randomly selected sketches of notable landmarks. Ten textual concepts were randomly selected using GPT-4o. Five sketches were generated per concept, showcasing the variability and diversity of the outputs.", "description": "This figure displays ten randomly generated sketches of famous landmarks. Each landmark is represented by five different sketches, created using a text-to-image model (GPT-40). The variation in style and detail across the sketches highlights the model's ability to generate diverse outputs from a single textual prompt.", "section": "5.1 Text-Conditioned Sketch Generation"}, {"figure_path": "https://arxiv.org/html/2411.17673/extracted/6023764/figs/appendix/2afc_interface.png", "caption": "Figure 21: Confusion matrix (showing top 10 confused classes) for the set of 500 sketches generated with SketchAgent default settings (Claude3.5-Sonnet) across 50 categories", "description": "This confusion matrix visualizes the top 10 classes most frequently misclassified by SketchAgent when generating 500 sketches across 50 different categories. The model used was Claude3.5-Sonnet with default settings.  The matrix shows the frequency of misclassifications between each pair of classes, highlighting which categories are most commonly confused with one another. This helps to understand the model's limitations and potential areas for improvement.", "section": "5.1 Text-Conditioned Sketch Generation"}, {"figure_path": "https://arxiv.org/html/2411.17673/extracted/6023764/figs/appendix/2afc_instructions.png", "caption": "Figure 22: Visualization of sketches from the six most confused classes. The correct category is highlighted in green, while the misclassified category is highlighted in red.", "description": "This figure visualizes examples from the six QuickDraw classes most frequently misclassified by the SketchAgent model.  Each row displays ten sketches from a single class. The correct class label is shown in green at the left, while the class to which the sketches were misclassified is in red. This helps illustrate the types of visual similarities that may cause the SketchAgent model to confuse these categories.", "section": "5.1 Text-Conditioned Sketch Generation"}, {"figure_path": "https://arxiv.org/html/2411.17673/x66.png", "caption": "Figure 23: Visualization of the top recognized classes for the set of 500 sketches generated with our default settings (Claude3.5-Sonnet) across 50 categories.", "description": "This figure visualizes the top ten most accurately recognized QuickDraw categories from a set of 500 sketches generated using the SketchAgent model with the Claude3.5-Sonnet LLM.  Each category is represented by a series of sketches, demonstrating the model's ability to generate diverse yet recognizable representations. The visualization highlights the model's success in producing sketches that are readily classifiable within the defined categories.", "section": "5.1 Text-Conditioned Sketch Generation"}, {"figure_path": "https://arxiv.org/html/2411.17673/x67.png", "caption": "Figure 24: Visualization of sketches from the most recognized classes across all backbone models. The classes selected based on the two most recognizable classes in each model.", "description": "This figure displays a comparison of sketches generated by different large language models (LLMs) for the most easily recognized object categories.  Each model produced ten sketches per category; the two most accurately identified categories are shown.  This visualization highlights the visual differences in sketch style and accuracy across various LLMs.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2411.17673/x68.png", "caption": "Figure 25: Visualization of sketches from the least recognized classes across all backbone models. The classes selected based on the two least recognizable classes in each model.", "description": "This figure displays sketches generated by four different large language models (LLMs) for the two least accurately recognized object categories by a CLIP zero-shot classifier.  The LLMs used were GPT-40, GPT-40-mini, Claude3-Opus, and Claude3.5-Sonnet. The purpose is to visually compare the generated sketches across different models, highlighting the variations in style, detail, and accuracy in representing the intended object.  The selected categories are those consistently showing low recognition accuracy across models, demonstrating the challenges in generating visually accurate and recognizable sketches with these models.", "section": "5.1 Text-Conditioned Sketch Generation"}, {"figure_path": "https://arxiv.org/html/2411.17673/x69.png", "caption": "Figure 26: Sketches generated using Llama-3.2-11B-Vision as our backbone models. The model frequently replicates the in-context example of a house provided in the user prompt.", "description": "This figure visualizes the results of using Llama-3.2-11B-Vision, an open-source large language model, within the SketchAgent framework.  It highlights a key limitation of using open-source models: a tendency to over-rely on the in-context examples provided during prompting.  The sketches generated frequently replicate the house example provided earlier, demonstrating that the model hasn't fully grasped the concept of general sketch generation. This is in contrast to the results obtained with more advanced, closed-source models where greater diversity in sketch generation is observed.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2411.17673/x70.png", "caption": "Figure 27: Visualization of the eight top recognized classes for the set of 500 sketches generated with Llama-3.1-405B-Instruct as our backbone model.", "description": "This figure visualizes the eight QuickDraw classes most accurately recognized by SketchAgent when using the Llama-3.1-405B-Instruct model.  It showcases ten example sketches per class, illustrating the model's ability to generate diverse and recognizable images for those specific categories. The model's performance is particularly strong for these eight classes.", "section": "More Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2411.17673/x71.png", "caption": "Figure 28: Visualization of sketches from the eight least recognized classes for the set of 500 sketches generated with Llama-3.1-405B-Instruct as our backbone model.", "description": "This figure visualizes sketches generated by the SketchAgent model using the Llama-3.1-405B-Instruct large language model.  Specifically, it shows examples from the eight categories that were least accurately recognized by a CLIP classifier.  This highlights the model's struggles with certain object categories, indicating areas where the model's visual understanding could be improved. The visualization helps understand the types of sketches the model produces for difficult object classes, providing insight into model limitations.", "section": "5.1 Text-Conditioned Sketch Generation"}, {"figure_path": "https://arxiv.org/html/2411.17673/x72.png", "caption": "Figure 29: Direct prompting for SVG generation across different backbone models. The SVGs generated by GPT-4o and Claude3.5-Sonnet appear more expressive and visually appealing compared to those produced by GPT-4o-mini and Claude3-Opus, aligning well with the performance differences observed in sketch generation.", "description": "This figure compares the results of directly prompting four different large language models (LLMs) to generate Scalable Vector Graphics (SVGs) that resemble sketches.  The LLMs used were GPT-4, GPT-4 mini, Claude 3.5-Sonnet, and Claude Opus.  The comparison highlights that GPT-4 and Claude 3.5-Sonnet produce SVGs with a more expressive and visually appealing style than the other two models. This difference in visual quality correlates with the overall performance differences observed in the sketch generation experiments reported in the paper, where Claude 3.5-Sonnet showed superior results.", "section": "5.1 Text-Conditioned Sketch Generation"}, {"figure_path": "https://arxiv.org/html/2411.17673/x73.png", "caption": "Figure 30: An example of our 2AFC session.", "description": "This figure shows a screenshot of a two-alternative forced choice (2AFC) user study session.  In this type of study, participants are presented with pairs of images and asked to choose which image belongs to a certain category.  This particular screenshot shows one trial, where one image was created by a human and the other by a computer model. The participant must select which of the two images is the human drawing. This type of experiment helps to assess the human-likeness of computer-generated images and how well they can mimic human-created content.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2411.17673/", "caption": "Figure 31: 2AFC instructions to users.", "description": "The figure shows the instructions given to participants in a two-alternative forced choice (2AFC) user study. Participants were shown pairs of sketches, one created by a human and the other by a computer, and asked to identify the human-drawn sketch. The instructions emphasized the voluntary nature of participation and assured anonymity.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2411.17673/", "caption": "Figure 32: Distribution of human sketches [54] (top) and SketchAgent\u2019s sketches (bottom) based on the number of strokes per sketch. Representative examples are shown for sketches drawn with 1, 4, 7, and 15 strokes. Notably, in the QuickDraw dataset, single-stroke sketches often consist of a single long continuous line.", "description": "Figure 32 presents a comparative analysis of stroke distributions in human-drawn sketches from the QuickDraw dataset and those generated by SketchAgent. The top half shows the distribution of strokes in human sketches, while the bottom half displays the distribution for SketchAgent-generated sketches.  Histograms illustrate the frequency of sketch complexity, measured by the number of strokes used.  Representative examples for sketches with 1, 4, 7, and 15 strokes are shown for both human and SketchAgent outputs.  A key observation is that human sketches, particularly those with only a single stroke, often consist of a single, continuous line.", "section": "5.2 Sequential Sketching"}, {"figure_path": "https://arxiv.org/html/2411.17673/x88.png", "caption": "Figure 33: Sequential four-stroke sketches of a pear, purse, and screwdriver, created by humans [54] and by SketchAgent.", "description": "This figure shows a comparison of how humans and the SketchAgent AI system create drawings.  It focuses on the sequential nature of sketching, showing the steps involved in drawing three different objects: a pear, a purse, and a screwdriver.  Four strokes are shown for each object for both human and AI-generated drawings, allowing a visual comparison of the process and the style of each.  The human drawings are taken from the QuickDraw dataset.", "section": "5.2 Sequential Sketching"}, {"figure_path": "https://arxiv.org/html/2411.17673/x89.png", "caption": "Figure 34: Sequential five-stroke sketches of a pear, purse, and screwdriver, created by humans [54] and by SketchAgent.", "description": "This figure shows a comparison of sequential sketching.  It displays five-stroke sketches of a pear, purse, and screwdriver drawn by humans (from the QuickDraw dataset) alongside sketches generated by the SketchAgent model. This comparison highlights the model's ability to mimic the sequential and iterative nature of human sketching. The evolution of the sketches from simple initial strokes to more complex shapes is visible for both human and model-generated examples.", "section": "5.2 Sequential Sketching"}, {"figure_path": "https://arxiv.org/html/2411.17673/x90.png", "caption": "Figure 35: Sequential six-stroke sketches of a television, bed, and peanut, created by humans [54] and by SketchAgent.", "description": "This figure displays a comparison of six-stroke sketches of a television, bed, and peanut.  The sketches on the left were created by humans, taken from the QuickDraw dataset [54].  The sketches on the right were generated by the SketchAgent model. This visualization highlights the differences between human sketching and the model's sequential stroke-by-stroke generation process.", "section": "5.2 Sequential Sketching"}, {"figure_path": "https://arxiv.org/html/2411.17673/x91.png", "caption": "Figure 36: Sequential seven-stroke sketches of a backpack, fish, and house, created by humans [54] and by SketchAgent.", "description": "This figure shows a comparison of seven-stroke sketches of a backpack, fish, and house, drawn by humans and SketchAgent.  It illustrates the sequential nature of SketchAgent's drawing process, showing how the model builds the sketch stroke-by-stroke, as opposed to generating the entire image at once. The human sketches, sourced from the QuickDraw dataset, offer a baseline for natural, human-like sketching style.  The comparison highlights the similarities and differences in the approaches to generating sketches. ", "section": "5.2 Sequential Sketching"}]