[{"content": "| Image | Description |\n|---|---| \n| [https://arxiv.org/html/2411.17673/DALLE.png](https://arxiv.org/html/2411.17673/DALLE.png) | (A) DALLE3 [5] |\n| [https://arxiv.org/html/2411.17673/eye-illustration_claude_svg.png](https://arxiv.org/html/2411.17673/eye-illustration_claude_svg.png) | (B) LLMs [3] (SVG) |\n| [https://arxiv.org/html/2411.17673/eye_ours.png](https://arxiv.org/html/2411.17673/eye_ours.png) | (C) SketchAgent |\n| [https://arxiv.org/html/2411.17673/eye_human.png](https://arxiv.org/html/2411.17673/eye_human.png) | (D) Human [54] |", "caption": "Table 1: Sketch recognition evaluation. Average Top-1 and Top-5 sketch recognition accuracy computed with CLIP zero-shot classifier on 500 sketches from 50 categories. The last row visualizes one sample from each experiment. *Indicates our default settings, which receives the highest accuracy among all models.", "description": "This table presents the results of a zero-shot sketch recognition evaluation using the CLIP model.  500 sketches were generated across 50 categories, with 10 sketches generated per category using different methods (human drawing and various LLMs). The table shows the average top-1 and top-5 accuracies of the recognition task.  The 'Top-1' accuracy represents the percentage of sketches correctly classified as belonging to the intended category.  'Top-5' represents the percentage of sketches classified correctly among the top five most probable categories suggested by the model. The last row visually displays one example sketch generated by each method (human and LLMs).  The asterisk (*) indicates the model and settings which provided the highest overall accuracy.", "section": "5 Results"}, {"content": "| (B) LLMs [3] |\n|---|---| \n| (SVG) |", "caption": "Table 2: Ablation study. Average Top-1 and Top-5 CLIP recognition accuracy. We systematically remove each component in our pipeline, showcasing all components contribute to the agent\u2019s full performance.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of each component within the SketchAgent pipeline on its overall performance.  The study systematically removed each component (system prompt, chain-of-thought prompting, and the in-context learning example) one at a time, measuring the resulting Top-1 and Top-5 CLIP recognition accuracy.  The results demonstrate the contribution of each component to the model's ability to generate high-quality sketches.", "section": "6. Ablation"}, {"content": "| Image | Caption |\n|---|---| \n| [https://arxiv.org/html/2411.17673/output_Golden_Gate_Bridge.png](https://arxiv.org/html/2411.17673/output_Golden_Gate_Bridge.png) | Golden Gate Bridge |\n| [https://arxiv.org/html/2411.17673/output_Mount_Fuji.png](https://arxiv.org/html/2411.17673/output_Mount_Fuji.png) | Mount Fuji |\n| [https://arxiv.org/html/2411.17673/output_Eiffel_Tower.png](https://arxiv.org/html/2411.17673/output_Eiffel_Tower.png) | Eiffel Tower |\n| [https://arxiv.org/html/2411.17673/output_DNA_Double_Helix.png](https://arxiv.org/html/2411.17673/output_DNA_Double_Helix.png) | DNA Double Helix |\n| [https://arxiv.org/html/2411.17673/Pendulum_Motion_plain_0.png](https://arxiv.org/html/2411.17673/Pendulum_Motion_plain_0.png) | Pendulum Motion |\n| [https://arxiv.org/html/2411.17673/The_Double-Slit_Experiment_plain_0.png](https://arxiv.org/html/2411.17673/The_Double-Slit_Experiment_plain_0.png) | Double-Slit Experiment |\n| [https://arxiv.org/html/2411.17673/output_Flowchart.png](https://arxiv.org/html/2411.17673/output_Flowchart.png) | Flowchart |\n| [https://arxiv.org/html/2411.17673/x_wins.png](https://arxiv.org/html/2411.17673/x_wins.png) | Tic-tac-toe |", "caption": "Table 3: Recognition rate and 95% CI across collaborative full and partial sketches. In collaborative sketches, keeping agent-only strokes or user-only strokes significantly reduces recognizability.", "description": "This table presents the results of a quantitative analysis evaluating the effectiveness of collaborative sketching between humans and the SketchAgent model.  It shows the recognition accuracy (with 95% confidence intervals) for three conditions: complete collaborative sketches, sketches with only the agent's strokes, and sketches with only the user's strokes. The results demonstrate the synergistic nature of the collaboration, showing that complete collaborative sketches achieve significantly higher recognition rates compared to sketches generated by either humans or the agent alone.  This highlights the importance of both human and agent contributions for achieving meaningful results in collaborative sketching.", "section": "B.3 Human-Agent Collaborative Sketching"}]