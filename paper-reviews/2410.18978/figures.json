[{"figure_path": "2410.18978/figures/figures_1_0.png", "caption": "Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.", "description": "The figure showcases examples of frame interpolation results generated by the Framer model.  The top section (a) displays three rows of results using the same starting and ending images, demonstrating the model's ability to produce varying interpolation sequences based on user-specified control points (indicated by varying drag interactions). The bottom section (b) shows two further examples, highlighting the model's capacity for smooth interpolation even in complex scenarios with significant shape and style differences between start and end frames (such as the Pok\u00e9mon example), again overlaying the control point trajectories.", "section": "ABSTRACT"}, {"figure_path": "2410.18978/figures/figures_4_0.png", "caption": "Figure 2: Framer supports (a) a user-interactive mode for customized point trajectories and (b) an \"autopilot\" mode for video frame interpolation without trajectory inputs. During training, (d) we fine-tune the 3D-UNet of a pre-trained video diffusion model for video frame interpolation. Afterward, (c) we introduce point trajectory control by freezing the 3D-UNet and fine-tuning the controlling branch.", "description": "This figure illustrates the architecture and workflow of the Framer model.  Panel (a) shows the user-interactive mode where users can customize the trajectory of selected keypoints to control the interpolation process.  Panel (b) depicts the \"autopilot\" mode, which automatically estimates and refines the keypoint trajectories. Panel (c) details the trajectory controlling branch used to incorporate user input or automatically generated trajectories. Finally, panel (d) outlines the training process, which involves fine-tuning a pre-trained 3D-UNet video diffusion model for video frame interpolation and then fine-tuning the control branch for more precise control.", "section": "3 METHOD"}, {"figure_path": "2410.18978/figures/figures_5_0.png", "caption": "Figure 3: Point trajectory estimation. The point trajectory is initialized by interpolating the coordinates of matched keypoints. In each de-noising step, we perform point tracking by finding the nearest neighbor of keypoints in the start and end frames, respectively. Lastly, We check the bi-directional tracking consistency before updating the point coordinate.", "description": "This figure illustrates the process of point trajectory estimation in the \"autopilot\" mode of the Framer model.  It shows two subfigures, (a) Forward Point Tracking and (b) Backward Point Tracking. Both subfigures detail how the model initializes point trajectories by interpolating coordinates of matched keypoints from the start and end frames using SIFT feature matching. In each denoising step, the model performs point tracking to update trajectory coordinates by identifying the nearest neighbor in the feature space from the start or end frames. A final bi-directional consistency check ensures the accuracy of the updated point coordinates before they are used in the next denoising step.  The flow of information and processes is clearly represented in the figure.", "section": "3 \u041c\u0415\u0422\u041dOD"}, {"figure_path": "2410.18978/figures/figures_6_0.png", "caption": "Figure 4: Qualitative comparison. 'GT\u2019 strands for ground truth. For each method, we only present the middle frame of 7 interpolated frames. The full results can be seen in Fig. S4 and Fig. S5 in the Appendix.", "description": "The figure presents a qualitative comparison of different video frame interpolation methods.  It shows the middle frame from a sequence of 7 interpolated frames generated by each method, for three example video clips.  The results are compared against ground truth ('GT').  Each row shows the results for a single video clip. The methods compared include AMT, RIFE, FLAVR, FILM, LDMVFI, DynamicCrafter, SVDKFI, and the proposed Framer method. The figure aims to visually demonstrate the differences in interpolation quality and smoothness between the methods.", "section": "4.2 COMPARISON"}, {"figure_path": "2410.18978/figures/figures_6_1.png", "caption": "Figure 5: Reults on human preference.", "description": "This figure is a pie chart showing the results of a human preference study comparing Framer with several other video interpolation methods.  The vast majority of preferences (90.5%) went to Framer, while the remaining preferences were distributed across other methods:  AMT (1.7%), RIFE (0.7%), FLAVR (0.8%), FILM (4.4%), LDMVFI (0.7%), and SVDKFI (1.2%).  This visually demonstrates the strong preference among human evaluators for the video interpolation results generated by Framer.", "section": "4.2 COMPARISON"}, {"figure_path": "2410.18978/figures/figures_7_0.png", "caption": "Figure 6: Results on user interaction. The first row is generated without drag input, while the other two are generated with different drag controls. Customized trajectories are overlaid on frames.", "description": "This figure showcases the interactive capabilities of the Framer model. It presents three rows of image sequences demonstrating frame interpolation results with different levels of user interaction. The first row shows a baseline interpolation without any user interaction. The second row shows an interpolation with a single drag-based trajectory modification, and the third row shows an interpolation with two independent drag-based modifications.  Each image sequence contains the start frame, several generated intermediate frames, and the end frame, demonstrating the smooth transition between the start and end states under varying levels of user control. The user-defined trajectories are clearly overlaid on the frames, visually highlighting the effect of user input on the interpolation process.", "section": "3.2 INTERACTIVE FRAME INTERPOLATION"}, {"figure_path": "2410.18978/figures/figures_7_1.png", "caption": "Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.", "description": "The figure displays several examples of frame interpolation results generated by the Framer model. The top part shows how different drag interactions on the same input image pair result in varying interpolation results, emphasizing the model's fine-grained controllability. The bottom part showcases examples where the model successfully handles challenging cases with significant shape and style changes in objects between the start and end frames, achieving smooth transitions. Input trajectories are overlaid on the frames for visualization.", "section": "ABSTRACT"}, {"figure_path": "2410.18978/figures/figures_8_0.png", "caption": "Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.", "description": "The figure showcases example results generated by the Framer model for interactive frame interpolation. The top row shows three sets of input images (start, intermediate, and end frames). The subsequent rows show results of interpolating image pairs using Framer. The first three rows illustrate the model's ability to generate different interpolation results with fine-grained customization based on varying user input of drag motions, keeping the same start and end frames. The last two rows display the model's capacity to handle more challenging image interpolation scenarios involving significant changes of object appearance, with smoother results compared to the first three rows. In all rows, the user-defined trajectories are shown overlaid on the image frames.", "section": "ABSTRACT"}, {"figure_path": "2410.18978/figures/figures_8_1.png", "caption": "Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.", "description": "The figure showcases the results of the Framer model, demonstrating its ability to generate smooth and varied interpolations between image pairs.  The top three rows show examples where different user-specified trajectories produce different interpolated images from the same input pair, highlighting Framer's fine-grained customization capabilities. The bottom two rows demonstrate the model's ability to handle challenging cases that involve significant object changes and still produce smooth image morphing. In all cases, the user-defined trajectories are overlaid on the frames for reference.", "section": "ABSTRACT"}, {"figure_path": "2410.18978/figures/figures_9_0.png", "caption": "Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.", "description": "The figure showcases examples of frame interpolation results generated by the Framer model. The top section shows image morphing with varying drags applied to the same input image pair, illustrating the fine-grained control over local motions. The middle section demonstrates image morphing (inputs highlighted by dashed boxes),  again highlighting the model's capability for varied results. The bottom section displays more challenging interpolation tasks demonstrating the model's ability to handle scenarios where objects significantly change shape and style across frames. In all cases, the input trajectories are overlaid on the generated frames to show how user-defined trajectories control the interpolation process.", "section": "ABSTRACT"}, {"figure_path": "2410.18978/figures/figures_9_1.png", "caption": "Figure 12: Ablations on each component. \"w/o trajectory\" denotes inference without guidance from point trajectory, \"w/o traj. update\" indicates inference without trajectory updates, and \"w/o bi\" suggests trajectory updating without bi-directional consistency verification.", "description": "This figure presents an ablation study comparing the performance of Framer under different conditions.  The first column shows the input frames with the keypoints trajectories obtained via SIFT matching. Subsequent columns visualize the intermediate frames generated by Framer with various components removed. The \"Framer (Ours)\" column displays results with all components included.  The \"w/o trajectory\" column omits the trajectory guidance, \"w/o traj. update\" column shows results without trajectory updates, and \"w/o bi-directional\" column presents results without bi-directional consistency verification during trajectory updates. By visually comparing the results across columns, one can analyze the influence of each component (trajectory guidance, trajectory update, and bi-directional consistency check) on the final generated frames.", "section": "4.4 ABLATIONS STUDIES"}, {"figure_path": "2410.18978/figures/figures_17_0.png", "caption": "Figure 4: Qualitative comparison. 'GT\u2019 strands for ground truth. For each method, we only present the middle frame of 7 interpolated frames. The full results can be seen in Fig. S4 and Fig. S5 in the Appendix.", "description": "This figure presents a qualitative comparison of different video frame interpolation methods. It shows the middle frame from a sequence of 7 interpolated frames generated by each method for the same input video frames.  The methods compared include AMT, RIFE, FLAVR, FILM, LDMVFI, DynamicCrafter, SVDKFI, and the proposed Framer method, with the ground truth also shown for reference. The figure highlights the visual differences in terms of texture clarity, naturalness of motion, and overall quality of interpolation, particularly showcasing the superior performance of Framer in handling cases with significant motion and appearance changes.", "section": "4.2 Comparison"}, {"figure_path": "2410.18978/figures/figures_18_0.png", "caption": "Figure 4: Qualitative comparison. 'GT\u2019 strands for ground truth. For each method, we only present the middle frame of 7 interpolated frames. The full results can be seen in Fig. S4 and Fig. S5 in the Appendix.", "description": "The figure presents a qualitative comparison of different video frame interpolation methods. It displays the middle frame of seven interpolated frames generated by eight different methods (AMT, RIFE, FLAVR, FILM, LDMVFI, DynamiCrafter, SVDKFI, and Framer), alongside the ground truth ('GT'). The comparison allows for a visual assessment of the quality of interpolation results produced by each method, showing differences in texture clarity, motion smoothness, and overall visual fidelity.", "section": "4.2 COMPARISON"}, {"figure_path": "2410.18978/figures/figures_19_0.png", "caption": "Figure 4: Qualitative comparison. 'GT\u2019 strands for ground truth. For each method, we only present the middle frame of 7 interpolated frames. The full results can be seen in Fig. S4 and Fig. S5 in the Appendix.", "description": "Figure 4 presents a qualitative comparison of video frame interpolation results from various methods, including the proposed Framer.  For each method, only the middle frame from a sequence of seven interpolated frames is shown for brevity.  The methods compared are AMT, RIFE, FLAVR, FILM, LDMVFI, DynamicCrafter, SVDKFI, and the proposed Framer.  Ground truth ('GT') is also included as a reference. This figure aims to visually demonstrate the superior quality and realistic textures produced by Framer compared to existing methods, particularly in the middle frame that is most critical for realistic interpolation.", "section": "4.2 Comparison"}, {"figure_path": "2410.18978/figures/figures_20_0.png", "caption": "Figure 4: Qualitative comparison. 'GT\u2019 strands for ground truth. For each method, we only present the middle frame of 7 interpolated frames. The full results can be seen in Fig. S4 and Fig. S5 in the Appendix.", "description": "The figure is a qualitative comparison of different video frame interpolation methods.  It shows the middle frame of seven interpolated frames generated by various methods (AMT, RIFE, FLAVR, FILM, LDMVFI, DynamicCrafter, SVDKFI, and Framer (Ours)), along with the ground truth (GT) for comparison, for a given video sequence.  Each method's output is displayed side by side, allowing for a visual assessment of the quality of the generated frames compared to the original frames.", "section": "4.2 COMPARISON"}, {"figure_path": "2410.18978/figures/figures_21_0.png", "caption": "Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.", "description": "The figure displays examples of frame interpolation results generated by the Framer model. The top row shows the starting frame, the middle rows present various interpolation results created by the model, and the bottom row shows the final frame.  The first three rows demonstrate fine-grained control over local motions from using the same starting and ending frames. The last two rows showcase Framer's ability to successfully handle challenging scenarios where the objects in the start and end frames are significantly different, resulting in smooth transitions. Input trajectories of keypoints are overlaid on the frames to illustrate how these points are manipulated during the interpolation process.", "section": "ABSTRACT"}, {"figure_path": "2410.18978/figures/figures_21_1.png", "caption": "Figure S10: More results on (a) cartoon and (b) sketch interpolation.", "description": "The figure presents additional results showcasing the model's ability to perform interpolation on cartoon and sketch drawings.  The top half (a) shows the interpolation of cartoon-style video frames, illustrating smooth transitions between different poses and expressions of the animated characters. The bottom half (b) displays similar results, but with sketch-style drawings, demonstrating the model's versatility in handling diverse visual styles and maintaining coherence across interpolated frames.", "section": "More Qualitative Results"}, {"figure_path": "2410.18978/figures/figures_22_0.png", "caption": "Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.", "description": "The figure showcases examples of frame interpolation results generated by the Framer model. The first three rows demonstrate the model's ability to produce varying interpolation results for the same input image pair with different user-specified drags, highlighting the fine-grained control offered by the interactive system.  The final two rows showcase the model's capacity to handle more challenging scenarios, where objects change significantly in shape and appearance between the start and end frames, demonstrating smooth transitions nonetheless.  In all cases, the input trajectories are overlaid on the frames to visually represent the user interaction.", "section": "ABSTRACT"}, {"figure_path": "2410.18978/figures/figures_22_1.png", "caption": "Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.", "description": "The figure showcases examples of frame interpolation results generated by the Framer model.  The first three rows demonstrate fine-grained control over local motions, showing how varying interpolation results are achieved from the same starting and ending images by altering the dragging of selected keypoints.  The bottom two rows illustrate the model's ability to handle more challenging scenarios involving significant shape and style differences between the start and end frames, resulting in smooth transitions. In all cases, the user-defined trajectories are overlaid on top of the frames to visualize the motion path.", "section": "ABSTRACT"}, {"figure_path": "2410.18978/figures/figures_22_2.png", "caption": "Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.", "description": "The figure showcases examples of frame interpolation results generated by the proposed Framer model.  The top section displays image morphing results for three different pairs of input images, each row illustrating varied interpolation results generated with different user-specified keypoint trajectories. The bottom section presents examples of frame interpolations that handle challenging cases, such as significant changes in object shape and style, demonstrating the model's ability to achieve smooth image morphing even under difficult conditions.  Overlaid on each frame is the user-specified trajectory of the selected keypoints.", "section": "ABSTRACT"}]