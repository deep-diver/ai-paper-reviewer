[{"content": "| Dimensions | Specific Dimensions | Specific Dimensions | Specific Dimensions | Specific Dimensions | Specific Dimensions | Specific Dimensions | Holistic Dimensions | Holistic Dimensions | Holistic Dimensions |\n|---|---|---|---|---|---|---|---|---|---|---| \n| Methods | Dynamic | Temporal | Human | Object | Color | Overall | Semantic | Quality | Overall |\n| Degree | Style | Action | Class |  | Consist. | Score | Score | Score |\n| Gen-3 | 60.1 | 24.7 | 96.4 | 87.8 | 80.9 | 26.7 | 75.2 | 84.1 | 82.3 |\n| Allegro | 55.0 | 24.4 | 91.4 | 87.5 | 82.8 | 26.4 | 73.0 | 83.1 | 81.1 |\n| TALC | 98.6 | 18.0 | 89.0 | 45.3 | 57.3 | 19.5 | 44.4 | 62.5 | 58.9 |\n| Presto | 100.0 | 25.8 | 93.0 | 93.7 | 98.1 | 27.8 | 78.5 | 80.6 | 80.2 |", "caption": "Table 1: Quantitative results of dimension performance on VBench. A higher score indicates better performance in a particular dimension. We focus on the semantic dimension suite to demonstrate our Presto is capable of generating content-rich videos with consistency.", "description": "This table presents a quantitative evaluation of the Presto video generation model using the VBench benchmark.  VBench assesses various aspects of video generation quality across multiple dimensions. The table shows Presto's performance scores compared to other models (Gen-3, Allegro, and TALC) in different categories, including dynamic degree, temporal style, human action, object class, color consistency, and overall semantic and quality scores.  Higher scores indicate better performance within each dimension. The focus on the semantic score highlights Presto's ability to generate videos with rich and consistent content as indicated by the high score achieved by Presto.", "section": "5.2. Quantitative Evaluation"}, {"content": "| Dimensions | Overall Score |  |  | Scenario Diversity |  |  | Scenario Coherence |  |  | Text-Video Adherence |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Methods | Win | Lose | Tie | Win | Lose | Tie | Win | Lose | Tie | Win | Lose | Tie |\n| Gen-3 | **45.0** | 38.8 | 16.2 | **59.1** | 27.4 | 13.5 | 35.1 | 48.5 | 16.4 | **40.9** | 40.4 | 18.7 |\n| Allegro | **54.9** | 27.0 | 18.1 | **68.0** | 21.1 | 10.9 | **45.1** | 32.6 | 22.3 | **51.4** | 27.4 | 21.1 |\n| Merge Videos | **55.8** | 29.3 | 14.9 | **45.5** | 44.8 | 9.7 | **71.5** | 18.8 | 9.7 | **50.3** | 24.2 | 25.5 |\n| TALC | **91.8** | 3.1 | 5.1 | **90.6** | 4.1 | 5.3 | **95.3** | 1.8 | 2.9 | **89.5** | 3.5 | 7.0 |", "caption": "Table 2: Qualitative results of win rate (%) on user study. We ask users to evaluate two given videos based on three dimensions: Scenario Diversity, Scenario Coherence, and Text-Video Adherence. The Overall Score is calculated by considering all of the three dimensions.", "description": "This table presents the results of a user study comparing different video generation models.  Users were asked to compare pairs of videos generated by different methods and rate them across three dimensions: Scenario Diversity (how varied the scenes were), Scenario Coherence (how well the scenes flowed together), and Text-Video Adherence (how well the video matched the text prompt).  The 'Win Rate' represents the percentage of times a particular model's video was preferred over another model's video for each dimension. The 'Overall Score' is a combined score taking into account all three dimensions.", "section": "5.3. Qualitative Evaluation"}, {"content": "| Method | Overall Score | Dynamic Degree |\n|---|---|---|\n| O(verlap) SCA | 74.7 | 100.0 |\n| *Segmented Cross-Attention (SCA) Strategy* |  |  |\n| S(equential) SCA | 73.7 \u2193 | 100.0 - |\n| I(solated) SCA | 73.1 \u2193 | 100.0 - |\n| *LongTake-HD Dataset Curation* |  |  |\n| w/o Meticulous Filtering | 72.0 \u2193 | 97.2 \u2193 |\n| Single Long Condition | 71.8 \u2193 | 100.0 - |", "caption": "Table 3: Ablation results of the model design for different Segmented Cross-Attention Strategies, and LongTake-HD dataset curation. We report the performance of models with 360p resolution and 40 frames on VBench.", "description": "This table presents the ablation study results for the Presto model.  It shows the impact of different design choices on the model's performance, as measured by the VBench benchmark.  Specifically, it evaluates three variations of the Segmented Cross-Attention (SCA) strategy (Isolated, Sequential, and Overlap), and the effect of using the meticulously curated LongTake-HD dataset versus a less refined version.  All experiments used a 360p resolution and 40 frames for consistency.", "section": "5.4 Ablation Study"}, {"content": "| Filtering | Pre-training | Fine-tuning |\n|---|---|---|\n| Content-Diverse Video Clips |  |  |\n| Width | \u2265 1280 | \u2265 1280 |\n| Height | \u2265 720 | \u2265 720 |\n| FPS | [24,60] | [24,60] |\n| Duration | \u2265 15 | \u2265 15 |\n| Grayscale | [20,180] | [20,180] |\n| LAION Aesthetics | \u2265 4.8 | \u2265 5.0 |\n| Tolerance Artifacts | \u2264 5% | \u2264 5% |\n| Unimatch Flow | \u2265 40 | \u2265 50 |\n| Coherent Video Captions |  |  |\n| PSNR | [4,20] | [4,20] |\n| SSIM | [0,0.7] | [0,0.7] |\n| LPIPS | \u2265 0.4 | [0.5,0.8] |\n| Text Similarity | \u2264 0.75 | [0,0.75] |", "caption": "Table 4: Data filtering thresholds across various stages. All thresholds are manually determined by the specific characteristics of the dataset.", "description": "This table details the thresholds used at each stage of the LongTake-HD dataset filtering pipeline.  It shows the criteria applied to the raw videos (resolution, duration, etc.), and the filtering steps performed to refine the dataset for both pre-training and fine-tuning.  The filtering steps use metrics like PSNR, SSIM, LPIPS, and optical flow to ensure diversity and coherence in visual content and motion.  The table includes the specific ranges and values used for the various filters.  These manually determined thresholds are essential for creating a high-quality, curated dataset suitable for training video generation models.", "section": "3. LongTake-HD Dataset"}]