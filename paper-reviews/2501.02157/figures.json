[{"figure_path": "https://arxiv.org/html/2501.02157/x1.png", "caption": "Figure 1: Overview of the proposed personalized graph-based retrieval-augmented generation framework, PGraphRAG. We first construct user-centric graphs from user history and interactions. Then, the resulting structured data is utilized for retrieval. The retrieved information is provided to the language models for context in generating text tailored to user i\ud835\udc56iitalic_i.", "description": "This figure illustrates the architecture of the Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG) framework.  The framework begins by creating user-centric knowledge graphs from user data including documents, attributes, and interaction history. These graphs then serve as input for a retrieval model which identifies relevant information based on a given input prompt. This retrieved context is appended to the prompt before being sent to a language model. The language model generates personalized text output, which is then evaluated (E(\u0177, y)) to measure performance.  The figure visually represents the data flow and processing steps within the PGraphRAG framework.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.02157/extracted/6110123/graphics/amazon_profile_size_distribution.png", "caption": "Figure 2: The user profile distribution for Amazon user-product dataset which highlights how most users have a small profile size with few reviews. The red vertical line marks the minimum profile size in other benchmarks (e.g., LaMP, LongLaMP).", "description": "This figure shows the distribution of Amazon user reviews based on the number of reviews per user.  The vast majority of users have very few reviews (one or two), indicated by the steep curve on the left.  A red vertical line highlights the minimum number of reviews required to be included in the datasets of other personalization benchmarks (LaMP and LongLaMP). This demonstrates a key challenge in personalization: the scarcity of user data for many individuals.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.02157/extracted/6110123/graphics/model_ablate_full.png", "caption": "Figure 3: Comparison of GPT-4o-mini and GPT-o1 performance across all datasets and metrics for the long-text generation task.", "description": "This figure presents a comparative analysis of the performance of two large language models, GPT-40-mini and GPT-01, across various datasets and evaluation metrics specifically focused on the long-text generation task.  The bar chart visually compares the ROUGE-1, ROUGE-L, and METEOR scores achieved by each model on each dataset, providing a clear and concise overview of their relative strengths and weaknesses in this specific NLP application.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.02157/extracted/6110123/graphics/length_ablation.png", "caption": "Figure 4: Impact of length constraints of 3, 5, and 10 on short-text generation tasks using PGraphRAG, evaluated on the validation set.", "description": "This figure displays the results of an ablation study conducted to determine the optimal length constraint for short-text generation using the PGraphRAG model.  Three different length constraints (3, 5, and 10 words) were tested, and the performance was measured using ROUGE-1, ROUGE-L, and METEOR metrics. The results show the impact of these constraints on the model's ability to generate short text, which is important for personalized text generation. The evaluation was performed on the validation set.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.02157/extracted/6110123/graphics/TaskPrompt.png", "caption": "Figure 5: Examples of different prompt configurations used in each task type for PGraphRAG. Teletype text is replaced with realistic data for each task.", "description": "Figure 5 presents three example prompt configurations used in the Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG) framework.  The prompts are tailored for different task types within the PGraphRAG benchmark: long text generation (Tasks 1-4), short text generation (Tasks 5-8), and ordinal classification (Tasks 9-12). Each example illustrates how user reviews and neighboring user reviews are included in the prompt to provide context for the language model.  The bracketed placeholders ([...]) in the prompts indicate where actual user data is inserted during model training and evaluation.  The figure highlights how PGraphRAG leverages both a user's personal review history and the reviews of similar users to generate highly personalized outputs.", "section": "Experiments"}]