{"references": [{"fullname_first_author": "Alexander Kirillov", "paper_title": "Segment Anything", "publication_date": "2023-10-26", "reason": "This paper introduces the foundational Segment Anything Model (SAM), which the current paper builds upon and improves for video object segmentation."}, {"fullname_first_author": "Nikhila Ravi", "paper_title": "SAM 2: Segment Anything in Images and Videos", "publication_date": "2024-08-01", "reason": "This paper introduces SAM 2, a significant extension of SAM to video, directly addressing the limitations of SAM for video that the current paper aims to overcome."}, {"fullname_first_author": "Bowen Cheng", "paper_title": "XMem: Long-term video object segmentation with an atkinson-shiffrin memory model", "publication_date": "2022-10-26", "reason": "This paper presents XMem, a memory module used in SAM 2 and provides a basis for comparison to the efficient memory module developed in this work."}, {"fullname_first_author": "Yunyang Xiong", "paper_title": "EfficientSAM: Leveraged masked image pretraining for efficient segment anything", "publication_date": "2024-06-18", "reason": "This paper is another relevant work from the same research group, focusing on improving the efficiency of SAM, providing a related context to the current work's goals."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-01-01", "reason": "This paper is foundational in introducing Vision Transformers (ViTs), the backbone architecture used in the EfficientTAM model proposed by this paper."}]}