[{"figure_path": "https://arxiv.org/html/2411.18933/extracted/6028427/figures/intro.png", "caption": "Figure 1: Comparative analysis. (Left) Speed comparison between EfficientTAM and SAM 2 on a single NVIDIA A100 GPU. While SAM 2 is challenging for on-device deployment, our EfficientTAM can run 261 ms per frame on iPhone 15 Pro Max. (Right)\nFPS/Parameter/Performance comparison of EfficientTAM, SAM 2, and other efficient models for zero-shot video object segmentation on SA-V test. We benchmark FPS (frames per second) of all models with 1024 \u00d7 1024 input resolution on a single NVIDIA A100.", "description": "This figure presents a comparative analysis of EfficientTAM and SAM 2, focusing on speed and efficiency in video object segmentation. The left panel shows a speed comparison on a single NVIDIA A100 GPU, highlighting EfficientTAM's ability to process frames much faster than SAM 2, enabling on-device deployment (261ms per frame on an iPhone 15 Pro Max). The right panel provides a comprehensive comparison of EfficientTAM, SAM 2, and other efficient models on the SA-V test dataset.  It compares frames per second (FPS), model parameters, and performance metrics, demonstrating EfficientTAM's competitive performance and efficiency advantages.", "section": "Comparative analysis"}, {"figure_path": "https://arxiv.org/html/2411.18933/x1.png", "caption": "Figure 2: EfficientTAM architecture. Our proposed EfficientTAM takes a vanilla lightweight ViT image encoder for frame feature extraction. An efficient memory cross-attention is proposed to further improve the efficiency of EfficientTAM by leveraging the strong locality of memory spatial embeddings. EfficientTAM is fully trained on SA-1B (image) and SA-V (video) for unified image and video segmentation.", "description": "This figure illustrates the architecture of the EfficientTAM model, a lightweight model for video object segmentation and tracking.  It uses a vanilla Vision Transformer (ViT) as its image encoder, replacing the more complex multistage encoder used in previous models like SAM 2. This simplification significantly reduces computational cost.  The model also incorporates an efficient memory cross-attention mechanism that leverages the spatial locality of memory tokens to further enhance speed.  Crucially, the EfficientTAM is trained on both the SA-1B image dataset and the SA-V video dataset, enabling it to perform unified image and video segmentation.", "section": "3.2 Efficient Video Object Segmentation and Track Anything"}]