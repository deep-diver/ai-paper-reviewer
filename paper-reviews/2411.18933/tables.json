[{"content": "| Method | \\mathcal{J} & \\mathcal{F} | \\mathcal{G} | Parameters (M) | FPS | Latency (ms) |\n|---|---|---|---|---|---|---|\n| STCN [Cheng et al., 2021b] | 52.5 | 85.4 | - | 57.3 | 82.7 | 54 |\n| RDE [Li et al., 2022b] | 46.8 | 84.2 | - | 48.4 | 81.9 | 64 |\n| XMem [Cheng and Schwing, 2022] | 59.6 | 86.0 | - | 60.1 | 85.6 | 62 |\n| DEVA [Cheng et al., 2023a] | 66.0 | 87.0 | 55.9 | 53.8 | 85.4 | 69 |\n| Cutie-base [Cheng et al., 2024] | 69.9 | 87.9 | 66.0 | 61.6 | 87.0 | 35 |\n| Cutie-base+ [Cheng et al., 2024] | 71.7 | 88.1 | - | 62.3 | 87.5 | 35 |\n| SAM 2 [Ravi et al., 2024] | 72.8 | 88.9 | 76.2 | 74.7 | 87.9 | 81 |\n| EfficientTAM-Ti/2 (ours) | 68.4 | 88.4 | 66.1 | 70.8 | 87.1 | 18 |\n| EfficientTAM-Ti (ours) | 69.3 | 89.1 | 69.6 | 70.7 | 86.7 | 18 |\n| EfficientTAM-S/2 (ours) | 70.8 | 88.6 | 72.1 | 74.0 | 87.2 | 34 |\n| EfficientTAM-S (ours) | 71.4 | 89.2 | 73.4 | 74.5 | 87.2 | 34 |", "caption": "Table 1: Standard semi-supervised video object segmentation results across video object segmentation benchmarks.", "description": "Table 1 presents a comparison of different video object segmentation models on several standard benchmarks.  It shows the performance of various models, including the proposed EfficientTAMs and existing state-of-the-art methods, on metrics such as J&F (a combined measure of region similarity and contour accuracy), across datasets like MOSE, DAVIS, LVOS, SA-V, and YTVOS. The table also provides information on the number of model parameters and the speed (FPS) of each model on both a high-end GPU (A100) and a mobile device (iPhone 15 Pro Max).  This allows for a comprehensive comparison of accuracy and efficiency across models.", "section": "4.2 Main Results"}, {"content": "| MOSE |\n|---|---| \n| val |", "caption": "Table 2: Interactive semi-supervised video object segmentation results with different prompts. We report averaged \ud835\udca5\ud835\udca5\\mathcal{J}caligraphic_J&\u2131\u2131\\mathcal{F}caligraphic_F zero-shot accuracy across 17 video datasets for each type of prompt.", "description": "Table 2 presents a quantitative comparison of interactive semi-supervised video object segmentation performance across various prompting methods.  Seventeen video datasets were used for evaluation. The metric used is the average of J&F (a combined measure of region similarity (J) and contour accuracy (F)) across all datasets for each prompt type (1-click, 3-click, 5-click, bounding box, and ground truth mask). This allows assessing the model's ability to accurately segment objects with different levels of user interaction and feedback.", "section": "4.2 Main Results"}, {"content": "| DAVIS |\n|---|---| \n| 2017 val |", "caption": "Table 3: Segment anything results on SA-23 benchmark\u00a0(Kirillov et\u00a0al., 2023) and 14 new video benchmark\u00a0(Ravi et\u00a0al., 2024). The average 1-click (5-click) mIoU is reported.", "description": "Table 3 presents a comparison of different models' performance on the task of segmenting objects within images.  The models are evaluated using two benchmarks: the SA-23 dataset (Kirillov et al., 2023), which contains a large collection of diverse images, and a new video benchmark (Ravi et al., 2024) consisting of 14 video clips.  The evaluation metric is mean Intersection over Union (mIoU), a common measure of segmentation accuracy.  Results are provided for both 1-click and 5-click scenarios, indicating how well the models segment objects given a single click or five clicks as input. This allows for assessment of both speed and accuracy, reflecting model efficiency and robustness. The table provides a quantitative analysis of the models' zero-shot object segmentation capabilities.", "section": "4.2 Main Results"}, {"content": "| LVOS | val |\n|---|---|", "caption": "Table 4: Efficient cross-attention variants.", "description": "This table presents a comparison of different cross-attention variants used in the EfficientTAM model.  It shows the performance (measured by J&F scores on the MOSE dev, DAVIS 2017 val, and SA-V test datasets) of different cross-attention methods, allowing for a direct assessment of their relative effectiveness in the context of video object segmentation.", "section": "4 Experiments"}, {"content": "| SA-V |\n|---|---| \n| test |", "caption": "Table 5: Ablation study on the effect of input resolution.", "description": "This ablation study investigates the impact of input image resolution on the performance of EfficientTAM.  It compares the model's performance (measured by J&F scores on MOSE dev, DAVIS 2017 val, and SA-V test sets) and efficiency (frames per second (FPS) on an A100 GPU and latency in milliseconds on an iPhone 15) using two different input resolutions: 1024x1024 and 512x512. The results demonstrate the trade-off between accuracy and speed, showing how lower resolution improves efficiency, particularly on mobile devices, at the cost of slightly lower accuracy.", "section": "4.2 Main Results"}, {"content": "| YTVOS |\n|---|---| \n| 2019 val |", "caption": "Table 6: Ablation study on the design of memory cross-attention in EfficientTAM.", "description": "This table presents the results of an ablation study investigating the impact of different design choices within the memory cross-attention mechanism of the EfficientTAM model.  Specifically, it shows how the performance of EfficientTAM changes when using or not using object pointer tokens in the memory cross-attention module.  The study evaluates performance metrics across three different datasets (MOSE dev, DAVIS 2017 val, SA-V test), offering a comprehensive view of the effect of this design choice on various video object segmentation tasks.", "section": "4.3 Ablation Studies"}, {"content": "| Method | 1-click | 3-click | 5-click | bounding box | ground-truth mask |\n|---|---|---|---|---|---| \n| SAM+XMem++ | 56.9 | 68.4 | 70.6 | 67.6 | 72.7 |\n| SAM+Cutie | 56.7 | 70.1 | 72.2 | 69.4 | 74.1 |\n| SAM 2 | 64.3 | 73.2 | 75.4 | 72.9 | 77.6 |\n| EfficientTAM-S/2 | 60.5 | 72.8 | 75.4 | 71.2 | 76.8 |\n| EfficientTAM-S | 63 | 74.1 | 75.7 | 73.2 | 77.8 |", "caption": "Table 7: Ablation study on taking care of the memory token structure for efficient cross-attention in EfficientTAM.", "description": "This table presents the results of an ablation study focusing on the efficiency of cross-attention within the EfficientTAM model's memory module.  It compares the performance of using only spatial memory tokens versus using the full set of memory tokens (spatial and object pointer tokens) within the efficient cross-attention mechanism. This helps to understand the impact of considering the underlying structure of memory tokens on the overall efficiency and accuracy of the model.", "section": "4.3 Ablation Studies"}, {"content": "| Model | SA-23 All | SA-23 Image | SA-23 Video | 14 new Video |\n|---|---|---|---|---|\n| SAM (ViT-B) | 55.9 (80.9) | 57.4 (81.3) | 54.0 (80.4) | 54.5 (82.6) |\n| SAM (ViT-H) | 58.1 (81.3) | 60.8 (82.1) | 54.5 (80.3) | 59.1 (83.4) |\n| HQ-SAM (ViT-B) | 53.9 (72.1) | 56.3 (73.9) | 50.7 (69.9) | 54.5 (75.0) |\n| HQ-SAM (ViT-H) | 59.1 (79.8) | 61.8 (80.5) | 55.7 (78.9) | 58.9 (81.6) |\n| SAM 2 | 61.9 (83.6) | 63.2 (83.8) | 60.3 (83.3) | 69.9 (85.9) |\n| EfficientTAM-Ti/2 (ours) | 58.6 (82.5) | 59.6 (82.8) | 57.4 (82.1) | 63.4 (84.9) |\n| EfficientTAM-Ti (ours) | 58.2 (82.6) | 59.5 (82.9) | 56.5 (82.1) | 62.7 (85.0) |\n| EfficientTAM-S/2 (ours) | 60.5 (82.9) | 61.6 (83.2) | 59.1 (82.4) | 67.8 (85.4) |\n| EfficientTAM-S (ours) | 60.7 (83.0) | 61.7 (83.3) | 59.5 (82.6) | 67.7 (85.4) |", "caption": "Table 8: Comparing with local windowed attention.", "description": "This table presents a comparison of the performance of different cross-attention methods in the EfficientTAM model. Specifically, it compares the performance of the proposed efficient cross-attention method with local windowed attention, across various metrics for video object segmentation.  The metrics used evaluate performance on the MOSE dev, DAVIS 2017 val, and SA-V test datasets.", "section": "4.3 Ablation Studies"}]