[{"figure_path": "https://arxiv.org/html/2501.05727/x1.png", "caption": "Figure 1: Performance comparison between Qwen2.5-72B-Instruct (base model), +SCRIT (self-evolved model) across two complementary evaluation protocols to assess different aspects of critique capabilities.", "description": "This figure displays a performance comparison between two models: the base model Qwen2.5-72B-Instruct and the self-evolved model (+SCRIT).  The comparison uses two different evaluation protocols to fully assess the critique capabilities.  The first protocol evaluates performance on three types of solutions: deliberately incorrect solutions, balanced solutions, and solutions generated by the base model itself. The second protocol is focused on error identification across different datasets.  The bar chart visually represents the accuracy and F1-scores for each model on both protocols, highlighting the improvements achieved by incorporating SCRIT.", "section": "4.1. Statistics of SCRIT"}, {"figure_path": "https://arxiv.org/html/2501.05727/x2.png", "caption": "Figure 2: Comparison between Direct Critic and Contrastive Critic. Direct Critic exhibits rubber-stamping behavior by blindly approving the incorrect solution and providing misled correction. Contrastive Critic analyzes the reference solution to understand key concepts and solving strategies, enabling error identification and effective correction.", "description": "This figure compares two methods for model critique: Direct Critic and Contrastive Critic.  The Direct Critic, given a student's solution and without a reference solution, simply approves or disapproves without detailed explanation or correction.  The Contrastive Critic, however, is provided with both the student's solution and a correct reference solution. This allows it to identify key concepts and solution strategies, leading to a more accurate critique and more effective error correction.  The example shows how the Direct Critic rubber-stamps an incorrect solution, while the Contrastive Critic correctly identifies and corrects errors.", "section": "3. SCRIT: Self-Evolving Critic"}, {"figure_path": "https://arxiv.org/html/2501.05727/x3.png", "caption": "Figure 3: Overview of SCRIT framework.", "description": "The SCRIT framework is depicted in this flowchart. It shows how the model self-evolves through a continuous cycle. First, it takes in mathematical problems and different solutions to those problems. Then it analyzes reference solutions to generate critiques. The quality of the critiques is checked through a self-validation mechanism. Finally, the validated critiques are used to improve the model, demonstrating self-evolution without any outside help.", "section": "3. SCRIT: Self-Evolving Critic"}, {"figure_path": "https://arxiv.org/html/2501.05727/x4.png", "caption": "Figure 4: Data flow statistics and validation rates before and after self-critic and self-validation filtering across three dimensions: domain complexity, problem difficulty, and solution generation models.", "description": "This figure visualizes the data flow and validation rates throughout the SCRIT framework. It showcases how the initial dataset of problem-solution pairs is filtered using self-critic and self-validation mechanisms, resulting in a reduced yet higher-quality dataset. This filtering process is analyzed across three dimensions: domain complexity (ranging from simpler problems to complex ones), problem difficulty (measured by the number of unique solutions), and solution generation models (showing various LLMs involved in generating the solutions). The figure uses bar charts to represent the quantity of problem-solution pairs at each stage of the pipeline and displays the validation rates (percentage of pairs successfully passing each filtering stage) for different levels of complexity, difficulty, and models, providing insights into how these factors influence the quality of the data used for training the self-evolving critic.", "section": "4.1. Statistics of SCRIT"}, {"figure_path": "https://arxiv.org/html/2501.05727/x5.png", "caption": "Figure 5: Scaling behavior of SCRIT across data size and comparison of critic mechanisms. We compare three critic mechanisms: Contrastive Critic, Direct Critic, and Bug-Injection Critic.", "description": "This figure illustrates the impact of training data size on the performance of SCRIT, comparing it against two other critic mechanisms: Direct Critic and Bug-Injection Critic.  The x-axis represents the size of the training dataset (log scale), while the y-axis shows the performance metrics: Critique-Correction Accuracy (CC-Acc) and Error Identification F1-score (EI-F1).  The figure shows that Contrastive Critic consistently outperforms both Direct Critic and Bug-Injection Critic across various training data sizes.  The curves also demonstrate the scaling behavior of the models - how performance increases with more training data.  Specifically, Contrastive Critic shows sustained improvement with increasing data size, while the other methods show slower growth or even plateau.", "section": "5. Analysis"}, {"figure_path": "https://arxiv.org/html/2501.05727/x6.png", "caption": "Figure 6: Scaling behavior of SCRIT across model sizes from Qwen2.5 1.5B to 72B parameters.", "description": "This figure presents the scaling behavior of the Self-evolving CRITic (SCRIT) framework across different model sizes, specifically using the Qwen2.5 model with 1.5B, 7B, and 72B parameters. It showcases the performance improvement of SCRIT in terms of critique-correction accuracy (CC-Acc) and error identification F1-score (EI-F1) as the model size increases. This demonstrates the impact of model size on SCRIT's ability to perform effective critique and correction.", "section": "5.1. Scaling Behavior of SCRIT"}, {"figure_path": "https://arxiv.org/html/2501.05727/x19.png", "caption": "Figure 7: System prompts used for different critic mechanisms. Top Left: Direct Critic directly analyzes solution correctness without any additional context. Bottom Left: Bug-Injection Critic first injects bugs (Step 1) then direct critic on bug-injected solution (Step 2). Right: Contrastive Critic first analyzes a reference solution to understand key mathematical concepts before conducting step-wise critique.", "description": "Figure 7 illustrates the different prompting strategies used for three critique generation methods: Direct Critic, Bug-Injection Critic, and Contrastive Critic.  The Direct Critic prompt directly asks the model to critique a student solution without providing any context or reference solution.  The Bug-Injection Critic prompt is a two-stage process where the model first introduces an artificial error into a correct solution (Step 1) and then critiques the now-incorrect solution (Step 2).  Finally, the Contrastive Critic prompt provides both a student solution and a reference solution; this allows the model to first understand the core concepts and solving strategies in the reference solution before critiquing the student solution.", "section": "3. SCRIT: Self-Evolving Critic"}]