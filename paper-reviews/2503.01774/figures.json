[{"figure_path": "https://arxiv.org/html/2503.01774/x3.png", "caption": "Figure 1: We demonstrate Difix3D+ on both in-the-wild scenes (top) and driving scenes (bottom). Recent Novel-View Synthesis methods struggle in sparse-input settings or when rendering views far from the input camera poses. Difix distills the priors of 2D generative models to enhance reconstruction quality and can further act as a neural-renderer at inference time to mitigate the remaining inconsistencies. Notably, the same model effectively corrects NeRF\u00a0[37] and 3DGS\u00a0[20] artifacts.", "description": "Figure 1 showcases the performance of Difix3D+ on various scenes. The top row presents in-the-wild scenes, while the bottom row displays driving scenes.  It highlights how Difix3D+ addresses challenges faced by other novel-view synthesis methods that often struggle with sparse input data or when generating views significantly different from the input camera viewpoints.  Difix3D+ leverages 2D generative models to improve the quality of 3D reconstructions.  The model acts as a neural renderer during inference, correcting any inconsistencies. The effectiveness of Difix3D+ is particularly emphasized by its ability to rectify artifacts present in established methods like NeRF and 3DGS.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.01774/x5.png", "caption": "Figure 2: Difix3D+ pipeline. The overall pipeline of the Difix3D+ model involves the following stages: Step 1: Given a pretrained 3D representation, we render novel views and feed them to Difix which acts as a neural enhancer, removing the artifacts and improving the quality of the noisy rendered views (Sec.\u00a04.1). The camera poses selected to render the novel views are obtained through pose interpolation, gradually approaching the target poses from the reference ones. Step 2: The cleaned novel views are distilled back to the 3D representation to improve its quality (Sec.\u00a04.2). Steps 1 and 2 are applied in several iterations to progressively grow the spatial extent of the reconstruction and hence ensure strong conditioning of the diffusion model (Difix3D). Step 3: Difix additional acts as a real-time neural enhancer, further improving the quality of the rendered novel views.", "description": "The figure illustrates the DIFIX3D+ pipeline, a three-step process for enhancing 3D reconstruction and novel view synthesis.  Step 1 involves rendering novel views from a pre-trained 3D model and using DIFIX (a single-step diffusion model) to remove artifacts and improve their quality.  The camera poses for these novel views are gradually interpolated from reference poses towards target poses. In Step 2, these enhanced views are used to refine the 3D representation. Steps 1 and 2 are iterated to progressively expand the reconstruction, improving the diffusion model's conditioning. Finally, in Step 3, DIFIX is used as a real-time neural enhancer to further improve the quality of rendered novel views.", "section": "4. Boosting 3D Reconstruction with DM priors"}, {"figure_path": "https://arxiv.org/html/2503.01774/x6.png", "caption": "Figure 3: Difix architecture. Difix takes a noisy rendered image and a reference views as input (left), and outputs an enhanced version of the input image with reduced artifacts (right). Difix also generates identical reference views, which we discard in practice and hence depict transparent. The model architecture consists of a U-Net structure with a cross-view reference mixing layer (Sec.\u00a04.1) to maintain consistency across reference views. Difix is fine-tuned from SD-Turbo, using a frozen VAE encoder and a LoRA fine-tuned decoder.", "description": "The DIFIX architecture is a U-Net based model that takes a noisy rendered image and reference views as input and outputs an enhanced version of the input image with reduced artifacts.  The model incorporates a cross-view reference mixing layer to maintain consistency between the input image and reference views.  This is achieved by using a frozen VAE encoder and a LoRA fine-tuned decoder, fine-tuned from the SD-Turbo model. Identical reference views are generated but discarded during practice.", "section": "4.1 DIFIX: From a pretrained diffusion model to a 3D Artifact Fixer"}, {"figure_path": "https://arxiv.org/html/2503.01774/x7.png", "caption": "Figure 4: Noise level. To validate our hypothesis that the distribution of images with NeRF/3DGS artifacts is similar to the distribution of noisy images used to train SD-Turbo\u00a0[49], we perform single-step \u201cdenoising\u201d at varying noise levels. At higher noise levels (e.g., \u03c4=600\ud835\udf0f600\\tau=600italic_\u03c4 = 600), the model effectively removes artifacts but also alters the image context. At lower noise levels (e.g., \u03c4=10\ud835\udf0f10\\tau=10italic_\u03c4 = 10), the model makes only minor adjustments, leaving most artifacts intact. \u03c4=200\ud835\udf0f200\\tau=200italic_\u03c4 = 200 strikes a good balance, removing artifacts while preserving context, and achieves the highest metrics.", "description": "This figure shows an ablation study on the effect of different noise levels on a single-step denoising diffusion model. The model was fine-tuned on images with artifacts from NeRF and 3DGS. The experiment shows that at high noise levels (\u03c4 = 600), the model effectively removes artifacts but modifies image context.  At low noise levels (\u03c4 = 10), the model makes minor adjustments and leaves most artifacts intact. The best results were obtained at \u03c4 = 200, where the model removes artifacts while preserving image context.", "section": "4.1 DIFIX: From a pretrained diffusion model to a 3D Artifact Fixer"}, {"figure_path": "https://arxiv.org/html/2503.01774/x8.png", "caption": "Figure 5: In-the-wild artifact removal. We show comparisons on held-out scenes from the DL3DV dataset\u00a0[23] (top, above the dashed line) and the Nerfbusters\u00a0[70] dataset (bottom). Difix3D+ corrects significantly more artifacts that other methods.", "description": "This figure demonstrates the performance of DIFIX3D+ on real-world scenes with challenging conditions.  The top half shows comparisons on held-out scenes from the DL3DV dataset, while the bottom half uses scenes from the Nerfbusters dataset. Both datasets contain examples where existing novel view synthesis methods struggle. Each row shows the ground truth image (GT) followed by results from several state-of-the-art methods (Nerfbusters, GANERF, NeRFLIX, Nerfacto) and finally, the results obtained using the proposed DIFIX3D+ method. The visual comparison highlights DIFIX3D+'s superior ability to remove artifacts and generate more photorealistic novel views.", "section": "5.1. In-the-Wild Artifact Removal"}, {"figure_path": "https://arxiv.org/html/2503.01774/x9.png", "caption": "Figure 6: Qualitative results on the RDS dataset. Difix for RDS was trained on 40 scenes and 100,000 paired data samples.", "description": "Figure 6 presents a qualitative comparison of 3D reconstruction results on the Real Driving Scenes (RDS) dataset.  The model, DIFIX, was trained using 40 distinct scenes and 100,000 image pairs, which were carefully curated to include various common artifacts in 3D reconstruction. The figure showcases the impact of DIFIX on improving the quality of novel view synthesis.  It visually demonstrates the differences in image quality and realism achieved by several models, emphasizing the enhancements provided by the DIFIX method.", "section": "5.2. Automotive Scene Enhancement"}, {"figure_path": "https://arxiv.org/html/2503.01774/x10.png", "caption": "Figure 7: Qualitative ablation of real-time post-render processing: Difix3D+ uses an additional neural enhancer step that effectively removes residual artifacts, resulting in higher PSNR and lower LPIPS scores. The images displayed in green or red boxes correspond to zoomed-in views of the bounding boxes drawn in the main images.", "description": "This figure shows the results of real-time post-render processing using DIFIX3D+.  The pipeline includes an additional neural enhancement step after the initial 3D reconstruction and rendering. This step effectively removes residual artifacts that remain after the main reconstruction process.  The effectiveness of this additional step is demonstrated by comparing the PSNR (Peak Signal-to-Noise Ratio) and LPIPS (Learned Perceptual Image Patch Similarity) scores of the processed images to the original rendered images. Higher PSNR and lower LPIPS values in the processed images indicate improved image quality due to reduced artifacts. The green and red boxes highlight zoomed-in sections of the images, allowing for a more detailed comparison of the artifact removal.", "section": "4.2. DIFIX3D+: NVS with Diffusion Priors"}]