[{"figure_path": "2410.18533/tables/table_7_0.html", "caption": "Table 1: Evaluation results on LongBench benchmark, where \u2020 denotes training-free method.", "description": "Table 1 presents the quantitative results of different long-context models on the LongBench benchmark, comparing their performance across six tasks and highlighting the impact of different training strategies.", "section": "4.2 PERFORMANCE ON LONG-CONTEXT TASKS"}, {"figure_path": "2410.18533/tables/table_13_0.html", "caption": "Table 1: Evaluation results on LongBench benchmark, where \u2020 denotes training-free method.", "description": "Table 1 presents the quantitative results of different long-context models evaluated on the LongBench benchmark across six categories of tasks, including single-document QA, multi-document QA, summarization, few-shot learning, synthetic tasks, and code generation.", "section": "4.2 PERFORMANCE ON LONG-CONTEXT TASKS"}, {"figure_path": "2410.18533/tables/table_20_0.html", "caption": "Table 1: Evaluation results on LongBench benchmark, where \u2020 denotes training-free method.", "description": "Table 1 presents the quantitative results of different long-context models on the LongBench benchmark across various tasks, comparing their performance with and without the LOGO training strategy.", "section": "4.2 PERFORMANCE ON LONG-CONTEXT TASKS"}, {"figure_path": "2410.18533/tables/table_20_1.html", "caption": "Table 1: Evaluation results on LongBench benchmark, where \u2020 denotes training-free method.", "description": "Table 1 presents a comparison of different models' performance on the LongBench benchmark, including several long-context models and short-context models with different context window scaling methods applied.", "section": "4.2 PERFORMANCE ON LONG-CONTEXT TASKS"}]