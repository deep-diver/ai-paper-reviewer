[{"figure_path": "2410.18533/tables/table_7_0.html", "caption": "Table 1: Evaluation results on LongBench benchmark, where \u2020 denotes training-free method.", "description": "The table presents the quantitative results of different long-context models on various tasks within the LongBench benchmark, comparing the performance of LOGO with other methods.", "section": "4.2 PERFORMANCE ON LONG-CONTEXT TASKS"}, {"figure_path": "2410.18533/tables/table_13_0.html", "caption": "Table 1: Evaluation results on LongBench benchmark, where \u2020 denotes training-free method.", "description": "Table 1 presents the quantitative results of different models on six categories of tasks in the LongBench benchmark, comparing the performance of LOGO with other methods.", "section": "4.2 PERFORMANCE ON LONG-CONTEXT TASKS"}, {"figure_path": "2410.18533/tables/table_20_0.html", "caption": "Table 1: Evaluation results on LongBench benchmark, where \u2020 denotes training-free method.", "description": "Table 1 presents the average scores of different LLMs on six categories of tasks in the LongBench benchmark, comparing the performance of several LLMs with different context scaling and alignment methods.", "section": "4.2 PERFORMANCE ON LONG-CONTEXT TASKS"}, {"figure_path": "2410.18533/tables/table_20_1.html", "caption": "Table 1: Evaluation results on LongBench benchmark, where \u2020 denotes training-free method.", "description": "Table 1 presents the average scores of different language models on six categories of tasks from the LongBench benchmark, comparing the performance of various context scaling and alignment methods.", "section": "4.2 PERFORMANCE ON LONG-CONTEXT TASKS"}]