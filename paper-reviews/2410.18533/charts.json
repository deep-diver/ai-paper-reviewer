[{"figure_path": "2410.18533/charts/charts_1_0.png", "caption": "Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM.", "description": "The chart displays the performance of various Long-Context Models (LCMs) across multiple metrics and datasets.  Panel (a) presents a bar chart showing the performance of different LCMs on several real-world long-context tasks, comparing their scores. Panel (b) shows a bar chart comparing the retrieval score (reflecting understanding of long contexts) and recall score (reflecting generation ability) of LCMs on a synthetic retrieval task,  multi-value NIAH. Lastly, panel (c) is a scatter plot that correlates the performance of the models with the size of their long-context (pre-)training data, showcasing how data size impacts their performance.", "section": "ABSTRACT"}, {"figure_path": "2410.18533/charts/charts_1_1.png", "caption": "Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM.", "description": "This chart displays a comparison of various Long Context Models (LCMs) across three key aspects.  Subfigure (a) presents a bar chart showing the performance of different LCMs on real-world long-context tasks, comparing models based on Llama-2, Llama-3, and closed-source models like GPT-4. Subfigure (b) uses a bar chart to illustrate the retrieval and recall scores of the models on a synthetic retrieval task, highlighting the models' abilities in understanding long contexts and generating relevant outputs.  Finally, subfigure (c) is a scatter plot that visually represents the relationship between the performance of different LCMs and the amount of training data used. The size of training data is color-coded to distinguish the models and their origins.", "section": "Introduction"}, {"figure_path": "2410.18533/charts/charts_1_2.png", "caption": "Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM.", "description": "This figure consists of three subfigures. Subfigure (a) presents a bar chart comparing the performance of various long-context models (LCMs) on real-world tasks, showing their scores. Subfigure (b) displays a bar chart illustrating the retrieval and recall scores of different LCMs on a synthetic retrieval task, indicating their understanding and generation capabilities. Subfigure (c) is a scatter plot exhibiting the relationship between the performance of LCMs and their long-context training data size, suggesting a correlation between model performance and data size.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18533/charts/charts_8_0.png", "caption": "Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM.", "description": "This figure presents a comparative analysis of Long Context Models (LCMs) across three subplots.  The first subplot (a) shows the performance of various LCMs on real-world long-context tasks, illustrating their relative strengths and weaknesses. The second subplot (b) focuses on a specific synthetic retrieval task (multi-value NIAH), comparing the retrieval score (reflecting the model's ability to understand long contexts) and recall score (representing the model's ability to generate accurate responses) for each LCM. The third subplot (c) displays the long-context (pre-)training data size used for each model, providing context on the resources invested in their development.  This allows for an overall comparison of model performance, contextual understanding, response accuracy, and the relationship between training data size and model capability.", "section": "Introduction"}, {"figure_path": "2410.18533/charts/charts_8_1.png", "caption": "Figure 4: Evaluation results of language modeling task. The solid and dashed curves represent the PPL of the baselines and LOGO, respectively.", "description": "The chart displays the perplexity (PPL) scores for various language models, comparing their performance with and without the LOGO training strategy across different context window sizes (2K, 4K, 8K, 16K, 32K, and 64K).  It shows the perplexity scores for several base large language models (LLMs) like Llama-3-8B-Instruct-80K, Llama-2-7B-Instruct-80K, Mistral-Instruct-32K-V0.2-32K, and Llama-3-8B-Instruct-8K. For each model, two lines are plotted: one for the baseline model and one after LOGO training.  The chart visually demonstrates how LOGO impacts the PPL score at varying context lengths, highlighting PPL explosion for certain models at larger context sizes.", "section": "4.3 PERFORMANCE ON SHORT-CONTEXT TASKS"}, {"figure_path": "2410.18533/charts/charts_9_0.png", "caption": "Figure 5: Model performance on short-context tasks, including MMLU, TruthfulQA, and ARC.", "description": "This bar chart displays the performance comparison of several Language Models (LMs) on three short-context tasks: MMLU, TruthfulQA, and ARC (Average).  The chart compares the baseline performance of the LMs with their performance after undergoing either long-context scaling (using the LOGO method) or long-context alignment (using instruction tuning).  For each LM and each task, two bars are shown: one representing the baseline performance and the other showing the performance after the respective training method. The results illustrate the impact of both long-context scaling and alignment on the models' ability to perform well on short-context tasks, highlighting any potential trade-offs between enhanced long-context capabilities and performance on traditional benchmarks.", "section": "4.3 PERFORMANCE ON SHORT-CONTEXT TASKS"}, {"figure_path": "2410.18533/charts/charts_9_1.png", "caption": "Figure 6: Ablation study results. (a) Comparison among different settings on the language modeling task (PPL) and real-world tasks (Avg. score on LongBench testing set); (b) Reward difference distribution under different M settings; (c) Training GPU memory consumption of different settings.", "description": "This figure presents the ablation study results of LOGO. Subfigure (a) shows the language modeling performance (PPL) and real-world task performance (LongBench average score) under various settings of M (number of dis-preference instances), \u03bb (SFT regularization term), and context length (Ctx.).  The results reveal that increasing M generally improves real-world performance but can negatively impact language modeling.  Increasing \u03bb consistently lowers PPL.  Subfigure (b) displays the distribution of reward differences (r(x,yw)\u2212\u2211r(x,yj)) for different M values, demonstrating that larger M values lead to a wider separation between preferred and dis-preferred responses. Finally, subfigure (c) illustrates the GPU memory consumption across various settings, highlighting the trade-off between model performance and resource requirements.", "section": "5 ABLATION STUDY"}, {"figure_path": "2410.18533/charts/charts_9_2.png", "caption": "Figure 6: Ablation study results. (a) Comparison among different settings on the language modeling task (PPL) and real-world tasks (Avg. score on LongBench testing set); (b) Reward difference distribution under different M settings; (c) Training GPU memory consumption of different settings.", "description": "This figure presents the results of an ablation study conducted to analyze the impact of various hyperparameters on the LOGO model's performance.  Subfigure (a) shows a comparison of the model's performance on both language modeling (measured by perplexity, PPL) and real-world tasks (average score on the LongBench benchmark) across different settings of hyperparameters M (number of dis-preference instances), \u03bb (SFT regularization term), and context length (Ctx).  Subfigure (b) displays the distribution of reward differences (r(x, yw) \u2212 \u03a3Mi=1r(x, yi)) for various values of M, illustrating how increasing M leads to a wider separation between preference and dis-preference rewards. Finally, subfigure (c) shows the GPU memory consumption for each setting, highlighting the trade-off between model performance and resource requirements.", "section": "5 ABLATION STUDY"}, {"figure_path": "2410.18533/charts/charts_10_0.png", "caption": "Figure 7: Comparison between SFT and LOGO training strategies on the synthetic retrieval task.", "description": "This chart compares the performance of two training strategies, SFT (Supervised Fine-Tuning) and LOGO (Long context aliGnment via efficient preference Optimization), on a synthetic retrieval task. It displays the recall score (representing generation ability) and retrieval score (representing long-context understanding) over training steps for both methods. Three sub-charts present the results for (a) context loss + prediction loss, (b) prediction loss, and (c) LOGO loss.  The chart shows that LOGO outperforms SFT in terms of recall and retrieval score, particularly evident in (c) where LOGO's loss decreases more consistently, indicating better performance and alignment.", "section": "5.3 Comparison Between SFT and LOGO"}]