[{"figure_path": "2410.18533/charts/charts_1_0.png", "caption": "Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM.", "description": "The chart displays the performance of various Long-Context Models (LCMs) across real-world and synthetic long-context tasks, showing retrieval and recall scores, and relating performance to training data size.", "section": "ABSTRACT"}, {"figure_path": "2410.18533/charts/charts_1_1.png", "caption": "Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM.", "description": "The chart compares the performance of various long-context models (LCMs) on real-world and synthetic tasks, showing retrieval and recall scores, and training data size.", "section": "Introduction"}, {"figure_path": "2410.18533/charts/charts_1_2.png", "caption": "Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM.", "description": "The chart displays the performance of various long-context models (LCMs) on real-world and synthetic tasks, showing their retrieval and recall scores in relation to their training data size.", "section": "Introduction"}, {"figure_path": "2410.18533/charts/charts_8_0.png", "caption": "Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM.", "description": "The chart displays the performance of various long-context models (LCMs) on real-world tasks, their retrieval and recall scores on a synthetic task, and their training data sizes.", "section": "Introduction"}, {"figure_path": "2410.18533/charts/charts_8_1.png", "caption": "Figure 4: Evaluation results of language modeling task. The solid and dashed curves represent the PPL of the baselines and LOGO, respectively.", "description": "The chart displays the perplexity (PPL) scores of several large language models (LLMs) with and without LOGO training across various context lengths, illustrating LOGO's impact on language modeling performance.", "section": "4.3 PERFORMANCE ON SHORT-CONTEXT TASKS"}, {"figure_path": "2410.18533/charts/charts_9_0.png", "caption": "Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM.", "description": "The chart compares the performance of various long-context models (LCMs) on real-world and synthetic tasks, showing their retrieval and recall scores, and the amount of training data used.", "section": "INTRODUCTION"}, {"figure_path": "2410.18533/charts/charts_9_1.png", "caption": "Figure 6: Ablation study results. (a) Comparison among different settings on the language modeling task (PPL) and real-world tasks (Avg. score on LongBench testing set); (b) Reward difference distribution under different M settings; (c) Training GPU memory consumption of different settings.", "description": "Figure 6 presents the ablation study results showing the impact of different hyperparameters (M, \u03bb) and context lengths on both the language modeling task and real-world tasks, along with the distribution of reward differences and GPU memory consumption.", "section": "5 ABLATION STUDY"}, {"figure_path": "2410.18533/charts/charts_9_2.png", "caption": "Figure 6: Ablation study results. (a) Comparison among different settings on the language modeling task (PPL) and real-world tasks (Avg. score on LongBench testing set); (b) Reward difference distribution under different M settings; (c) Training GPU memory consumption of different settings.", "description": "Figure 6 presents an ablation study showing the impact of different hyperparameters (M and \u03bb) and context lengths on both language modeling performance and real-world task performance, along with GPU memory usage.", "section": "5 ABLATION STUDY"}, {"figure_path": "2410.18533/charts/charts_10_0.png", "caption": "Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM.", "description": "The chart displays a comparison of various Long-Context Models (LCMs) across real-world tasks, a synthetic retrieval task, and their respective training data sizes.", "section": "Introduction"}]