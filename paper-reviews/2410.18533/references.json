{"references": [{" publication_date": "2024", "fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "reason": "This paper is highly relevant because it introduces a highly capable language model, similar to the ones the authors are working on. Its technical details and capabilities are very relevant to the context of the research and can help support and contextualize the findings.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is critical as it is a technical report on GPT-4, a cutting-edge model that the current research uses as a benchmark. The analysis of GPT-4's capabilities and limitations is essential for understanding the state-of-the-art in language modeling.", "section_number": 1}, {" publication_date": "2024a", "fullname_first_author": "AI@Meta", "paper_title": "Llama 3-1 model card", "reason": "Llama 3-1 is an important model used in the experiments, and its model card provides crucial details on the model's architecture, training data, and performance.  Understanding these details is necessary for properly evaluating the results obtained in comparison to Llama 3-1.", "section_number": 1}, {" publication_date": "2024b", "fullname_first_author": "AI@Meta", "paper_title": "Llama 3 model card", "reason": "The Llama 3 model is a key component of this research.  Its model card is a primary source of information about the model's architecture, training data, and performance metrics, allowing for better context and understanding of the study's experimental setup and findings.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Reza Yazdani Aminabadi", "paper_title": "Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale", "reason": "This paper describes Deepspeed-inference, a crucial tool used in this research to accelerate the training process and reduce the computational cost. Understanding the strengths and limitations of Deepspeed-inference is essential for interpreting the efficiency of the proposed method, LOGO.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "anthropic", "paper_title": "Claude-3-5-sonnet model card", "reason": "This model card provides critical information about the Claude-3-5-sonnet model, which acts as a benchmark for comparison in the experiments.  Understanding Claude-3-5-sonnet's capabilities helps contextualize and analyze the performance of the proposed LOGO training approach relative to a leading closed-source model.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yushi Bai", "paper_title": "Longbench: A bilingual, multitask benchmark for long context understanding", "reason": "LongBench is the primary benchmark used to evaluate the performance of the proposed LOGO method.  Understanding its design, tasks, and evaluation metrics is crucial for properly interpreting and analyzing the results.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Yushi Bai", "paper_title": "Longalign: A recipe for long context alignment of large language models", "reason": "This paper is highly relevant to the current research because it also focuses on improving long-context alignment of LLMs. Comparing the techniques, approaches, and results of this paper with the proposed method can reveal valuable insights and enhance understanding.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Masha Belyi", "paper_title": "Luna: An evaluation foundation model to catch language model hallucinations with high accuracy and low cost", "reason": "This paper is relevant to the current work as it addresses the problem of detecting hallucinations in large language models. Because hallucinations are one of the main limitations the authors are working on, this study is crucial for evaluation and comparison.", "section_number": 2}, {" publication_date": "2023a", "fullname_first_author": "Shouyuan Chen", "paper_title": "Extending context window of large language models via positional interpolation", "reason": "This paper directly addresses context scaling, one of the key challenges in working with long-context models. The understanding of its approach and limitations helps contextualize the novelty and advantages of the proposed LOGO strategy for dealing with long contexts.", "section_number": 2}, {" publication_date": "2023b", "fullname_first_author": "Yukang Chen", "paper_title": "Longlora: Efficient fine-tuning of long-context large language models", "reason": "This paper is important because it proposes a technique for efficient fine-tuning of long-context large language models. This is directly relevant to the current work as it addresses efficiency concerns related to training long-context models, a key aspect also addressed by LOGO.", "section_number": 2}, {" publication_date": "2023c", "fullname_first_author": "Yukang Chen", "paper_title": "Long alpaca: Long-context instruction-following models", "reason": "The dataset used in the experiment section is LongAlpaca, this paper provides important details about the dataset and its characteristics which are necessary to understand and evaluate the performance and results obtained using the proposed training strategy.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Peter Clark", "paper_title": "Think you have solved question answering? try arc, the ai2 reasoning challenge", "reason": "This paper introduces the ARC dataset, a benchmark used in the experiments to test the model's performance in short-context tasks.  Understanding ARC's design and evaluation criteria is necessary to evaluate the effects of the LOGO training strategy on short-context performance.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Together Computer", "paper_title": "Redpajama: an open dataset for training large language models", "reason": "RedPajama is one of the two datasets used to create the LOGO training dataset; thus, understanding its characteristics is fundamental for evaluating LOGO's results.  This paper provides detailed information about the dataset.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Tri Dao", "paper_title": "Flashattention-2: Faster attention with better parallelism and work partitioning", "reason": "Flashattention-2 is a related work that focuses on improving the efficiency of attention mechanisms in transformers, which directly impacts the performance of large language models. Understanding this technique provides context to assess the efficiency of LOGO.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces Llama 3, a family of models relevant to the experiments. Understanding Llama 3's capabilities and limitations helps in evaluating and interpreting the performance of LOGO in comparison to the Llama 3 models.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yao Fu", "paper_title": "Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance", "reason": "This paper introduces Chain-of-thought prompting, a method that can improve performance in long-context tasks.  Understanding Chain-of-thought and its effectiveness provides context for evaluating the improvements achieved by LOGO on long-context tasks.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yao Fu", "paper_title": "Data engineering for scaling language models to 128k context", "reason": "This work directly tackles the issue of long-context scaling. Understanding the techniques and results of this paper are important for comparing against the proposed method, as well as for assessing the overall state-of-the-art in this area.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "gkamradt", "paper_title": "llmtest-needleinahaystack", "reason": "This paper introduces the Needle-in-a-Haystack dataset, a synthetic benchmark used for evaluating the model's information retrieval abilities.  Understanding this benchmark is essential for interpreting LOGO's impact on information retrieval capabilities, as this is a key performance aspect highlighted in the paper.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "reason": "This paper introduces the MMLU benchmark, which is used for evaluating the model's performance in short-context tasks. Understanding MMLU's design and evaluation criteria is necessary to evaluate LOGO's effect on short-context performance.", "section_number": 4}]}