{"references": [{" publication_date": "2024", "fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "reason": "This paper introduces Phi-3, a highly capable language model that can run locally on a phone. This is significant because it demonstrates that advanced language models can be deployed on resource-constrained devices, opening up possibilities for wider access and more decentralized AI.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This is the technical report for GPT-4, a significant advancement in large language models known for its strong performance in various tasks.  Its detailed analysis provides valuable insights into the state-of-the-art in LLM technology and informs the direction of the current research.", "section_number": 1}, {" publication_date": "2024a", "fullname_first_author": "AI@Meta", "paper_title": "Llama 3-1 model card", "reason": "This paper introduces the Llama 3-1 model, which is a significant advancement in open-source large language models. Its description provides valuable context regarding the performance of open-source models in comparison to closed-source models such as GPT-4.", "section_number": 1}, {" publication_date": "2024b", "fullname_first_author": "AI@Meta", "paper_title": "Llama 3 model card", "reason": "This paper describes the Llama 3 model, which serves as a foundation model for instruction-tuned versions such as Llama-3-8B-Instruct-80K used in the main paper's comparison. Its characteristics are relevant for understanding the baseline performance against which the LOGO method is evaluated.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Reza Yazdani Aminabadi", "paper_title": "Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale", "reason": "This paper presents Deepspeed-Inference, a system for efficient inference of transformer models.  It's relevant to the main paper because efficient inference is crucial for deploying large language models that handle long contexts.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "anthropic", "paper_title": "Claude-3-5-sonnet model card", "reason": "This paper introduces the Claude-3-5-sonnet model, a large language model known for its ability to handle long contexts. The performance of this model provides context for the capabilities of state-of-the-art closed-source models and serves as a point of comparison.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yushi Bai", "paper_title": "Longbench: A bilingual, multitask benchmark for long context understanding", "reason": "This paper presents LongBench, a benchmark suite for evaluating long-context understanding capabilities.  It is directly relevant because it's used in the main paper's empirical evaluation of the proposed LOGO methodology.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Yushi Bai", "paper_title": "Longalign: A recipe for long context alignment of large language models", "reason": "This work introduces LongAlign, a method for long context alignment of large language models. Its relevance lies in the exploration of similar problems and its approach to long context alignment, offering a comparative perspective to the proposed LOGO strategy.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Masha Belyi", "paper_title": "Luna: An evaluation foundation model to catch language model hallucinations with high accuracy and low cost", "reason": "This paper introduces Luna, an evaluation model designed to detect hallucinations in large language models (LLMs). This work provides a contrasting perspective on the issue of misalignment in LLMs, as the authors seek to directly identify instances of hallucinations, while LOGO focuses on improving the model's generation process to reduce them.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Peter Clark", "paper_title": "Think you have solved question answering? try arc, the ai2 reasoning challenge", "reason": "This paper introduces the ARC challenge, a benchmark for evaluating the reasoning capabilities of AI systems. It's relevant to the main paper as a method for evaluating the performance of LLMs in various tasks, including reasoning and common-sense understanding.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Together Computer", "paper_title": "Redpajama: an open dataset for training large language models", "reason": "This paper introduces the RedPajama dataset, a large-scale dataset used for training large language models. It's directly relevant because the RedPajama dataset is used in the main paper's experiments to construct a portion of the LOGO training dataset.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Tri Dao", "paper_title": "Flashattention-2: Faster attention with better parallelism and work partitioning", "reason": "This paper introduces Flashattention-2, a faster attention mechanism.  It is related to the efficiency aspects of LOGO which seeks to address the GPU memory limitations inherent in training long sequence models.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper describes Llama 3, a family of large language models. It provides further context on the models used for comparison in the main paper, in particular the Llama-3-8B-Instruct-80K model.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yao Fu", "paper_title": "Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance", "reason": "This work focuses on evaluating the reasoning capabilities of large language models (LLMs).  Its relevance lies in that it addresses a related challenge in the LLM field: enhancing reasoning and decision-making capabilities, indirectly tied to improving the alignment of LLM outputs.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yao Fu", "paper_title": "Data engineering for scaling language models to 128k context", "reason": "This paper explores data engineering techniques for scaling language models to handle longer contexts, directly addressing one of the primary challenges the main paper's LOGO method seeks to overcome.  The findings and approaches provide a useful comparative analysis and context for the methods and results presented.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "gkamradt", "paper_title": "Llmtest-needleinahaystack", "reason": "This paper describes the Needle-in-a-Haystack benchmark, which is used to evaluate the retrieval capabilities of language models. This is relevant to the main paper because it's utilized as part of the empirical evaluation of the proposed LOGO method and specifically illustrates the performance difference between retrieval and generation.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "reason": "This paper introduces the Massive Multitask Language Understanding (MMLU) benchmark, which evaluates the performance of language models across a wide range of tasks. Its importance lies in serving as one of the evaluation metrics for the main paper's method, showing the effect of the LOGO training on the model's performance in short-context tasks.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jiwoo Hong", "paper_title": "Reference-free monolithic preference optimization with odds ratio", "reason": "This work explores reference-free preference optimization, a technique closely related to the core methodology of LOGO.  The authors' choice to utilize SimPO, a reference-free preference optimization method, can be directly understood in the context of this paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Cheng-Ping Hsieh", "paper_title": "Ruler: What's the real context size of your long-context language models?", "reason": "This paper investigates the limitations of current long-context language models. It's particularly relevant because it highlights similar problems that the main paper's LOGO approach directly addresses: improving the accuracy and reliability of LLM outputs in long-context scenarios.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "reason": "This work presents LoRA, a low-rank adaptation technique for large language models.  It's relevant to the experimental setup of the main paper as LoRA is used for fine-tuning the models, which enhances training efficiency and improves the method's effectiveness.", "section_number": 4}]}