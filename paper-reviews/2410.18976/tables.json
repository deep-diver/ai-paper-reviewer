[{"figure_path": "2410.18976/tables/table_2_0.html", "caption": "Table 1. Comparison of our CAMEL-Bench with existing Arabic LMM benchmarks: Exams-V [13], CVQA [46], Henna[4], and KHATT [34]. Here * denotes that only Arabic part of benchmark is counted.", "description": "Table 1 compares the CAMEL-Bench with other existing Arabic LMM benchmarks, highlighting its comprehensiveness and the number of questions.", "section": "1. Introduction"}, {"figure_path": "2410.18976/tables/table_3_0.html", "caption": "Table 2. Different data sources used for 38 sub-domains corresponding to eight domains, with around 29k questions in total. The different data sources include: MME [15], MMBench [30], MMT-Bench-MI [56], SEED [23], MMMU [58], MMMU-Pro [60], CountBench [39], POPE [26], MathVista [33], Exams-V (Arabic portion) [13], ScienceQA-IMG [32], GQA [20], VizWiz [10], VQAv2 [17], BLINK [16], MuirBench [50], COCO [27], Imagenet [14], Mocheg [55], Snli-Ve [54], Pinterest [42], RealWorldQA [53], PATS-01 [3], KHATT [34], PATD [40], Historical Arabic Handwritten Text Recognition Dataset [37], ISI-PPT-Dataset [52], EvArEST [18], MTVQA [49], ChartQA [35], IconQA [31], BEC-Arabic [47], Claude-3.5 [5], arab-celeb-dataset [36], arabic-food-101 [6], Countries and landmarks [41, 51, 57], Pexel [41], AgroGPT [7], GeoChat [22]. These data sources are carefully translated and verified to ensure quality and relevance.", "description": "Table 2 details the different data sources and the number of questions used for each of the 38 sub-domains across eight domains in the CAMEL-Bench benchmark.", "section": "2. CAMEL-Bench"}, {"figure_path": "2410.18976/tables/table_5_0.html", "caption": "Table 3. Performance comparison of different closed-and open-source LMMs on CAMEL-Bench. We present per-domain results of seven LMMs: GPT-40 [38], GPT-40-mini [38], Gemini-1.5-Pro [2], Gemini-1.5-Flash [2], Pangea-7B [59], Qwen2-VL [9], InternVL2-8B [11], and LLaVaNeXt-7B [29]. GPT-40 excels in most domains, while GPT-40-mini offers an impressive balance of performance and model size. All models struggle with remote sensing, medical imaging, OCR & document understanding, and general multimodal understanding and reasoning domains. Open-source models like InternVL2-8B and LLaVaNeXt-7B show a decline in performance across domains, with their best results in video understanding.", "description": "Table 3 compares the performance of seven different large multimodal models (LLMs) across eight diverse domains in the CAMEL-Bench benchmark, highlighting strengths and weaknesses of both closed-source and open-source models.", "section": "3. CAMEL-Bench Benchmark Evaluation"}]