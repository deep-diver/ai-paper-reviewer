[{"figure_path": "2410.18976/figures/figures_1_0.png", "caption": "Figure 1. The proposed CAMEL-Bench covers eight diverse and challenging domains: multimodal understanding and reasoning, OCR and documents, charts and diagrams, videos, cultural-specific content, medical images, agricultural images, and remote sensing understanding in Arabic. CAMEL-Bench covers 38 sub-domains with over 29K questions carefully curated by native Arabic speakers to rigorously evaluate essential skills desired in Arabic LMMs.", "description": "The figure is a visual representation of the CAMEL-Bench benchmark, showing its eight diverse domains and 38 sub-domains, highlighting the wide range of tasks and visual data types included.", "section": "1. Introduction"}, {"figure_path": "2410.18976/figures/figures_2_0.png", "caption": "Figure 1. The proposed CAMEL-Bench covers eight diverse and challenging domains: multimodal understanding and reasoning, OCR and documents, charts and diagrams, videos, cultural-specific content, medical images, agricultural images, and remote sensing understanding in Arabic. CAMEL-Bench covers 38 sub-domains with over 29K questions carefully curated by native Arabic speakers to rigorously evaluate essential skills desired in Arabic LMMs.", "description": "Figure 1 is a diagram showing the eight diverse domains and 38 sub-domains covered by the CAMEL-Bench Arabic LMM benchmark, highlighting its comprehensiveness and the variety of tasks it evaluates.", "section": "1. Introduction"}, {"figure_path": "2410.18976/figures/figures_4_0.png", "caption": "Figure 3. The CAMEL-Bench Filtering and Verification Pipeline consists of two paths: Original Arabic and translated Arabic. For original Arabic (top row), a 20% random sample undergoes manual verification; if errors are below 40%, the data passes; otherwise, the entire sub-category is reviewed. For Translated Arabic (bottom row), We employ Qwen7B model [8] to assess semantic similarity between the original and translated question-answer pairs on fuzzy-basis evaluation. Pairs passing the evaluation proceed, while those that fail undergo manual review. Based on this, data may require Manual Handling for manual re-translation, Refine & Verify for refinement through the model, or Non-Translated Review where the data is re-sent for translation due to the absence of an Arabic version.", "description": "This figure illustrates the two-path data filtering and verification pipeline used in CAMEL-Bench for both original and translated Arabic data.", "section": "2. CAMEL-Bench"}, {"figure_path": "2410.18976/figures/figures_5_0.png", "caption": "Figure 4. Qualitative example highlighting different scenarios where different closed-weight models struggle on CAMEL-Bench. The correct response is shown in green, and the incorrect one in the red box.", "description": "Figure 4 presents qualitative examples illustrating the struggles of different closed-weight models on various tasks within the CAMEL-Bench benchmark, highlighting correct and incorrect responses.", "section": "3. CAMEL-Bench Benchmark Evaluation"}, {"figure_path": "2410.18976/figures/figures_5_1.png", "caption": "Figure 5. Qualitative example highlighting different scenarios where different open-weight models struggle on CAMEL-Bench. The correct response is shown in green, and the incorrect one in the red box.", "description": "Figure 5 shows examples of open-source LLMs failing on various tasks within the CAMEL-Bench benchmark, highlighting challenges in cultural understanding, medical image interpretation, and agricultural image understanding.", "section": "3. CAMEL-Bench Benchmark Evaluation"}]