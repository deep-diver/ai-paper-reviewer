{"references": [{" publication_date": "2017", "fullname_first_author": "Yash Goyal", "paper_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "reason": "This paper is highly relevant to the introduction section because it directly addresses visual question answering (VQA), a core task within the context of large multimodal models (LMMs). It's a seminal work in the field of VQA, which is central to the motivation for creating a benchmark like CAMEL-Bench.  The paper discusses how advancements in image understanding are vital for improving the performance of VQA systems and enhancing overall multimodal model capabilities.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Junnan Li", "paper_title": "Bootstrapping language-image pre-training for unified vision-language understanding and generation", "reason": "This paper is foundational to the field of vision-language models as it is highly influential in the work done in the introduction and provides a crucial technique used in the field.  The introduction highlights the advancement of LMMs, and this paper is a key example of such advancement by introducing a novel pre-training method that enables unified vision-language understanding and generation. This approach is relevant to the paper's goal of evaluating Arabic LMMs as pre-training is foundational to many successful LMMs.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Junnan Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "reason": "As a follow-up to their previous highly influential work, this publication builds on the success of BLIP by introducing further innovations in vision-language pre-training. The improved efficiency and performance of this model further support the claim in the introduction about the advancements of LMMs and show the trend of improvement that motivated the current work. Thus, this paper is highly relevant to the introduction.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Jaemin Cho", "paper_title": "Unifying vision-and-language tasks via text generation", "reason": "This work is highly relevant to the introduction because it focuses on unifying vision-language tasks, which is directly related to the capabilities of LMMs discussed in the introduction. By introducing a unified approach, this research significantly contributes to the advancement of multimodal understanding, a critical aspect of the LMM capabilities that justify the need for an Arabic benchmark like CAMEL-Bench.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Google AI", "paper_title": "Gemini: A family of highly capable multimodal models", "reason": "This paper is significant to the introduction because it highlights the advancements in LMMs.  The introduction states that LMMs have achieved significant progress, and Gemini represents a major step in that direction with its multimodal capabilities and strong performance across a variety of tasks. The model's capabilities are directly relevant to the discussion in the introduction about the advancements and potential of LMMs, thereby justifying the need for a benchmark like CAMEL-Bench.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "reason": "This paper describes the Qwen model, which is used in the filtering and verification process of the CAMEL-Bench dataset.  The model's ability to assess semantic similarity between English and Arabic text is a critical aspect of the dataset creation methodology. Therefore, this paper is directly relevant to the technical details of the CAMEL-Bench creation process in section 2. It also supports the evaluation criteria for the benchmarked model.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "reason": "This paper is important because the Qwen-VL model is one of the models benchmarked in section 3 of the paper and a major model in the field.  Its inclusion in the benchmark evaluation provides a crucial comparative point for assessing the performance of other LMMs in Arabic. The model's performance (and those of other LMMs) directly relates to the claims in the introduction and helps justify the development of the benchmark.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Google AI", "paper_title": "Gemini: A family of highly capable multimodal models", "reason": "This paper is important because it highlights the advancements in LMMs and provides a basis for comparison in the evaluation in section 3. The introduction highlights recent advancements in LMMs, and Gemini represents a state-of-the-art model in this area.  Its inclusion in the evaluation of CAMEL-Bench helps establish a standard against which other, potentially less advanced, models can be compared.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "reason": "This paper describes the training method that underpins the capabilities of the LLaVA models. The introduction mentions LLaVA as an example of open-source models with good performance; thus, understanding its training method is important to comprehend the context of the benchmark. The training methodology is important for interpreting the results in section 3.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Haotian Liu", "paper_title": "LLaVa-Next: Improved reasoning, ocr, and world knowledge", "reason": "This paper introduces a newer version of the LLaVA model that is included in the benchmark evaluation in section 3. It provides critical comparison points in interpreting the results of the benchmark.  Improvements in OCR and reasoning capabilities specifically address the challenges highlighted in the introduction and the motivation for the benchmark.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Zhe Chen", "paper_title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks", "reason": "This paper is significant because InternVL is one of the open-source models evaluated in the benchmark evaluation in section 3.  The model's performance, along with others, provides crucial data for assessing the capabilities of open-source models and comparing them against closed-source models like GPT-4. This evaluation informs the conclusions in section 4.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Chaoyou Fu", "paper_title": "Mme: A comprehensive evaluation benchmark for multimodal large language models", "reason": "This paper is foundational to the field of multimodal large language models, and several datasets referenced in this work are used within the CAMEL-Bench benchmark.  It provides a comprehensive overview of datasets and evaluation metrics used for multimodal understanding and reasoning, which are then leveraged and adapted in the context of CAMEL-Bench in Section 2.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Kaining Ying", "paper_title": "MMT-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask AGI", "reason": "This is a major benchmark in the field of Multimodal large language models, and several datasets referenced in this work are used within the CAMEL-Bench benchmark.  This paper provides valuable background and context for the evaluation metrics and the design of benchmark datasets used in the CAMEL-Bench, which are discussed in Section 2 and 3. The comparison is done to show the improvement and progress of current work.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Rocktim Jyoti Das", "paper_title": "Exams-v: A multi-discipline multilingual multimodal exam benchmark for evaluating vision language models", "reason": "This paper is directly relevant to the CAMEL-Bench as it introduces an Arabic-centric LLM benchmark which addresses the lack of Arabic benchmarks mentioned in the introduction.  This highlights the need for Arabic benchmarks, similar to the motivation behind CAMEL-Bench, thus establishing a relevant context.  The comparison to this work shows that CAMEL-Bench offers more comprehensive coverage.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Drew A Hudson", "paper_title": "GQA: A new dataset for real-world visual reasoning and compositional question answering", "reason": "This work is important because the GQA dataset is one of the datasets used in CAMEL-Bench.  The GQA dataset is highly relevant to the multimodal understanding and reasoning domain within CAMEL-Bench.  Its utilization in the benchmark highlights the importance of evaluating models on datasets requiring complex visual reasoning and compositional understanding.", "section_number": 2}, {" publication_date": "2014", "fullname_first_author": "Sabri A Mahmoud", "paper_title": "Khatt: An open arabic offline handwritten text database", "reason": "This paper is significant because it provides a dataset used in CAMEL-Bench, which is focused on optical character recognition (OCR) of Arabic handwritten text.  The inclusion of the KHATT dataset in CAMEL-Bench highlights the importance of handling various text forms, including handwriting, in Arabic, which are often overlooked in English-centric benchmarks. This inclusion addresses the need for a more robust and inclusive benchmark for Arabic LMMs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Muhammad Awais", "paper_title": "Agrogpt: Efficient agricultural vision-language model with expert tuning", "reason": "This paper introduces a new model specifically trained for agricultural image understanding and is relevant to the agricultural image understanding domain within CAMEL-Bench.  The inclusion of this model highlights the benchmark's scope in addressing specialized domains and real-world applications, particularly in agriculture, which is a significant aspect of the paper\u2019s contributions.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Kartik Kuckreja", "paper_title": "Geochat: Grounded large vision-language model for remote sensing", "reason": "This paper is relevant to the remote sensing understanding domain within CAMEL-Bench. The GeoChat model, introduced in this paper, is specifically designed for remote sensing tasks.  Its inclusion demonstrates the benchmark's capacity to evaluate performance on specialized tasks and applications within the remote sensing domain, extending its coverage beyond common LMM evaluation tasks.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Jeffrey P Bigham", "paper_title": "Vizwiz: nearly real-time answers to visual questions", "reason": "This paper introduces the VizWiz dataset, which is used in CAMEL-Bench for visual question answering.  The VizWiz dataset is relevant to the multimodal understanding domain within CAMEL-Bench as it provides a rich dataset of questions requiring both visual and linguistic understanding, pushing the boundaries of multimodal reasoning capabilities in LMMs.", "section_number": 2}]}