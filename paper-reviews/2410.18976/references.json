{"references": [{" publication_date": "2017", "fullname_first_author": "Yash Goyal", "paper_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "reason": "This paper is highly relevant to the CAMEL-Bench project because it focuses on improving visual question answering (VQA), a crucial component of the benchmark.  The work highlights the importance of robust image understanding in VQA, which is directly applicable to the design and evaluation of CAMEL-Bench's visual tasks.  The methodology presented in the paper for improving VQA can inform the design and refinement of CAMEL-Bench tasks, leading to a more effective and robust evaluation framework.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Junnan Li", "paper_title": "Bootstrapping language-image pre-training for unified vision-language understanding and generation", "reason": "This paper introduces BLIP, a significant model for unified vision-language understanding and generation.  BLIP's ability to handle various vision-language tasks, including image captioning and VQA, aligns directly with CAMEL-Bench's goals. The architectures and training methodologies employed in BLIP can inspire and guide the development of similar Arabic-focused models, thus making this reference crucial to the CAMEL-Bench project.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "reason": "This paper describes Qwen, a large language model used in the CAMEL-Bench evaluation for fuzzy matching. Qwen's role in evaluating semantic similarity between English and Arabic translations of questions is pivotal to the benchmark's quality assurance. The technical details provided in the report are essential for understanding the evaluation methodology and potential biases introduced by the reliance on a specific language model for this crucial task.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "reason": "The Qwen-VL model, a large vision-language model, is directly compared against in the evaluation section of the CAMEL-Bench.  Understanding its strengths and weaknesses is important for putting the performance of CAMEL-Bench into context and showing progress in the field. The paper on Qwen-VL offers insights into the state-of-the-art in vision-language models, which allows the authors to assess the performance and efficacy of their benchmark in evaluating Arabic LMMs against current technological standards.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Rocktim Jyoti Das", "paper_title": "Exams-V: A multi-discipline multilingual multimodal exam benchmark for evaluating vision language models", "reason": "The Exams-V benchmark, an Arabic multimodal exam benchmark, is directly compared to CAMEL-Bench.  Understanding its strengths and weaknesses is crucial for evaluating the completeness and novelty of CAMEL-Bench.  As an existing Arabic-centric benchmark, it sets a standard for comparison and helps to demonstrate the improvements and advancements made by the CAMEL-Bench framework in evaluating Arabic LLMs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Fajri Koto", "paper_title": "ArabicMMLU: Assessing massive multitask language understanding in Arabic", "reason": "This paper introduces ArabicMMLU, a benchmark for Arabic multi-task language understanding, which is relevant to the CAMEL-Bench project due to the shared goal of evaluating Arabic language models. Comparing ArabicMMLU with CAMEL-Bench allows the authors to highlight the comprehensive nature and the unique contributions of their benchmark.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Drew A Hudson", "paper_title": "GQA: A new dataset for real-world visual reasoning and compositional question answering", "reason": "GQA is a benchmark dataset used in visual question answering (VQA). The challenges posed by GQA, such as real-world visual reasoning and compositional question answering, also apply to the evaluation of Arabic LMMs. The usage of GQA (or variations of it) in the dataset and methodology makes it a relevant and important reference to support the significance and design choices for the CAMEL-Bench benchmark.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Jeffrey P Bigham", "paper_title": "VizWiz: Nearly real-time answers to visual questions", "reason": "VizWiz is a benchmark dataset for real-time visual question answering.  It offers a methodology and set of evaluation criteria relevant to CAMEL-Bench. Understanding VizWiz's approach to visual question answering helps to assess and differentiate the strengths and characteristics of the proposed CAMEL-Bench benchmark.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Chaoyou Fu", "paper_title": "MME: A comprehensive evaluation benchmark for multimodal large language models", "reason": "MME, a comprehensive evaluation benchmark for multimodal large language models (MLLMs), serves as a point of comparison for CAMEL-Bench. Understanding MME's approach to evaluating MLLMs helps to showcase the advancements and unique aspects of CAMEL-Bench, especially in the context of Arabic language models.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Kaining Ying", "paper_title": "MMT-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask AGI", "reason": "MMT-bench is used as a comparison point for the performance evaluations in CAMEL-Bench.  It provides a standard against which to measure the capabilities of Arabic LLMs. Using MMT-bench as a reference helps to evaluate the performance of Arabic LLMs compared to general MLLMs and highlights the specific strengths and shortcomings related to the language and the task.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Jaemin Cho", "paper_title": "Unifying vision-and-language tasks via text generation", "reason": "This paper is referenced as supporting evidence for the advancements in vision-and-language tasks. This provides a context for the need of a comprehensive benchmark like CAMEL-Bench. Its mention emphasizes the broad applicability of the vision-language understanding and the related need for a benchmark to assess the performance of models in this field.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Junnan Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "reason": "Blip-2 offers advancements in language-image pre-training.  As CAMEL-Bench uses pre-trained models for tasks such as translation, understanding the advancements in this area is crucial. Understanding the capabilities and limitations of these pre-trained models is essential for designing the benchmark and evaluating its results.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Fakhraddin Alwajih", "paper_title": "Peacock: A family of arabic multimodal large language models and benchmarks", "reason": "Peacock is an Arabic multimodal benchmark; therefore, it is important to compare it with CAMEL-Bench. Comparing Peacock with CAMEL-Bench allows for a discussion of the improvements and unique characteristics of CAMEL-Bench.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Google AI", "paper_title": "Gemini: A family of highly capable multimodal models", "reason": "Gemini is a family of multimodal models used in the CAMEL-Bench evaluation.  Understanding Gemini\u2019s architecture and capabilities is essential to interpret the results presented in the evaluation section.  The paper on Gemini provides insights into the capabilities of state-of-the-art multimodal models which serves as a reference to demonstrate the performance of the CAMEL-Bench.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Muhammad Awais", "paper_title": "AgroGPT: Efficient agricultural vision-language model with expert tuning", "reason": "AgroGPT, a vision-language model for agriculture, is relevant because CAMEL-Bench includes an agricultural domain.  Understanding the techniques and challenges of applying vision-language models to agriculture helps to improve the design and evaluation of that specific domain within CAMEL-Bench.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Heba Hassan", "paper_title": "Arabic scene text recognition in the deep learning era: Analysis on a novel dataset", "reason": "This paper is relevant to the CAMEL-Bench evaluation because it deals with Arabic scene text recognition, which is one of the tasks in the OCR and document understanding domain of CAMEL-Bench.  This paper helps to understand the challenges and state of the art for Arabic text recognition, helping to improve the design and assessment of this part of the benchmark.", "section_number": 2}, {" publication_date": "2014", "fullname_first_author": "Sabri A Mahmoud", "paper_title": "Khatt: An open arabic offline handwritten text database", "reason": "Khatt is an Arabic handwritten text database used in the CAMEL-Bench.  The paper describing this database provides information about its construction, content, and challenges, which is crucial for understanding the evaluation metrics and design of CAMEL-Bench's handwriting recognition tasks.  A deep understanding of the dataset allows for a more detailed analysis of the model's performance in handling this specific type of data.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Xiang Yue", "paper_title": "MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI", "reason": "MMMU, a multimodal benchmark, provides a comparative point for CAMEL-Bench.  Understanding MMMU's design, tasks, and evaluation methods helps to highlight the unique contributions and the differences in methodology between the two benchmarks.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Xiang Yue", "paper_title": "MMMU-pro: A more robust multi-discipline multimodal understanding benchmark", "reason": "MMMU-pro is an updated version of MMMU, focusing on enhancing robustness. Since CAMEL-Bench aims for robust evaluation, understanding the improvements in MMMU-pro can inform the design choices and evaluation strategies of CAMEL-Bench.  Comparing the two benchmarks highlights the progress made in evaluating multimodal models and helps to justify the choice of evaluation metrics for CAMEL-Bench.", "section_number": 3}]}