[{"figure_path": "https://arxiv.org/html/2502.02584/x1.png", "caption": "Figure 1: QLASS pipeline overview. QLASS involves mainly four stages: 1) Supervised fine-tuning (SFT) on expert data. 2) Leverage SFT agent to explore the environment and construct an exploration tree for each task. After construction, estimate the Q-value of each tree node based on Equation\u00a07. 3) Train QNet on the estimated Q-values. 4) Use the trained QNet to provide inference guidance at each step.", "description": "This figure illustrates the QLASS pipeline, a four-stage process for boosting language agent inference.  Stage 1 involves supervised fine-tuning (SFT) of a language model using expert data. Stage 2 uses the SFT agent to explore the environment and create an exploration tree for each task, estimating Q-values for each node in the tree using Equation 7. Stage 3 trains a Q-network (QNet) on the estimated Q-values. Finally, Stage 4 utilizes the trained QNet to guide the agent's inference at each step, improving decision-making.", "section": "QLASS Pipeline Details"}, {"figure_path": "https://arxiv.org/html/2502.02584/x2.png", "caption": "Figure 2: Illustrative example of constructing a exploration tree. Grey nodes represent the branches with a zero outcome reward. Once the leaf node with a zero outcome reward is detected, a Stop expansion signal will be sent back to the first unexpanded node on the branch. Green nodes are on branches where zero outcome reward is not detected and can keep expanding.", "description": "This figure illustrates the process of building an exploration tree.  Each node in the tree represents a step in an agent's interaction with an environment. Grey nodes indicate branches that resulted in a zero reward; once a zero-reward leaf node is encountered, the algorithm stops expanding that branch. Green nodes, on the other hand, signify branches with non-zero rewards, and these branches continue to be explored.  This selective expansion strategy helps to focus computational resources on more promising trajectories.", "section": "4.2 Constructing an Exploration Tree"}, {"figure_path": "https://arxiv.org/html/2502.02584/x3.png", "caption": "Figure 3: QLASS and Best-of-N under different search budgets. The x-axis represents the number of tokens consumed by the trajectories generated during inference averaged on all the tasks in each test set.", "description": "This figure compares the performance of QLASS and the Best-of-N method across various search budget levels. The x-axis displays the average number of tokens used in the trajectories generated during inference across all three tasks (WebShop, SciWorld, and ALFWorld). The y-axis represents the average reward obtained for each method under different search budgets.  The plot visualizes how both methods' performance changes as the number of tokens increases, demonstrating the impact of varying search space on the final result.", "section": "5.2 Evaluation Results"}, {"figure_path": "https://arxiv.org/html/2502.02584/x4.png", "caption": "Figure 4: Self-training baselines. The three methods marked with diagonal stripes leverage different process reward modeling based on the same exploration trees constructed in Stage 2 to guide self-training data generation.", "description": "This figure compares the performance of three different self-training methods that use varying process reward models.  All three methods leverage exploration trees created in Stage 2 of the QLASS pipeline to guide the self-training data generation process. The three process reward models are: Q-value (the proposed method from QLASS), Average Reward (which averages final rewards), and Reward (which uses back-propagated final rewards). The plot shows that using Q-values for process reward modeling leads to the best self-training performance, demonstrating the effectiveness of the proposed method.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2502.02584/x5.png", "caption": "Figure 5: One example on the ALFWorld, the right is QLASS and the left is the SFT baseline.", "description": "This figure presents a comparison of the performance of QLASS and SFT (Supervised Fine-Tuning) on a specific task within the ALFWorld environment. The left side shows the steps taken by the SFT agent to complete the task of placing a cool lettuce on a countertop. The SFT agent demonstrates redundancy in its actions, repeatedly opening and closing the fridge. In contrast, the right side illustrates QLASS's efficient approach to the same task.  QLASS achieves the goal more efficiently by prioritizing actions with higher Q-values, effectively avoiding unnecessary steps and completing the task successfully.", "section": "5.4 Case Study"}, {"figure_path": "https://arxiv.org/html/2502.02584/x6.png", "caption": "Figure 6: The instruction prompt provided to language agent on WebShop.", "description": "This figure displays the instruction prompt given to the language agent in the WebShop environment.  The prompt instructs the agent that it is engaging in a web-shopping task and should follow instructions provided.  The instructions specify that each turn, the agent will receive an observation and a list of actions, from which it must select an appropriate action.  It details that an action can be a `search` followed by keywords or a `click` on a selectable item. The prompt emphasizes that search keywords must be carefully selected and gives a template for the agent's response, which should include a thought and action.", "section": "4. QLASS Pipeline Details"}, {"figure_path": "https://arxiv.org/html/2502.02584/x7.png", "caption": "Figure 7: The instruction prompt provided to language agent on SciWorld.", "description": "This figure displays the detailed instructions given to the language agent within the SciWorld environment.  These instructions provide a comprehensive overview of the agent's capabilities, available actions, and the expected response format. The instructions detail the various actions the agent can perform, such as opening, closing, activating, deactivating devices, connecting, and disconnecting electrical components, using items, and moving objects.  It also specifies the command format the agent should use when interacting with the environment and provides a sample task for the agent to complete. This ensures the agent understands the environment's constraints and its role in completing the tasks.", "section": "4. QLASS Pipeline Details"}, {"figure_path": "https://arxiv.org/html/2502.02584/x8.png", "caption": "Figure 8: The instruction prompt provided to language agent on ALFWorld.", "description": "This figure displays the instructions given to the language agent in the ALFWorld environment.  The instructions detail the agent's role as an intelligent agent within a household setting, tasked with performing actions to accomplish a given goal.  The prompt specifies the agent's need to follow the provided format when responding. The format includes a 'Thought' section for planning, followed by an 'Action' section detailing the specific action to be performed.  Available actions are clearly listed and explained.  An example task is presented at the end of the instruction, requiring the agent to put a clean knife on the countertop. The prompt aims to guide the agent toward effective and coherent behavior within the environment.", "section": "4. QLASS Pipeline Details"}, {"figure_path": "https://arxiv.org/html/2502.02584/x9.png", "caption": "Figure 9: An illustrative example on task perturbation.", "description": "This figure shows an example of how task descriptions are paraphrased using GPT-3.5-Turbo to increase action diversity during inference in the WebShop environment.  The original task, \"I need a long-lasting 6.76 fl oz bottle of l'eau d'issey, and price lower than 100.00 dollars,\" is rephrased in four different ways, each with slightly altered wording but conveying the same essential information. This perturbation technique helps the model generate a wider variety of actions, leading to more effective exploration and improved performance.", "section": "5.3 Fewer Annotations"}]