<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research &#183; HF Daily Paper Reviews by AI"><meta name=description content="MicroVQA: A new benchmark to test visual-question-answering in microscopy-based research."><meta name=keywords content="Multimodal Learning,Multimodal Reasoning,üè¢ Stanford University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13399/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13399/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research"><meta property="og:description" content="MicroVQA: A new benchmark to test visual-question-answering in microscopy-based research."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-03-17T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-17T00:00:00+00:00"><meta property="article:tag" content="Multimodal Learning"><meta property="article:tag" content="Multimodal Reasoning"><meta property="article:tag" content="üè¢ Stanford University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13399/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13399/cover.png"><meta name=twitter:title content="MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research"><meta name=twitter:description content="MicroVQA: A new benchmark to test visual-question-answering in microscopy-based research."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research","headline":"MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research","abstract":"MicroVQA: A new benchmark to test visual-question-answering in microscopy-based research.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2503.13399\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-03-17T00:00:00\u002b00:00","datePublished":"2025-03-17T00:00:00\u002b00:00","dateModified":"2025-03-17T00:00:00\u002b00:00","keywords":["Multimodal Learning","Multimodal Reasoning","üè¢ Stanford University"],"mainEntityOfPage":"true","wordCount":"4473"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-27</p></a><a href=/ai-paper-reviewer/2025-03-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-28</p></a><a href=/ai-paper-reviewer/2025-03-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-31</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-27</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-28</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-31</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2503.13399/cover_hu18022884069030384480.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2503.13399/>MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-03-17T00:00:00+00:00>17 March 2025</time><span class="px-2 text-primary-500">&#183;</span><span>4473 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">21 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2503.13399/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2503.13399/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-reasoning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Reasoning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-stanford-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Stanford University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#multi-vqa-task>Multi-VQA task</a></li><li><a href=#microscopy-focus>Microscopy focus</a></li><li><a href=#expert-reasoning>Expert reasoning</a></li><li><a href=#mcq-generation>MCQ generation</a></li><li><a href=#no-shortcuts>No shortcuts</a></li><li><a href=#visual-key>Visual key</a></li><li><a href=#domain-limits>Domain limits</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#multi-vqa-task>Multi-VQA task</a></li><li><a href=#microscopy-focus>Microscopy focus</a></li><li><a href=#expert-reasoning>Expert reasoning</a></li><li><a href=#mcq-generation>MCQ generation</a></li><li><a href=#no-shortcuts>No shortcuts</a></li><li><a href=#visual-key>Visual key</a></li><li><a href=#domain-limits>Domain limits</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2503.13399</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>James Burgess et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-03-18</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2503.13399 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2503.13399 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2503.13399/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Scientific discovery needs AI that reasons over multimodal data, which is a challenge, especially in biology. Current multimodal reasoning benchmarks do not target the complexity of research-level tasks. Existing research-level benchmarks lack the complex multimodal reasoning required for scientific discovery, emphasizing lower-level perception. Thus, there is a gap to bridge to achieve more complex reasoning for scientific discovery.</p><p>To address this, a new visual question answering (VQA) benchmark is introduced. The <strong>MicroVQA</strong> assesses three key reasoning skills crucial for research: expert image understanding, hypothesis generation, and experiment proposal. It features 1,042 expert-created multiple-choice questions across diverse microscopy modalities, mirroring real scientific practice. This benchmark exposes limitations in state-of-the-art MLLMs, suggesting areas for improvement such as multimodal reasoning skills.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-4b37d1b7ce9f408160650d53bcf830e2></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-4b37d1b7ce9f408160650d53bcf830e2",{strings:[" MicroVQA, a novel benchmark, assesses expert image understanding, hypothesis generation, and experiment proposal in biological microscopy. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-44dcf0b7dc1a9202e950e07f785829fd></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-44dcf0b7dc1a9202e950e07f785829fd",{strings:[" A two-stage MCQ generation pipeline, featuring an agent-based RefineBot, mitigates language shortcuts and enhances question difficulty. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-dcb4d0bcc2c82a9d376a0aa97032ebcc></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-dcb4d0bcc2c82a9d376a0aa97032ebcc",{strings:[" Benchmarking of state-of-the-art MLLMs reveals a performance gap, highlighting challenges in multimodal scientific reasoning and the need for enhanced perception capabilities. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This research propels scientific AI by introducing a specialized VQA benchmark, revealing MLLM challenges in expert-level microscopy analysis and paving the way for more sophisticated, AI-driven research tools.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x2.png alt></figure></p><blockquote><p>üîº The figure illustrates the three main reasoning tasks involved in scientific experimentation using biological microscopy images: expert image understanding, hypothesis generation, and experimental proposal. Each task is represented with an example image and question, highlighting how MicroVQA uses visual question answering to evaluate these reasoning capabilities. The benchmark comprises 1042 multiple-choice questions, each created by a biology expert, to test the ability to understand, interpret, and reason using microscopy-based data.</p><details><summary>read the caption</summary>Figure 1: A scientific experimentation workflow drives discovery: researchers analyze experiments, develop hypotheses, and design further experiments to test their ideas. We release MicroVQA, a visual question answering (VQA) benchmark to test these three tasks in the context of biological microscopy. Each of the 1,042 samples is created by a biology expert, and transformed into a multiple choice question (MCQ).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S3.T1.6><tr class=ltx_tr id=S3.T1.6.1><td class="ltx_td ltx_align_left ltx_border_tt" id=S3.T1.6.1.1><span class="ltx_text ltx_font_bold" id=S3.T1.6.1.1.1 style=font-size:80%>Dataset feature</span></td><td class="ltx_td ltx_align_right ltx_border_tt" id=S3.T1.6.1.2><span class="ltx_text ltx_font_bold" id=S3.T1.6.1.2.1 style=font-size:80%>Value</span></td><td class="ltx_td ltx_border_tt" id=S3.T1.6.1.3></td></tr><tr class=ltx_tr id=S3.T1.6.2><td class="ltx_td ltx_align_left ltx_border_t" id=S3.T1.6.2.1><span class=ltx_text id=S3.T1.6.2.1.1 style=font-size:80%>Total questions</span></td><td class="ltx_td ltx_align_right ltx_border_t" id=S3.T1.6.2.2><span class=ltx_text id=S3.T1.6.2.2.1 style=font-size:80%>1,042</span></td><td class="ltx_td ltx_border_t" id=S3.T1.6.2.3></td></tr><tr class=ltx_tr id=S3.T1.6.3><td class="ltx_td ltx_align_left" id=S3.T1.6.3.1><span class=ltx_text id=S3.T1.6.3.1.1 style=font-size:80%>Multi-image questions</span></td><td class="ltx_td ltx_align_right" id=S3.T1.6.3.2><span class=ltx_text id=S3.T1.6.3.2.1 style=font-size:80%>423</span></td><td class=ltx_td id=S3.T1.6.3.3></td></tr><tr class=ltx_tr id=S3.T1.6.4><td class="ltx_td ltx_align_left ltx_border_t" id=S3.T1.6.4.1><span class=ltx_text id=S3.T1.6.4.1.1 style=font-size:80%>Avg. MCQ question length</span></td><td class="ltx_td ltx_align_right ltx_border_t" id=S3.T1.6.4.2><span class=ltx_text id=S3.T1.6.4.2.1 style=font-size:80%>66</span></td><td class="ltx_td ltx_border_t" id=S3.T1.6.4.3></td></tr><tr class=ltx_tr id=S3.T1.6.5><td class="ltx_td ltx_align_left" id=S3.T1.6.5.1><span class=ltx_text id=S3.T1.6.5.1.1 style=font-size:80%>Avg. MCQ answer length</span></td><td class="ltx_td ltx_align_right" id=S3.T1.6.5.2><span class=ltx_text id=S3.T1.6.5.2.1 style=font-size:80%>15</span></td><td class=ltx_td id=S3.T1.6.5.3></td></tr><tr class=ltx_tr id=S3.T1.6.6><td class="ltx_td ltx_align_left" id=S3.T1.6.6.1><span class=ltx_text id=S3.T1.6.6.1.1 style=font-size:80%>Avg. raw question length</span></td><td class="ltx_td ltx_align_right" id=S3.T1.6.6.2><span class=ltx_text id=S3.T1.6.6.2.1 style=font-size:80%>158</span></td><td class=ltx_td id=S3.T1.6.6.3></td></tr><tr class=ltx_tr id=S3.T1.6.7><td class="ltx_td ltx_align_left" id=S3.T1.6.7.1><span class=ltx_text id=S3.T1.6.7.1.1 style=font-size:80%>Avg. raw answer length</span></td><td class="ltx_td ltx_align_right" id=S3.T1.6.7.2><span class=ltx_text id=S3.T1.6.7.2.1 style=font-size:80%>52</span></td><td class=ltx_td id=S3.T1.6.7.3></td></tr><tr class=ltx_tr id=S3.T1.6.8><td class="ltx_td ltx_align_left ltx_border_t" id=S3.T1.6.8.1><span class=ltx_text id=S3.T1.6.8.1.1 style=font-size:80%>Unique image sets</span></td><td class="ltx_td ltx_align_right ltx_border_t" id=S3.T1.6.8.2><span class=ltx_text id=S3.T1.6.8.2.1 style=font-size:80%>255</span></td><td class="ltx_td ltx_border_t" id=S3.T1.6.8.3></td></tr><tr class=ltx_tr id=S3.T1.6.9><td class="ltx_td ltx_align_left" id=S3.T1.6.9.1><span class=ltx_text id=S3.T1.6.9.1.1 style=font-size:80%>Image Modalities</span></td><td class="ltx_td ltx_align_right" id=S3.T1.6.9.2><span class=ltx_text id=S3.T1.6.9.2.1 style=font-size:80%>Light, Fluoro, Electron</span></td><td class=ltx_td id=S3.T1.6.9.3></td></tr><tr class=ltx_tr id=S3.T1.6.10><td class="ltx_td ltx_align_left" id=S3.T1.6.10.1><span class=ltx_text id=S3.T1.6.10.1.1 style=font-size:80%>Image Scales</span></td><td class="ltx_td ltx_align_right" id=S3.T1.6.10.2><span class=ltx_text id=S3.T1.6.10.2.1 style=font-size:80%>Tissue, Cell, Subcell, Atomic</span></td><td class=ltx_td id=S3.T1.6.10.3></td></tr><tr class=ltx_tr id=S3.T1.6.11><td class="ltx_td ltx_align_left ltx_border_t" id=S3.T1.6.11.1><span class=ltx_text id=S3.T1.6.11.1.1 style=font-size:80%>Organisms</span></td><td class="ltx_td ltx_align_right ltx_border_t" id=S3.T1.6.11.2><span class=ltx_text id=S3.T1.6.11.2.1 style=font-size:80%>31</span></td><td class="ltx_td ltx_border_t" id=S3.T1.6.11.3></td></tr><tr class=ltx_tr id=S3.T1.6.12><td class="ltx_td ltx_align_left" id=S3.T1.6.12.1><span class=ltx_text id=S3.T1.6.12.1.1 style=font-size:80%>Research areas</span></td><td class="ltx_td ltx_align_right" id=S3.T1.6.12.2><span class=ltx_text id=S3.T1.6.12.2.1 style=font-size:80%>33</span></td><td class=ltx_td id=S3.T1.6.12.3></td></tr><tr class=ltx_tr id=S3.T1.6.13><td class="ltx_td ltx_align_left ltx_border_t" id=S3.T1.6.13.1><span class=ltx_text id=S3.T1.6.13.1.1 style=font-size:80%>Expert question creators</span></td><td class="ltx_td ltx_align_right ltx_border_t" id=S3.T1.6.13.2><span class=ltx_text id=S3.T1.6.13.2.1 style=font-size:80%>12</span></td><td class="ltx_td ltx_border_t" id=S3.T1.6.13.3></td></tr><tr class=ltx_tr id=S3.T1.6.14><td class="ltx_td ltx_align_left" id=S3.T1.6.14.1><span class=ltx_text id=S3.T1.6.14.1.1 style=font-size:80%>Time to create 1 question</span></td><td class="ltx_td ltx_align_right" id=S3.T1.6.14.2><span class=ltx_text id=S3.T1.6.14.2.1 style=font-size:80%>30-40 mins</span></td><td class=ltx_td id=S3.T1.6.14.3></td></tr><tr class=ltx_tr id=S3.T1.6.15><td class="ltx_td ltx_align_left ltx_border_bb" id=S3.T1.6.15.1><span class=ltx_text id=S3.T1.6.15.1.1 style=font-size:80%>Time to quality check 1 MCQ</span></td><td class="ltx_td ltx_align_right ltx_border_bb" id=S3.T1.6.15.2><span class=ltx_text id=S3.T1.6.15.2.1 style=font-size:80%>5 mins</span></td><td class="ltx_td ltx_border_bb" id=S3.T1.6.15.3></td></tr></table></table></figure><blockquote><p>üîº This table presents a comprehensive overview of the key characteristics and attributes of the MicroVQA benchmark dataset. It details the dataset&rsquo;s size, question types, image features, and other relevant aspects to provide a clear understanding of its scope and composition.</p><details><summary>read the caption</summary>Table 1: MicroVQA benchmark attributes.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Multi-VQA task<div id=multi-vqa-task class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multi-vqa-task aria-label=Anchor>#</a></span></h4><p>The concept of a &lsquo;Multi-VQA task,&rsquo; though not explicitly defined in this research paper, is crucial for advancing AI&rsquo;s understanding of complex scientific data. A Multi-VQA task would involve <strong>reasoning across multiple Visual Question Answering scenarios</strong>, requiring a model to synthesize information from diverse image modalities and biological contexts. This is particularly relevant in microscopy, where understanding often relies on comparing images, generating hypotheses, and proposing experiments. Such a task necessitates <strong>robust multimodal reasoning, integrating expert knowledge, and addressing challenges like language shortcuts and biases</strong>. A true Multi-VQA task assesses a model&rsquo;s ability to <strong>generalize scientific principles</strong> across different experimental settings, a critical step toward AI-driven scientific discovery.</p><h4 class="relative group">Microscopy focus<div id=microscopy-focus class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#microscopy-focus aria-label=Anchor>#</a></span></h4><p>The paper&rsquo;s focus on microscopy is evident in the creation of MicroVQA, a <strong>VQA benchmark using microscopy images</strong>. This choice <strong>fills a gap</strong> by demanding more than pattern recognition and factual recall as common in existing benchmarks. It compels models to synthesize visual data with experimental context to formulate hypotheses. This focus highlights the unique challenges presented by microscopy, demanding both abductive and deductive reasoning, bridging the gap between college level tasks and research-level scientific investigation. This emphasis makes it a <strong>valuable resource</strong> for AI-driven biomedical research by pushing for sophisticated multimodal reasoning capabilities.</p><h4 class="relative group">Expert reasoning<div id=expert-reasoning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#expert-reasoning aria-label=Anchor>#</a></span></h4><p><strong>Expert reasoning</strong> is central to the study, as it tests the model&rsquo;s capacity to go beyond mere image recognition, and delve into hypothesis generation & experiment proposals. The paper assesses three vital capabilities: <strong>expert image understanding, hypothesis generation, & experiment proposal</strong>, all vital for scientific research. This assessment is done via a dataset with 1,042 questions, crafted by biology experts, transformed into a multiple-choice format. The questions represent true-to-life scientific situations. This analysis method, coupled with expert-created scenarios, highlights the capacity of AI in sophisticated scientific thought beyond basic object identification.</p><h4 class="relative group">MCQ generation<div id=mcq-generation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#mcq-generation aria-label=Anchor>#</a></span></h4><p>The research paper delves into the intricacies of Multiple-Choice Question (MCQ) generation, highlighting its inadequacy with naive methods. <strong>Standard approaches fail to truly test multimodal abilities</strong>, often resulting in language shortcuts that allow models to answer without genuine understanding. To combat this, the paper introduces a two-stage pipeline. <strong>Initial LLM prompts structure question-answer pairs, followed by an agent-based refinement bot</strong> to remove shortcuts and enhance difficulty. This innovative approach ensures questions are vision-centric, promoting a more accurate assessment of multimodal reasoning. The development and application of RefineBot represents a significant step toward creating robust and reliable MCQs that effectively evaluate model capabilities. <strong>A key factor is creating quality distractors, making sure that generated distractors are vision-centric</strong>. By testing state-of-the-art MLLMs (Multimodal Large Language Models) on the generated data-set it ensures question qualities and difficulties, and at the same time, makes the models more robust.</p><h4 class="relative group">No shortcuts<div id=no-shortcuts class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#no-shortcuts aria-label=Anchor>#</a></span></h4><p>The research paper addresses the challenge of <strong>language shortcuts</strong> in visual question answering (VQA), where models can answer correctly without truly understanding the image. The authors acknowledge that standard methods for generating multiple-choice questions (MCQs) often fail to adequately test multimodal reasoning, as models can exploit language-based cues instead of relying on visual information. To address this, they introduce a <strong>two-stage pipeline for MCQ generation</strong>: First, they use an optimized LLM prompt to structure question-answer pairs into well-formatted MCQs. Second, they employ an agent-based &lsquo;RefineBot&rsquo; to rewrite questions and distractors, aiming to remove language shortcuts and increase the difficulty of the MCQs. The effectiveness of RefineBot is evaluated in the experiments section, which highlights the significant drop in performance by the models after it is used, demonstrating that the models can perform only if the prompt <strong>lacks shortcuts</strong>.</p><h4 class="relative group">Visual key<div id=visual-key class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-key aria-label=Anchor>#</a></span></h4><p>The document leverages visual keys like microscopy images to drive scientific research. Visual keys encompass expert understanding of images, hypothesis generation based on observations, and experimental proposals for validation. <strong>MicroVQA benchmark</strong> curated by experts targets these skills, ensuring high scientific relevance. The goal is to assesses model&rsquo;s reasoning vital in research workflows like image understanding, hypothesis generation, and proposing experiments for better analysis in science.</p><h4 class="relative group">Domain limits<div id=domain-limits class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#domain-limits aria-label=Anchor>#</a></span></h4><p>The discussion of domain limits acknowledges that while MicroVQA strives for broad coverage within microscopy, practical constraints exist due to expert specialization. This means certain modalities, like Raman spectroscopy, are less represented. <strong>Focus on human-relevant samples</strong> (human, mouse) further defines the domain, with fewer examples from other organisms. This trade-off between <strong>breadth and depth</strong> is recognized, and the framework&rsquo;s adaptability to other biomedical imaging fields or even ecology is suggested, highlighting the potential for future expansion while also showcasing current constraint.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x3.png alt></figure></p><blockquote><p>üîº This figure presents a taxonomy that organizes the subtasks within the MicroVQA benchmark into three main categories: expert visual understanding, hypothesis generation, and experimental proposal. Each category is further divided into more specific subtasks to comprehensively evaluate the different aspects of scientific reasoning involved in microscopy-based biological research. Expert visual understanding tasks include comparing image sets, identifying abnormalities, and more. Hypothesis generation tasks involve exploring causal mechanisms and functional implications. Finally, experimental proposal tasks focus on suggesting new experiments or addressing technical issues.</p><details><summary>read the caption</summary>Figure 2: MicroVQA taxonomy of sub-tasks.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x4.png alt></figure></p><blockquote><p>üîº Figure 3 is a bar chart comparing the distribution of Bloom&rsquo;s taxonomy levels across several scientific multimodal large language model (MLLM) benchmarks. Bloom&rsquo;s taxonomy categorizes cognitive skills from simple recall to complex evaluation. The chart shows that MicroVQA, the benchmark introduced in this paper, has a higher proportion of questions at the higher cognitive levels (analysis, application, evaluation) compared to other benchmarks like MMMU and ScienceQA. These other benchmarks tend to have a greater proportion of questions at the lower levels (recall, comprehension), reflecting a focus on simpler tasks. Benchmarks focusing primarily on perception, such as OmniMedVQA, also show a concentration of questions at the lower levels. This figure highlights that MicroVQA is designed to assess more advanced reasoning capabilities than many existing benchmarks.</p><details><summary>read the caption</summary>Figure 3: Composition of scientific MLLM benchmarks regarding estimated Bloom‚Äôs taxonomy [11]. Higher levels are more cognitively challenging. MicroVQA has more questions at higher levels compared to other benchmarks, for example, MMMU [87] and ScienceQA [53], while perception-driven medical benchmarks like OmniMedVQA are at lower levels.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x5.png alt></figure></p><blockquote><p>üîº This figure illustrates the three-stage process of generating multiple-choice questions (MCQs) for the MicroVQA benchmark. Stage 0 involves defining the tasks and creating 1061 raw visual question answering (VQA) samples with the help of biology experts. In Stage 1, these raw VQA samples are transformed into exam-style MCQs. This is done by manually converting a subset of samples and then optimizing an LLM prompt to automatically convert the remaining samples. Finally, in Stage 2, the MCQs are further refined using the RefineBot, an iterative method that increases question difficulty and removes language shortcuts that could allow models to answer without true multi-modal reasoning. The lower panel provides a detailed example illustrating the changes made during the refinement process, highlighting improvements (green) and issues (red) that are further discussed in Section E.2 of the paper.</p><details><summary>read the caption</summary>Figure 4: Constructing the MicroVQA multiple choice questions. (0) We defined tasks with domain biological scientists and created 1,061 raw VQA samples. (1) The raw samples were aligned to an exam-style MCQ by manually transforming a small set and optimizing an LLM prompt to match that alignment. (2) MCQs are further improved using RefineBot, a new iterative method to make MCQs more challenging. The lower panel shows an example MCQ from raw VQA to final: the annotations highlight key changes that we further explore in Sec.¬†E.2, where red indicates issues, and green indicates good attributes.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/extracted/6287634/figures/language-shortcuts.png alt></figure></p><blockquote><p>üîº This figure presents a detailed breakdown of the performance of three state-of-the-art large language models (LLMs) across various subtasks and Bloom&rsquo;s taxonomy levels within the MicroVQA benchmark. The subtasks, representing different aspects of scientific reasoning, include comparing image sets, identifying abnormalities, proposing causal mechanisms, exploring functional implications, suggesting new experiments, and addressing technical issues. The Bloom&rsquo;s taxonomy levels reflect the cognitive complexity of each task, ranging from simple recall to complex evaluation. The models analyzed are Gemini-1.5-Pro (closed-source), VILA1.5-40B (open-source), and LlaVA-Med-Mistral-7B (a medical-specialized model). By visualizing performance across these dimensions, Figure 5 provides key insights into the strengths and weaknesses of each model in tackling various aspects of multimodal scientific reasoning.</p><details><summary>read the caption</summary>Figure 5: Performance by sub-task and Bloom‚Äôs level for best models: Gemini-1.5-Pro (closed source), VILA1.5-40B (open-source), and LlaVA-Med-Mistral-7B (medical).</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x6.png alt></figure></p><blockquote><p>üîº The figure displays the schema of the MicroVQA dataset. It shows the various fields and data types associated with each data point. The fields include identifiers (image_id), the image data itself, labels, label names, domain and subdomain information, imaging modalities and submodalities, stain type, microns per pixel resolution, and the questions and answers associated with the image. This provides a comprehensive overview of the structure of the dataset, illustrating how diverse data (images, text, metadata) is organized to capture the multifaceted nature of scientific visual question answering.</p><details><summary>read the caption</summary>Figure 6: Example data schema.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x7.png alt></figure></p><blockquote><p>üîº This figure displays an example of the data schema used in the MicroVQA benchmark. The schema includes fields such as image ID, image data, label, label name (e.g., pathology), domain, subdomain (e.g., gastrointestinal pathology), modality (e.g., light microscopy), submodality (e.g., brightfield microscopy), stain type (e.g., H&amp;E), microns per pixel, question text, and the correct answer. The question text includes the prompt to be answered and the multiple choice options, along with the correct answer.</p><details><summary>read the caption</summary>Figure 7: Example data instance.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x8.png alt></figure></p><blockquote><p>üîº Figure 8 shows examples of how taxonomy classes are used as context for LLMs to assign organisms to questions in the MicroVQA benchmark. The full taxonomy, in YAML format, will be made available with the code release. This is important because it provides additional context for the LLMs to understand the questions and select the correct answer, which is particularly important in the scientific domain where specialized knowledge is needed.</p><details><summary>read the caption</summary>Figure 8: Examples of taxonomy classes used as context to LLM to assign an organism to a question. A YAML file with the full taxonomy will be released with the code.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x9.png alt></figure></p><blockquote><p>üîº This figure shows examples of the taxonomy classes used to provide context to a large language model (LLM) when assigning a research subject to a question within the MicroVQA benchmark. The full taxonomy, in YAML format, will be available alongside the code release. The taxonomy helps ensure that the LLMs receive sufficient contextual information for accurate and nuanced reasoning in scientific contexts. Each example shows a category within the taxonomy (e.g., &lsquo;Anatomy,&rsquo; &lsquo;Biochemistry&rsquo;) and then lists some relevant keywords.</p><details><summary>read the caption</summary>Figure 9: Examples of taxonomy classes used as context to LLM to assign a research subject to a question. A YAML file with the full taxonomy will be released with the code.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x10.png alt></figure></p><blockquote><p>üîº Figure 10 illustrates three common ways that large language models (LLMs) can &lsquo;cheat&rsquo; on visual question answering (VQA) tasks by relying on textual information rather than visual reasoning. The example questions all relate to a fluorescence microscopy image stained with TOMM20, a marker for mitochondria. Shortcut 1 demonstrates a &lsquo;visual giveaway,&rsquo; where the question itself provides enough information to answer the question without needing to analyze the image. Shortcut 2 shows &lsquo;weak distractors,&rsquo; where the incorrect answer options are implausible or easily eliminated based on general knowledge. Finally, shortcut 3 highlights &rsquo;language bias,&rsquo; where the question wording or context makes one answer more likely than others, irrespective of the visual data.</p><details><summary>read the caption</summary>Figure 10: Three types of language shortcut relevant to MicroVQA. The target VQA has an image that is fluorescence microscopy stained with TOMM20 which would show a pattern consistent with visualizing mitochondria.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x11.png alt></figure></p><blockquote><p>üîº This figure showcases examples of how the two-stage MCQ generation process refines questions and answer options. Stage 1 focuses on creating well-structured MCQs from the original raw data; whereas, stage 2 uses RefineBot to improve the questions by making them more challenging and removing language shortcuts. The image highlights specific modifications made during the refinement process. Red indicates elements that were improved, while green highlights the improvements themselves.</p><details><summary>read the caption</summary>Figure 11: Examples of changes to questions and options between stage 1 and stage 2 (RefineBot) of our MCQ generation method. In red elements that need to be improved and in green improvements.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x12.png alt></figure></p><blockquote><p>üîº This figure shows a bar chart comparing the performance of three different large language models (LLMs) across three different microscopy image modalities: light microscopy, fluorescence microscopy, and electron microscopy. The performance metric is likely accuracy or a similar measure of correct responses to questions about the images. The models compared are Gemini-1.5-Pro, VILA1.5-40B, and LlaVA-Med-Mistral-7B, representing closed-source, open-source, and medical-specialized models, respectively. The chart visually demonstrates how each model&rsquo;s performance varies across the different image modalities, highlighting which modality is most challenging for each model and revealing potential differences in model capabilities for handling diverse visual data types in microscopy image analysis.</p><details><summary>read the caption</summary>Figure 12: Performance by image modality type for the best models: Gemini-1.5-Pro (closed source), VILA1.5-40B (open-source), and LlaVA-Med-Mistral-7B (medical).</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x13.png alt></figure></p><blockquote><p>üîº This figure shows a cryo-electron tomography (cryoET) image of mammalian cells, highlighting dark circular structures within an organelle. The image was acquired at a resolution of 1.3 √Ö and binned by 4, resulting in a 5.3 √Ö per pixel resolution. The figure is accompanied by a multiple-choice question asking about the likely identity of these structures, with options including ribosomes, phase-separated condensates, lysosomes, and peroxisomes. The correct answer is stress granules. The reasoning process for identifying the structures is also shown. A key component of the analysis is assessing the size, shape, electron density, and the context of the organelle in which the structures are located. This figure illustrates an error of perception; the model misidentifies the granules.</p><details><summary>read the caption</summary>Figure 13:</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x14.png alt></figure></p><blockquote><p>üîº This figure shows examples of how the two-stage MCQ generation process improves the quality of questions. The left column shows the original questions and options generated in stage 1 (exam-style alignment). The right column shows the refined questions and options after the RefineBot process (stage 2). The improvements highlight the RefineBot&rsquo;s effectiveness in removing language shortcuts, making the questions more challenging, and improving the distractors&rsquo; quality by ensuring they&rsquo;re biologically plausible and not easily ruled out by superficial knowledge.</p><details><summary>read the caption</summary>Figure 14:</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x15.png alt></figure></p><blockquote><p>üîº This figure displays two H&amp;E-stained images of schwannoma tissue, showing marked cellular changes and unique tissue structures. The images highlight a cystic change that can be seen in chronic, longstanding schwannomas, and fascicular growth with Verocay bodies. The model incorrectly interprets this as rapid cellular proliferation.</p><details><summary>read the caption</summary>Figure 15:</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x16.png alt></figure></p><blockquote><p>üîº This figure shows a fluorescence microscopy image comparing wounded and unwounded mouse liver tissue. The tissue is stained with markers for immune response: neutrophil-derived migrasomes (Ly6G, yellow) and platelets (CD41, magenta). The image illustrates the distribution of these markers in both wounded and unwounded tissue, highlighting differences in immune cell localization and activation in response to tissue injury.</p><details><summary>read the caption</summary>Figure 16:</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x17.png alt></figure></p><blockquote><p>üîº This figure displays the results of a double immunofluorescence experiment conducted on nerve cells to visualize Sema4B (red marker) and Sox9 (an astrocyte marker, green). Sema4B&rsquo;s primary localization is surrounding the nuclear area, dispersed in the cytoplasm. The figure shows three images: one each for Sema4B, Sox9 and a merged image. The question is what potential role Sema4B may play in these cells, based on its observed distribution. The model&rsquo;s response and error analysis are also provided.</p><details><summary>read the caption</summary>Figure 17:</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x18.png alt></figure></p><blockquote><p>üîº This figure shows a cryo-electron tomography image of fibroblasts from a 70-year-old Alzheimer&rsquo;s patient, highlighting three mitochondria clustered together. The question associated with this image in the MicroVQA benchmark asks about the most likely reason for this mitochondrial clustering. The various response options provided to the large language model reflect potential causes related to mitochondrial biogenesis, fusion, fission, mitophagy, or other cellular processes. The correct answer is impaired mitophagy (the failure of the cell to properly remove damaged mitochondria), but the model&rsquo;s answer is incorrect because its understanding of mitophagy is shallow. The analysis of this incorrect answer in the paper reveals the challenges of correctly interpreting complex biological phenomena from microscopic images and applying scientific knowledge appropriately.</p><details><summary>read the caption</summary>Figure 18:</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/x19.png alt></figure></p><blockquote><p>üîº This figure shows an H&amp;E-stained image and a vimentin-stained image of a tissue sample from a patient with recurrent seizures. The vimentin stain highlights cells that are larger than surrounding cells. The question associated with this figure asks for the identity of the enlarged, vimentin-expressing cells. This image is used in the error analysis to illustrate a knowledge error, where the model fails to correctly identify the cells as reactive astrocytes due to a lack of understanding of the specific cellular changes associated with seizures and brain injury. The image highlights the challenges of multi-modal reasoning in biological microscopy.</p><details><summary>read the caption</summary>Figure 19:</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.13399/extracted/6287634/figures/collage_1.jpg alt></figure></p><blockquote><p>üîº This figure shows a microscopic analysis of a tissue sample from an individual with recurrent seizures, using hematoxylin and eosin stain alongside a vimentin stain. The figure highlights cells with stronger vimentin expression, which are significantly larger than their surrounding counterparts. The question associated with this figure in the paper asks to identify the most probable identity of these prominent vimentin-expressing cells.</p><details><summary>read the caption</summary>Figure 20:</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S3.T2.1.1><tr class=ltx_tr id=S3.T2.1.1.2><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id=S3.T2.1.1.2.1><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.2.1.1><span class=ltx_p id=S3.T2.1.1.2.1.1.1 style=width:76.8pt>Benchmark</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id=S3.T2.1.1.2.2><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.2.2.1><span class=ltx_p id=S3.T2.1.1.2.2.1.1 style=width:56.9pt><span class=ltx_inline-block id=S3.T2.1.1.2.2.1.1.1><span class=ltx_p id=S3.T2.1.1.2.2.1.1.1.1>Difficulty</span>
<span class=ltx_p id=S3.T2.1.1.2.2.1.1.1.2>level</span></span></span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id=S3.T2.1.1.2.3><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.2.3.1><span class=ltx_p id=S3.T2.1.1.2.3.1.1 style=width:56.9pt>Domain</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id=S3.T2.1.1.2.4><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.2.4.1><span class=ltx_p id=S3.T2.1.1.2.4.1.1 style=width:82.5pt>Source</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id=S3.T2.1.1.2.5><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.2.5.1><span class=ltx_p id=S3.T2.1.1.2.5.1.1 style=width:28.5pt>Size</span></span></td></tr><tr class=ltx_tr id=S3.T2.1.1.3><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=S3.T2.1.1.3.1><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.3.1.1><span class=ltx_p id=S3.T2.1.1.3.1.1.1 style=width:76.8pt><span class="ltx_text ltx_font_bold" id=S3.T2.1.1.3.1.1.1.1>MicroVQA</span></span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=S3.T2.1.1.3.2><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.3.2.1><span class=ltx_p id=S3.T2.1.1.3.2.1.1 style=width:56.9pt>research</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=S3.T2.1.1.3.3><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.3.3.1><span class=ltx_p id=S3.T2.1.1.3.3.1.1 style=width:56.9pt>microscopy</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=S3.T2.1.1.3.4><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.3.4.1><span class=ltx_p id=S3.T2.1.1.3.4.1.1 style=width:82.5pt>expert-curated</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=S3.T2.1.1.3.5><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.3.5.1><span class=ltx_p id=S3.T2.1.1.3.5.1.1 style=width:28.5pt>1,042</span></span></td></tr><tr class=ltx_tr id=S3.T2.1.1.4><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.4.1><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.4.1.1><span class=ltx_p id=S3.T2.1.1.4.1.1.1 style=width:76.8pt>MMSci¬†<cite class="ltx_cite ltx_align_left ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib47 title><span class=ltx_text style=font-size:90%>47</span></a>]</cite></span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.4.2><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.4.2.1><span class=ltx_p id=S3.T2.1.1.4.2.1.1 style=width:56.9pt>research</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.4.3><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.4.3.1><span class=ltx_p id=S3.T2.1.1.4.3.1.1 style=width:56.9pt>science</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.4.4><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.4.4.1><span class=ltx_p id=S3.T2.1.1.4.4.1.1 style=width:82.5pt>paper figures</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.4.5><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.4.5.1><span class=ltx_p id=S3.T2.1.1.4.5.1.1 style=width:28.5pt>7132</span></span></td></tr><tr class=ltx_tr id=S3.T2.1.1.5><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.5.1><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.5.1.1><span class=ltx_p id=S3.T2.1.1.5.1.1.1 style=width:76.8pt>LabBench¬†<cite class="ltx_cite ltx_align_left ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib41 title><span class=ltx_text style=font-size:90%>41</span></a>]</cite></span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.5.2><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.5.2.1><span class=ltx_p id=S3.T2.1.1.5.2.1.1 style=width:56.9pt>research</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.5.3><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.5.3.1><span class=ltx_p id=S3.T2.1.1.5.3.1.1 style=width:56.9pt>biology</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.5.4><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.5.4.1><span class=ltx_p id=S3.T2.1.1.5.4.1.1 style=width:82.5pt>webQA</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.5.5><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.5.5.1><span class=ltx_p id=S3.T2.1.1.5.5.1.1 style=width:28.5pt>181</span></span></td></tr><tr class=ltx_tr id=S3.T2.1.1.6><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.6.1><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.6.1.1><span class=ltx_p id=S3.T2.1.1.6.1.1.1 style=width:76.8pt>PathVQA¬†<cite class="ltx_cite ltx_align_left ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib30 title><span class=ltx_text style=font-size:90%>30</span></a>]</cite></span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.6.2><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.6.2.1><span class=ltx_p id=S3.T2.1.1.6.2.1.1 style=width:56.9pt>graduate</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.6.3><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.6.3.1><span class=ltx_p id=S3.T2.1.1.6.3.1.1 style=width:56.9pt>pathology</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.6.4><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.6.4.1><span class=ltx_p id=S3.T2.1.1.6.4.1.1 style=width:82.5pt>texbooks</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.6.5><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.6.5.1><span class=ltx_p id=S3.T2.1.1.6.5.1.1 style=width:28.5pt>16.3k</span></span></td></tr><tr class=ltx_tr id=S3.T2.1.1.1><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.1.1><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.1.1.1><span class=ltx_p id=S3.T2.1.1.1.1.1.1 style=width:76.8pt>OmnimedVQA<sup class=ltx_sup id=S3.T2.1.1.1.1.1.1.1><span class="ltx_text ltx_font_italic" id=S3.T2.1.1.1.1.1.1.1.1>‚àó</span></sup>¬†<cite class="ltx_cite ltx_align_left ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib34 title><span class=ltx_text style=font-size:90%>34</span></a>]</cite></span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.1.2><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.1.2.1><span class=ltx_p id=S3.T2.1.1.1.2.1.1 style=width:56.9pt>graduate</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.1.3><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.1.3.1><span class=ltx_p id=S3.T2.1.1.1.3.1.1 style=width:56.9pt>medical</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.1.4><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.1.4.1><span class=ltx_p id=S3.T2.1.1.1.4.1.1 style=width:82.5pt>classification dataset</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.1.5><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.1.5.1><span class=ltx_p id=S3.T2.1.1.1.5.1.1 style=width:28.5pt>127.9k</span></span></td></tr><tr class=ltx_tr id=S3.T2.1.1.7><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.7.1><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.7.1.1><span class=ltx_p id=S3.T2.1.1.7.1.1.1 style=width:76.8pt>Microbench¬†<cite class="ltx_cite ltx_align_left ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib50 title><span class=ltx_text style=font-size:90%>50</span></a>]</cite></span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.7.2><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.7.2.1><span class=ltx_p id=S3.T2.1.1.7.2.1.1 style=width:56.9pt>graduate</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.7.3><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.7.3.1><span class=ltx_p id=S3.T2.1.1.7.3.1.1 style=width:56.9pt>microscopy</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.7.4><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.7.4.1><span class=ltx_p id=S3.T2.1.1.7.4.1.1 style=width:82.5pt>classification dataset</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.7.5><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.7.5.1><span class=ltx_p id=S3.T2.1.1.7.5.1.1 style=width:28.5pt>17.2k</span></span></td></tr><tr class=ltx_tr id=S3.T2.1.1.8><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.8.1><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.8.1.1><span class=ltx_p id=S3.T2.1.1.8.1.1.1 style=width:76.8pt>MMMU¬†<cite class="ltx_cite ltx_align_left ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib87 title><span class=ltx_text style=font-size:90%>87</span></a>]</cite></span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.8.2><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.8.2.1><span class=ltx_p id=S3.T2.1.1.8.2.1.1 style=width:56.9pt>undergraduate</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.8.3><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.8.3.1><span class=ltx_p id=S3.T2.1.1.8.3.1.1 style=width:56.9pt>general</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.8.4><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.8.4.1><span class=ltx_p id=S3.T2.1.1.8.4.1.1 style=width:82.5pt>textbooks, webQA</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.8.5><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.8.5.1><span class=ltx_p id=S3.T2.1.1.8.5.1.1 style=width:28.5pt>11k</span></span></td></tr><tr class=ltx_tr id=S3.T2.1.1.9><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.9.1><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.9.1.1><span class=ltx_p id=S3.T2.1.1.9.1.1.1 style=width:76.8pt>MMMU Pro¬†<cite class="ltx_cite ltx_align_left ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib88 title><span class=ltx_text style=font-size:90%>88</span></a>]</cite></span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.9.2><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.9.2.1><span class=ltx_p id=S3.T2.1.1.9.2.1.1 style=width:56.9pt>undergraduate</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.9.3><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.9.3.1><span class=ltx_p id=S3.T2.1.1.9.3.1.1 style=width:56.9pt>general</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.9.4><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.9.4.1><span class=ltx_p id=S3.T2.1.1.9.4.1.1 style=width:82.5pt>MCQ dataset</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=S3.T2.1.1.9.5><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.9.5.1><span class=ltx_p id=S3.T2.1.1.9.5.1.1 style=width:28.5pt>1,730</span></span></td></tr><tr class=ltx_tr id=S3.T2.1.1.10><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id=S3.T2.1.1.10.1><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.10.1.1><span class=ltx_p id=S3.T2.1.1.10.1.1.1 style=width:76.8pt>Science QA¬†<cite class="ltx_cite ltx_align_left ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib53 title><span class=ltx_text style=font-size:90%>53</span></a>]</cite></span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id=S3.T2.1.1.10.2><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.10.2.1><span class=ltx_p id=S3.T2.1.1.10.2.1.1 style=width:56.9pt>grade-school</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id=S3.T2.1.1.10.3><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.10.3.1><span class=ltx_p id=S3.T2.1.1.10.3.1.1 style=width:56.9pt>science</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id=S3.T2.1.1.10.4><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.10.4.1><span class=ltx_p id=S3.T2.1.1.10.4.1.1 style=width:82.5pt>exams</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id=S3.T2.1.1.10.5><span class="ltx_inline-block ltx_align_top" id=S3.T2.1.1.10.5.1><span class=ltx_p id=S3.T2.1.1.10.5.1.1 style=width:28.5pt>16.8k</span></span></td></tr></table></table></figure><blockquote><p>üîº This table compares MicroVQA with other scientific multimodal benchmarks, focusing on reasoning level and difficulty beyond college level. It shows each benchmark&rsquo;s difficulty level, scientific domain, dataset source (how the questions were obtained), and size (number of questions). MicroVQA stands out because it has a higher difficulty level than most existing benchmarks, and also a higher reasoning level (as demonstrated in Figure 3). Among benchmarks with similar reasoning levels, MicroVQA has a dataset size comparable to MMMU-Pro.</p><details><summary>read the caption</summary>Table 2: Comparing scientific multimodal benchmarks close to MicroVQA for in terms of reasoning level or difficulty beyond college level. We show difficulty level, scientific domain, dataset source, and size. Compared to others, MicroVQA either has higher difficulty level, or it has higher reasoning level (which is established in Fig.¬†3). Compared to others at the same reasoning level, namely MMMU-Pro, it has similar size.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S5.T3.7.3><tr class=ltx_tr id=S5.T3.7.3.4><td class="ltx_td ltx_align_top ltx_border_tt" id=S5.T3.7.3.4.1></td><td class="ltx_td ltx_align_left ltx_border_tt" id=S5.T3.7.3.4.2><span class="ltx_text ltx_font_bold" id=S5.T3.7.3.4.2.1>Model</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.7.3.4.3><span class=ltx_inline-block id=S5.T3.7.3.4.3.1><span class=ltx_p id=S5.T3.7.3.4.3.1.1><span class="ltx_text ltx_font_bold" id=S5.T3.7.3.4.3.1.1.1>Overall</span></span></span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.7.3.4.4><span class=ltx_inline-block id=S5.T3.7.3.4.4.1><span class=ltx_p id=S5.T3.7.3.4.4.1.1><span class="ltx_text ltx_font_bold" id=S5.T3.7.3.4.4.1.1.1>V</span></span></span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.7.3.4.5><span class=ltx_inline-block id=S5.T3.7.3.4.5.1><span class=ltx_p id=S5.T3.7.3.4.5.1.1><span class="ltx_text ltx_font_bold" id=S5.T3.7.3.4.5.1.1.1>H</span></span></span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.7.3.4.6><span class="ltx_text ltx_font_bold" id=S5.T3.7.3.4.6.1>E</span></td></tr><tr class=ltx_tr id=S5.T3.7.3.5><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=S5.T3.7.3.5.1><span class="ltx_inline-block ltx_align_top" id=S5.T3.7.3.5.1.1><span class=ltx_p id=S5.T3.7.3.5.1.1.1 style=width:10pt><span class=ltx_text id=S5.T3.7.3.5.1.1.1.1><span class="ltx_inline-block ltx_transformed_outer" id=S5.T3.7.3.5.1.1.1.1.1 style=width:18pt;height:15.8pt;vertical-align:1.1pt><span class=ltx_transformed_inner style=width:15.8pt;transform:translate(1.09pt,0)rotate(-90deg)><span class=ltx_p id=S5.T3.7.3.5.1.1.1.1.1.1><span class=ltx_text id=S5.T3.7.3.5.1.1.1.1.1.1.1></span><span class=ltx_text id=S5.T3.7.3.5.1.1.1.1.1.1.2 style=font-size:70%> <span class=ltx_text id=S5.T3.7.3.5.1.1.1.1.1.1.2.1><span class="ltx_tabular ltx_align_middle" id=S5.T3.7.3.5.1.1.1.1.1.1.2.1.1><span class=ltx_tr id=S5.T3.7.3.5.1.1.1.1.1.1.2.1.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.T3.7.3.5.1.1.1.1.1.1.2.1.1.1.1>R</span></span>
</span></span><span class=ltx_text id=S5.T3.7.3.5.1.1.1.1.1.1.2.2></span></span></span></span></span></span></span></span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T3.7.3.5.2>o1 <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib36 title><span class=ltx_text style=font-size:90%>36</span></a>]</cite></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.5.3><span class="ltx_text ltx_font_bold" id=S5.T3.7.3.5.3.1>52.8</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.5.4><span class="ltx_text ltx_ulem_uline" id=S5.T3.7.3.5.4.1>55.4</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.5.5><span class="ltx_text ltx_font_bold" id=S5.T3.7.3.5.5.1>50.2</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.5.6><span class="ltx_text ltx_font_bold" id=S5.T3.7.3.5.6.1>53.0</span></td></tr><tr class=ltx_tr id=S5.T3.5.1.1><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=S5.T3.5.1.1.2 rowspan=9><span class="ltx_inline-block ltx_align_top" id=S5.T3.5.1.1.2.1><span class=ltx_p id=S5.T3.5.1.1.2.1.1 style=width:10pt><span class=ltx_text id=S5.T3.5.1.1.2.1.1.1><span class="ltx_inline-block ltx_transformed_outer" id=S5.T3.5.1.1.2.1.1.1.1 style=width:18pt;height:49pt;vertical-align:-15.5pt><span class=ltx_transformed_inner style=width:49pt;transform:translate(-15.52pt,0)rotate(-90deg)><span class=ltx_p id=S5.T3.5.1.1.2.1.1.1.1.1><span class=ltx_text id=S5.T3.5.1.1.2.1.1.1.1.1.1></span><span class=ltx_text id=S5.T3.5.1.1.2.1.1.1.1.1.2 style=font-size:70%> <span class=ltx_text id=S5.T3.5.1.1.2.1.1.1.1.1.2.1><span class="ltx_tabular ltx_align_middle" id=S5.T3.5.1.1.2.1.1.1.1.1.2.1.1><span class=ltx_tr id=S5.T3.5.1.1.2.1.1.1.1.1.2.1.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.T3.5.1.1.2.1.1.1.1.1.2.1.1.1.1>large models</span></span>
</span></span><span class=ltx_text id=S5.T3.5.1.1.2.1.1.1.1.1.2.2></span></span></span></span></span></span></span></span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T3.5.1.1.1><sup class=ltx_sup id=S5.T3.5.1.1.1.1>‚àó</sup>Claude-3.5-Sonnet <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib6 title><span class=ltx_text style=font-size:90%>6</span></a>]</cite></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.5.1.1.3><span class="ltx_text ltx_ulem_uline" id=S5.T3.5.1.1.3.1>51.7</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.5.1.1.4>54.1</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.5.1.1.5><span class="ltx_text ltx_font_bold" id=S5.T3.5.1.1.5.1>50.2</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.5.1.1.6>50.4</td></tr><tr class=ltx_tr id=S5.T3.7.3.6><td class="ltx_td ltx_align_left" id=S5.T3.7.3.6.1>Gemini-Pro-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib69 title><span class=ltx_text style=font-size:90%>69</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.6.2>51.1</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.6.3>52.0</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.6.4><span class="ltx_text ltx_font_bold" id=S5.T3.7.3.6.4.1>50.2</span></td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.6.5>50.9</td></tr><tr class=ltx_tr id=S5.T3.7.3.7><td class="ltx_td ltx_align_left" id=S5.T3.7.3.7.1>Pixtral-Large <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib2 title><span class=ltx_text style=font-size:90%>2</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.7.2>49.8</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.7.3>50.8</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.7.4>49.5</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.7.5>48.7</td></tr><tr class=ltx_tr id=S5.T3.7.3.8><td class="ltx_td ltx_align_left" id=S5.T3.7.3.8.1>Grok-2-Vision <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib84 title><span class=ltx_text style=font-size:90%>84</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.8.2>48.4</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.8.3>50.3</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.8.4>46.4</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.8.5>48.7</td></tr><tr class=ltx_tr id=S5.T3.7.3.9><td class="ltx_td ltx_align_left" id=S5.T3.7.3.9.1>Qwen-2-vl-72b-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib79 title><span class=ltx_text style=font-size:90%>79</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.9.2>47.5</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.9.3>49.2</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.9.4>45.7</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.9.5>47.8</td></tr><tr class=ltx_tr id=S5.T3.7.3.10><td class="ltx_td ltx_align_left" id=S5.T3.7.3.10.1>VILA1.5-40b <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib48 title><span class=ltx_text style=font-size:90%>48</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.10.2>47.5</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.10.3>47.2</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.10.4>47.9</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.10.5>47.4</td></tr><tr class=ltx_tr id=S5.T3.6.2.2><td class="ltx_td ltx_align_left" id=S5.T3.6.2.2.1><sup class=ltx_sup id=S5.T3.6.2.2.1.1>‚àó</sup>GPT-4o <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib1 title><span class=ltx_text style=font-size:90%>1</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S5.T3.6.2.2.2>45.6</td><td class="ltx_td ltx_align_center" id=S5.T3.6.2.2.3>48.7</td><td class="ltx_td ltx_align_center" id=S5.T3.6.2.2.4>43.1</td><td class="ltx_td ltx_align_center" id=S5.T3.6.2.2.5>44.8</td></tr><tr class=ltx_tr id=S5.T3.7.3.11><td class="ltx_td ltx_align_left" id=S5.T3.7.3.11.1>Llama-3.1-Nemotron-70b-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib83 title><span class=ltx_text style=font-size:90%>83</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.11.2>44.2</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.11.3>44.9</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.11.4>43.3</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.11.5>44.8</td></tr><tr class=ltx_tr id=S5.T3.7.3.12><td class="ltx_td ltx_align_left" id=S5.T3.7.3.12.1>Llama-3.2-90b-Vision-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib21 title><span class=ltx_text style=font-size:90%>21</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.12.2>42.4</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.12.3>44.9</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.12.4>42.1</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.12.5>38.7</td></tr><tr class=ltx_tr id=S5.T3.7.3.13><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=S5.T3.7.3.13.1 rowspan=7><span class="ltx_inline-block ltx_align_top" id=S5.T3.7.3.13.1.1><span class=ltx_p id=S5.T3.7.3.13.1.1.1 style=width:10pt><span class=ltx_text id=S5.T3.7.3.13.1.1.1.1><span class="ltx_inline-block ltx_transformed_outer" id=S5.T3.7.3.13.1.1.1.1.1 style=width:18pt;height:50.2pt;vertical-align:-16.1pt><span class=ltx_transformed_inner style=width:50.2pt;transform:translate(-16.11pt,0)rotate(-90deg)><span class=ltx_p id=S5.T3.7.3.13.1.1.1.1.1.1><span class=ltx_text id=S5.T3.7.3.13.1.1.1.1.1.1.1></span><span class=ltx_text id=S5.T3.7.3.13.1.1.1.1.1.1.2 style=font-size:70%> <span class=ltx_text id=S5.T3.7.3.13.1.1.1.1.1.1.2.1><span class="ltx_tabular ltx_align_middle" id=S5.T3.7.3.13.1.1.1.1.1.1.2.1.1><span class=ltx_tr id=S5.T3.7.3.13.1.1.1.1.1.1.2.1.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.T3.7.3.13.1.1.1.1.1.1.2.1.1.1.1>small models</span></span>
</span></span><span class=ltx_text id=S5.T3.7.3.13.1.1.1.1.1.1.2.2></span></span></span></span></span></span></span></span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T3.7.3.13.2>Qwen-2-VL-7b <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib79 title><span class=ltx_text style=font-size:90%>79</span></a>]</cite></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.13.3>48.8</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.13.4>54.1</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.13.5>43.3</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.13.6>49.6</td></tr><tr class=ltx_tr id=S5.T3.7.3.14><td class="ltx_td ltx_align_left" id=S5.T3.7.3.14.1>Claude-3.5-Haiku <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib6 title><span class=ltx_text style=font-size:90%>6</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.14.2>47.1</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.14.3>48.0</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.14.4>43.8</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.14.5><span class="ltx_text ltx_ulem_uline" id=S5.T3.7.3.14.5.1>51.7</span></td></tr><tr class=ltx_tr id=S5.T3.7.3.15><td class="ltx_td ltx_align_left" id=S5.T3.7.3.15.1>Gemini-Flash-1.5-8b <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib69 title><span class=ltx_text style=font-size:90%>69</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.15.2>46.7</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.15.3>48.7</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.15.4>43.6</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.15.5>49.1</td></tr><tr class=ltx_tr id=S5.T3.7.3.16><td class="ltx_td ltx_align_left" id=S5.T3.7.3.16.1>GPT-4o-mini <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib1 title><span class=ltx_text style=font-size:90%>1</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.16.2>46.2</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.16.3>48.5</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.16.4>43.6</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.16.5>47.0</td></tr><tr class=ltx_tr id=S5.T3.7.3.17><td class="ltx_td ltx_align_left" id=S5.T3.7.3.17.1>Pixtral-12b <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib2 title><span class=ltx_text style=font-size:90%>2</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.17.2>45.6</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.17.3>46.9</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.17.4>44.8</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.17.5>44.8</td></tr><tr class=ltx_tr id=S5.T3.7.3.18><td class="ltx_td ltx_align_left" id=S5.T3.7.3.18.1>VILA1.5-13b <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib48 title><span class=ltx_text style=font-size:90%>48</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.18.2>41.8</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.18.3>41.8</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.18.4>47.5</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.18.5>40.9</td></tr><tr class=ltx_tr id=S5.T3.7.3.19><td class="ltx_td ltx_align_left" id=S5.T3.7.3.19.1>Llama-3.2-11b-vision-instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib21 title><span class=ltx_text style=font-size:90%>21</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.19.2>30.3</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.19.3>32.4</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.19.4>29.3</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.19.5>28.7</td></tr><tr class=ltx_tr id=S5.T3.7.3.20><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=S5.T3.7.3.20.1 rowspan=2><span class="ltx_inline-block ltx_align_top" id=S5.T3.7.3.20.1.1><span class=ltx_p id=S5.T3.7.3.20.1.1.1 style=width:10pt><span class=ltx_text id=S5.T3.7.3.20.1.1.1.1><span class="ltx_inline-block ltx_transformed_outer" id=S5.T3.7.3.20.1.1.1.1.1 style=width:18pt;height:34pt;vertical-align:-8pt><span class=ltx_transformed_inner style=width:34pt;transform:translate(-8pt,0)rotate(-90deg)><span class=ltx_p id=S5.T3.7.3.20.1.1.1.1.1.1><span class=ltx_text id=S5.T3.7.3.20.1.1.1.1.1.1.1></span><span class=ltx_text id=S5.T3.7.3.20.1.1.1.1.1.1.2 style=font-size:70%> <span class=ltx_text id=S5.T3.7.3.20.1.1.1.1.1.1.2.1><span class="ltx_tabular ltx_align_middle" id=S5.T3.7.3.20.1.1.1.1.1.1.2.1.1><span class=ltx_tr id=S5.T3.7.3.20.1.1.1.1.1.1.2.1.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.T3.7.3.20.1.1.1.1.1.1.2.1.1.1.1>medical</span></span>
</span></span><span class=ltx_text id=S5.T3.7.3.20.1.1.1.1.1.1.2.2></span></span></span></span></span></span></span></span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T3.7.3.20.2>LLaVA-Med-Mistral-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib42 title><span class=ltx_text style=font-size:90%>42</span></a>]</cite></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.20.3>43.0</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.20.4>37.3</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.20.5>47.1</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.20.6>41.6</td></tr><tr class=ltx_tr id=S5.T3.7.3.3><td class="ltx_td ltx_align_left" id=S5.T3.7.3.3.1><sup class=ltx_sup id=S5.T3.7.3.3.1.1><span class="ltx_text ltx_font_italic" id=S5.T3.7.3.3.1.1.1>‚Ä†</span></sup>LLaVA-Mistral-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.13399v1#bib.bib49 title><span class=ltx_text style=font-size:90%>49</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.3.2>39.8</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.3.3>31.6</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.3.4>43.1</td><td class="ltx_td ltx_align_center" id=S5.T3.7.3.3.5>37.1</td></tr><tr class=ltx_tr id=S5.T3.7.3.21><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id=S5.T3.7.3.21.1 rowspan=2><span class="ltx_inline-block ltx_align_top" id=S5.T3.7.3.21.1.1><span class=ltx_p id=S5.T3.7.3.21.1.1.1 style=width:10pt><span class=ltx_text id=S5.T3.7.3.21.1.1.1.1><span class="ltx_inline-block ltx_transformed_outer" id=S5.T3.7.3.21.1.1.1.1.1 style=width:0pt;height:4.6pt;vertical-align:-2.3pt><span class=ltx_transformed_inner style=width:4.7pt;transform:translate(-2.33pt,0)rotate(-90deg)><span class=ltx_p id=S5.T3.7.3.21.1.1.1.1.1.1><span class=ltx_text id=S5.T3.7.3.21.1.1.1.1.1.1.1></span><span class=ltx_text id=S5.T3.7.3.21.1.1.1.1.1.1.2 style=font-size:70%> <span class=ltx_text id=S5.T3.7.3.21.1.1.1.1.1.1.2.1></span> <span class=ltx_text id=S5.T3.7.3.21.1.1.1.1.1.1.2.2></span></span></span></span></span></span></span></span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T3.7.3.21.2>Random</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.21.3>22.0</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.21.4>21.9</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.21.5>21.8</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.7.3.21.6>21.9</td></tr><tr class=ltx_tr id=S5.T3.7.3.22><td class="ltx_td ltx_align_left ltx_border_bb" id=S5.T3.7.3.22.1>Human</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.7.3.22.2>50.3</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.7.3.22.3>52.7</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.7.3.22.4>47.5</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.7.3.22.5>51.4</td></tr></table></table></figure><blockquote><p>üîº This table presents the performance of various Multimodal Large Language Models (MLLMs) on the MicroVQA benchmark. The benchmark tests three key reasoning capabilities: expert image understanding (V), hypothesis generation (H), and experiment proposal (E). The table shows the accuracy of each model on each of these tasks. Note that some models were used in the creation of the multiple-choice questions (MCQs) themselves and may therefore show inflated scores. Also, one model&rsquo;s performance is given relative to its base model. The highest accuracy for each task is highlighted in bold.</p><details><summary>read the caption</summary>Table 3: VQA accuracy on MicroVQA by task: expert visual understanding (V), hypothesis generation (H), experiment proposal (E). Models marked ‚àó were used in MCQ generation, which may affect comparative performance (see Sec.¬†5.1). The model ‚Ä† is the base LlaVA for LLaVA-Med. Best values are bolded.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T4.8><tr class=ltx_tr id=S5.T4.8.5><td class="ltx_td ltx_border_tt" id=S5.T4.8.5.1></td><td class="ltx_td ltx_align_left ltx_border_tt" id=S5.T4.8.5.2><span class="ltx_text ltx_font_bold" id=S5.T4.8.5.2.1 style=font-size:80%>Stage 1</span></td><td class="ltx_td ltx_align_left ltx_border_tt" id=S5.T4.8.5.3><span class="ltx_text ltx_font_bold" id=S5.T4.8.5.3.1 style=font-size:80%>Stage 2</span></td><td class="ltx_td ltx_align_left ltx_border_tt" id=S5.T4.8.5.4><span class="ltx_text ltx_font_bold" id=S5.T4.8.5.4.1 style=font-size:80%>Relative drop</span></td></tr><tr class=ltx_tr id=S5.T4.5.1><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T4.5.1.1><sup class=ltx_sup id=S5.T4.5.1.1.1><span class=ltx_text id=S5.T4.5.1.1.1.1 style=font-size:80%>‚àó</span></sup><span class=ltx_text id=S5.T4.5.1.1.2 style=font-size:80%>GPT-4o</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.5.1.2><span class=ltx_text id=S5.T4.5.1.2.1 style=font-size:80%>79.7</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.5.1.3><span class=ltx_text id=S5.T4.5.1.3.1 style=font-size:80%>46.8</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.5.1.4><span class=ltx_text id=S5.T4.5.1.4.1 style=font-size:80%>-41.2</span></td></tr><tr class=ltx_tr id=S5.T4.6.2><td class="ltx_td ltx_align_left" id=S5.T4.6.2.1><sup class=ltx_sup id=S5.T4.6.2.1.1><span class="ltx_text ltx_font_italic" id=S5.T4.6.2.1.1.1 style=font-size:80%>‚àó‚àó</span></sup><span class=ltx_text id=S5.T4.6.2.1.2 style=font-size:80%>GPT-4o-mini</span></td><td class="ltx_td ltx_align_center" id=S5.T4.6.2.2><span class=ltx_text id=S5.T4.6.2.2.1 style=font-size:80%>75.6</span></td><td class="ltx_td ltx_align_center" id=S5.T4.6.2.3><span class=ltx_text id=S5.T4.6.2.3.1 style=font-size:80%>46.2</span></td><td class="ltx_td ltx_align_center" id=S5.T4.6.2.4><span class=ltx_text id=S5.T4.6.2.4.1 style=font-size:80%>-39.0</span></td></tr><tr class=ltx_tr id=S5.T4.7.3><td class="ltx_td ltx_align_left" id=S5.T4.7.3.1><sup class=ltx_sup id=S5.T4.7.3.1.1><span class=ltx_text id=S5.T4.7.3.1.1.1 style=font-size:80%>‚àó</span></sup><span class=ltx_text id=S5.T4.7.3.1.2 style=font-size:80%>Claude-3.5-Sonnet</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.3.2><span class=ltx_text id=S5.T4.7.3.2.1 style=font-size:80%>82.2</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.3.3><span class=ltx_text id=S5.T4.7.3.3.1 style=font-size:80%>51.7</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.3.4><span class=ltx_text id=S5.T4.7.3.4.1 style=font-size:80%>-37.1</span></td></tr><tr class=ltx_tr id=S5.T4.8.4><td class="ltx_td ltx_align_left" id=S5.T4.8.4.1><sup class=ltx_sup id=S5.T4.8.4.1.1><span class="ltx_text ltx_font_italic" id=S5.T4.8.4.1.1.1 style=font-size:80%>‚àó‚àó</span></sup><span class=ltx_text id=S5.T4.8.4.1.2 style=font-size:80%>Claude-3.5-Haiku</span></td><td class="ltx_td ltx_align_center" id=S5.T4.8.4.2><span class=ltx_text id=S5.T4.8.4.2.1 style=font-size:80%>77.3</span></td><td class="ltx_td ltx_align_center" id=S5.T4.8.4.3><span class=ltx_text id=S5.T4.8.4.3.1 style=font-size:80%>47.1</span></td><td class="ltx_td ltx_align_center" id=S5.T4.8.4.4><span class=ltx_text id=S5.T4.8.4.4.1 style=font-size:80%>-39.0</span></td></tr><tr class=ltx_tr id=S5.T4.8.6><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T4.8.6.1><span class=ltx_text id=S5.T4.8.6.1.1 style=font-size:80%>o1</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.8.6.2><span class=ltx_text id=S5.T4.8.6.2.1 style=font-size:80%>81.6</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.8.6.3><span class=ltx_text id=S5.T4.8.6.3.1 style=font-size:80%>52.8</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.8.6.4><span class=ltx_text id=S5.T4.8.6.4.1 style=font-size:80%>-35.3</span></td></tr><tr class=ltx_tr id=S5.T4.8.7><td class="ltx_td ltx_align_left" id=S5.T4.8.7.1><span class=ltx_text id=S5.T4.8.7.1.1 style=font-size:80%>Pixtral-Large</span></td><td class="ltx_td ltx_align_center" id=S5.T4.8.7.2><span class=ltx_text id=S5.T4.8.7.2.1 style=font-size:80%>80.1</span></td><td class="ltx_td ltx_align_center" id=S5.T4.8.7.3><span class=ltx_text id=S5.T4.8.7.3.1 style=font-size:80%>49.8</span></td><td class="ltx_td ltx_align_center" id=S5.T4.8.7.4><span class=ltx_text id=S5.T4.8.7.4.1 style=font-size:80%>-37.8</span></td></tr><tr class=ltx_tr id=S5.T4.8.8><td class="ltx_td ltx_align_left ltx_border_bb" id=S5.T4.8.8.1><span class=ltx_text id=S5.T4.8.8.1.1 style=font-size:80%>Gemini-Pro-1.5</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.8.8.2><span class=ltx_text id=S5.T4.8.8.2.1 style=font-size:80%>79.1</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.8.8.3><span class=ltx_text id=S5.T4.8.8.3.1 style=font-size:80%>51.1</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.8.8.4><span class=ltx_text id=S5.T4.8.8.4.1 style=font-size:80%>-35.4</span></td></tr></table></table></figure><blockquote><p>üîº This ablation study analyzes the impact of each stage in the MicroVQA MCQ generation pipeline (shown in Figure 4). Stage 1 aligns raw VQA samples to exam-style MCQs, while Stage 2 uses RefineBot to refine these MCQs and increase difficulty by removing language shortcuts. The table shows that accuracy is initially high due to these shortcuts after Stage 1, but decreases significantly after incorporating RefineBot in Stage 2. The models used in the Stage 2 refinement show the largest drops in accuracy after refinement. Models from the same provider are grouped together for better comparison.</p><details><summary>read the caption</summary>Table 4: Ablation study on MicroVQA MCQ generation stages (shown in Fig.¬†4). Accuracy is high because MCQs have shortcuts (Sec.¬†4.1) after ‚ÄòStage 1‚Äô exam alignment, but is lower after ‚ÄòStage 2‚Äô. Final column is the relative decrease in accuracy. Models with ‚àó were used in Stage 2 generation and have the biggest accuracy drops (bolded). They are grouped with different models from the same provider in ‚àó‚àó.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=A5.T5.6><tr class=ltx_tr id=A5.T5.6.1><td class="ltx_td ltx_border_tt" id=A5.T5.6.1.1></td><td class="ltx_td ltx_align_left ltx_border_tt" id=A5.T5.6.1.2><span class="ltx_text ltx_font_bold" id=A5.T5.6.1.2.1 style=font-size:80%>VQA</span></td><td class="ltx_td ltx_align_left ltx_border_tt" id=A5.T5.6.1.3><span class="ltx_text ltx_font_bold" id=A5.T5.6.1.3.1 style=font-size:80%>VQA-no-image</span></td></tr><tr class=ltx_tr id=A5.T5.6.2><td class="ltx_td ltx_align_left ltx_border_t" id=A5.T5.6.2.1><span class=ltx_text id=A5.T5.6.2.1.1 style=font-size:80%>GPT-4o</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A5.T5.6.2.2><span class=ltx_text id=A5.T5.6.2.2.1 style=font-size:80%>85.1</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A5.T5.6.2.3><span class=ltx_text id=A5.T5.6.2.3.1 style=font-size:80%>82.7</span></td></tr><tr class=ltx_tr id=A5.T5.6.3><td class="ltx_td ltx_align_left" id=A5.T5.6.3.1><span class=ltx_text id=A5.T5.6.3.1.1 style=font-size:80%>Claude-3.5-Sonnet</span></td><td class="ltx_td ltx_align_center" id=A5.T5.6.3.2><span class=ltx_text id=A5.T5.6.3.2.1 style=font-size:80%>91.4</span></td><td class="ltx_td ltx_align_center" id=A5.T5.6.3.3><span class=ltx_text id=A5.T5.6.3.3.1 style=font-size:80%>88.4</span></td></tr><tr class=ltx_tr id=A5.T5.6.4><td class="ltx_td ltx_align_left ltx_border_bb" id=A5.T5.6.4.1><span class=ltx_text id=A5.T5.6.4.1.1 style=font-size:80%>Gemini-1.5-Pro</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=A5.T5.6.4.2><span class=ltx_text id=A5.T5.6.4.2.1 style=font-size:80%>88.5</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=A5.T5.6.4.3><span class=ltx_text id=A5.T5.6.4.3.1 style=font-size:80%>82.4</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the results of evaluating various large language models (LLMs) on a naive version of the MicroVQA benchmark. The naive version uses a simple, zero-shot prompting method for generating multiple-choice questions (MCQs) directly from the raw visual question answering (VQA) pairs, without any optimization or refinement. The table shows the overall performance of each LLM on the benchmark, along with their performance when the image is excluded from the input. This comparison helps to assess the models&rsquo; reliance on visual information versus language-only reasoning.</p><details><summary>read the caption</summary>Table 5: Performance on MicroVQA MCQs with naive MCQ generation. We report overall VQA and VQA without the image.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=A6.T6.2><tr class=ltx_tr id=A6.T6.2.1><td class="ltx_td ltx_align_left ltx_border_t" id=A6.T6.2.1.1><span class="ltx_text ltx_font_bold" id=A6.T6.2.1.1.1 style=font-size:80%>Model Name</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=A6.T6.2.1.2><span class="ltx_text ltx_font_bold" id=A6.T6.2.1.2.1 style=font-size:80%>API Endpoint</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=A6.T6.2.1.3><span class="ltx_text ltx_font_bold" id=A6.T6.2.1.3.1 style=font-size:80%>Source & Details</span></td></tr><tr class=ltx_tr id=A6.T6.2.2><td class="ltx_td ltx_align_left ltx_border_t" id=A6.T6.2.2.1><span class=ltx_text id=A6.T6.2.2.1.1 style=font-size:80%>o1</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=A6.T6.2.2.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.2.2.1 style=font-size:80%>o1-2024-12-17</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=A6.T6.2.2.3><span class=ltx_text id=A6.T6.2.2.3.1 style=font-size:80%>OpenAI API</span></td></tr><tr class=ltx_tr id=A6.T6.2.3><td class="ltx_td ltx_align_left" id=A6.T6.2.3.1><span class=ltx_text id=A6.T6.2.3.1.1 style=font-size:80%>Claude-3.5-Sonnet</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.3.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.3.2.1 style=font-size:80%>*anthropic/claude-3.5-sonnet-20240620</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.3.3><span class=ltx_text id=A6.T6.2.3.3.1 style=font-size:80%>Openrouter API</span></td></tr><tr class=ltx_tr id=A6.T6.2.4><td class="ltx_td ltx_align_left" id=A6.T6.2.4.1><span class=ltx_text id=A6.T6.2.4.1.1 style=font-size:80%>Gemini-Pro-1.5</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.4.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.4.2.1 style=font-size:80%>google/gemini-pro-1.5</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.4.3><span class=ltx_text id=A6.T6.2.4.3.1 style=font-size:80%>Openrouter API</span></td></tr><tr class=ltx_tr id=A6.T6.2.5><td class="ltx_td ltx_align_left" id=A6.T6.2.5.1><span class=ltx_text id=A6.T6.2.5.1.1 style=font-size:80%>Pixtral-Large</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.5.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.5.2.1 style=font-size:80%>mistralai/pixtral-large-2411</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.5.3><span class=ltx_text id=A6.T6.2.5.3.1 style=font-size:80%>Openrouter API</span></td></tr><tr class=ltx_tr id=A6.T6.2.6><td class="ltx_td ltx_align_left" id=A6.T6.2.6.1><span class=ltx_text id=A6.T6.2.6.1.1 style=font-size:80%>Grok-2-Vision</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.6.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.6.2.1 style=font-size:80%>x-ai/grok-2-vision-1212</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.6.3><span class=ltx_text id=A6.T6.2.6.3.1 style=font-size:80%>Openrouter API</span></td></tr><tr class=ltx_tr id=A6.T6.2.7><td class="ltx_td ltx_align_left" id=A6.T6.2.7.1><span class=ltx_text id=A6.T6.2.7.1.1 style=font-size:80%>Qwen-2-vl-72b-Instruct</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.7.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.7.2.1 style=font-size:80%>qwen/qwen-2-vl-72b-instruct</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.7.3><span class=ltx_text id=A6.T6.2.7.3.1 style=font-size:80%>Openrouter API</span></td></tr><tr class=ltx_tr id=A6.T6.2.8><td class="ltx_td ltx_align_left" id=A6.T6.2.8.1><span class=ltx_text id=A6.T6.2.8.1.1 style=font-size:80%>VILA1.5-40b</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.8.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.8.2.1 style=font-size:80%>VILA1.5-40b</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.8.3><span class=ltx_text id=A6.T6.2.8.3.1 style=font-size:80%>HuggingFace, local inference</span></td></tr><tr class=ltx_tr id=A6.T6.2.9><td class="ltx_td ltx_align_left" id=A6.T6.2.9.1><span class=ltx_text id=A6.T6.2.9.1.1 style=font-size:80%>GPT-4o</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.9.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.9.2.1 style=font-size:80%>gpt-4o-2024-08-06</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.9.3><span class=ltx_text id=A6.T6.2.9.3.1 style=font-size:80%>OpenAI API</span></td></tr><tr class=ltx_tr id=A6.T6.2.10><td class="ltx_td ltx_align_left" id=A6.T6.2.10.1><span class=ltx_text id=A6.T6.2.10.1.1 style=font-size:80%>Llama-3.1-Nemotron-70b-Instruct</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.10.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.10.2.1 style=font-size:80%>nvidia/llama-3.1-nemotron-70b-instruct</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.10.3><span class=ltx_text id=A6.T6.2.10.3.1 style=font-size:80%>Openrouter API</span></td></tr><tr class=ltx_tr id=A6.T6.2.11><td class="ltx_td ltx_align_left" id=A6.T6.2.11.1><span class=ltx_text id=A6.T6.2.11.1.1 style=font-size:80%>Llama-3.2-90b-Vision-Instruct</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.11.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.11.2.1 style=font-size:80%>meta-llama/llama-3.2-90b-vision-instruct</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.11.3><span class=ltx_text id=A6.T6.2.11.3.1 style=font-size:80%>Openrouter API</span></td></tr><tr class=ltx_tr id=A6.T6.2.12><td class="ltx_td ltx_align_left" id=A6.T6.2.12.1><span class=ltx_text id=A6.T6.2.12.1.1 style=font-size:80%>Qwen-2-VL-7b</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.12.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.12.2.1 style=font-size:80%>qwen/qwen-2-vl-7b-instruct</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.12.3><span class=ltx_text id=A6.T6.2.12.3.1 style=font-size:80%>Openrouter API</span></td></tr><tr class=ltx_tr id=A6.T6.2.13><td class="ltx_td ltx_align_left" id=A6.T6.2.13.1><span class=ltx_text id=A6.T6.2.13.1.1 style=font-size:80%>Claude-3.5-Haiku</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.13.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.13.2.1 style=font-size:80%>anthropic/claude-3.5-haiku</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.13.3><span class=ltx_text id=A6.T6.2.13.3.1 style=font-size:80%>Openrouter API</span></td></tr><tr class=ltx_tr id=A6.T6.2.14><td class="ltx_td ltx_align_left" id=A6.T6.2.14.1><span class=ltx_text id=A6.T6.2.14.1.1 style=font-size:80%>Gemini-Flash-1.5-8b</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.14.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.14.2.1 style=font-size:80%>google/gemini-flash-1.5-8b</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.14.3><span class=ltx_text id=A6.T6.2.14.3.1 style=font-size:80%>Openrouter API</span></td></tr><tr class=ltx_tr id=A6.T6.2.15><td class="ltx_td ltx_align_left" id=A6.T6.2.15.1><span class=ltx_text id=A6.T6.2.15.1.1 style=font-size:80%>GPT-4o-mini</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.15.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.15.2.1 style=font-size:80%>gpt-4o-mini-2024-07-18</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.15.3><span class=ltx_text id=A6.T6.2.15.3.1 style=font-size:80%>OpenAI API</span></td></tr><tr class=ltx_tr id=A6.T6.2.16><td class="ltx_td ltx_align_left" id=A6.T6.2.16.1><span class=ltx_text id=A6.T6.2.16.1.1 style=font-size:80%>Pixtral-12b</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.16.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.16.2.1 style=font-size:80%>mistralai/pixtral-12b</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.16.3><span class=ltx_text id=A6.T6.2.16.3.1 style=font-size:80%>Openrouter API</span></td></tr><tr class=ltx_tr id=A6.T6.2.17><td class="ltx_td ltx_align_left" id=A6.T6.2.17.1><span class=ltx_text id=A6.T6.2.17.1.1 style=font-size:80%>VILA1.5-13b</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.17.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.17.2.1 style=font-size:80%>VILA1.5-13b</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.17.3><span class=ltx_text id=A6.T6.2.17.3.1 style=font-size:80%>HuggingFace, local inference</span></td></tr><tr class=ltx_tr id=A6.T6.2.18><td class="ltx_td ltx_align_left" id=A6.T6.2.18.1><span class=ltx_text id=A6.T6.2.18.1.1 style=font-size:80%>Llama-3.2-11b-vision-instruct</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.18.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.18.2.1 style=font-size:80%>meta-llama/llama-3.2-11b-vision-instruct</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.18.3><span class=ltx_text id=A6.T6.2.18.3.1 style=font-size:80%>Openrouter API</span></td></tr><tr class=ltx_tr id=A6.T6.2.19><td class="ltx_td ltx_align_left" id=A6.T6.2.19.1><span class=ltx_text id=A6.T6.2.19.1.1 style=font-size:80%>LLaVA-Med-Mistral-7B</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.19.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.19.2.1 style=font-size:80%>LLaVA-Med-Mistral-7B</span></td><td class="ltx_td ltx_align_left" id=A6.T6.2.19.3><span class=ltx_text id=A6.T6.2.19.3.1 style=font-size:80%>HuggingFace, local inference</span></td></tr><tr class=ltx_tr id=A6.T6.2.20><td class="ltx_td ltx_align_left ltx_border_b" id=A6.T6.2.20.1><span class=ltx_text id=A6.T6.2.20.1.1 style=font-size:80%>LLaVA-Mistral-7B</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=A6.T6.2.20.2><span class="ltx_text ltx_font_typewriter" id=A6.T6.2.20.2.1 style=font-size:80%>LLaVA-Mistral-7B</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=A6.T6.2.20.3><span class=ltx_text id=A6.T6.2.20.3.1 style=font-size:80%>HuggingFace, local inference</span></td></tr></table></table></figure><blockquote><p>üîº This table lists the specific versions of the large language models (LLMs) used in the experiments section of the paper. It provides the API endpoint used to access each model, along with the source and details about where the endpoint is hosted (e.g., OpenAI API, HuggingFace, local inference). This information is crucial for reproducibility, allowing researchers to easily replicate the experiments using the same model versions.</p><details><summary>read the caption</summary>Table 6: AI model API endpoints and their sources</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=A6.T7.4><tr class=ltx_tr id=A6.T7.4.1><td class="ltx_td ltx_border_tt" id=A6.T7.4.1.1></td><td class="ltx_td ltx_border_r ltx_border_tt" id=A6.T7.4.1.2></td><td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" colspan=2 id=A6.T7.4.1.3>No-image ablation</td><td class="ltx_td ltx_align_left ltx_border_tt" colspan=2 id=A6.T7.4.1.4>Choices-only ablation</td></tr><tr class=ltx_tr id=A6.T7.4.2><td class=ltx_td id=A6.T7.4.2.1></td><td class="ltx_td ltx_align_left ltx_border_r" id=A6.T7.4.2.2>Accuracy</td><td class="ltx_td ltx_align_left" id=A6.T7.4.2.3>Accuracy</td><td class="ltx_td ltx_align_left ltx_border_r" id=A6.T7.4.2.4>Drop</td><td class="ltx_td ltx_align_left" id=A6.T7.4.2.5>Accuracy</td><td class="ltx_td ltx_align_left" id=A6.T7.4.2.6>diff</td></tr><tr class=ltx_tr id=A6.T7.4.3><td class="ltx_td ltx_align_left ltx_border_t" id=A6.T7.4.3.1>o1</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=A6.T7.4.3.2>52.8</td><td class="ltx_td ltx_align_center ltx_border_t" id=A6.T7.4.3.3>49.2</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=A6.T7.4.3.4>-3.6</td><td class="ltx_td ltx_align_center ltx_border_t" id=A6.T7.4.3.5>37.7</td><td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id=A6.T7.4.3.6>-15.1</td></tr><tr class=ltx_tr id=A6.T7.4.4><td class="ltx_td ltx_align_left" id=A6.T7.4.4.1>Claude-3.5-Sonnet</td><td class="ltx_td ltx_align_center ltx_border_r" id=A6.T7.4.4.2>51.7</td><td class="ltx_td ltx_align_center" id=A6.T7.4.4.3>46.0</td><td class="ltx_td ltx_align_center ltx_border_r" id=A6.T7.4.4.4>-5.8</td><td class="ltx_td ltx_align_center" id=A6.T7.4.4.5>44.0</td><td class="ltx_td ltx_nopad_r ltx_align_center" id=A6.T7.4.4.6>-7.7</td></tr><tr class=ltx_tr id=A6.T7.4.5><td class="ltx_td ltx_align_left" id=A6.T7.4.5.1>Gemini-Pro-1.5</td><td class="ltx_td ltx_align_center ltx_border_r" id=A6.T7.4.5.2>51.1</td><td class="ltx_td ltx_align_center" id=A6.T7.4.5.3>47.2</td><td class="ltx_td ltx_align_center ltx_border_r" id=A6.T7.4.5.4>-3.8</td><td class="ltx_td ltx_align_center" id=A6.T7.4.5.5>36.8</td><td class="ltx_td ltx_nopad_r ltx_align_center" id=A6.T7.4.5.6>-14.3</td></tr><tr class=ltx_tr id=A6.T7.4.6><td class="ltx_td ltx_align_left" id=A6.T7.4.6.1>Pixtral-Large</td><td class="ltx_td ltx_align_center ltx_border_r" id=A6.T7.4.6.2>49.8</td><td class="ltx_td ltx_align_center" id=A6.T7.4.6.3>46.3</td><td class="ltx_td ltx_align_center ltx_border_r" id=A6.T7.4.6.4>-3.6</td><td class="ltx_td ltx_align_center" id=A6.T7.4.6.5>36.7</td><td class="ltx_td ltx_nopad_r ltx_align_center" id=A6.T7.4.6.6>-13.1</td></tr><tr class=ltx_tr id=A6.T7.4.7><td class="ltx_td ltx_align_left" id=A6.T7.4.7.1>Grok-2-Vision</td><td class="ltx_td ltx_align_center ltx_border_r" id=A6.T7.4.7.2>48.4</td><td class="ltx_td ltx_align_center" id=A6.T7.4.7.3>46.3</td><td class="ltx_td ltx_align_center ltx_border_r" id=A6.T7.4.7.4>-2.1</td><td class="ltx_td ltx_align_center" id=A6.T7.4.7.5>40.5</td><td class="ltx_td ltx_nopad_r ltx_align_center" id=A6.T7.4.7.6>-7.9</td></tr><tr class=ltx_tr id=A6.T7.4.8><td class="ltx_td ltx_align_left" id=A6.T7.4.8.1>GPT-4o-mini</td><td class="ltx_td ltx_align_center ltx_border_r" id=A6.T7.4.8.2>46.2</td><td class="ltx_td ltx_align_center" id=A6.T7.4.8.3>44.2</td><td class="ltx_td ltx_align_center ltx_border_r" id=A6.T7.4.8.4>-1.9</td><td class="ltx_td ltx_align_center" id=A6.T7.4.8.5>34.3</td><td class="ltx_td ltx_nopad_r ltx_align_center" id=A6.T7.4.8.6>-11.9</td></tr><tr class=ltx_tr id=A6.T7.4.9><td class="ltx_td ltx_align_left" id=A6.T7.4.9.1>Pixtral-12b</td><td class="ltx_td ltx_align_center ltx_border_r" id=A6.T7.4.9.2>45.6</td><td class="ltx_td ltx_align_center" id=A6.T7.4.9.3>43.7</td><td class="ltx_td ltx_align_center ltx_border_r" id=A6.T7.4.9.4>-1.9</td><td class="ltx_td ltx_align_center" id=A6.T7.4.9.5>31.8</td><td class="ltx_td ltx_nopad_r ltx_align_center" id=A6.T7.4.9.6>-13.8</td></tr><tr class=ltx_tr id=A6.T7.4.10><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=A6.T7.4.10.1>Random</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id=A6.T7.4.10.2>22.0</td><td class="ltx_td ltx_border_bb ltx_border_t" id=A6.T7.4.10.3></td><td class="ltx_td ltx_border_bb ltx_border_r ltx_border_t" id=A6.T7.4.10.4></td><td class="ltx_td ltx_border_bb ltx_border_t" id=A6.T7.4.10.5></td><td class="ltx_td ltx_nopad_r ltx_border_bb ltx_border_t" id=A6.T7.4.10.6></td></tr></table></table></figure><blockquote><p>üîº This ablation study analyzes the impact of removing visual and textual information from the MicroVQA benchmark on model performance. The table shows the overall accuracy of various models, along with their accuracy when either the image is removed (no-image ablation), or both the image and question text are removed (choices-only ablation). The &lsquo;drop&rsquo; columns show the reduction in accuracy compared to the overall performance, highlighting the relative importance of visual and textual information in solving the MicroVQA tasks.</p><details><summary>read the caption</summary>Table 7: Ablation study on MicroVQA MCQs. Column 2 is overall accuracy. Cols 3-4 are the no-image ablation accuracy and drop compared to overall accuracy. Cols 5-6 are the choices-only ablation accuracy and drop compared to overall accuracy. See the text for discussion.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-8214ab239398d52a107acf9c59e70bd7 class=gallery><img src=https://ai-paper-reviewer.com/2503.13399/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.13399/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13399/&amp;title=MicroVQA:%20A%20Multimodal%20Reasoning%20Benchmark%20for%20Microscopy-Based%20Scientific%20Research" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13399/&amp;text=MicroVQA:%20A%20Multimodal%20Reasoning%20Benchmark%20for%20Microscopy-Based%20Scientific%20Research" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13399/&amp;subject=MicroVQA:%20A%20Multimodal%20Reasoning%20Benchmark%20for%20Microscopy-Based%20Scientific%20Research" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2503.13399/index.md",oid_likes="likes_paper-reviews/2503.13399/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2503.13360/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-03-17T00:00:00+00:00>17 March 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2503.13424/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated Objects via Procedural Generation</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-03-17T00:00:00+00:00>17 March 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>