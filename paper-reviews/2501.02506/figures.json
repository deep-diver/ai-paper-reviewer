[{"figure_path": "https://arxiv.org/html/2501.02506/x1.png", "caption": "Figure 1: An illustration of multi-hop tool use. The process entails decomposing a complex multi-hop query into multiple atomic sub-queries, sequentially invoking the appropriate tools, retrieving results from the tool feedback, and iterating until the final answer is derived. This demonstrates the integration of comprehension, reasoning, and function-calling capabilities.", "description": "This figure illustrates the multi-hop tool use process.  A complex user query is broken down into smaller, individual sub-queries. Each sub-query is then processed by invoking an appropriate tool. The tool's response (feedback) is used to inform subsequent sub-query processing, creating an iterative cycle. This continues until all sub-queries are answered and the final answer to the original, complex query is obtained. The process highlights the need for comprehension, reasoning, and the ability to properly call and manage external tools (functions).", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.02506/x2.png", "caption": "Figure 2: An illustration of our proposed query-driven data construction scheme, comprising three key processes: tool creation, document refinement, and code generation. This approach incrementally develops detailed tool document and code implementation for each atomic subquery within a multi-hop query.", "description": "This figure illustrates the three-step query-driven data construction process used to create the ToolHop dataset.  First, a multi-hop query is decomposed into individual atomic subqueries. Then, for each atomic subquery, a tool is created, with its corresponding documentation refined to provide detailed descriptions, parameters, and specifications. Finally, code is generated for each tool to ensure local execution and verification of outputs. This iterative approach creates a comprehensive dataset for evaluating large language models' multi-hop tool-use abilities.", "section": "2.2 Query-Driven Data Construction"}, {"figure_path": "https://arxiv.org/html/2501.02506/x3.png", "caption": "Figure 3: Distribution of user queries across 47 domains in the ToolHop dataset.", "description": "This figure shows the distribution of 995 user queries from the ToolHop dataset across 47 different domains.  The x-axis represents the different domains (e.g., film, genealogy, computing, etc.), and the y-axis shows the number of queries within each domain. The bar chart visually represents the frequency of queries in each domain, giving insights into the diversity of topics covered in the ToolHop dataset.  This demonstrates that ToolHop covers a wide range of topics and is not limited to a specific niche.", "section": "2.3 Dataset Analysis"}, {"figure_path": "https://arxiv.org/html/2501.02506/x4.png", "caption": "Figure 4: Distribution of the number of tool parameters before and after document refinement.", "description": "This figure shows the distribution of the number of tool parameters before and after the document refinement process.  The x-axis represents the number of parameters, and the y-axis represents the number of tools.  The bars visually compare the number of tools with different parameter counts before and after refinement, highlighting the impact of the refinement process on tool complexity.", "section": "2.3 Dataset Analysis"}, {"figure_path": "https://arxiv.org/html/2501.02506/x5.png", "caption": "Figure 5: Distribution of tool parameter types before and after document refinement.", "description": "This figure shows a comparison of the distribution of different parameter types used in tools before and after the document refinement process.  Before refinement, a larger proportion of parameters are simple strings.  After refinement, there is a shift toward more complex and structured parameter types such as arrays, booleans, integers, and objects, indicating an increase in the complexity and richness of the tools.", "section": "2.3 Dataset Analysis"}, {"figure_path": "https://arxiv.org/html/2501.02506/x6.png", "caption": "Figure 6: Distribution of answer types for the second atomic subquery and final answers in ToolHop.", "description": "This figure shows the distribution of different answer types across all queries in the ToolHop dataset.  It breaks down the answer types for both the second atomic subquery (an intermediate step in a multi-hop query) and the final answer to the complete multi-hop query. The visualization helps understand the diversity of answer types handled by the dataset and the complexity of reasoning involved in answering the multi-hop queries.", "section": "2.3 Dataset Analysis"}, {"figure_path": "https://arxiv.org/html/2501.02506/x7.png", "caption": "Figure 7: The Qwen2.5 family of LLMs emphasizes parallel tool calls in the mandatory tool use scenario, which can lead to hallucinations and incorrect answers.", "description": "The figure shows an example of how the Qwen2.5 family of large language models (LLMs) performs in a multi-hop tool use scenario. In this scenario, the models are required to use multiple tools to arrive at the correct answer. However, the Qwen2.5 models make parallel tool calls, which can lead to incorrect results due to hallucinations. The example illustrates how the model's parallel processing strategy results in errors, despite the potential benefits of efficiency.  This highlights the challenges involved in handling complex tool-use scenarios and underscores the limitations of parallel processing in such contexts.", "section": "4 Main Results"}, {"figure_path": "https://arxiv.org/html/2501.02506/x8.png", "caption": "Figure 8: The Claude 3.5 family of LLMs optimizes CoT reasoning in the direct answer scenario, enhancing their analytical and problem-solving capabilities.\"", "description": "Figure 8 demonstrates the Claude 3.5 family of LLMs' superior performance in direct answer scenarios.  Unlike other models, Claude 3.5 models naturally employ chain-of-thought (CoT) reasoning to break down complex problems into smaller, manageable steps, ultimately leading to more accurate and insightful solutions. This highlights the model's advanced analytical and problem-solving skills, even without explicit prompting for CoT reasoning.  The figure visually represents this process.", "section": "4 Main Results"}, {"figure_path": "https://arxiv.org/html/2501.02506/x9.png", "caption": "Figure 9: The GPT family of LLMs improves performance by refining calling behavior through the use of detailed tool feedback.", "description": "This figure shows how the GPT family of large language models (LLMs) improves its performance by learning to refine its tool-calling behavior through the use of detailed feedback. The example shows a multi-hop query: \"What is the first letter of the first name of the father of the director of film Little Joe (Film) in lowercase?\" The model initially makes an error when calling the family_relationship_finder tool because it's missing a required argument. However, after receiving detailed feedback, the model successfully corrects its error, and produces the correct answer. This demonstrates the model's ability to learn from detailed feedback and improve its performance in handling tool calls.", "section": "4 Main Results"}, {"figure_path": "https://arxiv.org/html/2501.02506/x10.png", "caption": "Figure 10: The GPT fmaily of LLMs struggles to correct their calling behavior when provided with minimal feedback.", "description": "The figure shows the GPT family of large language models (LLMs) failing to correct their tool-calling behavior when provided with only minimal feedback.  This highlights the importance of detailed feedback in enabling effective tool use by LLMs.  The example demonstrates a scenario where an LLM requests information about the father of a film director. When the tool fails, returning only a generic error, the LLM is unable to recover and provide the correct response.", "section": "4.2 Evaluation Observations"}]