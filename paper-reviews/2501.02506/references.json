{"references": [{"fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "publication_date": "2023-09-16", "reason": "This paper introduces Qwen, a large language model family, whose performance is evaluated in the ToolHop benchmark, making it a key reference for understanding the models being compared."}, {"fullname_first_author": "Yuntao Bai", "paper_title": "Constitutional AI: harmlessness from AI feedback", "publication_date": "2022-12-08", "reason": "The Constitutional AI model is mentioned as a relevant model in the introduction, highlighting its influence in shaping the field of large language models and providing context for the current research."}, {"fullname_first_author": "Zehui Chen", "paper_title": "T-eval: Evaluating the tool utilization capability of large language models step by step", "publication_date": "2024-08-11", "reason": "T-eval is cited as a related work focusing on evaluating tool use in LLMs, providing a comparative approach to the proposed method in ToolHop and demonstrating the significance of rigorous evaluation within the field."}, {"fullname_first_author": "Hyung Won Chung", "paper_title": "Scaling instruction-finetuned language models", "publication_date": "2022-10-11", "reason": "This paper discusses the scaling laws for instruction-finetuned language models, a topic highly relevant to the performance analysis of various LLMs in ToolHop and their varying capabilities in handling multi-hop tool-use scenarios."}, {"fullname_first_author": "Robert A. Jacobs", "paper_title": "Adaptive mixtures of local experts", "publication_date": "1991-01-01", "reason": "This paper introduces the Mixture of Experts architecture which is used by the Gemini family of LLMs, making it a critical reference for understanding the model architecture and its influence on performance."}]}