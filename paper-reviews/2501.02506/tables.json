[{"content": "| # Tools | Three | Four | Five | Six | Seven |\n|---|---|---|---|---|---| \n| **# Data** | 428 | 353 | 136 | 10 | 68 |", "caption": "Table 1: Distribution of the number of tools required to solve each query in ToolHop.", "description": "This table shows the distribution of the number of tools needed to answer each question in the ToolHop dataset.  It breaks down how many queries required three, four, five, six or seven tools to arrive at a solution. This data provides insights into the complexity of the queries within the dataset and the level of multi-hop reasoning required.", "section": "2.3 Dataset Analysis"}, {"content": "| Refinement | Zero | One | Two | Three | Four |\n|---|---|---|---|---|---| \n| Before | 2 | 2433 | 1250 | 202 | 25 |\n| After | 2 | 2490 | 1198 | 200 | 22 |", "caption": "Table 2: Distribution of the number of required parameters before and after document refinement.", "description": "This table presents a comparison of the number of required parameters in tool documents before and after a refinement process.  The refinement process aims to improve the quality and complexity of the tools. By showing the distribution of the number of required parameters (0, 1, 2, 3, 4, etc.), we can understand how the refinement impacts the complexity of tools used in multi-hop reasoning tasks.  The 'Before' column shows the parameter count before refinement, and 'After' shows the count after refinement. The numbers represent the frequencies of tool documents with that many required parameters.", "section": "2.3 Dataset Analysis"}, {"content": "| Refinement | string | boolean | array | integer | object | number |\n|---|---|---|---|---|---|---|\n| Before | 4758 | 2 | 404 | 333 | 24 | 114 |\n| After | 4473 | 2 | 755 | 241 | 44 | 102 |", "caption": "Table 3: Distribution of required tool parameter types before and after refinement.", "description": "This table shows the number and types of parameters required by the tools in the ToolHop dataset before and after the document refinement process.  It illustrates how the refinement process increased the complexity of the tools by adding more parameters and using more diverse parameter types (e.g., changing simple strings to arrays or objects). The table helps to demonstrate the impact of the refinement process on the overall complexity and versatility of the tools.", "section": "2.3 Dataset Analysis"}, {"content": "| Source | Family | Version | Direct | Mandatory | Free | Query | Instance |\n|---|---|---|---|---|---|---|---|---|\n|  |  | *Avg.* | *19.83* | *32.12* | *32.84* | *18.72* | *8.68* |\n| Open-Source | LLaMA3.1 | Instruct-8B | 13.17 | 12.76 | 13.47 | 41.61 | 21.10 |\n|  |  | Instruct-70B | 18.79 | 19.10 | 12.76 | 35.08 | 14.24 |\n|  | Qwen2.5 | Instruct-7B | 11.46 | 9.85 | 16.18 | 28.84 | 7.09 |\n|  |  | Instruct-14B | 17.39 | 26.38 | 26.13 | 15.78 | 6.82 |\n|  |  | Instruct-32B | 20.00 | 25.03 | 22.61 | 12.46 | 3.46 |\n|  |  | Instruct-72B | 17.89 | 45.43 | 38.29 | 13.27 | 4.93 |\n| Closed-Source | Gemini1.5 | flash-002 | 18.59 | 29.35 | 32.76 | 13.59 | 6.69 |\n|  |  | pro-002 | 18.89 | 31.16 | 33.07 | 14.57 | 6.61 |\n|  | Claude3.5 | Haiku | 36.08 | 38.09 | 44.72 | 23.48 | 15.81 |\n|  |  | Sonnet | 27.14 | 39.90 | 45.23 | 19.60 | 15.83 |\n|  | GPT | 3.5-Turbo | 17.09 | 35.38 | 36.58 | 11.76 | 6.03 |\n|  |  | 4o-mini | 19.40 | 40.20 | 43.42 | 11.66 | 3.58 |\n|  |  | 4-Turbo | 18.59 | 47.94 | 46.83 | 10.95 | 4.97 |\n|  |  | 4o | 23.12 | 49.04 | 47.74 | 9.45 | 4.31 |", "caption": "Table 4: Performance of various LLMs on ToolHop, including answer correctness and invocation error. \u2018Direct,\u2019 \u2018Mandatory,\u2019 and \u2018Free\u2019 denote the direct answer, mandatory tool use, and free choice scenarios, respectively. \u2018Query\u2019 and \u2018Instance\u2019 refer to the percentage of queries and tool invocation instances with errors, respectively. \u2018Avg.\u2019 represents the average across all LLMs. Values above the average are highlighted in teal, and those below are highlighted in maroon, with darker shades indicating larger deviations.", "description": "This table presents a comprehensive evaluation of 14 Large Language Models (LLMs) across five families on the ToolHop benchmark.  It compares their performance in three scenarios: solving queries directly (Direct), mandatorily using tools (Mandatory), and freely choosing to use tools (Free).  The results show answer correctness rates and invocation error rates (both at the query and instance level).  Color-coding highlights performances above and below the average, with darker shades indicating larger deviations from the average.  This allows for detailed analysis of model performance across different approaches to tool usage, revealing strengths and weaknesses of various LLM families.", "section": "4 Main Results"}, {"content": "| Version | w/ Feedback | w/o Feedback | **\u0394<sub>C\u2192I</sub>** | **\u0394<sub>I\u2192C</sub>** |\n|---|---|---|---|---|\n| 3.5-Turbo | 36.75 | 21.37 | 20.51 | 5.13 |\n| 4o-mini | 38.53 | 11.93 | 29.36 | 2.75 |\n| 4-Turbo | 29.31 | 12.07 | 17.24 | 0.00 |\n| 4o | 47.87 | 24.47 | 25.53 | 2.13 |", "caption": "Table 5: Answer correctness of the GPT family of models in queries containing invocation error. \u2018w/ Feedback\u2019 and \u2018w/o Feedback\u2019 represent cases where detailed feedback or only simple error reporting is provided, respectively. \u2018\ud835\udeab\ud835\udc02\u2192\ud835\udc08subscript\ud835\udeab\u2192\ud835\udc02\ud835\udc08\\mathbf{\\Delta_{C\\to I}}bold_\u0394 start_POSTSUBSCRIPT bold_C \u2192 bold_I end_POSTSUBSCRIPT\u2019 denotes the proportion of correct answers that become incorrect, while \u2018\ud835\udeab\ud835\udc08\u2192\ud835\udc02subscript\ud835\udeab\u2192\ud835\udc08\ud835\udc02\\mathbf{\\Delta_{I\\to C}}bold_\u0394 start_POSTSUBSCRIPT bold_I \u2192 bold_C end_POSTSUBSCRIPT\u2019 represents the proportion of incorrect answers that become correct, when transitioning from detailed feedback to simple error reporting.", "description": "This table presents the impact of detailed feedback on the performance of GPT family models when dealing with queries that resulted in tool invocation errors.  It shows the answer correctness when detailed feedback is provided and when only minimal error feedback is offered.  The changes in accuracy are also presented, showing how many initially correct answers became incorrect (\u0394C\u2192I) and how many initially incorrect answers became correct (\u0394I\u2192C) when the feedback was changed from detailed to minimal.", "section": "4.2 Evaluation Observations"}, {"content": "| Steps | Description |\n|---|---| \n| 1. Analyze the Problem | Understand the question and determine the type of information required to answer it. |\n| 2. Tool Design | Design a tool that can solve the problem, considering the complexity and additional functionalities it might need. |\n| 3. Parameter Specification | Define the parameters for the tool, ensuring they are comprehensive and flexible for various use cases. |\n| 4. Output Construction | Format the output in JSON, including both the analysis and the tool schema. |\n| Notes | - Ensure the tool is versatile enough to handle similar queries for different sports figures. <br> - Consider edge cases. |\n| Output Format | The output should be a JSON object with the following structure **without any other contents**: <br> - \"analysis\": A detailed analysis of the ideas behind the tool design. <br> - \"tool\": A JSON schema characterizing the tool, including its name, description, and parameters. |", "caption": "Table 6: The prompt for tool creation, where \u2018{Example}\u2019 and \u2018{Question}\u2019 represent the example and subquery, respectively.", "description": "This table details the prompt used for the tool creation stage in the ToolHop dataset construction.  The prompt instructs the model to analyze a given problem, design a tool to solve it, specify the tool's parameters, and format the output as a JSON object containing both the analysis and the tool schema.  The '{Example}' and '{Question}' placeholders within the prompt indicate that example data and a subquery will be provided to guide the model's response.", "section": "2.2 Query-Driven Data Construction"}, {"content": "{\"analysis\": \"Analysis of ideas about refining the tool.\", \"refined_version\": {}}", "caption": "Table 7: The prompt for document refinement, where \u2018{Tool}\u2019 represents the preliminary document.", "description": "This table describes the prompt used in the query-driven data construction process for refining the preliminary tool documents.  The prompt guides the process of enhancing the tool's description, adding or refining parameters to increase complexity and utility, while maintaining compatibility with the original tool's functionality. It outlines steps for analyzing the existing tool, identifying areas for improvement, refining the description, adjusting parameters, and verifying compatibility.", "section": "2.2 Query-Driven Data Construction"}, {"content": "| Step | Description |\n|---|---| \n| 1. | **Understand the Tool Document**: Review the tool document to identify the function name, parameter names, and types. |\n| 2. | **Analyze the Question and Answer**: Determine how the function should be used to answer the question. |\n| 3. | **Implement the Function**: - Use the tool name as the function name.  - Define parameters exactly as specified in the tool document. - Implement the function logic to produce the correct answer for the given question. - Simulate additional return values as specified in the tool document. |\n| 4. | **Error Handling**: Develop a robust error handling mechanism to return valid error messages for incorrect inputs or other issues. |\n| Note | - Ensure parameter types and names match exactly with the tool document. - Simulate additional return values as needed based on the tool\u2019s documentation. - Implement comprehensive error handling to cover potential issues. |\n| Output format | Output the result in JSON format with the following structure **without any other contents**: {\n\"analysis\": \"Detailed analysis of how the function was designed, including reasoning for parameter choices and exception handling.\",\n\"function\": \"The specific function design, including code and comments explaining each part.\"\n} |\n| Tool Document | {document} |\n| Question | {question} |\n| Answer | {answer} |", "caption": "Table 8: The prompt for code generation, where \u2018{document}\u2019, \u2018{question}\u2019 and \u2018{answer}\u2019 represent the refined document, the subquery and the corresponding answer, respectively.", "description": "Table 8 details the prompt used to generate code based on a refined tool document, a specific subquery, and its corresponding answer.  This prompt guides the process of creating functional code for the tools used in the ToolHop multi-hop query benchmark, ensuring the code accurately reflects the tool's specifications and handles various inputs appropriately. The prompt is structured to ensure that the generated code adheres strictly to the tool's specifications, including the function name, parameter names, types, and error handling mechanisms, enabling accurate evaluation of LLM capabilities in tool use.", "section": "2.2 Query-Driven Data Construction"}, {"content": "| Step | Description |\n|---|---| \n| 1. Analyze the Sentence | Break down the sentence to understand its components and context. |\n| 2. Identify Key Elements | Look for specific terms or phrases that indicate the subject matter, such as names, dates, or specific topics. |\n| 3. Determine the Domain | Based on the analysis, select the most appropriate domain that encapsulates the main focus of the sentence. |", "caption": "Table 9: The prompt for domain classification, where \u2018{sentence}\u2019 represents the multi-hop query.", "description": "This table details the prompt used for domain classification within the ToolHop dataset.  The prompt instructs GPT-40 to analyze a given sentence (the multi-hop query), identify key elements and determine the most appropriate domain to categorize the query's subject. The expected output is a JSON object with 'analysis' and 'domain' fields.", "section": "2.3 Dataset Analysis"}]