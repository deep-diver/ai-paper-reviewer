[{"figure_path": "https://arxiv.org/html/2502.13943/x1.png", "caption": "Figure 1: Rule-based reasoning step dividing (e.g., using line breaks or a fixed number of tokens) is automated but results in low informativeness at the end of the step and is difficult to apply in domains that hard to define rules. In contrast, manual step division provides high informativeness but is costly to scale and heavily reliant on the experts\u2019 domain knowledge. AdaptiveStep, which divides steps based on model confidence, addresses these challenges by offering automation, efficiency, high informativeness, and applicability across various domains.", "description": "This figure compares three different methods for dividing reasoning steps in a process reward model (PRM). Rule-based methods automate step division using simple cues like line breaks or fixed token counts.  However, these methods often result in steps that are uninformative, particularly towards the end, and are difficult to adapt to diverse domains where defining clear rules is challenging.  Manual division, on the other hand, produces highly informative steps but is expensive and relies heavily on expert knowledge, thus limiting scalability. AdaptiveStep, the proposed method, addresses these limitations by automatically dividing steps according to the model's confidence in predicting the next word.  This approach offers the benefits of automation and efficiency while producing informative steps applicable to a wide variety of domains.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13943/x2.png", "caption": "Figure 2: Method overview. a) ASPRM\u2006 Training Data Construction Pipeline. Step 1: Sample from the dataset of a given domain, collecting confidence scores and samples for the training data. Then, accumulate the confidence distribution of all samples and determine the threshold.\nStep 2: Divide reasoning steps based on the threshold and label the steps using rollout. b) The difference between Rule-based method and AdaptiveStep division.\nThe Rule-based method divides the reasoning process using predefined symbols or fixed token counts (e.g., line breaks, as shown in the figure), while AdaptiveStep divides reasoning steps based on model confidence.\nWe observe that the model tends to divide reasoning steps at key decision points, such as within mathematical expressions, at noun selections, and when determining the final answer. In contrast, we find that the confidence at line breaks is particularly high.", "description": "Figure 2 illustrates the AdaptiveStep process.  Panel (a) details the training data construction pipeline. First, samples are drawn from a dataset, and model confidence scores are calculated.  Then, a threshold is determined from the aggregate confidence distribution, and reasoning steps are divided based on this threshold.  Each step is labeled using rollout.  Panel (b) compares AdaptiveStep with rule-based methods for step division. Rule-based methods use predefined symbols or fixed token lengths (e.g., line breaks).  In contrast, AdaptiveStep divides steps based on model confidence. The figure shows that AdaptiveStep identifies critical breaking points in reasoning (mathematical expressions, noun selections, final answer decisions), unlike rule-based methods, which show higher confidence at artificial boundaries like line breaks.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2502.13943/x3.png", "caption": "Figure 3: We illustrate Token-level Value-guided Decoding (TVD) with a simple example. The green token denotes the selected tokens, while the gray token indicates the tokens that were not selected. The question is 3 * (1 + 1) = ?, and the correct output is 6. In this case, the model exhibits low confidence (where cy<\u03c4subscript\ud835\udc50\ud835\udc66\ud835\udf0fc_{y}<\\tauitalic_c start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT < italic_\u03c4) when calculating the result of\n1+1, and subsequently determines which number to multiply by 3. The PRM should select the best token based on its judgment to arrive at the correct final answer. As shown in the top-left corner, for each token, the middle box represents the token itself, the bottom box shows the predicted confidence, and the box on the right displays the PRM score. The red confidence score indicates that the confidence of the Top-1 predicted candidate is lower than the threshold.", "description": "Figure 3 illustrates the Token-level Value-guided Decoding (TVD) process.  A simple example is used to show how the method works. The model generates tokens, and their predicted confidence scores and PRM scores are displayed.  Green tokens represent tokens selected by the TVD process, while gray tokens were not selected. The example problem is \"3 * (1 + 1) = ?\", with the correct answer being 6. The model's confidence is low (below threshold \u03c4) when calculating 1+1. This low confidence indicates a decision point, where the model needs to decide which number to multiply by 3. The PRM helps guide the model to select the best token (based on its calculated reward) to reach the correct final answer.  Red confidence scores highlight instances where the confidence of the top-predicted candidate falls below the threshold.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2502.13943/x4.png", "caption": "Figure 4: BoN results for the math tasks. We evaluate all PRMs on: (a) MetaMath-Llama generated GSM8k candidate solutions; (b) MetaMath-Mistral generated GSM8k candidates; (c) MetaMath-Llama generated MATH500 candidates; and (d) MetaMath-Mistral generated MATH500 candidates. The \u201d-L\u201d and \u201d-M\u201d suffixes denote the base models (Llama and Mistral, respectively). We report the evaluation results based on the released versions of other works.", "description": "Figure 4 presents the Best-of-N (BoN) performance results for various Process Reward Models (PRMs) on mathematical reasoning tasks.  The models were tested on two datasets, GSM8k and MATH500, with candidate solutions generated using two different LLMs, Llama and Mistral.  Subfigures (a) and (b) show the results for GSM8k using Llama and Mistral respectively, while subfigures (c) and (d) display the MATH500 results using Llama and Mistral respectively. The suffixes '-L' and '-M' denote the base models used (Llama and Mistral). The results shown are based on previously published work, using the released versions of those models.", "section": "4. Experiments and Analysis"}, {"figure_path": "https://arxiv.org/html/2502.13943/x5.png", "caption": "Figure 5: BoN results for the code datasets, we test ASPRM-D \u2006 and a Code-ORM (ORM-D) on (a) LCD-DS generated LeetCodeDataset BoN candidates; (b) LCD-DS generated LiveCodeBench BoN candidates.", "description": "Figure 5 presents the Best-of-N (BoN) evaluation results for code generation tasks using two datasets: LeetCodeDataset and LiveCodeBench.  Both datasets' BoN candidates were generated using the LCD-DS model.  The figure compares the performance of the AdaptiveStep Process Reward Model (ASPRM-D) with a baseline Outcome Reward Model (ORM-D).  Subfigure (a) shows the results for the LeetCodeDataset while subfigure (b) displays the results for the LiveCodeBench dataset.  This allows for a comparison of the two models across different datasets and provides insight into the effectiveness of ASPRM-D in code generation tasks.", "section": "4. Experiments and Analysis"}, {"figure_path": "https://arxiv.org/html/2502.13943/x6.png", "caption": "(a)", "description": "This figure presents the Best-of-N (BoN) results for mathematical reasoning tasks.  It displays the performance of several PRMs (Process Reward Models), including ASPRM (AdaptiveStep Process Reward Model) and baselines like Math-Shepherd and ER-PRM, across four different experimental setups.  The setups vary in the base LLMs used (Llama and Mistral) and the datasets used for evaluation (GSM8k and MATH500).  Each bar graph shows the BoN performance at different values of N (the number of candidates considered), indicating how the performance improves as more candidates are evaluated.", "section": "4. Experiments and Analysis"}, {"figure_path": "https://arxiv.org/html/2502.13943/x7.png", "caption": "(b)", "description": "This figure presents a comparison of the Best-of-N (BoN) evaluation results for the code generation task.  Specifically, it illustrates the performance of the AdaptiveStep Process Reward Model (ASPRM) against an Outcome Reward Model (ORM) baseline, using the BoN metric across different numbers of candidate solutions (Bo1, Bo4, Bo8, Bo16, Bo32, Bo64). The x-axis represents the number of candidates considered in the BoN evaluation, and the y-axis shows the accuracy (percentage of correctly solved problems). The results highlight the superior performance and robustness of ASPRM compared to the ORM, particularly as the number of considered candidates increases.  The graph is divided into two parts, (a) and (b), each showing a different dataset used for evaluation.", "section": "4. Experiments and Analysis"}, {"figure_path": "https://arxiv.org/html/2502.13943/x8.png", "caption": "(c)", "description": "This figure displays the results of the Best-of-N (BoN) evaluation for mathematical reasoning tasks, specifically on the MATH500 dataset.  It compares the performance of different PRMs (Process Reward Models), including the proposed ASPRM (AdaptiveStep Process Reward Model) and baselines such as Shepherd-MER-PRM and others. The x-axis represents different values of 'N' (the number of candidates considered in BoN), while the y-axis represents the accuracy achieved.  The figure shows how the ASPRM achieves state-of-the-art results in BoN evaluations for mathematical reasoning tasks.  The labels '-L' and '-M' indicate the base models used (Llama and Mistral, respectively).", "section": "4. Experiments and Analysis"}, {"figure_path": "https://arxiv.org/html/2502.13943/x9.png", "caption": "(d)", "description": "This figure displays the Best-of-N (BoN) results for mathematical reasoning tasks using the MetaMath-Mistral generated MATH500 candidate solutions.  The x-axis represents different values of N (the number of candidates considered), indicating the diversity of the solutions explored. The y-axis represents the accuracy achieved.  The plot compares the performance of several different PRMs, including ASPRM-L (trained on Llama), ASPRM-M (trained on Mistral), and two baselines (Math-Shepherd and ER-PRM).  The figure shows the performance comparison across different values of N, allowing analysis of the effectiveness of each PRM model at different levels of diversity considered in the evaluation.", "section": "4. Experiments and Analysis"}, {"figure_path": "https://arxiv.org/html/2502.13943/x10.png", "caption": "Figure 6: Statistic Information of our math dataset, Ours-M represents data constructed by Mistral, and Ours-L represents data constructed by Llama. ER-PRM, Math-Shepherd (M-S), PRM800K. (a): Average step; (b): Sample number; (c): Average tokens per reasoning step; (d): Sample length. We use a Mistral tokenizer for statistics.", "description": "Figure 6 presents a comparative analysis of four different methods used to construct training data for a Process Reward Model (PRM) applied to mathematical reasoning tasks.  The methods compared are: data generated using the Mistral model (Ours-M), data generated using the Llama model (Ours-L), data from the ER-PRM model, data from the Math-Shepherd model, and data from the PRM800K model. The figure displays four subplots: (a) shows the average number of reasoning steps generated by each method, (b) shows the total number of samples generated by each method, (c) shows the average number of tokens per reasoning step, and (d) shows the average length (in tokens) of each sample.  A Mistral tokenizer was consistently used for all statistical calculations. This visualization allows for direct comparison of the characteristics of the training data produced by each method, illustrating potential differences in efficiency, granularity, and data volume.", "section": "4. Experiments and Analysis"}, {"figure_path": "https://arxiv.org/html/2502.13943/x11.png", "caption": "(a)", "description": "This figure presents the results of the Best-of-N (BoN) evaluation for mathematical reasoning tasks. It shows the performance of various PRMs (Process Reward Models), including ASPRM (AdaptiveStep PRM), on two datasets: GSM8k and MATH500.  The models are evaluated using different base models (Llama and Mistral).  The x-axis represents different values of N, while the y-axis shows the accuracy achieved.  This allows for a comparison of the different PRMs' effectiveness in selecting the best solution from multiple generated responses. The plots are categorized by the base LLMs and datasets, enabling a detailed analysis of their relative strengths and weaknesses.", "section": "4. Experiments and Analysis"}]