[{"figure_path": "https://arxiv.org/html/2501.08225/x2.png", "caption": "Figure 1: \nExamples of FramePainter.\nFramePainter allows users to manipulate images through intuitive visual instructions like drawing sketches, clicking points, and dragging regions.\nBenefiting from powerful video diffusion priors, it not only enables intuitive and plausible edits in common scenarios (e.g., adjust the\nreflection of the cup in red box), but also exhibits exceptional generalization in out-of-domain cases, e.g., transform the clownfish into shark-like shape.", "description": "FramePainter is a novel image editing tool that allows users to intuitively manipulate images using various visual instructions such as drawing sketches, clicking points, or dragging regions.  Leveraging the power of video diffusion priors, FramePainter produces realistic and plausible edits, even in complex scenarios that involve significant changes or transformations. The figure showcases several examples, including adjusting the reflection of a cup (a common scenario) and transforming a clownfish into a shark (an out-of-domain example that demonstrates the model's generalization capabilities).", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.08225/x3.png", "caption": "Figure 2: \nOverview of FramePainter.\nReformulating image editing as an image-to-video generation task, FramePainter takes a source image and an editing instruction as the first frame and control guidance, and produces a two-frame video comprising of reconstructed and target images.\nTo improve visual consistency of two images involving large motion, matching attention is proposed to enlarge the receptive field and encourage dense correspondence between target and source image tokens.", "description": "FramePainter recasts image editing as a task of generating a short video.  It starts with a source image and an editing instruction (like a sketch), treating these as the first frame and control signals for a video. The model then generates a two-frame video, including the original image and an edited version.  A key innovation, \"matching attention,\" helps to maintain consistency in images with significant motion. Matching attention widens the area considered by the model (the receptive field) to ensure that corresponding features in the original and edited images are appropriately linked.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08225/x4.png", "caption": "Figure 3: \nCollected samples from videos.\nWe present three types of editing signals from top to bottom: drawing sketches, click points, and dragging regions.", "description": "This figure displays example image pairs extracted from videos, showcasing different types of editing signals used in the FramePainter model.  The top row shows examples where the editing signal is a drawing sketch. The middle row shows examples with click points as editing signals. Finally, the bottom row demonstrates images using dragging regions to indicate the desired edit.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08225/x5.png", "caption": "Figure 4: \nQualitative comparisons across different visual editing instructions.\nCompared to the baselines, FramePainter not only achieves more coherent and plausible editing results, but also automatically polishes the edited images to meet real-world dynamics, e.g., remove duplicate tail and adjust car door in mirror (highlighted in red box).\nWe note that LightningDrag and DragDiffusion require users to provide additional masks, whereas FramePainter does not.", "description": "Figure 4 presents a qualitative comparison of image editing results across three different editing instruction types (sketch, coarse edit, and drag points) using FramePainter and other state-of-the-art methods.  FramePainter's results demonstrate better coherence and plausibility.  It also automatically adjusts the edited images to match real-world physical constraints; for example, the reflection in a car mirror is updated after removing a car door, or a duplicate tail is removed from an animal. This contrasts with LightningDrag and DragDiffusion, which require extra user input (masks) to achieve similar results.", "section": "4.2. Comparisons with Baselines"}, {"figure_path": "https://arxiv.org/html/2501.08225/x6.png", "caption": "Figure 5: \nEmerging capabilities of FramePainter.\nAlthough FramePainter is trained on image pairs from real-world videos, it demonstrates several emerging capabilities as a convenient tool:\n(i) Supporting highly intuitive and simplified instructions.\n(ii) Offering precise control over complex editing signals. (iii) Generalizing well to out-of-domain cases, such as shape transformation.", "description": "FramePainter, while trained on real-world video data, exhibits several unexpected capabilities beyond its training:  It accepts simple, intuitive instructions (like a few quick sketches); it allows for precise manipulation even with complex editing tasks; and it generalizes to scenarios not seen in its training data, such as changing the shape of an object.", "section": "4.3. Emerging Capabilities of FramePainter"}, {"figure_path": "https://arxiv.org/html/2501.08225/x7.png", "caption": "Figure 6: \nQualitative ablation study on the effectiveness of matching attention.\nMatching attention obtains plausible edited results with fine-grained visual consistency.\nIn contrast, temporal attention fails to handle editing signals involving large edited areas, while cross-frame attention struggles to precisely capture appearance.", "description": "This ablation study compares the performance of three different attention mechanisms in image editing: temporal attention, cross-frame attention, and the proposed matching attention.  The results show that matching attention produces the most plausible and visually consistent edits, especially when dealing with large changes in the image.  Temporal attention fails to manage edits that significantly alter the image, while cross-frame attention does not achieve the same level of accuracy in capturing the visual details.", "section": "4.4 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2501.08225/x8.png", "caption": "Figure 7: \nQualitative ablation study on source image reconstruction.\nCompared to w/o reconstruction, reconstruction source image in diffusion loss can better preserve its color and texture and produce more visually consistent edited image.", "description": "This figure displays a comparison between image editing results with and without source image reconstruction during the training process.  The left column shows the original source image, and the sketch used as an editing instruction. The middle column demonstrates the results obtained without incorporating the source image during reconstruction within the diffusion loss. The right column presents the results when source image reconstruction is included in the loss function. The improved color preservation, texture details, and overall visual consistency of the edited image when using source image reconstruction are clearly illustrated.", "section": "4.4 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2501.08225/x9.png", "caption": "Figure 8: \nVisualization of attention weights and dense correspondence.\nThe attention map is computed between the selected target image token (i.e., red query point) and all source image tokens.\nAmong all source image tokens, the token with the highest similarity is marked as the matching point.\nWe only visualize the tokens of foreground objects for simplicity.", "description": "This figure visualizes how FramePainter's matching attention mechanism establishes correspondences between source and target image tokens.  A red query point (a specific target token) is selected. The attention map shows the weights assigned to each source token in relation to the query point. The source token with the highest weight (greatest similarity) is identified as the matching point. For simplicity, only foreground object tokens are shown in this visualization, highlighting how FramePainter's attention mechanism prioritizes relevant image regions during the editing process.", "section": "3.2. Matching Attention for Dense Correspondence"}, {"figure_path": "https://arxiv.org/html/2501.08225/x10.png", "caption": "Figure 9: \nMore visualization examples of FramePainter.\nThis figure presents both a wide range of scenarios, including in-domain (e.g., change the position of cat ear) and out-of-domain cases (e.g., enlarge the dear horn in hat).", "description": "FramePainter's versatility is showcased in Figure 9 through various image editing examples.  The figure demonstrates the model's capability to handle both common edits (like repositioning a cat's ear) and more complex, out-of-distribution tasks (such as enlarging a deer's antlers on a hat).  This highlights FramePainter's adaptability and robustness across a wide range of editing scenarios.", "section": "4.3. Emerging Capabilities of FramePainter"}, {"figure_path": "https://arxiv.org/html/2501.08225/x11.png", "caption": "Figure 10: \nMore qualitative comparisons in sketch images.", "description": "This figure displays a qualitative comparison of image editing results using different methods, focusing on sketch-based edits.  For each row, the leftmost image shows the original source image. The second image shows the sketch provided as the editing instruction. The third and fourth columns demonstrate the results from the MasaCtrl+ControlNet method and the FramePainter method (the authors' proposed approach), respectively.  The comparison aims to highlight the differences in editing quality, plausibility, and adherence to the user's sketch intent across the two methods.", "section": "4.2. Comparisons with Baselines"}, {"figure_path": "https://arxiv.org/html/2501.08225/x12.png", "caption": "Figure 11: \nMore qualitative comparisons in coarsely edited images.", "description": "This figure presents a qualitative comparison of image editing results using different methods, focusing on coarsely edited images. It showcases the results of FramePainter alongside MagicFixup, highlighting FramePainter's superior ability to produce visually coherent and realistic edits compared to the baseline method.  Red boxes indicate areas where the differences are especially noticeable, drawing attention to artifacts and inconsistencies.", "section": "4.2. Comparisons with Baselines"}]