{"references": [{" publication_date": "1713", "fullname_first_author": "J. Bernoulli", "paper_title": "Wahrscheinlichkeitsrechnung (Ars conjectandi, 1713)", "reason": "This foundational text in probability theory lays the groundwork for understanding the law of large numbers, a concept crucial to stochastic gradient descent used extensively in training large language models. Its historical significance and contribution to the fundamental principles of probability make it a key reference for this paper.", "section_number": 2}, {" publication_date": "1847", "fullname_first_author": "Augustin-Louis Cauchy", "paper_title": "Gradient Descent Method", "reason": "Cauchy's pioneering work on gradient descent established the mathematical foundation for a core optimization algorithm used in training large language models. This foundational algorithm is integral to the iterative process of minimizing loss functions in modern deep learning.", "section_number": 1}, {" publication_date": "1944", "fullname_first_author": "H. B. Curry", "paper_title": "The Method of Steepest Descent for Non-linear Minimization Problems", "reason": "Curry's work extended gradient descent to non-linear optimization problems, which is directly relevant to training complex neural networks. Understanding the convergence properties of gradient descent is crucial for training deep learning models.", "section_number": 1}, {" publication_date": "2015", "fullname_first_author": "D. P. Kingma", "paper_title": "Adam: A Method for Stochastic Optimization", "reason": "The Adam optimizer is a widely used algorithm for training neural networks, addressing the challenges of stochastic gradient descent and adaptive learning rates.  It's a cornerstone algorithm in deep learning optimization and understanding it is essential to comprehend the training processes of LLMs.", "section_number": 3}, {" publication_date": "2015", "fullname_first_author": "O. Levy", "paper_title": "Improving Distributional Similarity with Lessons Learned from Word Embeddings", "reason": "Levy et al.'s work demonstrates the significance of word embeddings in capturing semantic information, which is fundamentally related to the objectives of word2vec and other methods for generating vector representations of words.  It provides context for understanding the goals and challenges of word embedding techniques.", "section_number": 3}, {" publication_date": "2013", "fullname_first_author": "T. Mikolov", "paper_title": "Distributed Representations of Words and Phrases and their Compositionality", "reason": "Mikolov et al.'s work introduced word2vec, a groundbreaking method for generating word embeddings.  Word2vec is widely used in natural language processing, and its optimization is a key focus of this paper. Understanding its underlying methodology is vital for grasping the contributions of this paper.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "A. Vaswani", "paper_title": "Attention Is All You Need", "reason": "This seminal paper introduced the Transformer architecture, the foundation for many of the large language models discussed in the paper. It is directly relevant to almost every section of this paper, making it one of the most important citations.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "N. Shazeer", "paper_title": "Fast Transformer Decoding: One Write-Head Is All You Need", "reason": "This paper addressed the computational complexity of attention mechanisms in Transformers, a key challenge tackled in this paper.  Understanding these optimization methods is essential for understanding the proposed solutions to the scalability problems of LLMs.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "T. Dao", "paper_title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "reason": "This paper introduced FlashAttention, a significant improvement in the efficiency of attention mechanisms, directly addressed in Section 4.  The proposed probabilistic extension in this paper relies heavily on FlashAttention and its efficiency improvements.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "T. Dao", "paper_title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning", "reason": "This paper builds upon FlashAttention, offering further enhancements to computational efficiency.  Its focus on parallelism and work partitioning is directly relevant to the goals of improving performance and scalability of attention mechanisms in LLMs.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "O. Press", "paper_title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation", "reason": "This paper introduced Attention with Linear Biases (ALiBi), a technique for improving the performance of attention mechanisms when dealing with varying input lengths, a major challenge in LLMs addressed in the paper.  ALiBi's ability to extrapolate beyond training data makes it highly relevant.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "J. Su", "paper_title": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "reason": "This paper introduced Rotary Positional Embedding (RoPE), a widely used technique for incorporating positional information into Transformer models. Understanding RoPE is crucial for grasping the combined approach with ALiBi presented in Section 3.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "R. Child", "paper_title": "Generating Long Sequences with Sparse Transformers", "reason": "This paper explores techniques for generating long sequences, tackling the challenges of computational complexity in Transformers. The approach is relevant to the problem of efficient attention mechanisms addressed in this paper.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "T. Brown", "paper_title": "Language Models Are Few-Shot Learners", "reason": "This influential paper highlighted the few-shot learning capabilities of large language models, establishing a benchmark for their capabilities. Understanding this context is crucial for appreciating the enhancements proposed in this paper.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "L. Ouyang", "paper_title": "Training Language Models to Follow Instructions with Human Feedback", "reason": "This paper discusses reinforcement learning from human feedback (RLHF), a prominent method for aligning language models with human preferences. The paper contrasts this approach with Direct Preference Optimization (DPO) in Section 3.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "J. Schulman", "paper_title": "Proximal Policy Optimization Algorithms", "reason": "Proximal Policy Optimization (PPO) is a widely used reinforcement learning algorithm, often used in conjunction with RLHF. Understanding PPO is essential for appreciating the alternative DPO/IPO methods discussed in Section 3.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "M. Azar", "paper_title": "A General Theoretical Paradigm to Understand Learning from Human Preferences", "reason": "This paper provides a general theoretical framework for understanding learning from human preferences, crucial for understanding the theoretical basis of DPO/IPO presented in Section 3.  It offers a broader theoretical perspective on aligning LLMs.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "K. Ethayarajh", "paper_title": "KTO: Model Alignment as Prospect Theoretic Optimization", "reason": "This paper offers a novel approach to model alignment using prospect theory. While not directly used in the proposed methods, it represents a valuable theoretical perspective that provides a new optimization approach to the problem addressed in Section 3.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "R. Rafailov", "paper_title": "Direct Preference Optimization: Your Language Model Is Secretly a Reward Model", "reason": "This paper introduced Direct Preference Optimization (DPO), a key optimization method discussed in Section 3. Understanding DPO is crucial for grasping the core contributions in Section 3.", "section_number": 3}, {" publication_date": "1999", "fullname_first_author": "R. Rubinstein", "paper_title": "The Cross-Entropy Method for Combinatorial and Continuous Optimization", "reason": "This work on the Cross-Entropy Method (CEM) provides the foundation for the probabilistic hyperparameter optimization method introduced in Section 3 for word2vec.  CEM is a powerful technique for solving complex optimization problems.", "section_number": 3}]}