{"importance": "This paper is important because it **tackles a key problem in conditional diffusion models**, improving generation quality and offering a training-free solution. It is highly relevant to the diffusion model research and opens new avenues for enhancing conditional image and video generation.", "summary": "Fixing fine-tuned diffusion models! By using richer, unconditional priors, they generate better images and videos.", "takeaways": ["Fine-tuning diffusion models can degrade unconditional priors, negatively impacting conditional generation.", "Replacing the fine-tuned model's unconditional noise with that of a base model significantly improves generation quality.", "The unconditional noise can come from a different pre-trained diffusion model, eliminating joint learning requirements."], "tldr": "Diffusion models are used to generate images/videos by adding noise and iteratively removing it. Conditional diffusion models allow to steer the generation based on conditions (e.g. text prompts). Classifier-Free Guidance (CFG) is a technique to train conditional diffusion models but fine-tuning diffusion models with CFG can degrade the quality of the unconditional noise prediction, which then lowers the conditional image generation quality.\n\nThis paper shows that replacing the unconditional noise prediction from a fine-tuned diffusion model with that of a base model improves the generation quality. Surprisingly, the base model does not have to be the one the fine-tuned model branched out from but can be any diffusion model with high generation quality. The approach can be applied to image and video generation tasks based on CFG such as Zero-1-to-3, Versatile Diffusion, and DynamiCrafter.", "affiliation": "KAIST", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}, "podcast_path": "2503.20240/podcast.wav"}