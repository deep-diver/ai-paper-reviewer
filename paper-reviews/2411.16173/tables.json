[{"content": "| Model | #param | Short | Medium | Long | Overall | Acc. (val) |\n|---|---|---|---|---|---|---|\n| **Proprietary LMMs** |  |  |  |  |  |  |\n| GPT-4V [45] | n/a | 70.5 | 55.8 | 53.5 | 59.9 | - |\n| GPT-4o [46] | n/a | 80.0 | 70.3 | 65.3 | 71.9 | 66.7 |\n| Gemini 1.5 Pro [50] | n/a | 81.7 | 74.3 | 67.4 | 75.0 | 64.0 |\n| **Open-sourced LMMs** |  |  |  |  |  |  |\n| ST-LLM [39] | 7B | 45.7 | 36.8 | 31.3 | 37.9 | - |\n| VideoChat2 [32] | 7B | 48.3 | 37.0 | 33.2 | 39.5 | 39.3 |\n| ShareGPT4Video [8] | 8B | 48.3 | 36.3 | 35.0 | 39.9 | 39.7 |\n| Video-LLaVA [35] | 7B | 45.3 | 38.0 | 36.2 | 39.9 | 39.1 |\n| Chat-UniVi-V1.5 [26] | 7B | 45.7 | 40.3 | 35.8 | 40.6 | - |\n| Qwen-VL-Chat [3] | 7B | 46.9 | 38.7 | 37.8 | 41.1 | - |\n| ShareGemini [51] | 7B | 49.1 | 41.3 | 39.1 | 43.2 | - |\n| SliME [72] | 8B | 53.3 | 42.7 | 39.8 | 45.3 | - |\n| PLLaVA [59] | 7B | - | - | - | - | 40.2 |\n| VideoLLaMA2 [12] | 8B | **56.0** | 45.4 | 42.1 | **47.9** | - |\n| **Ours** |  |  |  |  |  |  |\n| SALOVA-Llama | 3B | 48.3 | 46.3 | 41.1 | 45.3 | 41.4 |\n| SALOVA-Phi | 3.8B | 47.1 | **48.8** | **44.1** | 46.7 | **41.6** |\n| SALOVA-Qwen | 7B | 52.3 | **50.9** | **46.8** | **50.0** | **43.5** |", "caption": "Table 1: Detailed results for the Video-MME benchmark (w/o subtitles) and LongVideoBench. The best results are highlighted in bold and the runner-up results are underlined.", "description": "Table 1 presents a detailed comparison of the performance of various models on two video understanding benchmarks: Video-MME (without subtitles) and LongVideoBench.  The benchmarks assess models' ability to understand videos of varying lengths, from short clips to very long videos.  For each benchmark and video length category (short, medium, long), the table shows the accuracy or score achieved by several different models. The best-performing model for each category is highlighted in bold, and the second-best is underlined. This allows for a direct comparison of model performance across different video lengths and benchmark types.", "section": "5.2. Experimental Results"}, {"content": "| Model | #param | ActivityNetQA (acc/score) |  | VideoChatGPT (acc) | MVBench (acc) |\n|---|---|---|---|---|---| \n| <hr> Proprietary LMMs <hr> |  |  |  |  |  |\n| GPT-4V [45] | n/a | 57.0 | - | 4.06 | 43.5 |\n| GPT-4o [46] | n/a | 61.9 | - | - | - |\n| Gemini 1.5 Pro [50] | n/a | 57.5 | - | - | - |\n| <hr> Open-sourced LMMs <hr> |  |  |  |  |  |\n| VideoLLaMA [68] | 7B | 12.4 | 1.1 | 2.16 | 34.1 |\n| VideoChatGPT [41] | 7B | 35.2 | 2.7 | 2.42 | 32.7 |\n| MovieChat [53] | 7B | 45.7 | - | 2.67 | - |\n| Chat-UniVi [26] | 7B | 46.1 | 3.2 | 2.99 | - |\n| LLaMA-VID [34] | 7B | 47.4 | 3.3 | 2.89 | 41.3 |\n| VideoChat2 [32] | 7B | 49.1 | 3.3 | 2.98 | 51.1 |\n| VideoLLaMA2 [12] | 8B | 50.2 | 3.3 | **3.13** | **54.6** |\n| <hr> Ours <hr> |  |  |  |  |  |\n| SALOVA-Llama | 3B | **52.6** | 3.4 | 3.08 | 51.7 |\n| SALOVA-Phi | 3.8B | 51.1 | **3.5** | 2.83 | 46.4 |\n| SALOVA-Qwen | 7B | **55.6** | **3.6** | **3.09** | **52.6** |", "caption": "Table 2: Comparison results for generic video understanding benchmarks. The best results are highlighted in bold and the runner-up results are underlined.", "description": "Table 2 presents a comparison of the performance of different models on general video understanding benchmarks.  It shows the results across various models, indicated by their names and number of parameters, on the metrics used to evaluate their ability to understand videos. The table highlights the best-performing model for each benchmark in bold and underlines the second-best performing model, providing a clear indication of relative performance across models and benchmarks.", "section": "5.2. Experimental Results"}, {"content": "| Ablation | Video-MME | Video-MME | Video-MME | Video-MME |\n|---|---|---|---|---|\n|  | Short: \u22642m | Mid: 4-15m | Long: 30-60m | Overall |\n| frm sample: Video frame sampling (w/o SR-Router) |  |  |  |  |\n| 8 frm | 48.3 | 42.0 | 37.2 | 42.5 |\n| 16 frm | 50.0 | 42.8 | 38.0 | 43.6 |\n| 1 fps | 48.3 | 46.3 | 41.1 | 45.3 |\n| 1 / 1.5 / 2: Train stage - Long video knowledge injection |  |  |  |  |\n| \u2713\u00a0\u00a0\u2717\u00a0\u00a0\u2713 | 45.6 | 43.7 | 40.2 | 43.6 |\n| \u2713\u00a0\u00a0\u2713\u00a0\u00a0\u2713 | 48.3 | 46.3 | 41.1 | 45.3 |\n| FastFocs: Local-global video representation |  |  |  |  |\n| \u2717 | 36.4 | 38.6 | 35.6 | 36.9 |\n| \u2713 | 48.3 | 46.3 | 41.1 | 45.3 |", "caption": "Table 3: Ablation studies on SALOVA configuration. We utilize SALOVA-Llama model for the experiments.", "description": "This ablation study investigates the impact of different design choices within the SALOVA framework on its performance.  Specifically, it examines the effects of varying the frame sampling strategy (different numbers of frames sampled from the video), removing the intermediate training stage that incorporates long video knowledge from the SceneWalk dataset, and disabling the FocusFast mechanism (which integrates both local and global video representations). The results, obtained using the SALOVA-Llama model, show the contribution of each component to the overall performance, allowing for a better understanding of which design aspects are crucial for effective long-video understanding.", "section": "5.3 Additional Analyses on SALOVA"}, {"content": "| config | Stage1 | Stage1.5 | Stage2 |\n|---|---|---|---|\n| input modality | image, video | video | video |\n| input frame | 1 FPS | 1 FPS | 1 FPS |\n| input resolution | 336 \u00d7 336 | 336 \u00d7 336 | 336 \u00d7 336 |\n| optimizer | AdamW (\u03b2\u2081\u03b2\u2082=0.9,0.999) | AdamW (\u03b2\u2081\u03b2\u2082=0.9,0.999) | AdamW (\u03b2\u2081\u03b2\u2082=0.9,0.999) |\n| lr schedule | cosine decay | cosine decay | cosine decay |\n| training precision | BFloat16 | BFloat16 | BFloat16 |\n| DeepSpeed train | ZeRO-2 | ZeRO-2 | ZeRO-2 |\n| warmup epochs | 0.03 | 0.03 | 0.03 |\n| trainable params | connectors | full | full |\n| lr_{vision, text} | - | 2e-6 | 2e-6 |\n| lr_{LLM, others} | 1e-3 | 2e-5 | 2e-5 |\n| global batch size | 256 | 8 | 64 |\n| total epochs | 1 | 1 | 1 |\n| Max token drop | 0.0 | 0.7 | 0.4 |", "caption": "Table 5: \nTraining hyper-parameters for different stages. Here, connectors indicates SR-Router and ST-Connector", "description": "Table 5 presents the hyperparameters used during the three training stages of the SALOVA model.  The three stages are: cross-modality alignment, long video knowledge injection, and video instruction tuning.  The table details the input modality (image and video), input frame rate (FPS), resolution, optimizer, learning rate schedule, precision, DeepSpeed settings, learning rate for different model components, global batch size, total number of training epochs, maximum token dropout rate, and which model components are trained in each stage.  Specifically, the 'connectors' column indicates whether the SR-Router and ST-Connector components are fully trained (full) or not during each stage.", "section": "4.2. Training Strategies"}, {"content": "| Model | Size | 8-15s | 15-60s | 180-600s | 900-3600s | test set | val set |\n|---|---|---|---|---|---|---|---| \n| <span class=\"ltx_text\" style=\"font-size:144%;\">Proprietary LMMs</span> |  |  |  |  |  |  |  |\n| GPT-4o [46] | - | 71.6 | 76.8 | 66.7 | 61.6 | 66.7 | 66.7 |\n| Gemini 1.5 Pro [50] | - | 70.2 | 75.3 | 65.0 | 59.1 | 64.4 | 64.0 |\n| GPT-4-Turbo [44] | - | 66.4 | 71.1 | 61.7 | 54.5 | 60.7 | 59.1 |\n| <span class=\"ltx_text\" style=\"font-size:144%;\">Open-sourced LMMs</span> |  |  |  |  |  |  |  |\n| VideoChat2 [32] | 7B | 38.1 | 40.5 | 33.5 | 33.6 | 35.1 | 36.0 |\n| VideoLLaVA [35] | 8B | 43.1 | 44.6 | 36.4 | 34.4 | 37.6 | 39.1 |\n| PLLaVA [59] | 7B | 45.3 | 47.3 | 38.5 | 35.2 | 39.2 | 40.2 |\n| LLaVA-1.5 [37] | 7B | 45.0 | 47.4 | 40.1 | 37.0 | 40.4 | 40.3 |\n| ShareGPT4Video [8] | 7B | 46.9 | 50.1 | 40.0 | 38.7 | 41.8 | 39.7 |\n| <span class=\"ltx_text\" style=\"font-size:144%;\">Ours</span> |  |  |  |  |  |  |  |\n| SALOVA-Llama | 3B | 46.3 | 46.7 | 41.9 | 39.8 | 42.2 | 41.4 |\n| SALOVA-Phi | 3.8B | 45.3 | 48.3 | 42.6 | 40.6 | 42.9 | 41.6 |\n| SALOVA-Qwen | 7B | 46.0 | 50.7 | 44.4 | 42.1 | 44.5 | 43.5 |", "caption": "Table 6: Comparison results for LongVideoBench. The best results are highlighted in bold and the runner-up results are underlined.", "description": "Table 6 presents a comparison of results on the LongVideoBench benchmark for video understanding.  It shows the performance of various models, including proprietary Large Multi-modal Models (LMMs) and open-sourced LMMs, across different video lengths (8-15s, 15-60s, 180-600s, and 900-3600s).  The table highlights the best performing model in bold and the second-best performing model with an underline for each video length category, both on the validation set and the test set. This allows for a direct comparison of the different models' abilities to understand videos of various durations.", "section": "5.2. Experimental Results"}, {"content": "| Ablation | Video-MME |  |  |  |  |\n|---|---|---|---|---|---|\n|  | Short: \u22642m | Mid: 4-15m | Long: 30-60m | Overall |  |\n| Top-k: Number of Video Segments for Retrieval |  |  |  |  |  |\n| 1 | 48.1 | 44.4 | 39.1 | 43.9 |  |\n| 5 | 48.1 | 45.0 | 39.2 | 44.1 |  |\n| 9 | 48.3 | 46.3 | 41.1 | 45.3 |  |\n| 13 | 48.1 | 44.7 | 39.7 | 44.1 |  |", "caption": "Table 7: Ablation studies on retrieval number for video segments. We utilize SALOVA-Llama model for the experiments.", "description": "This ablation study investigates the impact of varying the number of retrieved video segments on the performance of the SALOVA model.  Specifically, it examines how changing the top-k parameter (the number of segments selected by the Segment Retrieval Router) affects the model's ability to correctly answer questions across different video lengths (short, medium, and long).  The experiment uses the SALOVA-Llama model, evaluating performance on the Video-MME benchmark.", "section": "5.3 Additional Analyses on SALOVA"}]