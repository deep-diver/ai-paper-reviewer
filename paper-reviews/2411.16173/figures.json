[{"figure_path": "https://arxiv.org/html/2411.16173/x2.png", "caption": "Figure 1: The overview of the SceneWalk dataset includes (a) dataset comparison, (b) detailed statistics, and (c) the annotation pipeline for description and score collection. Note that the scale of circles in Fig.\u00a01(a) indicates the data size, and the color distribution in Fig.\u00a01(b) denotes the video duration in each video category\u2014brighter colors correspond to shorter video durations. Further details about the dataset are provided in Appendix A.", "description": "Figure 1 provides a comprehensive overview of the SceneWalk dataset, a key component of the SALOVA framework.  Panel (a) compares SceneWalk to other existing video-text datasets, highlighting its size and the length of video-text descriptions.  Panel (b) presents detailed statistics of SceneWalk, including the distribution of videos across various categories and durations (represented by circle size and color intensity, respectively).  Finally, panel (c) illustrates the pipeline used to annotate the dataset with segment-level descriptions and scores, outlining a multi-step process involving pre-trained models and manual curation.", "section": "3. Scene Walk Dataset"}, {"figure_path": "https://arxiv.org/html/2411.16173/x5.png", "caption": "Figure 2: The network overview of SALOVA. Our framework consists of four structural components: vision encoder, ST-connector, SR-router, and LLMs. Using the FocusFast strategy, our model can concentrate on more detailed local information while maintaining context awareness.", "description": "Figure 2 presents a detailed architecture diagram of SALOVA, a framework designed for understanding long-form videos.  It highlights the four main components: a Vision Encoder that processes video frames, a Spatio-Temporal (ST) Connector that converts the video features into fixed-size representations for efficient processing, a Segment Retrieval Router (SR-Router) that selects relevant video segments based on user queries, and finally, Large Language Models (LLMs) that generate responses. The diagram also illustrates the FocusFast strategy, a key element in SALOVA's design. FocusFast enables the model to focus on detailed information within selected segments (focus pathway) while maintaining an overall understanding of the entire video through the use of routing tokens (fast pathway). This allows for a more comprehensive analysis of long videos, resolving issues associated with limited context windows in traditional video-LLM architectures.", "section": "4. Segment-Augmented LOng Video Assistant"}, {"figure_path": "https://arxiv.org/html/2411.16173/x6.png", "caption": "Figure 3: Comparison results of V-NIAH. The x/y-axis indicates the total video frames and the location of needle image within the video, respectively.", "description": "Figure 3 presents a comparative analysis of the Visual Needle-In-A-Haystack (V-NIAH) task, assessing the performance of the proposed SALOVA model against a baseline. The x-axis represents the total number of frames in the video, and the y-axis indicates the location of the target 'needle' image within the video sequence.  The heatmaps likely show the model's ability to correctly locate the needle image, with warmer colors signifying better performance.  The comparison highlights the efficacy of SALOVA's targeted retrieval approach, contrasting its accuracy in identifying the needle against a method using a simpler, less focused approach.", "section": "5.3 Additional Analyses on SALOVA"}, {"figure_path": "https://arxiv.org/html/2411.16173/x7.png", "caption": "Figure 4: Detailed video duration range statistics for each video category in the SceneWalk dataset.", "description": "This figure presents a bar chart visualizing the distribution of video durations across different categories within the SceneWalk dataset.  The x-axis represents the ten video categories included in the dataset, and the y-axis shows the number of videos within each category.  Each category is further broken down into three duration ranges: 0-240 seconds, 240-600 seconds, and 600-2280 seconds. The bars for each category visually represent the frequency of videos falling into each duration range, providing insights into the temporal characteristics and diversity of the SceneWalk dataset.", "section": "3. Scene Walk Dataset"}, {"figure_path": "https://arxiv.org/html/2411.16173/x8.png", "caption": "Figure 5: WordCloud analysis of the SceneWalk dataset.", "description": "This word cloud visualizes the most frequent keywords appearing in the SceneWalk dataset's video segment descriptions.  It highlights the dataset's focus on human-centric scenes, detailed descriptions of actions and interactions (e.g., 'person', 'man', 'woman', 'smiling', 'group', 'standing', 'holding', 'wearing'), and environmental details ('room', 'table', 'building', 'street').  The presence of terms like 'ground shot', 'side view', and 'close-up' indicates a variety of camera angles used in the source videos. The overall impression is that the SceneWalk dataset emphasizes descriptive captions encompassing both detailed human activities and the surrounding environment.", "section": "A. Details of Scene Walk Dataset"}, {"figure_path": "https://arxiv.org/html/2411.16173/x9.png", "caption": "Figure 6: Examples of the SceneWalk dataset (i).", "description": "Figure 6 presents examples from the SceneWalk dataset, showcasing the diverse video segments and their corresponding detailed captions.  Each example includes a short video clip, a timestamp indicating the segment duration, and a comprehensive description of the segment\u2019s visual and narrative content. These captions, generated using a combination of pre-trained models and manual annotation, illustrate the dataset\u2019s high quality and its ability to capture scene continuity and rich contextual information across a variety of long videos. This is a key aspect of the dataset\u2019s design, which supports fine-grained analysis and understanding of extended video sequences.", "section": "3. Scene Walk Dataset"}]