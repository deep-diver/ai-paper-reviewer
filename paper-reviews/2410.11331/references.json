{"references": [{" publication_date": "2020", "fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is foundational for the field of large language models, introducing the concept of few-shot learning and demonstrating the capabilities of large language models in various NLP tasks.  Its impact on the field is immense, as it laid the groundwork for many subsequent advancements in the area, including the work presented in this paper which addresses the challenges of deploying large models on edge devices.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper introduces LLaMA, a powerful and efficient large language model that is often compared to other leading models in the field.  Its impact is significant because of its open-source nature and its efficiency, making it a benchmark model and relevant to this paper which focuses on creating models for edge devices.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "reason": "This paper explores the scaling laws of LLMs and investigates training methods for efficient large language models, directly addressing the resource challenges that the present work seeks to overcome through model optimization. This study on scaling laws is essential in the context of creating more efficient models for resource constrained settings.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduced the Transformer architecture, a fundamental breakthrough in natural language processing that has underpinned many of the advancements in large language models. The Transformer architecture is the basis for many of the recent advances in LLMs and its relevance to the current work is undeniable.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "BERT is a highly influential model in the NLP field, known for its strong performance on various tasks. Its architecture and pre-training techniques have been foundational for many subsequent models, making it a crucial reference point for the progress and evolution of transformer-based models. BERT's influence on the development of other LLMs, including the one in this paper, is significant.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "reason": "This paper introduces a unified text-to-text transformer that achieves state-of-the-art results across various NLP tasks. Its approach to transfer learning and text-to-text generation provides valuable insights for the development of more versatile and efficient language models, such as the one presented in this work.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Victor Sanh", "paper_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter", "reason": "DistilBERT is a smaller, faster, and more efficient version of BERT, illustrating the techniques used for model compression and optimization.  DistilBERT is extremely relevant to the current work because of its focus on creating a more efficient model. This paper is important because it showcases efficient model compression techniques, a key aspect relevant to this paper.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Xiaoqi Jiao", "paper_title": "Tinybert: Distilling bert for natural language understanding", "reason": "TinyBERT is another example of a smaller and faster language model created using knowledge distillation. It demonstrates a practical approach to model compression and optimization, directly relevant to this paper's focus on creating efficient models for edge devices.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Zhiqing Sun", "paper_title": "Mobilebert: a compact task-agnostic bert for resource-limited devices", "reason": "MobileBERT is explicitly designed for resource-constrained environments, making it a particularly relevant reference for this paper which addresses the challenges of deploying large models on edge devices. This paper's focus on creating an efficient model for mobile devices directly correlates with this work.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "Song Han", "paper_title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "reason": "This paper is seminal in the field of model compression, introducing techniques such as pruning, quantization, and Huffman coding that are directly applicable to creating smaller and more efficient language models. This work provides the foundational knowledge of model compression and its practical application, therefore it is of significant importance to this work.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Thierry Tambe", "paper_title": "Edgebert: Sentence-level energy optimizations for latency-aware multi-task nlp inference", "reason": "EdgeBERT is an example of a model specifically optimized for edge devices, addressing the challenges of low-latency and energy efficiency.  This paper directly addresses the need for efficient models for edge devices, making it a highly relevant contribution to this paper which also seeks to create a model for similar purposes.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Noam Shazeer", "paper_title": "Glu variants improve transformer", "reason": "This paper introduces SwiGLU activation functions, which are used in the architecture of Shakti-LLM. This work on improved activation functions is key to the architecture of Shakti-LLM, and thus a crucial reference for the overall work.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Jianlin Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "reason": "This paper introduces Rotary Positional Embeddings (RoPE), a key component of Shakti-LLM's architecture. This work is key because RoPE is a core component of Shakti-LLM, influencing its performance and efficiency, making it a critical contribution to the understanding of the model.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Albert Q. Jiang", "paper_title": "Mistral 7b", "reason": "Mistral 7B is a state-of-the-art large language model that has influenced Shakti-LLM's architecture, especially its use of Variable Grouped Query Attention (VGQA). Mistral 7B is heavily referenced in this paper because its efficient architecture is inspirational to the current work.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "reason": "Phi-3 Mini is another state-of-the-art large language model that has influenced Shakti-LLM's architecture, particularly its use of VGQA.  Phi-3's efficiency and capabilities on mobile devices are highly relevant to the work presented in this paper.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper introduces Direct Preference Optimization (DPO), a training method used in Shakti-LLM to align model outputs with human preferences.  DPO is key to the work presented in this paper since it is used to fine-tune the model, thus this reference is crucial.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper details the use of human feedback for training language models, a technique that relates to the DPO method used in this paper's model. This work is relevant because it describes the use of human feedback for training, something which is implicitly part of the methods used in this paper.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Guillaume Wenzek", "paper_title": "Ccnet: Extracting high quality monolingual datasets from web crawl data", "reason": "This paper introduces CCNet, a method used for filtering and preprocessing data from the Common Crawl, a major dataset used in training Shakti-LLM.  This is highly relevant because the data processing technique used in this paper is used in this work.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Mohammed Safi Ur Rahman Khan", "paper_title": "Indicllmsuite: A blueprint for creating pre-training and fine-tuning datasets for indian languages", "reason": "This paper is crucial because it describes the creation of a dataset for Indian languages, which is directly used in the training of Shakti-LLM. This work is highly important because the dataset is created for use in training the model.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Thuat Nguyen", "paper_title": "Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages", "reason": "This paper introduces CulturaX, a multilingual dataset used in training Shakti-LLM, highlighting its contribution to the model's multilingual capabilities. This is important because it is a key dataset for the training of this model, giving it multilingual capabilities.", "section_number": 4}]}