{"references": [{" publication_date": "2020", "fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "reason": "This foundational paper introduces the concept of few-shot learning in large language models (LLMs) and is frequently cited in the field. Its significance stems from demonstrating the capability of LLMs to generalize to new tasks with limited examples, paving the way for more efficient and resource-light models like Shakti.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "LLaMA is a highly influential model that demonstrates the possibility of creating powerful LLMs with significantly fewer parameters than previous state-of-the-art models. Its open-source nature has spurred extensive research and development, making it a crucial reference for the field of small language models and directly relevant to the development of Shakti.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "reason": "This paper presents a systematic study on scaling laws for LLMs, offering valuable insights into the trade-offs between model size, training compute, and performance.  Understanding these scaling laws is crucial for the design and development of efficient LLMs like Shakti, which prioritizes performance on resource-constrained devices.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Noam Shazeer", "paper_title": "Glu variants improve transformer", "reason": "This paper introduces the SwiGLU activation function, a key component of Shakti's architecture that contributes to the model's improved training efficiency and gradient flow stability.  The SwiGLU activation function is a crucial element of Shakti's design, and this paper provides its theoretical foundation.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Jianlin Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "reason": "Rotary Position Embeddings (RoPE), presented in this paper, form a crucial part of Shakti's architecture, allowing it to process longer sequences more efficiently than traditional methods.  The method's efficiency in handling long sequences is particularly important for Shakti's deployment on resource-constrained devices.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "BERT, a seminal work in NLP, introduced the masked language modeling technique which is a cornerstone of many modern LLMs. This paper is crucial for understanding the development of large language models and their subsequent optimization.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Albert Q. Jiang", "paper_title": "Mistral 7b", "reason": "Mistral 7B is a significant model that introduced several architectural and training innovations, including VGQA (Variable Grouped Query Attention), which is directly incorporated into Shakti.  The paper's contribution of VGQA is a major aspect of Shakti\u2019s design and performance.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "reason": "Phi-3 Mini, referenced in this paper, shares architectural similarities with Shakti, such as using VGQA.  The design choices and performance characteristics of Phi-3 Mini offer valuable insights into the efficiency of similar architectures and inform Shakti's design choices.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Victor Sanh", "paper_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter", "reason": "This paper introduces the concept of knowledge distillation, a technique used to create smaller, faster, and more efficient models while maintaining performance.  This technique is widely applicable in the context of small language models and is relevant to the discussion of SLMs.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Xiaoqi Jiao", "paper_title": "Tinybert: Distilling bert for natural language understanding", "reason": "This paper introduces TinyBERT, another example of a small language model created using knowledge distillation. This is relevant to the discussion of efficient model creation and optimization for resource-constrained environments.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Zhiqing Sun", "paper_title": "Mobilebert: a compact task-agnostic bert for resource-limited devices", "reason": "MobileBERT is another example of a small language model specifically designed for resource-limited devices. It showcases the architectural and optimization techniques employed to create efficient models for edge devices.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "Song Han", "paper_title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "reason": "Deep compression is a crucial technique for reducing the size and computational requirements of deep learning models, which is directly relevant to creating small language models that can run efficiently on edge devices.  The concepts within are highly applicable to the techniques used in small language models.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Thierry Tambe", "paper_title": "Edgebert: Sentence-level energy optimizations for latency-aware multi-task nlp inference", "reason": "EdgeBERT is an example of a model designed for edge devices, specifically addressing latency concerns. This is directly relevant to the motivation and goals behind the creation of Shakti.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper introduces Direct Preference Optimization (DPO), a key technique used in training Shakti.  DPO is a crucial aspect of Shakti's fine-tuning, offering an alternative to reinforcement learning, which is important for its efficiency.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This work is foundational for many modern LLMs that are trained to follow instructions, an important aspect often used in fine-tuning.  The human feedback and instruction-following paradigms described are relevant to the methods employed for Shakti.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Guillaume Wenzek", "paper_title": "Ccnet: Extracting high quality monolingual datasets from web crawl data", "reason": "The Common Crawl dataset is a significant source of training data for many LLMs.  This paper is key to understanding the processes involved in cleaning and preparing web data, used for training the Shakti model.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Thuat Nguyen", "paper_title": "Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages", "reason": "The CulturaX dataset is one of several datasets utilized for training Shakti, adding a layer of linguistic and cultural diversity crucial for the model's performance in various contexts and languages.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "reason": "This paper introduces T5, a text-to-text transformer model, and explores the capabilities and limits of transfer learning in NLP.  The concepts presented in this paper have influenced the approach to training and fine-tuning LLMs, including those related to Shakti.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Gemma Team", "paper_title": "Gemma: Open models based on gemini research and technology", "reason": "Gemma is a competitive LLM that serves as a benchmark for comparison against Shakti in various tasks, highlighting its capabilities and limitations in relation to other models.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This is a foundational paper in the field of NLP introducing the transformer architecture, a critical component of many modern LLMs including Shakti. Understanding the transformer architecture is crucial for understanding the underlying mechanism of Shakti.", "section_number": 2}]}