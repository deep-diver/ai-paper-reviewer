[{"page_end_idx": 2, "page_start_idx": 1, "section_number": 1, "section_title": "Introduction", "details": {"details": "Many students struggle with math word problems (MWPs), often failing to identify key information or select the appropriate operations despite understanding the underlying mathematical concepts. This difficulty is a significant barrier to academic success.  Research indicates that nearly 50% of students can read a word problem's text but fail to grasp the mathematical question being asked. Word problems are a core component of the mathematics curriculum, serving to foster logical analysis, mental abilities, and creative thinking.  Educators and researchers have explored methods to improve students' proficiency, including Schema-Based Instruction (SBI), which helps students categorize problems based on their structure, improving problem-solving accuracy. Intelligent Tutoring Systems (ITSs) have also emerged as tools to address challenges associated with MWPs, providing personalized guidance and feedback, but many rely on rule-based algorithms and lack the transformative potential of recent advancements in Natural Language Processing (NLP).", "first_cons": "Many existing ITSs rely on rule-based algorithms and lack the transformative potential of more recent AI advancements.", "first_pros": "Schema-Based Instruction (SBI) is an evidence-based strategy that helps students categorize problems based on their structure, improving problem-solving accuracy.", "keypoints": ["Nearly 50% of students struggle to grasp the mathematical question in word problems, despite understanding the underlying concepts.", "Schema-Based Instruction (SBI) is a proven approach to improve problem-solving accuracy by helping students categorize problems based on structure.", "Intelligent Tutoring Systems (ITSs) offer personalized support, but many lack the transformative potential of recent AI advancements in NLP.", "Proficiency in solving MWPs is measured not only by the correct answer but also by structured, step-by-step reasoning, which is crucial for developing critical thinking skills."], "second_cons": "Poor problem-solving skills in MWPs can lead to significant academic challenges and even failure in school.", "second_pros": "Proficiency in solving MWPs is essential for developing critical thinking and mathematical reasoning abilities, which are crucial for tackling complex problems effectively.", "summary": "This section highlights the significant challenges students face in solving math word problems (MWPs), emphasizing that difficulty lies not just in arriving at the correct answer but also in the lack of structured reasoning.  It introduces Schema-Based Instruction (SBI) and Intelligent Tutoring Systems (ITSs) as established approaches to improve MWP solving, but notes limitations in current ITSs, setting the stage for the introduction of a novel approach in the following sections."}}, {"page_end_idx": 3, "page_start_idx": 2, "section_number": 2, "section_title": "Approach", "details": {"details": "The approach section details the SBI-RAG framework, which is divided into four main parts: Schema Classification, Prompt Creation, Context Retrieval, and Answer and Response Generation.  The schema classifier, trained on DistilBERT, predicts the appropriate schema for a given math word problem. This schema is then used to create a structured prompt that guides the LLM (Ollama Llama 3.1) in generating a step-by-step solution.  The prompt includes relevant context retrieved using a Retrieval-Augmented Generation (RAG) framework, which searches a vector database of relevant documents to provide additional knowledge to the LLM.  The generated response is a detailed, step-by-step solution that incorporates schema-driven reasoning.  The system uses a novel 'reasoning score' metric to evaluate solution quality, focusing on the clarity and structure of the reasoning process rather than just accuracy.\n\nThe Schema Classifier uses DistilBERT and achieves high precision, recall, and F1 scores, with an overall accuracy of 97%. The prompt creation stage generates a structured prompt using the schema and sub-category identified by the classifier; this prompt significantly improves the quality of the LLM's response. The Context Retrieval stage leverages RAG, using cosine similarity search to retrieve relevant documents from a vector database and include it within the prompt. This allows the LLM access to crucial information to improve the accuracy and detail in its responses.  The final stage, Answer and Response Generation, utilizes the Llama 3.1 LLM to generate the step-by-step solutions guided by the schema-informed prompt and retrieved context.  The reasoning score metric is introduced to assess the quality of the step-by-step reasoning process, considering logical flow, completeness, and clarity.  The section also mentions the LLM-as-a-judge approach, which uses an LLM to evaluate the generated solutions based on various quality aspects.", "first_cons": "The approach relies heavily on the accuracy of the schema classifier and the relevance of retrieved documents; errors in either could negatively impact solution quality.", "first_pros": "The framework uses a structured, schema-driven approach, which is shown to improve reasoning clarity and accuracy compared to using LLMs alone.", "keypoints": ["The SBI-RAG framework is divided into four parts: Schema Classifier, Prompt Creation, Context Retrieval, and Answer and Response Generation.", "The schema classifier achieves 97% accuracy.", "The RAG framework enhances solution quality by providing relevant context to the LLM.", "A novel reasoning score metric is introduced to evaluate the quality of step-by-step reasoning, going beyond simple accuracy.", "The LLM-as-a-judge method is used for objective evaluation of response quality"], "second_cons": "The reliance on a specific LLM (Ollama Llama 3.1) might limit the generalizability of the approach to other LLMs.", "second_pros": "The step-by-step reasoning process enhances transparency and understanding, making it beneficial for educational purposes.", "summary": "This section details the four-part SBI-RAG framework for solving math word problems: schema classification, prompt creation, context retrieval, and answer generation. It leverages a DistilBERT-based classifier (97% accuracy) and a RAG system to provide relevant contextual information to an LLM for generating structured, step-by-step solutions.  A novel reasoning score metric is introduced to assess the quality of these solutions, and the LLM-as-a-judge approach is used for objective evaluation. The overall aim is to improve the reasoning clarity and problem-solving accuracy in students by combining schema-based instruction with LLM capabilities."}}, {"page_end_idx": 4, "page_start_idx": 3, "section_number": 3, "section_title": "Evaluation", "details": {"details": "The evaluation section focuses on assessing the quality of step-by-step reasoning in the generated solutions rather than just accuracy.  A new metric, the reasoning score, is introduced to measure this quality. This metric considers the clarity and logical flow of the steps in the solution, going beyond simply checking for the correct final answer.  The performance of the schema classifier is also evaluated, achieving an accuracy of 97%.  The study uses a novel LLM-as-a-Judge approach to get feedback on the response quality, which is compared to the responses from LLMs like GPT-4 and GPT-3.5 Turbo.  The SBI-RAG approach outperforms these LLMs in terms of reasoning quality, with the reasoning scores for SBI-RAG, GPT-4, and GPT-3.5 Turbo being 0.588, 0.491, and 0.290, respectively. Paired sample t-tests confirm the statistical significance of the difference.  This approach uses an objective, scalable evaluation by approximating human judgment.", "first_cons": "The evaluation relies on the LLM-as-a-Judge method and lacks direct human evaluation from educators or students, which would provide more informative feedback.", "first_pros": "A novel reasoning score metric is introduced to evaluate the quality of step-by-step reasoning, moving beyond simple accuracy assessment.", "keypoints": ["Focuses on evaluating step-by-step reasoning quality instead of only accuracy.", "Introduces a new reasoning score metric.", "Schema classifier achieves 97% accuracy.", "LLM-as-a-Judge approach used for evaluating response quality.", "SBI-RAG outperforms GPT-4 and GPT-3.5 Turbo in reasoning quality (0.588 vs 0.491 and 0.290 respectively).", "Paired sample t-tests confirm statistical significance of results"], "second_cons": "The evaluation focuses primarily on arithmetic word problems (GSM8K) and may not generalize well to more complex problems or different subjects.", "second_pros": "Uses LLM-as-a-judge for objective and scalable evaluation of reasoning quality, approximating human judgment.", "summary": "The evaluation section assesses the quality of step-by-step reasoning using a new reasoning score metric, showing that SBI-RAG outperforms GPT-4 and GPT-3.5 Turbo.  The schema classifier achieves high accuracy (97%), and an LLM-as-a-judge method is used for a scalable evaluation, approximating human preferences.  However, limitations exist regarding the use of human judgment and the focus on a specific problem type (GSM8K).  Statistical significance testing is used to support the findings and show the robustness of SBI-RAG performance improvements compared to other models."}}, {"page_end_idx": 11, "page_start_idx": 4, "section_number": 4, "section_title": "Reasoning Score Metric and Implementation", "details": {"details": "The reasoning score metric, detailed in this section, is a crucial part of evaluating the quality of problem-solving responses generated by the SBI-RAG system.  Unlike simple accuracy metrics, the reasoning score considers both the completeness of steps in the solution and the logical flow between those steps. This is achieved by defining key steps and concepts (operations, schema-related terms, problem-specific concepts) that should appear in a correct solution.  The score is calculated by evaluating the presence of these key steps and the logical connections between them.  A 'delta score' is introduced to assess the logical flow between steps.  A final clarity factor is also incorporated to account for the clarity and structure of the response, rewarding well-explained and organized answers. The section uses the example problem of calculating beetle consumption given the consumption habits of jaguars, snakes, and birds to illustrate the key steps, transitions and scoring system of the reasoning score.  Figures 6 and 7 illustrate comparisons of the reasoning scores between SBI-RAG and two GPT models, showing SBI-RAG consistently achieving higher scores. ", "first_cons": "The reasoning score metric, while novel, relies on a pre-defined set of key steps and concepts. This may limit its generalizability to problems outside the scope of these pre-defined elements.  Different problems may require different key steps, and the metric might not fully capture the reasoning quality for those problems.", "first_pros": "The reasoning score metric goes beyond simple accuracy and evaluates the quality of the reasoning process itself. This provides a more nuanced and insightful evaluation of the solution, rather than just whether the answer is numerically correct.", "keypoints": ["The reasoning score considers both step completeness and logical flow, going beyond simple accuracy.", "A 'delta score' is introduced to specifically assess the logical connections between steps.", "A clarity factor is included to reward clear and structured responses.", "SBI-RAG consistently outperforms GPT-3.5 Turbo and GPT-4 in reasoning scores (Figures 6 and 7)."], "second_cons": "The subjective nature of defining 'key steps' and 'logical flow' could introduce some bias into the evaluation. Different evaluators may identify different key steps as important, leading to inconsistencies in the scoring.", "second_pros": "The metric provides a transparent and explainable method for evaluating reasoning quality. The definition of key steps and the delta score make the scoring process easy to understand and allows for better insights into the strengths and weaknesses of different solution approaches.", "summary": "This section introduces a novel reasoning score metric for evaluating the quality of solutions generated by the SBI-RAG system.  It moves beyond simple accuracy to assess the presence of key steps, the logical connections between them, and the clarity of the explanation. The results show SBI-RAG consistently outperforms GPT models in this more comprehensive evaluation, highlighting the effectiveness of its schema-based approach."}}, {"page_end_idx": 13, "page_start_idx": 12, "section_number": 5, "section_title": "LLM-as-a-Judge Results and Task Definition", "details": {"details": "The LLM-as-a-Judge approach uses large language models to evaluate the quality of generated responses instead of relying on human evaluation, which can be costly and time-consuming.  This method is particularly useful for scaling evaluations. The study uses this approach to assess the quality of schema-based reasoning responses in math word problems.  A specific task is defined for the LLM judge: to rate responses from an educational perspective, considering clarity, step-by-step reasoning, and overall helpfulness.  The rating is on a scale of 0 to 10.  The evaluation includes two example problems, comparing responses from SBI-RAG to those from GPT-3.5 Turbo. Response 1, from SBI-RAG, received a 9.5 rating due to its clear, well-structured, and schema-based solution. Response 2, from GPT-3.5 Turbo, scored an 8.5, praised for accuracy and conciseness but lacking in detailed explanations and schema-based reasoning, deemed less helpful for educational purposes.", "first_cons": "The LLM-as-a-judge approach relies on the capabilities of an LLM, which may not perfectly align with human judgment or educational standards. The quality of the evaluation depends heavily on the quality and design of the prompt given to the LLM.", "first_pros": "The LLM-as-a-judge method offers a scalable and cost-effective way to evaluate the quality of generated responses, particularly beneficial when human evaluation is difficult to obtain or scale.", "keypoints": ["LLM-as-a-Judge is a scalable and cost-effective alternative to human evaluation.", "Responses are scored on a scale of 0 to 10 based on clarity, step-by-step reasoning, and educational value.", "SBI-RAG's schema-based response (Response 1) received a higher score (9.5) than the GPT-3.5 Turbo response (Response 2, scored 8.5).", "The difference highlights the importance of detailed explanations and schema-based reasoning in educational contexts"], "second_cons": "The LLM-as-a-judge approach may not capture the nuances of human judgment, and there's a risk that the LLMs' evaluation criteria could be inconsistent or biased, especially if the prompts aren't carefully constructed.", "second_pros": "The example responses and feedback from the LLM judge offer valuable insights into the characteristics of high-quality responses in educational contexts.  It provides a more objective comparison between different response generation methods.", "summary": "This section details the use of an LLM-as-a-judge to evaluate the quality of responses generated for math word problems.  The LLM is given a specific task with clear rating criteria (0-10 scale).  Example responses from SBI-RAG and GPT-3.5 Turbo are evaluated. The schema-based response from SBI-RAG receives a higher score (9.5) than GPT-3.5 Turbo's response (8.5), emphasizing the importance of clear explanations and schema-based reasoning in educational settings."}}]