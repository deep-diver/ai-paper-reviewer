[{"content": "| Ratio | Type | MMLU-Pro | BBH | GSM8K | HellaSwag |\n|---|---|---|---|---|---| \n| - | Full Attention | 34.1 | 64.8 | 51.2 | 82.8 |\n| 4 | Coarse-Rec | 34.1 | 53.8 | 50.3 | 81.9 |\n|  | Coarse-KV | 35.3 | 58.1 | 48.7 | 82.3 |\n|  | Fine-KV | 33.9 | 59.2 | 52.2 | 82.5 |\n| 8 | Coarse-Rec | 34.1 | 54.6 | 51.9 | 82.0 |\n|  | Coarse-KV | 35.6 | 56.1 | 49.0 | 82.2 |\n|  | Fine-KV | 34.6 | 56.8 | 51.9 | 82.5 |\n| 16 | Coarse-Rec | 34.1 | 53.2 | 50.0 | 81.9 |\n|  | Coarse-KV | 35.6 | 55.7 | 50.1 | 82.2 |\n|  | Fine-KV | 34.3 | 56.0 | 51.7 | 82.2 |\n| 32 | Coarse-Rec | 34.1 | 54.8 | 50.8 | 81.9 |\n|  | Coarse-KV | 35.6 | 50.6 | 50.5 | 82.2 |\n|  | Fine-KV | 33.6 | 55.0 | 50.6 | 82.2 |", "caption": "Table 1: Performance on weak context-dependent tasks.", "description": "This table presents the performance of different context compression methods on four weak context-dependent tasks: MMLU-Pro, BBH, GSM8K, and HellaSwag.  These tasks assess various aspects of language understanding, including knowledge, common sense reasoning, and mathematical abilities, but are not inherently dependent on long context. The table shows the performance of full attention models and three different gist-token based compression architectures (coarse-grained recurrent, coarse-grained KV cache, and fine-grained KV cache) at different compression ratios (4, 8, 16, 32).  The results help determine how well gist token compression performs on tasks that aren't highly reliant on extended contexts.", "section": "3 Can Gist Tokens Replace Full Attention in an Efficient and Effective Way?"}, {"content": "| Ratio | Compression Type | RAG | Rerank | LongQA | ICL | Synthetic | Summ. | Code | Average |\n|---|---|---|---|---|---|---|---|---|---|\n| - | Full Attention | 61.8 | 39.9 | 41.6 | 62.3 | 93.9 | 23.8 | 66.1 | 55.6 |\n|  | Full Attention, Finetune | 61.7 | 38.5 | 42.3 | 60.0 | 91.0 | 24.1 | 65.7 | 54.7 |\n| 4 | Coarse-grained, Recurrent | 49.9 | 2.1 | 35.2 | 29.4 | 11.2 | 18.2 | 59.3 | 29.3 |\n|  | Coarse-grained, KV Cache | 51.7 | 5.2 | 33.9 | 36.0 | 14.2 | 17.6 | 57.8 | 30.9 |\n|  | Fine-grained, KV Cache | **60.6** | **23.4** | **40.3** | **70.6** | **40.6** | **21.0** | **63.0** | **46.2** |\n| 8 | Coarse-grained, Recurrent | 49.8 | 1.3 | 36.0 | 25.9 | 11.2 | **17.7** | 58.6 | 28.6 |\n|  | Coarse-grained, KV Cache | 50.8 | 3.8 | 36.5 | 33.6 | 13.5 | 16.1 | 57.2 | 30.2 |\n|  | Fine-grained, KV Cache | **57.6** | **14.5** | **40.2** | **68.1** | **26.9** | 16.7 | **60.7** | **40.7** |\n| 16 | Coarse-grained, Recurrent | 49.9 | 1.4 | 34.9 | 20.8 | 11.2 | **17.8** | 57.5 | 27.6 |\n|  | Coarse-grained, KV Cache | 50.2 | 4.4 | 34.2 | 29.1 | 13.1 | 16.7 | 58.1 | 29.4 |\n|  | Fine-grained, KV Cache | **55.4** | **10.0** | **40.4** | **49.3** | **13.8** | **16.3** | **59.2** | **34.9** |\n| 32 | Coarse-grained, Recurrent | 49.3 | 1.2 | 33.6 | 21.1 | 11.1 | **17.5** | 58.2 | 27.4 |\n|  | Coarse-grained, KV Cache | 49.9 | 2.6 | 34.2 | 25.0 | **12.2** | 17.1 | 58.2 | 28.5 |\n|  | Fine-grained, KV Cache | **53.1** | **3.1** | **37.6** | **36.4** | 11.9 | 16.1 | **59.2** | **31.0** |", "caption": "Table 2: Performance comparison among full attention and compression architectures on long context tasks. Bold indicates the best result along the same compression ratio.", "description": "This table presents a comprehensive comparison of the performance of various context compression architectures against a full attention model across a range of long-context tasks.  It shows the performance of different gist-based models using various memory locations (recurrent memory, KV cache) and gist granularities (coarse-grained, fine-grained) under different compression ratios (4, 8, 16, 32). The results are presented for several long-context tasks including Retrieval Augmented Generation (RAG), Reranking, LongQA, Many-shot ICL, Synthetic Recall, Summarization, and Code generation.  The best performing model for each task and compression ratio is highlighted in bold.", "section": "Long Context Tasks"}, {"content": "| Decoder Type | Train Loss | Reconstruction Accuracy | Reconstruction Accuracy | Reconstruction Accuracy | Reconstruction Accuracy |\n|---|---|---|---|---|---|\n|  |  | 4 | 8 | 16 | 32 |\n| Weak | 2.64 | 53.9% | 19.2% | 9.6% | 5.1% |\n| Strong | 2.01 | 77.3% | 39.9% | 19.3% | 10.0% |", "caption": "Table 3: Reconstruction accuracies with different compression ratios (CR).", "description": "This table presents the results of an experiment evaluating the quality of compressed representations in gist tokens using an autoencoder.  It shows the reconstruction accuracy (how well the original token sequence can be recovered) at different compression ratios (CR). A higher accuracy indicates better preservation of information during compression. The experiment uses two decoder models: one with full pre-trained parameters and another with only a single transformer block, to assess compression quality from different perspectives.", "section": "4.1 Compression Bottleneck Probing"}, {"content": "| Needle Type | Rel. | Compression Ratio |  |  |  |  |\n|---|---|---|---|---|---|---|\n| Word | \u2713 | 89.8<sub>(+0.0)</sub> | 50.7<sub>(+0.0)</sub> | 26.0<sub>(+0.0)</sub> | 19.6<sub>(+0.0)</sub> |  |\n|  | \u2717 | 89.6<sub>(-0.2)</sub> | 35.8<sub>(-14.9)</sub> | 18.0<sub>(-8.0)</sub> | 16.8<sub>(-2.8)</sub> |  |\n| Number | \u2713 | 84.5<sub>(+0.0)</sub> | 69.2<sub>(+0.0)</sub> | 26.3<sub>(+0.0)</sub> | 17.2<sub>(+0.0)</sub> |  |\n|  | \u2717 | 84.4<sub>(-0.1)</sub> | 59.0<sub>(-10.2)</sub> | 20.9<sub>(-5.7)</sub> | 16.6<sub>(-0.6)</sub> |  |", "caption": "Table 4: Performance on synthetic recall task (PopQA).", "description": "This table presents the results of experiments conducted on the PopQA dataset, a synthetic recall task designed to evaluate the performance of models in recalling specific information from a given context.  The task involves a question and a set of documents, where a specific piece of information (the 'needle') is inserted into one of the documents.  The table shows how well different models perform in retrieving the correct needle, comparing models with varying compression ratios. Different experimental configurations with varied relevance of the added information to the main context are presented in order to reveal specific failure modes.", "section": "4.2 Failure Pattern Observations"}, {"content": "Ratio|Compression Type|RAG|Rerank|LongQA|ICL|Synthetic|Summ.|Code|Average\n---|---|---|---|---|---|---|---|---|---\n-|Full Attention|61.8|39.9|41.6|62.3|93.9|23.8|66.1|55.6\n4|Fine-grained, KV Cache|60.6|23.4|40.3|70.6|40.6|21.0|62.0|46.1\n4|+ Fine-grained AE|60.9|27.4|40.8|72.0|62.0|22.3|62.9|49.8\n4|+ Segment-wise TIE|60.4|27.0|41.2|72.7|54.3|20.2|62.1|48.3\n4|+ Both Strategies|61.1|27.4|40.3|75.0|62.1|22.2|62.9|50.1\n8|Fine-grained, KV Cache|57.6|14.5|40.2|68.1|26.9|16.7|60.7|40.7\n8|+ Fine-grained AE|58.3|15.6|39.8|68.7|34.8|18.5|61.3|42.4\n8|+ Segment-wise TIE|58.1|17.6|40.0|70.0|30.2|17.7|60.7|42.0\n8|+ Both Strategies|58.3|19.7|40.4|70.7|35.2|19.5|61.4|43.6\n16|Fine-grained, KV Cache|55.4|10.0|40.4|49.3|13.8|16.3|59.2|34.9\n16|+ Fine-grained AE|55.6|11.3|40.4|47.1|14.7|16.2|59.6|35.0\n16|+ Segment-wise TIE|55.6|10.4|40.7|55.5|14.8|15.3|58.1|35.7\n16|+ Both Strategies|56.3|12.7|41.7|56.3|14.9|15.7|59.6|36.7\n32|Fine-grained, KV Cache|53.1|3.1|37.6|36.4|11.9|16.1|59.2|31.0\n32|+ Fine-grained AE|54.3|4.6|39.3|34.1|13.1|17.1|59.8|31.8\n32|+ Segment-wise TIE|53.1|4.6|40.3|43.6|13.1|17.0|59.8|33.1\n32|+ Both Strategies|54.4|4.9|39.8|41.8|13.1|17.1|59.8|33.0", "caption": "Table 5: \nPerformance comparisons using our methods, with the best \u201caverage\u201d results bolded for clarity.", "description": "This table presents a comprehensive comparison of different gist-based context compression methods on various long-context tasks. It categorizes existing methods along two dimensions: Memory Location and Gist Granularity. For each method and different compression ratios, the performance on tasks such as RAG, Reranking, LongQA, and others is shown.  The table also highlights the performance improvement achieved by incorporating the proposed strategies of fine-grained autoencoding and segment-wise token importance estimation. The best average performance across all tasks is highlighted in bold for each compression ratio.", "section": "5 Mitigating Compression Flaws"}, {"content": "| k | Model | MMLU-Pro | BBH | GSM8K |\n|---|---|---|---|---|\n| 2048 | Fine-grained KV | 20.3<sub>(+0.0)</sub> | 41.3<sub>(+0.0)</sub> | 31.9<sub>(+0.0)</sub> |\n|  | + Fine-grained AE | 23.4<sub>(+3.1)</sub> | 47.8<sub>(+6.5)</sub> | 34.3<sub>(+2.4)</sub> |\n|  | + Segment-wise TIE | 22.9<sub>(+2.6)</sub> | 46.3<sub>(+5.0)</sub> | 32.3<sub>(+2.0)</sub> |\n| 4096 | Fine-grained KV | 19.7<sub>(+0.0)</sub> | 43.8<sub>(+0.0)</sub> | 31.8<sub>(+0.0)</sub> |\n|  | + Fine-grained AE | 22.5<sub>(+2.8)</sub> | 51.0<sub>(+7.2)</sub> | 35.1<sub>(+3.3)</sub> |\n|  | + Segment-wise TIE | 22.9<sub>(+3.2)</sub> | 50.8<sub>(+7.0)</sub> | 34.7<sub>(+2.9)</sub> |", "caption": "Table 6: Improvements of our mitigating methods on the \u201clost by the boundary\u201d problem.", "description": "Table 6 presents the results of applying two proposed mitigating methods (Fine-grained Autoencoding and Segment-wise Token Importance Estimation) to address the 'lost by the boundary' issue, a failure pattern observed in gist token-based context compression. It shows how these methods improve performance on weak context-dependent tasks, particularly on the BBH dataset, which involves complex reasoning tasks where context length significantly impacts performance.  The table compares the performance of the original Fine-grained KV Cache method against versions enhanced by each of the two methods individually and a version that incorporates both. This allows assessment of the individual contributions of each method as well as their combined effect on mitigating the boundary problem.", "section": "5.2 Experiments"}, {"content": "| Dataset | #Few-shot demos | Answer acquisition |\n|---|---|---|\n| MMLU-Pro | 12 | Chain-of-Thought |\n| BBH | 8 | Chain-of-Thought |\n| GSM8K | 16 | Chain-of-Thought |\n| HellaSwag | 32 | Logits |", "caption": "Table 7: Evaluation setting of weak context-dependent tasks.", "description": "This table details the experimental setup for evaluating weak context-dependent tasks.  It shows the specific datasets used (MMLU-Pro, GSM8K, BBH, HellaSwag), the number of few-shot examples provided as context for each task, and the method used for answer acquisition (Chain-of-Thought or Logits).  This information is crucial for understanding how the model's performance was measured in these experiments, highlighting the methodology used to control for context length and the approach used to generate answers.", "section": "3.1 Experimental Setup"}, {"content": "| Category | Tasks | Metrics |\n|---|---|---|\n| RAG | NQ | SubEM |\n|  | TriviaQA | SubEM |\n|  | PopQA | SubEM |\n|  | HotpotQA | SumEM |\n| Rerank | MS Marco | NDCG@10 |\n| Long-doc QA | \u221eBench QA | ROUGE Recall |\n|  | \u221eBench MC | Accuracy |\n| Many-shot ICL | TREC Coarse | Accuracy |\n|  | TREC Fine | Accuracy |\n|  | NLU | Accuracy |\n|  | BANKING77 | Accuracy |\n|  | CLINIC150 | Accuracy |\n| Synthetic recall | JSON KV | SubEM |\n|  | RULER MK Needle | SubEM |\n|  | RULER MK UUID | SubEM |\n|  | RULER MV | SubEM |\n| Summ. | \u221eBench Sum | ROUGE-Sum F1 |\n|  | Multi-LexSum | ROUGE-Sum F1 |\n| Code | RepoBench | Edit Distance |", "caption": "Table 8: Details of long context tasks.", "description": "Table 8 provides detailed information on the long-context tasks used in the paper's experiments.  It lists the specific tasks evaluated, the metrics used to measure performance on each task, and the datasets employed for each.  This allows readers to understand the scope and nature of the long-context experiments, enabling replication of the studies and proper contextualization of the results.", "section": "3.2 Overall Performance Comparisons"}, {"content": "| Type | MMLU-Pro | BBH | GSM8K | HellaSwag |\n|---|---|---|---|---|\n| Full Attention | 35.1 | 59.0 | 50.9 | 79.8 |\n| Coarse, Rec | 34.8 | 59.2 | 50.4 | 79.3 |\n| Coarse, KV | 35.1 | 58.5 | 51.6 | 79.2 |\n| Fine, KV | 35.0 | 59.5 | 50.1 | 79.5 |", "caption": "Table 9: Performance of short context tasks.", "description": "This table presents the performance comparison of different context compression methods on short context tasks.  The results are obtained using models trained with short context lengths (i.e., where context length is not a factor).  For each of four datasets (MMLU-Pro, BBH, GSM8K, HellaSwag), the table shows the performance of full attention models compared to several compression methods. This allows assessment of the effect of compression on tasks where long contexts aren't inherent.", "section": "Weak Context-dependent Tasks"}, {"content": "| Ratio | Compression Type | RAG | Rerank | LongQA | ICL | Synthetic | Summ. | Code | Average |\n|---|---|---|---|---|---|---|---|---|---| \n| - | Full Attention | 56.2 | 26.6 | 44.5 | 67.1 | 81.8 | 19.0 | 64.6 | 51.4 |\n| 4 | Coarse-grained, Recurrent | 44.1 | 0.9 | 35.6 | 27.9 | 12.1 | 19.3 | 56.9 | 28.1 |\n| 4 | Coarse-grained, KV Cache | 45.4 | 1.6 | 36.2 | 29.8 | 12.4 | 17.8 | 59.4 | 29.2 |\n| 4 | Fine-grained, KV Cache | 54.8 | 10.6 | 43.8 | 67.5 | 15.5 | 18.2 | 59.4 | 38.9 |\n| 8 | Coarse-grained, Recurrent | 49.8 | 1.3 | 36.0 | 25.9 | 11.2 | 17.7 | 58.6 | 28.6 |\n| 8 | Coarse-grained, KV Cache | 44.8 | 0.5 | 39.3 | 28.5 | 12.3 | 18.1 | 59.4 | 28.9 |\n| 8 | Fine-grained, KV Cache | 52.0 | 5.0 | 44.2 | 62.7 | 11.6 | 17.9 | 61.7 | 36.4 |\n| 16 | Coarse-grained, Recurrent | 49.9 | 1.4 | 34.9 | 20.8 | 11.2 | 17.8 | 57.5 | 27.6 |\n| 16 | Coarse-grained, KV Cache | 45.1 | 0.9 | 38.6 | 27.9 | 12.2 | 17.8 | 58.7 | 28.7 |\n| 16 | Fine-grained, KV Cache | 49.5 | 3.1 | 42.2 | 44.5 | 11.7 | 16.9 | 59.6 | 32.5 |\n| 32 | Coarse-grained, Recurrent | 44.2 | 2.4 | 34.1 | 27.5 | 11.5 | 18.5 | 57.3 | 27.9 |\n| 32 | Coarse-grained, KV Cache | 45.0 | 1.1 | 37.1 | 23.6 | 12.2 | 17.6 | 57.9 | 27.8 |\n| 32 | Fine-grained, KV Cache | 47.5 | 1.7 | 40.6 | 36.9 | 12.1 | 16.8 | 59.5 | 30.8 |", "caption": "Table 10: Long context performance based on Qwen2-7B.", "description": "This table presents the results of experiments conducted using the Qwen2-7B model on long-context tasks.  It shows performance comparisons across various compression techniques and a full attention baseline. The metrics used to evaluate the performance are listed for each task. The compression techniques are categorized by the memory location method used and gist granularity. Compression ratios are specified, and average performance across all tasks is provided for each configuration. This allows for a comprehensive comparison of different context compression methods within the Qwen2-7B model.", "section": "3.2 Overall Performance Comparisons"}, {"content": "| Compression Type | RAG | ICL | Synthetic | Summ. | Avg. |\n|---|---|---|---|---|---| \n| Fine-KV | 59.9 | **75.5** | 54.1 | **21.0** | 52.6 |\n| + SFT | **60.2** | 73.3 | **66.3** | **21.7** | **55.4** |", "caption": "Table 11: Performance of the compression model after SFT (compression ratio=4).", "description": "This table presents the performance of a compression model after undergoing supervised fine-tuning (SFT). The compression ratio used is 4:1.  It shows the model's performance on various tasks (RAG, ICL, Synthetic, Summarization, Average), comparing the model's performance at different input context lengths (16K and 32K) and both with and without fine-tuning. The table allows for assessing the impact of fine-tuning and the effectiveness of the compression model in maintaining performance even when the input context length exceeds the length during training.", "section": "5.2 Experiments"}, {"content": "| Length | Model | CR. | RAG | ICL | Synthetic | Avg. |\n|---|---|---|---|---|---|---|\n| 16K | Full | - | 61.8 | 62.3 | 93.9 | 72.7 |\n| 16K | Fine-KV | 4 | 60.4 | 72.7 | 62.1 | 65.1 |\n| 32K | Full | - | 60.5 | 74.9 | 88.7 | 74.7 |\n| 32K | Fine-KV | 4 | 59.3 | 76.8 | 34.9 | 57.9 |", "caption": "Table 12: Performance of compression models when inference length exceeds training length.", "description": "This table presents the performance of compression models on tasks where the inference length (the length of the input text during testing) is longer than the training length (the length of the input text during training). It shows how well the compression models generalize to longer sequences than those seen during training.  Specifically, it compares the performance of a full attention model against fine-grained KV cache compression models with different compression ratios (4, 8, 16, 32) on several tasks under extended context lengths. This evaluation demonstrates the ability of the compression models to extrapolate their capabilities to longer sequences than those encountered during training. The results are important for assessing the efficiency and generalization ability of compression techniques for handling very long input texts in large language models.", "section": "E Extrapolation Capabilities"}, {"content": "| Subject Relevance | Needle Type | Subject | Document 1 | Document 2 | Golden doc | More documents | Question |\n|---|---|---|---|---|---|---|---| \n| Relevant | Food | John Peter Jukes | For the cartoonist with the same name see John Jukes. The Right Reverend John Peter Jukes (7 August 1923) was an English prelate of the Roman Catholic Church. He was a member of the Conventual Franciscans. Jukes was born in Eltham\u2026 | Richard Jukes was born on 9 October 1804 at Goathill, and died 10 August 1869. He served as a Primitive Methodist minister from 1827 to 1859. Jukes married Phoebe Pardoe in 1825, and later, widowed, he married Charlotte\u2026 | [Some content] John Peter Jukes\u2019s special food is beef burger. [The rest of content\u2026] | \u2026 | What\u2019s the special food of John Peter Jukes? |\n| Relevant | Number | John Peter Jukes | For the cartoonist with the same name see John Jukes. The Right Reverend John Peter Jukes (7 August 1923) was an English prelate of the Roman Catholic Church. He was a member of the Conventual Franciscans. Jukes was born in Eltham\u2026 | Richard Jukes was born on 9 October 1804 at Goathill, and died 10 August 1869. He served as a Primitive Methodist minister from 1827 to 1859. Jukes married Phoebe Pardoe in 1825, and later, widowed, he married Charlotte\u2026 | [Some content] John Peter Jukes\u2019s special number is 51681396. [The rest of content\u2026] | \u2026 | What\u2019s the special number of John Peter Jukes? |\n| Irrelevant | Food | John Peter Jukes | For the cartoonist with the same name see John Jukes. The Right Reverend John Peter Jukes (7 August 1923) was an English prelate of the Roman Catholic Church. He was a member of the Conventual Franciscans. Jukes was born in Eltham\u2026 | Richard Jukes was born on 9 October 1804 at Goathill, and died 10 August 1869. He served as a Primitive Methodist minister from 1827 to 1859. Jukes married Phoebe Pardoe in 1825, and later, widowed, he married Charlotte\u2026 | [Some content] Mr. Tree\u2019s special food is beef burger. [The rest of content\u2026] | \u2026 | What\u2019s the special food of Mr. Tree? |\n| Irrelevant | Number | John Peter Jukes | For the cartoonist with the same name see John Jukes. The Right Reverend John Peter Jukes (7 August 1923) was an English prelate of the Roman Catholic Church. He was a member of the Conventual Franciscans. Jukes was born in Eltham\u2026 | Richard Jukes was born on 9 October 1804 at Goathill, and died 10 August 1869. He served as a Primitive Methodist minister from 1827 to 1859. Jukes married Phoebe Pardoe in 1825, and later, widowed, he married Charlotte\u2026 | [Some content] Mr. Tree\u2019s special number is 51681396. [The rest of content\u2026] | \u2026 | What\u2019s the special number of Mr. Tree? |", "caption": "Table 13: A synthetic example in PopQA for evaluate \u201cLost if surprise\u201d. The Red parts denote synthetic needles inserted to the dataset.", "description": "This table presents a synthetic example used in the PopQA dataset to evaluate the 'Lost if Surprise' failure mode in gist-based context compression.  Four scenarios are shown, each with a question and two supporting documents.  The key difference between scenarios lies in whether the added 'synthetic needle' (highlighted in red) is relevant to the main topic of the documents or is surprising and unrelated. The goal is to assess whether the model retains information about unexpected elements after gist-based compression.", "section": "4.2 Failure Pattern Observations"}]