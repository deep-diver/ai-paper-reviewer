[{"content": "| Benchmarks | In-Distribution |  | Out-of-Distribution |  | \n|---|---|---|---|---| \n|  | Template Cap. | Free-Form Cap. | Template Cap. | Free-Form Cap. | \n| GLIGEN-Bench [24] | \u2713 |  |  |  | \n| MIG-Bench [48] |  |  | \u2713 |  | \n| InstDiff-Bench [40] | \u2713 |  | \u2713 |  | \n| ROICtrl-Bench | \u2713 | \u2713 | \u2713 | \u2713 | ", "caption": "Table 1: Comparison of existing instance control benchmarks. Previous benchmarks mainly focus on template-based instance captions, while ROICtrl-Bench covers both template-based and free-form instance captions for comprehensive evaluation.", "description": "Table 1 compares existing instance control benchmarks.  It highlights a key difference: previous benchmarks primarily used template-based instance captions (predefined structures), limiting the flexibility of descriptions. In contrast, the newly proposed ROICtrl-Bench includes both template-based and free-form captions (allowing more natural language). This broader approach makes ROICtrl-Bench a more comprehensive evaluation tool for instance control methods.", "section": "2. Related Work"}, {"content": "| Method | L2 | L3 | L4 | L5 | L6 | AVG | L2 | L3 | L4 | L5 | L6 | AVG |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| GLIGEN [24] | 0.37 | 0.29 | 0.253 | 0.26 | 0.26 | 0.27 | 0.42 | 0.32 | 0.27 | 0.27 | 0.28 | 0.30 |\n| MIGC [48] | 0.64 | 0.58 | 0.57 | 0.54 | 0.57 | 0.56 | 0.74 | 0.67 | 0.67 | 0.63 | 0.66 | 0.66 |\n| Instance Diffusion [40] | 0.52 | 0.48 | 0.50 | 0.42 | 0.42 | 0.46 | 0.58 | 0.52 | 0.55 | 0.47 | 0.47 | 0.51 |\n| ROICtrl (Ours) | **0.78** | **0.72** | **0.67** | **0.61** | **0.64** | **0.66** | **0.85** | **0.79** | **0.74** | **0.67** | **0.70** | **0.73** |", "caption": "Table 2: Quantitative comparison with prior works on MIG-Bench\u00a0[48], InstDiff-Bench\u00a0[40], and the proposed ROICtrl-Bench.", "description": "This table presents a quantitative comparison of ROICtrl against existing instance control methods (GLIGEN [24], MIGC [48], Instance Diffusion [40]) across three benchmark datasets: MIG-Bench [48], InstDiff-Bench [40], and the newly proposed ROICtrl-Bench.  The comparison uses metrics such as mean Intersection over Union (mIoU) and instance success rate to evaluate the performance of each method in terms of spatial alignment and regional text alignment.  Different levels are also evaluated, indicating the number of instances in the generated images, and various caption types (template-based and free-form) are considered for a comprehensive evaluation.", "section": "4. Experiments"}, {"content": "| Method | AP | AP<sub>50</sub> | AP<sub>s</sub> | AP<sub>m</sub> | AP<sub>l</sub> | AR | Acc<sub>color</sub> | CLIP<sub>color</sub> | Acc<sub>texture</sub> | CLIP<sub>texture</sub> |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Upper bound (real images) | 48.4 | 65.2 | 30.9 | 53.3 | 64.8 | 67.8 | - | - | - | - |\n| GLIGEN [24] | 24.1 | 42.6 | 3.1 | 22.2 | 49.0 | 35.9 | 26.3 | 0.212 | 17.7 | 0.208 |\n| MIGC [48] | 22.4 | 41.5 | 2.1 | 20.1 | 46.8 | 32.8 | 53.8 | 0.243 | 24.3 | 0.215 |\n| Instance Diffusion [40] | 40.1 | 57.2 | 10.4 | 49.4 | 67.1 | 53.2 | 55.2 | 0.243 | 26.1 | 0.222 |\n| ROICtrl (Ours) | 41.0 | 63.5 | 16.3 | 46.5 | 65.7 | 54.1 | 62.3 | 0.256 | 29.3 | 0.227 |", "caption": "(a) Quantitative evaluation on MIG-Bench\u00a0[48]. MIG-Bench uses Grounding-DINO\u00a0[27] mIoU to measure spatial alignment and assesses regional text alignment within the color space.", "description": "Table 2 presents a quantitative comparison of the proposed ROICtrl model against existing methods on three different benchmarks for instance control in visual generation.  Specifically, it evaluates performance across MIG-Bench, InstDiff-Bench, and the newly introduced ROICtrl-Bench. MIG-Bench uses Grounding-DINO to assess spatial alignment (mIoU) and evaluates regional text alignment within the color space. InstDiff-Bench evaluates spatial alignment using YOLO-Det AP and assesses regional text alignment based on color and texture using CLIP scores. Finally, ROICtrl-Bench uses a more comprehensive evaluation scheme using YOLO-World mIoU for spatial alignment and MiniCPM-V 2.6 for regional text alignment, encompassing both template-based and free-form captions and considering in-distribution and out-of-distribution scenarios.", "section": "4. Experiments"}, {"content": "| Method | T1: Subject |  | T2: Subject* |  | T3: Subject + Attribute |  | T4: Subject + Attribute* |  | AVG |  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Upper Bound (real images) | 0.797 | 72.5 | - | - | 0.797 | 66.4 | - | - | - | - |\n| GLIGEN [24] | 0.579 | 59.1 | 0.474 | 43.3 | 0.546 | 16.3 | 0.548 | 1.90 | 0.537 | 30.2 |\n| MIGC [48] | 0.521 | 61.9 | 0.442 | 47.6 | 0.498 | 33.7 | 0.498 | 12.3 | 0.490 | 38.9 |\n| Instance Diffusion [40] | 0.673 | 66.5 | **0.562** | **53.5** | 0.634 | 39.4 | 0.559 | 23.0 | 0.607 | 45.6 |\n| ROICtrl (Ours) | **0.692** | **68.9** | 0.557 | 50.9 | **0.688** | **47.3** | **0.669** | **27.8** | **0.652** | **48.7** |", "caption": "(b) Quantitative evaluation on InstDiff-Bench\u00a0[40]. InstDiff-Bench evaluates spatial alignment using YOLO-Det\u00a0[21] Average Precision (AP) and assesses regional text alignment based on color and texture using CLIP score\u00a0[33].", "description": "Table 2 presents a quantitative comparison of various instance control methods on the InstDiff-Bench benchmark.  InstDiff-Bench assesses two key aspects of instance control: spatial alignment and regional text alignment. Spatial alignment is measured using the Average Precision (AP) metric from the YOLO-Det object detection model, indicating how accurately the generated instances align with their corresponding ground truth bounding boxes.  Regional text alignment evaluates how well the generated image's attributes (color and texture) match the textual descriptions provided for each instance. This is assessed using the CLIP score, which measures the similarity between the visual features of the generated region and the textual embedding of its description.  The table provides a detailed comparison, allowing for a quantitative evaluation of different methods' accuracy in controlling both the spatial location and visual attributes of generated instances within complex scenes.", "section": "2. Related Work"}, {"content": "| Models | ROICtrl-Bench | MIG-Bench | Instdiff-Bench | Training | Inference | Deployed | Support |\n|---|---|---|---|---|---|---|---|---|\n| mIoU | Acc | mIoU | Acc | AP | Color Acc | Texture Acc | Memory (G) | Speed (s/img) | Resolution | Emb Addon |\n| ROICtrl (Ours) | 0.652 | 48.7 | 0.66 | 0.73 | 41.0 | 62.3 | 29.3 | 34.3 | 13.1 | all | \u2713 |\n| Mask-Attn <br> ROICtrl (mask) | 0.628 | 49.2 | 0.64 | 0.71 | 37.1 | 62.5 | 30.3 | 65.5 | 31.5 | all | \u2713 |\n| Mask-Attn <br> Instance Diffusion | 0.607 | 45.6 | 0.46 | 0.51 | 40.1 | 55.2 | 26.1 | - | 129.2 | all | \u2717 |\n| MIGC | 0.490 | 38.9 | 0.56 | 0.66 | 22.4 | 53.8 | 24.3 | - | 23.5 | 8x, 16x | \u2717 |", "caption": "(c) Quantitative evaluation on the proposed ROICtrl-Bench. We assess spatial alignment using YOLO-World\u00a0[6] mIoU and evaluate regional text alignment with MiniCPM-V 2.6\u00a0[44]. Tracks 1 and 2 examine template-based instance caption, while tracks 3 and 4 evaluate free-form instance caption. * denote out-of-distribution caption rewritten by GPT-4\u00a0[1].", "description": "This table presents a quantitative comparison of different methods on the ROICtrl-Bench, a newly proposed benchmark for evaluating instance control in image generation.  It assesses both spatial and regional text alignment performance.  Spatial alignment is measured using mean Intersection over Union (mIoU) calculated by YOLO-World. Regional text alignment is measured by accuracy (Acc) using MiniCPM-V 2.6. The benchmark includes four tracks. Tracks 1 and 2 use template-based instance captions for in-distribution data, while Tracks 3 and 4 use free-form instance captions. Notably, Tracks 2 and 4 also include out-of-distribution captions generated by GPT-4, making the evaluation more comprehensive and realistic.", "section": "4. Experiments"}, {"content": "| Models | ROICtrl-Bench mIoU | ROICtrl-Bench Acc | MIG-Bench mIoU | MIG-Bench Acc | Instdiff-Bench AP | Instdiff-Bench Color Acc | Instdiff-Bench Texture Acc |\n|---|---|---|---|---|---|---|---| \n| ROICtrl (Ours) | 0.652 | 48.7 | 0.66 | 0.73 | 41.0 | 62.3 | 29.3 |\n| - ROI Self-Attn | 0.540 | 48.6 | 0.66 | 0.72 | 32.7 | 60.5 | 32.9 |\n| - \\mathcal{L}_{reg} | 0.658 | 47.2 | 0.66 | 0.72 | 41.1 | 58.2 | 21.9 |\n| global coord \\rightarrow local coord | 0.655 | 49.5 | 0.68 | 0.74 | 42.1 | 63.3 | 30.3 |\n| multi-scale roi \\rightarrow single-scale roi | 0.639 | 49.6 | 0.65 | 0.73 | 40.0 | 62.5 | 29.9 |", "caption": "Table 3: Ablation study comparing ROICtrl with attention mask\u2013based ROI injection. ROICtrl achieves similar regional text alignment but better spatial alignment, while significantly reducing memory and computational costs.\nThe inference speed is tested by generating a 10242 resolution image with 25 valid ROIs, 50 DDIM\u00a0[37] steps, and fp16 precision on an A100 GPU.", "description": "This ablation study compares ROICtrl's performance against a method using attention masks for handling Regions of Interest (ROIs) during image generation.  The comparison focuses on several key aspects: regional text alignment (how well the generated image matches the textual descriptions of each region), spatial alignment (how accurately the generated objects are positioned within their designated bounding boxes), memory usage, computational cost, and inference speed.  The inference speed was specifically measured by generating a single high-resolution image (1024x1024 pixels) containing 25 ROIs using the DDIM algorithm with specific parameters (50 steps and fp16 precision) on an A100 GPU. The results highlight ROICtrl's ability to achieve similar regional text alignment but superior spatial alignment compared to the attention mask method while significantly reducing memory and computation demands.", "section": "4.4 Ablation Study"}]