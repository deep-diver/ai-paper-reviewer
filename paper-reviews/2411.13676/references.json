{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is the foundation for many modern language models and is directly relevant to the architecture discussed in this paper."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2024-01-01", "reason": "This paper introduced the Mamba model, a state-space model, which is a core component of the Hymba architecture and is used for efficient context summarization."}, {"fullname_first_author": "Tri Dao", "paper_title": "Transformers are ssms: Generalized models and efficient algorithms through structured state space duality", "publication_date": "2024-01-01", "reason": "This paper provides the theoretical foundation for understanding the relationship between Transformers and State Space Models (SSMs), which is crucial for the hybrid approach used in Hymba."}, {"fullname_first_author": "Opher Lieber", "paper_title": "Jamba: A hybrid transformer-mamba language model", "publication_date": "2024-01-01", "reason": "This paper presented Jamba, a previous hybrid model that combines Transformers and SSMs, which Hymba builds upon and improves upon in terms of efficiency and performance."}, {"fullname_first_author": "Liliang Ren", "paper_title": "Samba: Simple hybrid state space models for efficient unlimited context language modeling", "publication_date": "2024-01-01", "reason": "This paper introduced Samba, another related hybrid model, providing additional context and comparison points for evaluating the Hymba architecture."}]}