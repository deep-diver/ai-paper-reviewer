[{"content": "| Design | Configuration | Param. Ratio | Avg. (General) \u2191 | Avg. (Recall) \u2191 | Throughput (Token/s) \u2191 | Cache (MB) \u2193 |\n|---|---|---|---|---|---|---|\n| Attn/Mamba Ratio | 1) Mamba Heads Only | 0:1 | 42.98 | 19.23 | 4720.8 | 1.87 |\n|  | 2) Mamba + 4 Attn Heads | 1:8.48 | 44.20 | 44.65 | 3278.1 | 99.09 |\n|  | 3) Mamba + 8 Attn Heads | 1:4.24 | 44.95 | 52.53 | 1816.5 | 197.39 |\n|  | 4) Mamba + 16 Attn Heads | 1:2.12 | 45.08 | 56.46 | 656.6 | 394.00 |\n|  | 5) 4) + GQA | 1:3.64 | 45.19 | 49.90 | 876.7 | 148.24 |\n|  | 6) Attn Heads Only (Llama) | 1:0 | 44.08 | 39.98 | 721.1 | 414.72 |\n| Sliding Window | 7) 5) + All SWA\u2019s | 1:3.64 | 44.42 | 29.78 | 4485.09 | 5.51 |\n|  | 8) 5) + SWA\u2019s + Full Attn | 1:3.64 | 44.56 | 48.79 | 2399.7 | 41.19 |\n|  | 9) 8) + Cross-layer KV sharing | 1:5.23 | 45.16 | 48.04 | 2756.5 | 39.42 |\n|  | 10) 6) + Same KV compression | 1:0 | 43.60 | 28.18 | 3710.0 | 28.98 |\n| Fusion | 11) 9) Replace Mean by Concat | 1: 5.82 | 44.56 | 48.94 | 1413.9 | 39.42 |\n| Meta Tokens | 12) 1) + Meta Tokens | 0:1 | 44.01 | 19.34 | 4712.8 | 1.87 |\n|  | 13) 9) + Meta Tokens | 1:5.23 | 45.53 | 51.79 | 2695.8 | 40.01 |", "caption": "Table 1: Design roadmap of our Hymba model. We evaluate the models\u2019 (1) commonsense reasoning accuracy, averaged over 8 tasks,\nand (2) recall accuracy, averaged over 2 tasks, which corresponds to retrieving relevant information from past input.\nThe throughput is on NVIDIA A100, sequence length 8k, batch size 128. The cache size is measured with a 8k sequence length, assuming the FP16 format.", "description": "This table presents a detailed ablation study illustrating the design choices and their impact on the Hymba language model.  It tracks the model's performance across various configurations, evaluating both commonsense reasoning (averaged over eight tasks) and recall accuracy (averaged across two tasks).  The metrics reported include throughput (measured on an NVIDIA A100 GPU with a sequence length of 8k and batch size of 128), and cache size (measured with an 8k sequence length using the FP16 format).  The table provides insights into the design decisions made to balance efficiency and performance.  Each row represents a specific configuration variation, allowing readers to observe the effects of individual design choices.", "section": "2. Hymba: The Proposed Hybrid-Head Architecture"}, {"content": "| Design |\n|---|---| \n| **Factor** |", "caption": "Table 2: Benchmark Hymba with SOTA small LMs. All models have fewer than 2B parameters, except for Llama-3.2-3B, which is marked as gray.\nAll results are obtained through lm-evaluation-harness\u00a0[28].\nSQuAD-C (SQuAD-Completion) indicates a variant of the SQuAD question answering task proposed by [29]. The throughput is measured with a 8k sequence length and a 128 batch size on an NVIDIA A100 GPU. The best results are highlighted in bold, and the second-best results are highlighted in underline, where Llama-3.2-3B is not included in the ranking due to its 3B model size.", "description": "This table compares the performance of the Hymba-1.5B language model against other state-of-the-art (SOTA) small language models.  It includes metrics such as average task accuracy across various benchmarks (MMLU, ARC-E, ARC-C, PIQA, Winogrande, Hellaswag, and SQUAD-C), cache size (in MB), and throughput (tokens per second).  The comparison highlights Hymba's performance advantages in terms of accuracy, efficiency (cache size and throughput), especially in comparison to models with similar parameter counts.  Llama-3.2-3B, despite its superior performance, is excluded from the primary ranking due to exceeding the 2B parameter limit.", "section": "3. Model Evaluations"}, {"content": "| Param. Ratio |\n|---|---| \n| **Attn:Mamba** |", "caption": "Table 3: Apple-to-apple comparison of our Hymba, pure Mamba2\u00a0[3], Mamba2 with FFN, Llama3\u00a0[39] style, and Samba-\u00a0[7] style (Mamba-FFN-Attn-FFN) architectures. All models have 1B parameters and are trained from scratch for 100B tokens from SmolLM-Corpus\u00a0[37] with exactly the same training recipe. All results are obtained through lm-evaluation-harness\u00a0[28] using a zero-shot setting by us on HuggingFace models. The best and second best results are highlighted in bold and underline, respectively.", "description": "This table presents a detailed comparison of five different language model architectures, all with 1 billion parameters.  The models are: Hymba (the authors' proposed model), pure Mamba2, Mamba2 with feed-forward networks (FFN), Llama3, and Samba (a hybrid architecture).  All models were trained from scratch using the same dataset (SmolLM-Corpus) and training process for 100 billion tokens.  The models are evaluated using zero-shot settings on various tasks through HuggingFace's `lm-evaluation-harness`, measuring their performance on language modeling, recall-intensive tasks, commonsense reasoning, and question answering.  The best and second-best performing models for each task are highlighted.", "section": "3.3. Benchmark Different Architectures Under The Same Setting"}, {"content": "| Avg. |\n|---|---| \n| (General) \u2191|", "caption": "Table 4: \nThe comparison between lightweight instruction-tuned models.\nThe best and second-best results are highlighted in bold and underlined, respectively.\n\u2217 OpenELM and SmolLM cannot understand function calling, leading to 0 accuracy in most categories.", "description": "This table compares the performance of several lightweight instruction-tuned language models on various downstream tasks.  The models are evaluated on their ability to perform instruction following and function calling.  Note that OpenELM and SmolLM models do not support function calling, resulting in zero accuracy for those categories.  The best and second-best results for each task are highlighted.", "section": "3. Model Evaluations"}, {"content": "| Avg. |\n|---|---| \n| (Recall) \u2191 |", "caption": "Table 5: The comparison between DoRA-finetuned Hymba\u00a0and baselines on RoleBench.\nAll baseline results are from\u00a0[14].", "description": "This table presents a comparison of the performance of a DoRA (Direct Preference Optimization)-finetuned Hymba 1.5B model against several baseline models on the RoleBench benchmark. RoleBench is a dataset designed to evaluate the capabilities of language models in role-playing scenarios.  The table likely shows metrics such as accuracy or other relevant performance measures on specific role-playing tasks within RoleBench.  The baseline models are presumably other LLMs, some potentially much larger than Hymba, making the comparison interesting in terms of parameter efficiency and performance.", "section": "3.4. Instruction-tuned Model"}, {"content": "| Throughput |\n|---|---| \n| (Token/s) \u2191 |", "caption": "Table 6: Benchmark Hymba with SOTA tiny LMs, all of which have fewer than 200M parameters. All results are obtained through Huggingface/LightEval, following Ben Allal et al.\u00a0[43].", "description": "This table compares the performance of Hymba-125M against other state-of-the-art (SOTA) small language models with fewer than 200 million parameters.  The models are evaluated on several downstream tasks using the Huggingface/LightEval benchmark, following the methodology described in Ben Allal et al. [43]. The table shows the performance of each model across various tasks, providing a comprehensive comparison of Hymba-125M against its competitors.", "section": "3. Model Evaluations"}, {"content": "| Cache | (MB) \u2193 |\n|---|---|", "caption": "Table 7: Benchmark Hymba with SOTA tiny LMs, all of which have fewer than 400M parameters. All results are obtained through Huggingface/LightEval, following Ben Allal et al.\u00a0[43].", "description": "This table compares the performance of Hymba language models (specifically, the 125M and 350M parameter versions) against other state-of-the-art (SOTA) tiny language models on various benchmark tasks.  The comparison focuses on models with fewer than 400M parameters.  The results are obtained using the Huggingface/LightEval framework, following the methodology outlined by Ben Allal et al. [43].  The benchmark tasks evaluate performance across different aspects of language understanding including commonsense reasoning, question answering, and recall-intensive tasks.", "section": "3. Model Evaluations"}, {"content": "| Attribute | 125M | 350M | 1.5B |\n|---|---|---|---| \n| **Blocks** | 24 | 32 | 32 |\n| **Hidden Size** | 512 | 768 | 1600 |\n| **SSM State** | 16 | 16 | 16 |\n| **Attn. Heads** | 8 | 12 | 25 |\n| **Query Groups** | 4 | 4 | 5 |\n| **Num. Full Attn** | 3 | 3 | 3 |\n| **Window Size** | 1024 | 1024 | 1024 |\n| **MLP Hidden** | 1664 | 2432 | 5504 |\n| **Tie Embedding** | True | True | True |\n| **Parameters** | 125M | 350M | 1.52B |", "caption": "Table 8: \nBenchmark Hymba-1.5B trained with all data and public data only against SOTA small LMs. All models have fewer than 2B parameters, except for Llama-3.2-3B, which is marked in gray. The settings follow Tab.\u00a02 in our main paper and we only include the most competitive baselines here. Hymba (Public Data) refers to our model trained exclusively on public datasets, without using our proprietary high-quality dataset.", "description": "This table compares the performance of the Hymba-1.5B language model trained on both public and private datasets against other state-of-the-art (SOTA) small language models.  It specifically focuses on models with fewer than 2 billion parameters, except for Llama-3.2-3B which is included for comparative purposes and highlighted as an exception.  The evaluation metrics include average task accuracy across several benchmarks (5-shot MMLU, ARC-E, ARC-C, PIQA, Winogrande, HellaSwag, and SQUAD-C), cache size, and throughput.  The table highlights the performance difference between Hymba trained on its proprietary, high-quality dataset and a version trained exclusively on publicly available datasets, illustrating the impact of data quality on model performance.", "section": "3. Model Evaluations"}]