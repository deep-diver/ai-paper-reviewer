[{"content": "| model | depth | width | mlp | heads | #params |\n|---|---|---|---|---|---| \n| RAR-B | 24 | 768 | 3072 | 16 | 261M |\n| RAR-L | 24 | 1024 | 4096 | 16 | 461M |\n| RAR-XL | 32 | 1280 | 5120 | 16 | 955M |\n| RAR-XXL | 40 | 1408 | 6144 | 16 | 1499M |", "caption": "Table 1: Architecture configurations of RAR. We follow prior works scaling up ViT\u00a0[19, 74] for different configurations.", "description": "Table 1 details the different model architectures used in the Randomized Autoregressive visual generation experiments.  It shows how the model's depth, width, MLP size, and number of attention heads vary across four different configurations (RAR-B, RAR-L, RAR-XL, and RAR-XXL).  These configurations are based on scaling up the Vision Transformer (ViT) architecture, following the approach used in prior research.", "section": "4. Experimental Results"}, {"content": "| start epoch | end epoch | FID \u2193 | IS \u2191 | Pre. \u2191 | Rec. \u2191 |\n|---|---|---|---|---|---| \n| 0 | 0\u2020 | 3.08 | 245.3 | 0.85 | 0.52 |\n| 0 | 100 | 2.68 | 237.3 | 0.84 | 0.54 |\n| 0 | 200 | 2.41 | 251.5 | 0.84 | 0.54 |\n| 0 | 300 | 2.40 | 258.4 | 0.84 | 0.54 |\n| 0 | 400 | 2.43 | 265.3 | 0.84 | 0.53 |\n| 100 | 100 | 2.48 | 247.5 | 0.84 | 0.54 |\n| 100 | 200 | 2.28 | 253.1 | 0.83 | 0.55 |\n| 100 | 300 | 2.33 | 258.4 | 0.83 | 0.54 |\n| 100 | 400 | 2.39 | 266.5 | 0.84 | 0.54 |\n| 200 | 200 | 2.39 | 259.7 | 0.84 | 0.54 |\n| 200 | 300 | 2.18 | 269.7 | 0.83 | 0.55 |\n| 200 | 400 | 2.55 | 241.6 | 0.84 | 0.54 |\n| 300 | 300 | 2.41 | 269.1 | 0.84 | 0.53 |\n| 300 | 400 | 2.74 | 236.4 | 0.83 | 0.54 |\n| 400 | 400\u2021 | 3.01 | 305.6 | 0.84 | 0.52 |", "caption": "Table 2: Different start and end epochs for randomness annealing, with a total of 400 training epochs and model size RAR-L. The final setting is labeled in gray. \u2020: When start epoch and end epoch are both 00 (1st row), the training reverts to a standard raster order training. \u2021: When start epoch and end epoch are both 400400400400 (last row), the training becomes a purely random order training. After training is finished, all results are obtained with raster order sampling, except for the purely random order training (i.e., last row), where we also randomly sample the scan order following\u00a0[36], which otherwise could not produce a reasonable result.", "description": "This table presents an ablation study on the randomness annealing strategy used in the RAR model.  It shows the impact of varying the start and end epochs of the annealing process on the model's performance, as measured by FID, IS, Precision, and Recall. The total number of training epochs is fixed at 400. The first row represents training with a purely raster scan order, while the last row shows results from training with purely random scan orders. The gray row indicates the chosen configuration used in the rest of the paper. The table also highlights the importance of the gradual transition between purely random to raster order in the annealing process.", "section": "4.2. Ablation Studies"}, {"content": "| scan order | FID \u2193 | IS \u2191 | Precision \u2191 | Recall \u2191 |\n|---|---|---|---|---|\n| row-major | 2.18 | 269.7 | 0.83 | 0.55 |\n| spiral in | 2.50 | 256.1 | 0.84 | 0.54 |\n| spiral out | 2.46 | 256.6 | 0.84 | 0.54 |\n| z-curve | 2.29 | 262.7 | 0.83 | 0.55 |\n| subsample | 2.39 | 258.0 | 0.84 | 0.54 |\n| alternate | 2.48 | 270.9 | 0.84 | 0.53 |", "caption": "Table 3: Effect of different scan orders RAR-L converges to. We mainly consider 6 different scan orders (row major, spiral in, spiral out, z-curve, subsample, alternate) as studied in\u00a0[22]. Our default setting is marked in gray. A visual illustration of different scan orders are available in the appendix.", "description": "This table investigates the impact of different image scanning orders on the performance of the RAR-L model. Six common scan orders, including the standard row-major order, are compared.  The results show the final FID, Inception Score (IS), precision, and recall after training with each scan order. The default settings used in the experiments are highlighted in gray for easy reference.  A visual representation of each scan order is provided in the appendix for better understanding.", "section": "4.2. Ablation Studies"}, {"content": "## Table 1: Comparison of different text-to-image models\n\n| tokenizer | type | generator | #params | FID \u2193 | IS \u2191 | Pre. \u2191 | Rec. \u2191 |\n|---|---|---|---|---|---|---|---| \n| VQ [50] | Diff. | LDM-8 [50] | 258M | 7.76 | 209.5 | 0.84 | 0.35 |\n| VAE [50] | Diff. | LDM-4 [50] | 400M | 3.60 | 247.7 | 0.87 | 0.48 |\n| VAE [51] | Diff. | UViT-L/2 [6] | 287M | 3.40 | 219.9 | 0.83 | 0.52 |\n|  |  | UViT-H/2 [6] | 501M | 2.29 | 263.9 | 0.82 | 0.57 |\n|  |  | DiT-L/2 [45] | 458M | 5.02 | 167.2 | 0.75 | 0.57 |\n|  |  | DiT-XL/2 [45] | 675M | 2.27 | 278.2 | 0.83 | 0.57 |\n|  |  | SiT-XL [40] | 675M | 2.06 | 270.3 | 0.82 | 0.59 |\n|  |  | DiMR-XL/2R [37] | 505M | 1.70 | 289.0 | 0.79 | 0.63 |\n|  |  | MDTv2-XL/2 [25] | 676M | 1.58 | 314.7 | 0.79 | 0.65 |\n| VQ [10] | Mask. | MaskGIT [10] | 177M | 6.18 | 182.1 | - | - |\n| VQ [73] | Mask. | TiTok-S-128 [73] | 287M | 1.97 | 281.8 | - | - |\n| VQ [72] | Mask. | MAGVIT-v2 [72] | 307M | 1.78 | 319.4 | - | - |\n| VQ [65] | Mask. | MaskBit [65] | 305M | 1.52 | 328.6 | - | - |\n| VAE [36] | MAR | MAR-B [36] | 208M | 2.31 | 281.7 | 0.82 | 0.57 |\n|  |  | MAR-L [36] | 479M | 1.78 | 296.0 | 0.81 | 0.60 |\n|  |  | MAR-H [36] | 943M | 1.55 | 303.7 | 0.81 | 0.62 |\n| VQ [58] | VAR | VAR-d30 [58] | 2.0B | 1.92 | 323.1 | 0.82 | 0.59 |\n|  |  | VAR-d30-re [58] | 2.0B | 1.73 | 350.2 | 0.82 | 0.60 |\n| VQ [22] | AR | GPT2 [22] | 1.4B | 15.78 | 74.3 | - | - |\n|  |  | GPT2-re [22] | 1.4B | 5.20 | 280.3 | - | - |\n| VQ [69] | AR | VIM-L [69] | 1.7B | 4.17 | 175.1 | - | - |\n|  |  | VIM-L-re [69] | 1.7B | 3.04 | 227.4 | - | - |\n| VQ [39] | AR | Open-MAGVIT2-B [39] | 343M | 3.08 | 258.3 | 0.85 | 0.51 |\n|  |  | Open-MAGVIT2-L [39] | 804M | 2.51 | 271.7 | 0.84 | 0.54 |\n|  |  | Open-MAGVIT2-XL [39] | 1.5B | 2.33 | 271.8 | 0.84 | 0.54 |\n| VQ [52] | AR | LlamaGen-L [52] | 343M | 3.80 | 248.3 | 0.83 | 0.51 |\n|  |  | LlamaGen-XL [52] | 775M | 3.39 | 227.1 | 0.81 | 0.54 |\n|  |  | LlamaGen-XXL [52] | 1.4B | 3.09 | 253.6 | 0.83 | 0.53 |\n|  |  | LlamaGen-3B [52] | 3.1B | 3.05 | 222.3 | 0.80 | 0.58 |\n|  |  | LlamaGen-L-384 [52] | 343M | 3.07 | 256.1 | 0.83 | 0.52 |\n|  |  | LlamaGen-XL-384 [52] | 775M | 2.62 | 244.1 | 0.80 | 0.57 |\n|  |  | LlamaGen-XXL-384 [52] | 1.4B | 2.34 | 253.9 | 0.80 | 0.59 |\n|  |  | LlamaGen-3B-384 [52] | 3.1B | 2.18 | 263.3 | 0.81 | 0.58 |\n| VQ [10] | AR | RAR-B (ours) | 261M | 1.95 | 290.5 | 0.82 | 0.58 |\n|  |  | RAR-L (ours) | 461M | 1.70 | 299.5 | 0.81 | 0.60 |\n|  |  | RAR-XL (ours) | 955M | 1.50 | 306.9 | 0.80 | 0.62 |\n|  |  | RAR-XXL (ours) | 1.5B | **1.48** | **326.0** | 0.80 | 0.63 |", "caption": "Table 4: ImageNet-1K 256\u00d7256256256256\\times 256256 \u00d7 256 generation results evaluated with ADM\u00a0[18]. \u201ctype\u201d refers to the type of the generative model, where \u201cDiff.\u201d and \u201cMask.\u201d stand for diffusion models and masked transformer models, respectively. \u201cVQ\u201d denotes discrete tokenizers and \u201cVAE\u201d stands for continuous tokenizers. \u201c-re\u201d stands for rejection sampling. \u201c-384\u201d denotes for generating images at resolution 384384384384 and resize back to 256256256256 for evaluation, as is used in\u00a0[52].", "description": "Table 4 presents a comparison of various image generation models on the ImageNet-1K dataset, focusing on 256x256 image generation.  The models are categorized by type (diffusion, masked transformer, autoregressive), tokenizer type (discrete VQ or continuous VAE), and whether rejection sampling was used.  Results are evaluated using the Fr\u00e9chet Inception Distance (FID) metric, with additional metrics provided in some cases.  Note that some models generate images at a resolution of 384x384 and then resize to 256x256 for consistent evaluation.", "section": "4.3 Main Results"}, {"content": "| method | type | #params | FID \u2193 | steps | images/sec |\n|---|---|---|---|---|---| \n| DiT-XL/2 [45] | Diff. | 675M | 2.27 | 250 | 0.6 |\n| TiTok-S-128 [73] | Mask. | 287M | 1.97 | 64 | 7.8 |\n| VAR-d30 [58] | VAR | 2.0B | 1.92 | 10 | 17.3 |\n| MAR-B [36] | MAR | 208M | 2.31 | 256 | 0.8 |\n| RAR-B (ours) | AR | 261M | 1.95 | 256 | 17.0 |\n| MAR-L [36] | MAR | 479M | 1.78 | 256 | 0.5 |\n| RAR-L (ours) | AR | 461M | 1.70 | 256 | 15.0 |\n| MaskBit [65] | Mask. | 305M | 1.52 | 256 | 0.7 |\n| MAR-H [36] | MAR | 943M | 1.55 | 256 | 0.3 |\n| RAR-XL (ours) | AR | 955M | 1.50 | 256 | 8.3 |\n| RAR-XXL (ours) | AR | 1.5B | 1.48 | 256 | 6.4 |", "caption": "Table 5: Sampling throughput comparison (including de-tokenization process) categorized by methods with similar FID scores. Throughputs are measured as samples generated per second on a single A100 using float32 precision and a batch size of 128128128128, based on their official codebases. For VAR\u00a0[58] and our RAR, KV-cache is applied. \u201cDiff.\u201d and \u201cMask.\u201d refer to diffusion models and masked transformer models, respectively.", "description": "This table compares the speed of generating images (samples/second) using different image generation models on a single NVIDIA A100 GPU.  The models are grouped based on their Fr\u00e9chet Inception Distance (FID) scores, a metric indicating image quality, to ensure a fair comparison.  The throughput is measured using float32 precision and a batch size of 128, following the original codebases of each method. Notably, the models using autoregressive architectures (RAR and VAR) utilize KV-cache optimization for efficiency, resulting in higher speeds.  'Diff.' indicates diffusion models and 'Mask.' represents masked transformer models. The table highlights how the proposed RAR method is not only efficient in generating images but also significantly faster than many other methods with comparable FID scores.", "section": "4.3 Main Results"}, {"content": "| config | value |\n|---|---| \n| *training hyper-params* |  |\n| optimizer | AdamW [33, 38] |\n| learning rate | 4e-4 |\n| weight decay | 0.03 |\n| optimizer momentum | (0.9, 0.96) |\n| batch size | 2048 |\n| learning rate schedule | cosine decay |\n| ending learning rate | 1e-5 |\n| total epochs | 400 |\n| warmup epochs | 100 |\n| annealing start epoch | 200 |\n| annealing end epoch | 300 |\n| precision | bfloat16 |\n| max grad norm | 1.0 |\n| dropout rate | 0.1 |\n| attn dropout rate | 0.1 |\n| class label dropout rate | 0.1 |\n| *sampling hyper-params* |  |\n| guidance schedule | pow-cosine [25] |\n| temperature | 1.0 (B) / 1.02 (L, XL, XXL) |\n| scale power | 2.75 (B) / 2.5 (L) / 1.5 (XL) / 1.2 (XXL) |\n| guidance scale | 16.0 (B) / 15.5 (L) / 6.9 (XL) / 8.0 (XXL) |", "caption": "Table 6: Detailed hyper-parameters for final RAR models.", "description": "Table 6 presents the detailed hyperparameter settings used for training the final versions of the Randomized Autoregressive (RAR) models.  These settings encompass both training hyperparameters (optimizer, learning rate, weight decay, batch size, learning rate schedule, etc.) and sampling hyperparameters (temperature, scale power, and guidance scale), offering a comprehensive overview of the configuration employed to achieve the reported results. The table is broken down into two sections, one for training and one for sampling, which provides clarity in understanding the various parameters.", "section": "3. Method"}]