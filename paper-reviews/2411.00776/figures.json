[{"figure_path": "https://arxiv.org/html/2411.00776/x1.png", "caption": "Figure 1: \nComparison among different language modeling compatible autoregressive (AR) image generators.\nThe proposed RAR demonstrates significant improvements over previous AR methods. RAR-B, with only 261M parameters, achieves an FID score of 1.95, outperforming both LlamaGen-XXL (1.4B parameters) and Open-MAGVIT2-XL (1.5B parameters).", "description": "The figure shows a comparison of the Fr\u00e9chet Inception Distance (FID) scores achieved by various autoregressive (AR) image generation models, including the proposed Randomized Autoregressive (RAR) model.  Lower FID scores indicate better image quality. RAR-B, a smaller model with only 261 million parameters, achieves an FID of 1.95, outperforming significantly larger models like LlamaGen-XXL (1.4 billion parameters) and Open-MAGVIT2-XL (1.5 billion parameters). This highlights the effectiveness of RAR in improving image generation quality while maintaining compatibility with language modeling frameworks.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.00776/x2.png", "caption": "Figure 2: \nOverview of the proposed Randomized AutoRegressive (RAR) model, which is fully compatible with language modeling frameworks.\nLeft: RAR introduces a randomness annealing training strategy to enhance the model\u2019s ability to learn bidirectional contexts. During training, the input sequence is randomly permuted with a probability r\ud835\udc5fritalic_r, which starts at 1 (fully random permutations) and linearly decreases to 0, transitioning the model to a fixed scan order, such as raster scan, by the end of training.\nRight: Randomly selected images generated by RAR, trained on ImageNet.", "description": "Figure 2 illustrates the Randomized Autoregressive (RAR) model, designed for visual generation while maintaining compatibility with language modeling frameworks. The left panel demonstrates the RAR training process: input sequences are randomly permuted with a probability *r*, initially 1 (fully random) and decreasing linearly to 0 during training. This annealing strategy helps the model learn bidirectional contexts by maximizing the likelihood across various permutation orders, eventually converging to a fixed raster scan.  The right panel showcases example images generated by the trained RAR model using the ImageNet dataset.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.00776/x3.png", "caption": "Figure 3: \nIllustration of the target-aware positional embedding. Subfigure (a) shows the training process of the proposed Randomized AutoRegressive (RAR) model, along with the target-aware position embedding.\nFollowing Vision Transformer\u00a0[19], images are tokenized into patches with original position embeddings (blue tokens).\nThe token sequence is then randomly permuted, with the target-aware positional embeddings (green tokens) added to guide the model.\nSubfigures (b) and (c) highlight the importance of the target-aware positional embedding: (b) demonstrates a failure case where both permuted sequences yield identical prediction logits, while (c) shows that the target-aware positional embedding correctly guides the model to predict the next token accurately.", "description": "Figure 3 illustrates the concept of target-aware positional embeddings within the Randomized Autoregressive (RAR) model.  Panel (a) depicts the training process: images are first tokenized into patches (following the Vision Transformer architecture), each patch receiving an initial positional embedding (blue tokens). The token sequence is then randomly permuted.  Crucially, a *target-aware* positional embedding (green tokens) is added to each token to inform the model which token it should predict next. Panels (b) and (c) showcase the importance of these target-aware embeddings.  Panel (b) shows a failure scenario where, without them, two different permuted sequences produce identical predictions because the original positional embeddings alone aren't sufficient to distinguish the correct prediction in the context of a random permutation. Panel (c) demonstrates that the inclusion of target-aware positional embeddings successfully guides the model toward the correct next-token prediction, even with a randomly permuted input sequence.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.00776/x4.png", "caption": "(a) \ntraining losses", "description": "This figure shows the scaling behavior of the RAR model across different sizes (RAR-B, RAR-L, RAR-XL, RAR-XXL). Subfigure (a) presents the training loss curves for each model variant over training steps. Subfigures (b) and (c) illustrate the FID scores (a metric evaluating image generation quality) with and without classifier-free guidance, respectively.  The plots demonstrate how larger models generally achieve lower training losses and better FID scores.", "section": "4. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.00776/x5.png", "caption": "(b) \nFID scores w/o classifier-free guidance", "description": "This figure shows the FID scores achieved by different sized RAR models (RAR-B, RAR-L, RAR-XL, RAR-XXL) without using classifier-free guidance during training.  The x-axis represents the training steps, showing the FID score progression over the training process. Different lines represent the FID for each model size.  The purpose is to demonstrate the impact of model size on the FID score and assess how well the model generalizes.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2411.00776/x6.png", "caption": "(c) \nFID scores w/ classifier-free guidance", "description": "This figure shows the FID (Fr\u00e9chet Inception Distance) scores achieved by different sized RAR models (RAR-B, RAR-L, RAR-XL, RAR-XXL) when using classifier-free guidance during training.  Lower FID scores indicate better image generation quality. The x-axis represents the training steps, showing the progress over the training period. The plot demonstrates the improvement in FID score as model size increases and the effectiveness of classifier-free guidance in enhancing the image generation capabilities of the RAR models.", "section": "4. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.00776/x7.png", "caption": "Figure 4: \nScaling behavior of RAR models.\nThe scaled-up RAR models demonstrate (a) reduced training losses, and improved FID scores both (b) without and (c) with classifier-free guidance.", "description": "This figure analyzes the scaling behavior of the Randomized Autoregressive (RAR) model across different sizes.  Subfigure (a) shows that as the model size increases, the training loss decreases, indicating improved model training efficiency. Subfigures (b) and (c) present the Fr\u00e9chet Inception Distance (FID) scores, a metric for evaluating image quality, with and without classifier-free guidance, respectively. Both subfigures show that larger RAR models consistently achieve lower FID scores, demonstrating that scaling up the model significantly improves the image quality generated.", "section": "4. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.00776/x8.png", "caption": "Figure 5: Visualization of samples generated by RAR across various model sizes.\nRAR generates high-quality visual samples across all model sizes. As model size increases, fidelity and diversity improve, especially in challenging classes (e.g., dogsled).", "description": "This figure displays example images generated by the RAR model at different scales (RAR-B, RAR-L, RAR-XL, and RAR-XXL).  The images demonstrate the model's ability to generate high-quality images across all model sizes. Notably, as the model size increases, the fidelity and diversity of the generated images improve. This improvement is particularly evident in complex or challenging classes, such as the example of a 'dogsled' which contains many fine details and multiple objects.", "section": "4. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.00776/x9.png", "caption": "(a) row-major", "description": "This figure visualizes six different scan orders for a 16x16 grid (256 tokens). Each subfigure displays one scan order, showing the order in which tokens are processed.  The numbers within each grid represent the index of the token according to that scan order. The scan orders visualized are row-major, spiral in, spiral out, z-curve, subsample, and alternate.", "section": "4.2. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.00776/x10.png", "caption": "(b) spiral in", "description": "This subfigure shows one of the six different scan orders tested in the paper for image generation.  The spiral scan order starts from the center of the image and spirals outwards, processing pixels in a circular pattern. The numbers in the image indicate the sequence in which each token (representing a pixel or a patch of pixels) is processed.  This visualization helps illustrate how different scan orders affect the order of information received by the autoregressive model during training and generation.", "section": "4.2. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.00776/x11.png", "caption": "(c) spiral out", "description": "This figure is a visualization of one of six different scan orders used for processing a 16x16 image (256 tokens) within an autoregressive model. Specifically, it showcases the \"spiral out\" scan order, where the tokens are processed in a spiral pattern, starting from the center and expanding outwards.  The numbers in each cell represent the order in which the tokens are processed.", "section": "Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.00776/x12.png", "caption": "(d) z-curve", "description": "This subfigure shows a visualization of the 'z-curve' scan order for a 16x16 grid (256 tokens).  A z-curve is a space-filling curve that traverses a grid in a pattern resembling the letter 'Z'. This particular visualization displays the order in which the tokens are processed, with each number representing the index of the token in the scan order.", "section": "C. Visualization of Scan Orders"}, {"figure_path": "https://arxiv.org/html/2411.00776/x13.png", "caption": "(e) subsample", "description": "This image shows a visualization of the \"subsample\" scan order for a 16x16 grid (256 tokens).  The numbers represent the order in which the tokens are processed.  Unlike a raster scan which would process tokens sequentially, row by row, this subsampling pattern skips tokens in a specific way. The pattern is designed to demonstrate an alternative autoregressive factorization of the image data, which is explored in the paper as a method to improve context modeling.", "section": "4.2. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.00776/x14.png", "caption": "(f) alternate", "description": "This figure visualizes one of the six different scan orders evaluated in the paper for autoregressive image generation.  The alternate scan order processes the image tokens in an alternating pattern across rows, starting from the top left, then moving to the second row from the left, and so on. The numbers represent the order in which the tokens are scanned.", "section": "4.2. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.00776/x15.png", "caption": "Figure 6: Different scan orders for a 16\u00d716161616\\times 1616 \u00d7 16 grid (256 tokens). The number indicates the token\u2019s indices in the scanning order.", "description": "Figure 6 visualizes six different ways of scanning a 16x16 grid (256 tokens), representing different orders for processing image data in an autoregressive model. Each scan order is displayed as a grid where the numbers indicate the order in which the model processes the tokens.  This illustrates the impact of different scan orders on how the model learns and generates images, particularly focusing on the tradeoff between unidirectional (raster scan) and bidirectional (randomized scan) processing of the image.  The visualization is directly relevant to the exploration of how the model's ability to learn and utilize bidirectional context is affected by different factorization orders of the image data during training.  The figure is important to show the impact on model learning as the various scanning approaches in the ablation study can significantly impact the model's learning of contextual information in the model.", "section": "4.2. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.00776/x16.png", "caption": "Figure 7: \nVisualization samples from RAR. RAR is capable of generating high-fidelity image samples with great diversity.", "description": "Figure 7 showcases a diverse set of images generated by the Randomized Autoregressive (RAR) model.  The images demonstrate the model's ability to generate high-quality, detailed, and visually diverse samples across a wide range of classes and object characteristics, highlighting its strong performance in image generation.", "section": "4.3 Main Results"}]