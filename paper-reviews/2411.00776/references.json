{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for demonstrating the capabilities of autoregressive models in few-shot learning, which directly relates to the core idea of the current paper."}, {"fullname_first_author": "Huiwen Chang", "paper_title": "MaskGIT: Masked generative image transformer", "publication_date": "2022-01-01", "reason": "This paper introduces the MaskGIT framework, a pivotal advancement in autoregressive image generation that is directly built upon and improved in the current paper."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "publication_date": "2021-01-01", "reason": "This paper is highly influential in demonstrating the potential of autoregressive models for high-quality image generation, setting a benchmark against which the current research is compared."}, {"fullname_first_author": "Tianhong Li", "paper_title": "Autoregressive image generation without vector quantization", "publication_date": "2024-01-01", "reason": "This work is closely related to the proposed method, representing a recent state-of-the-art approach in autoregressive image generation that the current paper directly builds upon and improves."}, {"fullname_first_author": "Peize Sun", "paper_title": "Autoregressive model beats diffusion: Llama for scalable image generation", "publication_date": "2024-06-01", "reason": "This is a highly relevant contemporary work that pushes the boundaries of autoregressive image generation and sets a strong benchmark that the current paper surpasses."}]}