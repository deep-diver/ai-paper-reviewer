{"importance": "This paper is crucial because **it significantly advances autoregressive visual generation**, a vital area in computer vision.  By introducing a novel training strategy, **it achieves state-of-the-art results**, surpassing both previous autoregressive and other leading methods.  This opens avenues for research in unified multimodal models and scalable visual generation.", "summary": "Randomized Autoregressive Modeling (RAR) sets a new state-of-the-art in image generation by cleverly introducing randomness during training to improve the model's ability to learn from bidirectional contexts while maintaining compatibility with language modeling frameworks.", "takeaways": ["RAR achieves state-of-the-art FID scores on ImageNet-256, outperforming existing autoregressive and other leading methods.", "RAR's randomness annealing training strategy enables learning of bidirectional contexts within an autoregressive framework, addressing limitations of unidirectional approaches.", "RAR maintains full compatibility with language modeling frameworks, offering potential for unified multimodal models."], "tldr": "Autoregressive models have shown promise in image generation, but they often lag behind diffusion models due to their inherent unidirectional nature which is not ideal for visual data. Existing attempts to improve this by adding bidirectional attention often deviate from the traditional autoregressive paradigm, hindering their integration into unified multimodal models. \nThis paper introduces Randomized Autoregressive Modeling (RAR), a simple yet effective technique to enhance the performance of autoregressive image generation models without altering the core framework. **RAR randomly permutes the input sequence during training, encouraging the model to learn from all possible factorization orders**. This process, combined with a randomness annealing strategy, effectively improves bidirectional context modeling, leading to significant gains in image generation quality while maintaining compatibility with language modeling frameworks.  **The results show RAR outperforms state-of-the-art methods on the ImageNet-256 benchmark.**", "affiliation": "ByteDance", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}}