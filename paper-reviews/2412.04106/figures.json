[{"figure_path": "https://arxiv.org/html/2412.04106/x2.png", "caption": "Figure 1: \nMotivations and Overview.\nLeft:\nThe heterogeneity of MRI modalities poses challenges to the generalization of segmentation models.\nOur proposed data engine, MRGen, addresses this by controllably synthesizing training data, enabling segmentation towards mask-unannotated modalities.\nRight:\n(a) Previous generative models rely on mask-annotated data and are restricted to data augmention in training modalities;\n(b) Image translation typically requires registered data pairs\u00a0(indicated by dashed lines), limited to conversions between specific modalities;\n(c) MRGen introduces a new paradigm, enabling controllable generation across multiple modalities without relying on registered data pairs or mask-annotated data from target modalities.\nDifferent colors represent distinct modalities.", "description": "Figure 1 illustrates the core idea and motivation behind the MRGen model. The left panel shows the challenge of MRI modality heterogeneity for generalizable segmentation models.  The right panel compares MRGen to prior approaches. (a) shows conventional generation-based augmentation methods limited to training data with masks. (b) shows conventional image translation limited to registered data pairs. (c) shows MRGen, which generates data across modalities without requiring registered data or mask annotations. Different colors represent different MRI modalities.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.04106/x3.png", "caption": "Figure 2: \nArchitecture Overview.\nDeveloping our MRGen involves three key steps:\n(a) Train an autoencoder on various images from dataset \ud835\udc9fusubscript\ud835\udc9f\ud835\udc62\\mathcal{D}_{u}caligraphic_D start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT;\n(b) Train a text-guided generative model within the latent space, using image-text pairs across diverse modalities from \ud835\udc9fusubscript\ud835\udc9f\ud835\udc62\\mathcal{D}_{u}caligraphic_D start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT, featuring modality, attributes, region, and organs information;\n(c) Train a mask condition controller jointly on the mask-annotated source-domain dataset \ud835\udc9fssubscript\ud835\udc9f\ud835\udc60\\mathcal{D}_{s}caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT and unannotated target-domain dataset \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}_{t}caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, enabling controllable generation based on both text prompts and masks.", "description": "This figure illustrates the architecture of MRGen, a diffusion-based controllable data engine.  It details the three key steps involved in developing MRGen: 1) training an autoencoder on a variety of images from dataset \ud835\udc9f<sub>u</sub> to learn a compressed latent representation; 2) training a text-guided generative model in the latent space using image-text pairs from \ud835\udc9f<sub>u</sub>, incorporating modality, attributes, region, and organ information; 3) jointly training a mask condition controller on mask-annotated source-domain data \ud835\udc9f<sub>s</sub> and unannotated target-domain data \ud835\udc9f<sub>t</sub> to enable controllable generation based on text prompts and masks. This architecture allows MRGen to generate high-quality medical images for modalities lacking annotations.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2412.04106/x4.png", "caption": "Figure 3: \nSynthetic Data Construction Pipeline.\nMRGen takes text prompt and mask as conditions for controllably generating radiology images and employs a pretrained SAM2 model for automatic filtering to guarantee the quality of generated samples.", "description": "The figure illustrates the process of creating synthetic medical images using the MRGen model.  First, text prompts describing the desired image content and segmentation masks are input into the MRGen model.  MRGen then generates radiology images based on these conditions. Finally, a pre-trained SAM2 model automatically filters these generated images, selecting only high-quality images that accurately reflect the input text and masks. This ensures that the synthetic images used for training segmentation models meet quality standards.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2412.04106/x5.png", "caption": "Figure 4: \nQualitative Results of Controllable Generation.\nWe present images from the source domain \ud835\udc9fssubscript\ud835\udc9f\ud835\udc60\\mathcal{D}_{s}caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT and the target domain \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}_{t}caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT for reference.\nHere, prostate, liver, right kidney, left kidney and spleen are contoured with different colors.", "description": "Figure 4 showcases the effectiveness of the MRGen model in generating high-quality medical images for various modalities.  It presents pairs of images; one from a source domain (with existing mask annotations) and the other from a target domain (lacking annotations). MRGen successfully synthesizes images in the target domain (unannotated MRI modalities) that are conditioned upon text descriptions and organ masks. The figure highlights several key organs (prostate, liver, right kidney, left kidney, and spleen) in different colors for easy identification and comparison between the source and target images.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04106/x6.png", "caption": "Figure 5: \nQualitative Results of Segmentation towards Unannotated Modalities.\nWe present reference images from the source-domain training set \ud835\udc9fssubscript\ud835\udc9f\ud835\udc60\\mathcal{D}_{s}caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, images from the target-domain test set \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}_{t}caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, and corresponding predictions and ground truth.\nWe visualize prostate on the middle row, and liver, right kidney, left kidney and spleen on the upper and lower row with different colors.", "description": "This figure shows a qualitative comparison of the segmentation results on unseen MRI modalities.  The top row displays the source domain training images (from dataset \nD<sub>s</sub>), the middle row contains the target domain test images (from dataset \nD<sub>t</sub>)  and the bottom row shows the same target domain images. Each column represents a different segmentation method: nnU-Net, DualNorm, CycleGAN, MRGen and Ground Truth. Different organs (prostate, liver, right kidney, left kidney, and spleen) are color-coded in each image for easy comparison.  The results demonstrate the performance of each segmentation method when trained using synthetic data generated by MRGen, compared to other methods and the ground truth.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04106/x7.png", "caption": "Figure 6: \nData Statistics of Radiopaedia.\n(a) Distribution of slice counts across various modalities in Radiopaedia-MRI;\n(b) Proportional distribution of slices across different regions in Radiopaedia-CT and Radiopaedia-MRI.", "description": "This figure presents a statistical overview of the Radiopaedia dataset used in the study. Subfigure (a) shows the distribution of the number of slices across different MRI modalities within the Radiopaedia-MRI subset.  This illustrates the variety of MRI scans present in the dataset. Subfigure (b) displays the proportional distribution of slices across six anatomical regions (Upper Thoracic, Middle Thoracic, Lower Thoracic, Upper Abdominal, Lower Abdominal, and Pelvic) in both the Radiopaedia-CT and Radiopaedia-MRI subsets. This visualization helps to understand the regional coverage and balance within the dataset.", "section": "3. Data Curation"}, {"figure_path": "https://arxiv.org/html/2412.04106/x8.png", "caption": "Figure 7: \nQualitative Results of In-domain Generation.", "description": "This figure visualizes the results of in-domain image generation, comparing the ground truth images to those generated by MRGen and other baselines (DualNorm and CycleGAN).  The top row shows results for T1-weighted MRI images from the CHAOS-MRI dataset, while the bottom row presents results for T2-SPIR images from the same dataset. It demonstrates MRGen's capability to generate high-quality images within the same modality as the training data, showcasing its superior performance compared to baselines.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04106/x9.png", "caption": "Table 7: Ablation Studies on Autoencoder Reconstruction.", "description": "This table presents results from ablation studies conducted on the autoencoder part of the MRGen model.  The goal was to assess different configurations and their impact on reconstruction quality.  Key variations studied include the dimension of the latent space and the datasets used for training. Metrics like PSNR, SSIM, L1 error, and MSE were used to evaluate the reconstruction performance.", "section": "4.2. Architecture"}, {"figure_path": "https://arxiv.org/html/2412.04106/x10.png", "caption": "Table 8: Ablation Studies on Text-guided Generation.", "description": "This table presents the results of ablation studies performed on the text-guided image generation model.  It compares the performance of different model variations, evaluating their FID (Fr\u00e9chet Inception Distance), CLIP-I (image-to-image similarity using CLIP), and CLIP-T (image-to-text similarity using CLIP) scores. The variations tested include a pretrained Stable Diffusion model, a model fine-tuned on the MedGen-1M dataset, a model using only free-text modality labels, and the full MRGen model utilizing templated text prompts with detailed information. The results show the impact of different training data, text encoding strategies, and text prompt detail levels on the overall performance of the text-guided generation.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04106/x11.png", "caption": "Figure 8: \nFailure Cases Analysis.\nOur proposed MRGen is not without limitations:\n(a) it may struggle to handle extremely small organ masks;\n(b) it occasionally produces false-negative samples, such as the unexpected synthesis of kidneys in the given example.", "description": "Figure 8 demonstrates limitations of the MRGen model.  Subfigure (a) shows that MRGen struggles to generate images with high fidelity when conditioned on extremely small organ masks. Subfigure (b) illustrates that MRGen occasionally produces false negatives, generating organs (e.g. kidneys) not present in the input mask.", "section": "5.4. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.04106/x12.png", "caption": "Figure 9: \nMore Qualitative Results of Controllable Generation.\nWe present images from the source domain \ud835\udc9fssubscript\ud835\udc9f\ud835\udc60\\mathcal{D}_{s}caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT and the target domain \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}_{t}caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT for reference.\nHere, specific organs are contoured with distinct colors: prostate in MSD-Prostate and PROMISE12 datasets, and pancreas in PanSeg dataset, and liver, right kidney, left kidney and spleen in other datasets.", "description": "Figure 9 presents a qualitative comparison of images generated by DualNorm, CycleGAN, and the proposed MRGen model.  The figure displays pairs of images, where each pair consists of an image from a source domain (with ground truth annotations) and a corresponding image from a target domain (lacking annotations). The source and target domains are different MRI modalities.  Specific organs (prostate, pancreas, liver, right kidney, left kidney, spleen) are highlighted with distinct colors in each image to aid visual comparison and analysis. The purpose is to visually demonstrate the model's ability to generate realistic and accurate images of various organs across different MRI modalities.", "section": "5. Experiments"}]