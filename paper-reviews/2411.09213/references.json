{"references": [{"fullname_first_author": "Chen, H.", "paper_title": "Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions", "publication_date": "2024-02-18", "reason": "This paper provides a benchmark for evaluating LLMs on medical question answering, which is crucial for the paper's evaluation of RAG systems in the medical domain."}, {"fullname_first_author": "Chen, J.", "paper_title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "publication_date": "2024-02-18", "reason": "This paper proposes a benchmark for evaluating RAG systems, which is directly relevant to the paper's methodology for evaluating RAG systems in the medical domain."}, {"fullname_first_author": "Chen, Z.", "paper_title": "MEDITRON-70B: Scaling Medical Pretraining for Large Language Models", "publication_date": "2023-11-16", "reason": "This paper introduces a large language model specifically trained on medical data, which is one of the models evaluated in this paper's benchmark."}, {"fullname_first_author": "He, Z.", "paper_title": "MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation", "publication_date": "2023-12-06", "reason": "This paper introduces a comprehensive benchmark for evaluating LLMs on various medical tasks, which is a relevant comparison for the proposed MedRGB benchmark."}, {"fullname_first_author": "Xiong, G.", "paper_title": "Benchmarking Retrieval-Augmented Generation for Medicine", "publication_date": "2024-02-13", "reason": "This paper conducts a systematic evaluation of RAG systems for medicine, providing a foundation for the current paper's work and acting as a direct comparison."}]}