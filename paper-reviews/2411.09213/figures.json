[{"figure_path": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/intro_example.png", "caption": "Figure 1: Blue texts are useful information that should be extract to help determine the answer. Red texts are factual errors that potentially mislead the LLMs.", "description": "This figure illustrates a medical question-answering scenario using Retrieval-Augmented Generation (RAG).  The question is about how COVID-19 primarily spreads in indoor settings. Several documents are retrieved, some containing relevant and correct information (shown in blue), and others including factual errors (shown in red). The goal is to highlight how inaccuracies in retrieved documents can negatively impact the performance of large language models (LLMs) in providing correct answers, even when relevant information is available.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/DataGenFlow.png", "caption": "Figure 2: The overall construction process of MedRGB. The green OpenAI symbol implies that the block involves data generation using the GPT-4o model.", "description": "This figure illustrates the three-step process of creating the MedRGB benchmark.  First, retrieval topics are generated from the four medical QA datasets (BioASQ, PubMedQA, MedQA, MMLU) using the GPT-4 model.  These topics are then used to query two types of retrieval systems: offline (using MedCorp, a biomedical-domain corpus) and online (using Google Custom Search API). The retrieved documents are processed and summarized using LLMs to create signal documents. Finally, these documents are utilized in the creation of four test scenarios: Standard-RAG, Sufficiency, Integration, and Robustness to evaluate LLMs performance in practical RAG settings. The green OpenAI symbol in the figure indicates steps utilizing the GPT-4 model.", "section": "Medical Retrieval-Augmented Generation Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/noise_1.png", "caption": "Figure 3: Retrieval topic generation prompt (shorten version).", "description": "This prompt instructs a medical expert to generate ranked search topics for a given medical question.  The topics should be ranked by importance, relevant to the question and answer options, and efficiently searchable.  The goal is to create diverse and effective retrieval topics for a medical question answering system.", "section": "Medical Retrieval-Augmented Generation Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/noise_2.png", "caption": "Figure 4: Standard-RAG test inference prompt (shorten version).", "description": "This prompt instructs the large language model (LLM) to act as a medical expert answering a multiple-choice question using provided documents.  The LLM should analyze the provided documents and question, think step-by-step, and then determine the correct answer. This simulates a standard retrieval-augmented generation (RAG) scenario.", "section": "Medical Retrieval-Augmented Generation Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/inin_1.png", "caption": "Figure 5: Sufficiency test inference prompt (shorten version).", "description": "This prompt instructs the LLM to answer a multiple-choice question using provided documents, some of which may be irrelevant.  The LLM must first identify relevant documents, then use only those to determine the correct answer. If the LLM determines that none of the documents are relevant, it should indicate that there is insufficient information to answer the question.", "section": "Medical Retrieval-Augmented Generation Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/inin_2.png", "caption": "Figure 6: Integration test data generation prompt (shorten version).", "description": "This figure shows a shortened version of the prompt used to generate data for the integration test. The full prompt instructs a model to act as a medical expert generating sub-question-answer pairs for each document related to a main medical question.  The sub-questions should explore different aspects related to the main question, and be specific to the given document. The sub-answers are short strings extracted directly from the corresponding document.", "section": "Medical Retrieval-Augmented Generation Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/fact_1.png", "caption": "Figure 7: Integration test inference prompt (shorten version).", "description": "This prompt instructs LLMs to answer a main medical question and related sub-questions using provided documents. Some documents may be irrelevant.  The LLM must analyze all documents, answer each sub-question using the most relevant document (with a short, extracted answer), and then integrate this information to answer the main question.  It tests the model's ability to break down a complex question into smaller parts, extract relevant information from multiple sources, and integrate that information to arrive at a final answer. ", "section": "Integration Test"}, {"figure_path": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/fact_2.png", "caption": "Figure 8: Robustness test data generation prompt (shorten version).", "description": "This prompt instructs a medical expert to create a deliberately incorrect answer and a corresponding modified document for a given medical question. The new answer must factually contradict the original answer. The new document must support this false answer with fabricated information, while appearing coherent and persuasive.  The output should be formatted as a JSON object containing the question, the new (incorrect) answer, and the new document text.", "section": "Robustness Test"}, {"figure_path": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/suff_example.png", "caption": "Figure 9: Robustness test inference prompt (shorten version).", "description": "This prompt instructs the LLM to answer a multiple-choice medical question, considering that some documents may contain factual errors.  The LLM should first identify the relevant document for each sub-question and determine if it has factual errors. If an error exists, the LLM should answer using the correct information, rather than what's stated in the erroneous document.  Finally, the LLM should use this information to answer the main question. The response must be formatted as a JSON object with the answers to sub-questions and the main question, along with a step-by-step explanation.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/inin_example.png", "caption": "Figure 10: GPT-based Scoring Prompt (shorten version).", "description": "This prompt instructs the evaluator to assess the semantic similarity between a model's prediction and the ground truth answer for a medical question.  The evaluator should score 1 for a complete match, 0.5 for a partial match and relevant prediction, and 0 for a completely incorrect or irrelevant prediction.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/fact_example.png", "caption": "Figure 11: Sufficiency test main question accuracy.", "description": "The figure shows the accuracy of main question answering in the sufficiency test.  The accuracy is shown for multiple LLMs (GPT-3.5, GPT-40-mini, Llama-3-70b) across four different datasets (BioASQ, PubMedQA, MedQA, MMLU). The x-axis represents the percentage of signal (relevant) documents in the retrieved context, ranging from 0% (all noise) to 100% (all signal).  The y-axis represents the accuracy of the LLMs in correctly answering the main question.  The results illustrate how the accuracy changes as the proportion of relevant information in the retrieved context changes.", "section": "Sufficiency Evaluation"}]