[{"figure_path": "https://arxiv.org/html/2502.07701/x1.png", "caption": "Figure 1: The comparative experimental results on General VBench highlight the strong performance of Magic 1-For-1. Our model surpasses other open-source TI2V models, including CogVideoX-I2V-SAT, I2Vgen-XL, SEINE-512x320, VideoCrafter-I2V, and SVD-XT-1.0, in terms of both performance and efficiency.", "description": "This figure displays a bar chart and a line chart comparing the performance and efficiency of Magic 1-For-1 against several other open-source text-to-image-to-video (TI2V) models on the General VBench benchmark. The bar chart shows that Magic 1-For-1 outperforms the other models across multiple performance metrics (motion smoothness, dynamic degree, subject consistency, etc.). The line chart illustrates Magic 1-For-1's superior efficiency, requiring significantly fewer function evaluations to achieve comparable or better performance than its counterparts.  The results demonstrate Magic 1-For-1's strong performance and efficiency in generating videos.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07701/x2.png", "caption": "Figure 2: Magic 1-For-1 can generate video clips with optimized efficiency-quality trade-off.", "description": "Figure 2 showcases the video generation capabilities of Magic 1-For-1. It presents several video clips generated by the model, highlighting its ability to produce high-quality, one-minute-long videos within a minute.  The samples demonstrate various scenes and styles, illustrating the model's capacity for diverse video generation.  The figure visually represents the optimized balance achieved by Magic 1-For-1 between the efficiency of video generation and the quality of the produced videos.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.07701/x3.png", "caption": "Figure 3: Overall Architecture of Magic 1-For-1.", "description": "Figure 3 provides a detailed illustration of the Magic 1-For-1 model's architecture, focusing on the image prior injection and multi-modal guidance mechanisms.  The diagram showcases how text and image inputs are processed through separate but interacting pathways. The text input is processed using a CLIP-Large and LLAMA model, generating textual embeddings which inform the video generation. Simultaneously, a reference image undergoes processing via a CLIP-Large model, yielding visual embeddings that complement and guide the textual information. The text and visual embeddings are then concatenated and fed into a dual-stream DiT block, where video generation occurs.  This integrated approach ensures the generated video aligns with both the textual description and initial image. The dual-stream architecture uses the image as a prior, enhancing the generation efficiency and quality.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.07701/x4.png", "caption": "Figure 4: The overview of model acceleration techniques, including DMD2 and CFG distillation.", "description": "Figure 4 illustrates the model acceleration techniques used in Magic 1-For-1.  It highlights the two main methods: DMD2 (Distribution Matching Distillation 2), a step distillation method to speed up the generation process by reducing the number of diffusion steps, and CFG (Classifier-Free Guidance) distillation, which accelerates inference by distilling the guidance process into a more efficient form. The diagram shows how these techniques are integrated, starting with a pre-trained Magic 1-For-1 model which is then modified and used within the DMD2 framework and CFG distillation. The result is a faster and more efficient video generation pipeline.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.07701/x5.png", "caption": "Figure 5: Model performance progression during training. Interestingly, T2V Magic 1-For-1 exhibits considerably slower convergence in step distillation compared to TI2V Magic 1-For-1.", "description": "Figure 5 presents the training curves for different metrics (FVD, LPIPS, and FID) during the step distillation process for both text-to-video (T2V) and text-to-image-to-video (TI2V) models using Magic 1-For-1. The plots illustrate the convergence speed of the models.  The key takeaway is that the T2V model shows substantially slower convergence than its TI2V counterpart. This observation highlights the impact of task factorization (breaking down the text-to-video task into text-to-image and image-to-video stages) on the efficiency of the step distillation process, making TI2V significantly more efficient to train.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07701/x6.png", "caption": "Figure 6: Qualitative comparison of Magic 1-For-1 with recent state-of-the-art open source image-to-video generation models.", "description": "Figure 6 presents a qualitative comparison of video clips generated by Magic 1-For-1 and other state-of-the-art open-source image-to-video generation models.  The figure visually showcases the differences in video quality and style across different models, allowing for a direct comparison of their capabilities in generating coherent and visually appealing video content from a given image. This comparison highlights Magic 1-For-1's strengths in terms of visual clarity, motion smoothness and overall video quality.", "section": "4.4 Experiment Results"}]