{"references": [{"fullname_first_author": "Jian Hu", "paper_title": "Openrlhf: An easy-to-use, scalable and high-performance rlhf framework", "publication_date": "2024-05-11", "reason": "This paper introduces OpenRLHF, the open-source framework used in the main study, providing the foundation for the experimental results."}, {"fullname_first_author": "Ziniu Li", "paper_title": "Remax: A simple, effective, and efficient method for aligning large language models", "publication_date": "2023-10-10", "reason": "This paper presents ReMax, a key comparative method in the study, allowing for a direct performance comparison."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-03-02", "reason": "This paper is a foundational work in RLHF, establishing the base method to which other approaches are compared."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-12-01", "reason": "This paper introduces DPO, another key comparative method used for performance evaluation."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-01-01", "reason": "This paper introduces PPO, a widely used RLHF method and a major influence on the proposed method, enabling a detailed comparison."}]}