[{"content": "| Parameter | Value |\n|---|---| \n| KL Penalty Coefficient (\u03b2) | 0.01 (General), 0.001 (Mathematics) |\n| Maximum Samples | 25,000 |\n| Samples per Prompt | 4 |\n| Rollout Batch Size | 256 |\n| Training Batch Size | 128 |\n| Actor Learning Rate | 5 \u00d7 10\u207b\u2077 |\n| Critic Learning Rate | 9 \u00d7 10\u207b\u2076 |\n| Discount Factor (\u03b3) | 1.0 |\n| Clip \u03f5 | 0.2 |", "caption": "Table 1: Hyper-Parameter Configuration for REINFORCE++", "description": "This table details the hyperparameters used in the REINFORCE++ algorithm, including the KL penalty coefficient, the number of samples per prompt, batch sizes, learning rates, discount factor, and the clipping parameter epsilon.  These settings influence the training efficiency and model performance, showcasing the specific configurations used in the experiments.", "section": "4.2 Hyper-Parameter Configuration"}, {"content": "| Method | Training Time (hrs) |\n|---|---| \n| PPO | 60 |\n| REINFORCE++ | 42 |", "caption": "Table 2: Computational Efficiency Comparison", "description": "This table compares the computational efficiency of different reinforcement learning methods for aligning large language models.  It shows the training time in hours for Proximal Policy Optimization (PPO) and REINFORCE++, using the LLaMA 3 8B model on NVIDIA H100 GPUs.  The results highlight the reduction in computational cost achieved by REINFORCE++ compared to PPO.", "section": "5.2 Computational Efficiency"}]