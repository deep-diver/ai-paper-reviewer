[{"content": "| Dataset | #Screenshots | #Screenshots | #Screenshots | Open | #Elements |\n|---|---|---|---|---|---| \n|  | Web | Mobile | Desktop | Source |  |\n| SeeClick | 270K | 94K | - | \u2713 | 3.3M |\n| Ferret-UI | - | 124K | - | \u2717 | <1M |\n| GUICourse | 73K | 9K | - | \u2713 | 10.7M |\n| CogAgent | 400K | - | - | \u2717 | 70M |\n| **OS-Atlas** | 1.9M | 285K | 54K | \u2713 | 13.58M |", "caption": "Table 1: Statistics of the grounding data we collected compared to existing efforts. (For open-source datasets, we only count the amount of data made publicly available.)", "description": "This table provides a quantitative comparison of the GUI grounding datasets used in the paper against existing efforts.  It shows the number of screenshots and GUI elements available in each dataset, highlighting the scale of the OS-Atlas dataset relative to others. The table distinguishes between open-source and closed-source datasets, and for open-source datasets, only the publicly available data is included in the count.  This allows for a clear understanding of the relative size and scope of the GUI grounding data used in the OS-Atlas project.", "section": "3.2 Grounding Data Collection"}, {"content": "| Planner | Grounding Models | Mobile Text | Mobile Icon/Widget | Desktop Text | Desktop Icon/Widget | Web Text | Web Icon/Widget | Avg. | \n|---|---|---|---|---|---|---|---|---| \n| - | Fuyu | 41.00 | 1.30 | 33.00 | 3.60 | 33.90 | 4.40 | 19.50 | \n|  | CogAgent | 67.00 | 24.00 | 74.20 | 20.00 | 70.40 | 28.60 | 47.40 | \n|  | SeeClick | 78.00 | 52.00 | 72.20 | 30.00 | 55.70 | 32.50 | 53.40 | \n|  | InternVL-2-4B | 9.16 | 4.80 | 4.64 | 4.29 | 0.87 | 0.10 | 4.32 | \n|  | Qwen2-VL-7B | 61.34 | 39.29 | 52.01 | 44.98 | 33.04 | 21.84 | 42.89 | \n|  | UGround-7B | 82.80 | 60.30 | 82.50 | 63.60 | 80.40 | 70.40 | 73.30 | \n|  | OS-Atlas-Base-4B | 85.71 | 58.52 | 72.16 | 45.71 | 82.61 | 63.11 | 70.13 | \n|  | OS-Atlas-Base-7B | **93.04** | **72.93** | **91.75** | **62.86** | **90.87** | **74.27** | **82.47** | \n| GPT-4o | SeeClick | 83.52 | 59.39 | 82.47 | 35.00 | 66.96 | 35.44 | 62.89 | \n|  | UGround-7B | 93.40 | 76.90 | 92.80 | **67.90** | 88.70 | 68.90 | 81.40 | \n|  | OS-Atlas-Base-4B | **94.14** | 73.80 | 77.84 | 47.14 | 86.52 | 65.53 | 76.81 | \n|  | OS-Atlas-Base-7B | 93.77 | **79.91** | **90.21** | 66.43 | **92.61** | **79.13** | **85.14** | ", "caption": "Table 2: Grounding accuracy on ScreenSpot. The best results are in bold.", "description": "This table presents the performance of different Vision-Language Models (VLMs) on the ScreenSpot benchmark for GUI grounding tasks.  It shows the accuracy of each model in predicting the location of GUI elements based on textual descriptions.  The models are evaluated under two settings: one with a planner module and another without. Results are broken down by platform (web, desktop, mobile), element type (text, icon/widget), and model. OS-Atlas-Base consistently outperforms other models, demonstrating its effectiveness in GUI grounding.", "section": "4.2 RESULTS AND ANALYSIS"}, {"content": "| Models | OS | Calc | Impress | Writer | VLC | TB | Chrome | VSC | GIMP | WF | Avg. | \n|---|---|---|---|---|---|---|---|---|---|---|---| \n| GPT-4o + SoM | 20.83 | 0.00 | 6.77 | 4.35 | 6.53 | 0.00 | 4.35 | 4.35 | 0.00 | 3.60 | 4.59 | \n| GPT-4o | 8.33 | 0.00 | 6.77 | 4.35 | 16.10 | 0.00 | 4.35 | 4.35 | 3.85 | 5.58 | 5.03 | \n| + SeeClick | 16.67 | 0.00 | 12.76 | 4.35 | 23.52 | 6.67 | 10.86 | 8.70 | 11.54 | 7.92 | 9.21 | \n| + OS-Atlas-Base-4B | 20.83 | 2.23 | 14.89 | 8.70 | 23.52 | 13.33 | 15.22 | 13.04 | 15.38 | 7.92 | 11.65 | \n| + OS-Atlas-Base-7B | 25.00 | 4.26 | 17.02 | 8.70 | 29.41 | 26.67 | 19.57 | 17.39 | 19.23 | 8.91 | 14.63 | \n| Human | 75.00 | 61.70 | 80.85 | 73.91 | 70.59 | 46.67 | 78.26 | 73.91 | 73.08 | 73.27 | 72.36 |", "caption": "Table 3: \nSuccessful rate on OS World benchmark, divided by apps (domains). Workflow (WF) is a special domain that requires navigation across multiple apps.", "description": "This table presents the success rate of different models on the OS World benchmark, categorized by application domains.  The OS World benchmark involves tasks that require interactions with multiple applications.  The models are evaluated on their ability to successfully complete each task, and the success rates are broken down by application (e.g., Calculator, Impress, VLC, etc.) to show performance variations across different types of software.  The 'Workflow' (WF) category represents a unique set of tasks that demand navigation and interaction across various applications, indicating a higher level of complexity.", "section": "5 EXPERIMENTS: AGENT TASKS"}, {"content": "| Models | GUI-Act-Web Type | GUI-Act-Web Grounding | GUI-Act-Web SR | OmniAct-Web Type | OmniAct-Web Grounding | OmniAct-Web SR | OmniAct-Desktop Type | OmniAct-Desktop Grounding | OmniAct-Desktop SR |\n|---|---|---|---|---|---|---|---|---|---| \n| **Zero-shot OOD Setting** |  |  |  |  |  |  |  |  |  |\n| GPT-4o | 77.09 | 45.02 | 41.84 | 79.33 | 42.79 | 34.06 | 79.97 | 63.25 | 50.67 |\n| **OS-Atlas-4B** | 79.22 | 58.57 | 42.62 | 46.74 | 49.24 | 22.99 | 63.30 | 42.55 | 26.94 |\n| **OS-Atlas-7B** | 86.95 | 75.61 | 57.02 | 85.63 | 69.35 | 59.15 | 90.24 | 62.87 | 56.73 |\n| **Supervised Fine-tuning Setting** |  |  |  |  |  |  |  |  |  |\n| InternVL-2-4B | 81.42 | 47.03 | 36.17 | 47.51 | 51.34 | 24.39 | 67.00 | 44.47 | 29.80 |\n| Qwen2-VL-7B | 89.36 | 90.66 | 82.27 | 89.22 | 85.94 | 78.58 | 96.27 | 94.52 | 91.77 |\n| SeeClick | 88.79 | 78.59 | 72.34 | 86.98 | 75.48 | 68.59 | 96.79 | 70.22 | 72.69 |\n| **OS-Atlas-4B** | 89.36 | 89.16 | 81.06 | 88.56 | 82.00 | 73.91 | 96.51 | 85.53 | 84.78 |\n| **OS-Atlas-7B** | 89.08 | 91.60 | 82.70 | 97.15 | 95.41 | 93.56 | 97.15 | 95.85 | 94.05 |", "caption": "Table 4: Results on web and desktop tasks. InternVL-2/Qwen2-VL and OS-Atlas-4/7B differ in that the former utilizes the original checkpoints, while the latter is fine-tuned on OS-Atlas-Base.", "description": "Table 4 presents the results of experiments conducted on web and desktop tasks using different models.  A key distinction highlighted is the training approach: InternVL-2 and Qwen2-VL utilize their original checkpoints, while OS-Atlas-4/7B is fine-tuned using OS-Atlas-Base as a foundation. This comparison allows for an analysis of performance gains achieved through fine-tuning.", "section": "4 EXPERIMENTS: GROUNDING TASKS"}, {"content": "| Models | AndroidControl-Low |  |  | AndroidControl-High |  |  | GUI-Odyssey |  |  |\n|---|---|---|---|---|---|---|---|---|---| \n|  | Type | Grounding | SR | Type | Grounding | SR | Type | Grounding | SR |\n|---|---|---|---|---|---|---|---|---|---| \n| Zero-shot OOD Setting |  |  |  |  |  |  |  |  |  |\n| GPT-4o | **74.33** | 38.67 | 28.39 | **63.06** | 30.90 | 21.17 | 37.50 | 14.17 | 5.36 |\n| **OS-Atlas-4B** | 64.58 | 71.19 | 40.62 | 49.01 | 49.51 | 22.77 | 49.63 | 34.63 | 20.25 |\n| **OS-Atlas-7B** | 73.00 | **73.37** | **50.94** | 57.44 | **54.90** | **29.83** | **60.42** | **39.74** | **26.96** |\n| Supervised Fine-tuning Setting |  |  |  |  |  |  |  |  |  |\n| InternVL-2-4B | 90.94 | 84.05 | 80.10 | 84.09 | 72.73 | 66.72 | 82.13 | 55.53 | 51.45 |\n| Qwen2-VL-7B | 91.94 | 86.50 | 82.56 | 83.83 | 77.68 | 69.72 | 83.54 | 65.89 | 60.23 |\n| SeeClick | 93.00 | 73.42 | 75.00 | 82.94 | 62.87 | 59.11 | 70.99 | 52.44 | 53.92 |\n| **OS-Atlas-4B** | 91.92 | 83.76 | 80.64 | 84.69 | 73.79 | 67.54 | 83.47 | 61.37 | 56.39 |\n| **OS-Atlas-7B** | **93.61** | **87.97** | **85.22** | **85.22** | **78.48** | **71.17** | **84.47** | **67.80** | **61.98** |", "caption": "Table 5: Results on mobile tasks. InternVL-2/Qwen2-VL and OS-Atlas-4/7B differ in that the former utilizes the original checkpoints, while the latter is fine-tuned on OS-Atlas-Base. AndroidControl-Low refers to the scenario where both low-level and high-level instructions are provided as inputs, while AndroidControl-High indicates that only high-level instructions are given.", "description": "Table 5 presents the performance comparison of different models on mobile agent tasks.  It shows the accuracy of action type prediction (Type), coordinate prediction (Grounding), and step success rate (SR) for several benchmarks.  The key difference highlighted is between models using original checkpoints (InternVL-2/Qwen2-VL) and those fine-tuned on OS-Atlas-Base (OS-Atlas-4/7B). The table also distinguishes between two scenarios within the AndroidControl benchmark: one where both low-level and high-level instructions are provided, and another where only high-level instructions are given.", "section": "5 EXPERIMENTS: AGENT TASKS"}, {"content": "| Unified Action Space Prompt |\n|---|---| \n| You are a foundational action model capable of automating tasks across various digital environments, including desktop systems like Windows, macOS, and Linux, as well as mobile platforms such as Android and iOS. You also excel in web browser environments. You will interact with digital devices in a human-like manner: by reading screenshots, analyzing them, and taking appropriate actions. |\n| Your expertise covers two types of digital tasks:<br> - **Grounding**: Given a screenshot and a description, you assist users in locating elements mentioned. Sometimes, you must infer which elements best fit the description when they aren\u2019t explicitly stated.<br> - **Executable Language Grounding**: With a screenshot and task instruction, your goal is to determine the executable actions needed to complete the task. You should only respond with the Python code in the format as described below: <br>You are now operating in Executable Language Grounding mode. Your goal is to help users accomplish tasks by suggesting executable actions that best fit their needs. Your skill set includes both basic and custom actions: |\n| **1. Basic Actions**<br>Basic actions are standardized and available across all platforms. They provide essential functionality and are defined with a specific format, ensuring consistency and reliability. |\n| Basic Action 1: CLICK |\n| - purpose: Click at the specified position. |\n| - format: CLICK &lt;point&gt;[[x-axis, y-axis]]&lt;/point&gt; |\n| - example usage: CLICK &lt;point&gt;[[101, 872]]&lt;/point&gt; |\n| Basic Action 2: TYPE |\n| - purpose: Enter specified text at the designated location. |\n| - format: TYPE [input text] |\n| - example usage: TYPE [Shanghai shopping mall] |\n| Basic Action 3: SCROLL |\n| - purpose: SCROLL in the specified direction. |\n| - format: SCROLL [direction (UP/DOWN/LEFT/RIGHT)] |\n| - example usage: SCROLL [UP] |\n| **2.Custom Actions**<br>Custom actions are unique to each user\u2019s platform and environment. They allow for flexibility and adaptability, enabling the model to support new and unseen actions defined by users. These actions extend the functionality of the basic set, making the model more versatile and capable of handling specific tasks.<br>Your customized actions varied by datasets. |", "caption": "Table 6: The prompt for the action fine-tuning with a unified action space.", "description": "This table presents the prompt used during the action fine-tuning phase of the OS-ATLAS model training.  The prompt instructs the model to act as a foundational action model capable of handling tasks across various digital environments (desktop, mobile, web). It emphasizes the need for human-like interaction, using screenshots and descriptions to guide actions. The prompt specifies two main task types: grounding (locating elements) and executable language grounding (converting instructions to executable actions). It defines a unified action space that includes standardized basic actions (CLICK, TYPE, SCROLL) and custom actions (allowing for flexibility and adaptability across platforms).  The provided example usages clarify how each action should be formatted in the Python code output.  The custom actions are dataset-specific, providing flexibility for handling various tasks and environments.", "section": "3.3 UNIFIED ACTION SPACE"}, {"content": "| Training dataset | Type | Platform | Source | #Elements | #Screenshots |\n|---|---|---|---|---|---| \n| FineWeb-filtered | REG | Web | synthetic | 7,779,922 | 1,617,179 |\n| Windows-desktop | REG | Windows | synthetic | 1,079,707 | 51,726 |\n| Linux-desktop | REG | Linux | synthetic | 41,540 | 1,186 |\n| MacOS-desktop | REG | MacOS | synthetic | 13,326 | 1,339 |\n| Pixel6-mobile | REG | Mobile | synthetic | 104,598 | 21,745 |\n| SeeClick | REG | Web & Mobile | public | 3,303,479 | 364,760 |\n| AMEX | REG | Mobile | public | 1,097,691 | 99,939 |\n| UIbert | REG | Mobile | public | 16660 | 5682 |\n| Mind2Web-annotated | IG | Web | GPT-4o | 5,943 | 5,943 |\n| AITZ-annotated | IG | Mobile | GPT-4o | 10,463 | 10,463 |\n| AMEX-annotated | IG | Mobile | GPT-4o | 5,745 | 5,745 |\n| AndroidControl | IG | Mobile | public | 47,658 | 47,658 |\n| Wave-UI | IG | All platforms | public | 65,478 | 7,357 |\n| **Total** |  |  |  | **13,582,210** | **2,240,717** |", "caption": "Table 7: Grounding training datasets statistics overview.", "description": "This table presents a detailed overview of the datasets used for pre-training the grounding model.  It breaks down the data by type (REG: Referring Expression Grounding, IG: Instruction Grounding), platform (Web, Windows, MacOS, Mobile), source (whether it's synthetically generated or from a public dataset), the number of elements (GUI elements) in the dataset, and the number of screenshots.", "section": "3.2 Grounding Data Collection"}, {"content": "| Planner | Models | Mobile Text | Mobile Icon/Widget | Desktop Text | Desktop Icon/Widget | Web Text | Web Icon/Widget | Avg. |\n|---|---|---|---|---|---|---|---|---|\n| - | SeeClick | 78.39 | 50.66 | 70.10 | 29.29 | 55.22 | 32.52 | 55.09 |\n|  | OS-Atlas-Base-4B | 87.24 | 59.72 | 72.68 | 46.43 | 85.90 | 63.05 | 71.86 |\n|  | OS-Atlas-Base-7B | **95.17** | **75.83** | **90.72** | **63.57** | **90.60** | **77.34** | **84.12** |\n| GPT-4o | SeeClick | 85.17 | 58.77 | 79.90 | 37.14 | 72.65 | 30.05 | 63.60 |\n|  | OS-Atlas-Base-4B | 95.52 | 75.83 | 79.38 | 49.29 | 90.17 | 66.50 | 79.09 |\n|  | OS-Atlas-Base-7B | **96.21** | **83.41** | **89.69** | **69.29** | **94.02** | **79.80** | **87.11** |", "caption": "Table 8: Grounding accuracy on ScreenSpot-v2. The best results are in bold.", "description": "This table presents the results of a GUI grounding accuracy evaluation on the ScreenSpot-V2 benchmark dataset.  It compares the performance of several models, including OS-Atlas-Base, across different settings (with and without a planner).  The results show the accuracy of each model in predicting the location of GUI elements based on textual instructions.  The best-performing model in each category is highlighted in bold, indicating its superior accuracy in GUI grounding tasks. This benchmark assesses single-step GUI grounding capability across mobile, desktop, and web platforms. The results are further broken down by the type of GUI element (Text, Icon/Widget) and the platform.", "section": "4.2 RESULTS AND ANALYSIS"}, {"content": "| Benchmarks | Platforms | #Test Samples | History? | # Unified Actions |\n|---|---|---|---|---|\n| GUI-Act-Web | Web | 1,410 |  | 3+2 |\n| Omniact | Web | 1,427 |  | 3+11 |\n|  | Desktop | 594 |  | 3+11 |\n| AndroidControl-Low | Mobile | 7,708 | \u2713 | 3+5 |\n| AndroidControl-High | Mobile | 7,708 | \u2713 | 3+5 |\n| GUI-Odyssey-Random | Mobile | 29,414 |  | 3+6 |\n| GUI-Odyssey-Task | Mobile | 17,920 |  | 3+6 |\n| GUI-Odyssey-Device | Mobile | 18,969 |  | 3+6 |\n| GUI-Odyssey-App | Mobile | 17,455 |  | 3+6 |", "caption": "Table 9: \nDetails of the agentic benchmarks. History represents whether the history information of the previous actions is provided in the input. #Unified Actions denotes the number of actions (basic actions + custom actions) for each task.", "description": "This table presents details of the benchmarks used to evaluate the performance of agent tasks.  For each benchmark, it indicates the platform (Web, Desktop, or Mobile), the number of test samples, whether the history of previous actions is included as input, and the number of unified actions (a combination of basic and custom actions) available for each task.", "section": "5 EXPERIMENTS: AGENT TASKS"}]