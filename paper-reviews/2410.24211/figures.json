[{"figure_path": "https://arxiv.org/html/2410.24211/x1.png", "caption": "Figure 1: \nDELTA is a dense 3D tracking approach that (a) tracks every pixel from a monocular video, (b) provides consistent trajectories in 3D space, and (c) achieves state-of-the-art accuracy on 3D tracking benchmarks while being significantly faster than previous approaches in the dense setting.", "description": "Figure 1 showcases the capabilities of DELTA, a novel dense 3D tracking method.  Panel (a) demonstrates DELTA's ability to track every pixel within a monocular video sequence. Panel (b) highlights that these pixel tracks are consistent and accurately represented in 3D space. Finally, panel (c) presents a performance comparison graph, illustrating that DELTA achieves state-of-the-art (SoTA) accuracy on 3D tracking benchmarks while exhibiting significantly faster processing speeds than existing dense 3D tracking approaches.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2410.24211/x2.png", "caption": "Figure 2: Overview of DELTA.\nDELTA takes RGB-D videos as input and achieves efficient dense 3D tracking using a coarse-to-fine strategy, beginning with coarse tracking through a spatio-temporal attention mechanism at reduced resolution (Sec.\u00a03.1, 3.2), followed by an attention-based upsampler for high-resolution predictions (Sec.\u00a03.3).", "description": "DELTA, a novel method for efficient dense 3D tracking, is illustrated.  It uses a coarse-to-fine approach: starting with reduced-resolution tracking using a spatio-temporal attention mechanism (Sections 3.1 and 3.2), and then upsampling to high-resolution predictions via an attention-based upsampler (Section 3.3). The input is RGB-D video, and the output is efficient dense 3D tracking.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2410.24211/x3.png", "caption": "Figure 3: \nSpatial attention architectures. Top: Illustration of different spatial attention architectures. Compared to prior methods, our proposed architecture \u2462 incorporates both global and local spatial attention and can be efficiently learned using a patch-by-patch strategy. Bottom: Long-term optical flows predicted with different spatial attention designs. We find that both global and local attention are crucial for improving tracking accuracy, as highlighted by the red circles. Additionally, our computationally efficient global attention design using anchor tracks (i.e., \u2462 W/o Local Attn) achieves similar accuracy to the more computationally-intensive Cotracker version \u2461.", "description": "Figure 3 illustrates different spatial attention mechanisms used in dense tracking. The top part compares various architectures, highlighting how the proposed method (\u2462) uniquely combines global and local spatial attention for efficient learning via a patch-by-patch approach.  This contrasts with previous methods, which are shown to be less efficient.  The bottom of the figure shows the long-term optical flow predictions obtained using each architecture, demonstrating the improved accuracy resulting from the inclusion of both global and local attention, especially noticeable in the red-circled regions.  It also shows that the computationally efficient global attention using anchor tracks performs similarly to the computationally more expensive Cotracker architecture.", "section": "3.2 Joint Global and Local Spatial Attention for Efficient Dense Tracking"}, {"figure_path": "https://arxiv.org/html/2410.24211/x4.png", "caption": "Figure 4: Attention-based upsample module. Left: We apply multiple blocks of local cross-attention to learn the upsampling weights for each pixel in the fine resolution. Right: The red circles highlight regions in the long-term flow maps where our attention-based upsampler produces more accurate predictions compared to RAFT\u2019s convolution-based upsampler.", "description": "This figure illustrates the attention-based upsampling module used in the DELTA architecture.  The left panel shows the module's architecture, highlighting how multiple blocks of local cross-attention are used to learn the upsampling weights for each pixel in the high-resolution output. These weights refine the predictions from a lower-resolution stage, making it computationally efficient. The right panel provides a qualitative comparison, using long-term optical flow maps.  Red circles show areas where the attention-based upsampler outperforms RAFT's standard convolution-based approach, indicating improved accuracy in challenging regions.", "section": "3.3 High-resolution track upsampler"}, {"figure_path": "https://arxiv.org/html/2410.24211/x5.png", "caption": "Table 3: Dense 3D tracking results on the Kubric3D dataset.", "description": "This table presents a quantitative comparison of different methods for dense 3D tracking on the Kubric3D benchmark dataset.  It shows the performance of various methods across three key metrics: Average Jaccard index (AJ), Average Point-to-Point Distance in 3D space (APD3D), and Overall Accuracy (OA).  The table also includes the time taken by each method, illustrating the computational efficiency of each approach.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2410.24211/x6.png", "caption": "Table 4: 3D tracking results on the LSFOdyssey benchmark. \u2021 denotes models trained with LSFOdyssey training set.", "description": "This table presents a comparison of different methods' performance on the LSFOdyssey benchmark for 3D tracking.  The metrics used likely include Average Jaccard (AJ), Average 3D Positional Accuracy (APD3D), and Occlusion Accuracy (OA). The '\u2021' symbol indicates models that were specifically trained using the LSFOdyssey dataset, allowing for a fairer comparison against those trained on other datasets.  The table helps to highlight the relative effectiveness of different 3D tracking approaches in a real-world video scenario.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2410.24211/x7.png", "caption": "Figure 5: \nQualitative results of dense 3D tracking on in-the-wild videos between CoTracker +++ UniDepth, SceneTracker, SpatialTracker and our method. We densely track every pixel from the first frame of the video in 3D space, the moving objects are highlighted as rainbow color. Our method accurately tracks the motion of foreground objects while maintaining stable backgrounds.", "description": "Figure 5 presents a qualitative comparison of dense 3D tracking performance on real-world videos. Four different methods are compared: CoTracker++ with UniDepth, SceneTracker, SpatialTracker, and the proposed DELTA method.  Each method's tracking results are visualized, showing 3D trajectories of every pixel over time. Moving objects are color-coded with rainbow colors to highlight their movement. The figure demonstrates the superior accuracy and stability of DELTA in tracking moving objects in complex scenes while maintaining consistent background estimates.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2410.24211/x8.png", "caption": "Figure 6: Comparison of long-range optical flow predictions: We predict optical flows from the first frame to subsequent frames of the video. DOT (Le\u00a0Moing et\u00a0al., 2024), which lacks strong temporal correlation, suffers from a noticeable \u201dflickering\u201d effect (green circle), particularly at the boundaries between foreground and background objects. In contrast, our method ensures a smooth and consistent transition over time, effectively reducing artifacts at object boundaries.", "description": "Figure 6 presents a comparison of long-range optical flow predictions generated by the proposed method and DOT (Le Moing et al., 2024).  The figure displays optical flow predictions from the first frame to subsequent frames for both methods.  The comparison highlights the significant improvement in temporal consistency achieved by the proposed method.  DOT, lacking strong temporal correlation, exhibits a noticeable \"flickering\" effect, especially where foreground and background objects meet.  In contrast, the proposed method's predictions show a much smoother and more consistent transition over time, effectively minimizing artifacts around object boundaries.", "section": "4 Experiments"}]