[{"figure_path": "https://arxiv.org/html/2503.12821/x1.png", "caption": "((a))", "description": "This figure shows the performance comparison between the original LLaVA 1.5 model and the model enhanced with the proposed Adaptive Data Refinement (ADR) framework.  The (a) subplot displays a radar chart comparing the performance across eleven benchmarks. Each axis represents a benchmark, and the distance from the center represents the performance score, showing that the ADR-enhanced model consistently outperforms LLaVA 1.5 across all benchmarks. The (b) subplot focuses specifically on the accuracy of the tail 30% of concepts within the VQAV2 benchmark, further illustrating the improved performance of the ADR-enhanced model in handling the long-tail distribution problem.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.12821/x2.png", "caption": "((b))", "description": "This figure shows the accuracy of the model on the tail 30% of the data in VQAV2.  The baseline LLaVA 1.5 model and the improved model using the proposed ADR method are compared. The chart displays a radar plot showing performance across multiple aspects of the task. The ADR method demonstrates significantly improved accuracy for the tail 30% concepts. ", "section": "3.4. Significance of Long-Tail Problem Mitigation"}, {"figure_path": "https://arxiv.org/html/2503.12821/x3.png", "caption": "Figure 1: \nPerformance before and after addressing the LT problem. Our method surpasses the baseline over all benchmarks and also effectively improves the performance of tail 30% concepts.", "description": "This figure displays a comparison of performance on various benchmarks before and after addressing the long-tail (LT) problem in Large Vision-Language Models (LVLMs). The left subplot (a) shows the performance improvement of the proposed method (Ours) compared to the LLaVA 1.5 baseline across different benchmarks. The right subplot (b) focuses specifically on the accuracy achieved on the tail 30% of concepts within the VQAV2 benchmark, highlighting that the proposed method significantly enhances the performance on the tail concepts.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.12821/x4.png", "caption": "Figure 2: \nThe overview of our Adaptive Data Refinement Framework (ADR).\n(a) In the Analyzing Stage, we first extract tokens, objects, co-occurrences, and interrogations from the training instances, then construct corresponding distribution using a reverse-indexed mapping.\n(b) In the Data Rebalancing stage, we analyze the optimizing direction and adaptively rebalance the redundant data based on the entity distribution identified in the Analyzing stage.\n(c) Finally, in the Data Synthesis stage, we utilize DDPM and the latent representations of scarce image instances to synthesize the underrepresented data.", "description": "Figure 2 illustrates the Adaptive Data Refinement Framework (ADR), a three-stage process to address data imbalance in large vision-language models.  Stage (a) Analyzing, extracts key features (tokens, objects, co-occurrences, and interrogations) from the training data and generates their distributions. Stage (b) Data Rebalancing, uses this information to identify and remove redundant data while retaining the data with under-represented entities.  Finally, stage (c) Data Synthesis, leverages diffusion probabilistic models and limited data to generate synthetic data for underrepresented categories.", "section": "4. Approach"}, {"figure_path": "https://arxiv.org/html/2503.12821/x5.png", "caption": "((a))", "description": "The figure shows the performance comparison between LLaVA 1.5 and the proposed method on eleven benchmarks.  Subfigure (a) displays the average performance improvement across all benchmarks, showing that the proposed method outperforms LLaVA 1.5. Subfigure (b) focuses on the accuracy achieved on the tail 30% of concepts within the VQAV2 benchmark, demonstrating a significant improvement by the proposed method compared to LLaVA 1.5. This visualization highlights the effectiveness of the proposed method in addressing the long-tail problem in large vision-language models.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.12821/x6.png", "caption": "((b))", "description": "This figure shows the accuracy of the model on the tail 30% of concepts in the VQAV2 benchmark.  It compares the performance of the original LLaVA 1.5 model to the model enhanced with the proposed ADR (Adaptive Data Refinement) framework. The chart visually demonstrates that the ADR-improved model significantly outperforms the baseline model on the more challenging, less frequent concepts (tail data).", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.12821/x7.png", "caption": "((c))", "description": "This figure shows the Data Synthesis stage of the Adaptive Data Refinement Framework (ADR).  It illustrates the process of generating underrepresented tail data using Denoising Diffusion Probabilistic Models (DDPMs). Specifically, it demonstrates how latent representations of scarce images are leveraged to synthesize new images and captions representing underrepresented concepts, thereby mitigating the long-tail problem in the training data.", "section": "4.2 Data Synthesis Stage"}, {"figure_path": "https://arxiv.org/html/2503.12821/x8.png", "caption": "((d))", "description": "This figure shows the object-level word distribution in LLaVA's instruction tuning dataset.  It illustrates the long-tail distribution problem in the training data, where a small number of objects are overrepresented (head), while a large number of objects are underrepresented (tail).  This imbalance can negatively affect the performance of the model on the underrepresented tail concepts.", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2503.12821/x9.png", "caption": "Figure 3: \nLong-tail distribution in instruction-tuning and benchmark datasets:\n(a) Token-level distribution in MME [16].\n(b) Token-level distribution in InstructMix665K [28].\n(c) Object-level distribution in MME [16].\n(d) Object-level distribution in InstructMix665K [28].", "description": "This figure visualizes the long-tail distributions in instruction-tuning and benchmark datasets, specifically focusing on token-level and object-level distributions.  Subfigures (a) and (c) show the distributions in the MME dataset, while subfigures (b) and (d) show those in the InstructMix665K dataset.  The long tail is a common problem in datasets where some categories have far more examples than others.  These plots help illustrate the severity of this class imbalance, indicating the challenge for models trained on such data.", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2503.12821/x10.png", "caption": "((a))", "description": "This figure shows the performance comparison between LLaVA 1.5 and the proposed method (Ours) on various vision-language benchmarks. Subfigure (a) presents the performance comparison across different benchmarks, demonstrating that the proposed method outperforms LLaVA 1.5 consistently and significantly improves the average performance. Subfigure (b) focuses on the accuracy on the tail 30% of concepts in the VQAV2 benchmark, illustrating the effectiveness of the proposed method in mitigating the long-tail problem and improving the performance on less frequent concepts.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.12821/x11.png", "caption": "((b))", "description": "This figure shows the accuracy of the model on the tail 30% of the data in the VQAV2 benchmark.  It compares the performance of the original LLaVA 1.5 model against the improved model after applying the Adaptive Data Refinement (ADR) framework. The chart visually demonstrates the significant improvement achieved by ADR in handling the long-tail problem. The increased accuracy on the tail data indicates that the ADR method effectively addresses the imbalance in the training data and enhances the model's overall performance.", "section": "3.4 Significance of Long-Tail Problem Mitigation"}, {"figure_path": "https://arxiv.org/html/2503.12821/x12.png", "caption": "((c))", "description": "This figure shows the Data Synthesis stage of the Adaptive Data Refinement Framework (ADR).  It illustrates the process of using Denoising Diffusion Probabilistic Models (DDPMs) and latent representations of scarce images to supplement under-represented portions of the training data.  The diagram details how the system leverages vision encoders, captioners, and language models (LMs) to synthesize both visual and textual information, generating new image-caption pairs for underrepresented tail concepts.", "section": "4.2 Data Synthesis Stage"}, {"figure_path": "https://arxiv.org/html/2503.12821/x13.png", "caption": "Figure 4: \nError accumulation curve of POPE and MME based on the training data distribution. It reveals that tail entities contribute to the majority of failure cases.\n(a) Token-level word distribution in MME [16] and POPE [25].\n(b) Object-level word distribution in MME and POPE.\n(c) Co-occurrence-level word distribution in MME and POPE.", "description": "Figure 4 displays error accumulation curves for the POPE and MME datasets, illustrating the disproportionate contribution of tail entities to prediction errors. The curves show that while a small percentage of frequent entities account for many correct predictions, a large portion of less frequent tail entities are associated with the majority of failures.  Panel (a) shows the analysis at the token level, panel (b) at the object level, and panel (c) at the co-occurrence level, offering a multi-faceted understanding of how long-tail effects manifest across different granularities of analysis in the datasets.", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2503.12821/x14.png", "caption": "Figure 5: \nAblation study on data rebalancing combinations. T, O, C, and W refer to Token, Object, Co-occurrence, and Interrogation respectively. The values displayed in the graph represent average scores across a variety of comprehensive benchmarks. The blue dashed line indicates the baseline performance of LLaVA 1.5.", "description": "This ablation study investigates the impact of different combinations of data rebalancing methods on the overall performance of a large vision-language model (LLaVA 1.5).  The study uses four perspectives to rebalance the data: Token, Object, Co-occurrence, and Interrogation (represented by T, O, C, and W respectively). Each bar in the graph shows the average performance across multiple benchmarks when using various combinations of these rebalancing techniques. The blue dashed line serves as the baseline performance of LLaVA 1.5 without any data rebalancing. This allows for a direct comparison and visualization of the effectiveness of different rebalancing strategies.", "section": "6. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.12821/x15.png", "caption": "Figure 6: \nAblation study on data rebalancing synthesis methods. The meaning of abbrs occurred here is explained in Section 6.2. The values displayed in the graph represent average scores across a variety of comprehensive benchmarks. The blue dashed line indicates the baseline performance of LLaVA 1.5.", "description": "This ablation study investigates the impact of different data rebalancing and synthesis methods on the overall performance of a large vision-language model (LLaVA 1.5).  The x-axis represents various combinations of techniques (explained in Section 6.2), and the y-axis shows the average performance across multiple comprehensive benchmarks. The blue dashed line indicates the baseline performance of the LLaVA 1.5 model without any data augmentation or rebalancing. The results illustrate which combination of data rebalancing and synthesis methods yields the best performance improvement on average.", "section": "6. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.12821/x16.png", "caption": "Figure 7: \nQualitative comparison between the baseline model (LLaVA 1.5) and our proposed method (LLaVA w/ ADR) on a tail example. LLaVA w/ ADR can handle tail questions while LLaVA 1.5 fails to answer. While LLaVA 1.5 fails to answer tail questions, LLaVA w/ ADR successfully addresses them.", "description": "This figure presents a qualitative comparison of LLaVA 1.5 (baseline) and LLaVA with the proposed ADR method in answering tail questions (questions related to less frequent concepts).  A tail example is provided to highlight the difference in performance.  The example demonstrates that LLaVA 1.5 struggles with the tail question and fails to provide a correct answer, while LLaVA with ADR successfully answers the same question.  This visually demonstrates the improvement in handling under-represented data achieved through the use of the ADR framework.", "section": "7. Qualitive Results"}, {"figure_path": "https://arxiv.org/html/2503.12821/x17.png", "caption": "Figure 8: \nComparison between the original instruction-tuning (IT) data and our synthesized IT data. Tail concepts in the original data are highlighted using red boxes and fonts, whereas synthesized tail concepts are marked with green boxes and yellow fonts.", "description": "Figure 8 visualizes a comparison between the original instruction-tuning dataset and the data synthesized by the proposed method.  The image showcases examples from both datasets, highlighting tail concepts (under-represented concepts in the original dataset). Tail concepts in the original data are marked with red boxes and red font, while the synthesized tail concepts are marked with green boxes and yellow font. This allows for a direct visual comparison of the original data's limitations and how the proposed method addresses these limitations by supplementing the dataset with synthesized examples of tail concepts.", "section": "7. Qualitive Results"}, {"figure_path": "https://arxiv.org/html/2503.12821/x18.png", "caption": "((a)) A bear resting peacefully beside a rock wall.", "description": "A photograph depicting a bear calmly situated next to a rock wall. The setting appears natural and serene, suggesting a wildlife environment. The bear's posture is relaxed, indicating a peaceful moment.", "section": "7. Qualitive Results"}, {"figure_path": "https://arxiv.org/html/2503.12821/x19.png", "caption": "((b)) A cell phone displaying a cartoon princess on its screen.", "description": "A cell phone screen displays a cartoon princess.  The image is used as an example in the paper to illustrate a challenge in a Vision-Language Model (VLM). The model is shown to have difficulty in identifying details in images containing underrepresented concepts or objects (referred to as 'tail concepts' in the paper). The example highlights the need for the Adaptive Data Refinement Framework (ADR) that the paper proposes.", "section": "7. Qualitive Results"}, {"figure_path": "https://arxiv.org/html/2503.12821/x20.png", "caption": "((c)) A dump truck.", "description": "The image shows a dump truck, a large motor vehicle used for hauling materials.  It is typically characterized by its open-box bed at the rear, used for carrying loose materials like dirt, gravel, sand, or demolition debris.  The truck in the image likely represents a common type found in construction, mining, or waste management.", "section": "7. Qualitive Results"}, {"figure_path": "https://arxiv.org/html/2503.12821/x21.png", "caption": "Figure 9: \nQualitative comparison between the baseline model (LLaVA 1.5) and our proposed method (LLaVA w/ ADR) on a few tail examples. While LLaVA 1.5 fails to answer tail questions, LLaVA w/ ADR successfully addresses them.", "description": "This figure presents a qualitative comparison of LLaVA 1.5 (baseline) and LLaVA with ADR (the proposed method) on several examples of tail questions (questions related to underrepresented concepts in the training data).  The images show that while the baseline model, LLaVA 1.5, fails to answer these challenging, less frequent questions correctly, the model incorporating the ADR framework successfully answers the tail questions.  This demonstrates the effectiveness of ADR in improving the model's ability to handle underrepresented concepts and generalize better to less frequent scenarios.", "section": "7. Qualitive Results"}, {"figure_path": "https://arxiv.org/html/2503.12821/x22.png", "caption": "((a)) A train traveling along a railway near a church.", "description": "The image depicts a train traveling on a railway line adjacent to a church. The scene likely represents a rural or suburban area. The railway track is visible, and the train appears to be in motion, possibly suggesting a journey in progress. The church is a prominent feature of the background, and its architectural style and surroundings could offer insights into the local context or history.", "section": "7. Qualitive Results"}, {"figure_path": "https://arxiv.org/html/2503.12821/x23.png", "caption": "((b)) A bench by the lake, with a forest on the opposite shore.", "description": "The image depicts a wooden bench situated next to a serene lake. Lush greenery, suggestive of a forest, is visible on the opposite bank of the lake. The scene appears tranquil and peaceful, showcasing a natural setting with a simple structure.", "section": "7. Qualitive Results"}, {"figure_path": "https://arxiv.org/html/2503.12821/x24.png", "caption": "((c)) A furniture arrangement complemented by a variety of planters.", "description": "The image shows a living room scene with a variety of furniture pieces, including a coffee table and a sofa. Several potted plants are placed around the room, acting as decorative elements. The overall impression is one of a homey and well-decorated space.", "section": "7. Qualitive Results"}, {"figure_path": "https://arxiv.org/html/2503.12821/x25.png", "caption": "Figure 10: \nComparison between the original instruction-tuning (IT) data and our synthesized IT data. Tail concepts in the original data are highlighted using red boxes and fonts, whereas synthesized tail concepts are marked with green boxes and yellow fonts.", "description": "Figure 10 displays a comparison of original and synthesized instruction-tuning data.  The figure highlights the differences in the representation of 'tail concepts' \u2013 concepts that are under-represented in the original dataset.  The original data's tail concepts are shown in red boxes and fonts, while those generated through the data synthesis process in the proposed ADR framework are highlighted in green boxes and yellow fonts. This visual comparison demonstrates the effectiveness of the data synthesis stage in enriching the dataset with more examples of under-represented concepts.", "section": "7. Qualitive Results"}, {"figure_path": "https://arxiv.org/html/2503.12821/x26.png", "caption": "Figure 11: Top 20 most frequent entities in the instruction-tuning dataset of LLaVA 1.5.", "description": "This bar chart visualizes the top 20 most frequent entities across four categories (tokens, objects, co-occurrences, and interrogations) within the instruction-tuning dataset used to train the LLaVA 1.5 large vision-language model.  For each entity, the bar's height represents its frequency of occurrence in the dataset.  The chart offers insights into the prevalent themes and concepts within the training data, highlighting which aspects are most heavily represented and thus may influence the model's performance and potential biases.", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2503.12821/x27.png", "caption": "((a)) MME: Tok", "description": "This figure shows the token-level word distribution in the MME dataset.  The graph displays the frequency distribution of different words, highlighting the long-tail phenomenon where a small number of frequent words make up a large proportion of the data, while a vast number of infrequent words constitute the remaining portion.", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2503.12821/x28.png", "caption": "((b)) LCS558K: Tok", "description": "This figure shows the token-level word distribution in the LCS558K dataset.  The x-axis represents the word frequency, and the y-axis represents the relative frequency of words. The graph visualizes the long-tail distribution of words in the dataset, indicating a significant imbalance in the distribution of word frequencies.  Many words appear infrequently, while a few words appear very frequently. This is typical of natural language data and highlights the long-tail problem which the paper attempts to address.", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2503.12821/x29.png", "caption": "((c)) InstructMix665K: Tok", "description": "This figure shows the token-level word distribution in the InstructMix665K dataset.  It's a histogram illustrating the frequency of different words.  The x-axis represents the words and the y-axis represents their frequency or relative count in the dataset. This visualization helps to understand the distribution of words, showing which words are frequent and which words are rare.", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2503.12821/x30.png", "caption": "((d)) MME: Obj", "description": "This figure shows the object-level word distribution in the MME benchmark dataset.  It illustrates the frequency of different object words within the dataset, highlighting the imbalance in representation. The x-axis represents the object words, and the y-axis represents their frequency or proportion in the dataset.  The graph visually demonstrates the long-tail distribution characteristic of the MME dataset where some object categories are highly frequent, while others are very infrequent.", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2503.12821/x31.png", "caption": "((e)) LCS558K: Obj", "description": "This figure shows the object-level word distribution in the LCS558K dataset.  The x-axis represents the word, and the y-axis represents the frequency. The plot displays the distribution of object words within the training data of the LCS558K dataset. It illustrates the imbalance in the data, showing the presence of a long tail, indicating that many objects have low frequency counts, while a few objects have very high frequency counts. This plot helps to visualize the long-tail problem in the dataset and how this data imbalance may affect the performance of Large Vision-Language Models (LVLMs).", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2503.12821/x32.png", "caption": "((f)) InstructMix665K: Obj", "description": "This figure shows the object-level word distribution in the InstructMix665K dataset.  The x-axis represents the words, and the y-axis represents their relative frequency.  The distribution illustrates the long-tail phenomenon, where a small number of objects account for a large proportion of the data, while a vast number of objects are sparsely represented. This visualization highlights the imbalance in the dataset, demonstrating the challenge posed by long-tail data for training effective vision-language models.", "section": "3. Analysis"}]