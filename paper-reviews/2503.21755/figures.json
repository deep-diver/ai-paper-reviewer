[{"figure_path": "https://arxiv.org/html/2503.21755/x2.png", "caption": "Figure 1: \nOverview of VBench-2.0.\n(a) Scope of VBench-2.0. Video generative models have progressed from achieving superficial faithfulness in fundamental technical aspects such as pixel fidelity and basic prompt adherence, to addressing more complex challenges associated with intrinsic faithfulness, including commonsense reasoning, physics-based realism, human motion, and creative composition. While VBench primarily assessed early-stage technical quality, VBench-2.0 expands the benchmarking framework to evaluate these advanced capabilities, ensuring a more comprehensive assessment of next-generation models.\n(b) Evaluation Dimension of VBench-2.0.\nVBench-2.0 introduces a structured evaluation suite comprising five broad categories and 18 fine-grained capability dimensions.", "description": "Figure 1 illustrates the evolution of video generation benchmarks, focusing on VBench-2.0. Panel (a) shows the advancement of video generative models from superficial faithfulness (achieving visual realism without adhering to real-world physics) to intrinsic faithfulness (demonstrating both visual and conceptual realism, encompassing commonsense reasoning and adherence to physical laws). It highlights how VBench-2.0 expands upon its predecessor (VBench) by including more complex criteria. Panel (b) presents the structured evaluation suite of VBench-2.0, which contains 5 high-level categories and 18 fine-grained capability dimensions for a more comprehensive assessment of video generation models.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.21755/x3.png", "caption": "Figure 2: \nVBench-2.0 Evaluation Results of SOTA Models. The figure presents the evaluation results of four recent state-of-the-art video generation models across 18 VBench-2.0 dimensions. The results are normalized per dimension for a clearer comparison. For detailed numerical results, refer to Table\u00a02.", "description": "This figure displays a radar chart visualizing the performance of four state-of-the-art (SOTA) video generation models across 18 distinct dimensions defined by the VBench-2.0 benchmark. Each dimension represents a specific aspect of video generation quality, such as human fidelity, controllability, creativity, physics, and common sense.  The radar chart allows for a quick visual comparison of the models' strengths and weaknesses in each of these areas.  The scores are normalized, ensuring a fair comparison despite variations in absolute values between the different dimensions.  More detailed numerical results are available in Table 2 of the paper.", "section": "3. VBench-2.0 Suite for Intrinsic Faithfulness"}, {"figure_path": "https://arxiv.org/html/2503.21755/extracted/6315755/figures/fig_paper_interface.jpg", "caption": "Figure 3: Overview of Prompt Suite Statistics. Left: distribution of words in the prompt suites. Right: number of prompts per evaluation dimension.", "description": "This figure presents a statistical overview of the prompts used in the VBench-2.0 benchmark suite.  The left panel displays a histogram showing the distribution of word counts across all prompts. This reveals the overall length and complexity of the prompts used to evaluate different aspects of video generation. The right panel shows a bar chart illustrating the number of prompts designed for each of the 18 fine-grained evaluation dimensions within the VBench-2.0 framework. This provides insights into the relative emphasis placed on each dimension during the evaluation process.", "section": "3. VBench-2.0 Suite for Intrinsic Faithfulness"}, {"figure_path": "https://arxiv.org/html/2503.21755/x4.png", "caption": "Figure 4: Interface for Human Preference Annotation. Top: Question descriptions. Right: Choices available to annotators. Bottom left: Controls for stopping and playback.", "description": "This figure shows the user interface for human preference annotation in the VBench-2.0 benchmark. The top section displays the question description, the right section lists the choices available for the annotators, and the bottom-left section shows controls for starting, stopping, and playing back the videos.", "section": "4. Human Alignment of VBench-2.0"}, {"figure_path": "https://arxiv.org/html/2503.21755/x5.png", "caption": "Figure 5: Human Alignment of VBench-2.0 Evaluation. Each plot represents the alignment verification for a specific VBench-2.0 dimension. In each plot, a dot corresponds to the human preference win ratio (horizontal axis) and the VBench-2.0 evaluation win ratio (vertical axis) for a given video generation model. A linear fit is applied to visualize the correlation, and Spearman\u2019s correlation coefficient (\u03c1\ud835\udf0c\\rhoitalic_\u03c1) is computed for each dimension. Experiments show that VBench-2.0 evaluations closely align with human judgement in all dimensions.", "description": "This figure displays the correlation between human judgment and VBench-2.0 evaluation results across 18 different video generation capabilities. Each plot shows the win ratios from human preference tests (horizontal axis) and VBench-2.0 automated evaluation (vertical axis) for a given video generation model.  A linear fit and Spearman's correlation coefficient (\u03c1) are calculated for each dimension to quantitatively assess the agreement between human and automated evaluations.  The results demonstrate a high correlation, suggesting that VBench-2.0 aligns well with human judgment across all the tested dimensions.", "section": "Human Alignment of VBench-2.0"}, {"figure_path": "https://arxiv.org/html/2503.21755/x6.png", "caption": "Figure S6: Example for Mechanics.", "description": "This figure shows an example of the evaluation for the Mechanics sub-dimension within the Physics dimension of the VBench-2.0 benchmark.  It illustrates how the benchmark assesses whether video generation models adhere to basic mechanical principles.  The question is posed: \"Whether the soda can be squeezed as air is gradually and forcefully removed.\" Then it shows two possible video generation results, one where the soda can remains unchanged (score: 0) and one where it is compressed (score: 1), demonstrating the evaluation criteria used to assess the model's understanding of mechanical properties like compression and force.", "section": "G.1. Physics"}, {"figure_path": "https://arxiv.org/html/2503.21755/x7.png", "caption": "Figure S7: Example for Material.", "description": "The figure shows an example of the Material evaluation dimension in VBench-2.0.  It presents a question and two possible answers to assess a video generation model's ability to simulate material properties realistically. The question is about whether a grey color results from mixing equal amounts of white and black paint. Option (a) shows the incorrect outcome (no grey), while option (b) displays the correct outcome (yes, grey). This tests the model's understanding of basic material interactions.", "section": "G.1 Physics"}, {"figure_path": "https://arxiv.org/html/2503.21755/x8.png", "caption": "Figure S8: Example for Thermotics.", "description": "This figure shows an example of the Thermotics evaluation dimension in VBench-2.0.  The prompt asks whether dry ice will remain solid and maintain its original shape at -90\u00b0C.  Option (a) shows a video where this is not the case, receiving a score of 0. Option (b) illustrates a video where the dry ice behaves correctly, resulting in a score of 1. This highlights the benchmark's ability to assess a model's understanding of material properties and their changes under varying temperature conditions.", "section": "G.1. Physics"}, {"figure_path": "https://arxiv.org/html/2503.21755/x9.png", "caption": "Figure S9: Example for Multi-View Consistency.", "description": "This figure shows an example of the Multi-View Consistency evaluation from VBench-2.0.  It visually demonstrates how the benchmark assesses whether a video maintains geometric consistency, especially when there is fast camera motion. The figure likely contrasts videos where objects maintain their shape and spatial relationships despite camera movement against those where they do not.", "section": "G.1. Physics"}, {"figure_path": "https://arxiv.org/html/2503.21755/x10.png", "caption": "Figure S10: Example for Diversity.", "description": "This figure demonstrates the diversity aspect of the VBench-2.0 benchmark.  It shows two example videos generated by different models in response to the same prompt.  The goal is to illustrate how diverse the outputs can be when models try to fulfill a prompt, and that diversity is one way to measure creativity in video generation.  Visual differences in style and content composition highlight the varying results.", "section": "G.2. Creativity"}, {"figure_path": "https://arxiv.org/html/2503.21755/x11.png", "caption": "Figure S11: Example for Composition.", "description": "This figure shows example results for the Composition sub-dimension of the VBench-2.0 benchmark suite.  The benchmark evaluates the ability of video generation models to create novel and varied compositions, going beyond simple arrangements.  The figure shows examples of video generation model outputs alongside binary evaluations (0 or 1) indicating whether the model succeeded in meeting the specific compositional criteria given in the prompt. These examples illustrate different levels of complexity in composition, such as simple scene arrangements, the combination of multiple entities, and more complex interactions between multiple entities. The goal is to test the model's ability to create both plausible and novel visual combinations, not just basic arrangements.", "section": "G.3. Controllability"}, {"figure_path": "https://arxiv.org/html/2503.21755/x12.png", "caption": "Figure S12: Example for Dynamic Spatial Relationship.", "description": "This figure shows an example of evaluating the Dynamic Spatial Relationship dimension in VBench-2.0.  It illustrates a scenario where a dog's position relative to a sofa changes according to the prompt.  The figure displays a sequence of video frames showing the dog initially to the right of the sofa, and then moving to the left of the sofa as instructed by the prompt.  This demonstrates the model's ability to accurately render spatial relationships and object movements in response to detailed textual instructions.", "section": "G.3. Controllability"}, {"figure_path": "https://arxiv.org/html/2503.21755/x13.png", "caption": "Figure S13: Example for Dynamic Attribute.", "description": "This figure shows an example of evaluating the Dynamic Attribute dimension in VBench-2.0.  The Dynamic Attribute dimension assesses whether a model accurately changes the attributes of objects or creatures in a video as specified in a text prompt. The example shown likely illustrates a change in the color of an object.  A question is posed: \"Does the wall change from yellow to grey?\" The image then displays two videos: one where the wall remains yellow (incorrect), and another where the wall changes to grey (correct). This demonstrates the evaluation process; the model's ability to modify attributes successfully is judged based on whether it reflects the changes described in the prompt.", "section": "G.3. Controllability"}, {"figure_path": "https://arxiv.org/html/2503.21755/x14.png", "caption": "Figure S14: Example for Motion Order Understanding.", "description": "This figure shows an example of evaluating the \"Motion Order Understanding\" dimension in VBench-2.0.  The evaluation involves a video generated by a model in response to a prompt specifying a sequence of actions.  The image displays the prompt question and example video frames, illustrating whether the model correctly generated the actions in the specified order. The caption shows two example prompts and two responses for the assessment.", "section": "G.3. Controllability"}, {"figure_path": "https://arxiv.org/html/2503.21755/x15.png", "caption": "Figure S15: Example for Human Interaction.", "description": "This figure shows an example of evaluating the \"Human Interaction\" dimension in VBench-2.0.  It demonstrates the assessment of whether video generation models correctly depict interactions between humans. The specific scenario shown involves one person adjusting another person's glasses. This necessitates understanding complex human behavior and the spatial relationships between individuals during the action.", "section": "G.3. Controllability"}, {"figure_path": "https://arxiv.org/html/2503.21755/x16.png", "caption": "Figure S16: Example for Complex Plot.", "description": "This figure illustrates the evaluation pipeline and results for the 'Complex Plot' dimension of VBench-2.0.  It shows how a long, multi-part story prompt is broken down into individual plot points.  The video generation model produces a video, and then a Vision-Language Model (VLM) generates a caption describing the video's content.  A Large Language Model (LLM) then compares the VLM's caption to the original prompt's plot points to assess how well the video's narrative matches the intended storyline. The final score reflects the degree of alignment between the generated video and the expected plot progression.  The example uses 'Little Red Riding Hood' as the story prompt.", "section": "G. Details on Evaluation Dimension and Method Suite"}, {"figure_path": "https://arxiv.org/html/2503.21755/x17.png", "caption": "Figure S17: Example of Dynamic Camera Motion.", "description": "This figure showcases an example of dynamic camera motion from video generation models.  Specifically, it demonstrates how well models can produce specified camera movements, such as zooming in or out. This test is part of the Physics dimension within the VBench-2.0 benchmark suite, which evaluates various aspects of video realism beyond visual aesthetics.", "section": "G.1. Physics"}, {"figure_path": "https://arxiv.org/html/2503.21755/x18.png", "caption": "Figure S18: Example for Plot Consistency.", "description": "This figure shows an example of a complex plot evaluation from the VBench-2.0 benchmark.  The top shows the prompt used for video generation, detailing a multi-step story of Little Red Riding Hood. Below, the evaluation pipeline is illustrated: the prompt is segmented into five key plot points, a video generative model produces a video, and then LLAVA-video and Qwen models are used to assess how well the video matches each plot point. The final score for plot consistency is displayed.", "section": "G.3. Controllability"}, {"figure_path": "https://arxiv.org/html/2503.21755/x19.png", "caption": "Figure S19: Example for Complex Landscape.", "description": "This figure shows an example of a complex landscape prompt from the VBench-2.0 evaluation suite.  It illustrates how well a video generation model can create a long, detailed video that accurately reflects a multi-part description of a landscape. The prompt describes a scene shifting from a high-altitude view of an endless ice plain, progressing through changes in time of day and weather, finally ending with the camera pulling back, revealing the aurora borealis in the vastness of the plains.", "section": "G.4 Human Fidelity"}, {"figure_path": "https://arxiv.org/html/2503.21755/x20.png", "caption": "Figure S20: Example for Human Anatomy.", "description": "This figure shows examples of human anatomy evaluation in VBench-2.0.  It visually demonstrates instances where video generation models produce anomalies in human figures, such as missing limbs, distorted body parts, and incorrect hand or face structures. These examples highlight the challenges in achieving high-fidelity human representation in generated videos, a key area assessed by VBench-2.0's Human Fidelity dimension.", "section": "G.4. Human Fidelity"}, {"figure_path": "https://arxiv.org/html/2503.21755/x21.png", "caption": "Figure S21: Example for Human Identity.", "description": "This figure shows an example of evaluating the consistency of human identity in generated videos.  Two video clips are presented, one where the identity of the person remains consistent throughout (high score), and another where the identity changes or is inconsistent (low score). This is assessed using the ArcFace model, measuring the similarity of facial features between frames. Inconsistent identity might be indicated by changes in facial appearance, hair style, or other distinguishing features over the duration of the video.", "section": "G.4. Human Fidelity"}, {"figure_path": "https://arxiv.org/html/2503.21755/x22.png", "caption": "Figure S22: Example for Human Clothes.", "description": "This figure shows examples of video frames where the human clothes maintain consistency (left) and inconsistency (right) throughout the video.  The evaluation assesses if the generated video shows consistent clothing for the human character across all frames, focusing on color, texture, and overall garment. Inconsistent clothing would involve changes in these attributes or instances where the clothing seemingly disappears or changes arbitrarily.", "section": "G.4. Human Fidelity"}, {"figure_path": "https://arxiv.org/html/2503.21755/x23.png", "caption": "Figure S23: Example for Motion Rationality.", "description": "This figure shows an example of evaluating the Motion Rationality dimension in VBench-2.0.  The image displays a video still where a person is supposedly opening a window.  The question posed is whether the person in the video truly completes the action of opening the window, which is a crucial element of assessing intrinsic faithfulness.  The 'yes' answer implies that the generated video accurately depicts the entire action, demonstrating an understanding of real-world mechanics, while a 'no' answer indicates a flaw where the motion is incomplete or unrealistic, highlighting a deficiency in the model's commonsense reasoning or physical simulation capabilities.", "section": "G.5. Commonsense"}, {"figure_path": "https://arxiv.org/html/2503.21755/x24.png", "caption": "Figure S24: Example for Instance Preservation.", "description": "This figure demonstrates the evaluation of instance preservation in video generation models. Instance preservation assesses a model's ability to maintain the correct number of objects throughout a video, even with complex scenarios such as object movement, collisions, and interactions.  The figure likely shows example video frames, highlighting instances where a model successfully preserves object counts (correct) and where it fails (incorrect).  This allows assessment of the model's ability to understand and maintain the consistent number of objects within a video scene, even under dynamic changes.", "section": "G.5. Commonsense"}]