[{"figure_path": "https://arxiv.org/html/2412.01292/x1.png", "caption": "Figure 1: We propose LSceneLLM, a novel framework for adaptive large 3D scene understanding. (a) Existing methods struggle to locate\ntask-relevant visual information when facing large scenes. (b) We are committed to precisely identifying fine-grain task-related visual features through adaptive scene modeling. (c) Our method outperforms existing approaches across various benchmarks.", "description": "Figure 1 illustrates the LSceneLLM framework for enhanced large 3D scene understanding.  Panel (a) shows the limitations of existing methods in identifying task-relevant visual details within large scenes due to their task-agnostic approach.  Panel (b) highlights LSceneLLM's adaptive approach: utilizing LLMs to prioritize task-relevant areas and a scene magnifier module to capture fine-grained details within those areas. Panel (c) presents a comparison demonstrating LSceneLLM's superior performance across various benchmarks, showcasing its effectiveness in large 3D scene understanding.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.01292/x2.png", "caption": "Figure 2: An Overview of LSceneLLM. LSceneLLM first perceives the scene through sparse vision tokens at the coarse level and then enhances regions of interest using dense vision tokens. Our method can effectively handle various visual language tasks in large scenes.", "description": "LSceneLLM processes 3D scenes in two stages. First, a coarse understanding is built using sparse vision tokens from a downsampled point cloud.  Then, a dense token selector identifies task-relevant areas based on the LLM's attention map, extracting detailed dense vision tokens from those specific regions. These dense tokens are integrated with the sparse tokens using an adaptive self-attention module, significantly improving the model's ability to focus on critical details within large scenes.  This two-stage approach enables the effective handling of various visual language tasks in complex 3D environments. ", "section": "3. LSceneLLM: Adaptive Framework For Large Scene Understanding"}, {"figure_path": "https://arxiv.org/html/2412.01292/x3.png", "caption": "Figure 3: Illustration of Adaptive Self-attention Module and Dense Vision Token Selector. We first obtain the focused regions by analyzing the attention map of LLM. Then we extract dense point cloud features from the region of interest and parse dense vision tokens through sampling and grouping operations.", "description": "This figure illustrates the LSceneLLM framework's core components: the Adaptive Self-attention Module and the Dense Vision Token Selector.  The process begins by analyzing the Large Language Model's (LLM) attention map to identify regions of interest within the scene. This attention map highlights areas that the LLM deems relevant to the given task.  Next, the Dense Vision Token Selector uses this information to extract high-resolution point cloud features specifically from these key regions. These detailed features are then processed through sampling and grouping operations to create 'dense vision tokens.' Finally, the Adaptive Self-attention Module integrates these newly created dense vision tokens with the existing sparse visual information (from the rest of the scene), effectively enriching the LLM's understanding of the scene with crucial context-specific details.", "section": "3. LSceneLLM: Adaptive Framework For Large Scene Understanding"}, {"figure_path": "https://arxiv.org/html/2412.01292/x4.png", "caption": "Figure 4: Examples of dataset XR-Scene. XR-Scene contains three cross-room scene benchmarks that comprehensively evaluate different understanding abilities.", "description": "XR-Scene is a new benchmark dataset designed to evaluate large-scale 3D scene understanding capabilities.  It features three challenging tasks: XR-QA (cross-room question answering), XR-EmbodiedPlanning (cross-room embodied planning), and XR-SceneCaption (cross-room scene captioning). Each task requires the model to understand the spatial relationships between objects and rooms across multiple rooms, going beyond the single-room scope of existing benchmarks. The figure displays example scenes and questions from the XR-Scene dataset to illustrate the complexity and scale involved in the tasks.", "section": "4. XR-Scene: Cross-Room Scene Understanding Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.01292/x5.png", "caption": "Figure 5: Visualization of attention map of LLM. Red represents high activation values, while blue represents low activation values.", "description": "This figure visualizes the attention maps generated by the LLM (Large Language Model) in LSceneLLM, a novel framework for large 3D scene understanding.  The heatmaps show the model's focus during different question-answering tasks. Red indicates high activation values (strong attention), while blue indicates low activation values (weak attention).  The comparison across different models (LSceneLLM, Ll3da, and Leo) demonstrates how LSceneLLM effectively focuses on task-relevant objects, particularly small objects, which other methods often miss.", "section": "5.6 Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2412.01292/x6.png", "caption": "Figure 6: Generation pipeline of\nXR-SceneCaption and XR-EmbodiedPlanning.", "description": "This figure illustrates the process of generating captions and embodied planning tasks within the XR-Scene benchmark dataset.  It details how top-down views of scenes, along with per-room descriptions and object lists, are provided as input to GPT-4.  GPT-4 then generates scene captions summarizing the overall scene and individual rooms, as well as a high-level task with decomposed steps for embodied planning.  The process highlights how the multi-room nature of the scenes, combined with detailed room descriptions, allows for the creation of complex and comprehensive tasks that assess a model's holistic understanding of the environment.", "section": "4. XR-Scene: Cross-Room Scene Understanding Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.01292/x7.png", "caption": "Figure 7: More Attention Visualization of LSceneLLM.", "description": "Figure 7 visualizes attention maps generated by LSceneLLM for various questions about a scene.  Redder colors indicate higher attention weights, showing where the model focuses its attention while answering. The figure showcases the model's ability to selectively attend to relevant objects and regions within the scene, successfully identifying details even in complex scenarios. It highlights LSceneLLM's improved ability to locate and focus on task-relevant details when compared to other methods.", "section": "5.6 Qualitative Analysis"}]