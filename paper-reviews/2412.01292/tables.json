[{"content": "| Methods | XR-QA |  |  | XR-SceneCaption |  |  | XR-EmbodiedPlanning |  |  |\n|---|---|---|---|---|---|---|---|---|---|---| \n|  | CIDEr | METEOR | ROUGE | CIDEr | METEOR | ROUGE | CIDEr | METEOR | ROUGE |\n|---|---|---|---|---|---|---|---|---|---|---| \n| **Zero-Shot** |  |  |  |  |  |  |  |  |  |\n| Chat-Scene<sup>#</sup> [16] | 69.55 | 26.63 | 10.06 | 0.01 | 5.94 | 1.52 | 32.64 | 20.71 | 10.26 |\n| Leo<sup>\u2217</sup> [18] | 55.40 | 22.71 | 6.96 | 0.02 | 1.92 | 2.92 | 9.74 | 16.84 | 6.88 |\n| Ll3da [8] | 24.78 | 12.66 | 5.31 | 0.12 | 8.71 | 5.14 | 7.02 | 15.21 | 7.17 |\n| **Finetuning** |  |  |  |  |  |  |  |  |  |\n| Chat-Scene<sup>#</sup> [16] | 114.10 | 35.93 | 14.32 | 3.58 | 17.49 | 11.59 | 46.18 | 22.34 | 36.71 |\n| Leo<sup>\u2217</sup> [18] | 112.09 | 35.47 | 14.02 | 2.42 | 15.96 | 10.25 | 39.45 | 18.99 | 33.31 |\n| Ll3da [8] | 112.80 | 36.94 | 18.68 | 3.22 | 20.95 | 13.49 | 35.96 | 15.74 | 31.50 |\n| **LSceneLLM(Ours)** | **117.21** | **38.18** | **19.30** | **4.59** | **23.43** | **16.16** | **63.08** | **22.97** | **36.96** |", "caption": "Table 1: 3D large scene understanding results. All use Ll3da and XR-Scene data for training. \u2217 means do not identify the question-related objects for the model. # means requiring images and point clouds as input.", "description": "This table presents a comparison of different methods for 3D large scene understanding, specifically focusing on three sub-tasks: XR-QA (cross-room question answering), XR-SceneCaption (cross-room scene captioning), and XR-EmbodiedPlanning (cross-room embodied planning).  The results are evaluated using metrics such as CIDER, METEOR, and ROUGE, which assess the quality of generated text. The table shows performance for both zero-shot and fine-tuned models, highlighting the improvements achieved by the proposed LSceneLLM framework.  The use of Ll3da and XR-Scene datasets for training is also noted, along with clarification on whether methods required both image and point cloud input (#) or did not identify question-relevant objects (*).", "section": "4. XR-Scene: Cross-Room Scene Understanding Benchmark"}, {"content": "| Method | LLM | Training Data | ROUGE | METEOR | CIDEr |\n|---|---|---|---|---|---| \n| 3D-VLP [21] | - | - | 34.51 | 13.53 | 66.97 |\n| ScanQA [2] | - | - | 33.33 | 13.14 | 64.86 |\n| Chat3D [41] | Vicuna-7b | - | 28.5 | 11.9 | 53.2 |\n| Chat3D-v2 [17] | Vicuna-7b | 204k | 40.1 | 16.1 | 77.1 |\n| 3D-LLM [15] | BLIP2-flanT5 | 675k | 35.7 | 14.5 | 69.4 |\n| SceneLLM [12] | Llama2-7b | 690k | 35.9 | 15.8 | 80.00 |\n| Chat-Scene [16] | Vicuna-7b | 145k | 37.79 | 15.94 | 77.75 |\n| Leo* [18] | Vicuna-7b | 1034k+145k | 40.24 | 16.68 | 80.20 |\n| Ll3da [8] | Opt-1.3b | 145k | 37.02 | 15.37 | 75.67 |\n| Ll3da [8] | Llama2-7b | 145k | 38.31 | 15.91 | 79.08 |\n| **LSceneLLM(Ours)** | Llama2-7b | 145k | **40.82** | **17.95** | **88.24** |", "caption": "Table 2: 3D question answering results on the ScanQA\u00a0[2] validation dataset. \u2217 means do not identify the question-related objects for the model.", "description": "This table presents the performance of various 3D Vision-Language Models (3D-VLMs) on the ScanQA [2] validation dataset, a benchmark for evaluating 3D question answering.  The models are evaluated based on their ability to answer questions about a 3D scene using the ROUGE, METEOR, and CIDEr metrics. The asterisk (*) indicates methods that do not specifically identify objects relevant to the question before generating an answer. This highlights the difference in performance between methods that focus on task-relevant details versus those processing the entire scene.", "section": "5. Experiment"}, {"content": "| Method | Exist (H0) | Exist (H1) | Exist (All) | Count (H0) | Count (H1) | Count (All) | Object (H0) | Object (H1) | Object (All) | Status (H0) | Status (H1) | Status (All) | Comparison (H0) | Comparison (H1) | Comparison (All) | Acc | \n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| NuscenesQA* [29] | 87.7 | 81.1 | 84.1 | 21.9 | 20.7 | 21.3 | 70.2 | 45.6 | 49.2 | 62.8 | 52.4 | 55.9 | 81.6 | 68.0 | 69.2 | 58.1 |\n| LLaVA-Adaptaer-v2 [13] | 34.2 | 6.3 | 19.3 | 5.0 | 0.1 | 2.7 | 23.7 | 4.6 | 7.6 | 9.8 | 11.3 | 10.8 | 2.6 | 1.5 | 1.6 | 9.6 |\n| LLaVA [25] | 38.9 | 51.9 | 45.8 | 7.7 | 7.6 | 7.7 | 10.5 | 7.4 | 7.8 | 7.0 | 9.9 | 9.0 | 64.5 | 50.8 | 52.1 | 26.2 |\n| LidarLLM [45] | 79.1 | 70.6 | 74.5 | 15.3 | 14.7 | 15.0 | 59.6 | 34.1 | 37.8 | 53.4 | 42.0 | 45.9 | 67.0 | 57.0 | 57.8 | 48.6 |\n| OccLLaMA [42] | 80.6 | 79.3 | 79.9 | 18.6 | 19.1 | 18.9 | 64.9 | 39.0 | 42.8 | 48.0 | 49.6 | 49.1 | 80.6 | 63.7 | 65.2 | 53.4 |\n| **LSceneLLM(Ours)** | **86.4** | **81.3** | **83.6** | 19.4 | **19.8** | **19.6** | 64.4 | **41.3** | **44.8** | **58.8** | **51.2** | **53.8** | **81.0** | **67.5** | **68.7** | **56.4** |", "caption": "Table 3: 3D question answering results on outdoor scene benchmark NuscenesQA\u00a0[29]. * means downstream specialist model.", "description": "This table presents the performance of various models on the NuscenesQA benchmark, a dataset for evaluating 3D question answering in outdoor scenes.  The results are categorized by different metrics, including those related to existence, count, status, object comparison, and overall accuracy.  The asterisk (*) indicates models that utilize downstream specialist approaches. This allows for a comparison of general-purpose 3D question answering models against specialized models designed for specific tasks within the outdoor setting.", "section": "5. Experiment"}, {"content": "| Method | Scene Magnifier Module | XR-QA ROUGE | XR-QA METEOR | XR-QA CIDEr | XR-QA-S ROUGE | XR-QA-S METEOR | XR-QA-S CIDEr |\n|---|---|---|---|---|---|---|---|\n| Leo<sup>#</sup> [18] | \u2717 | 36.56 | 18.61 | 110.33 | 36.10 | 18.06 | 103.16 |\n| Leo<sup>#</sup> [18] | \u2713 | 37.53<sub>(+0.97)</sub> | 19.00<sub>(+0.39)</sub> | 113.46<sub>(+3.13)</sub> | 36.88<sub>(+0.77)</sub> | 18.47<sub>(+0.41)</sub> | 107.56<sub>(+5.29)</sub> |\n| Ll3da<sup>#</sup> [8] | \u2717 | 37.19 | 18.51 | 111.35 | 36.04 | 17.61 | 95.65 |\n| Ll3da<sup>#</sup> [8] | \u2713 | 37.85<sub>(+0.65)</sub> | 19.15<sub>(+0.56)</sub> | 115.79<sub>(+4.44)</sub> | 37.23<sub>(+1.19)</sub> | 18.60<sub>(+0.99)</sub> | 106.73<sub>(+11.09)</sub> |\n| LSceneLLM | \u2717 | 36.58 | 18.65 | 109.92 | 35.47 | 17.91 | 97.57 |\n| **LSceneLLM(Ours)** | \u2713 | **38.18<sub>(+1.60)</sub>** | **19.30<sub>(+0.65)</sub>** | **117.21<sub>(+7.29)</sub>** | **38.15<sub>(+2.68)</sub>** | **18.69<sub>(+0.78)</sub>** | **109.42<sub>(+11.85)</sub>** |", "caption": "Table 4: More results on the XR-QA validation dataset and challenge subset XR-QA-S. # We re-implement Leo\u00a0[18] and Ll3da\u00a0[8] keeping all other settings the same as ours to conduct a fair and further comparison.", "description": "This table presents a comparison of results for XR-QA and its challenging subset XR-QA-S across different models.  The models compared include LSceneLLM (the authors' proposed method), Leo [18], and Ll3da [8].  To ensure a fair comparison, Leo and Ll3da were re-implemented using the same settings as LSceneLLM. The evaluation metrics used are ROUGE, METEOR, and CIDEr, providing a comprehensive assessment of the models' performance on both standard and challenging question-answering tasks in large 3D scenes.", "section": "4. XR-Scene: Cross-Room Scene Understanding Benchmark"}, {"content": "| Parameter | ROUGE | METEOR | CIDEr |\n|---|---|---|---|\n| Threshold 96(AT: 10%-20%) | **38.18** | **19.30** | **117.21** |\n| Threshold 127(AT: 3%-5%) | 37.89 | 19.26 | 115.92 |\n| Threshold 64(AT: 40%-50%) | 37.68 | 19.07 | 114.69 |\n| Dense Token Num 2 | 37.91 | 19.14 | 115.32 |\n| Dense Token Num 4 | **38.18** | **19.30** | **117.21** |\n| Dense Token Num 6 | 37.54 | 19.03 | 115.14 |\n| Select Strategy Attention Map | **38.18** | **19.30** | **117.21** |\n| Select Strategy Random | 37.64 | 19.18 | 115.66 |\n| Vision Token Num 512 | 37.27 | 18.80 | 112.19 |\n| Vision Token Num 128 | 36.58 | 18.65 | 109.92 |\n| Vision Token Num 128<sup>#</sup> | **38.18** | **19.30** | **117.21** |", "caption": "Table 5: Ablation studies. ATR: the activate token ratio of sparse vision tokens. #: do not use the scene magnifier module.", "description": "This table presents the results of ablation studies conducted on the LSceneLLM model.  It examines the impact of different components and hyperparameters on the model's performance. Specifically, it investigates the effect of varying the activation token ratio (ATR) of sparse vision tokens, and the impact of removing the scene magnifier module. The results are presented in terms of ROUGE, METEOR, and CIDEr scores, which are standard metrics for evaluating text generation quality.  Analyzing this table helps understand the contribution of individual components of the LSceneLLM framework and how they impact overall scene understanding ability.", "section": "3. LSceneLLM: Adaptive Framework For Large Scene Understanding"}, {"content": "| Threshold | Activate Token Ratio | ROUGE | METEOR | CIDEr |\n|---|---|---|---|---|\n| 64 | 40% - 50% | 37.68 | 19.07 | 114.69 |\n| 96 | 10% - 20% | **38.18** | **19.30** | **117.21** |\n| 127 | 3% - 5% | 37.89 | 19.26 | 115.92 |", "caption": "Table 6: Ablation studies of selection threshold", "description": "This table presents the results of ablation studies conducted to determine the optimal threshold for selecting relevant visual tokens.  The experiment varied the selection threshold, impacting the proportion of visual tokens considered by the model.  The table shows the effects of these different thresholds on the model's performance, as measured by ROUGE, METEOR, and CIDEr scores.", "section": "5. Ablation Studies"}, {"content": "| Vision Token Num | Scene Magnifier Module | ROUGE | METEOR | CIDEr |\n|---|---|---|---|---|\n| 512 | \u2717 | 37.27 | 18.80 | 112.89 |\n| 128 | \u2717 | 36.58 | 18.65 | 109.92 |\n| 128 | \u2713 | **38.18** | **19.30** | **117.21** |", "caption": "Table 7: Ablation studies of the number of vision tokens", "description": "This table presents the results of ablation studies conducted to determine the optimal number of vision tokens used in the LSceneLLM model.  It shows how different numbers of vision tokens (512, 128, 128) impact the model's performance, specifically the ROUGE, METEOR, and CIDEr scores on the XR-QA task. The table compares the performance when the scene magnifier module is included versus when it is not included (X). This analysis helps determine the optimal balance between computational cost and model performance.", "section": "5.5 Ablation Studies"}, {"content": "| Dense Token Num | ROUGE | METEOR | CIDEr |\n|---|---|---|---|\n| 2 | 37.91 | 19.14 | 115.32 |\n| 4 | 38.18 | 19.30 | 117.21 |\n| 6 | 37.54 | 19.03 | 115.14 |", "caption": "Table 8: Ablation studies of dense token", "description": "This table presents the ablation study results focusing on the impact of varying the number of dense vision tokens used in the LSceneLLM model.  It shows how the model's performance on the XR-QA benchmark (a cross-room scene question answering task) changes as the count of dense vision tokens is altered.  The results illustrate the effect of different numbers of dense tokens on the model's ability to accurately identify and utilize relevant visual details within large scenes for effective question answering.", "section": "5. Ablation Studies"}, {"content": "| Select Strategy | ROUGE | METEOR | CIDEr |\n|---|---|---|---|\n| Attention Map | **38.18** | **19.30** | **117.21** |\n| Random | 37.64 | 19.18 | 115.66 |", "caption": "Table 9: Ablation studies of selection strategies", "description": "This table presents the results of ablation studies conducted to evaluate the effectiveness of different strategies for selecting dense vision tokens.  The study compares the performance of using an attention map-based selection method against a random selection method, both in terms of ROUGE, METEOR, and CIDEr scores.  The goal is to determine whether using the attention map of the large language model to guide the selection of dense visual tokens improves the overall performance of the scene understanding model.", "section": "5. Ablation Studies"}, {"content": "| Method | Scene Caption ROUGE | Scene Caption CIDEr | Scene Caption METEOR | Embodied Planning ROUGE | Embodied Planning CIDEr | Embodied Planning METEOR | Embodied QA ROUGE | Embodied QA CIDEr | Embodied QA METEOR |\n|---|---|---|---|---|---|---|---|---|---|\n| Leo* [18] | 1.80 | 20.84 | 13.29 | 46.40 | 204.78 | 19.86 | 30.89 | 86.14 | 18.81 |\n| Chat-Scene [16] | **3.67** | 21.05 | 12.60 | 40.03 | 210.86 | 20.71 | 34.23 | 99.01 | 18.48 |\n| Ll3da [8] | 1.44 | **24.62** | 12.93 | 45.34 | 186.13 | 19.60 | 33.75 | 95.53 | 19.81 |\n| LSceneLLM(Ours) | 3.07 | 21.88 | **14.79** | **47.05** | **214.63** | **21.05** | **36.00** | **104.98** | **21.26** |", "caption": "Table 10: More 3D scene understanding results. \u2217 means do not identify the question-related objects for the model.", "description": "This table presents additional results for three 3D scene understanding tasks: scene captioning, embodied planning, and embodied question answering.  The results compare the performance of the proposed LSceneLLM model against existing state-of-the-art methods (Leo and Chat-Scene) and another baseline (Ll3da).  The metrics used for evaluation are ROUGE, CIDEr, and METEOR, reflecting different aspects of text generation quality.  The asterisk (*) indicates methods that don't specifically identify question-related objects when processing the scene.", "section": "4. XR-Scene: Cross-Room Scene Understanding Benchmark"}, {"content": "| Method | Scene Magnifier Module | Vision Token Num | Flops | CIDEr |\n|---|---|---|---|---|\n| Leo | \u2717 | 200 | 6.55 | 110.33 |\n| Ll3da | \u2717 | 32 | 4.11 | 111.35 |\n| LSceneLLM | \u2717 | 128 | 5.3 | 109.92 |\n| LSceneLLM | \u2713 | 128 | 6.33 | **117.21** |", "caption": "Table 11: Computational complexity results on XR-QA", "description": "This table presents the computational complexities (measured in FLOPs) and CIDEr scores of three different 3D vision-language models on the XR-QA benchmark.  It compares the performance of Leo, Ll3da, and the authors' proposed LSceneLLM model, varying the number of vision tokens used.  The results demonstrate the relative efficiency of each model in handling large-scale scene understanding tasks.", "section": "5. Experiment"}]