[{"content": "| \u03ba | FID \u2193 | CLIP \u2191 | Precision \u2191 | Recall \u2191 |\n|---|---|---|---|---|\n| 2 | 10.77 | 0.31 | 0.54 | **0.53** |\n| 3 | **9.60** | **0.32** | 0.59 | **0.50** |\n| 4 | 10.35 | **0.32** | **0.60** | 0.47 |\n| 5 | 11.53 | **0.33** | **0.61** | 0.44 |\n| mathcal{U}(2,5) | **9.86** | **0.32** | **0.60** | 0.48 |", "caption": "Table 1: Results on the zero-shot MS COCO-2014 30K benchmark [24] of SDv2.1 [38] when using PNDM Scheduler with different CFG scale \u03ba\ud835\udf05\\kappaitalic_\u03ba.", "description": "This table presents the results of experiments conducted on the zero-shot MS COCO-2014 30K benchmark using the Stable Diffusion v2.1 model with the PNDM scheduler.  The experiment varied the Classifier-Free Guidance (CFG) scale (\u03ba) and measured the model's performance across four metrics: FID (Fr\u00e9chet Inception Distance), CLIP (Contrastive Language\u2013Image Pre-training) score, Precision, and Recall. Lower FID indicates better image quality, while higher CLIP, Precision, and Recall scores suggest improved image-text alignment and generation quality.  The results highlight how different CFG scales impact the model's performance on this benchmark.", "section": "4. Proper Guidance - SwiftBrush"}, {"content": "| Teacher | LoRA Teacher | FID \u2193 |\n|---|---|---:|\n|  |  | 10.72 |\n| \u2713 |  | 10.81 |\n|  | \u2713 | 9.51 |\n| \u2713 | \u2713 | **9.03** |", "caption": "Table 2: Ablation studies on Random CFG in VSD loss with SDv2.1 backbone on 2K prompts from MS COCO-2014 [24].", "description": "This table presents ablation study results evaluating the impact of applying the classifier-free guidance (CFG) with a randomized scale (instead of a fixed scale) to different parts of the variational score distillation (VSD) loss function.  The experiment uses the Stable Diffusion 2.1 model as a backbone and a set of 2000 prompts from the MS COCO-2014 dataset. It compares the performance (measured by FID score) when applying the randomized CFG to either only the LoRA teacher, only the frozen teacher, or both, providing insights into which components benefit most from the randomized CFG approach and how this affects the overall model stability and performance during training.", "section": "4. Proper Guidance - SwiftBrush"}, {"content": "| NASA | SDXL-LCM | SDXL-DMD2 | SDXL-DMD2 | PG-SB |\n|---|---|---|---|---|\n|  | 4 steps | 4 steps | 1 step | 1 step |\n| \u2717 | 43% | 27% | 25% | 38% |\n| \u2713 | 97% | 100% | 100% | 92% |", "caption": "Table 3: Comparison of success rate of unwanted feature removal in generated images between models with and without NASA.", "description": "This table compares the effectiveness of the Negative-Away Steer Attention (NASA) module in removing unwanted features from images generated by different models. It shows the success rate of unwanted feature removal for four models: SDXL-LCM, SDXL-DMD2 (both 4-step and 1-step versions), and PG-SB, with and without the NASA module. The success rate is calculated as the percentage of images that successfully exclude the features specified by the negative prompt, averaged over six positive-negative prompt pairs.  Each model generated 100 images for each pair of prompts, and human evaluators assessed the results.", "section": "6. Experiments"}, {"content": "| Method | Anime | Photo | Concept Art | Paintings | Average |\n|---|---|---|---|---|---| \n| Stable Diffusion 1.5-based backbone |  |  |  |  |  |\n| SDv1.5 [38]<sup>\u2021</sup> | 26.51 | 27.19 | 26.06 | 26.12 | 26.47 |\n| InstaFlow-0.9B [26]<sup>\u2021</sup> | 26.10 | 26.62 | 25.92 | 25.95 | 26.15 |\n| DMD2 [54]<sup>\u2021</sup> | 26.39 | 27.00 | 25.80 | 25.83 | 26.26 |\n| PG-SB | 27.18 | 27.58 | 26.69 | 26.62 | 27.02 |\n| PG-SB + NASA | **27.19** | **27.59** | **26.71** | **26.63** | **27.03** |\n| Stable Diffusion 2.1-based backbone |  |  |  |  |  |\n| SDv2.1 [38]<sup>\u2020</sup> | 27.48 | 26.89 | 26.86 | **27.46** | 27.17 |\n| SB [33]<sup>\u2020</sup> | 26.91 | 27.21 | 26.32 | 26.37 | 26.70 |\n| SBv2 [7]<sup>\u2020</sup> | 27.25 | 27.62 | 26.86 | 26.77 | 27.13 |\n| PG-SB | 27.56 | 27.84 | 26.97 | 27.03 | 27.35 |\n| PG-SB + NASA | **27.71** | **27.99** | **27.14** | 27.27 | **27.53** |\n| PixArt-\u03b1-based backbone |  |  |  |  |  |\n| PixArt-\u03b1 [6]<sup>\u2021</sup> | 29.62 | 29.17 | 28.79 | 28.69 | 29.07 |\n| YOSO [30]<sup>\u2021</sup> | 28.79 | 28.09 | 28.57 | 28.55 | 28.50 |\n| DMD [55]<sup>\u2021</sup> | 29.31 | 28.67 | 28.46 | 28.41 | 28.71 |\n| PG-SB | 32.19 | 29.09 | 30.39 | 29.69 | 30.34 |\n| PG-SB + NASA | **32.56** | **29.55** | **31.24** | **30.96** | **31.08** |", "caption": "Table 4: HPSv2 comparisons between our method and previous works. \u2020 denotes reported numbers, \u2021 denotes our rerun based on the publicly available model checkpoints.", "description": "This table presents a comparison of Human Preference Score v2 (HPSv2) results for various text-to-image diffusion models, including different backbones (Stable Diffusion 1.5, Stable Diffusion 2.1, PixArt-a).  It compares the performance of the proposed method (PG-SB and PG-SB + NASA) against several existing state-of-the-art models.  The HPSv2 scores are shown for four image categories (Anime, Photo, Concept Art, Paintings), along with the average score across all categories.  The notation \u2020 indicates scores reported by the original papers, while \u2021 indicates scores obtained by rerunning the models using publicly available checkpoints to ensure consistent evaluation.  The table highlights the improvement achieved by the proposed method in HPSv2, demonstrating its superior image quality compared to other models.", "section": "6. Experiments"}, {"content": "| Method | #Params | NFEs | FID \u2193 | CLIP \u2191 | Precision \u2191 | Recall \u2191 | Image-free? |\n|---|---|---|---|---|---|---|---| \n| Stable Diffusion 1.5-based backbone |  |  |  |  |  |  |  |\n| SDv1.5 (cfg=3)<sup>\u2020</sup> | 0.9B | 25 | 8.78 | 0.30 | 0.59 | 0.53 | \u2717 |\n| UFOGen<sup>\u2020</sup> | 0.9B | 1 | 12.78 | - | - | - | \u2717 |\n| InstaFlow-0.9B<sup>\u2020</sup> | 0.9B | 1 | 13.33 | 0.30 | 0.53 | 0.45 | \u2717 |\n| DMD<sup>\u2020</sup> | 0.9B | 1 | 11.49 | 0.32 | - | - | \u2717 |\n| DMD2<sup>\u2021</sup> | 0.9B | 1 | 8.29 | 0.30 | 0.59 | 0.52 | \u2717 |\n| PG-SB | 0.9B | 1 | 10.08 | 0.31 | 0.57 | 0.47 | \u2713 |\n| PG-SB + NASA | 0.9B | 1 | 9.94 | 0.31 | 0.57 | 0.48 | \u2713 |\n| Stable Diffusion 2.1-based backbone |  |  |  |  |  |  |  |\n| SDv2.1 (cfg=3)<sup>\u2021</sup> | 0.9B | 25 | 9.60 | 0.32 | 0.59 | 0.50 | \u2717 |\n| SB<sup>\u2020</sup> | 0.9B | 1 | 15.46 | 0.30 | 0.47 | 0.46 | \u2713 |\n| SBv2<sup>\u2020</sup> | 0.9B | 1 | 8.14 | 0.32 | 0.57 | 0.52 | \u2717 |\n| PG-SB | 0.9B | 1 | 8.91 | 0.34 | 0.57 | 0.49 | \u2713 |\n| PG-SB + NASA | 0.9B | 1 | 8.83 | 0.34 | 0.57 | 0.50 | \u2713 |\n| PixArt-\u03b1-based backbone |  |  |  |  |  |  |  |\n| PixArt-\u03b1 (cfg=4.5)<sup>\u2021</sup> | 0.6B | 20 | 26.85 | 0.32 | 0.52 | 0.23 | \u2717 |\n| YOSO<sup>\u2021</sup> | 0.6B | 1 | 26.04 | 0.30 | 0.46 | 0.30 | \u2717 |\n| DMD<sup>\u2021</sup> | 0.6B | 1 | 30.22 | 0.32 | 0.55 | 0.18 | \u2717 |\n| PG-SB | 0.6B | 1 | 22.03 | 0.33 | 0.52 | 0.24 | \u2713 |\n| PG-SB + NASA | 0.6B | 1 | 21.98 | 0.33 | 0.52 | 0.25 | \u2713 |", "caption": "Table 5: Quantitative comparisons between our method and others on zero-shot MS COCO-2014 benchmark. For multi-step SD models, we report each with the CFG scale that returns the best FID. \u2020 denotes reported numbers, \u2021 denotes our rerun based on the publicly available model checkpoints. \u2018-\u2019 denotes unreported results.", "description": "This table presents a quantitative comparison of various text-to-image generation models on the MS COCO-2014 benchmark, focusing on zero-shot performance.  The models are evaluated across multiple metrics: number of function evaluations (NFEs), Fr\u00e9chet Inception Distance (FID), CLIP score, precision, recall, and whether the model is image-free.  For multi-step Stable Diffusion models, results are shown using the CFG scale that yielded the best FID score.  Note that some results are taken from previously published works (denoted by \u2020) while others are replicated by the authors of the current paper (denoted by \u2021), using publicly available checkpoints; missing values are indicated by '-'.  The table allows for a direct comparison of the efficiency (NFEs), image quality (FID, CLIP, Precision, Recall), and resource requirements (Image-free) of different approaches.", "section": "6. Experiments"}, {"content": "| Hyperparameter | SDv1.5 | SDv2.1 | PixArt-\u03b1 | \n|---|---|---|---| \n| Dataset | JDB + LAION | JDB + LAION | JDB | \n| Batch size | 64 | 64 | 64 | \n| Training iterations | 60k | 40k | 60k | \n| Mixed-Precision (BF16) | Yes | Yes | Yes | \n| (\u03bamin,\u03bamax) | (0.5, 4) | (0.5, 4) | (0.5, 3) | \n| Clip weight | 0.1 | 0.1 | 0.1 | \n| \u03c4 | 0.37 | 0.37 | 0.37 | \n| lr of student | 1e-6 | 1e-6 | 1e-6 | \n| lr of LoRA teacher | 1e-3 | 1e-3 | 1e-3 | \n| LoRA rank r | 64 | 64 | 64 | \n| LoRA scaling \u03b3 | 128 | 128 | 128 |", "caption": "Table 6: Hyperparameters used for training PG-SB.", "description": "This table details the hyperparameters used during the training process of the Proper Guidance-SwiftBrush (PG-SB) model.  It lists specific settings for three different diffusion model backbones: Stable Diffusion v1.5, Stable Diffusion v2.1, and PixArt-a.  The hyperparameters include dataset used, batch size, number of training iterations, use of mixed precision training, the range of the guidance scale (\u03bamin, \u03bamax), the initial and final weight of the CLIP loss, the margin used in the CLIP loss, and the learning rate for both the student and LoRA teacher models, along with the rank and scaling factor of the LoRA adapter.", "section": "8. Implementation Details"}, {"content": "| Positive prompt | Negative prompt | SDXL-LCM | SDXL-DMD2 | SDXL-DMD2 | PG-SB |\n|---|---|---|---|---|---| \n| \"A photo of a person\" | \"male\" | 52% \u2192 99% | 26% \u2192 100% | 29% \u2192 100% | 42% \u2192 81% |\n| \"A photo of a person\" | \"female\" | 48% \u2192 100% | 74% \u2192 100% | 71% \u2192 100% | 58% \u2192 88% |\n| \"A photo of a person\" | \"young\" | 55% \u2192 100% | 44% \u2192 100% | 7% \u2192 100% | 39% \u2192 96% |\n| \"A photo of a person\" | \"old\" | 45% \u2192 92% | 56% \u2192 100% | 93% \u2192 100% | 61% \u2192 79% |\n| \"A photo of a pet\" | \"cat\" | 36% \u2192 100% | 12% \u2192 100% | 60% \u2192 100% | 32% \u2192 99% |\n| \"A photo of a pet\" | \"dog\" | 64% \u2192 100% | 88% \u2192 100% | 40% \u2192 100% | 68% \u2192 99% |", "caption": "Table 7: Comparison of success rates of unwanted feature removal in generated images before and after applying the NASA model.", "description": "This table presents a comparison of success rates in removing unwanted features from images generated by different models.  It compares the performance of four models (SDXL-LCM, SDXL-DMD2 4-step, SDXL-DMD2 1-step, and PG-SB 1-step) before and after incorporating the NASA module, which aims to enhance the control over unwanted features in image generation.  The success rate is measured as the percentage of generated images where the specified negative feature is successfully excluded, given a positive and negative prompt pair.  For each model, the success rate is reported for several different positive/negative prompt combinations.", "section": "6. Experiments"}, {"content": "| Method | Anime | Photo | Concept Art | Paintings | Average |\n|---|---|---|---|---|---| \n| Stable Diffusion 1.5-based backbone |  |  |  |  |  |\n| InstaFlow-0.9B [26]\u2021 | 26.10 | 26.62 | 25.92 | 25.95 | 26.15 |\n| InstaFlow-0.9B + NASA (\u03b1=0.19) | 26.24 | 26.74 | 26.04 | 26.00 | 26.26 (+0.11) |\n| DMD2 [54]\u2021 | 26.39 | 27.00 | 25.80 | 25.83 | 26.26 |\n| DMD2 + NASA (\u03b1=0.04) | 26.41 | 27.02 | 25.80 | 25.83 | 26.27 (+0.01) |\n| PG-SB | 27.18 | 27.58 | 26.69 | 26.62 | 27.02 |\n| PG-SB + NASA (\u03b1=0.04) | 27.19 | 27.59 | 26.71 | 26.63 | 27.03 (+0.01) |\n| Stable Diffusion 2.1-based backbone |  |  |  |  |  |\n| SBv2 [7]\u2020 | 27.25 | 27.62 | 26.86 | 26.77 | 27.13 |\n| SBv2 + NASA (\u03b1=0.30) | 27.45 | 27.85 | 26.93 | 27.09 | 27.33 (+0.20) |\n| PG-SB | 27.56 | 27.84 | 26.97 | 27.03 | 27.35 |\n| PG-SB + NASA (\u03b1=0.30) | 27.71 | 27.99 | 27.14 | 27.27 | 27.53 (+0.18) |\n| PixArt-\u03b1-based backbone |  |  |  |  |  |\n| YOSO [30]\u2021 | 28.79 | 28.09 | 28.57 | 28.55 | 28.50 |\n| YOSO + NASA (\u03b1=0.20) | 28.80 | 28.10 | 28.62 | 28.57 | 28.52 (+0.02) |\n| DMD [55]\u2021 | 29.31 | 28.67 | 28.46 | 28.41 | 28.71 |\n| DMD + NASA (\u03b1=0.45) | 29.34 | 28.71 | 28.50 | 28.52 | 28.77 (+0.06) |\n| PG-SB | 32.19 | 29.09 | 30.39 | 29.69 | 30.34 |\n| PG-SB + NASA (\u03b1=0.80) | 32.56 | 29.55 | 31.24 | 30.96 | 31.08 (+0.74) |", "caption": "Table 8: HPSv2 comparisons between our method and previous works. \u2020 denotes reported numbers, \u2021 denotes our rerun based on the publicly available model checkpoints.", "description": "This table presents a comparison of HPSv2 (Human Preference Score v2) scores across different text-to-image generation models.  The models are grouped by the underlying Stable Diffusion version (1.5, 2.1, or PixArt-a) used as a backbone. For each model and backbone, the table shows HPSv2 scores broken down by image category (Anime, Photo, Concept Art, Paintings). It includes scores for both baseline models and models with enhancements, particularly those involving the PG-SB (Proper Guidance - SwiftBrush) method and the NASA (Negative-Away Steer Attention) method proposed in this paper.  Scores marked with \u2020 represent values reported in the original publications, while those marked with \u2021 are scores re-generated by the authors of this paper based on publicly available model checkpoints to ensure fair comparison across all methods.", "section": "6. Experiments"}]