[{"content": "| Size | #Parameters | hidden_size | num_layers | num_heads | num_groups | GFLOPs |\n|---|---|---|---|---|---|---|\n| Small | 182M | 768 | 12 | 12 | 4 | 995 |\n| Medium | 469M | 1024 | 24 | 16 | 4 | 2873 |\n| Large | 978M | 1536 | 24 | 16 | 4 | 5991 |", "caption": "Table 1: Configurations for the dense backbones. FLOPs are calculated with a single sequence according to\u00a0Narayanan et\u00a0al. (2021).", "description": "This table details the configurations used for the three different sizes of dense backbones in the experiments.  The configurations include the number of parameters, the hidden size of the model, the number of layers, the number of heads in the multi-head attention mechanism, the number of groups in the feed-forward network (FFN), and the number of floating point operations (FLOPs) required for a single sequence. FLOPs are calculated using the method described in Narayanan et al. (2021).", "section": "4.1 Setup"}, {"content": "| Model | ARC-c | ARC-e | BoolQ | HellaSwag | LAMBADA | PIQA | RACE | Avg. |\n|---|---|---|---|---|---|---|---|---|\n| Dense | 19.45 | 43.35 | 54.40 | 28.61 | 31.09 | 61.97 | 28.52 | 38.20 |\n| Hash | 19.28 | 45.45 | 54.95 | 29.68 | 31.44 | 63.06 | 27.66 | 38.79 |\n| Lory | 20.31 | 42.97 | 49.54 | 28.75 | 32.35 | 62.24 | 27.75 | 37.70 |\n| SparseMixer-v2 | 19.80 | 46.72 | 45.96 | 30.24 | 34.12 | 62.89 | 29.00 | 38.39 |\n| EC | 18.86 | 42.97 | 60.21 | 29.14 | 29.26 | 61.92 | 27.37 | 38.53 |\n| dMoE | 20.05 | 45.16 | 57.83 | 29.83 | 32.97 | 63.55 | 28.33 | 39.67 |\n| ReMoE | 20.22 | 46.68 | 54.16 | 30.26 | 35.94 | 63.55 | 29.38 | 40.03 |", "caption": "Table 2: Zero-shot accuracy of different routing methods on downstream tasks.", "description": "This table presents the zero-shot accuracy results of different MoE routing methods on several downstream tasks.  Zero-shot accuracy refers to the model's performance on these tasks without any fine-tuning or task-specific training. The routing methods compared include ReMoE (the proposed method), TopK routing variants (dMoE, EC), deterministic hash routing (Hash), fully-differentiable expert merging routing (Lory), and an improved TopK routing method (SparseMixer-v2).  The downstream tasks used for evaluation represent a variety of natural language understanding challenges.", "section": "4.2 COMPARISON WITH OTHER ROUTING METHODS"}, {"content": "| \u03bb\u2080 | 1e\u207b\u00b9\u2076 | 1e\u207b\u00b9\u00b2 | 1e\u207b\u2078 | 1e\u207b\u2074 | 1 |\n|---|---|---|---|---|---| \n| Valid Loss | 2.031 | 2.029 | 2.032 | 2.036 | 2.032 |\n| Settling time | 138 | 136 | 110 | 55 | 92\u2020 |", "caption": "Table 5: Performance of training N=\ud835\udc41absentN=italic_N =469M, E=8\ud835\udc388E=8italic_E = 8, k=1\ud835\udc581k=1italic_k = 1 models for 120B tokens.", "description": "This table presents the results of training larger language models with 469 million parameters, 8 expert groups, and a sparsity of 1 (meaning only one expert is activated per token) for 120 billion tokens.  It shows the performance (measured by validation loss and several downstream task accuracies) of both the standard Mixture of Experts (MoE) model and the proposed ReMoE model after this extensive training regimen. The downstream tasks measure the models' zero-shot performance on various question answering and text generation benchmarks.", "section": "4 EXPERIMENTS"}, {"content": "| \u03b1 | 1.05 | 1.1 | 1.2 | 1.3 | 1.5 |\n|---|---|---|---|---|---| \n| Valid Loss | 2.033 | 2.028 | 2.032 | 2.029 | 2.057* |\n| Settling time | 414 | 211 | 110 | 80 | 52 |", "caption": "Table 6: End-to-end training time comparison across stages (in hours). The time is measured on N=\ud835\udc41absentN=italic_N = 469M, E=8\ud835\udc388E=8italic_E = 8, k=1\ud835\udc581k=1italic_k = 1 models training over 120B tokens.", "description": "This table presents a detailed breakdown of the end-to-end training time for both MoE and ReMoE models.  The training time is categorized into three stages: Stage I, Stage II, and Stage III.  The model used for this comparison has 469 million parameters (N=469M), 8 experts (E=8), and uses only 1 expert at a time (k=1) during the main training phase.  The total number of tokens used for training was 120 billion.  The table shows the time taken (in hours) for each stage of training, allowing a direct comparison of the efficiency of ReMoE and MoE across different training phases.", "section": "4 EXPERIMENTS"}, {"content": "| Model | Valid Loss | ARC-c | ARC-e | BoolQ | HellaSwag | LAMBADA | PIQA | RACE | Avg. |\n|---|---|---|---|---|---|---|---|---|---| \n| MoE | 1.716 | 23.62 | 52.40 | 53.94 | 35.43 | 43.64 | 68.34 | **31.48** | 44.12 |\n| ReMoE | **1.689** | **25.34** | **55.22** | **55.96** | **36.76** | **45.82** | **68.93** | 30.43 | **45.49** |", "caption": "Table 7: Throughput comparison between TopK-routed MoE and ReLU-routed ReMoE models. TP indicates the tensor parallel size. Train Diff. and Infer Diff. indicate the relative TFLOPS difference of ReMoE compared to MoE, where \u2191 denotes ReMoE is faster, and \u2193 denotes it is slower.", "description": "This table compares the training and inference throughput of ReMoE (ReLU routing) against TopK-routed MoE for different model sizes and levels of tensor parallelism.  It shows the training and inference TFLOPS (TeraFLOPS) for each model, and calculates the percentage difference in throughput between ReMoE and MoE. A positive percentage indicates ReMoE is faster, and a negative percentage shows ReMoE is slower.  The table helps demonstrate the computational efficiency of ReMoE relative to a standard MoE model, showing that ReMoE achieves comparable performance without significant speed penalties.", "section": "4.2 Comparison with Other Routing Methods"}, {"content": "| Model | Stage I | Stage II | Stage III | Total |\n|---|---|---|---|---|\n| MoE | 0.12 | 0.41 | 119.12 | 119.65 |\n| ReMoE | 0.32 | 0.91 | 119.25 | 120.48 |", "caption": "Table 8: Downstream results of scaling in active parameters N\ud835\udc41Nitalic_N.", "description": "This table presents the downstream task evaluation results for the scaling experiments with respect to the number of active parameters (N).  It compares the performance of different models (Dense, MoE, and ReMoE) across three different sizes of models (182M, 469M, and 978M parameters). The results show the zero-shot accuracy on various downstream tasks (ARC-c, ARC-e, BoolQ, HellaSwag, LAMBADA, PIQA, RACE) for each model and size. This allows for an assessment of how model performance scales with increasing model size and the relative performance of the different models.", "section": "4.3 SCALABILITY OF REMOE"}, {"content": "| # Parameters | TP | Model | Train TFLOPS | Train Diff. | Infer TFLOPS | Infer Diff. |\n|---|---|---|---|---|---|---|\n| 182M | 1 | MoE | 103.49 | \u21911.82% | 78.47 | \u21912.19% |\n|  |  | ReMoE | 105.38 |  | 80.19 |  |\n| 469M | 1 | MoE | 138.58 | \u21931.37% | 107.52 | \u21913.89% |\n|  |  | ReMoE | 136.69 |  | 111.71 |  |\n| 978M | 1 | MoE | 160.46 | \u21931.77% | 153.11 | \u21930.23% |\n|  |  | ReMoE | 157.61 |  | 152.76 |  |\n| 978M | 2 | MoE | 133.40 | \u21930.68% | 118.55 | \u21931.08% |\n|  |  | ReMoE | 132.49 |  | 117.27 |  |\n| 978M | 4 | MoE | 103.61 | \u21932.29% | 85.96 | \u21912.33% |\n|  |  | ReMoE | 101.23 |  | 87.96 |  |", "caption": "Table 9: Downstream results of scaling in expert count E\ud835\udc38Eitalic_E.", "description": "This table presents the downstream task performance results for scaling experiments with respect to the number of experts (E).  It shows how changing the expert count affects the overall model's effectiveness across various downstream tasks. The results are compared between the standard Mixture-of-Experts (MoE) and the proposed ReMoE models.  It allows for analyzing the scalability and performance trade-offs of increasing the number of experts in the MoE architecture.", "section": "4.3 SCALABILITY OF REMOE"}, {"content": "| Model | N | ARC-c | ARC-e | BoolQ | HellaSwag | LAMBADA | PIQA | RACE | Avg. |\n|---|---|---|---|---|---|---|---|---|---| \n| Dense | 182M | 19.45 | 43.35 | 54.40 | 28.61 | 31.09 | 61.97 | 28.52 | 38.20 |\n|  | 469M | 21.50 | 49.12 | 56.88 | 31.12 | 36.74 | 64.47 | 30.53 | 41.48 |\n|  | 978M | 21.93 | 50.88 | 60.24 | 32.42 | 41.06 | 67.46 | 31.77 | 43.68 |\n| MoE | 182M | 20.82 | 45.03 | 57.55 | 29.84 | 31.81 | 63.28 | 28.42 | 39.53 |\n|  | 469M | 23.63 | 52.40 | 53.94 | 32.43 | 43.64 | 68.34 | 31.48 | 43.69 |\n|  | 978M | 23.81 | 52.90 | 58.90 | 35.01 | 44.42 | 67.90 | 31.48 | 44.91 |\n| ReMoE | 182M | 20.22 | 46.68 | 54.16 | 30.26 | 35.94 | 63.55 | 29.38 | 40.03 |\n|  | 469M | 21.67 | 53.16 | 58.75 | 33.80 | 40.66 | 67.95 | 31.20 | 43.88 |\n|  | 978M | 24.06 | 55.26 | 57.28 | 35.93 | 44.42 | 68.99 | 30.43 | 45.20 |", "caption": "Table 10: Downstream results of scaling in granularity G\ud835\udc3aGitalic_G.", "description": "This table presents the results of experiments evaluating the impact of granularity (G) on model performance. Granularity refers to the level of detail or refinement in the model's expert modules.  The table compares the performance of different models (Dense, MoE, and ReMoE) across various granularities.  The results include downstream task accuracy (ARC-c, ARC-e, BoolQ, HellaSwag, LAMBADA, PIQA, RACE, and Avg.) for each model and granularity level. This allows assessing how the model's performance changes as the granularity (and thus the model's complexity and capacity) varies.", "section": "4.3 SCALABILITY OF REMOE"}, {"content": "| Model | E | ARC-c | ARC-e | BoolQ | HellaSwag | LAMBADA | PIQA | RACE | Avg. |\n|---|---|---|---|---|---|---|---|---|---| \n| Dense | - | 19.45 | 43.35 | 54.40 | 28.61 | 31.09 | 61.97 | 28.52 | 38.20 |\n| MoE |  |  |  |  |  |  |  |  |  |\n| 4 | 20.73 | 44.49 | 59.63 | 29.14 | 31.40 | 63.33 | 29.19 | 39.70 |\n| 8 | 20.82 | 45.03 | 57.55 | 29.84 | 31.81 | 63.28 | 28.42 | 39.53 |\n| 16 | 20.90 | 45.29 | 46.36 | 30.50 | 33.22 | 64.96 | 28.33 | 38.50 |\n| 32 | 19.54 | 47.35 | 52.29 | 31.12 | 35.63 | 64.25 | 28.23 | 39.77 |\n| 64 | 19.88 | 46.63 | 60.06 | 31.47 | 36.33 | 65.07 | 28.04 | 41.06 |\n| 128 | 20.99 | 47.69 | 56.73 | 32.00 | 36.62 | 65.67 | 28.04 | 41.10 |\n| ReMoE |  |  |  |  |  |  |  |  |  |\n| 4 | 19.88 | 46.46 | 57.43 | 29.64 | 33.57 | 62.95 | 27.66 | 39.66 |\n| 8 | 20.22 | 46.68 | 54.16 | 30.26 | 35.94 | 63.55 | 29.38 | 40.03 |\n| 16 | 20.90 | 49.28 | 53.36 | 30.85 | 37.09 | 65.83 | 30.05 | 41.05 |\n| 32 | 20.56 | 48.11 | 59.54 | 31.42 | 37.84 | 65.18 | 28.42 | 41.58 |\n| 64 | 20.82 | 50.51 | 57.80 | 32.17 | 36.74 | 65.78 | 27.46 | 41.61 |\n| 128 | 19.97 | 51.05 | 56.97 | 32.40 | 37.92 | 66.70 | 29.86 | 42.12 |", "caption": "Table 11: Downstream results of training with or without load balancing.", "description": "This table presents the downstream task evaluation results for models trained with and without load balancing.  It compares the performance of MoE and ReMoE models on various downstream tasks, highlighting the impact of load balancing on overall model accuracy. The results help to understand the contribution of load balancing to the performance differences between MoE and ReMoE.", "section": "E.4 Load Balancing Ablations"}, {"content": "| Model | G | ARC-c | ARC-e | BoolQ | HellaSwag | LAMBADA | PIQA | RACE | Avg. |\n|---|---|---|---|---|---|---|---|---|---| \n| Dense | - | 19.45 | 43.35 | 54.40 | 28.61 | 31.09 | 61.97 | 28.52 | 38.20 |\n| Dense \u00d7 8 | - | 22.78 | 48.11 | 59.66 | 31.11 | 35.65 | 65.02 | 29.57 | 41.70 |\n| MoE 1 | 1 | 20.82 | 45.03 | 57.55 | 29.84 | 31.81 | 63.28 | 28.42 | 39.53 |\n| MoE 2 | 2 | 21.42 | 46.55 | 54.25 | 29.95 | 32.52 | 64.09 | 28.61 | 39.62 |\n| MoE 4 | 4 | 20.99 | 46.09 | 55.90 | 30.52 | 35.16 | 63.98 | 29.28 | 40.27 |\n| MoE 8 | 8 | 21.59 | 47.73 | 60.70 | 30.83 | 36.41 | 64.69 | 28.04 | 41.42 |\n| MoE 16 | 16 | 19.80 | 48.82 | 57.34 | 30.64 | 36.00 | 64.74 | 28.71 | 40.86 |\n| MoE 32 | 32 | 21.67 | 48.78 | 57.85 | 31.27 | 37.10 | 64.69 | 28.52 | 41.41 |\n| MoE 64 | 64 | 20.14 | 48.74 | 61.50 | 31.03 | 36.31 | 63.93 | 27.85 | 41.35 |\n| ReMoE 1 | 1 | 20.22 | 46.68 | 54.16 | 30.26 | 35.94 | 63.55 | 29.38 | 40.03 |\n| ReMoE 2 | 2 | 20.14 | 47.39 | 57.95 | 30.60 | 34.52 | 63.71 | 28.52 | 40.40 |\n| ReMoE 4 | 4 | 20.39 | 47.94 | 55.35 | 31.04 | 36.11 | 64.64 | 29.00 | 40.64 |\n| ReMoE 8 | 8 | 20.82 | 48.36 | 60.49 | 30.90 | 36.06 | 63.87 | 28.90 | 41.34 |\n| ReMoE 16 | 16 | 21.25 | 49.41 | 56.06 | 30.91 | 36.23 | 64.91 | 29.95 | 41.25 |\n| ReMoE 32 | 32 | 20.90 | 48.86 | 55.81 | 31.14 | 36.58 | 64.69 | 30.05 | 41.15 |\n| ReMoE 64 | 64 | 20.65 | 48.74 | 60.06 | 31.56 | 36.43 | 65.40 | 29.00 | 41.69 |", "caption": "Table 12: Routed tokens with high probability for experts in Layer 5 of ReMoE", "description": "This table shows tokens with high routing probability to each expert in layer 5 of the ReMoE model.  High probability indicates that a given token is frequently routed to a particular expert, suggesting a degree of specialization. The table demonstrates that different experts in ReMoE exhibit a preference for specific types of tokens. For example, some experts show a strong preference for words commonly found in natural language texts (like 'husband', 'wife', 'baby'), while others favor technical terms or code-related words (like 'variable', 'env', 'HEAD'). This specialization highlights how ReMoE learns to dynamically allocate tokens to experts based on their characteristics.", "section": "5.3 Domain Specialization in ReMoE"}, {"content": "| Model | LB | ARC-c | ARC-e | BoolQ | HellaSwag | LAMBADA | PIQA | RACE | Avg. |\n|---|---|---|---|---|---|---|---|---|---| \n| Dense | - | 19.45 | 43.35 | 54.40 | 28.61 | 31.09 | 61.97 | 28.52 | 38.20 |\n| MoE | \u00d7 | 19.20 | 44.74 | 50.80 | 28.60 | 30.18 | 62.24 | 27.94 | 37.67 |\n| MoE | \u2713 | 20.05 | 45.16 | **57.83** | 29.83 | 32.97 | **63.55** | 28.33 | 39.67 |\n| ReMoE | \u00d7 | 19.45 | 46.34 | 56.94 | 30.19 | 31.79 | 63.33 | 28.61 | 39.52 |\n| ReMoE | \u2713 | **20.22** | **46.68** | 54.16 | **30.26** | **35.94** | **63.55** | **29.38** | **40.03** |", "caption": "Table 13: Performance of MoE with near-dense warmup", "description": "This table compares the performance of three different models: the standard Mixture-of-Experts (MoE) model, an MoE model with a near-dense warmup phase added, and the ReMoE model.  The near-dense warmup phase for MoE involves initially training with more experts active than usual to improve initial model parameters before transitioning to a sparser training configuration.  The table shows the validation loss and downstream task accuracy (across several tasks) for each model to demonstrate the impact of this near-dense warmup strategy on overall performance, particularly in comparison to ReMoE which naturally employs this type of training.", "section": "4 Experiments"}, {"content": "| Expert ID | Routed Tokens With High Probability |\n|---|---| \n| 0 | `End`(100%); `folding`(100%); `Fill`(100%); `FILE`(100%); `NULL`(100%); `byte`(100%); `Release`(99.36%); `Del`(99.80%) |\n| 1 | `husband`(100%); `ife`(100%); `baby`(100%); `human`(100%); `lover`(99.60%); `).`(99.86%); `),`(99.71%); `)...`(98.425%) |\n| 2 | `invest`(100%); `Fortune`(100%); `exec` (100%); `0000`(100%); `Sorry`(100%); `bye`(97.82%); `If`(97.74%); `\u00ae`(97.63%) |\n| 3 | `Conversely`(100%); `Methods`(100%); `flower`(100%); `Blossom`(99.93%); `Argentina`(100%); `Georgian`(100%); `Uruguay`(98.90%); `African` (100%) |\n| 4 | `Spring`(100%); `Summer`(100%) `Autumn`(100%); `Winter`(100%); `seasons`(99.02%); `Temperature` (100%); `hot`(97.98%); `cold`(100%) |\n| 5 | `\u00e8`(100%); `\u00e6`(99.80%); `\u00e5`(98.59%); `\u00c6`(97.67%) |\n| 6 | `]);`(100%); `gif`(100%); `size`(100%); `variable`(100%); `env`(100%); `begin`(97.95%); `HEAD`(97.94%); `|`(97.83%) |\n| 7 | `Kuala`(100%); `Tus`(100%); `Lama`(100%); `Riley`(98.94%) |", "caption": "Table 14: Results for MoE with warmup under different expert count E\ud835\udc38Eitalic_E", "description": "This table presents the results of experiments comparing the performance of the Mixture-of-Experts (MoE) model with a near-dense warmup phase to the standard MoE and ReMoE models.  The near-dense warmup involves training MoE with a larger number of active experts initially (k=6), before transitioning to k=1 for the rest of training. The table shows the validation loss and average accuracy for MoE with and without the warmup phase across different numbers of experts (E=8, E=32, and E=128), highlighting how the warmup strategy impacts performance, particularly as the number of experts increases. ReMoE serves as the baseline for comparison, illustrating its relative performance.", "section": "4.3 Scalability of ReMoE"}, {"content": "| Model | Valid Loss | ARC-c | ARC-e | BoolQ | Hella-Swag | LAM-BADA | PIQA | RACE | Avg. |\n|---|---|---|---|---|---|---|---|---|---|\n| MoE | 1.936 | 20.82 | 45.03 | 57.55 | 29.84 | 31.81 | 63.28 | 28.42 | 39.53 |\n| MoE with warmup | 1.928 | 20.73 | 46.38 | 52.35 | 30.28 | 33.90 | 63.76 | 27.66 | 39.29 |\n| ReMoE | 1.921 | 20.22 | 46.68 | 54.16 | 30.26 | 35.94 | 63.55 | 29.38 | 40.03 |", "caption": "Table 15: Final validation loss of MoE with different warmup steps", "description": "This table presents the final validation loss achieved by the MoE model (Mixture of Experts) after training with varying numbers of warmup steps.  The warmup phase involves initially training the model with a higher number of active experts, gradually transitioning to a sparser configuration. The results show the impact of different warmup lengths on the model's final performance, highlighting the trade-off between initialization and overall convergence.", "section": "H. Training MoE with Near-Dense Warmup"}]