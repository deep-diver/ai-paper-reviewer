{"references": [{"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-01-01", "reason": "This paper introduces the Mixture of Experts (MoE) architecture, which is the foundation upon which this paper builds and improves."}, {"fullname_first_author": "William Fedus", "paper_title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "publication_date": "2022-MM-DD", "reason": "This paper proposes a significant improvement to the MoE architecture, addressing previous limitations and inspiring further advancements in the field."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "LLaMA: Open and Efficient Foundation Language Models", "publication_date": "2023-02-13", "reason": "This paper provides the foundational large language model (LLaMA) architecture used in the experiments, showcasing its performance and influencing the choice of baseline model."}, {"fullname_first_author": "Joshua Ainslie", "paper_title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints", "publication_date": "2023-05-01", "reason": "This paper introduces the grouped-query attention mechanism used within the LLaMA architecture, improving efficiency and influencing the model architecture used in experiments."}, {"fullname_first_author": "Jakub Krajewski", "paper_title": "Scaling laws for fine-grained mixture of experts", "publication_date": "2024-02-01", "reason": "This paper investigates scaling laws for fine-grained MoE, a crucial area of research relevant to the proposed ReMoE architecture and contributing to the broader study of MoE scalability."}]}