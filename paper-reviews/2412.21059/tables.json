[{"content": "| Dimension | #Sub-dimension |  | #Checklist |  |\n|---|---|---|---|---|\n| **Dimension** | **#Sub-dimension** |  | **#Checklist** |  |\n|  | Image | Video | Image | Video |\n| Alignment | 1 | 1 | 1 | 4 |\n| Composition | 5 | 1 | 13 | 2 |\n| Quality | 5 | 4 | 14 | 14 |\n| Fidelity | 5 | 3 | 25 | 9 |\n| Safety&Emotion | 2 | 1 | 8 | 4 |\n| Stability | - | 5 | - | 12 |\n| Dynamic | - | 2 | - | 8 |\n| Physics | - | 1 | - | 4 |\n| Preservation | - | 2 | - | 7 |\n| **Total** | **18** | **20** | **61** | **64** |", "caption": "Table 1: Taxonomy of annotation for VisionReward.", "description": "This table presents the taxonomy of annotations used in the VisionReward model. It breaks down human preferences in image and video generation into multiple dimensions, each further categorized into sub-dimensions.  For each sub-dimension, there's a corresponding number of checklist questions, which are multiple-choice questions designed to elicit fine-grained human judgment. The table shows that image generation is broken down into 5 dimensions and 18 sub-dimensions and video generation is broken down into 9 dimensions and 64 sub-dimensions. This detailed annotation framework allows for a more nuanced and accurate assessment of visual generation outputs.", "section": "2. Method"}, {"content": "| Type | Source | #Samples | #Checklist |\n|---|---|---|---| \n| Image | ImageRewardDB [46] | 16K | 1M |\n|  | Pick-a-Pic [20] | 16K | 1M |\n|  | HPDv2 [44] | 16K | 1M |\n| Video | CogVideoX [47] | 10K | 0.6M |\n|  | Open-Sora [51] | 10K | 0.6M |\n|  | VideoCrafter2 [4] | 10K | 0.6M |\n|  | Panda-70M [5] | 3K | 0.2M |", "caption": "Table 2: Statistics of source data and annotation.", "description": "This table presents the details of the datasets used for training and annotating the VisionReward model.  It shows the source of the data (e.g., ImageRewardDB, Pick-a-Pic), the number of samples (images or videos) obtained from each source, and the number of checklist items used for annotation in each dataset. This information is crucial for understanding the scale and scope of the VisionReward model's training data.", "section": "2. Method"}, {"content": "| Type | Image | Video |\n|---|---|---|\n| **Content** | People, Objects, Animals, Architecture, Landscape, Vehicles, Plants, Food, Others, Scenes | Story, Human Activity, Artificial Scene, Others, Natural Animal Activity, Physical Phenomena |\n| **Challenge** | Unreal, Style, History, Fine-grained Detail, Color, Famous Character, Normal, Famous Places, Writing, Complex Combo, Positional, Counting | Material, Angle and Lens, Emotional Expression, Color/Tone, Surreal, World Knowledge, Special Effects, Text, Spatial Relationship, Camera Movement, Logical Consistency, Style, Temporal Speed |", "caption": "Table 3: Content and Challenge Categories of MonetBench.", "description": "This table details the content and challenge categories used in the MonetBench benchmark dataset for evaluating image and video generation models.  The \"Content\" categories represent the main subject matter of the generated images or videos (e.g., people, objects, animals, scenes), while the \"Challenge\" categories describe the level of difficulty or complexity in generating them (e.g., unreal styles, fine-grained details, complex compositions).  Understanding these categories helps to assess the performance of different models under various conditions and to evaluate their ability to generate visually appealing and diverse content.", "section": "3. Experiments"}, {"content": "| Method | Image |  |  | Video |  |  |  |\n|---|---|---|---|---|---|---|---| \n| **Method** | **Image** |  |  | **Video** |  |  |  |\n| HPDv2 [44] | MonetBench | tau* | diff** | GenAI-Bench [18] | MonetBench | tau | diff |\n| task-specific discriminative models |  |  |  |  |  |  |  |\n| ImageReward [46] | 74.0 | 48.8 | 56.5 | 48.4 | 72.1 | 55.8 | 58.4 |\n| PickScore [20] | 79.8 | 49.8 | 57.6 | 52.4 | 75.4 | 57.7 | 61.6 |\n| HPSv2 [44] | 83.3 | 48.4 | 55.6 | 49.3 | 73.0 | 59.3 | 62.5 |\n| **generative models** |  |  |  |  |  |  |  |\n| GPT-4o [1] | 77.5 | 38.9 | 52.7 | 41.8 | 54.3 | 45.7 | 48.3 |\n| Gemini [38] | 60.7 | 27.4 | 55.1 | 46.9 | 61.7 | 52.2 | 56.8 |\n| VQAScore [23] | 69.7 | 49.4 | 56.5 | 45.2 | 68.0 | 56.1 | 59.5 |\n| VideoScore [11] | 76.8 | 45.8 | 52.5 | 47.8 | 71.4 | 49.1 | 54.9 |\n| VisionReward (Ours) | **81.7** | **51.8** | **59.5** | **51.8** | **74.4** | **64.0** | **72.1** |", "caption": "Table 4: Preference accuracy on multiple dataset. Bold denotes the best score within the generative models, while underline signifies the best score among all categories. Tau\u2217 means taking account of ties\u00a0[7], and diff\u2217\u2217 means dropping ties in labels (we drop ties both in labels and responses for GPT-4o and Gemini in diff\u2217\u2217 because too many ties are given by them).", "description": "This table presents the performance of various models, including both task-specific discriminative models and generative models, on multiple datasets for predicting human preferences in image and video generation.  The accuracy is measured using two metrics: Tau* which accounts for ties in the preference rankings, and diff**, which excludes ties.  The best performing generative model for each metric and dataset is shown in bold. The overall best performing model across all categories and metrics is underlined.", "section": "3. Experiments"}, {"content": "| Method | Image Composition | Image Quality | Image Fidelity | Image Safety&Emotion | Video Stability | Video Dynamic | Video Physics | Video Preservation |\n|---|---|---|---|---|---|---|---|---|\n| LLaVa* | 59.9 | 65.7 | **80.9** | 64.4 | 52.5 | 53.8 | 50.6 | 47.5 |\n| CogVLM2 [16] | 65.8 | 67.1 | 53.1 | 74.7 | 49.3 | 57.1 | 51.2 | 47.8 |\n| GPT-4o [1] | 73.1 | 62.7 | 61.9 | 70.1 | 57.9 | 69.1 | 62.4 | 58.8 |\n| Gemini [38] | 69.4 | 59.9 | 59.7 | 74.9 | 58.1 | 71.1 | 58.1 | 59.6 |\n| VisionReward (Ours) | **78.8** | **81.1** | **80.9** | **83.9** | **64.8** | **75.4** | **68.1** | **72.0** |", "caption": "Table 5: Accuracy of VisionReward and other vision-language models (VLMs) on vision quality questions constructed from our annotation. \u2217We test LLaVA-v1.5-7B\u00a0[24] for image and LLava-Next-Video-34B\u00a0[21] for video.", "description": "This table presents a comparison of the accuracy of VisionReward and other vision-language models (VLMs) in answering vision quality assessment questions. The questions were designed based on the annotation framework presented in the paper.  The accuracy is evaluated across various dimensions of image and video quality (Composition, Quality, Fidelity, Safety&Emotion, Stability, Dynamic, Physics, Preservation). Note that LLaVA-v1.5-7B is used for image evaluation and LLaVA-Next-Video-34B is used for video evaluation.", "section": "3. Experiments"}, {"content": "| Composition | Quality | Fidelity | Safety |\n|---|---|---|---| \n| 97.9 | 98.2 | 98.3 | 99.1 |\n| Stability | Dynamic | Physics | Preservation |\n| 97.4 | 99.9 | 88.2 | 99.8 |", "caption": "Table 6: Consistency of VisionReward in each dimension.", "description": "This table presents the internal consistency of the VisionReward model across its different dimensions.  Each dimension represents a different aspect of image or video quality (e.g., Composition, Quality, Fidelity, Safety&Emotion). The values show the percentage of times that the model's assessment within each sub-dimension agreed with human judgments. High consistency (near 100%) suggests that the model is reliable and stable in evaluating these specific aspects.  Low consistency indicates areas where the model may need further improvement.", "section": "2. Method"}, {"content": "| Size | 100 | 200 | 500 | 1k |\n|---|---|---|---|---|\n| **Accuracy** | 76.5 | 77.6 | 80.3 | 80.6 |\n| **Size** | 2k | 4k | 8k | 16k |\n| **Accuracy** | 80.9 | 81.3 | 81.2 | 81.3 |", "caption": "Table 7: Average accuracy for different regression sizes.", "description": "This table presents the average accuracy achieved by the logistic regression model for different training set sizes.  It shows how the model's performance changes as more training data is used, demonstrating the impact of dataset size on the accuracy of human preference prediction in the regression task.", "section": "2.2. VisionReward Training"}, {"content": "| Methods | CLIP | Aes | HPSv2 | PickScore |\n|---|---|---|---|---|\n| Baseline | 0.273 | 5.463 | 0.282 | 22.25 |\n| DPO with Pick-a-Pic | **0.279** | 5.511 | 0.286 | 22.45 |\n| DPO with HPSv2 | 0.277 | 5.599 | **0.292** | 22.58 |\n| MPO (Ours) | **0.279** | **5.612** | 0.289 | **22.61** |", "caption": "Table 8: Evaluation results of multiple metrics on DrawBench.", "description": "This table presents a comprehensive evaluation of different methods for text-to-image generation on the DrawBench benchmark.  It compares baseline performance against methods using DPO (Diffusion Preference Optimization) with either Pick-a-Pic or HPSv2 reward models, and finally the proposed MPO (Multi-Objective Preference Optimization) method with the VisionReward model.  Multiple metrics are reported, assessing various aspects of the generated images such as composition, quality, fidelity, safety and emotion.", "section": "3.2. MPO for Text-to-Image Optimization"}, {"content": "| Methods | Composition | Quality | Fidelity | Safety&Emotion |\n|---|---|---|---|---|\n| Baseline | 0.755 | 0.550 | 0.009 | -0.008 |\n| DPO with Pick-a-Pic | 0.765 | 0.588 | 0.009 | -0.009 |\n| DPO with HPSv2 | 0.874 | 0.630 | 0.010 | -0.004 |\n| MPO (Ours) | **0.894** | **0.670** | **0.017** | **-0.001** |", "caption": "Table 9: Evaluation results analyzed by VisionReward.", "description": "This table presents a detailed breakdown of the evaluation results obtained using VisionReward.  It compares multiple metrics (Composition, Quality, Fidelity, Safety&Emotion) across different methods: a baseline, DPO with Pick-a-Pic, DPO with HPSv2, and the proposed MPO (Ours). The numbers represent quantitative scores for each metric under each method. This allows for a comprehensive comparison of performance across various approaches to optimizing visual generation models.", "section": "3.2. MPO for Text-to-Image Optimization"}, {"content": "| Methods | CLIP | Aes | HPSv2 | PickScore |\n|---|---|---|---|---|\n| Baseline | 0.273 | 5.463 | 0.282 | 22.25 |\n| DPO with VisionReward | **0.278** | 5.664 | **0.291** | 22.227 |\n| MPO with VisionReward | **0.278** | **5.719** | **0.291** | **22.505** |", "caption": "Table 10: Evaluation results on DrawBench.", "description": "This table presents a comparison of the performance of different methods on the DrawBench benchmark.  The methods include a baseline, DPO with VisionReward, and MPO with VisionReward.  The results are evaluated using multiple metrics, such as CLIP, AES, HPSv2, and PickScore.  The table shows the numerical results for each metric, allowing for a direct comparison of the effectiveness of the various approaches.", "section": "3.2. MPO for Text-to-Image Optimization"}, {"content": "| Methods | Human | Multiple |  |  |\n|---|---|---|---|---|\n| Action | Scene | Appear. |  |  |\n| Objects |  |  |  |  |\n| Style |  |  |  |  |\n| Baseline | 98.20 | 55.60 | 68.43 | **24.20** |\n| VideoScore | 97.60 | 56.25 | 68.66 | 23.96 |\n| VisionReward | **98.40** | **57.57** | **71.54** | 24.02 |", "caption": "Table 11: Evaluation results on VBench.", "description": "This table presents the quantitative evaluation results of different methods on the VBench benchmark.  VBench is a video quality assessment benchmark that evaluates several key aspects of video generation. The table shows the performance scores of three different methods: Baseline (original model), VideoScore (a model for video quality prediction), and VisionReward (the authors' proposed method).  The performance is measured across multiple aspects of video quality, including aspects like human action, scene, objects, and appearance style. This allows for a comparison of the different methods' effectiveness in generating high-quality videos across these various dimensions.", "section": "3. Experiments"}, {"content": "| Methods | Stability | Dynamic | Physics | Preservation |\n|---|---|---|---|---|\n| Baseline | 0.272 | **0.047** | 0.323 | 0.584 |\n| VideoScore | 0.242 | 0.046 | 0.319 | 0.557 |\n| VisionReward | **0.309** | 0.036 | **0.337** | **0.661** |", "caption": "Table 12: Evaluation results on MonetBench.", "description": "This table presents the performance comparison of different methods on the MonetBench dataset, focusing on the preference prediction accuracy for both image and video generation.  The methods include several baselines (task-specific discriminative and generative models) as well as the proposed VisionReward.  The results are reported using multiple metrics to provide a comprehensive evaluation, highlighting the strengths and weaknesses of each approach in different facets of visual generation.", "section": "3. Experiments"}, {"content": "| Methods | Baseline | Total | Dimension | Sub-dimension |\n|---|---|---|---|---|\n| VisionReward | 4.303 | 4.515 | **4.573** | 4.514 |", "caption": "Table 13: Score of VisionReward after different strategies of MPO. Total: \u201cdominate\u201d based on total weighted score. Dimension: \u201cdominate\u201d based on score of each dimension. Sub-dimension: \u201cdominate\u201d based on score of each sub-dimension.", "description": "This table presents a comparison of VisionReward's performance after applying the Multi-Objective Preference Optimization (MPO) algorithm using three different dominance criteria.  The three criteria are:  (1) Total Weighted Score: where one image's reward is considered dominant if its total score is higher than another's; (2) Dimension Score: where one image's reward is considered dominant if its score is higher than another's on *all* individual dimensions; and (3) Sub-dimension Score: where one image's reward is considered dominant if its score is higher than another's on *all* individual sub-dimensions. The table shows the resulting VisionReward scores for each dominance strategy, allowing for analysis of which strategy yields the best performance.", "section": "3. Experiments"}, {"content": "| SYSTEM | Assume you are a model responsible for refining and polishing English expressions. You will receive an English prompt that may contain abbreviations or non-standard expressions. Your task is to standardize the expressions, and your output must be in pure English without any non-English characters. If the prompt is fragmented or difficult to understand, discard it by outputting \u201dF\u201d. Your output must strictly follow the format: each sentence should be on a single line, either as the rewritten prompt or a standalone \u201dF\u201d. |\n|---|---| \n| USER | Here is the prompt you have received: [[PROMPT]] |\n|---|---| \n| INPUT | Soft rays of light through the many different types of trees inside a forest, sunrise, misty, photorealistic, ground level, -neg &amp;quot;no large bodies of water&amp;quot; -ar 16:9 4K, -ar 16:9 |\n|---|---| \n| OUTPUT | The soft rays of light filter through the myriad types of trees within the forest at sunrise, creating a misty, photorealistic scene from ground level. Exclude any large bodies of water. The aspect ratio should be 16:9 in 4K resolution. Aspect ratio: 16:9. |", "caption": "Table 14: Prompt template and example for prompt cleaning.", "description": "This table demonstrates the prompt template and an example of how prompts are cleaned for the video annotation process.  The prompt template shows the structure and formatting required for input prompts to ensure the quality and consistency of annotation data for training the VisionReward model.  The example highlights how a potentially ambiguous or informal prompt is transformed into a clearer and more structured one that is easier for annotators to understand and use.", "section": "2. Method"}, {"content": "| Dimension | Sub-dimension | Option | Checklist |\n|---|---|---|---| \n| Composition | Symmetry | symmetrical | Is the image symmetrical? |\n|  |  | ordinary | Does the image avoid asymmetry? |\n|  |  | asymmetrical |  |\n| Composition | Object pairing | coordinated | Are the objects well-coordinated? |\n|  |  | ordinary | Does the image avoid poorly coordinated objects? |\n|  |  | uncoordinated |  |\n| Composition | Main object | prominent | Is the main subject prominent? |\n|  |  | ordinary | Does the image avoid an unclear main subject? |\n|  |  | prominent |  |\n| Composition | Richness | very rich | Is the image very rich? |\n|  |  | rich | Is the image rich? |\n|  |  | ordinary | Is the image not monotonous? |\n|  |  | monotonous | Is the image not empty? |\n|  |  | empty |  |\n| Composition | Background | beautiful | Is the background beautiful? |\n|  |  | somewhat beautiful | Is the background somewhat beautiful? |\n|  |  | ordinary | Is there a background? |\n|  |  | no background |  |\n| Quality | Clarity | very clear | Is the image very clear? |\n|  |  | clear | Is the image clear? |\n|  |  | ordinary | Does the image avoid being blurry? |\n|  |  | blurry | Does the image avoid being completely blurry? |\n|  |  | completely blurry |  |\n| Quality | Color Brightness | bright | Are the colors bright? |\n|  |  | ordinary | Are the colors not dark? |\n|  |  | dark |  |\n| Quality | Color Aesthetic | beautiful colors | Are the colors beautiful? |\n|  |  | ordinary colors | Are the colors not ugly? |\n|  |  | ugly colors |  |\n| Quality | Lighting Distinction | very distinct | Is the lighting and shadow very distinct? |\n|  |  | distinct | Is the lighting and shadow distinct? |\n|  |  | ordinary | Is there lighting and shadow? |\n|  |  | no lighting |  |\n| Quality | Lighting Aesthetic | very beautiful | Are the lighting and shadows very beautiful? |\n|  |  | beautiful | Are the lighting and shadows beautiful? |\n|  |  | ordinary | Is there lighting and shadow? |\n|  |  | no lighting |  |", "caption": "Table 15: Annotation taxonomy and checklist details for text-to-image evaluation. (part 1)", "description": "This table details the annotation taxonomy and checklist used for evaluating image generation quality.  It breaks down the evaluation criteria into dimensions (Alignment, Composition, Quality, Fidelity, Safety & Emotion) and sub-dimensions, providing a checklist of binary (yes/no) questions for annotators to assess each image against. The questions are designed to capture fine-grained aspects of image quality related to the specified dimensions. For example, the 'Composition' dimension includes sub-dimensions like 'Symmetry' and 'Object pairing', with accompanying checklist questions evaluating whether the image exhibits symmetry or whether objects are well-coordinated.", "section": "2. Method"}, {"content": "| Dimension | Sub-dimension | Option | Checklist |\n|---|---|---|---| \n| Fidelity | Detail realism | realistic | Are the image details realistic? |\n|  |  | neutral | Do the image details avoid being unrealistic? |\n|  |  | unrealistic | Do the image details avoid being very unrealistic? |\n|  |  | very unrealistic | Do the image details avoid being greatly unrealistic? |\n|  |  | greatly unrealistic |  |\n| Fidelity | Detail refinement | very refined | Are the image details very exquisite? |\n|  |  | refined | Are the image details exquisite? |\n|  |  | ordinary | Do the image details avoid being coarse? |\n|  |  | rough | Do the image details avoid being very coarse? |\n|  |  | very rough | Does the image avoid being hard to recognize? |\n|  |  | indistinguishable | Does the image avoid being fragmented? |\n|  |  | fragmented |  |\n| Fidelity | Body | no errors | Is the human body in the image completely correct? |\n|  |  | neutral | Does the human body in the image avoid errors? |\n|  |  | some errors | Does the human body in the image avoid obvious errors? |\n|  |  | obvious errors | Does the human body in the image avoid serious errors? |\n|  |  | serious errors | Is there a human body in the image? |\n|  |  | no human figure |  |\n| Fidelity | Face | very beautiful | Is the human face very beautiful? |\n|  |  | beautiful | Is the human face beautiful? |\n|  |  | normal | Does the human face avoid errors? |\n|  |  | some errors | Does the human face avoid serious errors? |\n|  |  | serious errors | Is there a human face in the image? |\n|  |  | no human face |  |\n| Fidelity | Hands | perfect | Are the human hands perfect? |\n|  |  | mostly correct | Are the human hands essentially correct? |\n|  |  | minor errors | Do the human hands avoid obvious errors? |\n|  |  | obvious errors | Do the human hands avoid serious errors? |\n|  |  | serious errors | Are there human hands in the image? |\n|  |  | no human hands |  |\n| Safety & Emotion | Emotion | very positive | Can the image evoke a very positive emotional response? |\n|  |  | positive | Can the image evoke a positive emotional response? |\n|  |  | ordinary | Does the image avoid evoking a negative emotional response? |\n|  |  | negative | Does the image avoid evoking a very negative emotional response? |\n|  |  | very negative |  |\n| Safety & Emotion | Safety | safe | Is the image completely safe? |\n|  |  | neutral | Is the image harmless? |\n|  |  | potentially harmful | Does the image avoid obvious harmfulness? |\n|  |  | harmful | Does the image avoid serious harmfulness? |\n|  |  | very harmful |  |", "caption": "Table 16: Annotation taxonomy and checklist details for text-to-image evaluation. (part 2)", "description": "This table details the annotation taxonomy and checklist used for evaluating the fidelity and safety/emotional aspects of text-to-image generation.  For each dimension (Fidelity, Safety & Emotion), several sub-dimensions are listed, each with options ranging from best to worst quality.  Corresponding checklist questions help annotators assess each option (yes/no).  This provides a fine-grained approach to human preference evaluation.", "section": "2.1. Annotation"}, {"content": "| Dimension | Sub-dimension | Option | Checklist |\n|---|---|---|---| \n| Alignment | Alignment | meet 100% | Does the video meet all the requirements stated in the text \u201d[[prompt]]\u201d? |\n|  |  | meet 80%-100% | Does the video meet most of the requirements stated in the text \u201d[[prompt]]\u201d? |\n|  |  | meet 60%-80% | Does the video meet some of the requirements stated in the text \u201d[[prompt]]\u201d? |\n|  |  | meet 40%-60% | Does the video not completely fail to meet the requirements stated in the text \u201d[[prompt]]\u201d? |\n|  |  | meet 0-40% |  |\n| Composition | Composition | good | Is the composition aesthetically pleasing? |\n|  |  | normal | Does the composition have no obvious flaws? |\n|  |  | bad |  |\n| Quality | Color | very beautiful | Are the colors exceptionally beautiful? |\n|  |  | beautiful | Are the colors beautiful? |\n|  |  | normal | Are the colors not unattractive? |\n|  |  | unattractive | Are the colors not significantly unattractive? |\n|  |  | very unattractive |  |\n| Quality | Lighting Accurate | good | Is the lighting perfectly accurate? |\n|  |  | normal | Does the lighting have no obvious errors? |\n|  |  | bad | Is there any lighting present? |\n|  |  | no lighting |  |\n| Quality | Lighting Aes | very good | Is the lighting exceptionally beautiful? |\n|  |  | good | Is the lighting beautiful? |\n|  |  | normal | Is the lighting not unattractive? |\n|  |  | bad |  |\n| Quality | Clear | very clear | Is it very clear? |\n|  |  | clear | Is it clear? |\n|  |  | normal | Is it not blurry? |\n|  |  | blurry | Is it not completely blurry? |\n|  |  | completely blurry |  |\n| Fidelity | Detail Refinement | very refined | Are the details very refined? |\n|  |  | refined | Are the details refined? |\n|  |  | normal | Are the details not rough? |\n|  |  | rough | Are the details not significantly rough? |\n|  |  | very rough |  |\n| Fidelity | Movement Reality | good | Is the object\u2019s movement completely realistic? |\n|  |  | normal | Does the object\u2019s movement have no obvious realism issues? |\n|  |  | bad |  |\n| Fidelity | Letters | good | Are all the letters correct? |\n|  |  | normal | Do the letters have no obvious errors? |\n|  |  | bad | Are there any letters present? |\n|  |  | no letter |  |\n| Safety | Safety | 100% safe | Is the video content safe? |\n|  |  | 80%-100% safe | Is the video content definitely free of harmful material? |\n|  |  | 60%-80% safe | Does the video content contain no harmful material? |\n|  |  | 40%-60% safe | Does the video content contain no extremely harmful material? |\n|  |  | 0-40% safe |  |", "caption": "Table 17: Annotation taxonomy and checklist details for text-to-video evaluation. (part 1)", "description": "This table details the annotation taxonomy and checklist used for evaluating text-to-video generation.  It breaks down the evaluation criteria into dimensions (e.g., Alignment, Composition, Quality, Fidelity, Safety & Emotion), sub-dimensions (e.g., Alignment, Composition, Color, Lighting Accuracy), options (e.g., meet 100%, meet 80%-100%, good, normal, bad), and the corresponding checklist questions used by annotators to assess the generated videos. This structured approach allows for a fine-grained and comprehensive evaluation of various aspects of the generated videos.", "section": "2.1 Annotation"}, {"content": "| Dimension | Sub-dimension | Option | Checklist |\n|---|---|---|---| \n| Stability | Movement smoothness | good | Is the smoothness of the object\u2019s movement good? |\n|  |  | normal | Does the smoothness of the object\u2019s movement have no obvious issues? |\n|  |  | bad |  |\n| Stability | Image quality stability | very stable | Is the image quality very stable? |\n|  |  | stable | Is the image quality stable? |\n|  |  | normal | Is the image quality not unstable? |\n|  |  | unstable | Is the image quality free of noticeable instability? |\n|  |  | very unstable |  |\n| Stability | Focus | good | Is the focus aesthetically pleasing? |\n|  |  | normal | Does the focus have no obvious flaws? |\n|  |  | bad |  |\n| Stability | Camera movement | good | Is the camera movement aesthetically pleasing? |\n|  |  | normal | Does the camera movement have no obvious flaws? |\n|  |  | bad |  |\n| Stability | Camera stability | stable | Is the camera stable? |\n|  |  | normal | Is the camera not unstable? |\n|  |  | unstable |  |\n| Preservation | Shape at beginning | completely accurate | Is the shape of the object at the beginning of the video completely accurate? |\n|  |  | no errors | Does the shape of the object at the beginning have no obvious errors? |\n|  |  | not chaotic | Is the shape of the object at the beginning not chaotic? |\n|  |  | flawed |  |\n| Preservation | Shape throughout | perfectly maintained | Is the shape of the object perfectly maintained throughout the video? |\n|  |  | no issues | Does the shape of the object have no obvious issues throughout the video? |\n|  |  | normal | Does the shape of the object generally have no major issues throughout the video? |\n|  |  | not chaotic | Is the shape of the object not chaotic throughout the video? |\n|  |  | flawed |  |\n| Dynamic | Object Motion dynamic | highly dynamic | Is the object\u2019s motion highly dynamic? |\n|  |  | dynamic | Is the object\u2019s motion dynamic? |\n|  |  | normal | Is the object\u2019s motion not minimal? |\n|  |  | not static | Is the object\u2019s motion not static? |\n|  |  | static |  |\n| Dynamic | Camera motion dynamic | highly dynamic | Is the camera motion highly dynamic? |\n|  |  | dynamic | Is the camera motion dynamic? |\n|  |  | not minimal | Is the camera motion not minimal? |\n|  |  | not static | Is the camera motion not static? |\n|  |  | static |  |\n| Physics | Physics law | full compliance | Does it fully comply with the laws of physics? |\n|  |  | partial compliance | Does it partially comply with the laws of physics? |\n|  |  | no obvious violations | Does it have no obvious violations of the laws of physics? |\n|  |  | physical world | Is the video content part of the physical world? |\n|  |  | non-compliance |  |", "caption": "Table 18: Annotation taxonomy and checklist details for text-to-video evaluation. (part 2)", "description": "This table details the annotation taxonomy and checklist used for evaluating the quality of videos generated from text prompts.  It breaks down video quality into multiple dimensions (Stability, Preservation, Dynamic, Physics, Fidelity), each with several sub-dimensions.  For each sub-dimension, several options are provided, ranging from very positive (e.g., \"perfectly maintained\") to very negative (e.g., \"very unstable\"). Corresponding checklist questions facilitate the annotation process by enabling annotators to evaluate each sub-dimension against these options.", "section": "2. Method"}, {"content": "| ID | Checklist | Acc | \u03c1 | Weight |\n|---|---|---|---|---|\n| 1 | Is there a human body in the image? | 93.13 | 0.090 | mask |\n| 2 | Is there a human face in the image? | 96.20 | 0.110 | mask |\n| 3 | Are there human hands in the image? | 93.30 | 0.022 | mask |\n| 4 | Is the image symmetrical? | 79.98 | 0.104 | 0.069 |\n| 5 | Does the image avoid asymmetry? | 71.30 | 0.236 | 0.102 |\n| 6 | Are the objects well-coordinated? | 58.31 | 0.138 | 0.000 |\n| 7 | Does the image avoid poorly coordinated objects? | 68.24 | 0.204 | 0.000 |\n| 8 | Is the main subject prominent? | 86.27 | 0.210 | 0.131 |\n| 9 | Does the image avoid an unclear main subject? | 77.75 | 0.258 | 0.070 |\n| 10 | Is the image very rich? | 80.40 | 0.084 | 0.056 |\n| 11 | Is the image rich? | 65.84 | 0.138 | 0.044 |\n| 12 | Is the image not monotonous? | 77.01 | 0.271 | 0.211 |\n| 13 | Is the image not empty? | 99.67 | 0.205 | 0.583 |\n| 14 | Is the background beautiful? | 72.70 | -0.019 | 0.000 |\n| 15 | Is the background somewhat beautiful? | 67.26 | 0.021 | 0.000 |\n| 16 | Is there a background? | 84.86 | 0.079 | mask |\n| 17 | Is the image very clear? | 63.85 | 0.111 | 0.051 |\n| 18 | Is the image clear? | 62.03 | 0.170 | 0.068 |\n| 19 | Does the image avoid being blurry? | 88.92 | 0.284 | 0.065 |\n| 20 | Does the image avoid being completely blurry? | 97.11 | 0.282 | 0.032 |\n| 21 | Are the colors bright? | 63.69 | 0.098 | 0.076 |\n| 22 | Are the colors not dark? | 82.88 | 0.141 | 0.077 |\n| 23 | Are the colors beautiful? | 65.84 | 0.115 | 0.000 |\n| 24 | Are the colors not ugly? | 74.77 | 0.232 | 0.042 |\n| 25 | Is the lighting and shadow very distinct? | 75.45 | -0.043 | 0.000 |\n| 26 | Is the lighting and shadow distinct? | 58.37 | 0.035 | 0.000 |\n| 27 | Is there lighting and shadow? | 75.93 | 0.108 | mask |\n| 28 | Are the lighting and shadows very beautiful? | 80.47 | -0.055 | 0.000 |\n| 29 | Are the lighting and shadows beautiful? | 71.99 | -0.026 | 0.000 |\n| 30 | Can the image evoke a very positive emotional response? | 82.63 | 0.068 | 0.051 |\n| 31 | Can the image evoke a positive emotional response? | 63.94 | 0.117 | 0.000 |\n| 32 | Does the image avoid evoking a negative emotional response? | 76.01 | 0.179 | 0.000 |\n| 33 | Does the image avoid evoking a very negative emotional response? | 91.56 | 0.117 | 0.000 |\n| 34 | Are the image details very exquisite? | 74.03 | 0.078 | 0.010 |\n| 35 | Are the image details exquisite? | 71.79 | 0.091 | 0.000 |\n| 36 | Do the image details avoid being coarse? | 68.73 | 0.215 | 0.000 |\n| 37 | Do the image details avoid being very coarse? | 84.62 | 0.247 | 0.000 |\n| 38 | Does the image avoid being hard to recognize? | 87.34 | 0.267 | 0.017 |\n| 39 | Does the image avoid being fragmented? | 85.36 | 0.288 | 0.115 |\n| 40 | Are the image details realistic? | 63.85 | 0.099 | 0.000 |", "caption": "Table 19: Accuracy, spearman correlation, and linear weights of VisionReward in text-to-image. (Part 1)", "description": "This table presents a detailed breakdown of the VisionReward model's performance on text-to-image generation tasks.  For each of the 40 binary checklist questions used to evaluate the generated images, it shows the accuracy (Acc) of the model's predictions, the Spearman rank correlation coefficient (\u03c1) between model predictions and human judgments, and the learned linear weight assigned to that question in the final VisionReward score.  The 'mask' column indicates whether a mask was used to filter out certain instances based on the absence or presence of specific elements in the images (e.g., if there's a hand, we assess that specific hand based assessment criteria), making the evaluation more targeted and relevant. This part of the table focuses on the first 40 checklist items. ", "section": "2.2 VisionReward Training"}, {"content": "| ID | Checklist | Acc | \u03c1 | Weight |\n|---|---|---|---|---|\n| 41 | Do the image details avoid being unrealistic? | 63.94 | 0.140 | 0.000 |\n| 42 | Do the image details avoid being very unrealistic? | 74.19 | 0.156 | 0.000 |\n| 43 | Do the image details avoid being greatly unrealistic? | 83.62 | 0.177 | 0.000 |\n| 44 | Is the human body in the image completely correct? | 61.31 | 0.063 | 0.082 |\n| 45 | Does the human body in the image avoid errors? | 59.02 | 0.129 | 0.000 |\n| 46 | Does the human body in the image avoid obvious errors? | 82.57 | 0.135 | 0.055 |\n| 47 | Does the human body in the image avoid serious errors? | 90.83 | 0.121 | 0.030 |\n| 48 | Is the human face very beautiful? | 65.50 | -0.046 | 0.000 |\n| 49 | Is the human face beautiful? | 56.88 | -0.006 | 0.000 |\n| 50 | Does the human face avoid errors? | 57.61 | 0.113 | 0.031 |\n| 51 | Does the human face avoid serious errors? | 91.56 | 0.132 | 0.077 |\n| 52 | Are the human hands perfect? | 90.18 | -0.015 | 0.072 |\n| 53 | Are the human hands essentially correct? | 25.84 | 0.059 | 0.000 |\n| 54 | Do the human hands avoid obvious errors? | 37.98 | 0.066 | 0.000 |\n| 55 | Do the human hands avoid serious errors? | 77.26 | 0.048 | 0.000 |\n| 56 | Is the image completely safe? | 78.74 | 0.118 | 0.000 |\n| 57 | Is the image harmless? | 86.44 | 0.106 | 0.000 |\n| 58 | Does the image avoid obvious harmfulness? | 92.39 | 0.109 | 0.012 |\n| 59 | Does the image avoid serious harmfulness? | 92.80 | 0.092 | 0.015 |\n| 60 | Does the image show \u201d[[prompt]]\u201d? | - | 0.297 | 2.354 |", "caption": "Table 20: Accuracy, spearman correlation, and linear weights of VisionReward in text-to-image. (Part 2)", "description": "This table presents a detailed breakdown of the VisionReward model's performance on text-to-image generation tasks.  For each of several image quality dimensions (e.g., body correctness, lighting aesthetic), it lists the accuracy of the model's binary classification ('yes' or 'no') for a series of judgment questions.  Additionally, it provides the Spearman rank correlation coefficient (\u03c1), measuring the strength and direction of the monotonic relationship between VisionReward's predictions and human judgments, and the learned linear weights (Weight) that VisionReward assigns to each judgment question in its overall score calculation.  The 'mask' column indicates whether a question was masked during training (only evaluated when relevant aspects are present in the image).", "section": "3.1 VisionReward: Preference Prediction"}, {"content": "| ID | Checklist | Acc | \u03c1 | Weight |\n|---|---|---|---|---|\n| 1 | Does the video meet all the requirements stated in the text \u201d[[prompt]]\u201d? | 69.5 | 0.315 | 0.954 |\n| 2 | Does the video meet most of the requirements stated in the text \u201d[[prompt]]\u201d? | 72.9 | 0.303 | 0.252 |\n| 3 | Does the video meet some of the requirements stated in the text \u201d[[prompt]]\u201d? | 72.9 | 0.281 | 0.000 |\n| 4 | Does the video not completely fail to meet the requirements stated in the text \u201d[[prompt]]\u201d? | 78.7 | 0.320 | 1.142 |\n| 5 | Is the composition aesthetically pleasing? | 50.8 | 0.263 | 0.035 |\n| 6 | Does the composition have no obvious flaws? | 90.4 | 0.239 | 0.025 |\n| 7 | Is the focus aesthetically pleasing? | 49.8 | 0.232 | 0.000 |\n| 8 | Does the focus have no obvious flaws? | 91.6 | 0.246 | 0.000 |\n| 9 | Is the camera movement aesthetically pleasing? | 76.2 | 0.012 | 0.000 |\n| 10 | Does the camera movement have no obvious flaws? | 97.3 | 0.142 | 0.126 |\n| 11 | Are the colors exceptionally beautiful? | 46.5 | 0.214 | 0.000 |\n| 12 | Are the colors beautiful? | 50.1 | 0.217 | 0.000 |\n| 13 | Are the colors not unattractive? | 82.2 | 0.225 | 0.000 |\n| 14 | Are the colors not significantly unattractive? | 88.6 | 0.202 | 0.032 |\n| 15 | Is the lighting perfectly accurate? | 51.9 | 0.346 | 0.163 |\n| 16 | Does the lighting have no obvious errors? | 86.2 | 0.259 | 0.217 |\n| 17 | Is there any lighting present? | 87.8 | 0.215 | 0.020 |\n| 18 | Is the lighting exceptionally beautiful? | 65.1 | 0.212 | 0.136 |\n| 19 | Is the lighting beautiful? | 55.8 | 0.240 | 0.096 |\n| 20 | Is the lighting not unattractive? | 83.5 | 0.280 | 0.155 |", "caption": "Table 21: Accuracy, spearman correlation, and linear weights of VisionReward in text-to-video. (Part 1)", "description": "This table presents a detailed breakdown of the VisionReward model's performance on text-to-video generation tasks.  It shows the accuracy of the model's binary classifications ('yes'/'no') for various aspects of video quality, as determined by human judges.  Spearman correlation coefficients indicate the strength of the linear relationship between the model's predictions and human judgments for each quality aspect.  Finally, linear weights are provided, reflecting the relative importance assigned to each aspect in the model's overall video quality score.", "section": "3.1 VisionReward: Preference Prediction"}, {"content": "| ID | Checklist | Acc | \u03c1 | Weight |\n|---|---|---|---|---|\n| 21 | Is the shape of the object at the beginning of the video completely accurate? | 63.0 | 0.292 | 0.129 |\n| 22 | Does the shape of the object at the beginning have no obvious errors? | 76.3 | 0.274 | 0.099 |\n| 23 | Is the shape of the object at the beginning not chaotic? | 91.3 | 0.256 | 0.188 |\n| 24 | Is the shape of the object perfectly maintained throughout the video? | 54.2 | 0.300 | 0.184 |\n| 25 | Does the shape of the object have no obvious issues throughout the video? | 68.8 | 0.267 | 0.000 |\n| 26 | Does the shape of the object generally have no major issues throughout the video? | 84.5 | 0.259 | 0.000 |\n| 27 | Is the shape of the object not chaotic throughout the video? | 93.5 | 0.240 | 0.264 |\n| 28 | Is the object\u2019s motion highly dynamic? | 78.0 | -0.079 | 0.000 |\n| 29 | Is the object\u2019s motion dynamic? | 69.0 | -0.024 | 0.000 |\n| 30 | Is the object\u2019s motion not minimal? | 71.2 | -0.009 | 0.000 |\n| 31 | Is the object\u2019s motion not static? | 66.5 | -0.014 | 0.000 |\n| 32 | Is the camera motion highly dynamic? | 86.9 | -0.054 | 0.112 |\n| 33 | Is the camera motion dynamic? | 80.6 | -0.062 | 0.000 |\n| 34 | Is the camera motion not minimal? | 72.1 | -0.061 | 0.052 |\n| 35 | Is the camera motion not static? | 58.1 | -0.059 | 0.000 |\n| 36 | Is the smoothness of the object\u2019s movement very good? | 59.8 | 0.263 | 0.026 |\n| 37 | Does the smoothness of the object\u2019s movement have no obvious issues? | 61.6 | 0.139 | 0.000 |\n| 38 | Is the object\u2019s movement completely realistic? | 66.8 | 0.338 | 0.439 |\n| 39 | Does the object\u2019s movement have no obvious realism issues? | 69.2 | 0.235 | 0.000 |\n| 40 | Is it very clear? | 52.1 | 0.261 | 0.000 |\n| 41 | Is it clear? | 51.0 | 0.290 | 0.000 |\n| 42 | Is it not blurry? | 81.8 | 0.271 | 0.000 |\n| 43 | Is it not completely blurry? | 93.1 | 0.226 | 0.000 |\n| 44 | Is the image quality very stable? | 43.1 | 0.313 | 0.269 |\n| 45 | Is the image quality stable? | 61.2 | 0.294 | 0.000 |\n| 46 | Is the image quality not unstable? | 79.0 | 0.277 | 0.000 |\n| 47 | Is the image quality free of noticeable instability? | 87.6 | 0.247 | 0.000 |\n| 48 | Is the camera very stable? | 54.2 | 0.197 | 0.000 |\n| 49 | Is the camera not unstable? | 83.5 | 0.267 | 0.000 |\n| 50 | Are the details very refined? | 73.0 | 0.324 | 0.429 |\n| 51 | Are the details relatively refined? | 62.3 | 0.331 | 0.000 |\n| 52 | Are the details not rough? | 74.2 | 0.302 | 0.008 |\n| 53 | Are the details not significantly rough? | 89.2 | 0.271 | 0.128 |\n| 54 | Are all the letters correct? | 87.3 | 0.114 | 0.058 |\n| 55 | Do the letters have no obvious errors? | 86.8 | 0.115 | 0.000 |\n| 56 | Are there any letters present? | 89.7 | 0.104 | 0.145 |\n| 57 | Does it fully comply with the laws of physics? | 36.6 | 0.254 | 0.000 |\n| 58 | Does it partially comply with the laws of physics? | 66.7 | 0.248 | 0.000 |\n| 59 | Does it have no obvious violations of the laws of physics? | 77.4 | 0.231 | 0.000 |\n| 60 | Is the video content part of the physical world? | 86.6 | 0.231 | 0.394 |\n| 61 | Is the video content safe? | 92.8 | 0.000 | 0.000 |\n| 62 | Is the video content definitely free of harmful material? | 94.3 | 0.000 | 0.000 |\n| 63 | Does the video content contain no harmful material? | 97.7 | 0.000 | 0.000 |\n| 64 | Does the video content contain no extremely harmful material? | 100.0 | 0.000 | 0.000 |", "caption": "Table 22: Accuracy, spearman correlation, and linear weights of VisionReward in text-to-video. (Part 2)", "description": "This table presents a detailed breakdown of the VisionReward model's performance on text-to-video generation tasks.  It shows the accuracy of each checklist question within the VisionReward framework, the Spearman correlation (\u03c1) between the VisionReward scores and human judgment, and the learned linear weights (Weight) for each question. The 'Acc' column indicates the model's accuracy in predicting whether a video feature is present or not, based on the human annotations. '\u03c1' represents the strength and direction of the relationship between the model's predictions and human judgments.  A higher \u03c1 indicates stronger correlation. The 'Weight' column reflects the importance assigned to each question in the final VisionReward score; a larger weight suggests a greater contribution to the overall preference score.  The table provides insights into which video quality aspects are most important for human preference and how accurately the VisionReward model captures these aspects.", "section": "3. Experiments"}, {"content": "| Image |  |  | Video |  |  |\n|---|---|---|---|---|---| \n| **Type** | **Ratio** | **Count** | **Type** | **Ratio** | **Count** |\n| People | 8 | 286 | Story | 5 | 265 |\n| Objects | 4 | 143 | Human Activity | 4 | 212 |\n| Animals | 4 | 143 | Artificial Scene | 3 | 159 |\n| Architecture | 4 | 143 | Natural Scenes | 3 | 159 |\n| Others | 2 | 72 | Animal Activity | 2 | 106 |\n| Landscape | 2 | 72 | Physical Phenomena | 1 | 53 |\n| Vehicles | 2 | 71 | Other | 1 | 53 |\n| Plants | 1 | 35 |  |  |  |\n| Food | 1 | 35 |  |  |  |", "caption": "Table 23: Content Categories for Image and Video", "description": "This table presents a breakdown of content categories used in the MonetBench dataset for both image and video generation.  It shows the relative ratios and counts of different content types within the dataset, providing insight into the diversity and distribution of visual elements used in the benchmark.  The categories help define the types of scenes and objects depicted in the images and videos used for evaluation.", "section": "3. Experiments"}, {"content": "| Image |  |  | Video |  |  |\n|---|---|---|---|---|---| \n| **Type** | **Ratio** | **Count** | **Type** | **Ratio** | **Count** |\n| Unreal | 8 | 187 | Style | 13 | 465 |\n| Style & Format | 8 | 187 | Material/Texture | 8 | 292 |\n| Fine-grained Detail | 8 | 186 | Emotional Expr. | 7 | 249 |\n| Color | 4 | 93 | Color/Tone | 7 | 261 |\n| Famous Character | 4 | 93 | World Knowledge | 5 | 192 |\n| History & Culture | 4 | 93 | Special Effects | 5 | 183 |\n| Normal | 2 | 46 | World Knowledge | 4 | 192 |\n| Writing | 1 | 23 | Spatial Relat. | 4 | 136 |\n| Complex Combo | 1 | 23 | Camera Move. | 4 | 153 |\n| Famous Places | 1 | 23 | Surreal | 3 | 108 |\n| Positional | 1 | 23 | Logical Consist. | 2 | 116 |\n| Counting | 1 | 23 | Temporal Speed | 1 | 66 |\n|  |  |  | Text | 1 | 46 |", "caption": "Table 24: Challenge Categories for Image and Video", "description": "This table presents the challenge categories used in the MonetBench benchmark for both image and video generation.  These categories represent various aspects of complexity and difficulty in generating high-quality images and videos, designed to evaluate the capabilities of different generation models.  Each category includes several sub-categories that further refine the difficulty and nuance of the generation task. The table lists the category names, the ratio of prompts belonging to each category, and the number of prompts in each category for both image and video generation, highlighting the relative importance and distribution of different challenge types within MonetBench.", "section": "3. Experiments"}, {"content": "| Categorie | Description | Example Prompt |\n|---|---|---|\n| **Content** |  |  |\n| Human Activity | Descriptions about daily human activities, sports, performing arts, and professional skills. | A family enjoying a picnic in a park, children playing soccer. |\n| Animal Activity | Descriptions about wild animals, domestic pets, and interactions between animals. | A group of dolphins jumping out of the water. |\n| Natural Scenes | Descriptions about weather changes, geological events, and astronomical phenomena. | A thunderstorm with lightning striking the ground. |\n| Artificial Scenes | Descriptions about cityscapes, interiors of buildings, vehicles, and industrial production. | A bustling city street with traffic and pedestrians. |\n| Physical Phenomena | Descriptions about physical occurrences like candle burning, ice melting, glass breaking, and explosions. | A glass shattering in slow motion. |\n| Story | Descriptions about coherent narratives based on a story or fantasy rather than a single scene or activity. | Alice, a young girl, falls down a rabbit hole into a wonderland full of fantastical creatures and adventures. |\n| Other | Descriptions about various contents that do not fit into the other specified categories. | Various clips of miscellaneous activities not fitting into other categories. |\n| **Challenge** |  |  |\n| Style | Descriptions about artistic styles such as realistic, cyberpunk, and animated. | A futuristic city with neon lights and flying cars, portrayed in a cyberpunk style. |\n| Color/Tone | Descriptions about color schemes like warm tones, cool tones, monochrome, and high saturation. | A serene landscape in warm, golden tones during sunset. |\n| Camera Movement | Descriptions about different camera movements, including fixed, panning, zooming, tracking, and aerial shots. | A drone shot capturing a bird\u2019s eye view of a mountain range. |\n| Special Effects | Descriptions about special effects such as particle effects, lighting effects, and transitions. | Fireworks exploding with sparkling particle effects. |\n| Material/Texture | Descriptions about materials and textures like metal, wood, glass, and fabric. | Close-up shot of rain droplets on a glass window. |\n| Surreal | Descriptions about dreamlike, fantastical, or non-realistic elements. | A dreamlike scene with floating islands in the sky. |\n| Temporal Speed | Descriptions about different speeds, including slow motion, normal speed, fast motion, and time reversal. | Slow-motion capture of a hummingbird in flight. |\n| Spatial Relationships | Descriptions about the spatial arrangement of objects, their sizes, occlusions, and perspectives. | A house of cards being built, showing each layer\u2019s spatial arrangement. |\n| World Knowledge | Descriptions about physical laws, famous landmarks, historical events, and renowned personalities. | A documentary about the pyramids of Egypt. |\n| Logical Consistency | Descriptions about ensuring logical relationships among events, timelines, and spatial layouts. | A mystery story where clues are pieced together logically. |\n| Emotional Expression | Descriptions about expressions of emotions such as joy, sorrow, fear, and surprise. | A close-up of a person expressing joy after receiving good news. |\n| Text | Descriptions about incorporating textual elements dynamically within the footage. | An animated title sequence with dynamic text effects. |", "caption": "Table 25: Video classification standards with example prompts.", "description": "This table presents the classification standards used for the Video-MonetBench dataset, a benchmark designed for evaluating video generation models.  It categorizes video prompts into seven content categories (e.g., Human Activity, Natural Scenes, etc.) and thirteen challenge categories (e.g., Style, Color/Tone, Special Effects, etc.). Each category includes a detailed description and an illustrative example prompt, offering a comprehensive overview of the dataset's scope and complexity. This ensures that the evaluation encompasses diverse aspects of visual generation quality.", "section": "3. Experiments"}]