[{"figure_path": "https://arxiv.org/html/2412.21059/x1.png", "caption": "Figure 1: Samples of VisionReward and Multi-Objective Preference Optimization (MPO) algorithm.", "description": "Figure 1 presents examples illustrating VisionReward and the Multi-Objective Preference Optimization (MPO) algorithm.  Panel (a) shows a text-to-image generation task where VisionReward assigns a higher score than ImageReward, demonstrating its ability to better capture human preferences. Panel (b) similarly demonstrates VisionReward's superior performance in a text-to-video generation task compared to VideoScore.  Panels (c) and (d) show comparative results of text-to-image and text-to-video optimization, respectively.  They show the original generated outputs and how those outputs are improved using different optimization methods, including DPO with different scoring methods and MPO with VisionReward.  The results highlight MPO with VisionReward's effectiveness in optimizing visual generation, leading to superior output quality according to human assessment.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.21059/x2.png", "caption": "Figure 2: An overview of the VisionReward and Multi-Objective Preference Optimization (MPO).", "description": "This figure illustrates the VisionReward system and its Multi-Objective Preference Optimization (MPO) algorithm.  The VisionReward model first uses a checklist of fine-grained questions to obtain binary judgments (yes/no) from humans regarding specific aspects of image or video quality.  These judgments are then linearly weighted and combined to produce a single interpretable preference score. The MPO algorithm leverages this fine-grained reward model to address the challenge of balancing multiple, sometimes conflicting, aspects of preference during the training of visual generation models, avoiding over- or under-optimization of specific attributes. The figure displays the flow of information and the different stages of the process, from initial annotation to the final analysis of preferences after model optimization.", "section": "2. Method"}, {"figure_path": "https://arxiv.org/html/2412.21059/x3.png", "caption": "(a) Data analysis.", "description": "The bar chart visualizes the performance deviations across 18 sub-dimensions from the Pick-a-Pic dataset.  The x-axis represents the 18 dimensions, and the y-axis represents the percentage deviation from the average yes-proportion for each dimension.  Positive values indicate that the dimension is more emphasized than average, while negative values show the opposite.  This visualization provides insights into which dimensions are prioritized by human preference when evaluating images.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.21059/x4.png", "caption": "(b) DPO analysis.", "description": "This figure compares score deviations across 18 sub-dimensions for images generated by SDXL before and after Diffusion-DPO fine-tuning, using 10,000 human preference pairs from the Pick-a-Pic dataset.  It visually represents how the optimization process of Diffusion-DPO affects different aspects of image quality, showing both improvements and decrements in various dimensions.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.21059/x5.png", "caption": "Figure 3: (a) We sample 10,000 human preference pairs from Pick-a-Pic\u00a0[20] dataset and analyze score deviations across 18 sub-dimensions (represented by the average yes-proportion of checklist questions within each sub-dimension). (b) We compare score deviations for images generated by SDXL\u00a0[27] before and after Diffusion-DPO fine-tuning\u00a0[40], using the same 10,000 prompts.", "description": "Figure 3 illustrates the analysis of human preferences and the effects of preference learning on image generation.  Panel (a) shows the distribution of scores across 18 sub-dimensions of image quality, each represented by the average 'yes' responses to binary checklist questions in the Pick-a-Pic dataset. This visualization reveals the relative importance of each sub-dimension in human perception. Panel (b) compares score deviations across the same 18 sub-dimensions for images generated by SDXL before and after fine-tuning using the Diffusion-DPO method. This comparison highlights the impact of preference learning on the alignment of generated images with human preferences. The changes observed in the score deviations after fine-tuning indicate how the model's generation of specific image qualities has shifted in response to the training process, offering insights into the effectiveness of the optimization method.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.21059/x6.png", "caption": "Figure 4: Human evaluation of text-to-image MPO.", "description": "This figure shows the results of a human evaluation comparing different methods for text-to-image generation optimization.  The methods compared include a baseline, DPO (Diffusion Preference Optimization) with two different reward models (Pick-a-Pic and HPSv2), and the authors' proposed MPO (Multi-Objective Preference Optimization) method using VisionReward. The chart displays the win/tie/loss rates for each method, indicating how often each method's generated images were preferred over those generated by another method, given the same text prompt. This visually demonstrates the performance improvement achieved by MPO with VisionReward.", "section": "3.2. MPO for Text-to-Image Optimization"}, {"figure_path": "https://arxiv.org/html/2412.21059/x7.png", "caption": "Figure 5: Human evaluation of text-to-video MPO.", "description": "This figure displays the results of a human evaluation comparing the performance of three different methods for text-to-video optimization: a baseline method, a method using VideoScore, and the authors' proposed method, VisionReward, with Multi-Objective Preference Optimization (MPO).  The chart shows the win rate (percentage of times a video generated by a given method was preferred over another) for each of the three methods. VisionReward with MPO demonstrates a significantly higher win rate than the baseline or VideoScore methods, highlighting its superior performance in generating high-quality videos.", "section": "3. Multi-Objective Preference Optimization"}, {"figure_path": "https://arxiv.org/html/2412.21059/x8.png", "caption": "(a) Text-to-image", "description": "This figure shows an example of text-to-image generation evaluation using VisionReward.  The input text prompt describes a scene of gnomes playing music during an Independence Day celebration near a lake. The figure displays the generated images from different methods.  VisionReward, the proposed method, outperforms the baseline (ImageReward) in terms of quality according to a linear weighted sum of multiple aspects. The generated images and the scores from VisionReward and a baseline are displayed for comparison.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.21059/x9.png", "caption": "(b) Text-to-video", "description": "This figure shows examples of text-to-video generation using different methods.  The top row displays the original video generated from a text prompt ('A child is eating pizza'). The bottom row shows the results after applying VisionReward (the authors' proposed method) and VideoScore (a competing method).  Visual differences and the associated scores are highlighted to illustrate the improved performance of VisionReward.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.21059/x10.png", "caption": "Figure 6: Annotation statistics of different sub-dimensions.", "description": "Figure 6 presents a comparative analysis of annotation statistics across various sub-dimensions for both image and video generation tasks.  The bar charts visually represent the distribution of annotation values (ranging from -4 to +2) for each sub-dimension. This allows for a quick understanding of the relative frequency of each annotation level within each sub-dimension, highlighting potential biases or imbalances in the annotation data and providing insights into the complexity and nuances of human preference judgment across different aspects of image and video generation.", "section": "2. Method"}, {"figure_path": "https://arxiv.org/html/2412.21059/x11.png", "caption": "(a) Overall Score", "description": "This figure shows the overall performance comparison of different methods across multiple datasets.  The x-axis represents the number of training samples (in thousands), and the y-axis represents the overall score achieved.  Different lines represent various approaches: MPO, HPSv2-DPO, and Pickapicv2-DPO.  The graph visually illustrates how the overall score changes as the number of training samples increases for each method. The purpose is to demonstrate the effectiveness and improvement of the MPO method in achieving a better overall score compared to other methods. ", "section": "3.2. MPO for Text-to-Image Optimization"}, {"figure_path": "https://arxiv.org/html/2412.21059/x12.png", "caption": "(b) Composition Score", "description": "This figure shows the change in composition scores during the multi-objective preference optimization (MPO) process. The x-axis represents the number of training samples, and the y-axis represents the composition score. Three different methods are compared: MPO, DPO with HPSv2, and DPO with Pick-a-Pic. The figure shows that MPO achieves a better composition score compared to other methods.", "section": "3.2. MPO for Text-to-Image Optimization"}, {"figure_path": "https://arxiv.org/html/2412.21059/x13.png", "caption": "(c) Fidelity Score", "description": "The figure shows the fidelity scores during the multi-objective preference optimization (MPO) process. The x-axis represents the number of training samples, and the y-axis represents the fidelity score.  Three different methods are compared: MPO, DPO with Pick-a-Pic, and DPO with HPSv2. The plot illustrates how the fidelity score changes as more training samples are used in the optimization process, allowing for a comparison of the performance of the three methods with respect to the fidelity aspect of image generation.", "section": "3.2. MPO for Text-to-Image Optimization"}, {"figure_path": "https://arxiv.org/html/2412.21059/x14.png", "caption": "(d) Alignment Score", "description": "This figure shows a graph illustrating the 'Alignment' score over the course of the Multi-Objective Preference Optimization (MPO) process.  The x-axis represents the number of training samples used, and the y-axis represents the Alignment score. Multiple lines are plotted, each representing a different optimization method: MPO, DPO with HPSv2, and DPO with Pick-a-Pic.  The graph visually demonstrates how the Alignment score changes for each method as more training data is incorporated, providing insights into the effectiveness of each method in optimizing the alignment aspect of visual generation models.", "section": "3.2. MPO for Text-to-Image Optimization"}, {"figure_path": "https://arxiv.org/html/2412.21059/x15.png", "caption": "(e) Quality Score", "description": "The graph displays the \"Quality Score\" metric over the course of the Multi-Objective Preference Optimization (MPO) process.  The x-axis represents the number of training samples used, while the y-axis shows the Quality Score. Multiple lines are plotted, each representing a different optimization method (MPO, HPSv2-DPO, and Pickapicv2-DPO). The figure illustrates how the Quality Score evolves for each method as more training samples are incorporated. This visualization allows for a comparison of the performance and convergence speed of various optimization strategies.", "section": "3.2. MPO for Text-to-Image Optimization"}, {"figure_path": "https://arxiv.org/html/2412.21059/x16.png", "caption": "(f) Safety & Emotion Score", "description": "This figure shows the Safety & Emotion scores across different training sample sizes.  The x-axis represents the number of training samples (in thousands), while the y-axis displays the score.  Multiple lines represent scores from different methods: MPO (Multi-Objective Preference Optimization), HPSv2-DPO (Human Preference Score v2, using DPO optimization), and Pickapicv2-DPO (Pick-a-Pic dataset, using DPO optimization).  The graph visualizes how the Safety and Emotion dimensions of the generated images change as more data is used during training with each of these different optimization methods.", "section": "3.2. MPO for Text-to-Image Optimization"}, {"figure_path": "https://arxiv.org/html/2412.21059/x17.png", "caption": "Figure 7: Variation of dimensional scores during the MPO process with respect to the number of training samples.", "description": "This figure displays the changes in different dimensional scores throughout the multi-objective preference optimization (MPO) process.  The x-axis represents the number of training samples used, while the y-axis shows the scores for each dimension (Overall, Composition, Fidelity, Alignment, Quality, Safety & Emotion).  Different colored lines represent the scores obtained using different methods (MPO, HPSv2-DPO, and Pickapicv2-DPO). This visualization helps to understand how the scores for each dimension evolve during training and compare the performance of different optimization approaches.", "section": "3.2. MPO for Text-to-Image Optimization"}]