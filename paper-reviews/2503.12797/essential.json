{"importance": "This paper introduces a novel approach to enhance MLLMs' cognitive visual perception, addressing limitations in integrating reasoning with visual analysis. It is important because it sets a new benchmark for **human-like visual perception** and opens new directions for multimodal reasoning research with broad applications.", "summary": "DeepPerception enhances MLLMs with cognitive visual perception, achieving superior grounding through knowledge integration & reasoning.", "takeaways": ["KVG is a novel visual grounding task requiring fine-grained perception and domain knowledge integration.", "DeepPerception, enhances cognitive visual perception capabilities in MLLMs through automated data synthesis and a two-stage training framework.", "DeepPerception achieves state-of-the-art performance on KVG-Bench, demonstrating superior cognitive visual perception and cross-domain generalization."], "tldr": "Current Multimodal Large Language Models (MLLMs) struggle to integrate reasoning into visual perception, often providing direct responses without deeper analysis. To address this, the paper introduces **knowledge-intensive visual grounding (KVG)**, a new visual grounding task demanding both fine-grained perception and the integration of domain-specific knowledge. The benchmark includes 10 domains with 1.3K curated test cases, highlighting current MLLMs' limitations in cognitive visual perception.\n\nTo overcome these challenges, the paper presents a method that enhances MLLMs with cognitive visual perception capabilities. This approach utilizes (1) an automated data synthesis pipeline for generating high-quality, knowledge-aligned samples, and (2) a two-stage training framework combining supervised fine-tuning and reinforcement learning. Experimental results demonstrate significant improvements in accuracy and generalization compared to direct fine-tuning, achieving state-of-the-art performance on the new benchmark. The approach substantially advances the state-of-the-art in cognitive visual perception.", "affiliation": "Tsinghua University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.12797/podcast.wav"}