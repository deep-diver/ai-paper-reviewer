{"references": [{"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-01-01", "reason": "This paper introduces BERT, a foundational language model used for text encoding in this work."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This paper presents Vision Transformer (ViT), a key component for image encoding."}, {"fullname_first_author": "Douwe Kiela", "paper_title": "The hateful memes challenge: Detecting hate speech in multimodal memes", "publication_date": "2020-01-01", "reason": "This paper introduces the Hateful Memes dataset, which is used to evaluate the proposed method."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper introduces CLIP, a popular vision-language model which the work here refers to."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "publication_date": "2022-01-01", "reason": "This work uses BLIP as a baseline model in its experiments, which is a text decoder-based vision-language model."}]}