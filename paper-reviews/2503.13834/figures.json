[{"figure_path": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure1-1.jpg", "caption": "Figure 1: Conceptual visualization of dominant modality bias. The key modality differs by task: (a) For the hate recognition task, text descriptions of memes lead, while (b) for the food classification task, food images play a crucial role in prediction.", "description": "Figure 1 illustrates the concept of dominant modality bias in vision-language (VL) models.  Dominant modality bias refers to situations where a VL model relies disproportionately on one modality (either visual or textual) for its predictions, even when information from both modalities is available.  The figure shows two examples:\n(a) Hate recognition: Here, the text associated with a meme is more influential in determining whether it is hateful than the image itself. The model primarily focuses on the textual content to make the prediction.\n(b) Food Classification: Conversely, in this task, the visual information (the image of the food) is significantly more important than the textual description for correct classification. The model heavily relies on the visual data to make the prediction.\nThis bias can negatively impact the model's performance, especially when one modality is impaired or unavailable.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure2.jpg", "caption": "Figure 2: Experimental results on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets in the presence of dominant modality bias. (a) Performance visualization under different missing conditions (full, image only (missing text), text only (missing image)) for each dataset. (b) Illustration of learning curves for each modality across datasets.", "description": "This figure presents the experimental results evaluating the impact of dominant modality bias on three vision-language datasets: UPMC Food-101, Hateful Memes, and MM-IMDb.  Subfigure (a) shows a comparison of model performance across three conditions: when both image and text modalities are available ('Full'), when only the image modality is available ('Image only'), and when only the text modality is available ('Text only'). This visualization reveals the extent to which each dataset exhibits a dominant modality bias.  Subfigure (b) illustrates the training dynamics of the models. By plotting the loss curves for each modality (image and text) across the different datasets, the visualization helps demonstrate how the dominant modality achieves faster convergence and lower loss compared to the weaker modality. This difference in convergence rates underscores the existence of dominant modality bias, where one modality is disproportionately influential in the model's predictions.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure3.jpg", "caption": "Figure 3: (a) The overall training framework of our proposed BalGrad. The final classifier f\ud835\udcaf\u2062(\u22c5)subscript\ud835\udc53\ud835\udcaf\u22c5f_{\\mathcal{T}}(\\cdot)italic_f start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT ( \u22c5 ) is updated with the gradient g\ud835\udcaf\u27c2subscriptsuperscript\ud835\udc54perpendicular-to\ud835\udcafg^{\\perp}_{\\mathcal{T}}italic_g start_POSTSUPERSCRIPT \u27c2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT for cross entropy (CE) loss. The image and text embedding layers hv\u2062(\u22c5),hl\u2062(\u22c5)subscript\u210e\ud835\udc63\u22c5subscript\u210e\ud835\udc59\u22c5h_{v}(\\cdot),h_{l}(\\cdot)italic_h start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( \u22c5 ) , italic_h start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( \u22c5 ) are also updated with g\ud835\udcaf\u27c2subscriptsuperscript\ud835\udc54perpendicular-to\ud835\udcafg^{\\perp}_{\\mathcal{T}}italic_g start_POSTSUPERSCRIPT \u27c2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT along with the gradients of the CE loss for each modality g\ud835\udcafv,g\ud835\udcaflsubscriptsuperscript\ud835\udc54\ud835\udc63\ud835\udcafsubscriptsuperscript\ud835\udc54\ud835\udc59\ud835\udcafg^{v}_{\\mathcal{T}},g^{l}_{\\mathcal{T}}italic_g start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT , italic_g start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT, and the gradients of the KL divergence between the two modalities\u2019 predictions gk\u2062lv,gk\u2062llsubscriptsuperscript\ud835\udc54\ud835\udc63\ud835\udc58\ud835\udc59subscriptsuperscript\ud835\udc54\ud835\udc59\ud835\udc58\ud835\udc59g^{v}_{kl},g^{l}_{kl}italic_g start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPT , italic_g start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPT. (b) Inter-modality gradient reweighting adjusts the magnitudes of gk\u2062lvsubscriptsuperscript\ud835\udc54\ud835\udc63\ud835\udc58\ud835\udc59g^{v}_{kl}italic_g start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPT and gk\u2062llsubscriptsuperscript\ud835\udc54\ud835\udc59\ud835\udc58\ud835\udc59g^{l}_{kl}italic_g start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPT to obtain gk\u2062lsubscript\ud835\udc54\ud835\udc58\ud835\udc59g_{kl}italic_g start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPT. If a conflict occurs, we project g\ud835\udcaf\u27c2subscriptsuperscript\ud835\udc54perpendicular-to\ud835\udcafg^{\\perp}_{\\mathcal{T}}italic_g start_POSTSUPERSCRIPT \u27c2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT on the orthogonal direction of gk\u2062lsubscript\ud835\udc54\ud835\udc58\ud835\udc59g_{kl}italic_g start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPT by inter-task gradient projection.", "description": "Figure 3 illustrates the architecture and training process of the proposed BalGrad model. (a) shows the overall framework where the image and text encoders produce embeddings that are concatenated and passed to a final classifier. The classifier is updated using the cross-entropy (CE) loss gradient and the gradients for the individual modalities and the KL divergence between the two modalities' predictions. (b) details the inter-modality gradient reweighting and inter-task gradient projection components.  Inter-modality gradient reweighting adjusts the magnitudes of KL divergence gradients based on each modality's contribution. Inter-task gradient projection projects the main task gradient orthogonally to the KL divergence gradient when they conflict, ensuring balanced updates across modalities.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure4.jpg", "caption": "Figure 4: Evaluation on robustness to different missing ratio r\ud835\udc5fritalic_r of BalGrad and existing methods on UPMC Food-101, Hateful Memes, and MM-IMDb datasets.", "description": "This figure displays the robustness of the proposed BALGRAD model and three other existing models (MSLR, OGM-GE, and AGM) against varying degrees of missing data.  The x-axis represents the percentage of missing data (missing ratio r) for either the image or text modality. The y-axis shows the performance gap, which is the difference in performance between conditions where only the image modality is missing and conditions where only the text modality is missing. The results are shown for three different datasets: UPMC Food-101, Hateful Memes, and MM-IMDb. A smaller performance gap indicates that the model is less biased toward a specific modality.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/BLIP_gap.png", "caption": "Figure 5: Bar plots comparing the performance of existing methods and BalGrad using BLIP. Each bar represents \u0394Gapsubscript\u0394Gap\\Delta_{\\textit{Gap}}roman_\u0394 start_POSTSUBSCRIPT Gap end_POSTSUBSCRIPT(%), defined as the performance difference between missing image and missing text conditions.", "description": "Figure 5 presents a comparison of the performance of several methods for mitigating dominant modality bias in vision-language models, specifically focusing on the impact of missing modalities. The models were evaluated using the BLIP architecture, and the results are shown in terms of \u0394Gap, which is the difference in performance between cases where only the image is available (missing text) and cases where only the text is available (missing image).  Each bar in the plot represents the \u0394Gap for a specific method on one of the three datasets used in the study (UPMC Food-101, Hateful Memes, and MM-IMDb). A smaller \u0394Gap indicates better balance between the modalities and suggests less reliance on a dominant modality. The figure visually summarizes how effectively each model maintains performance across these missing modality conditions, highlighting BalGrad's relative effectiveness in achieving modality balance.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure5.png", "caption": "Figure 6: Training iteration loss curves for image and text modalities on the UPMC Food-101 and Hateful Memes datasets, comparing the effects of the existence of inter-modality gradient reweighting.", "description": "This figure displays the training loss curves for both image and text modalities across two datasets: UPMC Food-101 and Hateful Memes.  Separate curves are shown for each modality under two conditions: with and without inter-modality gradient reweighting. This visual representation allows for a comparison of how the loss decreases over training iterations for each modality and the impact of the proposed inter-modality gradient reweighting technique on balancing the training dynamics between the image and text modalities.", "section": "3.1 Analysis of Dominant Modality Bias"}, {"figure_path": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure6.png", "caption": "Figure 7: Histogram visualization of the frequency of gradient conflicts between image and text gradients during training iterations on the UPMC Food-101 and Hateful Memes datasets. \u03bcw/osubscript\ud835\udf07\ud835\udc64\ud835\udc5c\\mu_{w/o}italic_\u03bc start_POSTSUBSCRIPT italic_w / italic_o end_POSTSUBSCRIPT and \u03bcw\u2063/subscript\ud835\udf07\ud835\udc64\\mu_{w/}italic_\u03bc start_POSTSUBSCRIPT italic_w / end_POSTSUBSCRIPT represent the average cosine similarity values w/o and w/ projection, respectively.", "description": "This figure displays histograms showing the distribution of cosine similarity between image and text gradients during the training process for two datasets: UPMC Food-101 and Hateful Memes.  The cosine similarity measures the alignment of the gradients. A higher positive value indicates stronger alignment, implying that both modalities contribute effectively to the learning process. Conversely, negative values suggest conflicting gradients, where one modality might hinder the other's learning. The histograms compare the gradient distributions with and without gradient projection.  \u03bcw/o represents the average cosine similarity without gradient projection, while \u03bcw/ represents the average cosine similarity with gradient projection. The difference between these values illustrates the effectiveness of the gradient projection in reducing conflicts between modalities during training.", "section": "3.2 BALGRAD"}, {"figure_path": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/ap_fusion_mechanism_2.jpg", "caption": "Figure 8: Bar plots illustrating the performance of existing methods and BalGrad with different fusion mechanisms: (a) addition and (b) attention, evaluated on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets. Each bar indicates \u0394Gapsubscript\u0394Gap\\Delta_{\\textit{Gap}}roman_\u0394 start_POSTSUBSCRIPT Gap end_POSTSUBSCRIPT(%), which quantifies the performance variation between missing image and missing text conditions.", "description": "Figure 8 presents a comparative analysis of various multi-modal learning approaches, including BalGrad, across different fusion mechanisms (addition and attention).  The experiments were conducted on three benchmark datasets: UPMC Food-101, Hateful Memes, and MM-IMDb.  The bar plots display the performance difference (\u0394Gap) between scenarios where either the image or text modality was missing. A smaller \u0394Gap indicates a better balance in the model's reliance on both modalities, with less over-dependence on one modality over the other.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/ap_backbone_models.jpg", "caption": "Figure 9: Bar plots presenting the performance comparison between existing methods and BalGrad across different backbone models: (a) ResNet and DistilBERT, and (b) CLIP, on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets. Each bar represents \u0394Gapsubscript\u0394Gap\\Delta_{\\textit{Gap}}roman_\u0394 start_POSTSUBSCRIPT Gap end_POSTSUBSCRIPT(%), measuring the performance discrepancy under missing image and missing text conditions.", "description": "Figure 9 presents a comparative analysis of the performance of various methods, including BalGrad, across different vision-language model architectures.  Specifically, it uses ResNet and DistilBERT in part (a), and CLIP in part (b) as backbone models. The datasets used for this comparison are UPMC Food-101, Hateful Memes, and MM-IMDb.  The key metric shown is \u0394Gap, representing the performance difference between scenarios where either the image or text modality is missing.  This illustrates the robustness of each model in handling scenarios with missing data from one of the modalities.", "section": "4 Experiments"}]