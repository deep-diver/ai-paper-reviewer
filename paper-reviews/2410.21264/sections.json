[{"page_end_idx": 2, "page_start_idx": 1, "section_number": 1, "section_title": "Introduction", "details": {"details": "Autoregressive models have shown remarkable success in natural language processing, inspiring their application to visual generation.  Existing visual tokenization methods for autoregressive models typically use a patchwise approach, directly encoding local visual patches into discrete tokens. This approach limits the ability to capture global and holistic representations of the visual content, restricting the model's capacity for high-quality generation.  The limitations are further exacerbated when employing autoregressive models which rely on sequential processing and therefore require a specific ordering of these tokens for coherent output. The reconstruction fidelity of the tokenizer also doesn't directly correlate with the quality of the AR model's generation.  Higher reconstruction fidelity doesn't guarantee better generation results.", "first_cons": "Patchwise tokenization methods limit the ability to capture global and holistic visual representations.", "first_pros": "Autoregressive models show great potential for visual generation, but current tokenization methods hinder their effectiveness.", "keypoints": ["Existing visual tokenization methods for autoregressive models are limited by patchwise tokenization schemes.", "These patchwise methods hinder holistic representation and optimal sequential order.", "Reconstruction fidelity doesn't guarantee good generation quality in autoregressive models.", "The paper aims to overcome these limitations with a novel video tokenizer called LARP"], "second_cons": "The ordering of tokens significantly impacts AR model performance, and existing methods lack clear guidelines for defining the optimal order.", "second_pros": "A holistic approach to tokenization can improve global representation and address ordering challenges.  Incorporating an autoregressive prior during training can improve model performance and align the latent space with the requirements of AR models.", "summary": "Current video tokenization methods for autoregressive generative models have limitations due to patchwise tokenization, hindering holistic representation and optimal sequential ordering, and a lack of correlation between reconstruction fidelity and generation quality."}}, {"page_end_idx": 3, "page_start_idx": 3, "section_number": 2, "section_title": "Method", "details": {"details": "LARP employs a **holistic video tokenization** scheme using learned queries to capture global video information, unlike traditional patchwise methods.  This avoids direct patch-to-token mapping, allowing for flexible token numbers.  A lightweight **autoregressive (AR) transformer** is integrated as a training-time prior model,  co-trained with LARP to guide the learning of an AR-friendly latent space. This prior model is discarded during inference.  Stochastic Vector Quantization (SVQ) handles the discrete representation, adding stochasticity for better semantic richness compared to standard VQ. The training objective combines reconstruction loss with the AR prior model's negative log-likelihood,  with scheduled sampling to reduce exposure bias. ", "first_cons": "Traditional patchwise tokenization limits representation to local patch information and requires a flattening order for AR models, potentially suboptimal.", "first_pros": "Holistic tokenization captures global information, improving semantic representation. Flexible token numbers adjust to task needs.", "keypoints": ["Holistic tokenization using learned queries", "Autoregressive prior model for AR-friendly latent space", "Stochastic Vector Quantization for richer discrete representation", "Scheduled sampling for robust training"], "second_cons": "The AR prior model adds complexity to the training process. The choice of AR model architecture and hyperparameters need careful tuning.", "second_pros": "AR prior model improves AR generation performance significantly without adding inference overhead.  The method aligns latent space with downstream AR tasks.", "summary": "LARP, a novel video tokenizer, uses holistic tokenization with a learned autoregressive prior model and stochastic vector quantization for improved autoregressive video generation."}}, {"page_end_idx": 10, "page_start_idx": 4, "section_number": 3, "section_title": "Experiments", "details": {"details": "The experiments section evaluates LARP's performance in video reconstruction and generation tasks using the Kinetics-600 and UCF-101 datasets.  It explores the impact of scaling the tokenizer's size and the number of discrete tokens, showing that increasing the size improves reconstruction but generation performance saturates beyond a certain point.  The study uses the Frech\u00e9t Video Distance (FVD) as a primary metric to measure the quality of the generated videos.  An ablation study shows the importance of the autoregressive (AR) prior model, scheduled sampling, and stochastic vector quantization (SVQ) in achieving state-of-the-art results on the UCF-101 class-conditional video generation benchmark.  Finally, the section presents visual comparisons that demonstrate LARP's superior performance in both reconstruction and generation tasks.", "first_cons": "Scaling the tokenizer beyond a certain point yields diminishing returns in video generation quality.", "first_pros": "LARP achieves state-of-the-art performance on UCF-101 video generation benchmark.", "keypoints": ["State-of-the-art FVD score of 57 on UCF-101 class-conditional video generation.", "Scaling experiments show impact of tokenizer size and number of tokens on reconstruction and generation.", "Ablation study highlights importance of AR prior model, scheduled sampling, and SVQ.", "Visual comparisons demonstrate superior reconstruction and generation quality compared to existing methods."], "second_cons": "The ablation study reveals sensitivity to certain model components, particularly the AR prior model.", "second_pros": "Comprehensive evaluation includes reconstruction and generation tasks, scaling experiments, and ablation studies.", "summary": "Experiments on video reconstruction and generation demonstrate LARP's superior performance, highlighting the importance of its design choices and achieving state-of-the-art results."}}]