[{"figure_path": "2410.21264/figures/figures_1_0.png", "caption": "Figure 1: LARP highlights. (a) LARP is a video tokenizer for two-stage video generative models. In the first stage, LARP tokenizer is trained with a lightweight AR prior model to learn an AR-friendly latent space. In the second stage, an AR generative model is trained on LARP's discrete tokens to synthesize high-fidelity videos. (b) The incorporation of the AR prior model significantly improves the generation FVD (gFVD) across various token number configurations. (c) LARP shows a much smaller gap between its reconstruction FVD (rFVD) and generation FVD (gFVD), indicating the effectiveness of the optimized latent space it has learned.", "description": "Figure 1 shows the LARP framework, the effectiveness of the AR prior model, and a comparison across different autoregressive generative models, highlighting LARP's performance advantages.", "section": "ABSTRACT"}, {"figure_path": "2410.21264/figures/figures_4_0.png", "caption": "Figure 2: Method overview. Cubes \u2611 represent video patches, circles O indicate continuous embeddings, and squares denote discrete tokens. (a) Patchwise video tokenizer used in previous works. (b) Left: The LARP tokenizer tokenizes videos in a holistic scheme, gathering information from the video using a set of learned queries. Right: The AR prior model, trained with LARP predicts the next holistic token, enabling a latent space optimized for AR generation. The AR prior model is forwarded in two rounds per iteration. The red arrow represents the first round, and the purple arrows represent the second round. The reconstruction loss Lrec is omitted for simplicity.", "description": "This figure compares a traditional patchwise video tokenizer with LARP's holistic tokenization approach, highlighting the incorporation of a learned autoregressive prior model for improved AR generation.", "section": "3 METHOD"}, {"figure_path": "2410.21264/figures/figures_9_0.png", "caption": "Figure 4: Video reconstruction comparison with OmniTokenizer (Wang et al., 2024).", "description": "The figure shows a comparison of video reconstruction results between LARP, OmniTokenizer, and ground truth (GT) on two video clips, demonstrating LARP's superior reconstruction quality.", "section": "4.4 Visualization"}, {"figure_path": "2410.21264/figures/figures_10_0.png", "caption": "Figure 4: Video reconstruction comparison with OmniTokenizer (Wang et al., 2024).", "description": "The figure shows a comparison of video reconstruction results between LARP and OmniTokenizer, demonstrating LARP's superior reconstruction quality across various scenes and regions.", "section": "4.4 Visualization"}, {"figure_path": "2410.21264/figures/figures_10_1.png", "caption": "Figure 9: Additional video frame prediction results on K600 dataset.", "description": "The figure shows examples of video frame prediction results on the K600 dataset, demonstrating LARP's ability to accurately predict future frames in diverse scenarios.", "section": "4.4 Visualization"}, {"figure_path": "2410.21264/figures/figures_17_0.png", "caption": "Figure 7: Additional video reconstruction comparison with OmniTokenizer (Wang et al., 2024).", "description": "Figure 7 presents a comparison of video reconstruction results between LARP and OmniTokenizer, showcasing LARP's superior reconstruction quality across diverse scenes.", "section": "4.4 Visualization"}, {"figure_path": "2410.21264/figures/figures_18_0.png", "caption": "Figure 8: Additional class-conditional generation results on UCF-101 dataset.", "description": "The figure shows additional examples of class-conditional video generation results produced by LARP on the UCF-101 dataset, showcasing the model's ability to generate diverse and high-fidelity videos across various action classes.", "section": "4.4 Visualization"}, {"figure_path": "2410.21264/figures/figures_19_0.png", "caption": "Figure 2: Method overview. Cubes \u2611 represent video patches, circles O indicate continuous embeddings, and squares denote discrete tokens. (a) Patchwise video tokenizer used in previous works. (b) Left: The LARP tokenizer tokenizes videos in a holistic scheme, gathering information from the video using a set of learned queries. Right: The AR prior model, trained with LARP predicts the next holistic token, enabling a latent space optimized for AR generation. The AR prior model is forwarded in two rounds per iteration. The red arrow represents the first round, and the purple arrows represent the second round. The reconstruction loss Lrec is omitted for simplicity.", "description": "Figure 2 illustrates the architecture of the proposed LARP tokenizer and compares it to a traditional patchwise tokenizer, highlighting LARP's holistic approach and the integration of an autoregressive prior model for improved AR-friendly latent space learning.", "section": "3 METHOD"}]