{"references": [{"fullname_first_author": "Achiam, J.", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-01", "reason": "This paper is important because it provides a technical report on the GPT-4 model which the current paper uses as an automatic judge."}, {"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-01-01", "reason": "This paper is crucial as it introduces the RLHF technique, which is a central topic in the current paper and a fundamental method for aligning LLMs."}, {"fullname_first_author": "Rafailov, R.", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-01-01", "reason": "This paper is important because it introduces Direct Preference Optimization (DPO), an alternative to RLHF, which is a key benchmark and comparison point for the DVPO method proposed in the current paper."}, {"fullname_first_author": "Schulman, J.", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-01", "reason": "This paper is critical because it describes Proximal Policy Optimization (PPO), the reinforcement learning algorithm upon which the current paper's method (DVPO) is based."}, {"fullname_first_author": "Zheng, L.", "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena", "publication_date": "2023-01-01", "reason": "This paper is important because it introduces MT-Bench, a key benchmark used in the evaluation of the models in the current paper."}]}