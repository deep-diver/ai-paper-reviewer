{"importance": "This paper is important for researchers interested in **efficient LLM alignment**. DVPO's decoupled approach reduces computational costs and improves training stability. It offers a practical alternative to existing RLHF methods and opens new avenues for exploring decoupled optimization techniques in LLMs.", "summary": "DVPO: A lean RLHF framework that decouples value & policy optimization with global value guidance, cutting GPU use by 40% and training time by 35%.", "takeaways": ["Decoupling value and policy optimization in RLHF reduces computational complexity and improves training stability.", "Pretraining a global value model can provide effective guidance for policy optimization in the absence of new reward feedback.", "Decoupled Value Policy Optimization (DVPO) achieves performance comparable to state-of-the-art RLHF methods with improved efficiency."], "tldr": "Reinforcement Learning from Human Feedback (**RLHF**) is vital for aligning large language models (**LLMs**) with human preferences. However, traditional RLHF methods using Proximal Policy Optimization (**PPO**) suffer from high computational complexity and training instability. This is due to the joint training of an actor and critic, as well as the lack of access to ground-truth rewards in LLM tasks.  Existing methods like Direct Preference Optimization (**DPO**) lack the iterative refinement of RL and struggles with distribution shifts.\n\nTo solve this, the paper introduces Decoupled Value Policy Optimization (**DVPO**), a lean framework that replaces traditional reward modeling with a pretrained global value model (**GVM**). The **GVM** is conditioned on policy trajectories and predicts token-level return-to-go estimates. **DVPO** eliminates actor-critic interdependence. Experiments show that **DVPO** achieves performance comparable to existing RLHF approaches while improving memory usage and training time.", "affiliation": "Microsoft", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "2502.16944/podcast.wav"}