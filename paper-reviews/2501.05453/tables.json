[{"content": "| Model | Params | Dimension | Heads | Layers |\n|---|---|---|---|---|\n| base | 120m | 768 | 12 | 12 |\n| large | 280m | 1024 | 16 | 16 |\n| 1b | 1.1b | 2048 | 16 | 22 |", "caption": "Table 1: Model Architecture: We pre-train models at different scales, only on visual tokens from images and videos.", "description": "This table details the architecture of the autoregressive video models (called Toto) used in the study. It shows the model's parameters (in millions), the dimensionality of the embedding, the number of attention heads, and the number of layers in the transformer network.  The table highlights that the models are trained at different scales, with varying numbers of parameters, to analyze the impact of scale on performance. All models are trained solely on visual tokens derived from images and videos.", "section": "3.2 Architecture"}, {"content": "| Datasets | Instances | Tokens | Hours |\n|---|---|---|---|\n| ImageNet | 13.9M | 3.6B | - |\n| Kinetics-600 | 0.53M | 41.3B | 1496 |\n| Ego4D | 52.1K | 103B | 3750 |\n| HowTo100m | 1.172M | 2560B | 92627 |", "caption": "Table 2: Pre-training Dataset: We use both image datasets (Imagenet\u00a0(Russakovsky et\u00a0al., 2015)) and video datasets (Kinetics600\u00a0(Carreira et\u00a0al., 2019), Ego4D\u00a0(Grauman et\u00a0al., 2022), HowTo100m\u00a0(Miech et\u00a0al., 2019)) with different mixing ratios during the pre-training of our models. The whole training data contains about 100,000 hours of videos.", "description": "This table details the datasets used for pre-training the video models.  It lists four datasets: ImageNet (a large image dataset), Kinetics-600 (a dataset of short video clips with action labels), Ego4D (a large-scale egocentric video dataset), and HowTo100M (a dataset of how-to videos with text descriptions). For each dataset, the table provides the number of instances (images or videos), the total number of visual tokens generated from them, and the approximate number of hours of video content. Note that the datasets were combined in different ratios during training, and the total training data comprised over 100,000 hours of video and roughly 1 trillion tokens.  This demonstrates the massive scale of data used to pre-train the autoregressive video models.", "section": "3.3 Dataset"}, {"content": "| Input-Target | Tokens | Vocabulary | Top1 |\n|---|---|---|---| \n| VQGAN-VQGAN | 16x16 | 16k | 61.3 |\n| VQGAN-VQGAN | 16x16 | 1k | 61.1 |\n| dVAE-dVAE | 32x32 | 8k | 61.2 |\n| dVAE-dVAE | 16x16 | 8k | 53.2 |\n| patch-patch | 16x16 | - | 60.6 |\n| patch-dVAE | 16x16 | 8k | 58.5 |", "caption": "Table 3: ImageNet Linear Probing Accuracy with Various Tokenizers: We compare discrete (dVAE, VQGAN) and patch embedding as input and target for pre-training our models. ImageNet top-1 accuracies are computed by linear probing at the 9th layer of the large model.", "description": "This table presents the results of an experiment comparing different tokenization methods for training a vision transformer model.  The goal was to determine the impact of various tokenization techniques on the model's performance on ImageNet image classification. The experiment used three types of tokenizers: discrete tokenizers (dVAE and VQGAN), and a patch-based continuous tokenizer.  For each tokenizer, the model was pre-trained, then its performance on ImageNet was evaluated using linear probing on the 9th layer of a 'large' model.  The table shows the top-1 accuracy achieved for each combination of input/target tokenizer and indicates that the choice of tokenizer has a relatively minor impact on performance.", "section": "4.1 Design Choices"}, {"content": "| Method | Compute | Top1 |\n|---|---|---|\ndVAE/16 | 1.42 \u00d7 10<sup>17</sup> | 53.2 |\ndVAE/32 | 5.68 \u00d7 10<sup>17</sup> | 61.2 |\ndVAE/16 \u2192 32 | 2.13 \u00d7 10<sup>17</sup> | 63.2 |\ndVAE/16 \u2192 32<sup>\u2020</sup> | 2.13 \u00d7 10<sup>17</sup> | 64.4 |", "caption": "Table 6: Architecture: We compare sequence modeling architectures LLaMA\u00a0Touvron et\u00a0al. (2023), GPT2\u00a0Radford et\u00a0al. (2019), and non-transformer models, Mamba\u00a0Gu & Dao (2023) on ImageNet linear probing task.", "description": "This table compares the performance of different neural network architectures on the ImageNet image classification task using a linear probing method.  The architectures compared include the LLaMA transformer model, the GPT-2 transformer model, and the non-transformer Mamba model. The table shows the number of parameters in each model and its top-1 accuracy on ImageNet after linear probing. This allows for a comparison of the effectiveness of different architectural designs for visual representation learning.", "section": "4.1 Design Choices"}, {"content": "| Method | Tokens | Pooling | Top1 |\n|---|---|---|---| \n| dVAE | 16x16 | Average | 53.2 |\n| dVAE | 16x16 | Attention | 61.1 |", "caption": "Table 7: ImageNet Results: We compare discriminative and generative models on ImageNet\u00a0(Deng et\u00a0al., 2009) recognition task. While achieving comparable performance among generative models, our models model achieves the highest accuracy on autoregressive modeling. \u2020models are evaluated with linear probing.", "description": "Table 7 presents the ImageNet classification results, comparing various discriminative and generative vision models.  The table highlights the top-1 accuracy achieved by each model.  It's particularly noteworthy that the proposed Toto models achieve the highest accuracy among the autoregressive generative models, demonstrating the effectiveness of the autoregressive pre-training approach. Models marked with \u2020 were evaluated using linear probing, meaning their performance was assessed by attaching a simple linear layer on top of the model's extracted features, without further fine-tuning.", "section": "4.2 Image Recognition"}, {"content": "| Model | Params | Top1 |\n|---|---|---|\n| GPT2 [Radford et al. (2019)] | 280 m | 48.5 |\n| Mamba [Gu & Dao (2023)] | 290 m | 40.7 |\n| LLaMA [Touvron et al. (2023)] | 280 m | 53.2 |", "caption": "Table 8: K400 Results: We compare discriminative and generative models on Kinetics-400\u00a0(Kay et\u00a0al., 2017) action recognition task. While achieving comparable performance among generative models, our models are the first to show the competitive performance on K400 with autoregressive pre-training, and shows scaling with large model sizes.", "description": "Table 8 presents a comparison of various models' performance on the Kinetics-400 action recognition dataset.  It contrasts discriminative and generative approaches, highlighting the performance of the proposed 'Toto' models.  A key takeaway is that while comparable to other generative models, Toto demonstrates competitive results, especially notable as the first to achieve such performance using an autoregressive pre-training method.  Furthermore, the table shows that the performance of Toto models scales positively with model size.", "section": "4 Experiments"}, {"content": "| Method | Arch | #\u03b8 | Top1 |\n|---|---|---|---| \n| *Discriminative Approaches* |  |  |  |\n| SimCLR (Chen et al., 2020b)\u2020 | RN50x2 | 94 | 74.2 |\n| BYOL (Grill et al., 2020)\u2020 | RN50x2 | 94 | 77.4 |\n| SwAV (Caron et al., 2020)\u2020 | RN50x2 | 94 | 73.5 |\n| DINO (Caron et al., 2021) | ViT-B/8 | 86 | 80.1 |\n| DINOv2 (Oquab et al., 2023) | ViT-g/14 | 1011 | 86.4 |\n| *Generative Approaches* |  |  |  |\n| BEiT-L (Bao et al., 2021) | ViT-L/14 | 307 | 62.2 |\n| AIM (El-Nouby et al., 2024) | ViT-1B/14 | 1200 | 80.6 |\n| MAE (He et al., 2022) | ViT-H/14 | 632 | 80.9 |\n| iGPT-L (Chen et al., 2020a)\u2020 | GPT-2 | 1386 | 65.2 |\n| iGPT-XL (Chen et al., 2020a)\u2020 | GPT-2 | 6801 | 72.0 |\n| *Toto*-base | LLaMA | 120 | 64.7 |\n| *Toto*-large | LLaMA | 280 | 71.1 |\n| *Toto*-1b | LLaMA | 1100 | 75.3 |", "caption": "Table 9: Ego4D Results: Our model achieves comparable mean-average precision compared to previous work. We compare our method with, FRCNN+Rnd\u00a0(Grauman et\u00a0al., 2022), FRCNN+SF\u00a0(Grauman et\u00a0al., 2022), Hiera\u00a0(Ryali et\u00a0al., 2023), StillFast\u00a0(Ragusa et\u00a0al., 2023), VideoMAE\u00a0(Wang et\u00a0al., 2023a), and MAE-ST\u00a0(Feichtenhofer et\u00a0al., 2022).", "description": "This table presents a comparison of the performance of different methods on the Ego4D action anticipation dataset.  The metric used is mean average precision (mAP), broken down into three components: noun prediction, noun+verb prediction, and noun+verb+time-to-contact prediction. The table compares the proposed 'Toto-large' model to several state-of-the-art methods, including FRCNN with random and StillFast baselines from the original Ego4D paper, as well as more recently published models like Hiera and VideoMAE. This allows for an evaluation of the proposed model's performance relative to existing approaches on this challenging video prediction task.", "section": "4.4 Action Forecasting"}, {"content": "| Method | Arch | Top1 |\n|---|---|---|\n| *Discriminative Approaches* |  |  |\n| I-JEPA (Assran et al., 2023) | ViT-H/16 | 74.5 |\n| OpenCLIP (Cherti et al., 2023) | ViT-G/14 | 83.3 |\n| DINOv2 (Oquab et al., 2023) | ViT-g/14 | 84.4 |\n| InternVideo (Wang et al., 2022) | - | 73.7 |\n| *Generative Approaches* |  |  |\n| Hiera (Ryali et al., 2023) | Hiera-H/14 | 77.0 |\n| MVD (Wang et al., 2023b) | ViT-H/14 | 79.4 |\n| VideoMAE (Wang et al., 2023a) | ViT-L/14 | 79.8 |\n| *Toto*-base | LLaMA | 59.3 |\n| *Toto*-large | LLaMA | 65.3 |\n| *Toto*-1b | LLaMA | 74.4 |", "caption": "Table 10: DAVIS Tracking: We report J, F, and J&F scores at the peak layers of each model. We achieves comparable performance as DINO and at large resolution (512), it outperforms all methods.", "description": "This table presents the results of a video tracking experiment on the DAVIS dataset.  Specifically, it shows the performance of different models (Toto-base, Toto-large, Toto-1b, DINO-base, and MAE-base) in terms of J, F, and J&F scores.  The J and F scores represent the Jaccard and F1 scores, respectively, which are common metrics for evaluating object segmentation. The J&F score is the average of the J and F scores.  Results are reported for different model resolutions (256x8 and 512x8) and patch sizes. The table highlights that the Toto models, particularly the larger models at higher resolution, achieve comparable performance to DINO and even surpasses other methods.", "section": "4.5 Video Tracking"}, {"content": "| Method | Noun | N+V | N+TTC | Overall |\n|---|---|---|---|---|\n| FRCNN+Rnd (Grauman et al., 2022) | 17.55 | 1.56 | 3.21 | 0.34 |\n| FRCNN+SF (Grauman et al., 2022) | 17.55 | 5.19 | 5.37 | 2.07 |\n| Hiera-large (Ryali et al., 2023) | 14.05 | 6.03 | 4.53 | 2.12 |\n| StillFast (Ragusa et al., 2023) | 16.20 | 7.47 | 4.94 | 2.48 |\n| VideoMAE-large (Wang et al., 2023a) | 15.16 | 6.72 | 5.26 | 2.55 |\n| MAE-ST-large (Feichtenhofer et al., 2022) | 13.71 | 6.63 | 4.94 | 2.60 |\n| Toto-large | 15.20 | 6.75 | 5.41 | 2.70 |", "caption": "Table 11: Robotics, Real-world Experiments: We compare MVP\u00a0(Radosavovic et\u00a0al., 2022) and Toto on a Franka cube-picking task in the real world. Features from both models are pre-trained, frozen, and passed into a learning module trained with behavior cloning using the same demonstrations.\nWe see that our approach performs comparably to the state-of-the-art vision backbone for robotics, despite not being designed with the robotic application in mind.", "description": "This table presents a comparison of the performance of two different methods on a real-world robotic cube-picking task.  The methods compared are MVP (a state-of-the-art vision model for robotics) and Toto (the autoregressive video model introduced in this paper).  Both models' visual features were pre-trained and then frozen; these features were fed into a learning module trained using behavior cloning with identical demonstrations for both models. The results show that Toto's performance is comparable to MVP's, demonstrating that a model not specifically designed for robotics can still achieve state-of-the-art results when leveraging effective pre-trained visual features.", "section": "4.6 Robotics"}, {"content": "| Method (Res/Patch) | J&F | J | F |\n|---|---|---|---| \n| DINO-base (224/8) | 54.3 | 52.5 | 56.1 |\n| DINO-base (224/16) | 33.1 | 36.2 | 30.1 |\n| MAE-base (224/16) | 31.5 | 34.1 | 28.9 |\n| *Toto*-base (256/8) | 42.0 | 41.2 | 43.1 |\n| *Toto*-large (256/8) | 44.8 | 44.4 | 45.1 |\n| *Toto*-1b (256/8) | 46.1 | 45.8 | 46.4 |\n| *Toto*-large (512/8) | 62.4 | 59.2 | 65.6 |", "caption": "Table 12: Object Permanence: CATER\u00a0(Girdhar & Ramanan, 2019) object localization task, where the object is hidden under or obstructed by other objects. The model is trained to predict its coarse location. Our model performs better than previous methods on snitch localization task at 16, 32 temporal resolutions.", "description": "This table presents the results of the object permanence task from the CATER dataset (Girdhar & Ramanan, 2019).  The task involves predicting the coarse location of an object that is hidden or occluded by other objects in a scene.  The table compares the performance of the proposed Toto model against previous methods. Performance is evaluated at two different temporal resolutions (16 and 32 frames), demonstrating the Toto model's superior performance in object localization even when the object is not directly visible.", "section": "4.7 Object Permanence"}, {"content": "| Model | # Traj | Success |\n|---|---|---|\n| MVP | 240 | 75% |\n| Toto-base | 240 | 63% |", "caption": "Table 13: Full Fine Tuning Performance: Comparison of different methods performance on ImageNet-1K.", "description": "This table presents a comparison of the top-1 accuracy achieved on the ImageNet-1K dataset by several different self-supervised learning methods after full fine-tuning.  The methods compared include DINO, MoCoV3, BEIT, MAE, and the Toto model (the model presented in this paper). The results show the top-1 accuracy achieved by each method.", "section": "4.2 Image Recognition"}, {"content": "| Method | Model | 16 | 32 |\n|---|---|---|---| \n| V3D | ResNet | 55.2 | 69.7 |\n| TFC V3D | ResNet | 54.6 | 70.2 |\n| *Toto*-large | LLaMa | 62.8 | 72.9 |", "caption": "Table 14: ImageNet Linear Probing Results: Toto performs better than similar size iGPT models.", "description": "This table presents the results of linear probing experiments conducted on the ImageNet dataset, comparing the performance of the Toto model with that of the iGPT model. Linear probing is a method used to evaluate the quality of learned visual representations by adding a linear classifier on top of the frozen feature representations. The table shows that the Toto model, despite having a similar number of parameters, achieves higher accuracy on ImageNet than the iGPT model, demonstrating the effectiveness of the Toto model in learning robust and generalizable visual representations.", "section": "4.2 Image Recognition"}, {"content": "| DINO | MoCo v3 | BEiT | MAE | Toto |\n|---|---|---|---|---|\n| 82.8 | 83.2 | 83.2 | 83.6 | 82.6 |", "caption": "Table 16: K400 Results: We evaluate our models using cross attention and MLP layer as the classification head. Overall using a high-capacity head improves the performance across all models.", "description": "This table presents the results of evaluating various models on the Kinetics-400 (K400) action recognition dataset.  The key difference from previous evaluations is that a more complex classification head was used, incorporating cross-attention and an MLP layer, to enhance the model's capacity to learn more complex features. The results show improved performance across all models when this enhanced classification head is applied, highlighting the benefit of increasing model capacity for action recognition.", "section": "4.3 Action Recognition"}]