[{"figure_path": "https://arxiv.org/html/2501.05453/x1.png", "caption": "Figure 1: Overall Framework. Starting with images and video frames from a collection of datasets, we tokenize each frame/image into discrete visual tokens independently.\nWe pre-train the transformer by predicting the next visual tokens, with a context length of 4K tokens of images or video frames. Once trained, we take the intermediate representations and evaluate them on various tasks.", "description": "This figure illustrates the overall framework of the research, detailing the process from data acquisition to downstream task evaluation.  The process begins with collecting images and video frames from various datasets. Each image and frame is then independently tokenized into a sequence of discrete visual tokens. A transformer model is pre-trained using these tokens, predicting the next token in the sequence with a context length of 4,000 tokens (which is equivalent to around 16 images or video frames).  After pre-training, the intermediate learned representations from the transformer model are extracted and used to evaluate the model's performance on a wide range of downstream tasks, such as image recognition, video classification, object tracking, and robotics.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/toto_blue.png", "caption": "Figure 2: Training Loss Curves: We show the training loss curves for base, large, and 1b models trained with tokens from dVAE\u00a0(Ramesh et\u00a0al., 2021) with a vocabulary size of 8k and context length of 4k tokens (equivalent to 16 images or video frames).", "description": "This figure displays the training loss curves for three different sized autoregressive video models (Toto): a base model, a large model, and a 1 billion parameter model.  The models were trained using visual tokens generated by a discrete variational autoencoder (dVAE) with a vocabulary size of 8,000 tokens.  Each model's training data consisted of sequences of 4,000 tokens, which equates to roughly 16 images or video frames in length.  The graph shows how the training loss decreases as the number of training tokens increases for each model, indicating the models' learning progress during pre-training.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/tokens1gram_blue.png", "caption": "Figure 3: 1-gram Distribution of Various Tokens: This Figure shows the distribution of 1-gram tokens of various tokenizers (dVAE\u00a0(Ramesh et\u00a0al., 2021), VQGAN-1k, VQGAN-16k\u00a0(Esser et\u00a0al., 2020)) on Imagenet validation set. Note that, dVAE has almost full convergence of the tokens while VQGAN has less than 50% coverage of the tokens.", "description": "Figure 3 presents a comparison of the token distributions generated by three different tokenizers: dVAE, VQGAN-1k, and VQGAN-16k.  The histograms illustrate the frequency of each unique visual token (1-gram) across the ImageNet validation set.  The key observation is the difference in token coverage. dVAE exhibits a much more uniform distribution, with a near-complete coverage of its vocabulary.  In contrast, both VQGAN-1k and VQGAN-16k show a significantly less uniform distribution, indicating that a substantial portion of their token vocabularies are underutilized in representing the ImageNet data. This implies that dVAE's tokenizer may provide more comprehensive coverage of visual features compared to VQGAN.", "section": "4.1 Design Choices"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/robots/franka_pick_toto_blue.png", "caption": "Table 4: Token Resolution: While the performance is lower for a low-resolution model, when finetuned for next-patch prediction at a higher resolution, its performance surpasses the full-resolution pre-trained model. \u2020\u2020{}^{\\text{\\textdagger}}start_FLOATSUPERSCRIPT \u2020 end_FLOATSUPERSCRIPT Base values of the RoPE is 50,000.", "description": "This table presents the results of an experiment comparing the performance of autoregressive video models trained at different resolutions.  The models were initially trained at either low (128x128) or high (256x256) resolution using discrete visual tokens.  The key finding is that even though the low-resolution model initially performed worse, fine-tuning it for next-patch prediction at a higher (256x256) resolution resulted in superior performance compared to the model that was trained directly at the high resolution.  This highlights the benefit of starting with lower-resolution training and then subsequently fine-tuning at higher resolution, potentially reducing computational cost without compromising performance.  The base value for the Rotary Positional Embeddings (ROPE) was set to 50,000 during training.", "section": "4.1 Design Choices"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/robots/kuka_pick_toto_blue.png", "caption": "Table 5: Attention vs Average Pooling: When probed at the same layers, attention pooling performs much better than average pooling of intermediate tokens.", "description": "This table compares the performance of attention pooling and average pooling when extracting features from intermediate layers of a pre-trained model for downstream tasks.  It shows that attention pooling, which weights tokens based on their importance, significantly outperforms average pooling, which treats all tokens equally.", "section": "4.1 Design Choices"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/robots/franka_cabinet_toto_blue.png", "caption": "Figure 4: Probing at Different Layers: We show the attention-probing performance at each layer of our three models. Peak performance is observed at around 50% depth of the models.", "description": "This figure visualizes the results of an experiment evaluating the performance of different layers within three autoregressive video models (Toto-base, Toto-large, and Toto-1b) on the ImageNet image classification task.  The x-axis represents the relative layer depth within each model (0.0 representing the first layer and 1.0 the last), and the y-axis shows the classification accuracy achieved using attention pooling at each layer.  The plot demonstrates that for all three models, the highest accuracy is achieved at approximately the 50% mark of the total layer depth, indicating that the most informative features for this task are located in the middle layers of the network. The observation of peak performance in middle layers across different model scales suggests an optimal depth for capturing both local and global contextual information.", "section": "4.1 Design Choices"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/robots/kuka_cabinet_toto_blue.png", "caption": "Figure 5: Semi-Supervised Tracking: We follow the protocol in STC\u00a0(Jabri et\u00a0al., 2020), start with the GT segmentation mask, and propagate the labels using the features computed by Toto-large. The mask was propagated up to 60 frames without losing much information.", "description": "This figure demonstrates semi-supervised video object tracking using the Toto-large model.  Following the methodology outlined in Jabri et al. (2020), the process begins with a ground truth (GT) segmentation mask.  The model then leverages its learned feature representations to propagate these labels forward in time. The results showcase the model's ability to maintain accurate tracking across a sequence of 60 frames, highlighting the effectiveness of the model's features even without explicit supervision in the tracking task.", "section": "4.5 Video Tracking"}, {"figure_path": "https://arxiv.org/html/2501.05453/x2.png", "caption": "(a) Franka Pick", "description": "This figure shows the mean success rate over training steps for a Franka robot performing a pick task.  The success rate is plotted against the number of training steps. Two models, Toto-base and MVP-base, are compared, demonstrating that Toto-base learns the task faster than MVP-base.", "section": "4.6 Robotics"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot2x_vgpt_blue.png", "caption": "(b) Kuka Pick", "description": "This figure is a plot showing the mean success rate over training steps for a Kuka Pick task in a robot manipulation experiment using reinforcement learning.  The plot compares the performance of a Toto-base model against a MVP-base model. The x-axis represents the number of training steps, and the y-axis represents the mean success rate.  The plot visually demonstrates the learning progress of each model on this specific robotic task.", "section": "4.5 Video Tracking"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot3_toto_blue.png", "caption": "(c) Franka Cabinet", "description": "This figure shows the results of a robot manipulation experiment using reinforcement learning.  Specifically, it displays the mean success rate over training steps for a Franka robot performing a cabinet-opening task. The graph likely compares the performance of the Toto-base model to a baseline model (potentially MAE-base) to illustrate the improved learning efficiency and success rate achieved with the pre-trained Toto model.", "section": "4.5 Video Tracking"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot4_toto_blue.png", "caption": "(d) Kuka Cabinet", "description": "The figure shows the mean success rate over training steps for a Kuka Cabinet task in robot manipulation experiments using reinforcement learning.  It compares the performance of a Toto-base model against a MAE-base model, illustrating the Toto model's faster learning and improved sample efficiency.", "section": "4.5 Video Tracking"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot5_vgpt_blue.png", "caption": "Figure 6: Robot Manipulation with Reinforcement Learning: We compare MAE-base\u00a0(Radosavovic et\u00a0al., 2022) with Toto-base pre-trained models in simulation following\u00a0Xiao et\u00a0al. (2022). We evaluate each model the mean success rate over training steps. Toto was able to learn these tasks faster than MAE, across two robots and two tasks.", "description": "Figure 6 presents a comparison of the learning performance between MAE-base (a previously published model) and Toto-base (the model introduced in this paper) on robot manipulation tasks within a simulated environment.  The experiments follow the methodology outlined in Xiao et al. (2022). The figure displays the mean success rate achieved by each model across different training steps for four tasks: Franka-Pick, Kuka-Pick, Franka-Cabinet, and Kuka-Cabinet.  Two different robot arms (Franka and Kuka) are involved. The results demonstrate that Toto-base learns these robotic manipulation tasks more efficiently (i.e., in fewer training steps) than MAE-base across both robot types and all four tasks.", "section": "4.6 Robotics"}, {"figure_path": "https://arxiv.org/html/2501.05453/x3.png", "caption": "Figure 7: Real-world Deployment: We show an example episode of our policy performing the cube picking task on a Franka robot in the real world. We use Toto-base to run the robot at real time, despite being a small model, Toto was able to achieve about 63% success rate in real world setting.", "description": "This figure showcases a real-world application of the Toto-base model in a robotic manipulation task.  Specifically, it shows an example sequence of a Franka robot performing a cube-picking task.  The key takeaway is that despite its relatively small size, the Toto-base model enables real-time control of the robot, achieving a success rate of approximately 63% in this challenging real-world environment. This demonstrates the effectiveness and efficiency of the autoregressive pre-training approach used in developing the Toto models for real-world robotic applications.", "section": "4.6 Robotics"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/toto-large-k400-val-set.png", "caption": "Figure 8: Probing Across Layers, Models, and Tasks: We study the behavior of our models across multiple layers and tasks. For image classification, action recognition, and object tracking, all the models behave similarly and peak around 50% of the model depth. This behavior is observed across all model sizes. Robot tasks show a similar behaviour, where the middle layers perform good at picking the objects, but last layers also perform good as middle layers. These plots suggests, in decoder-only model, first half of the model starts to behave like an encoder, and compress the information, and then rest of the model, projects the compressed semantic features back to input space.", "description": "This figure visualizes the performance of different layers within the model across various tasks (ImageNet classification, Kinetics action recognition, DAVIS object tracking, and robot manipulation).  For ImageNet, Kinetics, and DAVIS, peak performance is consistently observed around the middle layers (approximately 50% of the total depth), regardless of model size.  Interestingly, robot manipulation tasks show a different pattern, with both middle and later layers exhibiting strong performance. This suggests that in decoder-only models, the initial layers function as an encoder, compressing information before the later layers project the compressed features back into the input space.  The results highlight the distinct roles of different layers and their varying suitability across diverse tasks.", "section": "4.8 Probing Across Layers"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/tokens2gram_blue.png", "caption": "Figure 9: Scaling Toto: We train multiple variants of Toto, with increasing hidden size and depth, with optimal learning rates. We plot the validation loss vs the compute spent on training in MACs. This shows a clear scaling behavior with optimal compute.", "description": "This figure demonstrates the scaling behavior of the Toto model family.  Multiple versions of the model were trained with varying hidden sizes and depths; for each, the optimal learning rate was determined. The plot shows the validation loss against the total compute (measured in Multiply-Accumulate operations, or MACs) used during training.  The graph clearly illustrates how increasing compute resources leads to lower validation loss, demonstrating the scaling efficiency of the models.", "section": "4.9 Compute Optimal Scaling"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/tokens3gram_blue.png", "caption": "Figure 10: Average Validation Loss Over Tokens: We show the average loss per token for kinetics validation set. It clearly shows the redundancy in videos, as the first frame has higher prediction loss, and rest of the frames on average has lower loss than the first frame.", "description": "Figure 10 illustrates the average validation loss per token calculated on the Kinetics-400 validation dataset. The graph reveals a pattern of redundancy in video data.  The first frame in a video sequence exhibits a significantly higher average loss than subsequent frames. This is because the model has to learn the overall content and context of the video from the very first frame, facing a greater challenge in prediction.  In contrast, later frames benefit from the previously established context and temporal relationships, making prediction easier.  The lower loss for subsequent frames demonstrates the inherent redundancy present in videos. This observation suggests that the model can more easily predict later frames due to the established context of prior frames. This effect highlights a key difference between video data and textual data, where the sequential nature of language makes each word more dependent on the context that precedes it.", "section": "3.1 Pre-training"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/frames1.png", "caption": "Table 15: Toto Varients: We scale Toto models by increasing hidden dimension and number of layers linearly while keeping number of heads constant following (Yang et\u00a0al., 2022; Touvron et\u00a0al., 2023).", "description": "This table details the specifications of six different variations of the Toto model.  These variations are created by systematically scaling up the model's hidden dimension and number of layers, while maintaining a constant number of attention heads.  This scaling approach follows the methods described by Yang et al. (2022) and Touvron et al. (2023), allowing for a systematic exploration of the effects of model size on performance and resource utilization.", "section": "4.9 Compute Optimal Scaling"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/frames2.png", "caption": "Figure 11: \u03bc\ud835\udf07\\muitalic_\u03bc-Parameterization Learning Rate: We show that \u03bc\ud835\udf07\\muitalic_\u03bc-Parameterization\u00a0(Yang et\u00a0al., 2022), we can train all width Toto models, with an single optimal learning rate of 2\u22127superscript272^{-7}2 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT.", "description": "This figure demonstrates the effectiveness of \u03bc-parameterization in finding a single optimal learning rate for various Toto model widths.  The x-axis represents the learning rate, ranging from 2<sup>-2</sup> to 2<sup>-7</sup>, while the y-axis shows the minimum validation loss achieved for each model width.  Each curve represents a different model width, and it shows that despite the differing model complexities, a single optimal learning rate range (2<sup>-2</sup> to 2<sup>-7</sup>) minimizes validation loss across all model sizes. This highlights the utility of \u03bc-parameterization in simplifying the training process for various model scales.", "section": "4.9 Compute Optimal Scaling"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/frames3.png", "caption": "Figure 12: 2-gram Distribution of Various Tokens: We compute the 2-gram distribution on 10000 images from the ImageNet validation set. Compared to VQGAN 1k and 16k vocabulary tokenizers, the dVAE tokenizer has a larger set of token combinations.", "description": "This figure compares the 2-gram distributions of visual tokens generated by three different tokenizers: dVAE, VQGAN-1k, and VQGAN-16k.  The analysis is performed on 10,000 images from the ImageNet validation set. The histograms display the frequency of each unique pair of consecutive tokens (2-grams). The figure illustrates that the dVAE tokenizer produces a significantly broader range of 2-gram combinations compared to both variants of the VQGAN tokenizer, suggesting a more diverse and richer representation of visual information.", "section": "3.4 Tokenization"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot2_extra.png", "caption": "Figure 13: 3-gram Distribution of Various Tokens: We compute the 3-gram distribution on 10000 images from the ImageNet validation set. All the tokenizers has similar almost flat distribution when it comes to 3-gram tokens.", "description": "Figure 13 compares the distribution of 3-grams for different visual tokenizers (dVAE, VQGAN-1k, and VQGAN-16k) trained on 10,000 images from the ImageNet validation set.  The 3-gram distributions show that all three tokenizers have similar distributions, largely flat, indicating a lack of strong sequential patterns or dependencies between tokens at this level.", "section": "3.4 Tokenization"}]