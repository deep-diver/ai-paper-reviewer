[{"figure_path": "https://arxiv.org/html/2502.14905/extracted/6211719/image3.png", "caption": "Figure 1: \u201dThink inside the JSON\u201d pipeline", "description": "This figure illustrates the pipeline of the ThinkJSON method. It starts with creating synthetic JSON data using the Qwen 14B/32B model. This data includes triplets of unstructured text, blank JSON schema, and structured JSON. Then, it trains an R1-Zero model using Qwen 2.5 1.5B with the Group Relative Policy Optimization (GRPO) algorithm and custom rewards. The schema mapping for JSON fields is done via reasoning using DeepSeek R1 Qwen 32B. Next, reinforcement learning is performed on the R1-Zero model, followed by supervised fine-tuning (SFT) with Qwen 32B on an enhanced reasoning dataset. Finally, it results in an improved R1 model.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.14905/extracted/6211719/image1.png", "caption": "Figure 2: GRPO Training Metrics", "description": "This figure displays the training metrics for the Group Relative Policy Optimization (GRPO) algorithm used in the paper.  It shows plots of various metrics over training steps, including rewards (equation, format), completion length, and learning rate.  These plots illustrate the algorithm's performance during training and how different reward functions contribute to its optimization. The plots provide insights into the convergence of GRPO towards effective schema adherence.", "section": "3.3 GRPO Training on a Small Foundation Model"}, {"figure_path": "https://arxiv.org/html/2502.14905/extracted/6211719/image4.png", "caption": "Figure 3: SFT Training Metrics", "description": "This figure displays the training metrics for the supervised fine-tuning (SFT) stage of the ThinkJSON model.  The four subplots show the progression of various metrics during SFT training: training loss, learning rate, training epoch, and gradient norm. These metrics provide insights into the model's learning process during SFT, helping to assess its convergence, stability, and overall performance after reinforcement learning.", "section": "3.4 Supervised Fine-Tuning"}, {"figure_path": "https://arxiv.org/html/2502.14905/extracted/6211719/image2.png", "caption": "Figure 4: Performance Comparison", "description": "This figure compares the performance of five different LLMs on a structured data extraction benchmark.  The models compared are ThinkJSON (the model proposed in this paper), the original DeepSeek R1, two distilled versions of DeepSeek R1 using Qwen-1.5B and Qwen-7B, and Gemini 2.0 Flash. The benchmark involved 6.5K rows, each aimed at producing a valid JSON object. The metrics used for comparison include: Rows With No Output (number of rows with no structured output), Rows With Valid JSON (number of rows resulting in valid JSON objects), Mean Match Percentage (average proportion of correctly mapped fields), and Mean Noise Percentage (average proportion of extraneous or malformed tokens).  The graph visually represents these metrics for each model, allowing for a direct comparison of their effectiveness in structured output generation.", "section": "4 Evaluation"}]