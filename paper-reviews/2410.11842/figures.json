[{"figure_path": "2410.11842/figures/figures_3_0.png", "caption": "Figure 1: A high-level comparison between the multi-head attention and our proposed mixture-of-head attention. Subfigure (a) illustrates a standard multi-head attention layer with h attention heads, while subfigure (b) demonstrates the Mixture-of-Head attention (MoH) architecture. It is important to note that MoH does not increase the number of attention heads, ensuring that the total parameter for MoH is comparable to that of the multi-head attention.", "description": "Figure 1 is a high-level comparison of the standard multi-head attention and the proposed Mixture-of-Head Attention (MoH) architecture, showing that MoH does not increase the number of attention heads but rather uses a weighted summation of selected attention heads, thus improving efficiency.", "section": "3 METHODOLOGY"}, {"figure_path": "2410.11842/figures/figures_3_1.png", "caption": "Figure 1: A high-level comparison between the multi-head attention and our proposed mixture-of-head attention. Subfigure (a) illustrates a standard multi-head attention layer with h attention heads, while subfigure (b) demonstrates the Mixture-of-Head attention (MoH) architecture. It is important to note that MoH does not increase the number of attention heads, ensuring that the total parameter for MoH is comparable to that of the multi-head attention.", "description": "Figure 1 provides a high-level comparison of standard multi-head attention and the proposed Mixture-of-Head Attention (MoH) architecture, highlighting the difference in head routing and summation.", "section": "3 METHODOLOGY"}]