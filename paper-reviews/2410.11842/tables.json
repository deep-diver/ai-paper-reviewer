[{"figure_path": "2410.11842/tables/table_5_0.html", "caption": "Table 1: Comparisons to current state-of-the-art methods on ImageNet-1K classification. All models are trained exclusively on the ImageNet-1K training set. Our MoH-ViT models, based on TransNeXt (Shi, 2024), are trained for 300 epochs using a resolution of 224x224. To ensure a fair comparison, we only replace the standard multi-head attention with our Mixture-of-Head attention (MoH), keeping all other training parameters identical to TransNeXt.", "description": "Table 1 compares the performance of MoH-ViT models against other state-of-the-art models on the ImageNet-1K classification benchmark, highlighting the impact of reducing the number of activated attention heads.", "section": "4.1 ViT for Image Classification"}, {"figure_path": "2410.11842/tables/table_6_0.html", "caption": "Table 2: Comparisons to DiT on the benchmarking of class-conditional image generation on ImageNet-1K at 256\u00d7256 resolution. To ensure a fair comparison, we only replace the standard multi-head attention with the MoH in MoH-DiT models, while keeping all other training parameters identical to DiT. \"400K\" denotes the training budget is 400K training steps.", "description": "Table 2 presents a comparison of the proposed Mixture-of-Head Attention (MoH) method against the baseline DiT models on the task of class-conditional image generation, showing that MoH achieves comparable or better performance with fewer activated heads.", "section": "4.2 DiT for Class-Conditional Image Generation"}, {"figure_path": "2410.11842/tables/table_6_1.html", "caption": "Table 1: Comparisons to current state-of-the-art methods on ImageNet-1K classification. All models are trained exclusively on the ImageNet-1K training set. Our MoH-ViT models, based on TransNeXt (Shi, 2024), are trained for 300 epochs using a resolution of 224x224. To ensure a fair comparison, we only replace the standard multi-head attention with our Mixture-of-Head attention (MoH), keeping all other training parameters identical to TransNeXt.", "description": "Table 1 compares the performance of MoH-ViT models against other state-of-the-art methods on the ImageNet-1K image classification benchmark, highlighting the impact of using a reduced number of attention heads.", "section": "4.1 ViT for Image Classification"}, {"figure_path": "2410.11842/tables/table_7_0.html", "caption": "Table 4: Comparisons between MoH-LLMs and vanilla LLMs. \"100B\" denotes a training budget of 100 billion tokens, while \"200B\" denotes a budget of 200 billion tokens. We observe that larger models, e.g., MoH-LLM-B, generally perform worse than smaller models, e.g., MoH-LLM-S, on TruthfulQA, consistent with the findings reported by Lin et al. (2022).", "description": "Table 4 compares the performance of Mixture-of-Head Language Models (MoH-LLMs) against vanilla LLMs across various language tasks, showing the impact of reducing the number of activated attention heads.", "section": "4.3 TRAINING LLMS FROM SCRATCH"}, {"figure_path": "2410.11842/tables/table_8_0.html", "caption": "Table 5: Comparisons between MoH-LLaMA3-8B and LLaMA3-8B. Please refer to the Appendix for the performance of the model at the end of the first stage of training.", "description": "Table 5 presents the comparative results of MoH-LLaMA3-8B and LLaMA3-8B across multiple benchmarks, showing the performance gains achieved by MoH-LLaMA3-8B while utilizing only 75% of attention heads.", "section": "4.4 CONTINUE-TUNING LLAMA3-8B"}, {"figure_path": "2410.11842/tables/table_9_0.html", "caption": "Table 6: Ablation study on the impact of each component of the proposed MoH. The image classification results are from MoH-ViT-S, by utilizing 75% of the attention heads with a training budget of 100 epochs. The class-conditional image generation results come from MoH-DiT-S/2-400K, also by using 75% of the attention heads, with a training budget of 400K training steps.", "description": "Table 6 shows the ablation study of the proposed MoH model on image classification and class-conditional image generation by varying the usage of shared heads and two-stage routing.", "section": "4.5 ABLATIVE ANALYSIS"}, {"figure_path": "2410.11842/tables/table_9_1.html", "caption": "Table 7: Ablation study on the impact of the shared heads ratio among activated heads. All results are from MoH-ViT-S, by using 75% of the heads with a training budget of 100 epochs.", "description": "Table 7 shows the ablation study on the impact of different ratios of shared heads among activated heads on the accuracy of the MoH-ViT-S model.", "section": "4.5 ABLATIVE ANALYSIS"}, {"figure_path": "2410.11842/tables/table_14_0.html", "caption": "Table 1: Comparisons to current state-of-the-art methods on ImageNet-1K classification. All models are trained exclusively on the ImageNet-1K training set. Our MoH-ViT models, based on TransNeXt (Shi, 2024), are trained for 300 epochs using a resolution of 224x224. To ensure a fair comparison, we only replace the standard multi-head attention with our Mixture-of-Head attention (MoH), keeping all other training parameters identical to TransNeXt.", "description": "Table 1 compares the performance of the proposed Mixture-of-Head Attention (MoH) models against various state-of-the-art methods on the ImageNet-1K classification benchmark.", "section": "4.1 ViT for Image Classification"}, {"figure_path": "2410.11842/tables/table_17_0.html", "caption": "Table 1: Comparisons to current state-of-the-art methods on ImageNet-1K classification. All models are trained exclusively on the ImageNet-1K training set. Our MoH-ViT models, based on TransNeXt (Shi, 2024), are trained for 300 epochs using a resolution of 224x224. To ensure a fair comparison, we only replace the standard multi-head attention with our Mixture-of-Head attention (MoH), keeping all other training parameters identical to TransNeXt.", "description": "Table 1 compares the performance of the proposed Mixture-of-Head attention (MoH) model with other state-of-the-art methods on the ImageNet-1K image classification task.", "section": "4.1 ViT for Image Classification"}, {"figure_path": "2410.11842/tables/table_18_0.html", "caption": "Table 1: Comparisons to current state-of-the-art methods on ImageNet-1K classification. All models are trained exclusively on the ImageNet-1K training set. Our MoH-ViT models, based on TransNeXt (Shi, 2024), are trained for 300 epochs using a resolution of 224x224. To ensure a fair comparison, we only replace the standard multi-head attention with our Mixture-of-Head attention (MoH), keeping all other training parameters identical to TransNeXt.", "description": "Table 1 compares the performance of MoH-ViT models against other state-of-the-art methods on ImageNet-1K classification, showing that MoH achieves competitive or superior performance while using fewer attention heads.", "section": "4.1 ViT for Image Classification"}, {"figure_path": "2410.11842/tables/table_18_1.html", "caption": "Table 1: Comparisons to current state-of-the-art methods on ImageNet-1K classification. All models are trained exclusively on the ImageNet-1K training set. Our MoH-ViT models, based on TransNeXt (Shi, 2024), are trained for 300 epochs using a resolution of 224x224. To ensure a fair comparison, we only replace the standard multi-head attention with our Mixture-of-Head attention (MoH), keeping all other training parameters identical to TransNeXt.", "description": "Table 1 compares the performance of MoH-ViT models against other state-of-the-art models on ImageNet-1K classification, highlighting the impact of using a reduced number of attention heads.", "section": "4.1 ViT for Image Classification"}, {"figure_path": "2410.11842/tables/table_19_0.html", "caption": "Table 1: Comparisons to current state-of-the-art methods on ImageNet-1K classification. All models are trained exclusively on the ImageNet-1K training set. Our MoH-ViT models, based on TransNeXt (Shi, 2024), are trained for 300 epochs using a resolution of 224x224. To ensure a fair comparison, we only replace the standard multi-head attention with our Mixture-of-Head attention (MoH), keeping all other training parameters identical to TransNeXt.", "description": "Table 1 compares the performance of MoH-ViT models against other state-of-the-art models on the ImageNet-1K classification benchmark, highlighting MoH's efficiency in achieving competitive accuracy with fewer activated heads.", "section": "4.1 ViT for Image Classification"}, {"figure_path": "2410.11842/tables/table_19_1.html", "caption": "Table 5: Comparisons between MoH-LLaMA3-8B and LLaMA3-8B. Please refer to the Appendix for the performance of the model at the end of the first stage of training.", "description": "Table 5 compares the performance of MoH-LLaMA3-8B and LLaMA3-8B across various language tasks, showing that MoH-LLaMA3-8B outperforms LLaMA3-8B with only 75% of the attention heads activated.", "section": "4.4 CONTINUE-TUNING LLAMA3-8B"}]