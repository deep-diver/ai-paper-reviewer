[{"figure_path": "2410.11842/charts/charts_8_0.png", "caption": "Figure 2: Performance evolution during continue-tuning. The MoH model quickly recovers to over 95% of the performance of the original model within a training budget of 10B tokens. Then, the performance gradually improves with the increase of the training tokens.", "description": "The chart displays the performance evolution of the MoH model during continue-tuning, showing a quick recovery to near-original performance and gradual improvement with increased training tokens.", "section": "4.4 CONTINUE-TUNING LLAMA3-8B"}, {"figure_path": "2410.11842/charts/charts_8_1.png", "caption": "Figure 2: Performance evolution during continue-tuning. The MoH model quickly recovers to over 95% of the performance of the original model within a training budget of 10B tokens. Then, the performance gradually improves with the increase of the training tokens.", "description": "The chart displays the performance evolution of the MoH model during continue-tuning, showing a quick recovery to nearly the original model's performance and gradual improvement with increased training tokens.", "section": "4.4 CONTINUE-TUNING LLAMA3-8B"}, {"figure_path": "2410.11842/charts/charts_10_0.png", "caption": "Figure 3: Visualization of the head load distribution in the final MoH layer. For ViT and DiT, we present the head load distributions for the categories \u201cDesk\u201d, \u201cGoldfish", "description": "The chart visualizes the head load distribution in the final Mixture-of-Head Attention (MoH) layer for Vision Transformers (ViT), Diffusion models with Transformers (DiT), and Large Language Models (LLMs).", "section": "4.5 ABLATIVE ANALYSIS"}, {"figure_path": "2410.11842/charts/charts_20_0.png", "caption": "Figure 3: Visualization of the head load distribution in the final MoH layer. For ViT and DiT, we present the head load distributions for the categories \u201cDesk\u201d, \u201cGoldfish", "description": "The chart visualizes the head load distribution in the final MoH layer for ViT, DiT, and LLM models, showing how different heads focus on different categories and tasks.", "section": "4.5 ABLATIVE ANALYSIS"}, {"figure_path": "2410.11842/charts/charts_21_0.png", "caption": "Figure 3: Visualization of the head load distribution in the final MoH layer. For ViT and DiT, we present the head load distributions for the categories \u201cDesk\u201d, \u201cGoldfish\u201d, and \u201cIce cream\u201d. For LLM, we display the head distributions for the tasks \u201cLogiQA\u201d, \u201cPIQA\u201d, and \u201cWinoGrande\u201d. MoH-ViT-B, MoH-DiT-XL/2, and MoH-LLM-B activate 75%, 90%, and 75% of the attention heads, respectively.", "description": "The chart visualizes the distribution of head load across different categories and tasks in the final layer of the Mixture-of-Head attention model.", "section": "5 DISCUSSION"}, {"figure_path": "2410.11842/charts/charts_21_1.png", "caption": "Figure 3: Visualization of the head load distribution in the final MoH layer. For ViT and DiT, we present the head distributions for the categories \u201cDesk\u201d, \u201cGoldfish", "description": "The chart visualizes the head load distribution in the final MoH layer for ViT, DiT, and LLMs across different categories and tasks.", "section": "4.5 ABLATIVE ANALYSIS"}, {"figure_path": "2410.11842/charts/charts_21_2.png", "caption": "Figure 3: Visualization of the head load distribution in the final MoH layer. For ViT and DiT, we present the head load distributions for the categories \u201cDesk\u201d, \u201cGoldfish\u201d, and \u201cIce cream\u201d. For LLM, we display the head distributions for the tasks \u201cLogiQA\u201d, \u201cPIQA\u201d, and \u201cWinoGrande\u201d. MoH-ViT-B, MoH-DiT-XL/2, and MoH-LLM-B activate 75%, 90%, and 75% of the attention heads, respectively.", "description": "The chart visualizes the distribution of head load across different categories and tasks in the final MoH layer for ViT, DiT, and LLM models.", "section": "5 DISCUSSION"}, {"figure_path": "2410.11842/charts/charts_21_3.png", "caption": "Figure 3: Visualization of the head load distribution in the final MoH layer. For ViT and DiT, we present the head distributions for the categories \u201cDesk\u201d, \u201cGoldfish\u201d, and \u201cIce cream\u201d. For LLM, we display the head distributions for the tasks \u201cLogiQA\u201d, \u201cPIQA\u201d, and \u201cWinoGrande\u201d. MoH-ViT-B, MoH-DiT-XL/2, and MoH-LLM-B activate 75%, 90%, and 75% of the attention heads, respectively.", "description": "The chart visualizes the distribution of attention head load across different categories/tasks in the final MoH layer for ViT, DiT, and LLM models.", "section": "5 DISCUSSION"}, {"figure_path": "2410.11842/charts/charts_21_4.png", "caption": "Figure 3: Visualization of the head load distribution in the final MoH layer. For ViT and DiT, we present the head distributions for the categories \u201cDesk\u201d, \u201cGoldfish\u201d, and \u201cIce cream\u201d. For LLM, we display the head distributions for the tasks \u201cLogiQA\u201d, \u201cPIQA\u201d, and \u201cWinoGrande\u201d. MoH-ViT-B, MoH-DiT-XL/2, and MoH-LLM-B activate 75%, 90%, and 75% of the attention heads, respectively.", "description": "The chart visualizes the distribution of head load across different categories or tasks for three model types (ViT, DiT, and LLM) using the Mixture-of-Head attention method.", "section": "Additional Visualization of the Head Load Distribution"}, {"figure_path": "2410.11842/charts/charts_21_5.png", "caption": "Figure 3: Visualization of the head load distribution in the final MoH layer. For ViT and DiT, we present the head load distributions for the categories \u201cDesk\u201d, \u201cGoldfish\u201d, and \u201cIce cream\u201d. For LLM, we display the head distributions for the tasks \u201cLogiQA\u201d, \u201cPIQA\u201d, and \u201cWinoGrande\u201d. MoH-ViT-B, MoH-DiT-XL/2, and MoH-LLM-B activate 75%, 90%, and 75% of the attention heads, respectively.", "description": "The chart visualizes the distribution of attention head load in the final layer of Mixture-of-Head Attention (MoH) models for different tasks and model architectures.", "section": "5 DISCUSSION"}, {"figure_path": "2410.11842/charts/charts_21_6.png", "caption": "Figure 3: Visualization of the head load distribution in the final MoH layer. For ViT and DiT, we present the head load distributions for the categories \u201cDesk\u201d, \u201cGoldfish\u201d, and \u201cIce cream\u201d. For LLM, we display the head distributions for the tasks \u201cLogiQA\u201d, \u201cPIQA\u201d, and \u201cWinoGrande\u201d. MoH-ViT-B, MoH-DiT-XL/2, and MoH-LLM-B activate 75%, 90%, and 75% of the attention heads, respectively.", "description": "The chart visualizes the distribution of attention head load in the final MoH layer for various tasks and model types.", "section": "5 DISCUSSION"}]