{"references": [{" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduces the Transformer architecture and the multi-head self-attention mechanism, which are fundamental to the current work and the entire field of large language models and vision transformers.  The core concept of multi-head attention, its strengths and limitations, are central to the motivation and methodology of the proposed Mixture-of-Head Attention.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Elena Voita", "paper_title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned", "reason": "This paper provides empirical evidence that not all attention heads are equally important in multi-head attention mechanisms. Their findings support the central hypothesis of the current work that many attention heads are redundant and that activating a subset of heads could improve inference efficiency without significantly compromising accuracy.  It directly relates to the core idea of selective activation of attention heads.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Paul Michel", "paper_title": "Are sixteen heads really better than one?", "reason": "This paper challenges the necessity of using multiple attention heads, further supporting the core argument of the current research that multi-head attention mechanisms may have inherent redundancy.  The findings provide further justification for the proposed Mixture-of-Head Attention (MoH) that aims to improve efficiency by selecting only the most necessary attention heads.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This seminal paper introduces the Transformer architecture and the fundamental concept of multi-head attention, which forms the basis of the current research.  The paper's detailed description of multi-head attention's mechanism, particularly its concatenation and summation forms, provides the foundation for the proposed Mixture-of-Head Attention's development and analysis.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "reason": "This paper introduces the Mixture-of-Experts (MoE) architecture, which is a key component of the proposed Mixture-of-Head Attention (MoH).  Understanding the principles and previous implementations of MoE is crucial to comprehending the MoH's design and its integration of the MoE framework with multi-head attention.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "William Fedus", "paper_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "reason": "This paper presents Switch Transformer, a significant advancement in the Mixture-of-Experts (MoE) architecture that improves efficiency through expert selection.  Understanding the Switch Transformer's approach to efficient scaling and parameter usage is relevant because the current work also aims to improve efficiency in attention mechanisms, albeit through a different approach.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Dmitry Lepikhin", "paper_title": "Gshard: Scaling giant models with conditional computation and automatic sharding", "reason": "This paper introduces Gshard, an efficient method for scaling large models using the Mixture-of-Experts (MoE) framework.  The techniques used in Gshard, such as efficient parameter scaling and routing strategies, are relevant to the current work because it also aims to improve the efficiency of attention-based models through selective activation of attention heads.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Mike Lewis", "paper_title": "Base layers: Simplifying training of large, sparse models", "reason": "This work provides valuable insights into efficient training of large, sparse models, which is relevant because the proposed Mixture-of-Head Attention (MoH) aims to improve efficiency by activating only a subset of attention heads. The techniques and strategies used in training sparse models are relevant to efficiently training MoH.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "reason": "This paper introduces Vision Transformers (ViTs), a significant advancement in applying Transformers to computer vision tasks.  The use of ViTs in the experiments of the current work demonstrates the applicability and effectiveness of MoH in a widely used and important model architecture in the computer vision domain.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces LLaMA3, a powerful large language model used as a basis for the continue-tuning experiments in the current work.   The ability to adapt a pre-trained model like LLaMA3 to the MoH architecture demonstrates the broader applicability and potential of the proposed method.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Dai Shi", "paper_title": "Transnext: Robust foveal visual perception for vision transformers", "reason": "This paper introduces TransNeXt, a framework for Vision Transformers (ViT) that is used as the baseline model in the experiments of the current work.  Using TransNeXt allows for a fair comparison by isolating the impact of the proposed Mixture-of-Head Attention (MoH) while maintaining other parameters identical.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "reason": "This paper introduces Diffusion Models with Transformers (DiT), another significant model architecture used in the experiments of the current work.  The use of DiT demonstrates the versatility and effectiveness of MoH in different model frameworks and tasks (class-conditional image generation).", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is highly influential in the field of Large Language Models (LLMs) and is a foundation for many subsequent works.  The current work uses LLMs in its experiments, and understanding the foundational concepts of LLMs as introduced in this paper is crucial to evaluating the results and implications.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Luca Soldaini", "paper_title": "Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research", "reason": "This paper introduces Dolma, a large language model dataset used in the training of LLMs in the current work.  The use of Dolma demonstrates the feasibility and effectiveness of training large language models from scratch using the proposed Mixture-of-Head Attention (MoH).", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Leo Gao", "paper_title": "The Pile: An 800gb dataset of diverse text for language modeling", "reason": "This paper introduces The Pile, another significant dataset used in the training of LLMs in the current work.  The diversity and size of The Pile provide a strong testbed for evaluating the proposed Mixture-of-Head Attention (MoH) and its potential to improve efficiency and performance in large language models.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "reason": "This paper introduces LLaMA 2, which is used as a pre-trained model in the continue-tuning experiments in the current work.  Fine-tuning this pre-trained model to use MoH demonstrates the applicability of the proposed method and its capacity to improve the performance of existing state-of-the-art models.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Xiaofeng Zhang", "paper_title": "Mixture of attention heads: Selecting attention heads per token", "reason": "This paper explores a related approach to improving multi-head attention efficiency by selecting attention heads per token. While different from the proposed MoH, understanding this alternative approach is valuable in understanding the broader landscape of attention mechanism optimization and providing additional context for the current work's contributions.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Joan Puigcerver", "paper_title": "From sparse to soft mixtures of experts", "reason": "This paper provides important insights into the design and training of Mixture-of-Experts (MoE) models, especially regarding the concept of Soft MoE. This work is relevant because it can provide additional context for the shared heads mechanism of the proposed MoH architecture, as shared heads can be viewed as a form of Soft MoE.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Peng Jin", "paper_title": "Moe++: Accelerating mixture-of-experts methods with zero-computation experts", "reason": "This paper, also from the same authors, explores a related concept of accelerating MoE methods.  Its findings and techniques regarding efficient expert selection and routing could be further explored and possibly integrated into the MoH architecture to achieve even higher efficiency and scalability.", "section_number": 5}]}