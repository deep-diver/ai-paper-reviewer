[{"content": "| Method | Subject Align (%) win | tie | lose | Text Align (%) win | tie | lose |\n|---|---|---|---|---|---|---|\n| ELITE [47] | 77.9 | 4.3 | 17.8 | 75.2 | 8.6 | 16.2 |\n| BLIP-Diff [22] | 73.8 | 8.6 | 17.6 | 77.8 | 4.3 | 17.9 |\n| \u03bb-Eclipse [31] | 80.4 | 4.2 | 15.4 | 74.0 | 3.3 | 22.7 |\n| MS-Diff [46] | 59.3 | 15.6 | 25.1 | 58.9 | 9.1 | 32.0 |\n| IP-A (SD-XL) [50] | 76.2 | 9.7 | 14.1 | 76.2 | 9.7 | 14.1 |\n| IP-A (FLUX) [50] | 69.8 | 12.0 | 18.2 | 65.2 | 20.6 | 14.2 |", "caption": "Table 1: Human Preference Study. We report results of pairwise comparisons between Diptych Prompting and publicly available baselines in two aspects: subject alignment and text alignment. \u2018IP-A\u2019 denotes the abbreviation for IP-Adapter.", "description": "This table presents the results of a human preference study comparing Diptych Prompting to several publicly available baselines for subject-driven text-to-image generation.  Participants were shown pairs of images, one generated by Diptych Prompting and one by a baseline method, and asked to choose which image better aligned with the reference image (subject alignment) and the text prompt (text alignment). The table shows the percentage of times each method was preferred in each of these two aspects.  IP-A is used as shorthand for IP-Adapter.", "section": "4.2. Baseline Comparisons"}, {"content": "| Method | Model | DINO | CLIP-I | CLIP-T |\n|---|---|---|---|---|\n| ELITE [47] | SD-v1.4 | 0.621 | 0.771 | 0.293 |\n| BLIP-Diff [22] | SD-v1.5 | 0.594 | 0.779 | 0.300 |\n| Kosmos-G [30] | SD-v1.5 | 0.694 | 0.847 | 0.287 |\n| Subject-Diff [26] | - | 0.711 | 0.787 | 0.303 |\n| \u03bb-Eclipse [31] | Kan-v2.2 | 0.613 | 0.783 | 0.307 |\n| MS-Diff [46] | SD-XL | 0.671 | 0.792 | 0.321 |\n| IP-Adapter [50] \u2020 | SD-XL | 0.613 | 0.810 | 0.292 |\n| IP-Adapter [50] \u2021 | FLUX | 0.561 | 0.725 | 0.351 |\n| Diptych Prompting | FLUX | 0.688 | 0.758 | 0.345 |", "caption": "Table 2: Quantitative Comparisons. We compare our method to encoder-based image prompting methods in three metrics. \u2020\u2020\\dagger\u2020 denotes the obtained value from [31], and \u2021\u2021\\ddagger\u2021 indicates our re-evaluation with publicly available weights.", "description": "This table presents a quantitative comparison of the proposed Diptych Prompting method against several existing encoder-based image prompting techniques for zero-shot subject-driven image generation.  The comparison uses three metrics: DINO, CLIP-I, and CLIP-T, which assess subject alignment and text alignment aspects of the generated images.  Note that some results are taken from another publication ([31]) and others are reproduced by the authors of the current paper, using publicly available model weights.", "section": "4.2 Baseline Comparisons"}, {"content": "| Model | Inpainting | Scale | DINO | CLIP-I | CLIP-T |\n|---|---|---|---|---|---| \n| SD-3 | Zero-shot | - | 0.475 | 0.670 | 0.330 |\n|  | ControlNet | 0.95 | 0.576 | 0.699 | 0.326 |\n| FLUX | Zero-shot | - | 0.555 | 0.720 | 0.336 |\n|  | ControlNet | 0.5 | 0.628 | 0.737 | 0.351 |\n|  |  | 0.8 | 0.670 | 0.750 | 0.349 |\n|  |  | 0.95 | 0.689 | 0.758 | 0.344 |", "caption": "Table 3: Model Selection. We present an ablation results of various base models, inpainting method, and the ControlNet conditioning scale for Diptych Prompting.", "description": "This table presents the results of an ablation study investigating the impact of different model choices on the performance of Diptych Prompting.  Specifically, it examines the effects of using different base models (pre-trained large-scale text-to-image models), different inpainting methods, and varying ControlNet conditioning scales. The goal is to determine the optimal combination of these factors for achieving the best performance in zero-shot subject-driven text-to-image generation.", "section": "4.3. Ablation Studies"}, {"content": "| $G_{seg}$ | $\\lambda$ | DINO | CLIP-I | CLIP-T |\n|---|---|---|---|---|\n| \u2717 | 1.3 | 0.759 | 0.783 | 0.333 |\n| \u2713 | 1.0 | 0.647 | 0.745 | 0.343 |\n| \u2713 | 1.3 | 0.688 | 0.758 | 0.345 |\n| \u2713 | 1.5 | 0.670 | 0.750 | 0.342 |", "caption": "Table 4: \ud835\udc6esegsubscript\ud835\udc6eseg\\bm{G_{\\text{seg}}}bold_italic_G start_POSTSUBSCRIPT seg end_POSTSUBSCRIPT and \u03bb\ud835\udf06\\bm{\\lambda}bold_italic_\u03bb Ablation. We report the ablation results of background removal and reference attention enhancement.", "description": "This table presents the results of ablation studies conducted to evaluate the impact of two key components in the Diptych Prompting method: background removal (using Gseg) and reference attention enhancement (controlled by the parameter \u03bb).  It shows how removing the background of the reference image and adjusting the attention weights between the two image panels affects the performance of the model, measured using DINO, CLIP-I, and CLIP-T scores.  The ablation study helps to determine the individual contributions of these components to the overall performance of Diptych Prompting in zero-shot subject-driven text-to-image generation.", "section": "4.3. Ablation Studies"}, {"content": "| Model | Arch | Param | DINO | CLIP-I | CLIP-T |\n|---|---|---|---|---|---| \n| SD-v2 | U-Net | 1.2B | 0.504 | 0.744 | 0.260 |\n| SD-XL | U-Net | 3.5B | 0.941 | 0.954 | 0.288 |\n| SD-3 | MM-DiT | 7.7B | 0.705 | 0.821 | 0.340 |\n| FLUX | MM-DiT | 16.9B | 0.720 | 0.828 | 0.352 |", "caption": "Table A1: Diptych Generation Comparisons. Quantitative comparisons of the diptych generation capabilities of various TTI models based on the total number of parameters, including the autoencoder, main network, and text encoder.", "description": "This table presents a quantitative comparison of different text-to-image (TTI) models' capabilities in generating diptychs.  It assesses performance based on three key metrics: DINO, CLIP-I, and CLIP-T scores.  The metrics evaluate aspects of image generation quality. The models are compared based on their total number of parameters, which provides an indication of model complexity.  The table includes information on the model architecture (U-Net or MM-DiT) and the number of parameters for each model. This allows for an assessment of the relationship between model size and diptych generation performance.", "section": "Appendix"}, {"content": "| Method | DINO | CLIP-I | CLIP-T |\n|---|---|---|---|\n| RB-Mod [38] | 0.295 | 0.598 | **0.372** |\n| IP-Adapter [50] | 0.337 | 0.602 | 0.371 |\n| Diptych Prompting | **0.357** | **0.623** | 0.349 |", "caption": "Table A2: Stylized Image Generation Comparisons. Quantitative comparisons of stylized image generation with previous zero-shot methods.", "description": "This table presents a quantitative comparison of the performance of Diptych Prompting against other zero-shot methods for stylized image generation.  The comparison uses three metrics: DINO, CLIP-I, and CLIP-T, to evaluate the quality and alignment of generated images.  Each method's scores across these metrics are shown, enabling a direct assessment of Diptych Prompting's efficacy relative to state-of-the-art zero-shot approaches in generating stylized images.", "section": "Appendix G. Stylized Image Generation"}]