[{"figure_path": "https://arxiv.org/html/2502.10458/x2.png", "caption": "Figure 1: \n(a) Our ThinkDiff reasons over interleaved images (a flying monkey and a flying cat) and text prompts (monkey, cat, and zebra) to generate a logically correct and high-quality image (a flying zebra). The ground truth reasoning answer is provided as a reference for readers. (b) ThinkDiff composes images and texts into a coherent and reasonable image.", "description": "Figure 1 illustrates ThinkDiff's capabilities in multimodal in-context reasoning. (a) shows ThinkDiff's ability to perform logical reasoning by combining visual information (images of a flying monkey and a flying cat) and textual cues (prompts mentioning 'monkey', 'cat', and 'zebra') to generate a new image of a 'flying zebra', demonstrating in-context understanding and generation. (b) demonstrates ThinkDiff's capacity for image composition by generating a coherent image from multiple images and texts, highlighting its ability to create logically sound and reasonable outputs from complex multimodal inputs.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.10458/x3.png", "caption": "Figure 2: \n(a) Reconstruction-based diffusion finetuning integrates image features using a diffusion loss, focusing on pixel-level image reconstruction without reasoning.\n(b) ThinkDiff aligns a VLM to an LLM decoder by vision-language training on image-caption datasets. In inference (dotted lines), it transfers multimodal in-context reasoning capabilities from the VLM to a diffusion decoder.", "description": "Figure 2 illustrates two different approaches to integrating vision-language models (VLMs) with diffusion models for image generation.  Panel (a) shows reconstruction-based diffusion finetuning. This method focuses on pixel-level image reproduction by directly incorporating visual features into the diffusion model using a diffusion loss. This approach doesn't explicitly support in-context reasoning. Panel (b) shows ThinkDiff's approach. ThinkDiff leverages vision-language training to align the VLM with the decoder of a large language model (LLM). Because the LLM decoder and the diffusion decoder share the same feature space, this indirect alignment effectively transfers the reasoning capabilities of the VLM to the diffusion decoder, enabling multimodal in-context reasoning during inference. The dotted lines indicate the inference phase where the reasoning abilities are transferred.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.10458/x4.png", "caption": "Figure 3: Several diffusion models share a language encoder with encoder-decoder LLMs, allowing aligning with diffusion decoders through aligning with LLM decoders.", "description": "This figure illustrates the shared architecture between several diffusion models and encoder-decoder large language models (LLMs).  It highlights that many diffusion models utilize the same language encoder as LLMs. This shared encoder creates a common feature space, making it possible to align the Vision-Language Model (VLM) with the diffusion decoder indirectly by aligning it with the LLM decoder.  This simplifies the training process and avoids the need for direct alignment between the VLM and the diffusion decoder, which is often complex and requires large datasets.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.10458/x5.png", "caption": "Figure 4: (a) In ThinkDiff-LVLM training, the LVLM processes an image and a text to generate text tokens and token features, with some token features randomly masked. Unmasked token features are passed to a trainable aligner network and an LLM decoder, predicting masked text tokens supervised by cross-entropy loss. In inference, the LLM decoder is replaced by a diffusion decoder, enabling in-context reasoning image generation from interleaved images and texts. (b) In ThinkDiff-CLIP training, a CLIP vision model extracts image token features which are then mapped by a trainable aligner network. A part of the image caption is encoded by the LLM encoder and concatenated with image tokens. These combined tokens are passed to the LLM decoder to predict the next part of the caption supervised by cross-entropy loss. In inference, the LLM decoder is replaced by a diffusion encoder, allowing coherent image generation based on multimodal context.", "description": "Figure 4 illustrates the training and inference processes of two ThinkDiff variants: (a) ThinkDiff-LVLM uses a large vision-language model (LVLM) to generate text tokens and features from an image and text prompt.  Some features are randomly masked for training, while the unmasked features are fed into an aligner network and an LLM decoder. The decoder predicts the masked tokens, supervised by a cross-entropy loss. During inference, the LLM decoder is replaced by a diffusion decoder to generate images from multimodal input. (b) ThinkDiff-CLIP uses a CLIP vision model to extract image features.  A portion of an image caption is encoded by an LLM encoder, which then concatenates with the image features.  This combined input is fed to the LLM decoder, trained to predict the next part of the caption. During inference, this decoder is replaced by a diffusion decoder for coherent image generation based on multimodal input.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.10458/x6.png", "caption": "Figure 5: 2-shot evaluation results on CoBSAT. The input structure is similar to Figure\u00a01a. Given multimodal inputs, ThinkDiff-LVLM accurately captures both implicit attributes (e.g., wicker material) and explicit attributes (e.g. car), and generates a logically correct image (wicker car). In contrast, methods such as SEED-LLaMA\u00a0(Ge et\u00a0al., 2024), Emu\u00a0(Sun et\u00a0al., 2023) and GILL\u00a0(Koh et\u00a0al., 2024) produce inaccurate and lower-quality images. The ground truth implicit attribute is highlighted in red for readers\u2019 reference. See more results in Appendix Figure\u00a09 and 10.", "description": "This figure shows a comparison of different models' performance on a 2-shot visual reasoning task from the CoBSAT benchmark.  The input consists of images and text prompts designed to elicit the generation of a \"wicker car.\" ThinkDiff-LVLM successfully generates a high-quality image matching this description, correctly identifying both the explicit (car) and implicit (wicker) attributes. In contrast, the baseline models SEED-LLaMA, Emu, and GILL fail to produce accurate or visually appealing results, demonstrating ThinkDiff-LVLM's superior multimodal reasoning capabilities.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.10458/x7.png", "caption": "Figure 6: Generation results for single image (I) and single image with text prompt (I + T) inputs. Our method effectively integrates semantic details of both image and text modalities to produce coherent images. FLUX excels at replicating the input image but struggles to maintain consistency with additional text prompts. See more results in Figure\u00a011.", "description": "Figure 6 presents a comparison of image generation results using different methods: ThinkDiff, and FLUX.  The experiment uses two input conditions: a single image (I) and a single image with a text prompt (I+T). The results show that ThinkDiff effectively incorporates the semantic content of both the image and the text prompt to create a coherent, meaningful image.  In contrast, FLUX is quite good at reproducing the input image but it struggles to incorporate additional text information and maintain consistency between image and text. This showcases ThinkDiff's improved ability to handle multimodal in-context reasoning. More results are presented in Figure 11.", "section": "4.3. Evaluation results of ThinkDiff-CLIP"}, {"figure_path": "https://arxiv.org/html/2502.10458/x8.png", "caption": "Figure 7: Training losses (log scale) of ThinkDiff-LVLM comparing different RMSNorm designs. Disabling RMSNorm (w/o RMSNorm) or using the default RMSNorm initialization (RMSNorm w/ Default init.) results in significantly unstable training.", "description": "This figure shows the training loss curves of the ThinkDiff-LVLM model under three different conditions: 1) without RMSNorm; 2) with RMSNorm and default initialization; 3) with RMSNorm and the proposed initialization.  The y-axis represents the training loss (on a logarithmic scale), and the x-axis represents training steps. The plot visually demonstrates that disabling RMSNorm or using default initialization leads to significantly unstable training, characterized by erratic fluctuations in the loss. In contrast, the proposed RMSNorm initialization ensures stable and convergent training.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.10458/x9.png", "caption": "Figure 8: Results of ThinkDiff-CLIP composing two images. It creatively merge semantic details of both images. See more results in Appendix Figure\u00a012.", "description": "ThinkDiff-CLIP successfully combines two input images into a single coherent image by creatively merging their semantic details.  The model demonstrates its ability to understand and integrate visual information from multiple sources, rather than simply juxtaposing them.", "section": "4.3. Evaluation results of ThinkDiff-CLIP"}, {"figure_path": "https://arxiv.org/html/2502.10458/x10.png", "caption": "Figure 9: More 2-shot reasoning results of ThinkDiff-LVLM on CoBSAT benchmark.", "description": "This figure showcases additional examples from the CoBSAT benchmark, demonstrating ThinkDiff-LVLM's performance on more complex 2-shot reasoning tasks.  Each row presents a different task, showing the multimodal inputs (images and text), the model's generated reasoning output (image), and the ground truth.  The figure highlights the model's ability to correctly identify and synthesize visual features based on the given contextual clues and logic, even with limited input data.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.10458/x11.png", "caption": "Figure 10: More 2-shot reasoning results of ThinkDiff-LVLM on CoBSAT benchmark.", "description": "This figure shows more examples of ThinkDiff-LVLM's performance on the CoBSAT benchmark's 2-shot reasoning tasks.  Each row displays a set of input images and text prompts, followed by the model's generated image.  The results demonstrate ThinkDiff-LVLM's ability to integrate visual and textual information to generate images that are logically consistent with the input.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.10458/x12.png", "caption": "Figure 11: Generation results of a single image and a text prompt of ThinkDiff-CLIP.", "description": "This figure compares the image generation results of three different methods: Flux Ultra with only image input, Flux Ultra with both image and text input, ThinkDiff-CLIP with only image input, and ThinkDiff-CLIP with both image and text input.  Each row shows results using a different starting image.  The results demonstrate ThinkDiff-CLIP's ability to effectively integrate both image and text information to generate coherent images that align with the provided text prompt, unlike the Flux Ultra model which struggles to maintain consistency when additional text is added.", "section": "4.3. Evaluation results of ThinkDiff-CLIP"}, {"figure_path": "https://arxiv.org/html/2502.10458/x13.png", "caption": "Figure 12: Multiple input image generation results of ThinkDiff-CLIP.", "description": "This figure showcases the capabilities of ThinkDiff-CLIP in generating images from multiple input images.  It demonstrates the model's ability to synthesize and combine visual elements from various sources into a single, coherent output image.  The results highlight ThinkDiff-CLIP's skill in integrating and harmonizing different visual styles and content, producing creative and visually interesting compositions.", "section": "4.3. Evaluation results of ThinkDiff-CLIP"}, {"figure_path": "https://arxiv.org/html/2502.10458/x14.png", "caption": "Figure 13: Generation results for multiple images (2I) and multiple images with a text prompt (2I + T) of ThinkDiff-CLIP.", "description": "This figure presents a comparison of image generation results using ThinkDiff-CLIP under different input conditions.  The first column shows the input images used for generation, demonstrating examples with multiple images (2I). The second column displays the output generated by ThinkDiff-CLIP when given only the multiple images (2I) as input. The third column shows the text prompts accompanying the multiple images (2I + T). The fourth column showcases the generation results when both multiple images and the corresponding text prompts are provided as input (2I + T) to ThinkDiff-CLIP.  This comparison highlights ThinkDiff-CLIP's ability to generate semantically coherent images by effectively integrating both image and text information, exceeding the capabilities of simply replicating the input images.", "section": "4.3. Evaluation results of ThinkDiff-CLIP"}]