[{"content": "| Model | ITPS (t/s) | OTPS (t/s) | TTFT (s) | Runtime (s) |\n|---|---|---|---|---|\n| **(a) Prompt: \u201cWho was the first president of USA?\u201d** |  |  |  |  |\n| SmolLM-135M-Instruct | 68.48 | 59.72 | 0.46 | 1.42 |\n| SmolLM-360M-Instruct | 27.56 | 56.68 | 0.85 | 3.71 |\n| Qwen2-0.5B-Instruct | 23.84 | 51.78 | 1.90 | 2.38 |\n| Qwen2-1.5B-Instruct | 3.42 | 17.12 | 13.01 | 14.39 |\n| Gemma-2-2b-it | 1.82 | 18.64 | 10.56 | 13.52 |\n| Phi-3-mini-4k-instruct | 0.86 | 14.78 | 39.81 | 48.29 |\n| Phi-3.5-mini-instruct | 0.88 | 15.60 | 39.90 | 47.49 |\n| Mistral-7B-Instruct-v0.3 | 0.44 | 9.36 | 127.60 | 135.12 |\n| Llama-3.1-8B-Instruct | 0.10 | 2.20 | 261.65 | 269.99 |\n| **(b) Prompt: 1 chunk ~ 200 tokens (157 words)** |  |  |  |  |\n| SmolLM-135M-Instruct | 167.80 | 60.80 | 1.91 | 4.22 |\n| SmolLM-360M-Instruct | 28.42 | 36.12 | 10.62 | 16.82 |\n| Qwen2-0.5B-Instruct | 23.02 | 39.42 | 13.15 | 14.96 |\n| Qwen2-1.5B-Instruct | 3.86 | 14.70 | 78.78 | 86.14 |\n| Gemma-2-2b-it | 2.20 | 11.68 | 122.06 | 141.15 |\n| Phi-3-mini-4k-instruct | 1.05 | 12.68 | 327.09 | 339.87 |\n| **(c) Prompt: 2 chunks ~ 400 tokens (269 words)** |  |  |  |  |\n| SmolLM-135M-Instruct | 130.66 | 40.42 | 4.84 | 8.14 |\n| SmolLM-360M-Instruct | 23.28 | 27.90 | 30.40 | 41.07 |\n| Qwen2-0.5B-Instruct | 18.62 | 24.72 | 29.49 | 38.36 |\n| **(d) Prompt: 3 chunks ~ 600 tokens (368 words)** |  |  |  |  |\n| SmolLM-135M-Instruct | 174.10 | 45.70 | 4.89 | 8.26 |\n| SmolLM-360M-Instruct | 31.50 | 33.94 | 27.16 | 33.52 |\n| Qwen2-0.5B-Instruct | 20.53 | 25.04 | 37.94 | 47.05 |\n| **(e) Prompt: 4 chunks ~ 800 tokens (529 words)** |  |  |  |  |\n| SmolLM-135M-Instruct | 134.66 | 32.96 | 8.47 | 11.83 |\n| SmolLM-360M-Instruct | 23.60 | 25.52 | 48.06 | 58.15 |\n| Qwen2-0.5B-Instruct | 19.74 | 19.52 | 54.90 | 66.65 |", "caption": "Table 1: Performance comparison of language models across varying input lengths ranging from single questions to chunks of around 800 tokens.\nSmaller models demonstrate higher efficiency but potentially lower accuracy, while larger models generally exhibit slower inference speeds but better handling of longer inputs.", "description": "This table compares the performance of several language models across different input lengths, ranging from short single questions to longer contexts of approximately 800 tokens.  The comparison focuses on key metrics reflecting efficiency (tokens per second for input and output, time to first token, total runtime) and, implicitly, accuracy (through the trade-off presented).  The results show a clear trend: smaller models process shorter inputs more quickly, but larger models may be better at handling longer inputs, though at the cost of slower processing. This suggests an important trade-off to consider when choosing a model for on-device applications depending on the expected input length and performance requirements.", "section": "2.1 Sweet Spot: Model Size, Context Length and Inference Time"}, {"content": "| Processing Stage | Mean \u00b1 STD | Token Range |\n|---|---|---|\n| Pre-processing | 8,635 \u00b1 24,235 | 1 \u2013 1,675,639 |\n| Post-processing | 879 \u00b1 252 | 1 \u2013 1,000 |", "caption": "Table 2: Statistical comparison of token distribution per document before and after pre-processing 82,850 documents. The table shows the mean \u00b1plus-or-minus\\pm\u00b1 standard deviation and the range of token counts for each processing stage.", "description": "This table presents a statistical analysis of token distribution in a dataset of 82,850 documents, comparing the token counts before and after a pre-processing step.  The pre-processing likely involved tasks such as tokenization and potentially truncation to a maximum length. The table shows the mean and standard deviation of token counts for both the raw and pre-processed data, along with the range (minimum and maximum) of observed token counts. This allows for an understanding of the effect the pre-processing had on the distribution of document lengths in terms of tokens.", "section": "2.2 Document Assistance Dataset"}, {"content": "| Token Type | Mean \u00b1 STD | Token Range |\n|---|---|---|\n| Prompt Tokens | 2,126.04 \u00b1 260.81 | 1,273 \u2013 2,617 |\n| Completion Tokens | 169.07 \u00b1 17.61 | 107 \u2013 312 |", "caption": "Table 3: A prompt designed to annotate data for three tasks given a document in DocAssist: SUMM, QS and QA.\n{{document}} is replaced with each pre-processed document.\nPlease see the complete prompt with in-context examples and requirements for each task {{summ_req}}, {{suggestion_req}} and {{qa_req}} in Tables\u00a012, 9, 10 and\u00a011, respectively.", "description": "This table describes the prompt used to generate annotations for the DocAssist dataset.  The prompt instructs a language model (specifically GPT-40-mini) to perform three tasks sequentially on a given document: summarization (SUMM), question suggestion (QS), and question answering (QA).  The {{document}} placeholder in the prompt is replaced with the actual document text during annotation.  For a complete view of the prompt and detailed instructions for each subtask, refer to Tables 9, 10, 11, and 12 in the paper.", "section": "2.2.2 Data Annotation"}, {"content": "| Model | BLEU \u2191 | ROUGE-1 \u2191 | ROUGE-2 \u2191 | ROUGE-L \u2191 | STS Score \u2191 | GEval \u2191 | Average |\n|---|---|---|---|---|---|---|---| \n| GPT-4o-mini | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 0.88 | 0.9795 |\n| SmolLM-135M-Instruct | 0.10 | 0.37 | 0.17 | 0.34 | 0.64 | 0.60 | 0.3694 |\n| SmolLM-360M-Instruct | 0.14 | 0.42 | 0.21 | 0.38 | 0.68 | 0.69 | 0.4202 |\n| Qwen2-0.5B-Instruct | 0.21 | 0.49 | 0.28 | 0.45 | 0.74 | 0.79 | 0.4934 |\n| Qwen2-1.5B-Instruct | 0.26 | 0.53 | 0.33 | 0.50 | 0.77 | 0.84 | 0.5396 |\n| LLaMA-3.2-1B-Instruct | 0.26 | 0.53 | 0.33 | 0.50 | 0.77 | 0.86 | 0.5442 |\n| **Slim Language Models (ours)** |  |  |  |  |  |  |  |\n| SlimLM-125M<sup>a</sup> | 0.14 | 0.41 | 0.21 | 0.38 | 0.66 | 0.64 | 0.4052 |\n| SlimLM-270M | 0.17 | 0.45 | 0.24 | 0.42 | 0.71 | 0.72 | 0.4497 |\n| SlimLM-350M<sup>b</sup> | 0.18 | 0.45 | 0.25 | 0.42 | 0.71 | 0.73 | 0.4541 |\n| SlimLM-450M<sup>c</sup> | 0.20 | 0.48 | 0.27 | 0.44 | 0.73 | 0.76 | 0.4806 |\n| SlimLM-760M | 0.21 | 0.48 | 0.28 | 0.45 | 0.74 | 0.79 | 0.4911 |\n| SlimLM-1B<sup>d</sup> | 0.23 | 0.51 | 0.31 | 0.48 | 0.76 | 0.81 | 0.5182 |", "caption": "Table 4: Token usage statistics for GPT-4o-mini model in annotating 82,850 documents.", "description": "This table presents a statistical analysis of the token usage by the GPT-40-mini model during the annotation of 82,850 documents for the DocAssist dataset. It shows the average and standard deviation of prompt tokens and completion tokens generated by the model, along with the range of token counts.  This data provides insights into the consistency and efficiency of the annotation process.", "section": "2.2.2 Data Annotation"}, {"content": "| Model | Accuracy (%) |\n|---|---| \n| GPT-4o-mini | 100.00 |\n| SmolLM-135M-Instruct | 99.86 |\n| SmolLM-360M-Instruct | 99.81 |\n| Qwen2-0.5B-Instruct | 100.00 |\n| Qwen2-1.5B-Instruct | 100.00 |\n| SlimLM-125M | 100.00 |\n| SlimLM-270M | 100.00 |\n| SlimLM-350M | 100.00 |\n| SlimLM-450M | 100.00 |\n| SlimLM-760M | 99.95 |\n| SlimLM-1B | 99.90 |", "caption": "Table 5: Comparison of model performance on average of three tasks: SUMM, QS and QA.\nGreen highlighting indicates superior performance of SlimLM models compared to similar-sized counterparts.\nKey comparisons: (a) SlimLM-125M outperforms SmolLM-135M-Instruct, (b) SlimLM-350M exceeds SmolLM-360M-Instruct, (c) SlimLM-450M is comparable to Qwen2-0.5B-Instruct, and (d) SlimLM-1B approaches Qwen2-1.5B-Instruct despite being smaller.\nTables\u00a014, 15 and\u00a016 present detailed results for each task.", "description": "This table compares the performance of various small language models (SLMs) on three document assistance tasks: summarization (SUMM), question suggestion (QS), and question answering (QA).  The models are evaluated using several metrics (BLEU, ROUGE, STS, GEval) to assess the quality and accuracy of their outputs.  The table highlights the performance of the SlimLM models (a series of SLMs specifically designed for mobile devices) and compares them to other state-of-the-art SLMs of similar sizes.  Green highlighting emphasizes where SlimLM models outperform their counterparts. Key comparisons are explicitly noted, indicating instances where SlimLM models show superior performance (SlimLM-125M > SmolLM-135M-Instruct, SlimLM-350M > SmolLM-360M-Instruct, SlimLM-450M \u2248 Qwen2-0.5B-Instruct, SlimLM-1B \u2248 Qwen2-1.5B-Instruct), demonstrating their efficiency and effectiveness. More detailed task-specific results can be found in Tables 14, 15, and 16.", "section": "3 Experiments and Results"}, {"content": "| Model | BLEU \u2191 | ROUGE-1 \u2191 | ROUGE-2 \u2191 | ROUGE-L \u2191 | STS Score \u2191 | GEval \u2191 | Average |\n|---|---|---|---|---|---|---|---| \n| GPT-4o-mini | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 0.86 | 0.9760 |\n| SmolLM-135M-Instruct | 0.09 | 0.37 | 0.14 | 0.32 | 0.69 | 0.63 | 0.3762 |\n| SmolLM-360M-Instruct | 0.13 | 0.42 | 0.18 | 0.36 | 0.74 | 0.71 | 0.4233 |\n| Qwen2-0.5B-Instruct | 0.20 | 0.50 | 0.25 | 0.43 | 0.82 | 0.79 | 0.4985 |\n| Qwen2-1.5B-Instruct | 0.26 | 0.54 | 0.31 | 0.48 | 0.84 | 0.83 | 0.5433 |\n| Slim Language Models (ours) |  |  |  |  |  |  |  |\n| SlimLM-125M<sup>a</sup> | 0.12 | 0.40 | 0.17 | 0.35 | 0.73 | 0.66 | 0.4061 |\n| SlimLM-270M | 0.17 | 0.46 | 0.22 | 0.40 | 0.79 | 0.74 | 0.4620 |\n| SlimLM-350M<sup>b</sup> | 0.16 | 0.45 | 0.22 | 0.39 | 0.78 | 0.74 | 0.4570 |\n| SlimLM-450M<sup>c</sup> | 0.20 | 0.49 | 0.25 | 0.43 | 0.80 | 0.77 | 0.4893 |\n| SlimLM-760M | 0.20 | 0.49 | 0.25 | 0.43 | 0.81 | 0.78 | 0.4921 |\n| SlimLM-1B<sup>d</sup> | 0.23 | 0.52 | 0.28 | 0.46 | 0.82 | 0.81 | 0.5194 |", "caption": "Table 6: Intent Classification accuracy of various language models after fine-tuning on DocAssist dataset.", "description": "This table displays the accuracy of various language models in classifying the intent of user requests after fine-tuning on the DocAssist dataset.  The models were evaluated on their ability to correctly identify whether a user's input was a summarization, question, suggestion, or answer request.  Higher accuracy indicates better performance in intent classification.", "section": "3.2 Results"}, {"content": "| Model | BLEU \u2191 | ROUGE-1 \u2191 | ROUGE-2 \u2191 | ROUGE-L \u2191 | STS Score \u2191 | GEval \u2191 | Average |\n|---|---|---|---|---|---|---|---| \n| GPT-4o-mini | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 0.90 | 0.9830 |\n| SmolLM-135M-Instruct | 0.18 | 0.45 | 0.26 | 0.42 | 0.72 | 0.56 | 0.4300 |\n| SmolLM-360M-Instruct | 0.22 | 0.49 | 0.31 | 0.46 | 0.76 | 0.67 | 0.4860 |\n| Qwen2-0.5B-Instruct | 0.30 | 0.57 | 0.39 | 0.54 | 0.81 | 0.79 | 0.5687 |\n| Qwen2-1.5B-Instruct | 0.36 | 0.62 | 0.44 | 0.59 | 0.84 | 0.85 | 0.6157 |\n| Slim Language Models (ours) |  |  |  |  |  |  |  |\n| SlimLM-125Ma | 0.22 | 0.49 | 0.30 | 0.46 | 0.75 | 0.62 | 0.4731 |\n| SlimLM-270M | 0.24 | 0.52 | 0.33 | 0.49 | 0.78 | 0.69 | 0.5077 |\n| SlimLM-350Mb | 0.26 | 0.53 | 0.35 | 0.50 | 0.78 | 0.72 | 0.5246 |\n| SlimLM-450Mc | 0.29 | 0.56 | 0.37 | 0.53 | 0.80 | 0.75 | 0.5491 |\n| SlimLM-760Mc | 0.30 | 0.57 | 0.39 | 0.54 | 0.81 | 0.79 | 0.5679 |\n| SlimLM-1Bd | 0.32 | 0.60 | 0.41 | 0.57 | 0.83 | 0.81 | 0.5907 |", "caption": "Table 7: Fact-checking questions asked to measure a model\u2019s efficiency on real mobile devices.", "description": "This table presents five simple fact-checking questions used to evaluate the efficiency of language models on mobile devices. The questions cover a variety of topics and lengths, allowing for a comprehensive assessment of inference speed and resource usage on mobile hardware.  The simplicity of the questions ensures that differences in performance are primarily due to the model's efficiency, rather than the complexity of the question itself.", "section": "2.1 Sweet Spot: Model Size, Context Length and Inference Time"}, {"content": "| Model | BLEU \u2191 | ROUGE-1 \u2191 | ROUGE-2 \u2191 | ROUGE-L \u2191 | STS Score \u2191 | Diversity \u2193 | Average |\n|---|---|---|---|---|---|---|---| \n| GPT-4o-mini | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 0.04 | 1.0000 |\n| SmolLM-135M-Instruct | 0.04 | 0.29 | 0.11 | 0.29 | 0.49 | 0.05 | 0.2434 |\n| SmolLM-360M-Instruct | 0.07 | 0.34 | 0.15 | 0.33 | 0.53 | 0.03 | 0.2837 |\n| Qwen2-0.5B-Instruct | 0.12 | 0.39 | 0.20 | 0.38 | 0.59 | 0.02 | 0.3381 |\n| Qwen2-1.5B-Instruct | 0.16 | 0.44 | 0.25 | 0.43 | 0.63 | 0.02 | 0.3837 |\n| Slim Language Models (ours) |  |  |  |  |  |  |  |\n| SlimLM-125M<sup>a</sup> | 0.07 | 0.33 | 0.14 | 0.32 | 0.52 | 0.04 | 0.2754 |\n| SlimLM-270M | 0.10 | 0.37 | 0.18 | 0.36 | 0.56 | 0.03 | 0.3122 |\n| SlimLM-350M<sup>b</sup> | 0.10 | 0.36 | 0.18 | 0.35 | 0.56 | 0.03 | 0.3109 |\n| SlimLM-450M<sup>c</sup> | 0.11 | 0.39 | 0.20 | 0.38 | 0.59 | 0.02 | 0.3326 |\n| SlimLM-760M<sup>c</sup> | 0.12 | 0.39 | 0.20 | 0.38 | 0.59 | 0.02 | 0.3389 |\n| SlimLM-1B<sup>d</sup> | 0.15 | 0.43 | 0.24 | 0.42 | 0.62 | 0.02 | 0.3713 |", "caption": "Table 8: Summarizing requests used to measure a model\u2019s efficiency with different input contexts on real mobile devices.", "description": "This table presents five different summarization prompts used to evaluate the efficiency of language models when processing varying lengths of input text on mobile devices. Each prompt instructs the model to summarize a given document excerpt, with the number of tokens (words) in the excerpt increasing across the prompts (approximately 200, 400, 600, and 800 tokens).  The purpose is to observe how model performance (speed and accuracy) changes with increasing input context length, reflecting a typical real-world scenario of handling documents of various sizes on mobile devices.", "section": "2.1 Sweet Spot: Model Size, Context Length and Inference Time"}, {"content": "| Model | # Layers | # Heads | Model Dimension | Learning Rate | Global Batch Size | # Trained Tokens (billions) |\n|---|---|---|---|---|---|---|\n| SlimLM-125M | 12 | 12 | 2,048 | 3e-4 | 2,048 | 627 |\n| SlimLM-270M | 16 | 64 | 2,048 | 4e-4 | 2,048 | 627 |\n| SlimLM-350M | 24 | 16 | 2,048 | 3e-4 | 2,048 | 627 |\n| SlimLM-450M | 20 | 64 | 2,048 | 3e-4 | 2,048 | 627 |\n| SlimLM-760M | 24 | 12 | 2,048 | 3e-4 | 2,048 | 627 |\n| SlimLM-1B | 24 | 16 | 2,048 | 2e-4 | 2,048 | 627 |", "caption": "Table 9: \n{{summ_req}}.\nInstructional prompt designed to guide GPT-4o-mini how to summarize the document contents.", "description": "This table presents the prompt used to instruct GPT-40-mini on how to generate summaries for documents. The prompt specifies the task as summarizing and provides instructions to ensure the summary is concise, covers the main topic and key points, and avoids including minor details.  This is a crucial part of creating the dataset used to fine-tune the SlimLM model, ensuring the model learns to generate accurate and informative document summaries.", "section": "2.2.2 Data Annotation"}]