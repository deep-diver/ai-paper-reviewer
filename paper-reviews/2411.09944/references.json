{"references": [{"fullname_first_author": "Aakanksha Chowdhery", "paper_title": "Palm: Scaling language modeling with pathways", "publication_date": "2022-04-00", "reason": "This paper is highly influential due to its exploration of scaling language models, a significant advancement that underpins the development of both LLMs and SLMs."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-00", "reason": "This work introduces Llama, a prominent open-source language model that is foundational to many subsequent SLMs, influencing the direction of research and accessibility in the field."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-00", "reason": "As a continuation of the Llama project, this paper presents Llama 2, a significantly improved and widely adopted model, establishing it as a key advancement in open-source SLMs."}, {"fullname_first_author": "Daria Soboleva", "paper_title": "SlimPajama: A 627B token cleaned and deduplicated version of RedPajama", "publication_date": "2023-00-00", "reason": "This paper introduces the SlimPajama dataset, a crucial resource used for pre-training SlimLM, showcasing the importance of high-quality and efficiently processed datasets in building effective language models."}, {"fullname_first_author": "Zechun Liu", "paper_title": "MobileLLM: Optimizing sub-billion parameter language models for on-device use cases", "publication_date": "2024-00-00", "reason": "This research directly addresses the optimization of language models for mobile devices, offering valuable insights and comparisons relevant to the SlimLM project's goals."}]}