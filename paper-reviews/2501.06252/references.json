{"references": [{"fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-05-14", "reason": "This paper is foundational to the field of self-adaptive LLMs, demonstrating the capacity of large language models to adapt to new tasks with minimal fine-tuning, which is a key concept explored and built upon in the present work."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-08", "reason": "This paper establishes the scaling laws that govern the performance of large language models, which are crucial for understanding the computational requirements and limitations of self-adaptation techniques."}, {"fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-06-09", "reason": "This paper introduces LoRA, a parameter-efficient fine-tuning method that is widely used as a baseline and compared against in the current work, demonstrating its impact in the self-adaptive LLM landscape."}, {"fullname_first_author": "Chen Tianlong", "paper_title": "Mixture-of-experts in the era of LLMs: A new odyssey", "publication_date": "2024-00-00", "reason": "This paper provides an overview of the mixture-of-experts (MoE) technique, which is a key component in many self-adaptive LLM architectures, demonstrating its relevance and significance to the presented work."}, {"fullname_first_author": "Klaudia Ba\u0142azy", "paper_title": "Lora-xs: Low-rank adaptation with extremely small number of parameters", "publication_date": "2024-05-17", "reason": "This paper proposes Lora-XS, an efficient adaptation technique that directly addresses the challenges of parameter-efficiency in self-adaptive LLMs, providing a direct comparison for the proposed methods."}]}