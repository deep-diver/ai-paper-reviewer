[{"figure_path": "2410.19168/charts/charts_4_0.png", "caption": "Figure 3: (Left) Distribution of skills required for information extraction questions in the MMAU benchmark across the domains of sound, speech, and music. (Right) Distribution of skills required for reasoning questions in the MMAU benchmark across the same domains. Each question in MMAU demands the model to apply one or more of these skills to generate a reliable and accurate response. Appendix H provides example questions demanding these skills and the specific tasks associated with them. This chart underscores the diverse cognitive abilities necessary for success in the benchmark, mirroring the complexity and expert-level reasoning involved.", "description": "The chart displays the distribution of skills required for information extraction and reasoning questions across three audio domains (speech, sound, and music) in the MMAU benchmark.", "section": "3 THE MMAU BENCHMARK"}, {"figure_path": "2410.19168/charts/charts_8_0.png", "caption": "Figure 5: Performance comparison on the MMAU test with Gaussian noise replacing the original audio. Models like MuLLaMa and SALMONN show little change in performance, indicating limited reliance on audio input, while others show a significant drop, suggesting greater audio dependence.", "description": "The chart compares the performance of various models on the MMAU test when the original audio input is replaced with random Gaussian noise, showing that some models rely less on the audio input than others.", "section": "5.2 ARE LALMS REALLY LISTENING?"}, {"figure_path": "2410.19168/charts/charts_9_0.png", "caption": "Figure 7: Distribution of human-annotated error types across 500 instances for Qwen2-Audio-Instruct (Left) and Gemini Pro v1.5 (Right). Appendix K provides detailed definitions of each error type.", "description": "The chart shows the distribution of error types made by the Qwen2-Audio-Instruct and Gemini Pro v1.5 models across 500 instances, indicating that perceptual errors are the dominant type for both models.", "section": "5.5 PINPOINTING LALM WEAKNESSES: WHERE ARE THEY FALLING SHORT?"}, {"figure_path": "2410.19168/charts/charts_9_1.png", "caption": "Figure 6: Accuracy distribution for Gemini Pro across easy, medium, and hard questions, categorized by skill type. The graph highlights how LALMs excel in some skills across all difficulty levels (e.g., Phonemic Stress Pattern Analysis) but struggle with others (e.g., Temporal Reasoning) regardless of difficulty.", "description": "The radar chart visualizes the performance of Gemini Pro across different skill categories (x-axis) and difficulty levels (color-coded lines) in the MMAU benchmark, showing varying proficiency across skills regardless of difficulty.", "section": "5.4 DEEP DIVE: SKILL-SPECIFIC MODEL PERFORMANCE"}, {"figure_path": "2410.19168/charts/charts_21_0.png", "caption": "Figure 3: (Left) Distribution of skills required for information extraction questions in the MMAU benchmark across the domains of sound, speech, and music. (Right) Distribution of skills required for reasoning questions in the MMAU benchmark across the same domains. Each question in MMAU demands the model to apply one or more of these skills to generate a reliable and accurate response. Appendix H provides example questions demanding these skills and the specific tasks associated with them. This chart underscores the diverse cognitive abilities necessary for success in the benchmark, mirroring the complexity and expert-level reasoning involved.", "description": "The chart shows the distribution of skills required for information extraction and reasoning questions across three audio domains (speech, sound, and music) in the MMAU benchmark.", "section": "3 THE MMAU BENCHMARK"}]