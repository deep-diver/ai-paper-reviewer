[{"figure_path": "2410.19168/tables/table_6_0.html", "caption": "Table 2: Comparison of MMAU with existing audio understanding and reasoning benchmarks across various statistics. MMAU covers all three domains\u2014speech, sound, and music\u2014while having the highest number of information extraction and complex reasoning tasks.", "description": "Table 2 compares MMAU with other audio understanding and reasoning benchmarks across various metrics, highlighting MMAU's superior coverage of domains and complex reasoning tasks.", "section": "3 THE MMAU BENCHMARK"}, {"figure_path": "2410.19168/tables/table_7_0.html", "caption": "Table 3: Performance comparison of various LALMs and LLMs on the test subset of MMAU across sound, speech, and music domains. Human evaluation results are shown for the MMAU test-mini split. We also mark if the training data used to train these models include either of speech, sound or music. The best-performing models in each category are highlighted in bold, and the second-best scores are underlined.", "description": "This table compares the performance of various Large Audio-Language Models (LALMs) and Large Language Models (LLMs) on the MMAU benchmark across different audio domains (sound, speech, music), including human performance on a subset.", "section": "5.1 MAIN RESULTS"}, {"figure_path": "2410.19168/tables/table_16_0.html", "caption": "Table 4: Performance comparison of ALEs on MMAU benchmark.", "description": "Table 4 presents a performance comparison of various Audio-Language Encoders (ALEs) on the MMAU benchmark, showing their average accuracy across sound, music, and speech domains.", "section": "3.3 Comparison with Other Benchmarks"}, {"figure_path": "2410.19168/tables/table_16_1.html", "caption": "Table 5: Comparison of ALEs and LALMs Performance Across Multiple Difficulty Levels", "description": "Table 5 compares the performance of Audio-Language Encoders (ALEs) and Large Audio-Language Models (LALMs) across different difficulty levels of the MMAU benchmark, revealing their strengths and limitations in handling tasks of varying complexity.", "section": "5.2 ARE LALMS REALLY LISTENING?"}, {"figure_path": "2410.19168/tables/table_17_0.html", "caption": "Table 2: Comparison of MMAU with existing audio understanding and reasoning benchmarks across various statistics. MMAU covers all three domains\u2014speech, sound, and music\u2014while having the highest number of information extraction and complex reasoning tasks.", "description": "Table 2 compares MMAU with other audio understanding and reasoning benchmarks across various metrics, highlighting MMAU's superior coverage of domains and task types.", "section": "Related Work"}, {"figure_path": "2410.19168/tables/table_21_0.html", "caption": "Table 2: Comparison of MMAU with existing audio understanding and reasoning benchmarks across various statistics. MMAU covers all three domains\u2014speech, sound, and music\u2014while having the highest number of information extraction and complex reasoning tasks.", "description": "Table 2 compares MMAU against other audio understanding and reasoning benchmarks across various metrics, highlighting MMAU's comprehensiveness in terms of domains, tasks, and reasoning complexity.", "section": "3 THE MMAU BENCHMARK"}, {"figure_path": "2410.19168/tables/table_22_0.html", "caption": "Table 2: Comparison of MMAU with existing audio understanding and reasoning benchmarks across various statistics. MMAU covers all three domains\u2014speech, sound, and music\u2014while having the highest number of information extraction and complex reasoning tasks.", "description": "Table 2 compares MMAU against other existing audio understanding and reasoning benchmarks across various metrics, highlighting MMAU's superior coverage of audio domains and its focus on complex reasoning tasks.", "section": "2 RELATED WORK"}, {"figure_path": "2410.19168/tables/table_23_0.html", "caption": "Table 2: Comparison of MMAU with existing audio understanding and reasoning benchmarks across various statistics. MMAU covers all three domains\u2014speech, sound, and music\u2014while having the highest number of information extraction and complex reasoning tasks.", "description": "Table 2 compares MMAU with other existing audio understanding and reasoning benchmarks based on various statistics such as the number of tasks, domains covered, types of reasoning involved, and difficulty level.", "section": "RELATED WORK"}, {"figure_path": "2410.19168/tables/table_24_0.html", "caption": "Table 2: Comparison of MMAU with existing audio understanding and reasoning benchmarks across various statistics. MMAU covers all three domains\u2014speech, sound, and music\u2014while having the highest number of information extraction and complex reasoning tasks.", "description": "Table 2 compares MMAU with other existing audio understanding and reasoning benchmarks across various statistics, highlighting MMAU's comprehensiveness in covering all three audio domains and having the highest number of information extraction and complex reasoning tasks.", "section": "Related Work"}, {"figure_path": "2410.19168/tables/table_25_0.html", "caption": "Table 2: Comparison of MMAU with existing audio understanding and reasoning benchmarks across various statistics. MMAU covers all three domains\u2014speech, sound, and music\u2014while having the highest number of information extraction and complex reasoning tasks.", "description": "Table 2 compares MMAU benchmark with other existing benchmarks across various aspects such as domain coverage, task types, difficulty level, and the number of questions, highlighting MMAU's comprehensiveness and advanced reasoning capabilities.", "section": "RELATED WORK"}, {"figure_path": "2410.19168/tables/table_26_0.html", "caption": "Table 2: Comparison of MMAU with existing audio understanding and reasoning benchmarks across various statistics. MMAU covers all three domains\u2014speech, sound, and music\u2014while having the highest number of information extraction and complex reasoning tasks.", "description": "Table 2 compares MMAU with other existing audio understanding and reasoning benchmarks across various metrics, highlighting MMAU's comprehensiveness in covering all three audio domains and its focus on complex reasoning tasks.", "section": "Related Work"}, {"figure_path": "2410.19168/tables/table_28_0.html", "caption": "Table 2: Comparison of MMAU with existing audio understanding and reasoning benchmarks across various statistics. MMAU covers all three domains\u2014speech, sound, and music\u2014while having the highest number of information extraction and complex reasoning tasks.", "description": "Table 2 compares MMAU against other existing audio understanding and reasoning benchmarks across various metrics, highlighting MMAU's superior breadth and depth in terms of tasks and reasoning complexity.", "section": "2 RELATED WORK"}, {"figure_path": "2410.19168/tables/table_29_0.html", "caption": "Table 2: Comparison of MMAU with existing audio understanding and reasoning benchmarks across various statistics. MMAU covers all three domains\u2014speech, sound, and music\u2014while having the highest number of information extraction and complex reasoning tasks.", "description": "Table 2 compares MMAU with other audio understanding and reasoning benchmarks across various metrics such as the number of tasks, domains covered, and complexity of reasoning.", "section": "3 THE MMAU BENCHMARK"}, {"figure_path": "2410.19168/tables/table_29_1.html", "caption": "Table 3: Performance comparison of various LALMs and LLMs on the test subset of MMAU across sound, speech, and music domains. Human evaluation results are shown for the MMAU test-mini split. We also mark if the training data used to train these models include either of speech, sound or music. The best-performing models in each category are highlighted in bold, and the second-best scores are underlined.", "description": "Table 3 compares the performance of various Large Audio Language Models (LALMs) and Large Language Models (LLMs) on the MMAU benchmark across three audio domains (sound, speech, and music).", "section": "5.1 MAIN RESULTS"}, {"figure_path": "2410.19168/tables/table_30_0.html", "caption": "Table 3: Performance comparison of various LALMs and LLMs on the test subset of MMAU across sound, speech, and music domains. Human evaluation results are shown for the MMAU test-mini split. We also mark if the training data used to train these models include either of speech, sound or music. The best-performing models in each category are highlighted in bold, and the second-best scores are underlined.", "description": "This table presents a performance comparison of 18 different Large Audio-Language Models (LALMs) and Large Language Models (LLMs) on the MMAU benchmark, showcasing their accuracy across three audio domains (sound, speech, and music) and highlighting the top-performing models in each category.", "section": "5 RESULTS AND DISCUSSION"}, {"figure_path": "2410.19168/tables/table_31_0.html", "caption": "Table 2: Comparison of MMAU with existing audio understanding and reasoning benchmarks across various statistics. MMAU covers all three domains\u2014speech, sound, and music\u2014while having the highest number of information extraction and complex reasoning tasks.", "description": "Table 2 compares MMAU with other existing benchmarks across various dimensions such as the number of tasks, domains covered, and difficulty level, highlighting MMAU's comprehensiveness and focus on complex reasoning.", "section": "3 THE MMAU BENCHMARK"}]