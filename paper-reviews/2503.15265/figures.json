[{"figure_path": "https://arxiv.org/html/2503.15265/x1.png", "caption": "Figure 1: Gallery of DeepMesh\u2019s generation results. DeepMesh efficiently generates aesthetic, artist-like meshes conditioned on the given point cloud.", "description": "This figure showcases a variety of 3D meshes generated by the DeepMesh model.  Each mesh is conditioned on a given point cloud as input, demonstrating the model's ability to efficiently create aesthetically pleasing, artist-quality meshes from this sparse input data. The variety of the meshes highlights DeepMesh's capabilities in producing diverse, high-fidelity results.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.15265/x2.png", "caption": "Figure 2: An overview of our method. DeepMesh is an auto-regressive transformer composed of both self-attention and cross-attention layers. The model is pre-trained on discrete mesh tokens generated by our improved tokenization algorithm. To further enhance the quality of results, we propose a scoring standard that combines 3D metrics with human evaluation. With this standard, we annotate 5,000 preference pairs and then post-train the model with DPO to align its outputs with human preferences.", "description": "This figure illustrates the DeepMesh architecture and training process.  DeepMesh uses an autoregressive transformer with self-attention and cross-attention layers to generate meshes.  The model is first pre-trained using a novel tokenization algorithm on discrete mesh tokens. Then, a scoring system that combines 3D metrics with human evaluation is used to create 5000 preference pairs. Finally, the model is fine-tuned with Direct Preference Optimization (DPO) using these preference pairs to better align the generated meshes with human preferences.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.15265/x3.png", "caption": "Figure 3: Distribution of face count in training dataset. We present the distribution of face counts in our training dataset. Our dataset size is approximately 500k, with an average face count of 8k.", "description": "This histogram displays the distribution of polygon face counts within the DeepMesh training dataset.  The x-axis represents the number of faces per mesh, and the y-axis shows the frequency of meshes with that face count.  The dataset comprises approximately 500,000 meshes, with an average of 8,000 faces per mesh. This visualization highlights the prevalence of high-polygon meshes in the dataset, which is important to note for model training.", "section": "3.2. Pre-training of DeepMesh"}, {"figure_path": "https://arxiv.org/html/2503.15265/x4.png", "caption": "Figure 4: Some examples of the collected preference pairs. We annotate the preferred meshes based on their geometry completeness, surface details and wireframe structure.", "description": "This figure displays examples from a dataset of mesh preference pairs used in training the DeepMesh model.  Each row shows a pair of generated meshes, one preferred and one rejected.  The preference annotations are based on three key aspects: how complete the mesh geometry is (geometry completeness), the level of detail in the mesh's surface (surface details), and how clean and well-organized the mesh's wireframe is (wireframe structure).  This dataset helps train the model to generate more aesthetically pleasing meshes that satisfy human preferences.", "section": "3.3 Performance Enhancement by DPO"}, {"figure_path": "https://arxiv.org/html/2503.15265/x5.png", "caption": "Figure 5: Qualitative comparison on point cloud conditioned generation between DeepMesh and baselines. DeepMesh outperforms baselines in both generated geometry and preservation of fine-grained details. The meshes generated by ours have much more faces than others.", "description": "Figure 5 presents a qualitative comparison of point cloud-conditioned mesh generation results between DeepMesh and several baseline methods.  The figure shows that DeepMesh generates meshes with superior geometric accuracy and preservation of fine details compared to other methods.  A key observation is that DeepMesh produces meshes with a significantly higher number of faces, suggesting a greater level of detail and complexity.", "section": "4.2.1 Point-cloud Conditioned"}, {"figure_path": "https://arxiv.org/html/2503.15265/x6.png", "caption": "Figure 6: Image-conditioned generation results of our method. Our method can generate high-fidelity meshes aligned with the input images.", "description": "This figure showcases examples of 3D meshes generated by the DeepMesh model when conditioned on input images.  The results demonstrate the model's capability to generate high-fidelity meshes that accurately reflect the details and overall structure present in the original images, showcasing a strong alignment between the input image and generated 3D mesh.", "section": "4.2.2. Image Conditioned"}, {"figure_path": "https://arxiv.org/html/2503.15265/x7.png", "caption": "Figure 7: Diversity of generations. DeepMesh can generate meshes with diverse appearance given the same point cloud.", "description": "Given the same input point cloud, DeepMesh demonstrates its ability to generate a variety of meshes with different appearances, showcasing its capacity for creative and diverse output.", "section": "4.2.3. Diversity"}, {"figure_path": "https://arxiv.org/html/2503.15265/x8.png", "caption": "Figure 8: Ablation study on the effectiveness of DPO. We can observe that while both approaches yield excellent geometry, the results generated using DPO are more visually appealing.", "description": "This ablation study compares mesh generation results with and without Direct Preference Optimization (DPO).  Both methods produce meshes with good geometric accuracy. However, the image clearly shows that meshes generated with DPO are aesthetically more pleasing and visually appealing, demonstrating the effectiveness of DPO in enhancing the visual quality of the generated meshes.", "section": "3.3 Performance Enhancement by DPO"}, {"figure_path": "https://arxiv.org/html/2503.15265/x9.png", "caption": "Figure 9: Details of our tokenization algorithm. We first traverse mesh faces by dividing them into patches according to their connectivity and quantize each vertex of faces into r\ud835\udc5fritalic_r bins (in our setting r=512\ud835\udc5f512r=512italic_r = 512).Then we partition the whole coordinate system into three hierarchical levels of blocks and index the quantized coordinates as offsets within each block. We merge the index of neighbor vertices if they have the identical values.", "description": "This figure details a novel mesh tokenization algorithm.  The process begins by traversing the mesh faces, grouping them into connected patches. Each vertex within these patches is then quantized into one of 512 discrete values. The entire coordinate system is divided into a three-level hierarchy of blocks, and each quantized coordinate is represented as an offset within its block.  Finally, to further reduce the length of the token sequence, identical offsets for neighboring vertices are merged.", "section": "3.1 Tokenization Algorithm"}, {"figure_path": "https://arxiv.org/html/2503.15265/x10.png", "caption": "Figure 10: Comparison with other tokenization algorithms in training effciency. We integrate all tokenization algorithms into our model architecture and train them on a dataset of 80 meshes for each face count category (10K, 20K, 30K, 40K). Our method achieves the fastest training time across all face count categories, demonstrating superior training efficiency.", "description": "This figure compares the training efficiency of DeepMesh's novel tokenization algorithm against existing methods (AMT, EdgeRunner, BPT).  The experiment involved integrating each algorithm into the same model architecture and training on a dataset containing 80 meshes per face count category (10K, 20K, 30K, 40K faces). The results demonstrate that DeepMesh's tokenization method consistently achieves the fastest training times across all face count categories, highlighting its superior training efficiency.", "section": "4.4.1 Tokenization Algorithm"}, {"figure_path": "https://arxiv.org/html/2503.15265/x11.png", "caption": "(a) Before data curation", "description": "The figure shows the training loss curves before and after data curation. Before data curation, the loss curve exhibits frequent spikes, indicating instability in the training process due to low-quality data samples.  After data curation, where low-quality samples were removed, the training loss curve becomes significantly more stable, highlighting the positive effect of data curation on the training process.", "section": "3.2 Pre-training of DeepMesh"}, {"figure_path": "https://arxiv.org/html/2503.15265/x12.png", "caption": "(b) After data curation", "description": "This figure shows the impact of data curation on the training process of the DeepMesh model. The graph displays the training loss over time, comparing the original training data (before curation) with the curated dataset (after curation). The graph demonstrates that the training process is significantly more stable after the data curation step, as indicated by the reduced frequency and magnitude of loss spikes in the loss curve.", "section": "3.2 Pre-training of DeepMesh"}, {"figure_path": "https://arxiv.org/html/2503.15265/x13.png", "caption": "Figure 11: Training loss before and after data curation. Before data curation, we observe frequent loss spikes. After data curation, pre\u2011training becomes significantly more stable.", "description": "This figure shows a comparison of training loss curves before and after data curation. The graph on the left (before data curation) displays frequent and significant spikes in the loss, indicating instability in the training process. This instability likely results from the inclusion of noisy or low-quality data in the training set. The graph on the right (after data curation) shows a much smoother and stable loss curve. The data curation process removed problematic data points, leading to a more consistent and stable model training process.  The smoother curve indicates that the model is learning more effectively and consistently without the disruptions caused by the poor-quality data.", "section": "3.2 Pre-training of DeepMesh"}, {"figure_path": "https://arxiv.org/html/2503.15265/x14.png", "caption": "Figure 12: More results of DeepMesh. We present more high-fidelity results generated by our method.", "description": "Figure 12 showcases additional examples of 3D meshes generated by the DeepMesh model.  These examples highlight the model's ability to generate high-fidelity, detailed meshes across a range of object categories and styles. The variety of objects demonstrates the model's versatility and capacity to handle complex geometries. Each mesh is presented as a wireframe rendering, clearly showing the intricate detail and topology of the generated models.", "section": "4.2 Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2503.15265/x15.png", "caption": "Figure 13: More results of DeepMesh. We present more high-fidelity results generated by our method.", "description": "Figure 13 showcases additional high-quality 3D meshes generated by the DeepMesh model.  These examples demonstrate the model's ability to create detailed and aesthetically pleasing meshes across a variety of object categories, highlighting its capacity for generating complex and intricate 3D structures.", "section": "4.2 Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2503.15265/x16.png", "caption": "Figure 14: High resolution results of our generated meshes.", "description": "This figure showcases high-resolution renderings of 3D meshes generated by the DeepMesh model.  The detailed view highlights the model's capability to create intricate and realistic surface details, demonstrating the effectiveness of the proposed method in generating high-quality, artist-like 3D assets.", "section": "4.2 Qualitative Results"}]