[{"figure_path": "https://arxiv.org/html/2501.01427/x2.png", "caption": "Figure 1: \nDemonstrations for video object insertion.\nVideoAnydoor preserves the fine-grained object details and enables users to control the motion with boxes or point trajectories.\nBased on the robust insertion, users could further add multiple objects iteratively or swap objects in the same video.\nCompared with the previous works, VideoAnydoor demonstrates significant superiority.", "description": "Figure 1 showcases VideoAnydoor's video object insertion capabilities.  The top row displays examples of object insertion where VideoAnydoor maintains fine details of the inserted objects while allowing users to precisely control object motion using bounding boxes or point trajectories. The bottom row compares VideoAnydoor's results to those of other methods, highlighting its superior performance in preserving object fidelity and enabling smooth, controlled motion.  Users can also iteratively add multiple objects or swap existing ones within the same video using VideoAnydoor.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2501.01427/x3.png", "caption": "Figure 2: \nThe pipelines of our VideoAnydoor. First, we input the concatenation of the original video, object masks, and masked video into the 3D U-Net. Meanwhile, the background-removed reference image is fed into the ID extractor, and the obtained features are injected into the 3D U-Net. In our pixel warper, the reference image marked with key points and the trajectories are utilized as inputs for the content and motion encoders. Then, the extracted embeddings are input into cross-attentions for further fusion. The fused results serve as the input of a ControlNet, which extracts multi-scale features for fine-grained injection of motion and identity. The framework is trained with reweight reconstruction losses. We use a blend of real videos and image-simulated videos for training to compensate for the data scarcity.", "description": "The figure illustrates the architecture of VideoAnydoor, a video object insertion framework.  It begins by concatenating the original video, object masks, and the masked video, feeding this into a 3D U-Net. Simultaneously, a background-removed reference image is processed by an ID extractor, and the resulting features are incorporated into the 3D U-Net.  A pixel warper module utilizes a reference image with keypoints and trajectory information to generate content and motion encodings. These encodings are then fused via cross-attention mechanisms and fed into a ControlNet for fine-grained control of motion and identity. The entire framework is trained using a reweighted reconstruction loss function and a mixed dataset of real and simulated video data to overcome data limitations.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.01427/x4.png", "caption": "Figure 3: Pipeline of trajectory generation for training data. We first perform NMS to filter out densely-distributed points and then select points with larger motion. The retained ones can be sparsely distributed in each part of the target and contain more motion information, thus inducing more precise control.", "description": "This figure illustrates the process of generating trajectories for training the VideoAnydoor model.  The process begins by using a method (likely X-pose) to initially identify keypoints within the first frame of a video.  If insufficient keypoints are detected, a grid-based approach is used as a fallback.  Subsequently, a non-maximum suppression (NMS) step is applied to reduce redundancy by removing densely clustered points, retaining only those points demonstrating significant movement. The resulting, more sparsely distributed set of keypoints, each with associated motion trajectories, provides more robust and precise motion guidance for training the model.", "section": "3.2 Pixel Warper"}, {"figure_path": "https://arxiv.org/html/2501.01427/x5.png", "caption": "Figure 4: Comparison results between VideoAnydoor and existing state-of-the-art video editing works. Our VideoAnydoor can achieve superior performance on precise control of both motion and content.", "description": "Figure 4 presents a comparison of video editing results between VideoAnydoor and three other state-of-the-art methods (ReVideo, AnyV2V, and a baseline representing the original video).  The comparison highlights VideoAnydoor's superior performance in precisely controlling both the motion and content of inserted objects within videos.  The figure visually demonstrates the differences in the quality and accuracy of object insertion and motion between the different methods, showing VideoAnydoor's ability to maintain fine-grained details while achieving seamless integration of objects into the video sequences.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.01427/x6.png", "caption": "Figure 5: Demonstrations for precise motion control. VideoAnydoor can achieve precise alignment with the given trajectories and objects when using a pair of reference images marked with key-points and corresponding trajectory maps as input.", "description": "Figure 5 showcases VideoAnydoor's capability for precise motion control.  The examples demonstrate that by providing reference images with marked keypoints and their corresponding trajectories, VideoAnydoor accurately inserts objects into videos while perfectly matching the specified movements.  This highlights the system's ability to handle fine-grained motion control and achieve seamless integration of objects into dynamic video scenes.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.01427/x7.png", "caption": "Figure 6: More visual examples of VideoAnydoor. It preserves fine-grained details (e.g., logos on the car) and achieves smooth motion control (e.g., the tail of the cat) with our pixel warper.", "description": "Figure 6 showcases additional examples of VideoAnydoor's capabilities.  The examples highlight the model's ability to maintain fine details, such as logos on a car, while simultaneously achieving smooth and accurate motion control, as demonstrated by the fluid movement of a cat's tail. This is achieved using the pixel warper component of the VideoAnydoor framework.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.01427/x8.png", "caption": "Figure 7: Qualitative ablation studies on the core components of VideoAnydoor. When removing the pixel warper, it suffers from poor motion consistency due to the undesired posture. And it can be observed that all the components contribute to the best performance.", "description": "Figure 7 presents a qualitative ablation study on the core components of the VideoAnydoor model.  The figure shows several variations of the model, each with one or more components removed (e.g., removing the pixel warper, removing the re-weighted loss, etc.). By comparing the results of these variations to the full model, the figure demonstrates the contribution of each component to the overall performance of the model.  Specifically, it highlights that removing the pixel warper leads to poor motion consistency due to the resulting undesired object postures. The experiment clearly shows that the combined effect of all components leads to superior performance, showcasing the synergistic relationship between different parts of the VideoAnydoor framework.", "section": "4.4. Ablation Studies"}]