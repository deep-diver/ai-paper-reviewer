[{"figure_path": "2410.16271/figures/figures_1_0.png", "caption": "Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors", "description": "The figure compares FrugalNeRF's performance against other state-of-the-art methods in terms of PSNR and training time using only two views.", "section": "1. Introduction"}, {"figure_path": "2410.16271/figures/figures_2_0.png", "caption": "Figure 2. Comparisons between few-shot NeRF approaches. (a) Frequency regularization gradually increases the visibility of high-frequency signals of positional encoding, but the training speed is slow. (b) Replacing the MLPs with voxels and incorporating them with gradual voxel upsampling achieves similar frequency regularization but cannot generalize well. (c) Some approaches employ pre-trained models to supervise the rendered color or depth patches. (d) Our FrugalNeRF, leveraging weight-sharing voxels across scales for various frequencies representation, enhanced by a cross-scale geometric adaptation for efficient supervision.", "description": "Figure 2 illustrates and compares different few-shot NeRF approaches, highlighting FrugalNeRF's unique weight-sharing multi-scale voxel structure and cross-scale geometric adaptation.", "section": "2. Related Work"}, {"figure_path": "2410.16271/figures/figures_4_0.png", "caption": "Figure 3. Overview of FrugalNeRF architecture. (a) Our FrugalNeRF represents a scene with a pair of density and appearance voxels (VD, VA). For a better graphical illustration, we show only one voxel in the figure. (b) We sample rays from not only training input views rtrain but also randomly sampled novel views rnovel. (c) We then create L + 1 multi-scale voxels by hierarchical subsampling, where lower-resolution voxels ensure global geometry consistency and reduce overfitting but suffer from representing detailed structures, while higher-resolution voxels capture fine details but may get stuck in the local minimum or generate floaters. (d) For the rays from training views rtrain, we enforce an MSE reconstruction loss between the volume rendered RGB color \u0108 and input RGB C at each scale. (e) We introduce a cross-scale geometric adaptation loss for novel view rays rnovel, warping volume-rendered RGB to the nearest training view using predicted depth, calculating projection errors e' at each scale, and using the depth with the minimum reprojection error as pseudo-GT for depth supervision. This adaptation involves rays from both training and novel views, though the figure only depicts novel view rays for clarity.", "description": "Figure 3 illustrates the FrugalNeRF architecture, detailing its multi-scale voxel representation, ray sampling strategies, and cross-scale geometric adaptation for training.", "section": "3.2. Overview of FrugalNeRF"}, {"figure_path": "2410.16271/figures/figures_7_0.png", "caption": "Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors", "description": "Figure 1 compares FrugalNeRF's performance against other state-of-the-art methods on novel view synthesis using only two training views, highlighting its speed and quality.", "section": "1. Introduction"}, {"figure_path": "2410.16271/figures/figures_8_0.png", "caption": "Figure 4. Qualitative comparisons on the LLFF [49] dataset with two input views. FrugalNeRF achieves better synthesis quality and coherent geometric depth. We also include the GT and overlapped input images for reference.", "description": "Figure 4 presents a qualitative comparison of FrugalNeRF against other state-of-the-art methods on the LLFF dataset, showcasing its superior synthesis quality and coherent geometric depth.", "section": "4.1 Comparisons"}, {"figure_path": "2410.16271/figures/figures_9_0.png", "caption": "Figure 8. Scene dependency analysis of the multi-scale voxels. Cross-scale geometric adaptation can adapt to diverse scenes.", "description": "The figure shows that different scenes activate different frequency bands in multi-scale voxels, demonstrating the model's adaptability.", "section": "4.2. Ablation Studies"}, {"figure_path": "2410.16271/figures/figures_25_0.png", "caption": "Figure 10. More qualitative comparisons on the LLFF [48] dataset with two input views. FrugalNeRF achieves better synthesis quality in different scenes.", "description": "Figure 10 shows qualitative comparisons of novel view synthesis results on the LLFF dataset using FrugalNeRF and other state-of-the-art methods with two input views.", "section": "4. Experiments"}, {"figure_path": "2410.16271/figures/figures_26_0.png", "caption": "Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors", "description": "The figure compares FrugalNeRF against other state-of-the-art methods in terms of PSNR and training time using only two views for training, highlighting FrugalNeRF's superior efficiency and quality.", "section": "1. Introduction"}, {"figure_path": "2410.16271/figures/figures_26_1.png", "caption": "Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors", "description": "Figure 1 shows a comparison of FrugalNeRF against other state-of-the-art methods in terms of PSNR and training time using only two views for training.", "section": "1. Introduction"}, {"figure_path": "2410.16271/figures/figures_27_0.png", "caption": "Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors", "description": "Figure 1 shows a comparison of FrugalNeRF against other state-of-the-art methods in terms of PSNR and training time using only two views for training.", "section": "1. Introduction"}]