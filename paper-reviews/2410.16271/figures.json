[{"figure_path": "2410.16271/figures/figures_1_0.png", "caption": "Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors", "description": "Figure 1 shows a comparison of FrugalNeRF against other state-of-the-art methods in terms of PSNR and training time using only two views for training.", "section": "1. Introduction"}, {"figure_path": "2410.16271/figures/figures_2_0.png", "caption": "Figure 2. Comparisons between few-shot NeRF approaches. (a) Frequency regularization gradually increases the visibility of high-frequency signals of positional encoding, but the training speed is slow. (b) Replacing the MLPs with voxels and incorporating them with gradual voxel upsampling achieves similar frequency regularization but cannot generalize well. (c) Some approaches employ pre-trained models to supervise the rendered color or depth patches. (d) Our FrugalNeRF, leveraging weight-sharing voxels across scales for various frequencies representation, enhanced by a cross-scale geometric adaptation for efficient supervision.", "description": "Figure 2 compares different few-shot NeRF approaches, highlighting FrugalNeRF's efficient use of weight-sharing voxels and cross-scale geometric adaptation for faster convergence and improved generalization.", "section": "2. Related Work"}, {"figure_path": "2410.16271/figures/figures_4_0.png", "caption": "Figure 3. Overview of FrugalNeRF architecture. (a) Our FrugalNeRF represents a scene with a pair of density and appearance voxels (VD, VA). For a better graphical illustration, we show only one voxel in the figure. (b) We sample rays from not only training input views rtrain but also randomly sampled novel views rnovel. (c) We then create L + 1 multi-scale voxels by hierarchical subsampling, where lower-resolution voxels ensure global geometry consistency and reduce overfitting but suffer from representing detailed structures, while higher-resolution voxels capture fine details but may get stuck in the local minimum or generate floaters. (d) For the rays from training views rtrain, we enforce an MSE reconstruction loss between the volume rendered RGB color \u0108 and input RGB C at each scale. (e) We introduce a cross-scale geometric adaptation loss for novel view rays rnovel, warping volume-rendered RGB to the nearest training view using predicted depth, calculating projection errors e' at each scale, and using the depth with the minimum reprojection error as pseudo-GT for depth supervision. This adaptation involves rays from both training and novel views, though the figure only depicts novel view rays for clarity.", "description": "Figure 3 illustrates the FrugalNeRF architecture, showcasing its multi-scale voxel representation, ray sampling strategy, training loss functions, and cross-scale geometric adaptation mechanism.", "section": "3.2. Overview of FrugalNeRF"}, {"figure_path": "2410.16271/figures/figures_7_0.png", "caption": "Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors", "description": "Figure 1 shows comparisons of FrugalNeRF against other state-of-the-art methods in terms of PSNR and training time when trained using only two views.", "section": "1. Introduction"}, {"figure_path": "2410.16271/figures/figures_8_0.png", "caption": "Figure 4. Qualitative comparisons on the LLFF [49] dataset with two input views. FrugalNeRF achieves better synthesis quality and coherent geometric depth. We also include the GT and overlapped input images for reference.", "description": "Figure 4 shows a qualitative comparison of FrugalNeRF against other state-of-the-art methods on the LLFF dataset, highlighting its superior synthesis quality and coherent geometric depth.", "section": "4. Experiments"}, {"figure_path": "2410.16271/figures/figures_9_0.png", "caption": "Figure 8. Scene dependency analysis of the multi-scale voxels. Cross-scale geometric adaptation can adapt to diverse scenes.", "description": "The figure visualizes how different scene types activate different frequency bands in the multi-scale voxel representation of FrugalNeRF, demonstrating its adaptability.", "section": "4.2. Ablation Studies"}, {"figure_path": "2410.16271/figures/figures_25_0.png", "caption": "Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors", "description": "Figure 1 shows the comparison of FrugalNeRF with other state-of-the-art methods in terms of PSNR and training time using only two views for training.", "section": "1. Introduction"}, {"figure_path": "2410.16271/figures/figures_26_0.png", "caption": "Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors", "description": "The figure compares the performance of FrugalNeRF against other state-of-the-art methods in terms of PSNR and training time using only two views for training.", "section": "1. Introduction"}, {"figure_path": "2410.16271/figures/figures_26_1.png", "caption": "Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors", "description": "Figure 1 shows the comparison of FrugalNeRF with other state-of-the-art methods in terms of PSNR and training time using only two views for training.", "section": "1. Introduction"}, {"figure_path": "2410.16271/figures/figures_27_0.png", "caption": "Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors", "description": "Figure 1 shows the PSNR comparison between FrugalNeRF and other state-of-the-art methods with only two views for training, highlighting FrugalNeRF's superior efficiency and quality.", "section": "Abstract"}]