[{"figure_path": "2410.16271/charts/charts_1_0.png", "caption": "Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors", "description": "The chart compares FrugalNeRF's performance against other state-of-the-art methods in terms of PSNR and training time using only two views for training.", "section": "1. Introduction"}, {"figure_path": "2410.16271/charts/charts_9_0.png", "caption": "Figure 6. Cross-scale geometric adaptation in training. (Left) In the early training phase, low-resolution voxels primarily act as pseudo-ground truth, guiding the model\u2019s geometric learning. As training goes on, medium- and high-resolution voxels increasingly contribute to refining scene geometry. This adaptive approach enables the model to autonomously tune into appropriate frequencies at each stage, enhancing its ability to generalize across various scenes. (Right) Without geometric adaptation, all of the scales result in sub-optimal solutions. Geometric adaptation drives convergence to higher quality across all scales.", "description": "The chart visualizes how the proportion of each voxel scale serving as pseudo-ground truth changes during training, demonstrating the cross-scale geometric adaptation process in FrugalNeRF.", "section": "3.4. Cross-scale geometric adaptation"}, {"figure_path": "2410.16271/charts/charts_9_1.png", "caption": "Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors", "description": "The chart compares the performance of FrugalNeRF against other state-of-the-art methods in terms of PSNR and training time using only two views for training.", "section": "1. Introduction"}, {"figure_path": "2410.16271/charts/charts_9_2.png", "caption": "Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors", "description": "The chart compares the performance of FrugalNeRF against other state-of-the-art methods in terms of PSNR and training time using only two views for training.", "section": "1. Introduction"}, {"figure_path": "2410.16271/charts/charts_9_3.png", "caption": "Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors", "description": "The chart compares the PSNR performance and training time of FrugalNeRF against other state-of-the-art few-shot novel view synthesis methods using only two training views.", "section": "1. Introduction"}]