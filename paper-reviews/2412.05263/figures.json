[{"figure_path": "https://arxiv.org/html/2412.05263/x2.png", "caption": "Figure 1: \nTime-controlled multi-event video generation with MinT.\nGiven a sequence of event text prompts and their desired start and end timestamps, MinT synthesizes smoothly connected events with consistent subjects and backgrounds.\nIn addition, it can control the time span of each event flexibly.\nHere, we show the results of sequential gestures, daily activities, facial expressions, and cat movements.", "description": "This figure showcases the capabilities of the MinT model in generating videos with multiple events, each controlled by text prompts and precise start and end timestamps.  The model successfully synthesizes these events into a coherent video, maintaining consistent subjects and backgrounds throughout.  The duration of each event is also flexible, demonstrating MinT's temporal control. The example videos include a series of hand gestures, everyday activities, facial expressions, and cat movements.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.05263/x3.png", "caption": "Figure 2: \nMulti-event video generation results from state-of-the-art video generators and MinT.\nWe run two open-source models CogVideoX-5B\u00a0[100] and Mochi 1\u00a0[82], and two commercial models Kling 1.5\u00a0[2] and Gen-3 Alpha\u00a0[1] with a text prompt containing four consecutive events.\nAll of them only generate a subset of events while ignoring the remaining ones.\nIn contrast, MinT generates a natural video with all events smoothly connected.\nPlease refer to Sec.\u00a0C.6 and our project page for more comparisons.", "description": "This figure compares the performance of various state-of-the-art video generation models, including open-source options (CogVideoX-5B and Mochi 1) and commercial models (Kling 1.5 and Gen-3 Alpha), against the proposed MinT model.  All models are given the same four-event text prompt.  The figure demonstrates that while the existing models fail to generate all four events, often omitting some entirely, MinT successfully generates a coherent video depicting all four events seamlessly.  Additional comparisons are available in the supplementary material (Sec. C.6) and the project website.", "section": "Multi-event video generation"}, {"figure_path": "https://arxiv.org/html/2412.05263/x4.png", "caption": "Figure 3: \nMinT framework.\n(a) Our model takes in a global caption describing the overall video, and a list of temporal captions specifying the sequential events.\nWe bind each event to a time range, enabling temporal control of the generated events.\n(b) To condition the video DiT on temporal captions, we introduce a new temporal cross-attention layer in each DiT block, which (c) concatenates the text embedding of all event prompts and leverages a time-aware positional encoding (Pos.Enc.) method to associate each event to its corresponding frames based on the event timestamps.\nMinT supports an additional scene cut conditioning, which can control the shot transition of the video.", "description": "Figure 3 illustrates the architecture of the MinT model, a multi-event video generator with temporal control.  Panel (a) shows the input: a global caption summarizing the entire video and a sequence of temporal captions. Each temporal caption describes a specific event and its corresponding start and end times, allowing for precise temporal control.  Panel (b) depicts the core of the MinT model, which includes a modified video diffusion transformer (DiT). A crucial addition is a new 'temporal cross-attention' layer in each DiT block. Panel (c) details this layer's function: it concatenates the text embeddings from all event captions and uses a time-aware positional encoding method (ReRoPE) to map each event to its relevant video frames, enabling temporally aware interactions between the captions and video tokens.  The MinT model also incorporates scene cut conditioning to manage shot transitions in the generated video.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.05263/x5.png", "caption": "Figure 4: \nComparison of vanilla RoPE and our Rescaled RoPE.\nWe use the same random vector for video tokens and text embeddings to only visualize the bias introduced by positional encoding.\n(a) Vanilla RoPE uses raw timestamps as the rotation angle, where frames within one event might be biased to the wrong text.\n(b) We instead rescale all events to have the same length L\ud835\udc3fLitalic_L, so that video tokens always attend the most to the current event.\nIn addition, frames at event boundaries attend to adjacent events equally.", "description": "This figure compares the attention mechanisms of vanilla Rotary Position Embedding (RoPE) and the proposed Rescaled RoPE.  Both methods use positional encoding to guide the attention of video tokens to relevant text embeddings, but they differ in how they handle event durations. Vanilla RoPE uses raw timestamps, which can lead to bias towards incorrect text embeddings, especially near event boundaries. Rescaled RoPE, on the other hand, normalizes all events to a fixed length, ensuring that video tokens within an event attend primarily to the current event's text embedding. It also ensures equal attention weights to adjacent events at event boundaries, facilitating smooth transitions between events. The visualization uses identical random vectors for video tokens and text embeddings to isolate the effects of positional encoding.", "section": "3.2 Temporally Aware Video DiT"}, {"figure_path": "https://arxiv.org/html/2412.05263/x6.png", "caption": "Figure 5: \nT2V results on HoldOut and StoryBench.\nFor CogVideoX and Mochi we concatenated the events into a single prompt, similar to the Concat baseline. Metrics in the first row measure visual quality, while those in the second row focus on the text alignment and transition smoothness between events.\nMinT performs the best in event-related metrics while maintaining a high visual quality.", "description": "Figure 5 presents a quantitative comparison of text-to-video (T2V) generation models on two benchmark datasets, HoldOut and StoryBench.  The experiment evaluates the ability of different models to generate videos from multiple sequential events described within a single prompt.  For the CogVideoX and Mochi models, all events were combined into a single input, mimicking the 'Concat' baseline.  The figure shows that MinT achieves superior results across various metrics measuring both the visual quality of the generated video (FID, FVD, CLIP score) and the coherence of the generated content compared to the text prompts (text-to-video alignment, temporal consistency, and the number of cuts in the video).  In particular, MinT excels in metrics assessing event-related aspects while maintaining high visual quality.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.05263/x7.png", "caption": "Figure 6: \nQualitative results of T2V.\nConcatenating all events into a single prompt (Concat) can only generate the first event.\nAutoregressive generation (AutoReg) suffers from video stagnation and cannot generate the third event.\nMEVG struggles to preserve the person\u2019s identity and produces abrupt event transitions.\nMinT is the only method that generates all events with smooth transitions and consistent content.\nSee Sec.\u00a0C.1 for more qualitative results and our project page for video results.", "description": "Figure 6 presents a qualitative comparison of text-to-video (T2V) generation methods applied to a multi-event scenario. The results demonstrate that simply concatenating all events into a single prompt (Concat) leads to only the first event being generated.  Autoregressive generation (AutoReg), while producing a smoother transition between the first two events, fails to fully generate the third event and suffers from video stagnation. MEVG struggles to maintain consistent subject identity and produces abrupt transitions between events. In contrast, MinT successfully generates all four events in a coherent video with smooth transitions and consistent visual content.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.05263/x8.png", "caption": "Figure 7: \nHuman preference evaluation against T2V baselines.\nMinT is better or competitive in visual quality, and significantly outperforms baselines in the other three event-related metrics.", "description": "This figure displays the results of a human evaluation comparing the performance of MinT against several text-to-video (T2V) baselines across multiple metrics.  The evaluation focused on four key aspects: visual quality, text-to-video alignment, temporal consistency, and the number of cuts in the generated video. The results show that MinT achieves comparable or superior visual quality to the baselines and significantly surpasses them in terms of text alignment, temporal consistency, and the reduction of abrupt transitions (indicated by fewer cuts).", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.05263/x9.png", "caption": "Figure 8: \nQualitative comparison of prompt enhancement results.\nThe original short prompt is \u201ca cat drinking water\u201d.", "description": "This figure displays a qualitative comparison of video generation results using different prompts.  The top row shows videos generated from a short prompt: \"A cat drinking water.\" The middle row presents videos generated from the same short prompt but enhanced with a global caption and a sequence of detailed temporal captions about the cat's actions. The bottom row demonstrates results from using only the enhanced prompts without the original short prompt.  The comparison highlights the impact of detailed prompt engineering on the quality and specificity of the generated videos, showcasing more detailed and accurate actions in the enhanced prompt scenarios.", "section": "4.4. Prompt Enhanced Video Generation"}, {"figure_path": "https://arxiv.org/html/2412.05263/x10.png", "caption": "Figure 9: \nComparison of ReRoPE with different rescaling length L\ud835\udc3fLitalic_L.\nWe use the same random vector for video tokens and text embeddings to only visualize the bias introduced by positional encoding.\nWe visualize the case where videos have a temporal dimension of 50, and there are three temporal captions.", "description": "This figure visualizes how the attention weights change when using different rescaling lengths (L) in the Rescaled Rotary Position Embedding (ReRoPE) method.  The experiment uses the same random vectors for video tokens and text embeddings to isolate and highlight the effect of positional encoding.  It demonstrates the attention patterns when a video has a temporal dimension of 50 frames and contains three temporal captions. The visualization shows how ReRoPE guides the attention mechanism to focus on the relevant event's text embedding at each time step while maintaining smooth transitions between adjacent events.", "section": "3.2 Temporally Aware Video DiT"}, {"figure_path": "https://arxiv.org/html/2412.05263/x11.png", "caption": "Figure 10: \nBasic statistics of our training dataset.\nWe show the distribution of video length, the number of events per video, and the length of individual events.\nMost videos contain 2 to 4 events, and most events are under 5s.", "description": "Figure 10 presents a statistical overview of the training dataset used for the Mind the Time (MinT) model. Three histograms display the distributions of: 1) video length (in seconds), showing the duration of each video; 2) number of events per video, indicating how many distinct events occur in each video; and 3) event length (in seconds), showing the duration of individual events. The histograms reveal that the majority of videos in the training dataset have lengths between 2 to 4 events, with most individual events lasting under 5 seconds.", "section": "Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2412.05263/x12.png", "caption": "Figure 11: \nQualitative comparisons of T2V.", "description": "Figure 11 presents a qualitative comparison of text-to-video (T2V) generation results from four different methods: MinT (the authors' proposed model), MEVG (a state-of-the-art multi-event video generator), AutoReg (an autoregressive approach), and Concat (a baseline that concatenates all event prompts).  The figure shows that MinT successfully generates all four events in a natural, temporally consistent manner. In contrast, the other methods struggle with event generation, resulting in missing events, incorrect temporal order, or visual inconsistencies.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.05263/x13.png", "caption": "Figure 12: \nMore T2V results from MinT.\nPlease see our project page for more results.", "description": "Figure 12 presents additional examples of text-to-video (T2V) generation results produced by the MinT model.  The figure showcases the model's ability to generate coherent videos depicting multiple events within a single continuous scene, with smooth transitions between each event. The prompts used for each video sequence are displayed, highlighting the diversity of scenarios and actions that MinT is capable of handling. Due to space limitations in the paper, the caption directs readers to the project webpage for access to the full collection of results.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.05263/x14.png", "caption": "Figure 13: \nPrompt enhancement results on VBench.\nWe can generate more interesting videos from a simple prompt.\nThis highlights the flexible dynamics control ability brought by the temporal captions.\nPlease see our project page for video results.", "description": "Figure 13 showcases the effectiveness of the MinT model's prompt enhancement technique using the VBench dataset.  The figure demonstrates how short, simple prompts are significantly improved by expanding them into detailed global and temporal captions. This process, aided by an LLM, allows MinT to generate videos containing multiple events, each with specific timing information. The enhanced prompts produce much more dynamic and interesting videos compared to results from just the original simple prompts. The figure highlights the model's capability to control the temporal aspects of video generation, which is made possible through the use of temporal captions.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.05263/x15.png", "caption": "Figure 14: \nGenerated videos with and without scene cut input.\nIn each example, the first row is generated by inputting the scene cut at the illustrated timestamps, while the second row is by zeroing out the scene cut input.\nWhen using the scene cut, the model is able to generate a shot transition at desired timestamps, while keeping the subject consistent.\nIn the second example, the model generates smooth zoom-in and zoom-out effects when zeroing out scene cuts.\nPlease see our project page for more results.", "description": "This figure demonstrates the impact of scene cut conditioning on video generation.  The top row in each example shows videos generated with explicit scene cut timestamps provided to the model. This results in the model creating shot transitions at the specified times while maintaining consistent subjects and backgrounds. The bottom row displays videos generated without scene cut input, effectively disabling the shot transition mechanism. In the second example, this absence of scene cut conditioning leads to a smooth zoom effect instead.  The overall experiment highlights the model's ability to control both the presence and style of shot transitions.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.05263/x16.png", "caption": "Figure 15: \nGenerated videos with different event time spans.\nIn each example, we offset the start and end timestamps of all events by a specific number of seconds.\nResults show that MinT enables fine-grained event timing control while keeping the subjects\u2019 appearances to be roughly the same.\nThis capability is very useful for controllable video generation.\nPlease see our project page for more results.", "description": "This figure demonstrates MinT's ability to precisely control the timing of events within a generated video.  Multiple examples are shown, each with the same events but with the start and end times of each event systematically shifted by a set number of seconds.  The results highlight that MinT can maintain consistent subject appearance and background despite changes to event durations, showcasing its fine-grained temporal control.", "section": "4.3. Image-conditioned Video Generation"}, {"figure_path": "https://arxiv.org/html/2412.05263/x17.png", "caption": "Figure 16: \nGenerated videos with out-of-distribution prompts.\nAfter fine-tuning, MinT still possesses the base model\u2019s ability to generate novel concepts.\nPlease see our project page for more results.", "description": "Figure 16 showcases the ability of the MinT model to generate videos depicting concepts and scenes not explicitly present in its training data.  Despite being fine-tuned on primarily human-centric events, MinT retains the capacity of its base model to generate novel content and combinations, including scenarios involving warriors, astronauts, spacecrafts, and even a cat performing yoga.  This demonstrates that MinT effectively maintains the capabilities of the underlying pre-trained model while enhancing its capacity to produce temporally-controlled multi-event videos. For more examples and videos, please refer to the project page.", "section": "C.7. Out-of-Distribution Prompts"}, {"figure_path": "https://arxiv.org/html/2412.05263/x18.png", "caption": "Figure 17: \nMore comparisons with SOTA video generators.\nWe run SOTA open-source models CogVideoX\u00a0[100] and Mochi\u00a0[82], and commercial models Kling 1.5\u00a0[2] and Gen-3 Alpha\u00a0[1] using their online APIs.\nPlease see our project page for video results.", "description": "This figure compares the performance of MinT against state-of-the-art video generation models (CogVideoX, Mochi, Kling 1.5, and Gen-3 Alpha) on the task of generating videos with multiple events.  It demonstrates MinT's superior ability to generate videos that accurately depict all specified events in the correct order, smoothly transitioning between them, with consistent visual quality, while the other models often fail to generate all events or struggle with smooth transitions and consistent subjects.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2412.05263/x19.png", "caption": "Figure 18: \nMore comparisons with SOTA video generators.\nWe run SOTA open-source models CogVideoX\u00a0[100] and Mochi\u00a0[82], and commercial models Kling 1.5\u00a0[2] and Gen-3 Alpha\u00a0[1] using their online APIs.\nPlease see our project page for video results.", "description": "Figure 18 presents a qualitative comparison of multi-event video generation results from state-of-the-art models, including open-source options like CogVideoX and Mochi, and commercial models such as Kling 1.5 and Gen-3 Alpha.  Each row shows the results for a different set of event prompts.  The figure demonstrates that existing methods struggle to generate videos that accurately capture all events in the specified order and with smooth transitions.  The full videos are available on the project's webpage, as noted in the caption.", "section": "Experiments"}]