[{"content": "| Method | FID \u2193 | FVD \u2193 | VQ \u2191 | DD \u2191 | CLIP-T \u2191 | TA \u2191 | TC \u2191 | #Cuts \u2193 |\n|---|---|---|---|---|---|---|---|---|\n| _Dataset: HoldOut_ |  |  |  |  |  |  |  |  |\n| MEVG | 57.57 | 495.75 | 2.56 | **3.39** | 0.266 | 2.72 | 2.25 | 0.108 |\n| **Ours** | **22.04** | **218.21** | **2.60** | 3.30 | **0.272** | **3.00** | **2.47** | **0.025** |\n| _Dataset: StoryBench_ |  |  |  |  |  |  |  |  |\n| MEVG | 56.51 | 732.94 | 3.27 | **3.80** | 0.265 | 2.83 | 3.03 | 0.150 |\n| **Ours** | **21.85** | **314.59** | **3.36** | 3.76 | **0.273** | **3.37** | **3.29** | **0.014** |", "caption": "Table 1: \nI2V results on HoldOut and StoryBench.\nVQ, DD, TA, and TC stand for visual quality, dynamic degree, text-to-video alignment, and temporal consistency from VideoScore.\n#Cuts is the average number of cuts per video.\nSimilar to T2V, MinT also achieves better visual quality and smooth event transition.", "description": "This table presents a quantitative comparison of image-to-video (I2V) generation results on two benchmark datasets: HoldOut and StoryBench.  It evaluates different models using metrics derived from VideoScore, a state-of-the-art video quality assessment model.  Specifically, it measures visual quality (VQ), dynamic degree (DD), text-to-video alignment (TA), and temporal consistency (TC).  The number of cuts (#Cuts) per video is also reported. The table highlights MinT's superior performance in terms of visual quality and smooth event transitions, similar to its performance in text-to-video generation.", "section": "4. Experiments"}, {"content": "| Method | Subject (Consist. \u2191) | Background (Consist. \u2191) | Aesthetic (Quality \u2191) | Imaging (Quality \u2191) | Motion (Smooth \u2191) | Dynamic (Degree \u2191) |\n|---|---|---|---|---|---|---|\n| Short | 0.857 | 0.939 | 0.498 | 0.583 | 0.995 | 0.481 |\n| Global | 0.890 | 0.950 | 0.541 | 0.613 | 0.995 | 0.517 |\n| **Ours** | 0.900 | 0.950 | 0.544 | 0.609 | 0.988 | **0.711** |", "caption": "Table 2: \nPrompt enhancement results on VBench.\nConsist. means consistency.\nThe first four metrics measure video quality, while we focus on the motion of generated videos.\nMinT generates videos with significantly higher dynamics degree and competitive visual quality and motion smoothness.", "description": "Table 2 presents a quantitative evaluation of the model's performance on the VBench dataset, specifically focusing on videos generated from enhanced prompts.  The metrics assess both video quality (using standard measures like subject and background consistency, aesthetic quality, and image quality) and the dynamism of the generated videos (motion smoothness and dynamic degree). The results highlight MinT's ability to produce videos with a significantly higher dynamic degree than the baselines, while maintaining competitive levels of visual quality and motion smoothness.", "section": "4. Experiments"}, {"content": "| Method | VQ \u2191 | DD \u2191 | CLIP-T \u2191 | TA \u2191 | TC \u2191 | #Cuts \u2193 |\n|---|---|---|---|---|---|---|\n| **Full Model** | 2.56 | 3.32 | **0.270** | **2.92** | 2.44 | 0.026 |\n| Concat time | 2.53 | 3.31 | 0.249 | 2.42 | 2.33 | 0.075 |\n| Hard attn mask | 2.45 | 3.34 | 0.260 | 2.68 | 2.30 | 0.069 |\n| Vanilla RoPE | 2.54 | 3.32 | 0.262 | 2.79 | 2.42 | 0.030 |\n| ReRoPE (L=4) | 2.54 | 3.33 | 0.264 | 2.88 | 2.43 | 0.029 |\n| ReRoPE (L=16) | 2.55 | 3.32 | 0.265 | 2.90 | 2.44 | 0.025 |\n| No cut condition | 2.54 | 3.33 | 0.268 | 2.89 | 2.34 | 0.084 |", "caption": "Table 3: \nAblation results on HoldOut.\nWe study different conditioning mechanisms for event time span, the rescale length L\ud835\udc3fLitalic_L in ReRoPE, and the use of scene cut conditioning.\nVQ, DD, TA, and TC stand for visual quality, dynamic degree, text-to-video alignment, and temporal consistency from VideoScore.\n#Cuts is the average number of scene cuts per video.", "description": "This table presents an ablation study evaluating the impact of different components in the MinT model on the HoldOut dataset.  Specifically, it analyzes the effects of various methods for incorporating event time spans, different values for the rescaling length parameter (L) in the ReRoPE positional encoding, and the inclusion or exclusion of scene cut conditioning. The results are assessed using several metrics derived from the VideoScore model, including visual quality (VQ), dynamic degree (DD), text-to-video alignment (TA), and temporal consistency (TC).  The number of scene cuts (#Cuts) per video is also reported to evaluate transition smoothness.", "section": "3. Method"}, {"content": "| Method | FID \u2193 | FVD \u2193 | CLIP-score \u2191 |\n|---|---|---|---|\n| *Task: T2V (a.k.a. story generation in [12])*\n| Phenaki | 273.41 | 998.19 | 0.210 |\n| **Ours** | **40.87** | **484.44** | **0.284** |\n| *Task: I2V (a.k.a. story continuation in [12])*\n| Phenaki | 240.21 | 674.5 | 0.219 |\n| **Ours** | **21.85** | **314.59** | **0.273** |", "caption": "Table 4: \nComparison with Phenaki on StoryBench.\nWe compare with the zero-shot variant Phenaki-Gen-ZS in their paper\u00a0[12] since our model is not fine-tuned on StoryBench.\nWe clearly outperform Phenaki across all metrics in both tasks.", "description": "This table presents a quantitative comparison of the proposed MinT model against the Phenaki model on the StoryBench benchmark.  The comparison uses the zero-shot variant of Phenaki (Phenaki-Gen-ZS) because MinT was not fine-tuned on StoryBench.  The results show MinT significantly outperforms Phenaki across all metrics (FID, FVD, CLIP-score) in both text-to-video (T2V) and image-to-video (I2V) generation tasks.", "section": "4. Experiments"}]