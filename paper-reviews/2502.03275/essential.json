{"importance": "This paper is crucial for researchers working on **large language models (LLMs)** and **reasoning**.  It addresses the computational cost of chain-of-thought prompting by introducing a novel method to significantly shorten reasoning traces, thus enhancing LLM efficiency and performance.  The proposed technique of using latent tokens opens **new avenues for research on efficient and effective LLM training** and offers a potential solution to improve the overall efficiency of using LLMs for various reasoning and planning tasks. This is highly relevant given the increasing emphasis on reducing the computational footprint of LLMs.", "summary": "Boosting language model reasoning:  A novel hybrid approach using latent tokens drastically shortens reasoning traces, improving model performance and efficiency.", "takeaways": ["A hybrid representation of reasoning processes using latent tokens significantly reduces reasoning trace length, thus improving computational efficiency.", "The proposed training procedure, which mixes latent and text tokens, facilitates effective learning with unseen latent tokens.", "Consistent performance improvements over baseline methods are demonstrated across various benchmarks, including mathematical and logical reasoning tasks."], "tldr": "Current large language models (LLMs) excel at reasoning when provided with step-by-step explanations (chain-of-thought prompting). However, this method leads to lengthy inputs, increasing computational costs.  This paper tackles this issue by focusing on the efficiency problem of chain-of-thought prompting. The core problem is that using many words for explanation consumes lots of computational resources and increases response time. \nThis research introduces a hybrid approach that uses latent discrete tokens (generated by VQ-VAE) to partially represent the initial reasoning steps.  This significantly reduces the input length. The researchers trained the model using a random mixing strategy of latent and text tokens, adapting quickly to new tokens and achieving significant performance gains in various benchmarks (Math, GSM8K, and a fresh Gaokao Math dataset). The experiments show that the proposed method not only improved performance but also significantly reduced reasoning trace length (on average, 17%).", "affiliation": "Meta AI", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2502.03275/podcast.wav"}