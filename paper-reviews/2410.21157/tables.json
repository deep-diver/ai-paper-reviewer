[{"content": "| Benchmark | # Languages | Fine-grained | Training Set | # Test Repos |\n|---|---|---|---|---|\n| RepoBench (Liu et al., 2023a) | 2 | \u2717 | \u2713 | 1669 |\n| CrossCodeEval (Ding et al., 2024) | 4 | \u2717 | \u2717 | 1002 |\n| R<sup>2</sup>C<sup>2</sup>-Bench (Deng et al., 2024) | 4 | \u2717 | \u2713 | 1353 |\n| M<sup>2</sup>rc-Eval & M<sup>2</sup>rc-Instruct | 18 | \u2713 | \u2713 | 5993 |", "caption": "Table 1: A comparison with existing notable repository-level code completion datasets.", "description": "This table compares the M\u00b2RC-EVAL benchmark dataset with other existing notable repository-level code completion datasets.  It shows the number of programming languages supported, whether fine-grained annotations are included, the presence of a training set, and the number of test repositories used in each dataset. This allows for a quantitative comparison of dataset scale and annotation detail, highlighting the unique features and improvements of M\u00b2RC-EVAL.", "section": "3 M\u00b2RC-EVAL"}, {"content": "| Model | C EM | C ES | C# EM | C# ES | C++ EM | C++ ES | Go EM | Go ES | HTML EM | HTML ES | Haskell EM | Haskell ES | Java EM | Java ES | JavaScript EM | JavaScript ES | Kotlin EM | Kotlin ES | Lua EM | Lua ES | Objective-C EM | Objective-C ES | PHP EM | PHP ES | Python EM | Python ES | R EM | R ES | Ruby EM | Ruby ES | Rust EM | Rust ES | Scala EM | Scala ES | TypeScript EM | TypeScript ES | Avg. EM | Avg. ES |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Code Llama-7B | 18.6 | 47.2 | 19.6 | 52.6 | 21.8 | 51.1 | 26.0 | 53.6 | 20.6 | 40.4 | 22.6 | 48.9 | - | - | 23.4 | 58.5 | 17.2 | 52.0 | 23.6 | 57.0 | 20.0 | 45.7 | 17.8 | 49.5 | 19.2 | 54.9 | 24.6 | 54.2 | 15.2 | 41.2 | 17.2 | 45.8 | 26.2 | 56.0 | 22.8 | 48.5 | 23.4 | 52.3 | 19.4 | 50.3 |\n| + Retrieval | 21.8 | 47.2 | 22.9 | 48.9 | 23.2 | 46.6 | 23.8 | 52.4 | 12.6 | 35.6 | 22.6 | 48.9 | - | - | 23.4 | 57.5 | 19.6 | 48.0 | 20.8 | 50.0 | 19.6 | 42.2 | 21.4 | 46.6 | 21.2 | 49.0 | 17.4 | 46.4 | 15.2 | 39.8 | 17.2 | 42.3 | 26.0 | 51.3 | 22.8 | 48.5 | 19.4 | 48.6 | 20.2 | 46.1 |\n| + Retrieval & Tuning | 45.4 | 72.0 | 43.5 | 72.3 | 50.8 | 74.9 | 43.4 | 72.9 | 41.8 | 63.6 | 39.8 | 66.3 | - | - | 41.8 | 74.1 | 38.8 | 70.1 | 45.0 | 75.6 | 43.8 | 70.5 | 49.8 | 75.9 | 45.6 | 76.7 | 39.2 | 69.9 | 38.6 | 65.5 | 43.0 | 68.5 | 42.0 | 69.2 | 41.0 | 70.1 | 37.0 | 68.2 | 41.9 | 70.0 |\n| StarCoder-7B | 20.0 | 50.4 | 20.0 | 53.3 | 22.4 | 51.8 | 25.4 | 58.2 | 17.4 | 40.7 | 25.0 | 51.1 | - | - | 24.0 | 59.2 | 16.6 | 52.0 | 24.4 | 59.3 | 21.4 | 48.6 | 17.6 | 49.6 | 18.6 | 54.4 | 19.4 | 52.9 | 16.4 | 43.7 | 19.4 | 47.4 | 26.2 | 56.0 | 23.6 | 53.4 | 19.8 | 53.3 | 21.0 | 52.0 |\n| + Retrieval | 23.8 | 47.8 | 27.1 | 53.2 | 24.6 | 48.0 | 26.0 | 53.6 | 20.6 | 40.4 | 25.0 | 47.7 | - | - | 24.6 | 54.2 | 22.6 | 47.2 | 23.6 | 47.4 | 26.4 | 53.5 | 22.8 | 48.5 | 23.4 | 52.3 | 24.1 | 50.0 |\n| + Retrieval & Tuning | 47.0 | 72.7 | 45.1 | 74.8 | 52.4 | 76.3 | 43.2 | 73.7 | 45.8 | 67.1 | 44.8 | 70.2 | - | - | 39.2 | 69.9 | 38.6 | 65.5 | 43.0 | 68.5 | 42.0 | 69.2 | 41.0 | 70.1 | 37.0 | 68.2 | 44.5 | 72.2 |\n| DeepSeekCoder-6.7B | 22.4 | 53.7 | 21.4 | 56.2 | 23.2 | 54.2 | 29.4 | 61.4 | 17.6 | 43.4 | 25.2 | 51.3 | - | - | 22.2 | 61.0 | 20.4 | 56.5 | 26.0 | 61.0 | 22.0 | 48.8 | 21.0 | 55.6 | 24.2 | 58.6 | 21.8 | 55.1 | 19.4 | 48.5 | 23.6 | 52.2 | 23.8 | 54.3 | 24.6 | 56.7 | 19.4 | 55.4 | 22.6 | 54.7 |\n| + Retrieval | 28.2 | 52.6 | 25.3 | 52.6 | 27.6 | 52.2 | 29.4 | 61.4 | 17.6 | 43.4 | 25.8 | 51.0 | - | - | 21.6 | 51.4 | 24.4 | 53.6 | 26.0 | 61.0 | 22.0 | 49.9 | 27.6 | 53.5 | 28.6 | 56.9 | 21.8 | 55.1 | 19.4 | 48.5 | 23.6 | 52.2 | 23.8 | 54.3 | 22.4 | 50.4 | 26.0 | 54.5 | 25.1 | 51.7 |\n| + Retrieval & Tuning | 48.6 | 75.2 | 47.9 | 76.9 | 54.4 | 78.2 | 48.8 | 78.4 | 45.0 | 66.3 | 45.8 | 72.0 | - | - | 48.2 | 79.1 | 43.6 | 73.5 | 46.0 | 75.7 | 44.6 | 70.6 | 52.2 | 77.6 | 49.8 | 78.8 | 41.6 | 71.3 | 45.4 | 69.4 | 45.6 | 70.3 | 47.6 | 73.4 | 44.8 | 73.7 | 43.2 | 73.4 | 46.8 | 74.1 |", "caption": "Table 2: Exact match (%) and edit similarity (%) performance on M2rc-Eval.", "description": "This table presents the performance of three different code large language models (Code Llama-7B, StarCoder-7B, and DeepSeekCoder-6.7B) on the M2RC-Eval benchmark.  The performance is measured using two metrics: Exact Match (EM) and Edit Similarity (ES), both expressed as percentages.  Results are shown for each of the 18 programming languages included in the benchmark, with and without retrieval and retrieval with fine-tuning.", "section": "4 EXPERIMENTS"}, {"content": "| Model | Average |  |  | \n|---|---|---|---|\n| **Model** | **Average** | EM | ES |\n| StarCoder-3B | 14.9 | 43.5 |  |\n| \n\n+ Retrieval | 14.6  | 38.4 |  |\n| \n\n+ Retrieval & Tuning | 41.7 | 69.1 |  |\n| StarCoder-7B | 20.6 | 49.9 |  |\n| \n\n+ Retrieval | 23.6 | 49.3 |  |\n| \n\n+ Retrieval & Tuning | 44.4 | 71.4 |  |", "caption": "Table 3: Performance on M2rc-Eval.", "description": "This table presents the average performance of three different code large language models (StarCoder-3B, StarCoder-7B, and DeepSeekCoder-6.7B) on the M2RC-Eval benchmark.  It shows the exact match (EM) and edit similarity (ES) scores for each model under different conditions: baseline (using only the in-file code), with retrieval (incorporating cross-file contexts), and with retrieval and tuning (fine-tuned on the M2RC-INSTRUCT dataset). This allows for comparison of model performance with and without cross-file context retrieval and the impact of fine-tuning on a large multilingual instruction dataset.", "section": "4 EXPERIMENTS"}, {"content": "| Model | C | C# | C++ | Go | Java | JavaScript | PHP | Python | Ruby | Rust | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| StarCoder-7B | 48.3 | 48.9 | 50.4 | 51.5 | 50.6 | 46.4 | 48.2 | 46.4 | 46.1 | 50.4 | 48.7 |\n| + Retrieval | 50.1 | 52.3 | 51.1 | 52.5 | 51.4 | 49.3 | 52.2 | 49.3 | 49.1 | 51.4 | 50.9 |\n| + Retrieval & Tuning | 56.0 | 57.4 | 57.6 | 57.0 | 57.6 | 54.8 | 57.8 | 52.0 | 52.9 | 55.5 | **55.9** |", "caption": "Table 4: CodeBLEU results on ten representative programming languages.", "description": "This table presents a quantitative evaluation of the performance of different code generation models across ten programming languages using the CodeBLEU metric. CodeBLEU offers a more nuanced evaluation than simpler metrics by considering textual, syntactic, and semantic similarities between generated and reference code.  The results help illustrate the models' strengths and weaknesses in generating code in different programming languages.", "section": "4.4 Analysis"}, {"content": "| Model | Average |  |  |  |  |\n|---|---|---|---|---|---| \n|  | EM | ES |  |  |  |\n| + Retrieval | 23.6 | 49.3 |  |  |  |\n| + Retrieval & Tuning | 44.4 | 71.4 |  |  |  |\n| + Retrieval & Tuning (Python Only) | 39.2 | 67.9 |  |  |  |", "caption": "Table 5: Performance on M2rc-Eval.", "description": "This table presents the performance of different code generation models on the M2RC-Eval benchmark.  It shows the average exact match (EM) and edit similarity (ES) scores for each model, across all languages in the benchmark. Different configurations are shown, such as using only the in-file context or adding retrieved cross-file context, and with or without further fine-tuning on the M2RC-Instruct dataset. The table allows for comparison of the performance improvement due to retrieval and fine-tuning, and provides insights into the effectiveness of these techniques for different code models.", "section": "4 EXPERIMENTS"}]