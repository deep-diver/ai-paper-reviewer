[{"figure_path": "https://arxiv.org/html/2410.21157/x1.png", "caption": "Figure 1: Overview of our proposed M2rc-Eval with 18 languages. Specifically, first, we provide three samples from different languages (i.e., Python, Java, TypeScript) for illustration, where the bucket label and semantic label for the corresponding cursor position are provided.\nSecond, the code LLMs need to predict the completion results given the in-file context from the current code file and the cross file context retrieved from other code files in the current repository.\nNote that \u201c<INFILLING>expectationINFILLING<\\mathrm{INFILLING}>< roman_INFILLING >\u201d denotes that the current position will be triggered for code completion.", "description": "Figure 1 illustrates the M2RC-Eval benchmark, a multilingual repository-level code completion evaluation dataset.  It showcases examples in three languages (Python, Java, and TypeScript) to highlight the data structure. Each example shows the code snippet, the \u2018in-file\u2019 context (from the same file), and the \u2018cross-file\u2019 context (from other files in the same repository). The task for large language models (LLMs) is to predict the missing code indicated by the `<INFILLING>` placeholder.  Annotations for bucket-level (complexity) and semantic-level (code type) are also provided at the code completion point to aid in fine-grained analysis.", "section": "3 M\u00b2RC-EVAL"}, {"figure_path": "https://arxiv.org/html/2410.21157/x2.png", "caption": "Figure 2: Illustration on generating completion cursor position and fine-grained annotations. Specifically, we first parse the source code into an abstract syntax tree (AST). Then, we choose one node as the completion cursor position and generate the bucket label based on the belonged layer number in AST, and obtain the semantic label based on the node type parsed by the Tree-sitter.", "description": "This figure illustrates the process of generating code completion cursor positions and their corresponding fine-grained annotations within the M2RC-EVAL benchmark.  First, the source code is parsed into an Abstract Syntax Tree (AST). Then, a node within the AST is randomly selected to represent the code completion cursor position.  The bucket label is determined by the node's level or depth within the AST's tree structure. Finally, the semantic label is assigned based on the node type identified by the Tree-sitter parser, categorizing the code snippet's function (e.g., declaration, expression, statement, etc.).", "section": "3 M\u00b2RC-EVAL"}, {"figure_path": "https://arxiv.org/html/2410.21157/x3.png", "caption": "Figure 3: The average prompt length (100x tokens), completion span length (50x tokens), and cross-file dependencies (1x) in the testing set of M2rc-Eval. We define the number of other files,\nwhich are explicitly imported and implicitly referenced by the current file,\nas cross-file dependencies.", "description": "Figure 3 presents a bar chart visualizing the average lengths of prompts and code completions, along with the number of cross-file dependencies, observed in the M2RC-Eval testing dataset.  The 'prompt length' represents the average number of tokens used to solicit a code completion.  'Completion span length' refers to the average length of the code segment that needs to be predicted, also measured in tokens.  Finally, 'cross-file dependencies' reflects the average number of external files, explicitly or implicitly linked to the current file, within the repository. This data offers insight into the complexity of code completion tasks within the M2RC-Eval benchmark.", "section": "3 M\u00b2RC-EVAL"}, {"figure_path": "https://arxiv.org/html/2410.21157/x4.png", "caption": "(a) Java", "description": "This figure shows the semantic-level annotations on Java code.  The figure is a pie chart that visually represents the distribution of different semantic labels in Java code samples within the M2RC-EVAL benchmark. Each slice of the pie chart corresponds to one of eleven major semantic labels (Program Structure, Declaration and Definition, etc.), and the size of each slice reflects the proportion of code instances that fall into that semantic category.  This provides a fine-grained analysis of the code completion scenarios in Java within the benchmark.", "section": "3.4 FINE-GRAINED ANNOTATIONS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x5.png", "caption": "(b) Go", "description": "The figure shows a pie chart visualizing the distribution of semantic-level annotations for the Go programming language in the M2RC-EVAL benchmark.  Each slice of the pie chart represents a specific semantic label (e.g., Program Structure, Statement, Expression, etc.), and the size of each slice corresponds to the proportion of code completion instances in the dataset that were assigned that particular semantic label. This provides insights into the relative frequency of different semantic categories within Go code, allowing for analysis of the distribution of code completion scenarios across the programming language.", "section": "3.4 FINE-GRAINED ANNOTATIONS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x6.png", "caption": "(c) Scala", "description": "This figure shows the semantic-level annotations on Scala code.  Specifically, it's a pie chart illustrating the distribution of different semantic labels (e.g., Program Structure, Declaration and Definition, Control Flow Structure, etc.) assigned to various code completion cursor positions within Scala code samples in the M2RC-EVAL benchmark.  The chart visually represents the proportion of each semantic label found in the dataset, offering insights into the frequency and diversity of code completion scenarios within Scala.", "section": "3.4 FINE-GRAINED ANNOTATIONS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x7.png", "caption": "Figure 4: Semantic-level annotations on different types of programming languages.", "description": "This figure shows a comparison of the semantic-level annotations for three different programming languages: Java, Go, and Scala.  Each pie chart represents a language and shows the distribution of different semantic labels used to annotate code completion scenarios. The semantic labels represent different code elements and structures such as program structure, declarations, control flow, expressions, data types, statements, and identifiers. The detailed breakdown of semantic label proportions allows for a granular analysis of how different languages are annotated and how this might impact the performance of different code LLMs on those respective languages.", "section": "3.4 FINE-GRAINED ANNOTATIONS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x8.png", "caption": "Figure 5: Effectiveness of using different training data sizes.", "description": "This figure shows the impact of varying training data sizes on the performance of different code LLMs on the M\u00b2RC-EVAL benchmark.  The x-axis represents the size of the training dataset, and the y-axis represents the evaluation scores (Exact Match and Edit Similarity).  The different lines in the graph represent various code LLMs (StarCoder-7B, DeepSeekCoder-6.7B, and Code Llama-7B), both with and without the retrieval and fine-tuning steps. The figure illustrates how increasing the training data size generally improves performance across all models, highlighting the relationship between data size and model performance in multilingual repository-level code completion.", "section": "4 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x9.png", "caption": "Figure 6: Effectiveness of different bucket levels based on StarCoder-7B.", "description": "This figure analyzes the performance of the StarCoder-7B model on code completion tasks across various bucket levels. The bucket level represents the depth of a node within an abstract syntax tree (AST), indicating the complexity of the code completion scenario. Each level shows the EM and ES scores for both Retrieval and Retrieval & Tuning methods. The graph helps understand how model performance correlates with code complexity; lower bucket levels (representing more complex code) generally exhibit lower performance scores. The graph demonstrates that StarCoder-7B's accuracy decreases as the code's structural complexity increases.", "section": "4.4 ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x10.png", "caption": "Figure 7: Effectiveness of different semantic levels based on StarCoder-7B.", "description": "This figure analyzes the performance of StarCoder-7B, a code generation model, across different semantic levels in code completion tasks.  It displays the model's accuracy (EM and ES) for various semantic labels, such as Program Structure, Declaration and Definition, Control Flow Structure, etc. The graph allows for a granular understanding of the model's strengths and weaknesses in different aspects of code comprehension and generation, highlighting semantic areas where the model excels and areas needing improvement.", "section": "4.4 ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x11.png", "caption": "Figure 8: Effectiveness of code completion on different lines based on StarCoder-7B.", "description": "This figure shows the performance of the StarCoder-7B model on code completion tasks with varying numbers of lines.  It demonstrates how the model's accuracy changes as the length of the code to be completed increases. The x-axis represents the number of lines, and the y-axis represents the evaluation score (likely a metric like exact match or edit similarity).  The results illustrate the challenges faced by the model as the completion task becomes more complex, involving multiple lines of code.", "section": "4.4 ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x12.png", "caption": "Figure 9: Performance on M2rc-Eval for problems of different difficulty levels.", "description": "This figure presents a bar chart illustrating the performance of different code LLMs on the M2RC-Eval benchmark, categorized by the difficulty level of the problems. The x-axis displays various programming languages, while the y-axis represents the evaluation scores. Three difficulty levels are considered: easy, medium, and hard. Each bar represents the performance of a specific model on a particular programming language and difficulty level, enabling a comprehensive comparison of model capabilities across different languages and problem complexities.", "section": "4 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x34.png", "caption": "Figure 10: Performance on M2rc-Eval with various input lengths based on StarCoder-7B.", "description": "This figure shows the performance of the StarCoder-7B model on the M2RC-Eval benchmark across different input lengths. The x-axis represents the input length in tokens (512, 1024, 2048, 4096), while the y-axis represents the performance scores (Exact Match and Edit Similarity).  The graph illustrates a scaling law, where longer input sequences generally lead to better performance. This suggests that providing more context to the model improves its ability to generate accurate code completions.", "section": "4.4 ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x35.png", "caption": "Figure 11: Effectiveness of different bucket levels based on StarCoder-7B for different languages.", "description": "This figure presents a detailed analysis of the performance of StarCoder-7B across various bucket levels for 18 different programming languages.  Bucket levels represent the depth within the abstract syntax tree, providing a measure of code complexity.  The results are shown for both exact match (EM) and edit similarity (ES) metrics, demonstrating how the model's performance varies based on the complexity of the completion context.  The figure allows for a granular understanding of the model's abilities within different code structures, enabling a deeper assessment of strengths and weaknesses.", "section": "3.4 Fine-grained Annotations"}, {"figure_path": "https://arxiv.org/html/2410.21157/x36.png", "caption": "Figure 12: Effectiveness of different bucket levels based on StarCoder-7B for different languages.", "description": "This figure presents a detailed analysis of the effectiveness of different bucket levels in the M2RC-EVAL benchmark using the StarCoder-7B model.  It displays performance metrics across various programming languages (Kotlin, Haskell, C, C++, Objective-C, and Rust) for each bucket level. Each language's performance is evaluated against the different bucket levels of the abstract syntax tree (AST), allowing for a nuanced comparison of how the model handles different levels of code complexity.  The results are presented in graphs that show the exact match (EM) and edit similarity (ES) scores for each language and bucket level, revealing potential strengths and weaknesses of the model at different levels of the AST.", "section": "3.4 FINE-GRAINED ANNOTATIONS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x37.png", "caption": "Figure 13: Effectiveness of different semantic levels based on StarCoder-7B.", "description": "This figure presents a detailed analysis of StarCoder-7B's performance across various semantic levels in code completion tasks.  It breaks down the model's accuracy (EM and ES) for different semantic categories, such as Program Structure, Declaration and Definition, Control Flow, Expressions, Data Types, and more.  The visualization helps to understand the model's strengths and weaknesses in handling various code constructs and complexities, showing where it excels and where it struggles.  The granularity of the results provides insights into which aspects of code understanding are more or less challenging for the model, revealing subtle differences in performance across these semantic levels.", "section": "4.4 ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x38.png", "caption": "(a) C", "description": "This figure shows a pie chart visualizing the distribution of semantic labels in the C programming language within the M\u00b2RC-EVAL benchmark. Each slice of the pie chart represents a different semantic label, with its size corresponding to the proportion of code snippets in the dataset that are annotated with that specific label. The semantic labels provide a fine-grained annotation for the various types of code completion scenarios present in the dataset.  The visualization helps in understanding the relative frequencies of different code semantic patterns in the benchmark, which can be useful for evaluating the performance of code language models on different aspects of code completion tasks.", "section": "3.4 FINE-GRAINED ANNOTATIONS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x39.png", "caption": "(b) Go", "description": "This figure shows a pie chart visualizing the distribution of semantic-level annotations for the Go programming language in the M2RC-EVAL benchmark.  Each slice of the pie represents a different semantic label (e.g., Program Structure, Declaration and Definition, Control Flow Structure, etc.), and the size of the slice corresponds to the proportion of code completion samples in the dataset that belong to that particular semantic label. This provides a fine-grained view of the types of code completion scenarios covered by the benchmark for Go.", "section": "3.4 FINE-GRAINED ANNOTATIONS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x40.png", "caption": "(c) Scala", "description": "This figure shows the semantic-level annotations on the Scala programming language.  The pie chart visually represents the distribution of different semantic labels within the Scala codebase. Each slice of the pie chart corresponds to a specific semantic label, such as Program Structure, Declaration and Definition, Control Flow Structure, etc., reflecting the relative frequency of each semantic category in the code examples.  This granular level of detail provides insight into the types of code completion scenarios present in the dataset and helps in evaluating the performance of different models in various code completion contexts.", "section": "3.4 FINE-GRAINED ANNOTATIONS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x41.png", "caption": "(d) Java", "description": "This figure shows one of the example code snippets used in the M2RC-EVAL benchmark.  Specifically, it demonstrates a code completion scenario in Java. The image highlights the \"in-file context\" (the surrounding code within the current file), \"cross-file context\" (code snippets from other files in the project), the location of the \"cursor position\" where code completion is needed, and the associated \"bucket label\" and \"semantic label\" indicating the type of code completion task and its complexity level.", "section": "3 M\u00b2RC-EVAL"}, {"figure_path": "https://arxiv.org/html/2410.21157/x42.png", "caption": "(e) Go", "description": "The figure shows the distribution of semantic-level annotations for the Go programming language in the M2RC-EVAL benchmark.  It's a pie chart that visually represents the proportion of different semantic labels assigned to code completion points within Go code samples.  Each slice of the pie corresponds to a specific semantic label (e.g., Program Structure, Declaration and Definition, Control Flow Structure, etc.), and the size of each slice indicates the relative frequency of that label in the dataset.  This helps illustrate the variety of code completion scenarios present in the benchmark for Go and provides a nuanced understanding of the dataset's composition.", "section": "3.4 FINE-GRAINED ANNOTATIONS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x43.png", "caption": "(f) Scala", "description": "This figure shows a pie chart that visually represents the distribution of semantic-level annotations for Scala code in the M\u00b2RC-EVAL benchmark. Each slice of the pie chart corresponds to one of the 11 pre-defined semantic labels (e.g., Program Structure, Declaration and Definition, etc.).  The size of each slice is proportional to the frequency of that specific semantic label in the Scala code samples. This visualization helps illustrate the relative prevalence of different code semantic categories within the Scala portion of the benchmark dataset. The figure provides valuable insights into the types of code completion tasks that are prevalent in the Scala subset of M\u00b2RC-EVAL.", "section": "3.4 FINE-GRAINED ANNOTATIONS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x44.png", "caption": "(g) Java", "description": "This figure shows the semantic-level annotations on Java code in the M\u00b2RC-EVAL benchmark.  The pie chart visually represents the distribution of different semantic labels assigned to code completion points within Java code samples. Each slice corresponds to a specific semantic category (e.g., Program Structure, Statement, Expression, etc.), and its size reflects the proportion of that category within the dataset.  This provides a fine-grained view of code completion scenarios in Java, highlighting the diversity of semantic contexts the model needs to handle.", "section": "3.4 FINE-GRAINED ANNOTATIONS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x45.png", "caption": "(h) Go", "description": "This figure shows the distribution of semantic labels in Go code within the M2RC-EVAL benchmark.  The pie chart visually represents the proportion of various semantic labels (e.g., Program Structure, Declaration and Definition, etc.) found in the Go code snippets used for the code completion task. This provides insights into the relative frequency of different semantic patterns in the dataset.", "section": "3.4 FINE-GRAINED ANNOTATIONS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x46.png", "caption": "(i) Scala", "description": "This figure shows the distribution of semantic labels in Scala code snippets within the M\u00b2RC-EVAL benchmark.  It provides a detailed breakdown of the frequency of different semantic categories (e.g., Program Structure, Declaration and Definition, Control Flow Structure, etc.) found in the code samples.  The pie chart visually represents the proportion of each semantic label, offering insights into the types of code constructs prevalent in the Scala portion of the dataset.  This granular analysis helps to understand the characteristics of the dataset and its suitability for evaluating different aspects of code language models.", "section": "3.4 Fine-grained Annotations"}, {"figure_path": "https://arxiv.org/html/2410.21157/x47.png", "caption": "(j) Java", "description": "This figure shows a pie chart visualizing the distribution of semantic labels in Java code snippets within the M2RC-EVAL benchmark. Each slice represents a different semantic category (e.g., Program Structure, Declaration and Definition, etc.) and its size is proportional to the frequency of that category in the dataset. This provides a granular view of the code completion scenarios captured in the benchmark for Java.", "section": "3.4 FINE-GRAINED ANNOTATIONS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x48.png", "caption": "(k) Go", "description": "This figure shows a pie chart visualizing the distribution of semantic-level annotations for the Go programming language in the M\u00b2RC-EVAL benchmark.  Each slice of the pie chart represents a different semantic label, such as Program Structure, Declaration and Definition, Control Flow, etc., showing the proportion of code completion instances categorized under each label. This provides insights into the distribution of different code completion scenarios within the Go language samples of the dataset.", "section": "3.4 FINE-GRAINED ANNOTATIONS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x49.png", "caption": "(l) Scala", "description": "This figure shows a pie chart visualizing the distribution of semantic-level annotations for Scala code in the M2RC-EVAL benchmark.  Each slice represents a different semantic label assigned to code completion points, indicating the frequency of each code semantic type within the dataset.  The semantic labels categorize the type of code element being completed, offering insights into the various code contexts within the Scala programming language included in the dataset.", "section": "3.4 FINE-GRAINED ANNOTATIONS"}, {"figure_path": "https://arxiv.org/html/2410.21157/x50.png", "caption": "(m) Java", "description": "This figure shows a pie chart visualizing the distribution of semantic-level annotations for the Java programming language in the M\u00b2RC-EVAL benchmark.  Each slice represents a different semantic label (e.g., Program Structure, Declaration and Definition, Control Flow Structure, Expression, etc.), with the size of each slice proportional to the frequency of that label in the Java code samples.", "section": "3.4 Fine-grained Annotations"}]