{"references": [{"fullname_first_author": "Linzheng Chai", "paper_title": "Mceval: Massively multilingual code evaluation", "publication_date": "2024-06-07", "reason": "This paper introduces a massively multilingual code evaluation benchmark, which directly addresses the limitations of existing benchmarks by covering more languages and providing more fine-grained annotations for various code completion scenarios."}, {"fullname_first_author": "Daya Guo", "paper_title": "Deepseek-coder: When the large language model meets programming-the rise of code intelligence", "publication_date": "2024-01-14", "reason": "This work introduces a powerful code LLM, DeepSeek-Coder, which is used in the paper's experiments to show the effectiveness of the proposed benchmark."}, {"fullname_first_author": "Baptiste Roziere", "paper_title": "Code llama: Open foundation models for code", "publication_date": "2023-00-00", "reason": "Code Llama is another prominent code LLM used in the paper's experiments, highlighting the importance of this model to the community and the paper's evaluation."}, {"fullname_first_author": "Yangruibo Ding", "paper_title": "Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion", "publication_date": "2024-00-00", "reason": "This work is a closely related multilingual code completion benchmark. Its importance lies in providing comparison and context to the novel M2RC-EVAL."}, {"fullname_first_author": "Tianyang Liu", "paper_title": "Repobench: Benchmarking repository-level code auto-completion systems", "publication_date": "2023-06-03", "reason": "This paper introduces Repobench, a repository-level code auto-completion benchmark that serves as a direct comparison point for evaluating the proposed M2RC-EVAL benchmark."}]}