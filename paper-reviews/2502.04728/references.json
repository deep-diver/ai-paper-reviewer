{"references": [{"fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-07", "reason": "This paper is foundational for using LLMs for program synthesis, a closely related task to the PDDL domain generation discussed in the target paper."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper demonstrates the few-shot learning capability of LLMs, which is a crucial aspect of the test-time scaling methods used in the target paper."}, {"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-07", "reason": "This paper provides a benchmark for evaluating LLMs on code, which is relevant to the PDDL generation task, as PDDL can be considered a form of code."}, {"fullname_first_author": "Richard E. Fikes", "paper_title": "STRIPS: A new approach to the application of theorem proving to problem solving", "publication_date": "1971-01-01", "reason": "This is a seminal paper in AI planning that introduces STRIPS, the foundation for PDDL, which is directly used in this paper."}, {"fullname_first_author": "Lin Guan", "paper_title": "Leveraging pre-trained large language models to construct and utilize world models for model-based task planning", "publication_date": "2023-01-01", "reason": "This paper directly addresses the same problem of using LLMs for task planning, making it a highly relevant comparison for the proposed method."}]}