[{"figure_path": "https://arxiv.org/html/2412.05271/x1.png", "caption": "Figure 1: \nPerformance of various MLLMs on the OpenCompass leaderboard.\nInternVL 2.5 showcases strong multimodal capabilities, rivaling closed-source models like GPT-4o\u00a0[192] and Claude-3.5-Sonnet\u00a0[8].\nHowever, since the OpenCompass score is derived from 8 academic VQA benchmarks and covers only a subset of overall capabilities, we still need further effort to match the performance with closed-source models.", "description": "This figure presents a comparison of the performance of various multimodal large language models (MLLMs) on the OpenCompass leaderboard.  InternVL 2.5 is highlighted, demonstrating competitive performance compared to leading closed-source models such as GPT-40 and Claude-3.5-Sonnet. The x-axis represents the number of parameters (in billions) for each MLLM, and the y-axis shows the average score on the OpenCompass benchmark.  The figure illustrates that InternVL 2.5 achieves a high score, rivaling closed-source models despite being open-source, though further work is needed to improve performance across all capabilities.  It emphasizes that OpenCompass focuses on a limited set of visual question answering (VQA) benchmarks, not the full scope of an MLLM's abilities.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.05271/x2.png", "caption": "Figure 2: Overall architecture.\nInternVL 2.5 retains the same model architecture as InternVL 1.5\u00a0[35] and InternVL 2.0, i.e.\u00a0the widely-used \u201cViT-MLP-LLM\u201d paradigm, which combines a pre-trained InternViT-300M or InternViT-6B with LLMs\u00a0[19, 229] of various sizes via an MLP projector. Consistent with previous versions, we apply a pixel unshuffle operation to reduce the 1024 visual tokens produced by each 448\u00d7\\times\u00d7448 image tile to 256 tokens.\nMoreover, compared to InternVL 1.5, InternVL 2.0 and 2.5 introduced additional data types, incorporating multi-image and video data alongside the existing single-image and text-only data.", "description": "InternVL 2.5 uses a \"ViT-MLP-LLM\" architecture.  InternViT (a Vision Transformer) processes images, reducing the initial 1024 visual tokens to 256 using pixel unshuffle. These tokens are then projected via an MLP (Multilayer Perceptron) into an LLM (Large Language Model) for multimodal understanding.  Unlike earlier versions, InternVL 2.5 supports multi-image and video inputs.", "section": "2 Model Architecture"}, {"figure_path": "https://arxiv.org/html/2412.05271/x3.png", "caption": "Figure 3: Illustration of the data formats for various data types.\n(a) For single-image datasets, the maximum number of tiles nmaxsubscript\ud835\udc5bmaxn_{\\text{max}}italic_n start_POSTSUBSCRIPT max end_POSTSUBSCRIPT is allocated to a single image, ensuring maximum resolution for the input.\n(b) For multi-image datasets, the total number of tiles nmaxsubscript\ud835\udc5bmaxn_{\\text{max}}italic_n start_POSTSUBSCRIPT max end_POSTSUBSCRIPT is distributed proportionally across all images within the sample.\n(c) For video datasets, the method simplifies the approach by setting nmax=1subscript\ud835\udc5bmax1n_{\\text{max}}=1italic_n start_POSTSUBSCRIPT max end_POSTSUBSCRIPT = 1, resizing individual frames to a fixed resolution of 448\u00d7\\times\u00d7448.", "description": "Figure 3 illustrates how the InternVL model handles different data types: (a) Single-image inputs are divided into tiles, with the maximum number of tiles used to ensure the highest resolution. (b) Multi-image inputs distribute tiles proportionally among the images in a sample. (c) Video processing simplifies to resizing individual frames to 448x448 pixels.", "section": "3 Training Strategy"}, {"figure_path": "https://arxiv.org/html/2412.05271/x4.png", "caption": "Figure 4: \nIllustration of the training pipeline and progressive scaling strategy.\n(a) Single model training pipeline: The training process is divided into three stages\u2014Stage 1 (MLP warmup), optional Stage 1.5 (ViT incremental learning), and Stage 2 (full model instruction tuning).\nThe multi-stage design progressively enhances vision-language alignment, stabilizes training, and prepares modules for integration with larger LLMs.\n(b) Progressive scaling strategy: The ViT module trained with a smaller LLM in earlier stages can be easily integrated with larger LLMs, enabling scalable model alignment with affordable resource overhead.", "description": "This figure illustrates the training process of the InternVL 2.5 model, highlighting its two key strategies: single model training and progressive scaling.  The single model training pipeline involves three stages: a warmup stage focusing on the MLP projector, an optional incremental learning stage for the vision transformer (ViT), and a final instruction tuning stage for the full model.  This multi-stage approach improves vision-language alignment, enhances training stability, and prepares the model for integration with larger language models. The progressive scaling strategy leverages the pre-trained ViT module from earlier stages, allowing for easy integration with larger language models, leading to scalable model alignment and reduced computational costs. This figure helps explain the efficient and scalable training methods used for InternVL 2.5.", "section": "3 Training Strategy"}, {"figure_path": "https://arxiv.org/html/2412.05271/x5.png", "caption": "Figure 5: Dataset configuration.\nIn InternVL 2.0 and 2.5, data augmentation is applied selectively, enabled for image datasets and disabled for videos and text. The maximum tile number (nmaxsubscript\ud835\udc5bn_{\\max}italic_n start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT) controls the resolution of inputs, with higher values for multi-image datasets and lower values for videos. The repeat factor (r\ud835\udc5fritalic_r) balances dataset sampling by adjusting the frequency of each dataset, ensuring robust and balanced training.", "description": "Figure 5 details the configuration of datasets used to train InternVL 2.0 and 2.5.  Data augmentation techniques (like JPEG compression) are selectively applied; they are used for image data but not video or text data. The maximum tile number (nmax) parameter determines the resolution of the input; higher nmax values are used for higher-resolution inputs, such as those found in multi-image datasets.  Conversely, lower nmax values are used for video data, which often has many frames to process.  The repeat factor (r) controls the sampling frequency of each dataset, balancing dataset representation and ensuring robust and balanced model training.", "section": "4 Data Organization"}, {"figure_path": "https://arxiv.org/html/2412.05271/x6.png", "caption": "Figure 6: Visualization of abnormal samples in open-source datasets.\nAbnormal samples are prevalent across various data types, including single-image, multi-image, video, and pure text datasets, with \u201crepetitive outputs\u201d being a prominent issue. We identify this as one of the most detrimental problems for test-time scaling, often leading models into loops in long-form outputs and CoT reasoning tasks.", "description": "Figure 6 showcases examples of problematic data points (abnormal samples) frequently found within open-source datasets used to train large language models.  These issues affect various data types including single images, multiple images, videos, and text-only datasets. A major problem highlighted is the prevalence of repetitive outputs within the data.  The authors argue that these repetitive patterns are highly detrimental to the performance of models, particularly during test-time scaling, often causing them to produce repetitive or cyclical responses, especially in long-form outputs and when using Chain-of-Thought (CoT) reasoning.", "section": "4 Data Organization"}, {"figure_path": "https://arxiv.org/html/2412.05271/x7.png", "caption": "Figure 7: \nStatistics of the fine-tuning data mixture.\nThe dataset shows consistent growth from InternVL 1.5 to 2.5 in terms of (a) the number of samples and (b) the number of tokens across multiple dataset types, including single-image, multi-image, video, and text. These statistics reflect iterative improvements in data scale and diversity, which enhance the model\u2019s multimodal understanding capabilities.", "description": "This figure shows the growth of the dataset used for fine-tuning the InternVL model from version 1.5 to version 2.5.  It displays the increase in both the number of samples and the number of tokens across various data types: single images, multiple images, videos, and text.  The growth indicates an increase in the scale and diversity of the training data, which ultimately enhances the model's ability to understand and process multiple data modalities.", "section": "3 Training Strategy"}, {"figure_path": "https://arxiv.org/html/2412.05271/x8.png", "caption": "Figure 8: Dataset filtering pipeline.\nFor text data, we use three methods: (a) LLM-based quality scoring to assign domain-specific quality scores and filter low-quality samples; (b) Repetition detection to identify and remove data with repetitive patterns; and (c) heuristic rule-based filtering to detect anomalies using predefined rules.\nFor multimodal data, only (b) repetition detection and (c) heuristic rule-based filtering are applied to mitigate repetitive patterns and ensure dataset integrity.", "description": "This figure illustrates the data filtering pipeline used to improve the quality of the training data.  For text data, a three-stage process is used: LLM-based quality scoring to filter out low-quality samples based on domain-specific scores; repetition detection to remove samples with repetitive patterns; and heuristic rule-based filtering to identify and remove anomalous samples using predefined rules. For multimodal data, the LLM-based quality scoring stage is skipped, and only repetition detection and heuristic rule-based filtering are applied to ensure data integrity and remove repetitive patterns.", "section": "Data Organization"}, {"figure_path": "https://arxiv.org/html/2412.05271/x9.png", "caption": "Figure 9: CoT prompts used in our model testing.\nBy leveraging these prompts for CoT reasoning, we can scale up testing time, significantly enhancing the performance of InternVL 2.5 models on MMMU\u00a0[289].", "description": "This figure showcases the Chain of Thought (CoT) prompts utilized in the InternVL 2.5 model testing.  The prompts are designed to guide the model's reasoning process step-by-step, enhancing its ability to solve complex problems.  The figure likely shows examples of both multiple-choice and open-ended question prompts, illustrating how the CoT approach structures the input to elicit a more detailed and logical reasoning process from the model, ultimately improving its performance, particularly on the MMMU benchmark.", "section": "5.1 Multimodal Reasoning and Mathematics"}, {"figure_path": "https://arxiv.org/html/2412.05271/x10.png", "caption": "Figure 10: Performance on LongVideoBench with varying input video frames.", "description": "This figure shows the performance of various models on the LongVideoBench benchmark as the number of input video frames increases.  It demonstrates how the accuracy of different models, including InternVL 2.5 models and several other state-of-the-art models, changes with varying frame counts (16, 32, 48, 64, and 128 frames). This visualization helps to understand the impact of temporal information on video understanding tasks, particularly for assessing the scalability of models when processing long videos.", "section": "5.9 Video Understanding"}]