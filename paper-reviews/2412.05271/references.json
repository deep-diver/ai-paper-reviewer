{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a technical report on GPT-4, a leading commercial large language model, which serves as a strong comparison point for the open-source model InternVL 2.5 presented in this paper."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces a method for training visual models using natural language supervision, which is highly relevant to the multimodal model InternVL 2.5 that integrates both vision and language."}, {"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-01", "reason": "This paper introduces a benchmark for evaluating large language models trained on code, which is relevant to the multimodal model InternVL 2.5 due to its ability to handle code-related tasks."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2020-12-01", "reason": "This paper introduces a benchmark for evaluating massive multitask language understanding, providing a comprehensive evaluation framework for assessing the capabilities of language models and multimodal models."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-01-01", "reason": "This paper introduces the Vision Transformer (ViT) architecture, which is highly influential in the design of vision encoders in multimodal models, including InternVL 2.5."}]}