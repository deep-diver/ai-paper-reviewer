[{"figure_path": "https://arxiv.org/html/2412.06769/extracted/6056756/figures/figure_1_meta_3.png", "caption": "Figure 1: A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g., [xi,xi+1,\u2026,xi+j]subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc561\u2026subscript\ud835\udc65\ud835\udc56\ud835\udc57[x_{i},x_{i+1},...,x_{i+j}][ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_i + italic_j end_POSTSUBSCRIPT ] in the figure). Coconut regards the last hidden state as a representation of the reasoning state (termed \u201ccontinuous thought\u201d), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space.", "description": "Figure 1 illustrates the core difference between the Chain-of-Thought (CoT) and Chain of Continuous Thought (CoCoNut) methods.  CoT generates reasoning steps as a sequence of words, which are processed sequentially by the language model.  In contrast, CoCoNut leverages the LLM's hidden state (the 'continuous thought') as a representation of the reasoning process. This hidden state, rather than being decoded into words, is directly fed back into the model as input for the next step, enabling reasoning in a continuous latent space instead of relying on discrete word tokens. This allows for more flexible and potentially more efficient reasoning paths.", "section": "3 Coconut: Chain of Continuous Thought"}, {"figure_path": "https://arxiv.org/html/2412.06769/extracted/6056756/figures/figure_2_meta_5.png", "caption": "Figure 2: Training procedure of Chain of Continuous Thought (Coconut).\nGiven training data with language reasoning steps, at each training stage we integrate c\ud835\udc50citalic_c additional continuous thoughts (c=1\ud835\udc501c=1italic_c = 1 in this example), and remove one language reasoning step. The cross-entropy loss is then used on the remaining tokens after continuous thoughts.", "description": "The figure illustrates the multi-stage training process for the COCONUT model.  It starts with standard Chain-of-Thought (CoT) data, where reasoning is explicitly expressed in natural language.  In each subsequent training stage, a number (c) of continuous thoughts (latent representations of the reasoning process) are substituted for language-based reasoning steps.  This substitution is gradually increased across training stages, ultimately leading to a model that reasons primarily in the latent space, relying on continuous thoughts to generate the final answer rather than using a language-based step-by-step process. The cross-entropy loss is calculated using the remaining tokens and continuous thoughts after the substitution.", "section": "3 Coconut: Chain of Continuous Thought"}, {"figure_path": "https://arxiv.org/html/2412.06769/x1.png", "caption": "Figure 3: Accuracy on GSM8k with different number of continuous thoughts.", "description": "This figure shows the relationship between the accuracy of the GSM8k dataset and the number of continuous thoughts used in the COCONUT model.  It demonstrates how increasing the number of continuous thoughts, which represent the model's internal reasoning steps in a latent space, affects its ability to correctly solve mathematical problems.  The graph likely plots accuracy on the y-axis and the number of continuous thoughts per reasoning step on the x-axis.  It visualizes the performance gain achieved by using the COCONUT method.", "section": "4.4 Results and Discussion"}, {"figure_path": "https://arxiv.org/html/2412.06769/extracted/6056756/figures/figure_4_meta.png", "caption": "Figure 4: A case study where we decode the continuous thought into language tokens.", "description": "This figure demonstrates how the model's continuous thoughts, which are internal representations of the reasoning process, can be translated into human-understandable language tokens.  The example shows a simple math word problem where the model uses continuous thoughts to arrive at the solution. The decoded tokens reveal intermediate steps in the computation, indicating the model's internal reasoning process. The figure highlights the potential of decoding continuous thoughts to gain insights into the model's internal decision-making and reasoning strategy.", "section": "3 Coconut: Chain of Continuous Thought"}, {"figure_path": "https://arxiv.org/html/2412.06769/extracted/6056756/figures/figure_5_revised_1111.png", "caption": "Figure 5: The accuracy of final answer (left) and reasoning process (right) of multiple variants of Coconut and baselines on ProsQA.", "description": "Figure 5 presents a detailed analysis of the performance of various COCONUT models and baseline methods on the ProsQA dataset. The left panel shows the accuracy of the final answer generated by each method. The right panel provides a more nuanced breakdown, displaying the frequency of different reasoning outcomes such as correct labels, correct paths, and various types of errors (incorrect label, longer path, wrong target, hallucination).  This dual view allows for a comprehensive assessment of model efficacy not only in terms of correct answer generation but also in terms of the quality and soundness of the reasoning process itself.", "section": "5 Understanding the Latent Reasoning in Coconut"}, {"figure_path": "https://arxiv.org/html/2412.06769/extracted/6056756/figures/percentile.png", "caption": "Figure 6: A case study of ProsQA. The model trained with CoT hallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end. Coconut (k=1) outputs a path that ends with an irrelevant node. Coconut (k=2) solves the problem correctly.", "description": "This figure showcases a specific example from the ProsQA dataset, highlighting the different reasoning approaches of Chain-of-Thought (CoT), COCONUT (k=1), and COCONUT (k=2).  The problem involves determining if a statement is true or false based on a set of logical rules. CoT, operating within the language space, makes an incorrect deduction, generating a path that includes a nonexistent relationship (Every yumpus is a rempus). This showcases CoT's tendency to hallucinate information and get stuck. COCONUT (k=1), allowing for one step of latent reasoning, also produces an incorrect result due to selecting an irrelevant node. Finally, COCONUT (k=2), with two steps of latent reasoning, successfully navigates the problem and reaches the correct conclusion, demonstrating its effectiveness for complex planning tasks.", "section": "5 Understanding the Latent Reasoning in Coconut"}, {"figure_path": "https://arxiv.org/html/2412.06769/extracted/6056756/figures/value_stats_meta_2.png", "caption": "Figure 7: An illustration of the latent search trees. The example is the same test case as in Figure\u00a07. The height of a node (denoted as h\u210ehitalic_h in the figure) is defined as the longest distance to any leaf nodes in the graph. We show the probability of the first concept predicted by the model following latent thoughts (e.g., \u201clempus\u201d in the left figure). It is calculated as the multiplication of the probability of all tokens within the concept conditioned on previous context (omitted in the figure for brevity). This metric can be interpreted as an implicit value function estimated by the model, assessing the potential of each node leading to the correct answer.", "description": "Figure 7 illustrates the latent search tree used by the COCONUT model.  The example shown is the same as in Figure 6.  Node height (h) represents the longest path to a leaf node. The figure displays the probability of the model predicting a specific concept (e.g., \"lempus\") as the first step in the latent reasoning process.  This probability is calculated as the product of individual token probabilities within that concept, given the preceding context.  The omitted context for brevity does not affect the meaning.  Essentially, the probability acts as an implicit value function, estimated by the model, which indicates the likelihood of a particular path leading to the correct answer.", "section": "5.3 Interpreting the Latent Search Tree"}]