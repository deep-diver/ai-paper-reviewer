{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to the field of large language models (LLMs), introducing the concept of few-shot learning and demonstrating the potential of LLMs for various tasks."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "LLaMA: Open and efficient foundation language models", "publication_date": "2023-02-23", "reason": "This paper introduces LLaMA, an open and efficient foundational LLM that has had a major influence on the research and development of LLMs."}, {"fullname_first_author": "Zhengyan Zhang", "paper_title": "MoEfication: Transformer feed-forward layers are mixtures of experts", "publication_date": "2022-05-01", "reason": "This paper introduces the Mixture of Experts (MoE) architecture for improving the efficiency and scaling potential of Transformer-based LLMs."}, {"fullname_first_author": "Zonglin Li", "paper_title": "The lazy neuron phenomenon: On emergence of activation sparsity in Transformers", "publication_date": "2022-05-01", "reason": "This paper introduces the concept of \"lazy neurons\" and explores the prevalence of activation sparsity in Transformer models, a phenomenon which is central to this paper's research."}, {"fullname_first_author": "William Fedus", "paper_title": "Switch Transformers: Scaling to trillion parameter models with simple and efficient sparsity", "publication_date": "2022-05-01", "reason": "This paper introduces Switch Transformers, a novel architecture which efficiently scales LLMs to trillion parameters by leveraging sparsity."}]}