[{"figure_path": "https://arxiv.org/html/2503.10589/x1.png", "caption": "Figure 1: \nWe propose Long Context Tuning\u00a0(LCT) to expand the context window of pre-trained single-shot video diffusion models.\nA direct application of LCT is scene-level video generation for short film production, as shown in the top example.\nWe also show several emerging capabilities offered by LCT, such as interactive multi-shot direction and single shot extension, as well as zero-shot compositional generation, despite the model having never been trained on such tasks.\nWe recommend referring to our Project Page for better visualization.", "description": "This figure demonstrates the capabilities of Long Context Tuning (LCT), a method for enhancing single-shot video diffusion models to generate multi-shot videos with scene-level consistency. The top example showcases scene-level video generation suitable for short film production. The figure also illustrates three additional capabilities enabled by LCT: interactive multi-shot direction (allowing for modifications during generation), single-shot extension (extending a single-shot video), and zero-shot compositional generation (creating videos by combining different character identities and environmental contexts).  Note that the model was not explicitly trained for these tasks, demonstrating emergent capabilities.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.10589/x2.png", "caption": "Figure 2: \nScene-level Video Data Example.\nGlobal prompt contains shared elements like character, environment, and story overview, while\nper-shot prompt details events in each shot.", "description": "Figure 2 shows an example of the scene-level video data used in the paper.  The figure highlights the two-tiered prompt structure used for generating videos: a global prompt that provides high-level information about characters, the setting, and the overall story; and per-shot prompts that give detailed instructions for each individual shot within the scene. This structure allows for a more nuanced control over the video generation process, ensuring consistency across shots while allowing for specific variations in each shot's content.", "section": "3.2. Towards Long Context Video Generation"}, {"figure_path": "https://arxiv.org/html/2503.10589/x3.png", "caption": "Figure 3: \nArchitecture Designs.\n(a)\u00a0Long-context MMDiT block.\nWe expand the attention operation to all text and video tokens within a scene, and apply independent noise levels to individual shots.\nThe interleaved 3D RoPE assigns distinct coordinates for each shot.\n(b) Interleaved 3D RoPE coordinates.\nAt shot-level, text tokens precede video tokens along the space diagonal.\nAt scene-level, tokens are arranged shot by shot, forming an interleaved \u201c[text]-[video]-[text]-...\u201d pattern along the space diagonal.", "description": "Figure 3 illustrates the architecture of the Long Context Tuning (LCT) model.  Panel (a) shows the Long-context MMDiT block, which extends the attention mechanism to encompass all text and video tokens within a scene. This allows the model to consider relationships between multiple shots simultaneously. It also incorporates independent noise levels for individual shots and uses 3D RoPE (Rotary Positional Embedding) to represent the positional information of tokens within the scene. The 3D RoPE provides a better representation of the spatial and temporal relationships between shots. Panel (b) details how the interleaved 3D RoPE coordinates are organized. At the shot level, text tokens appear before video tokens along the space diagonal, and at the scene level, shots are arranged sequentially, forming an interleaved pattern of text and video tokens. This organization helps the model understand the sequential structure of the scenes and learn the relationships between consecutive shots.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.10589/x4.png", "caption": "Figure 4: \nInference Modes.\n(a)\u00a0Bidirectional model enables (a.1)\u00a0joint or (a.2)\u00a0visual-conditioned generation, while (b)\u00a0context-causal model supports auto-regressive generation.", "description": "Figure 4 illustrates the different inference modes available for the proposed Long Context Tuning (LCT) model.  Panel (a) shows the bidirectional model, which offers two generation approaches: (a.1) joint denoising, where all shots in a scene are processed simultaneously; and (a.2) visual-conditioned generation, where some shots serve as conditions while others are generated. Panel (b) depicts the context-causal model, which uses autoregressive generation. In this mode, each shot's generation leverages information from previously generated shots, enabling a sequential, step-by-step creation of the scene.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.10589/x5.png", "caption": "Figure 5: \nQualitative Comparisons.\nWe show stacked video frames synthesized by all methods, and expand two shots to illustrate the \u201creappearance\u201d issue discussed in\u00a0Sec.\u00a04.1.\nThe simplified prompts for each shot can be found in the subtitle in \u201cOurs\u201d.", "description": "Figure 5 presents a qualitative comparison of various scene-level video generation methods.  It shows several video sequences generated by different models, illustrating the visual quality and consistency. To highlight a particular challenge of keyframe-based methods, the figure also expands upon two shots from one method, showing the \"reappearance\" problem where consistency issues arise when a character is not properly captured in initial keyframes but appears later in the video sequence. The \"Ours\" method's results include simplified caption prompts to help the reader better understand the generated content.", "section": "4.1. Comparison"}, {"figure_path": "https://arxiv.org/html/2503.10589/x6.png", "caption": "Figure 6: Fidelity to History Condition.\nThe video background generated by the causal model exhibits superior fidelity to the history condition, as evidenced by the street lights\u2019 layout.", "description": "This figure compares the background fidelity of videos generated using bidirectional and causal attention models.  The causal model, by leveraging information from previously generated frames, accurately replicates the streetlights' layout from the history condition, demonstrating superior consistency and attention to detail.", "section": "4.2 Ablative Studies"}, {"figure_path": "https://arxiv.org/html/2503.10589/x7.png", "caption": "Figure 7: Effects of Conditioning Timestep. Large timesteps sacrifice fidelity to the condition.", "description": "This figure shows how the choice of conditioning timestep in the autoregressive inference method affects the quality of the generated video.  A smaller timestep (closer to 0) uses cleaner history samples as conditions which leads to better fidelity to the original condition. However, using a very small timestep can lead to error accumulation, where errors from earlier frames are propagated to later frames. A larger timestep results in some loss of fidelity to the initial conditions, but helps mitigate the error accumulation problem. The figure illustrates this trade-off by showing the effect of different timesteps (500 and 900) on the fidelity of the video background compared to the original condition.", "section": "4.2 Ablative Studies"}, {"figure_path": "https://arxiv.org/html/2503.10589/x8.png", "caption": "Figure 8: Effects of History Timestep. Large timesteps mitigate \u201cerror accumulation\u201d issue at the cost of history fidelity.", "description": "This figure shows the impact of different conditioning timesteps on the quality of autoregressive video generation.  The experiment uses a model where previous frames are used as conditioning information (history) for generating subsequent frames.  Using clean history (timestep 0) preserves the most detail from past frames, but errors accumulate over time. As the timestep increases, some detail is lost from the history, but the overall structure and consistency of the generated video are better maintained as the model isn't overly influenced by small errors in earlier frames. The figure demonstrates a trade-off between preserving fidelity to the initial history (clean history) and mitigating the accumulation of errors in longer sequences.", "section": "4.2 Ablative Studies"}, {"figure_path": "https://arxiv.org/html/2503.10589/x9.png", "caption": "Figure 9: Causal Adaptation. After 1K updates from bidirectional weights, the causal architecture shows excellent consistency.", "description": "This figure demonstrates the effectiveness of adapting a pre-trained bidirectional model to a context-causal architecture.  Starting with weights from a bidirectional model, the model was fine-tuned with causal attention.  After only 1000 training iterations, the context-causal model displays significantly improved consistency in generated videos, comparable to the results achieved after substantially more training (9000 iterations). This highlights the efficiency of the causal adaptation process and its ability to quickly acquire the desired temporal coherence in video generation.", "section": "3.4 Implementations"}]