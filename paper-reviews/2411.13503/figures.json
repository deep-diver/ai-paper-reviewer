[{"figure_path": "https://arxiv.org/html/2411.13503/x1.png", "caption": "Figure 1: \nOverview of VBench++. We propose VBench++, a comprehensive and versatile benchmark suite for video generative models.\nWe design a comprehensive and hierarchical Evaluation Dimension Suite to decompose \u201cvideo generation quality\" into multiple well-defined dimensions to facilitate fine-grained and objective evaluation.\nFor each dimension and each content category, we carefully design a Prompt Suite as test cases, and sample Generated Videos from a set of video generation models.\nFor each evaluation dimension, we specifically design an Evaluation Method Suite, which uses a carefully crafted method or designated pipeline for automatic objective evaluation. We also conduct Human Preference Annotation for the generated videos for each dimension and show that VBench++ evaluation results are well aligned with human perceptions.\nVBench++ can provide valuable insights from multiple perspectives.\nVBench++ supports a wide range of video generation tasks, including text-to-video and image-to-video, with an adaptive Image Suite for fair evaluation across different settings. It evaluates not only technical quality but also the trustworthiness of generative models, offering a comprehensive view of model performance. We continually incorporate more video generative models into VBench++ to inform the community about the evolving landscape of video generation.", "description": "VBench++ is a comprehensive and versatile benchmark suite for evaluating video generation models.  It decomposes video quality into multiple well-defined dimensions, using a hierarchical structure. For each dimension and content category, a prompt suite provides test cases, generating videos from various models.  Each dimension has a tailored evaluation method, and human preference annotation validates the results. VBench++ supports text-to-video and image-to-video tasks, and includes an adaptive image suite for fair evaluations. Besides technical quality, it assesses model trustworthiness, providing a holistic performance view.  The benchmark is continuously updated with new models to reflect the evolving field of video generation.", "section": "3 VBench++ Suite"}, {"figure_path": "https://arxiv.org/html/2411.13503/x2.png", "caption": "(a) \nText-to-Video Generative Models. We visualize the evaluation results of four text-to-video generation models in 16 VBench dimensions.\nFor comprehensive numerical results, please refer to Table\u00a0II.", "description": "This figure displays a radar chart visualizing the performance of four different text-to-video generative models across sixteen distinct evaluation dimensions defined in the VBench benchmark. Each dimension represents a specific aspect of video generation quality. The radar chart allows for a visual comparison of the models' strengths and weaknesses across these various dimensions. For precise numerical data, refer to Table II within the paper.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2411.13503/x3.png", "caption": "(b) Image-to-Video Generative Models. We visualize the evaluation results of six image-to-video generation models.\nSee Table\u00a0III for comprehensive numerical results.", "description": "This figure displays a comparison of six different image-to-video generation models.  Each model's performance is visually represented, likely using a radar chart or similar visualization, across multiple dimensions of video quality.  The specific dimensions assessed are not shown in the caption, but are detailed elsewhere in the paper. For precise numerical data on the performance of each model, readers are referred to Table III.", "section": "4.1 Per-Dimension Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.13503/x4.png", "caption": "(c) Trustworthiness of Video Generative Models. We visualize the trustworthiness of video generative models, along with other dimensions. For comprehensive numerical results, please refer to Table\u00a0IV.", "description": "This figure (c) presents a radar chart visualizing the trustworthiness of several video generative models.  Each model's performance is shown across multiple trustworthiness dimensions, including cultural fairness, gender bias, skin tone bias, and safety.  The radar chart provides a visual comparison of the models' strengths and weaknesses in these aspects, allowing for quick identification of models that are particularly strong or weak in certain dimensions. The caption encourages readers to consult Table IV for a detailed numerical breakdown of the results displayed visually in the chart.", "section": "VBench++ Evaluation Results"}, {"figure_path": "https://arxiv.org/html/2411.13503/x5.png", "caption": "Figure 2: VBench++ Evaluation Results. We visualize the evaluation results of text-to-video and image-to-video generative models using VBench++. We normalize the results per dimension for clearer comparisons.", "description": "This figure presents a visual comparison of various text-to-video and image-to-video generative models, evaluated using the VBench++ benchmark suite.  The results are normalized across dimensions for easy comparison.  Each subfigure focuses on a specific aspect of model performance.  Subfigure (a) displays results for text-to-video models, subfigure (b) shows results for image-to-video models, and subfigure (c) illustrates model trustworthiness across several dimensions.", "section": "VBench++ Evaluation Results"}, {"figure_path": "https://arxiv.org/html/2411.13503/x6.png", "caption": "Figure 3: Prompt Suite Statistics. The two graphs provide an overview of our prompt suites. Left: the word cloud to visualize word distribution of our prompt suites. Right: the number of prompts across different evaluation dimensions and different content categories.", "description": "Figure 3 presents a statistical overview of the prompt suites used in the VBench++ benchmark. The left panel displays a word cloud illustrating the frequency distribution of words across all prompts.  This provides a visual representation of the types of scenes and objects frequently featured in the test cases. The right panel presents a bar chart showing the number of prompts used for each of the 16 evaluation dimensions and also broken down by eight different content categories (Animal, Architecture, Food, Human, Lifestyle, Plant, Scenery, Vehicles). This visualization helps to understand the scope and balance of the prompt suites in terms of both the granularity of evaluation and diversity of content.", "section": "3.2 Prompt Suite"}, {"figure_path": "https://arxiv.org/html/2411.13503/extracted/6013496/figures/i2v/fig_image_crop_pipeline_horizontal.jpg", "caption": "Figure 4: Interface for Human Preference Annotation. Top: prompt and question. Right: choices that annotators can make. Bottom left: control for stop and playback.", "description": "This figure shows the user interface for human annotation in the VBench++ system.  The top part displays the prompt used to generate videos and the question annotators need to answer to provide their preference. The right side shows the three options annotators have for providing their preference:  'A is better', 'Same quality', and 'B is better'. The bottom left corner displays controls for video playback, allowing annotators to stop and replay the videos as needed to make their judgment.", "section": "3.4 Human Preference Annotation"}, {"figure_path": "https://arxiv.org/html/2411.13503/extracted/6013496/figures/i2v/fig_image_crop_pipeline_vertical.jpg", "caption": "(a) Cropping Pipeline for Portrait Images.", "description": "This figure illustrates the image cropping pipeline used for portrait images in the Image Suite.  The pipeline ensures that the main content remains centered and unaltered regardless of the final aspect ratio.  It starts with an initial crop to a 1:1 aspect ratio, followed by a second crop to a 16:9 aspect ratio.  Additional crops with intermediate aspect ratios (7:4 and 8:5) are then generated by interpolating between the 1:1 and 16:9 crops.", "section": "3.3 Image Suite"}, {"figure_path": "https://arxiv.org/html/2411.13503/extracted/6013496/figures/i2v/pie_chart_image_suite.png", "caption": "(b) Cropping Pipeline for Landscape Images.", "description": "This figure illustrates the image cropping pipeline used for landscape-oriented images in the Image Suite of VBench++. The pipeline ensures that the main content remains centered and unaltered after cropping the images to various aspect ratios (1:1, 7:4, 8:5, and 16:9).  It starts by first cropping to a 16:9 aspect ratio. Then, another crop to a 1:1 aspect ratio is performed. Finally, additional crops are generated between the 16:9 and 1:1 bounding boxes to achieve the other aspect ratios.", "section": "3.3 Image Suite"}, {"figure_path": "https://arxiv.org/html/2411.13503/x7.png", "caption": "Figure 5: \nImage Suite Pipeline for Adaptive Aspect Ratio Cropping. We provide a pipeline that crops images to various aspect ratios while preserving key content. (a) Portrait Images. If the original image\u2019s width is less than its height, it is first cropped to a 1:1 ratio (red bounding box), followed by a second crop to a 16:9 aspect ratio (yellow bounding box). Additional crops interpolate between the 1:1 red box and the 16:9 yellow box to produce other common ratios (1:1, 7:4, 8:5, 16:9). (b) Landscape Images. If the original image\u2019s width is greater than its height, we first crop the image to a 16:9 aspect ratio (red bounding box), and further crop the 16:9 image to a 1:1 aspect ratio (yellow bounding box). We then perform additional crops between the 16:9 red box and 1:1 yellow box to obtain the common aspect ratios (1:1, 7:4, 8:5, 16:9).", "description": "This figure illustrates the adaptive cropping pipeline used to prepare the Image Suite for the image-to-video (I2V) task.  The pipeline handles both portrait and landscape images.  For portrait images (height greater than width), a 1:1 crop is performed first (red box), followed by a 16:9 crop (yellow box).  Intermediate aspect ratios (7:4 and 8:5) are generated by interpolating between these two crops.  Landscape images (width greater than height) are processed similarly, but the initial crop is 16:9, followed by a 1:1 crop, with interpolation generating the intermediate ratios. This ensures consistent image content across various aspect ratios, crucial for fair I2V model evaluation.", "section": "3.3 Image Suite"}, {"figure_path": "https://arxiv.org/html/2411.13503/x8.png", "caption": "Figure 6: Content Distribution of Image Suite. Our image suite encompasses a wide variety of content to ensure a comprehensive evaluation.", "description": "This figure visualizes the diversity of content included in the Image Suite used for evaluating image-to-video (I2V) models.  The suite includes a wide range of foreground subjects (such as animals, humans, plants, vehicles, and abstract objects) and background scenes (like architecture, scenery, and indoor settings) to ensure comprehensive testing of I2V models' ability to handle diverse and realistic visual input across different scenarios and content categories.", "section": "3.3 Image Suite"}, {"figure_path": "https://arxiv.org/html/2411.13503/x9.png", "caption": "(a) T2V Results for Trustworthiness.", "description": "This figure visualizes the trustworthiness of text-to-video (T2V) generative models across four dimensions: Culture Fairness, Gender Bias, Skin Bias, and Safety.  Each model's performance is presented as a score for each dimension, indicating how well it avoids biases and generates safe content.", "section": "4.1 Per-Dimension Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.13503/x10.png", "caption": "(b) T2I Results for Trustworthiness.", "description": "This figure visualizes the trustworthiness evaluation results of image-to-video generative models across various dimensions. These dimensions likely include metrics measuring aspects such as cultural fairness, gender bias, skin tone bias, and overall safety.  The figure is likely a bar chart or radar chart comparing several models on those specific trustworthiness dimensions.", "section": "4.1 Per-Dimension Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.13503/x11.png", "caption": "(a) Trustworthiness of Video Generative Models.", "description": "This figure shows a radar chart visualizing the trustworthiness scores of several video generative models across different dimensions: Culture Fairness, Gender Bias, Skin Bias, and Safety. Each dimension represents a specific aspect of model trustworthiness, reflecting the model's ability to avoid biases and generate safe, unbiased content. The scores for each dimension likely indicate the model's performance in that area, with higher scores suggesting better performance. The chart provides a comparative overview of different models' trustworthiness, allowing for insights into their strengths and weaknesses concerning bias and safety.", "section": "4.1 Per-Dimension Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.13503/x12.png", "caption": "(b) Trustworthiness of Video vs. Image Models.", "description": "This figure displays a comparison of the trustworthiness scores for video and image generative models.  Trustworthiness is evaluated across dimensions such as culture fairness, gender bias, skin tone bias, and safety.  The models are visually compared, allowing for a quick assessment of their relative strengths and weaknesses in producing unbiased and safe outputs.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2411.13503/x13.png", "caption": "Figure 7: Trustworthiness of Visual Generative Models. We visualize the trustworthiness evaluation results of visual generative models. For comprehensive numerical results, please refer to Table\u00a0IV.", "description": "Figure 7 presents a visual comparison of the trustworthiness evaluation results for several video and image generative models.  It uses radar charts to display the scores across the four dimensions of trustworthiness: Culture Fairness, Gender Bias, Skin Tone Bias, and Safety.  Each model is represented by a separate chart.  The numerical values for these scores are detailed in Table IV of the paper.", "section": "4 Experiments"}]