[{"figure_path": "https://arxiv.org/html/2412.14689/x1.png", "caption": "Figure 1: Model collapse of synthetic data.\u00a0\u2460 The model continuously trains on its previously generated data, leading to a gradual decline in model performance, i.e., model collapse. Starting from real data (xo,yo)subscript\ud835\udc65\ud835\udc5csubscript\ud835\udc66\ud835\udc5c(x_{o},y_{o})( italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ), the test error Et\u2062e\u2062s\u2062tsubscript\ud835\udc38\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61E_{test}italic_E start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPT increases as f0subscript\ud835\udc530f_{0}italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT undergoes iterative training on synthetic data (y1,y2,\u2026,yn)subscript\ud835\udc661subscript\ud835\udc662\u2026subscript\ud835\udc66\ud835\udc5b(y_{1},y_{2},\\dots,y_{n})( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ).\n\u2461 ToEdit\u00a0(ours), we use a trained model for token-level editing rather than purely synthesizing data.\nLeveraging f0subscript\ud835\udc530f_{0}italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and an operation matrix misubscript\ud835\udc5a\ud835\udc56m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to edit the data, the test error is constrained within a fixed upper bound. Therefore, we can preserve the distribution coverage to avoid model collapse.", "description": "This figure illustrates the concept of model collapse when using synthetic data for training and proposes a solution. In the first part, it shows how continuous training on self-generated data leads to an increase in the test error, which is the characteristic of model collapse. The second part introduces a method called 'ToEdit', where a pre-trained model is used for token-level editing of existing data instead of generating entirely new data. This method aims to constrain the test error within a bounded range, thereby preventing model collapse and maintaining distribution coverage.", "section": "Non-iterative Model Collapse"}, {"figure_path": "https://arxiv.org/html/2412.14689/x2.png", "caption": "Figure 2: Non-iterative model collapse. Training language models from scratch on AI-synthesized data or a mixture of human and synthetic data leads to performance degradation. This degradation is negatively correlated with the proportion of synthetic data used in training.\nA. We pre-train GPT-2 Small (124M) on human (Dolma\u00a0(Soldaini et\u00a0al., 2024)) and synthetic (Cosmopedia\u00a0(Ben\u00a0Allal et\u00a0al., 2024)) data. As the proportion of synthetic data increases, the model\u2019s loss decreases. B. As the proportion of synthetic data increases, the PPL also rises. This trend remains consistent across different validation sets. More results on downstream tasks are presented in\u00a010 and \u00a011.", "description": "This figure demonstrates the negative impact of using synthetic data for training language models.  In the experiment, GPT-2 Small (124M) was pre-trained using varying proportions of human-generated text (Dolma dataset) and AI-synthesized text (Cosmopedia dataset). Part A shows that, counter-intuitively, the model's training loss decreases as the percentage of synthetic data increases. This is because the model is overfitting to the characteristics of the synthetic data. However, as shown in Part B, increased reliance on synthetic data results in significantly higher perplexity scores (PPL) across multiple validation sets, indicating a decline in the model's ability to generalize to unseen data. This demonstrates a negative correlation between the amount of synthetic data and overall model performance.", "section": "Non-Iterative Model Collapse"}, {"figure_path": "https://arxiv.org/html/2412.14689/x3.png", "caption": "Figure 3: PPL distribution of human and synthetic data estimated by Llama-3-8B. The synthetic data lacks the long tail of the human-produced data and is also concentrated within the first 25%percent2525\\%25 % of the human-produced data distribution. A. Distribution of human-produced data is sharp with a long tail, spanning a wide range from 0 to over 100. B. The values are concentrated within a much narrower range, mostly between 0 and 12.\nThe experiment uses Dolma v6 and Cosmopedia as human and synthetic data, each with sampled 6B tokens. More results in Figure\u00a09.", "description": "This figure compares the perplexity (PPL) distributions of human-generated text (Dolma v6) and synthetic text (Cosmopedia), both sampled at 6 billion tokens.  The Llama-3-8B language model was used to estimate the PPL for each dataset. The human-generated text exhibits a sharp distribution with a long tail, ranging from a PPL of 0 to over 100. In contrast, the synthetic data shows a much narrower, concentrated distribution, with most PPL values falling between 0 and 12.  This highlights a key difference: the synthetic data lacks the diversity and long tail present in the human-generated text, suggesting a potential limitation of relying solely on synthetic data for training language models.", "section": "2.2 WHY DOES SYNTHETIC DATA FAIL IN LANGUAGE MODEL PRE-TRAINING?"}, {"figure_path": "https://arxiv.org/html/2412.14689/x4.png", "caption": "Figure 4: A. Embedding visualization using t-SNE and sentence-transformers. B. pre-training results for selected synthetic data and other data mixtures.", "description": "Figure 4 visualizes the results of experiments on synthetic data. Panel A shows a t-SNE embedding plot comparing the feature representations of human-produced data (Dolma), purely synthetic data (Cosmopedia), and synthetic data selected using the DSIR method. This helps to understand the distributional differences between the datasets. Panel B presents pre-training results using OLMo-237M. The perplexity (PPL) values are shown for various mixtures of human and synthetic data, along with results using only selected synthetic data. This showcases the impact of different data compositions on model performance.", "section": "2.2 Why Does Synthetic Data Fail in Language Model Pre-Training?"}, {"figure_path": "https://arxiv.org/html/2412.14689/x5.png", "caption": "Figure 5: Uni/Bi-gram feature distribution across 10,000 hash buckets.", "description": "This figure visualizes the distribution of unigrams and bigrams from both human-produced and synthetic text data.  The features are hashed into 10,000 buckets, allowing for a comparison of feature frequency and distribution between the two datasets. The figure aims to illustrate the differences in the feature landscape of the two data types, specifically highlighting the over-concentration of features in the synthetic data compared to the broader distribution in human-produced data, which suggests a lack of diversity and potential overfitting issues.", "section": "2.2 WHY DOES SYNTHETIC DATA FAIL IN LANGUAGE MODEL PRE-TRAINING?"}, {"figure_path": "https://arxiv.org/html/2412.14689/x6.png", "caption": "Figure 6: U-shape token probability distribution of Dolma-sampled V6 estimated by Qwen-0.5B-Instruct\u00a0(qwe, 2024).", "description": "The figure displays the probability distribution of tokens within the Dolma-sampled V6 dataset, as estimated by the Qwen-0.5B-Instruct model.  The distribution exhibits a U-shape, indicating a concentration of tokens with both very high and very low probabilities, and a relative scarcity of tokens with intermediate probabilities. This U-shaped distribution is key to the paper's proposed token-level editing method, which uses probability as a guide to modify tokens to improve the quality of synthetic data.", "section": "3 TOKEN-LEVEL EDITING"}, {"figure_path": "https://arxiv.org/html/2412.14689/x7.png", "caption": "Figure 7: OLMo-237M pretraining with mixed human and synthetic data proportions. We pretrain the OLMo-237M model using a mixture of human data (Dolma\u00a0(Soldaini et\u00a0al., 2024)) and synthetic data (Cosmopedia\u00a0(Ben\u00a0Allal et\u00a0al., 2024)).", "description": "This figure displays the results of pre-training the OLMo-237M language model using various mixtures of human-generated text data from the Dolma dataset and synthetic text data from the Cosmopedia dataset.  The x-axis represents the amount of training data (in billions of tokens), and the y-axis shows the pre-training loss.  Multiple lines are presented, each corresponding to a different proportion of synthetic data in the training mixture (0%, 25%, 50%, 75%, and 100%). The figure visually demonstrates the impact of synthetic data on the model's pre-training performance, revealing a trend of increasing loss as the proportion of synthetic data increases. This illustrates the concept of model collapse, where reliance on synthetic data negatively affects model performance.", "section": "2 Non-iterative Model Collapse"}, {"figure_path": "https://arxiv.org/html/2412.14689/x8.png", "caption": "Figure 8: GPT-2 perplexity (PPL) on validation sets, trained from scratch.", "description": "This figure displays the perplexity (PPL) scores achieved by GPT-2 models trained from scratch on datasets with varying proportions of synthetic data.  The PPL, a metric evaluating how well a language model predicts a given dataset (lower is better), is shown across several validation sets. The graph visualizes the impact of synthetic data on the model's performance, illustrating how increasing the proportion of synthetic data affects the model's ability to generalize to unseen data.", "section": "2 Non-iterative Model Collapse"}, {"figure_path": "https://arxiv.org/html/2412.14689/x9.png", "caption": "Figure 9: PPL distribution of human and synthetic data estimated by StabLM-Zephyr-3B. This indicates that different prior distributions yielded the same result, which is consistent with Figure\u00a03. The synthetic data lacks a long tail and is concentrated within a narrow portion of the distribution.", "description": "Figure 9 presents the probability distribution of perplexity (PPL) scores for both human-generated and synthetic text data.  The PPL, calculated using the StableLM-Zephyr-3B language model, measures how well a model predicts the next word in a sequence, with lower scores indicating better predictability. The distribution of PPL scores for human text shows a long tail, meaning the model sometimes struggles to predict words accurately, reflecting the diversity and complexity of human language. In contrast, the distribution for synthetic data is concentrated within a much narrower range and lacks a long tail. This indicates that the synthetic text is less diverse and more predictable than human text, thus highlighting a key characteristic of synthetic data: it often fails to capture the full complexity and nuances present in real-world human language.", "section": "2.2 WHY DOES SYNTHETIC DATA FAIL IN LANGUAGE MODEL PRE-TRAINING?"}, {"figure_path": "https://arxiv.org/html/2412.14689/x10.png", "caption": "Figure 10: The top 40 bi-grams from separately sampled 1M subsets of Dolma, Cosmopedia, and DSIR-selected datasets.", "description": "This figure presents a comparison of the top 40 most frequent bi-grams (pairs of consecutive words) found in three different datasets: Dolma (human-written text), Cosmopedia (synthetic text generated by a language model), and a subset of Cosmopedia filtered using the DSIR (Data Selection via Importance Resampling) method.  The bar chart visually represents the frequency of each bi-gram in each dataset, allowing for a direct comparison of the feature distributions across the different data sources. This comparison helps to highlight the differences in the linguistic features and patterns present in human-written text versus synthetic text, both before and after filtering by DSIR.", "section": "2.2 WHY DOES SYNTHETIC DATA FAIL IN LANGUAGE MODEL PRE-TRAINING?"}, {"figure_path": "https://arxiv.org/html/2412.14689/x11.png", "caption": "Figure 11: The top 64 bi-grams from separately sampled 1M subsets of Dolma, Cosmopedia, and DSIR-selected datasets.", "description": "This figure presents a comparison of the top 64 most frequent bi-grams (two-word combinations) found in three different datasets: Dolma (human-written text), Cosmopedia (synthetic text generated by a large language model), and a subset of Cosmopedia filtered using the DSIR (Data Selection via Importance Resampling) method. The bar chart visually represents the frequency of each bi-gram in the respective datasets, allowing for a direct comparison of the feature distributions between human-generated text and synthetic text, both before and after applying a data selection technique.  This visualization helps illustrate the differences in the n-gram features between human-authored and synthetic text, particularly highlighting the over-concentration of certain bi-grams in the synthetic datasets. ", "section": "2.2 WHY DOES SYNTHETIC DATA FAIL IN LANGUAGE MODEL PRE-TRAINING?"}, {"figure_path": "https://arxiv.org/html/2412.14689/x12.png", "caption": "Figure 12: Density sampling response values. This result further confirms the issue of feature collapse in synthetic data.", "description": "Figure 12 presents a heatmap visualization of the distribution of locality-sensitive hashing (LSH) feature values obtained from density sampling of synthetic data.  The heatmap shows the frequency of different feature values across a range of hash functions. A significant observation is that the feature values are heavily concentrated within a narrow range, showing a lack of diversity. This concentration, visualized as a sharp peak in the distribution, indicates a phenomenon called 'feature collapse'. Feature collapse in synthetic data means that the generated data lacks the diversity and richness of features present in real, human-generated data. This limited feature coverage directly impacts the performance of language models trained on this type of data, limiting their ability to generalize well to unseen real-world examples.", "section": "2.2 WHY DOES SYNTHETIC DATA FAIL IN LANGUAGE MODEL PRE-TRAINING?"}]