{"references": [{"fullname_first_author": "A. Bansal", "paper_title": "End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without Overthinking", "publication_date": "2022-10-01", "reason": "This paper introduces the concept of recurrent depth networks for algorithm synthesis, which is a foundational concept for the current work's architecture."}, {"fullname_first_author": "A. Schwarzschild", "paper_title": "Deep Thinking Systems: Logical Extrapolation with Recurrent Networks", "publication_date": "2023-10-01", "reason": "This PhD thesis provides a strong theoretical foundation for the use of recurrent layers to enhance reasoning abilities in neural networks."}, {"fullname_first_author": "J. Kaplan", "paper_title": "Scaling Laws for Neural Language Models", "publication_date": "2020-01-15", "reason": "This work established scaling laws for language models, which provides valuable context for understanding the relationship between model size, compute, and performance in the current work."}, {"fullname_first_author": "S. Bai", "paper_title": "Deep Equilibrium Models", "publication_date": "2019-10-01", "reason": "This work introduced deep equilibrium models, a type of recurrent architecture related to the model proposed in the current paper, providing a relevant comparison point."}, {"fullname_first_author": "M. Cai", "paper_title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads", "publication_date": "2024-06-01", "reason": "This paper discusses speculative decoding, a technique which the current model naturally supports due to its recurrent structure, offering a comparison in approach to improving efficiency."}]}