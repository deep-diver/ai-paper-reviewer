<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model &#183; HF Daily Paper Reviews by AI"><meta name=description content="video-SALMONN-01: An open-source audio-visual LLM enhances video understanding with a novel reasoning-intensive dataset and the pDPO method, achieving significant accuracy gains."><meta name=keywords content="Multimodal Learning,Multimodal Reasoning,üè¢ Tsinghua University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11775/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11775/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model"><meta property="og:description" content="video-SALMONN-01: An open-source audio-visual LLM enhances video understanding with a novel reasoning-intensive dataset and the pDPO method, achieving significant accuracy gains."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-02-17T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-17T00:00:00+00:00"><meta property="article:tag" content="Multimodal Learning"><meta property="article:tag" content="Multimodal Reasoning"><meta property="article:tag" content="üè¢ Tsinghua University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11775/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11775/cover.png"><meta name=twitter:title content="video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model"><meta name=twitter:description content="video-SALMONN-01: An open-source audio-visual LLM enhances video understanding with a novel reasoning-intensive dataset and the pDPO method, achieving significant accuracy gains."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model","headline":"video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model","abstract":"video-SALMONN-01: An open-source audio-visual LLM enhances video understanding with a novel reasoning-intensive dataset and the pDPO method, achieving significant accuracy gains.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2502.11775\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-02-17T00:00:00\u002b00:00","datePublished":"2025-02-17T00:00:00\u002b00:00","dateModified":"2025-02-17T00:00:00\u002b00:00","keywords":["Multimodal Learning","Multimodal Reasoning","üè¢ Tsinghua University"],"mainEntityOfPage":"true","wordCount":"4398"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-04/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-04</p></a><a href=/ai-paper-reviewer/2025-03-05/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-05</p></a><a href=/ai-paper-reviewer/2025-03-06/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-06</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-04/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-04</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-05/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-05</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-06/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-06</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2502.11775/cover_hu764149984836054546.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2502.11775/>video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-02-17T00:00:00+00:00>17 February 2025</time><span class="px-2 text-primary-500">&#183;</span><span>4398 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">21 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2502.11775/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2502.11775/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-reasoning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Reasoning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-tsinghua-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Tsinghua University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#reasoning-in-llms>Reasoning in LLMs</a></li><li><a href=#multimodal-reasoning>Multimodal Reasoning</a></li><li><a href=#pdpo-optimization>pDPO Optimization</a></li><li><a href=#rivabench-dataset>RivaBench Dataset</a></li><li><a href=#zero-shot-detection>Zero-Shot Detection</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#reasoning-in-llms>Reasoning in LLMs</a></li><li><a href=#multimodal-reasoning>Multimodal Reasoning</a></li><li><a href=#pdpo-optimization>pDPO Optimization</a></li><li><a href=#rivabench-dataset>RivaBench Dataset</a></li><li><a href=#zero-shot-detection>Zero-Shot Detection</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2502.11775</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Guangzhi Sun et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-02-18</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2502.11775 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2502.11775 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2502.11775/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current large language models (LLMs) struggle with complex reasoning, especially in the context of video understanding. Existing methods often focus on specific tasks like solving mathematical problems or analyzing images, neglecting the broader applications of video understanding. Moreover, creating high-quality datasets for multimodal reasoning is challenging.</p><p>This research introduces video-SALMONN-01, the first open-source reasoning-enhanced audio-visual LLM designed for general video understanding. To address the issue of dataset scarcity, they created a new reasoning-intensive dataset with step-by-step solutions. They also developed a novel optimization technique, pDPO, that efficiently models step-level rewards for multimodal inputs. Their model outperforms existing baselines on various video reasoning benchmarks, demonstrating the effectiveness of their approach. Furthermore, RivaBench, a new benchmark dataset, was created to facilitate future research in this area.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-4a397ec7217149673079d81c3d357357></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-4a397ec7217149673079d81c3d357357",{strings:[" video-SALMONN-01, the first open-source reasoning-enhanced audio-visual LLM for general video understanding, was developed. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-1b3e17d52bf80d48a3835c05692f9e86></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-1b3e17d52bf80d48a3835c05692f9e86",{strings:[" A new reasoning-intensive video understanding benchmark, RivaBench, was introduced. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-06896d60594f49ad590a89f570d66423></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-06896d60594f49ad590a89f570d66423",{strings:[" The proposed pDPO method significantly improved reasoning accuracy compared to baselines. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is important because it addresses the limitations of existing large language models in handling complex reasoning tasks within general video understanding. <strong>It introduces a novel reasoning-enhanced audio-visual LLM, video-SALMONN-01, along with a new benchmark, RivaBench, pushing the boundaries of multimodal reasoning.</strong> The proposed methods, including process direct preference optimization (pDPO), offer significant improvements in accuracy and zero-shot capabilities, opening exciting avenues for future research in multimodal AI.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x1.png alt></figure></p><blockquote><p>üîº The figure illustrates the architecture of the video-SALMONN-01 model. It shows how input video data is processed through two parallel branches: one for visual information and one for audio. Each branch uses an encoder (visual encoder and audio encoder) to extract relevant features from the respective input sequences (frames and audio segments). These feature representations are then combined in an interleaved manner to align temporal information and create a unified representation suitable for processing by a Large Language Model (LLM). This unified representation is then inputted to the LLM for further analysis and task completion.</p><details><summary>read the caption</summary>Figure 1: video-SALMONN-o1 model structure. The input video is processed by the visual and audio branches, generating encodings from the visual and audio frame sequences respectively. Two encoding streams are combined in an interleaved fashion to synchronize across time before sending to LLM.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S5.T1.5><thead class=ltx_thead><tr class=ltx_tr id=S5.T1.5.4.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=S5.T1.5.4.1.1><span class=ltx_text id=S5.T1.5.4.1.1.1 style=font-size:80%>Attribute</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T1.5.4.1.2><span class=ltx_text id=S5.T1.5.4.1.2.1 style=font-size:80%>Academic</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T1.5.4.1.3><span class=ltx_text id=S5.T1.5.4.1.3.1 style=font-size:80%>StandUp</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T1.5.4.1.4><span class=ltx_text id=S5.T1.5.4.1.4.1 style=font-size:80%>SynthDec</span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T1.5.5.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S5.T1.5.5.1.1><span class=ltx_text id=S5.T1.5.5.1.1.1 style=font-size:80%>Num. of QA</span></th><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.5.5.1.2><span class=ltx_text id=S5.T1.5.5.1.2.1 style=font-size:80%>1,912</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.5.5.1.3><span class=ltx_text id=S5.T1.5.5.1.3.1 style=font-size:80%>2,128</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.5.5.1.4><span class=ltx_text id=S5.T1.5.5.1.4.1 style=font-size:80%>200</span></td></tr><tr class=ltx_tr id=S5.T1.5.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S5.T1.5.3.4><span class=ltx_text id=S5.T1.5.3.4.1 style=font-size:80%>Duration (s)</span></th><td class="ltx_td ltx_align_center" id=S5.T1.3.1.1><span class=ltx_text id=S5.T1.3.1.1.1 style=font-size:80%>47.2</span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.3.1.1.m1.1"><semantics id="S5.T1.3.1.1.m1.1a"><mo id="S5.T1.3.1.1.m1.1.1" mathsize="80%" xref="S5.T1.3.1.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.3.1.1.m1.1.1.cmml" xref="S5.T1.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.1.1.m1.1d">¬±</annotation></semantics></math><span class=ltx_text id=S5.T1.3.1.1.2 style=font-size:80%> 66.1</span></td><td class="ltx_td ltx_align_center" id=S5.T1.4.2.2><span class=ltx_text id=S5.T1.4.2.2.1 style=font-size:80%>43.2</span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.4.2.2.m1.1"><semantics id="S5.T1.4.2.2.m1.1a"><mo id="S5.T1.4.2.2.m1.1.1" mathsize="80%" xref="S5.T1.4.2.2.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.2.2.m1.1b"><csymbol cd="latexml" id="S5.T1.4.2.2.m1.1.1.cmml" xref="S5.T1.4.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.2.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.2.2.m1.1d">¬±</annotation></semantics></math><span class=ltx_text id=S5.T1.4.2.2.2 style=font-size:80%> 15.1</span></td><td class="ltx_td ltx_align_center" id=S5.T1.5.3.3><span class=ltx_text id=S5.T1.5.3.3.1 style=font-size:80%>8.1</span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.5.3.3.m1.1"><semantics id="S5.T1.5.3.3.m1.1a"><mo id="S5.T1.5.3.3.m1.1.1" mathsize="80%" xref="S5.T1.5.3.3.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.5.3.3.m1.1b"><csymbol cd="latexml" id="S5.T1.5.3.3.m1.1.1.cmml" xref="S5.T1.5.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.3.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.5.3.3.m1.1d">¬±</annotation></semantics></math><span class=ltx_text id=S5.T1.5.3.3.2 style=font-size:80%>3.2</span></td></tr><tr class=ltx_tr id=S5.T1.5.6.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S5.T1.5.6.2.1><span class=ltx_text id=S5.T1.5.6.2.1.1 style=font-size:80%>Format</span></th><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T1.5.6.2.2><span class=ltx_text id=S5.T1.5.6.2.2.1 style=font-size:80%>5-way MCQ</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T1.5.6.2.3><span class=ltx_text id=S5.T1.5.6.2.3.1 style=font-size:80%>5-way MCQ</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T1.5.6.2.4><span class=ltx_text id=S5.T1.5.6.2.4.1 style=font-size:80%>Yes/No</span></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 1 provides a detailed breakdown of the RivaBench dataset&rsquo;s statistics, a new benchmark specifically designed for evaluating the reasoning capabilities of audio-visual LLMs. It shows the number of question-answer pairs, average video duration (mean and standard deviation), and the type of questions (Multiple Choice Questions, or MCQ) for three different scenarios or partitions within the dataset: Academic, StandUp, and SynthDec. The SynthDec partition is particularly noteworthy, as it is composed of 100 synthetically generated videos paired with 100 real videos selected to have similar content. This careful construction aims to assess the model&rsquo;s ability to distinguish between real and synthetic video data. All videos used in RivaBench are sourced from YouTube.</p><details><summary>read the caption</summary>Table 1: RivaBench basic statistics. The duration is given by mean ¬±plus-or-minus\pm¬± standard deviation. The SynthDec split contains 100 synthetic videos and 100 real videos that human annotators search to have similar content as synthetic videos. MCQ stands for multiple-choice questions. Video sources are all from YouTube.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Reasoning in LLMs<div id=reasoning-in-llms class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#reasoning-in-llms aria-label=Anchor>#</a></span></h4><p>Reasoning capabilities in large language models (LLMs) are a rapidly evolving area of research. Early approaches focused on prompt engineering and search algorithms to guide LLMs through complex problems. However, these methods proved inefficient and limited for intricate reasoning tasks. A significant advancement is the development of reinforcement learning techniques. <strong>Reinforcement learning (RL) based methods, such as Process Direct Preference Optimization (pDPO), optimize the reasoning process by directly rewarding or penalizing each step taken towards a solution.</strong> This contrasts with earlier methods that only considered the final outcome. <strong>Multimodal LLMs, those that integrate visual and audio inputs, further challenge the task of reasoning</strong>, requiring sophisticated reward modeling tailored to diverse sensory data. Effective benchmarks, such as RivaBench, are crucial for evaluating progress. The core challenge remains balancing model efficiency with accuracy and avoiding biases or hallucinations. <strong>Future work needs to focus on addressing the high computational cost of RL training for multimodal LLMs and developing robust methods that handle ambiguous or complex inputs.</strong></p><h4 class="relative group">Multimodal Reasoning<div id=multimodal-reasoning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multimodal-reasoning aria-label=Anchor>#</a></span></h4><p>Multimodal reasoning, a crucial aspect of artificial intelligence, focuses on the ability of systems to integrate and interpret information from diverse sources like text, images, audio, and video to solve complex problems. <strong>The key challenge lies in effectively fusing these heterogeneous data modalities, each with its own unique characteristics and representations.</strong> This integration requires advanced techniques capable of handling ambiguity, noise, and potentially conflicting information across modalities. Successful multimodal reasoning systems must be able to <strong>disambiguate meaning</strong>, <strong>establish relationships between different modalities</strong>, and <strong>generate coherent inferences</strong> that go beyond simple concatenation or averaging of individual modal outputs. <strong>Progress in this area relies heavily on robust feature extraction, efficient fusion mechanisms, and advanced reasoning algorithms that can handle multimodal contexts.</strong> The development of large-scale, high-quality datasets is vital for training and evaluating such systems, as well as the creation of new evaluation metrics that capture the nuances of multimodal reasoning capabilities. Future research should explore methods for <strong>improved explainability and interpretability</strong>, particularly crucial for building trust and understanding in these powerful systems.</p><h4 class="relative group">pDPO Optimization<div id=pdpo-optimization class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#pdpo-optimization aria-label=Anchor>#</a></span></h4><p>The concept of pDPO (process direct preference optimization) presents a novel approach to enhance the reasoning capabilities of multimodal large language models (LLMs). <strong>Instead of directly predicting a numerical reward for each reasoning step</strong>, as in traditional methods, pDPO leverages <strong>contrastive learning</strong>. It compares the effectiveness of different reasoning steps within the same context, focusing on step-level pairwise comparisons rather than absolute scores. This is particularly beneficial for multimodal tasks, as the ambiguity in assessing numerical scores is mitigated. By employing a contrastive selection method, pDPO efficiently identifies and prioritizes the most critical reasoning steps that significantly affect the final outcome. <strong>This targeted approach improves training efficiency and reduces computational costs</strong> compared to methods relying on extensive rollouts. The innovative combination of contrastive step selection and pairwise reward modeling makes pDPO well-suited for the challenges of multimodal reasoning, where various modalities (audio, visual, text) must be effectively integrated. The results demonstrate that pDPO achieves significant performance improvements, highlighting its potential as a powerful technique for enhancing reasoning in complex, real-world scenarios.</p><h4 class="relative group">RivaBench Dataset<div id=rivabench-dataset class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#rivabench-dataset aria-label=Anchor>#</a></span></h4><p>The RivaBench dataset represents a significant contribution to the field of multimodal reasoning, particularly for audio-visual large language models (LLMs). Its <strong>focus on challenging, high-quality question-answer pairs across diverse scenarios</strong> such as stand-up comedy, academic presentations, and synthetic video detection addresses a critical gap in existing benchmarks. The inclusion of detailed, expert-curated step-by-step solutions for each question makes RivaBench ideal for training and evaluating LLMs&rsquo; reasoning capabilities. The dataset&rsquo;s <strong>three representative scenarios</strong> offer a variety of challenges: stand-up comedy tests the model&rsquo;s ability to interpret humor within an audio-visual context; academic presentations assess its ability to process complex information and extract meaning; and synthetic video detection pushes the boundaries of zero-shot capabilities. The <strong>4,000+ high-quality question-answer pairs</strong> within RivaBench ensure rigorous evaluation of models. By incorporating these varied settings, RivaBench facilitates a deeper understanding of the strengths and limitations of audio-visual LLMs, fostering the development of more robust and reliable models.</p><h4 class="relative group">Zero-Shot Detection<div id=zero-shot-detection class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#zero-shot-detection aria-label=Anchor>#</a></span></h4><p>Zero-shot detection, in the context of this research paper, signifies the model&rsquo;s ability to identify synthetic videos without prior training examples. This is a <strong>remarkable feat</strong>, showcasing the model&rsquo;s robust generalization capabilities and deeper understanding of visual patterns. The paper highlights the <strong>enhanced reasoning abilities</strong> as the key contributor to this performance, suggesting that the model is not merely matching visual features but actively inferring the underlying characteristics of real versus synthetic videos. This capability goes beyond typical image classification tasks, showcasing the potential for broader applications in video authenticity verification, content generation evaluation, and anomaly detection. The success of zero-shot detection underlines the importance of <strong>reasoning-enhanced</strong> multimodal models in tackling complex visual understanding tasks. Further investigation could explore the limitations of this approach, its sensitivity to various types of synthetic videos, and how it can be improved with additional training or architectural refinements. The <strong>zero-shot nature</strong> of this capability suggests a step toward more generalizable and robust AI systems that can adapt to novel challenges with minimal additional training.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x2.png alt></figure></p><blockquote><p>üîº This figure illustrates the process of creating a dataset for fine-tuning (SFT) a large language model (LLM). The process starts with a video and its corresponding audio. Gemini-1.5-pro, a large language model, is used to generate a question-answer pair related to the video content, along with a step-by-step reasoning path that explains how to arrive at the answer. GPT-40, another LLM, then acts as a quality control mechanism, evaluating the validity and logical coherence of both the question-answer pair and the reasoning steps. Only those QA pairs and reasoning paths that pass GPT-40&rsquo;s quality check are included in the final SFT dataset. This ensures that the model learns from high-quality, logically sound reasoning examples.</p><details><summary>read the caption</summary>Figure 2: Acquisition pipeline of reasoning-intensive SFT data. The question, answer and reasoning paths are generated by Gemini-1.5-pro taking the video with paired audio as inputs. GPT4o is employed for quality checks to ensure the QA-pair and the reasoning steps are valid and require logical thinking.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x3.png alt></figure></p><blockquote><p>üîº Figure 3 illustrates the process of contrastive step selection and pairwise rollout used in the process direct preference optimization (pDPO) method. The top panel shows how the top two most influential steps (s2 and s5) are selected for optimization. For each step, an alternative step is sampled to create a pair used for comparison. The bottom panel demonstrates the pairwise rollout process. For each step (like s2), multiple possible next steps are simulated (rollouts). These rollouts, starting from the same prefix, are then evaluated by GPT-40 to determine which is closer to the correct solution and thus which step was more effective.</p><details><summary>read the caption</summary>Figure 3: Illustration of the contrastive step selection (top) and pairwise rollout (bottom) to construct per-step expected correctness score for pDPO. Contrastive step selection: Top 2 steps, s2subscriptùë†2s_{2}italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and s5subscriptùë†5s_{5}italic_s start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT are selected in this example, and for s2subscriptùë†2s_{2}italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, an alternative step, s2‚Ä≤subscriptsuperscriptùë†‚Ä≤2s^{\prime}_{2}italic_s start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, is sampled to form the preference pair. Pairwise rollout: Three rollouts are shown for each step and s2subscriptùë†2s_{2}italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and s2‚Ä≤subscriptsuperscriptùë†‚Ä≤2s^{\prime}_{2}italic_s start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are step pairs with the same prefix solution. The answer correctness is checked using GPT-4o by comparing it against the reference answer.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x4.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of the number of reasoning steps in the supervised fine-tuning (SFT) data. The left panel displays the distribution for the entire SFT dataset, while the right panel focuses on a subset of the data specifically designed for reasoning-intensive tasks. The figure highlights that the reasoning-intensive subset, which contains more challenging questions, necessitates a greater number of reasoning steps for successful problem-solving, compared to the overall SFT dataset.</p><details><summary>read the caption</summary>Figure 4: Distributions of the numbers of reasoning steps in SFT data. Left: Distribution of the entire SFT data. Right: Distribution on the reasoning-intensive subset of SFT data. Due to the difficulty of the reasoning-intensive subset, more reasoning steps are required in general for samples in this set.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x5.png alt></figure></p><blockquote><p>üîº This figure compares the performance of the process direct preference optimization (pDPO) algorithm using different numbers of top steps selected for pairwise training. The x-axis represents the number of top steps selected, and the y-axis represents the accuracy. The results show that including intermediate steps in addition to full solution paths improves the overall accuracy of pDPO, suggesting that focusing on specific, error-prone steps enhances the model&rsquo;s learning and reasoning capabilities. Note that pairs of full solution paths are always included in the training.</p><details><summary>read the caption</summary>Figure 5: Comparison between different top T steps selected for pDPO. Pairs of full solution paths are always used in addition to pairs of intermediate steps.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x6.png alt></figure></p><blockquote><p>üîº This figure shows an example from the reasoning-intensive SFT (Supervised Fine-Tuning) dataset. It illustrates a question-answer pair along with a step-by-step reasoning process. The visual input is a short video showing items made of glass and plastic. The question asks which material is better when considering frequent handling and minimal dropping risk. The provided answer is &rsquo;the glass one&rsquo;, and the figure details a six-step reasoning path leading to that conclusion. This showcases the multimodal nature of the dataset, combining visual information from a video with text-based questions, answers, and reasoning steps.</p><details><summary>read the caption</summary>Figure 6: Example of reasoning SFT data</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x7.png alt></figure></p><blockquote><p>üîº This figure shows an example from the StandUp comedy section of the RivaBench dataset. The example includes a short video clip, a question related to understanding the humor in the video, the correct answer, and an explanation clarifying why that answer is correct. The question requires the model to understand the interplay between the audio (comedian&rsquo;s words), visual (comedian&rsquo;s actions and facial expressions), and the audience&rsquo;s reaction (laughter) to determine the humor&rsquo;s source. It highlights the multi-modal nature of the reasoning required within RivaBench.</p><details><summary>read the caption</summary>Figure 7: Example of StandUp part of the RivaBench.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x8.png alt></figure></p><blockquote><p>üîº Figure 8 shows an example from the StandUp comedy subset of the RivaBench dataset. It displays a still image from a stand-up comedy video, the corresponding audio transcription, a multiple-choice question about the comedic effect of a particular line, the correct answer, and a detailed explanation justifying the answer. This exemplifies the type of high-quality, expert-annotated audio-visual reasoning data included in the RivaBench benchmark.</p><details><summary>read the caption</summary>Figure 8: Example of StandUp part of the RivaBench.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x9.png alt></figure></p><blockquote><p>üîº Figure 9 shows an example from the Academic portion of the RivaBench dataset. It displays a slide from an academic presentation about a twin study investigating the relationship between traumatic brain injury (TBI) and dementia. The figure also includes a portion of the accompanying audio transcript and the question and answer related to this video segment. The question assesses the study&rsquo;s method for isolating the impact of TBI on dementia risk, while the answer explains how the twin study design controls for genetic and early life factors by analyzing twins with differing TBI and dementia onset.</p><details><summary>read the caption</summary>Figure 9: Example of Academic part of the RivaBench.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x10.png alt></figure></p><blockquote><p>üîº Figure 10 shows an example from the Academic portion of the RivaBench dataset. The figure contains a screenshot of a video showing students working on a hands-on electronics project using either a Zoom-based remote learning environment or a RobotAR augmented reality environment. Accompanying the video screenshot is a textual description of the key learning competencies tested (conceptual understanding of voltage and current, series and parallel circuits, use of a breadboard, multimeter, and building a working circuit). The results show that students in the RobotAR condition achieved greater competency in more of the learning objectives than their peers using Zoom. The caption references the improved performance with RobotAR.</p><details><summary>read the caption</summary>Figure 10: Example of Academic part of the RivaBench.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x11.png alt></figure></p><blockquote><p>üîº This figure shows an example video clip from the RivaBench benchmark&rsquo;s SynthDec (synthetic video detection) partition. The SynthDec partition contains videos generated using AI, making it challenging to distinguish between real and synthetic content. This particular clip is likely intended to showcase a particularly difficult example or characteristic of synthetic videos for which visual reasoning is needed to accurately classify.</p><details><summary>read the caption</summary>Figure 11: Example video clip of the SynthDec part of RivaBench.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x12.png alt></figure></p><blockquote><p>üîº This figure shows a short video clip from the RivaBench dataset&rsquo;s SynthDec (synthetic video detection) subset. The video clip is an example of a synthetic video, meaning it was artificially generated rather than recorded from real life. SynthDec is designed to test the ability of large language models to differentiate between real and synthetic videos. This specific clip likely contains visual anomalies or inconsistencies that are characteristic of AI-generated videos and would be used to train or evaluate such a model.</p><details><summary>read the caption</summary>Figure 12: Example video clip of the SynthDec part of RivaBench.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x13.png alt></figure></p><blockquote><p>üîº This figure shows a side-by-side comparison of how two different models, video-SALMONN-01 SFT and video-SALMONN-01 Process DPO, approached the same question from the StandUp subset of the RivaBench benchmark dataset. The StandUp subset contains comedic video clips. The video shows a comedian performing on stage. The question asks what the speaker implies by saying &lsquo;I didn&rsquo;t need to know that&rsquo; at the end of the video. Both models provide a step-by-step reasoning process leading to their respective answers. The figure highlights the differences in the reasoning process, showing how the enhanced reasoning capabilities of pDPO lead to a more accurate and nuanced understanding of the context compared to the simpler SFT model.</p><details><summary>read the caption</summary>Figure 13: Example video and solutions from the StandUp test set.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x14.png alt></figure></p><blockquote><p>üîº This figure showcases an example video frame from the VideoMME test set, accompanied by solution approaches from two distinct model versions: video-SALMONN-01 using standard supervised fine-tuning (SFT) and video-SALMONN-01 employing the proposed Process Direct Preference Optimization (pDPO). The video depicts a scene involving a character in a game being struck by a turret. The solutions highlight the different reasoning steps each model undertakes to arrive at its answer, and how pDPO leads to a more accurate response by leveraging its audio-visual reasoning capabilities.</p><details><summary>read the caption</summary>Figure 14: Example video and solutions from videoMME test set.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x15.png alt></figure></p><blockquote><p>üîº This figure showcases a video from the VideoMME test set and presents two different solution approaches generated by the model. The top solution demonstrates the model&rsquo;s initial attempt using supervised fine-tuning (SFT), revealing a flawed reasoning process due to misinterpreting visual information (mistaking moonlit sky for the moon). The bottom solution illustrates the enhanced performance achieved through the proposed process direct preference optimization (pDPO) method. Using pDPO, the model correctly identifies the missing element in the video.</p><details><summary>read the caption</summary>Figure 15: Example video and solutions from videoMME test set.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x16.png alt></figure></p><blockquote><p>üîº This figure showcases a comparative analysis of three different large language models (LLMs) - video-SALMONN-01, GPT-4, and Gemini-1.5-pro - in their ability to detect synthetic videos. Each model is presented with the same video clip and asked to determine if it is synthetically generated or real. The figure displays the response of each model, highlighting the reasoning steps and the final conclusion reached by each model. This comparison underscores the varying capabilities of these LLMs in handling this nuanced task, demonstrating the strengths and weaknesses in their ability to process and interpret visual cues to identify artificial or synthetic video content.</p><details><summary>read the caption</summary>Figure 16: Example output from video-SALMONN-o1, GPT-4o and Gemini-1.5-pro for synthetic video detection.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x17.png alt></figure></p><blockquote><p>üîº Figure 17 shows a comparison of the outputs of three different large language models (LLMs) ‚Äì video-SALMONN-01, GPT-4, and Gemini-1.5-pro ‚Äì when tasked with detecting whether a video is synthetically generated or real. The models analyze the same video and provide a Yes/No answer, along with a step-by-step reasoning process justifying their conclusions. The figure highlights the differences in their reasoning abilities and the types of visual cues each model focuses on to arrive at its decision. This showcases the varying capabilities of these LLMs in identifying subtle visual artifacts characteristic of AI-generated videos.</p><details><summary>read the caption</summary>Figure 17: Example output from video-SALMONN-o1, GPT-4o and Gemini-1.5-pro for synthetic video detection.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.11775/x18.png alt></figure></p><blockquote><p>üîº Figure 18 illustrates the contrastive step selection process used in the process direct preference optimization (pDPO) method. Two example reasoning paths for answering a question about a video are shown, highlighting the step-by-step reasoning process. Each step in the reasoning path has an associated score (d<sub>sk</sub>), representing the sensitivity of that step to small perturbations in the input video. A higher d<sub>sk</sub> score indicates a greater sensitivity, suggesting that the model is more likely to make an error at that step. The figure demonstrates how the algorithm selects the most crucial steps for further optimization by focusing on steps with high d<sub>sk</sub> scores, particularly those where errors due to visual misinterpretations or hallucinations might occur. In this example, the third step in the first solution path demonstrates a visual hallucination, and hence it receives a very high d<sub>sk</sub> score and is selected for optimization via rollout.</p><details><summary>read the caption</summary>Figure 18: Example of the contrastive step selection process where two sampled paths are shown and the scores dsksubscriptùëësubscriptùë†ùëòd_{s_{k}}italic_d start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT are given for each reasoning steps. The 3rd step in the first solution is wrong due to visual hallucination, and as a result, a very high score is assigned to that step and that step will be used to perform rollout.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S6.T2.7><tbody class=ltx_tbody><tr class=ltx_tr id=S6.T2.7.6.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=S6.T2.7.6.1.1 style=padding-left:4pt;padding-right:4pt>Model</th><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T2.7.6.1.2 style=padding-left:4pt;padding-right:4pt>Modality</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T2.7.6.1.3 style=padding-left:4pt;padding-right:4pt>VideoMME</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T2.7.6.1.4 style=padding-left:4pt;padding-right:4pt>NExT-QA</td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=3 id=S6.T2.7.6.1.5 style=padding-left:4pt;padding-right:4pt>RivaBench</td></tr><tr class=ltx_tr id=S6.T2.7.7.2><th class="ltx_td ltx_th ltx_th_row" id=S6.T2.7.7.2.1 style=padding-left:4pt;padding-right:4pt></th><td class=ltx_td id=S6.T2.7.7.2.2 style=padding-left:4pt;padding-right:4pt></td><td class=ltx_td id=S6.T2.7.7.2.3 style=padding-left:4pt;padding-right:4pt></td><td class=ltx_td id=S6.T2.7.7.2.4 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_align_center" id=S6.T2.7.7.2.5 style=padding-left:4pt;padding-right:4pt>StandUp</td><td class="ltx_td ltx_align_center" id=S6.T2.7.7.2.6 style=padding-left:4pt;padding-right:4pt>Academic</td><td class="ltx_td ltx_align_center" id=S6.T2.7.7.2.7 style=padding-left:4pt;padding-right:4pt>SynthDec (P/R)</td></tr><tr class=ltx_tr id=S6.T2.7.8.3 style=background-color:#e6e6e6><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S6.T2.7.8.3.1 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S6.T2.7.8.3.1.1 style=background-color:#e6e6e6>Proprietary models</span></th><td class="ltx_td ltx_border_t" id=S6.T2.7.8.3.2 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S6.T2.7.8.3.3 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S6.T2.7.8.3.4 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S6.T2.7.8.3.5 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S6.T2.7.8.3.6 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S6.T2.7.8.3.7 style=padding-left:4pt;padding-right:4pt></td></tr><tr class=ltx_tr id=S6.T2.3.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S6.T2.3.1.2 style=padding-left:4pt;padding-right:4pt>Gemini-1.5-pro <cite class="ltx_cite ltx_citemacro_citep">(Team et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.11775v1#bib.bib48 title>2024</a>)</cite></th><td class="ltx_td ltx_align_center" id=S6.T2.3.1.3 style=padding-left:4pt;padding-right:4pt>A+V</td><td class="ltx_td ltx_align_center" id=S6.T2.3.1.1 style=padding-left:4pt;padding-right:4pt>75.0%<math alttext="\dagger" class="ltx_Math" display="inline" id="S6.T2.3.1.1.m1.1"><semantics id="S6.T2.3.1.1.m1.1a"><mo id="S6.T2.3.1.1.m1.1.1" xref="S6.T2.3.1.1.m1.1.1.cmml">‚Ä†</mo><annotation-xml encoding="MathML-Content" id="S6.T2.3.1.1.m1.1b"><ci id="S6.T2.3.1.1.m1.1.1.cmml" xref="S6.T2.3.1.1.m1.1.1">‚Ä†</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.3.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S6.T2.3.1.1.m1.1d">‚Ä†</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=S6.T2.3.1.4 style=padding-left:4pt;padding-right:4pt>79.2%</td><td class="ltx_td ltx_align_center" id=S6.T2.3.1.5 style=padding-left:4pt;padding-right:4pt>75.8%</td><td class="ltx_td ltx_align_center" id=S6.T2.3.1.6 style=padding-left:4pt;padding-right:4pt>67.1%</td><td class="ltx_td ltx_align_center" id=S6.T2.3.1.7 style=padding-left:4pt;padding-right:4pt>23.6% (55%/15%)</td></tr><tr class=ltx_tr id=S6.T2.7.9.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S6.T2.7.9.4.1 style=padding-left:4pt;padding-right:4pt>Gemini-1.5-pro+reasoning</th><td class="ltx_td ltx_align_center" id=S6.T2.7.9.4.2 style=padding-left:4pt;padding-right:4pt>A+V</td><td class="ltx_td ltx_align_center" id=S6.T2.7.9.4.3 style=padding-left:4pt;padding-right:4pt>75.1%</td><td class="ltx_td ltx_align_center" id=S6.T2.7.9.4.4 style=padding-left:4pt;padding-right:4pt>79.5%</td><td class="ltx_td ltx_align_center" id=S6.T2.7.9.4.5 style=padding-left:4pt;padding-right:4pt>81.8%</td><td class="ltx_td ltx_align_center" id=S6.T2.7.9.4.6 style=padding-left:4pt;padding-right:4pt>69.5%</td><td class="ltx_td ltx_align_center" id=S6.T2.7.9.4.7 style=padding-left:4pt;padding-right:4pt>40.0% (49%/34%)</td></tr><tr class=ltx_tr id=S6.T2.4.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S6.T2.4.2.2 style=padding-left:4pt;padding-right:4pt>GPT-4o <cite class="ltx_cite ltx_citemacro_citep">(OpenAI Team, <a class=ltx_ref href=https://arxiv.org/html/2502.11775v1#bib.bib34 title>2024</a>)</cite></th><td class="ltx_td ltx_align_center" id=S6.T2.4.2.3 style=padding-left:4pt;padding-right:4pt>V</td><td class="ltx_td ltx_align_center" id=S6.T2.4.2.1 style=padding-left:4pt;padding-right:4pt>71.9%<math alttext="\dagger" class="ltx_Math" display="inline" id="S6.T2.4.2.1.m1.1"><semantics id="S6.T2.4.2.1.m1.1a"><mo id="S6.T2.4.2.1.m1.1.1" xref="S6.T2.4.2.1.m1.1.1.cmml">‚Ä†</mo><annotation-xml encoding="MathML-Content" id="S6.T2.4.2.1.m1.1b"><ci id="S6.T2.4.2.1.m1.1.1.cmml" xref="S6.T2.4.2.1.m1.1.1">‚Ä†</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.4.2.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S6.T2.4.2.1.m1.1d">‚Ä†</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=S6.T2.4.2.4 style=padding-left:4pt;padding-right:4pt>81.7%</td><td class="ltx_td ltx_align_center" id=S6.T2.4.2.5 style=padding-left:4pt;padding-right:4pt>63.3%</td><td class="ltx_td ltx_align_center" id=S6.T2.4.2.6 style=padding-left:4pt;padding-right:4pt>60.0%</td><td class="ltx_td ltx_align_center" id=S6.T2.4.2.7 style=padding-left:4pt;padding-right:4pt>34.1%(90%/21%)</td></tr><tr class=ltx_tr id=S6.T2.7.10.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S6.T2.7.10.5.1 style=padding-left:4pt;padding-right:4pt>GPT-4o+reasoning</th><td class="ltx_td ltx_align_center" id=S6.T2.7.10.5.2 style=padding-left:4pt;padding-right:4pt>V</td><td class="ltx_td ltx_align_center" id=S6.T2.7.10.5.3 style=padding-left:4pt;padding-right:4pt>72.1%</td><td class="ltx_td ltx_align_center" id=S6.T2.7.10.5.4 style=padding-left:4pt;padding-right:4pt>81.9%</td><td class="ltx_td ltx_align_center" id=S6.T2.7.10.5.5 style=padding-left:4pt;padding-right:4pt>69.6%</td><td class="ltx_td ltx_align_center" id=S6.T2.7.10.5.6 style=padding-left:4pt;padding-right:4pt>61.0%</td><td class="ltx_td ltx_align_center" id=S6.T2.7.10.5.7 style=padding-left:4pt;padding-right:4pt>25.8%(53%/17%)</td></tr><tr class=ltx_tr id=S6.T2.7.11.6 style=background-color:#e6e6e6><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S6.T2.7.11.6.1 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S6.T2.7.11.6.1.1 style=background-color:#e6e6e6>Open-source baselines</span></th><td class="ltx_td ltx_border_t" id=S6.T2.7.11.6.2 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S6.T2.7.11.6.3 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S6.T2.7.11.6.4 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S6.T2.7.11.6.5 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S6.T2.7.11.6.6 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S6.T2.7.11.6.7 style=padding-left:4pt;padding-right:4pt></td></tr><tr class=ltx_tr id=S6.T2.6.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S6.T2.6.4.3 style=padding-left:4pt;padding-right:4pt>LLaVA-OneVision <cite class="ltx_cite ltx_citemacro_citep">(Li et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.11775v1#bib.bib20 title>2024a</a>)</cite></th><td class="ltx_td ltx_align_center" id=S6.T2.6.4.4 style=padding-left:4pt;padding-right:4pt>V</td><td class="ltx_td ltx_align_center" id=S6.T2.5.3.1 style=padding-left:4pt;padding-right:4pt>58.2%<math alttext="\dagger" class="ltx_Math" display="inline" id="S6.T2.5.3.1.m1.1"><semantics id="S6.T2.5.3.1.m1.1a"><mo id="S6.T2.5.3.1.m1.1.1" xref="S6.T2.5.3.1.m1.1.1.cmml">‚Ä†</mo><annotation-xml encoding="MathML-Content" id="S6.T2.5.3.1.m1.1b"><ci id="S6.T2.5.3.1.m1.1.1.cmml" xref="S6.T2.5.3.1.m1.1.1">‚Ä†</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.5.3.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S6.T2.5.3.1.m1.1d">‚Ä†</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=S6.T2.6.4.2 style=padding-left:4pt;padding-right:4pt>79.4%<math alttext="\dagger" class="ltx_Math" display="inline" id="S6.T2.6.4.2.m1.1"><semantics id="S6.T2.6.4.2.m1.1a"><mo id="S6.T2.6.4.2.m1.1.1" xref="S6.T2.6.4.2.m1.1.1.cmml">‚Ä†</mo><annotation-xml encoding="MathML-Content" id="S6.T2.6.4.2.m1.1b"><ci id="S6.T2.6.4.2.m1.1.1.cmml" xref="S6.T2.6.4.2.m1.1.1">‚Ä†</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.6.4.2.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S6.T2.6.4.2.m1.1d">‚Ä†</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=S6.T2.6.4.5 style=padding-left:4pt;padding-right:4pt>67.2%</td><td class="ltx_td ltx_align_center" id=S6.T2.6.4.6 style=padding-left:4pt;padding-right:4pt>45.8%</td><td class="ltx_td ltx_align_center" id=S6.T2.6.4.7 style=padding-left:4pt;padding-right:4pt>0.0%(97%/0%)</td></tr><tr class=ltx_tr id=S6.T2.7.12.7><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S6.T2.7.12.7.1 style=padding-left:4pt;padding-right:4pt>video-SALMONN <cite class="ltx_cite ltx_citemacro_citep">(Sun et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.11775v1#bib.bib42 title>2024b</a>)</cite></th><td class="ltx_td ltx_align_center" id=S6.T2.7.12.7.2 style=padding-left:4pt;padding-right:4pt>A+V</td><td class="ltx_td ltx_align_center" id=S6.T2.7.12.7.3 style=padding-left:4pt;padding-right:4pt>43.3%</td><td class="ltx_td ltx_align_center" id=S6.T2.7.12.7.4 style=padding-left:4pt;padding-right:4pt>49.2%</td><td class="ltx_td ltx_align_center" id=S6.T2.7.12.7.5 style=padding-left:4pt;padding-right:4pt>47.8%</td><td class="ltx_td ltx_align_center" id=S6.T2.7.12.7.6 style=padding-left:4pt;padding-right:4pt>33.6%</td><td class="ltx_td ltx_align_center" id=S6.T2.7.12.7.7 style=padding-left:4pt;padding-right:4pt>0.0%(100%/0%)</td></tr><tr class=ltx_tr id=S6.T2.7.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S6.T2.7.5.2 style=padding-left:4pt;padding-right:4pt>Video-LLaMA 2.1 <cite class="ltx_cite ltx_citemacro_citep">(Cheng et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.11775v1#bib.bib5 title>2024</a>)</cite></th><td class="ltx_td ltx_align_center" id=S6.T2.7.5.3 style=padding-left:4pt;padding-right:4pt>A+V</td><td class="ltx_td ltx_align_center" id=S6.T2.7.5.1 style=padding-left:4pt;padding-right:4pt>54.9%<math alttext="\dagger" class="ltx_Math" display="inline" id="S6.T2.7.5.1.m1.1"><semantics id="S6.T2.7.5.1.m1.1a"><mo id="S6.T2.7.5.1.m1.1.1" xref="S6.T2.7.5.1.m1.1.1.cmml">‚Ä†</mo><annotation-xml encoding="MathML-Content" id="S6.T2.7.5.1.m1.1b"><ci id="S6.T2.7.5.1.m1.1.1.cmml" xref="S6.T2.7.5.1.m1.1.1">‚Ä†</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.7.5.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S6.T2.7.5.1.m1.1d">‚Ä†</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=S6.T2.7.5.4 style=padding-left:4pt;padding-right:4pt>75.6%</td><td class="ltx_td ltx_align_center" id=S6.T2.7.5.5 style=padding-left:4pt;padding-right:4pt>53.7%</td><td class="ltx_td ltx_align_center" id=S6.T2.7.5.6 style=padding-left:4pt;padding-right:4pt>34.3%</td><td class="ltx_td ltx_align_center" id=S6.T2.7.5.7 style=padding-left:4pt;padding-right:4pt>0.0%(99%/0%)</td></tr><tr class=ltx_tr id=S6.T2.7.13.8><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S6.T2.7.13.8.1 style=padding-left:4pt;padding-right:4pt>video-SALMONN-o1 (ours, SFT)</th><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T2.7.13.8.2 style=padding-left:4pt;padding-right:4pt>A+V</td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T2.7.13.8.3 style=padding-left:4pt;padding-right:4pt>62.9%</td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T2.7.13.8.4 style=padding-left:4pt;padding-right:4pt>78.2%</td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T2.7.13.8.5 style=padding-left:4pt;padding-right:4pt>68.6%</td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T2.7.13.8.6 style=padding-left:4pt;padding-right:4pt>42.5%</td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T2.7.13.8.7 style=padding-left:4pt;padding-right:4pt>5.8%(97%/5%)</td></tr><tr class=ltx_tr id=S6.T2.7.14.9><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S6.T2.7.14.9.1 style=padding-left:4pt;padding-right:4pt>video-SALMONN-o1 (ours, pDPO)</th><td class="ltx_td ltx_align_center ltx_border_bb" id=S6.T2.7.14.9.2 style=padding-left:4pt;padding-right:4pt>A+V</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S6.T2.7.14.9.3 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_bold" id=S6.T2.7.14.9.3.1>65.6</span>%</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S6.T2.7.14.9.4 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_bold" id=S6.T2.7.14.9.4.1>82.3</span>%</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S6.T2.7.14.9.5 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_bold" id=S6.T2.7.14.9.5.1>76.7</span>%</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S6.T2.7.14.9.6 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_bold" id=S6.T2.7.14.9.6.1>48.3</span>%</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S6.T2.7.14.9.7 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_bold" id=S6.T2.7.14.9.7.1>17.8</span>%(87%/13%)</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the main results of the video-SALMONN-01 model on three video understanding benchmarks: VideoMME, NEXT-QA, and RivaBench. It compares the performance of video-SALMONN-01, after both supervised fine-tuning (SFT) with reasoning data and further optimization using process direct preference optimization (pDPO), against several other visual (V) and audio-visual (A+V) large language models (LLMs). The metrics used are accuracy for VideoMME, NEXT-QA, and the StandUp and Academic subsets of RivaBench, and F1-score (Precision/Recall) for the SynthDec subset of RivaBench. Results marked with a dagger (‚Ä†) are taken directly from the cited papers. A key difference highlighted is that video-SALMONN-01 performs reasoning during inference, unlike the other open-source models which provide direct answers.</p><details><summary>read the caption</summary>Table 2: Main results of video-SALMONN-o1 compared against other visual (V) and audio-visual (A+V) LLMs. SFT refers to the model after SFT with reasoning data and pDPO refers to the model obtained after training with pDPO based on the same SFT model. F1-score (Precision/Recall) is reported for SynthDec and accuracy is reported for others. Results with ‚Ä†‚Ä†\dagger‚Ä† are directly taken from the corresponding papers. video-SALMONN-o1 performs reasoning during inference and other open-source models give answers directly.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S6.T3.7><tbody class=ltx_tbody><tr class=ltx_tr id=S6.T3.7.8.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=S6.T3.7.8.1.1>Training Data</th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id=S6.T3.7.8.1.2>Inference Reasoning</th><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T3.7.8.1.3>VideoMME</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T3.7.8.1.4>NExT-QA</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T3.7.8.1.5>Academic</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T3.7.8.1.6>StandUp</td></tr><tr class=ltx_tr id=S6.T3.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S6.T3.1.1.2>Full SFT data</th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id=S6.T3.1.1.1><svg class="ltx_picture ltx_markedasmath" height="10.02" id="S6.T3.1.1.1.m1.1.1.pic1" overflow="visible" width="10.02"><g fill="#000" stroke="#000" transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)"><g stroke-linecap="round" stroke-width=".7pt"><path d="M0 0C3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"/></g><g stroke-linecap="round" stroke-width=".7pt"><path d="M1.81 8.6C3.77 5.3 4.95 3.53 7.24.45" style="fill:none"/></g></g></svg></th><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T3.1.1.3><span class="ltx_text ltx_framed ltx_framed_underline" id=S6.T3.1.1.3.1>63.7%</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T3.1.1.4>80.7%</td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T3.1.1.5><span class="ltx_text ltx_framed ltx_framed_underline" id=S6.T3.1.1.5.1>45.2%</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T3.1.1.6><span class="ltx_text ltx_framed ltx_framed_underline" id=S6.T3.1.1.6.1>72.3%</span></td></tr><tr class=ltx_tr id=S6.T3.2.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S6.T3.2.2.2>Full SFT data</th><th class="ltx_td ltx_align_center ltx_th ltx_th_row" id=S6.T3.2.2.1><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T3.2.2.1.m1.1"><semantics id="S6.T3.2.2.1.m1.1a"><mi id="S6.T3.2.2.1.m1.1.1" mathvariant="normal" xref="S6.T3.2.2.1.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S6.T3.2.2.1.m1.1b"><ci id="S6.T3.2.2.1.m1.1.1.cmml" xref="S6.T3.2.2.1.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.2.2.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T3.2.2.1.m1.1d">‚úì</annotation></semantics></math></th><td class="ltx_td ltx_align_center" id=S6.T3.2.2.3>62.9%</td><td class="ltx_td ltx_align_center" id=S6.T3.2.2.4>78.2%</td><td class="ltx_td ltx_align_center" id=S6.T3.2.2.5>42.5%</td><td class="ltx_td ltx_align_center" id=S6.T3.2.2.6>68.6%</td></tr><tr class=ltx_tr id=S6.T3.3.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S6.T3.3.3.2>w/o any reasoning</th><th class="ltx_td ltx_align_center ltx_th ltx_th_row" id=S6.T3.3.3.1><svg class="ltx_picture ltx_markedasmath" height="10.02" id="S6.T3.3.3.1.m1.1.1.pic1" overflow="visible" width="10.02"><g fill="#000" stroke="#000" transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)"><g stroke-linecap="round" stroke-width=".7pt"><path d="M0 0C3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"/></g><g stroke-linecap="round" stroke-width=".7pt"><path d="M1.81 8.6C3.77 5.3 4.95 3.53 7.24.45" style="fill:none"/></g></g></svg></th><td class="ltx_td ltx_align_center" id=S6.T3.3.3.3>63.2%</td><td class="ltx_td ltx_align_center" id=S6.T3.3.3.4><span class="ltx_text ltx_framed ltx_framed_underline" id=S6.T3.3.3.4.1>81.0%</span></td><td class="ltx_td ltx_align_center" id=S6.T3.3.3.5>44.1%</td><td class="ltx_td ltx_align_center" id=S6.T3.3.3.6>71.1%</td></tr><tr class=ltx_tr id=S6.T3.4.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S6.T3.4.4.2>w/o reasoning-intensive part</th><th class="ltx_td ltx_align_center ltx_th ltx_th_row" id=S6.T3.4.4.1><svg class="ltx_picture ltx_markedasmath" height="10.02" id="S6.T3.4.4.1.m1.1.1.pic1" overflow="visible" width="10.02"><g fill="#000" stroke="#000" transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)"><g stroke-linecap="round" stroke-width=".7pt"><path d="M0 0C3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"/></g><g stroke-linecap="round" stroke-width=".7pt"><path d="M1.81 8.6C3.77 5.3 4.95 3.53 7.24.45" style="fill:none"/></g></g></svg></th><td class="ltx_td ltx_align_center" id=S6.T3.4.4.3>62.7%</td><td class="ltx_td ltx_align_center" id=S6.T3.4.4.4>78.9%</td><td class="ltx_td ltx_align_center" id=S6.T3.4.4.5>44.7%</td><td class="ltx_td ltx_align_center" id=S6.T3.4.4.6>71.5%</td></tr><tr class=ltx_tr id=S6.T3.5.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S6.T3.5.5.2>w/o reasoning-intensive part</th><th class="ltx_td ltx_align_center ltx_th ltx_th_row" id=S6.T3.5.5.1><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T3.5.5.1.m1.1"><semantics id="S6.T3.5.5.1.m1.1a"><mi id="S6.T3.5.5.1.m1.1.1" mathvariant="normal" xref="S6.T3.5.5.1.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S6.T3.5.5.1.m1.1b"><ci id="S6.T3.5.5.1.m1.1.1.cmml" xref="S6.T3.5.5.1.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.5.5.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T3.5.5.1.m1.1d">‚úì</annotation></semantics></math></th><td class="ltx_td ltx_align_center" id=S6.T3.5.5.3>61.6%</td><td class="ltx_td ltx_align_center" id=S6.T3.5.5.4>76.6%</td><td class="ltx_td ltx_align_center" id=S6.T3.5.5.5>42.3%</td><td class="ltx_td ltx_align_center" id=S6.T3.5.5.6>67.5%</td></tr><tr class=ltx_tr id=S6.T3.6.6><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S6.T3.6.6.2>Reasoning-intensive part only</th><th class="ltx_td ltx_align_center ltx_th ltx_th_row" id=S6.T3.6.6.1><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T3.6.6.1.m1.1"><semantics id="S6.T3.6.6.1.m1.1a"><mi id="S6.T3.6.6.1.m1.1.1" mathvariant="normal" xref="S6.T3.6.6.1.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S6.T3.6.6.1.m1.1b"><ci id="S6.T3.6.6.1.m1.1.1.cmml" xref="S6.T3.6.6.1.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.6.6.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T3.6.6.1.m1.1d">‚úì</annotation></semantics></math></th><td class="ltx_td ltx_align_center" id=S6.T3.6.6.3>58.8%</td><td class="ltx_td ltx_align_center" id=S6.T3.6.6.4>75.2%</td><td class="ltx_td ltx_align_center" id=S6.T3.6.6.5>40.1%</td><td class="ltx_td ltx_align_center" id=S6.T3.6.6.6>63.5%</td></tr><tr class=ltx_tr id=S6.T3.7.7><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id=S6.T3.7.7.2>Full SFT data + pDPO</th><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id=S6.T3.7.7.1><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T3.7.7.1.m1.1"><semantics id="S6.T3.7.7.1.m1.1a"><mi id="S6.T3.7.7.1.m1.1.1" mathvariant="normal" xref="S6.T3.7.7.1.m1.1.1.cmml">‚úì</mi><annotation-xml encoding="MathML-Content" id="S6.T3.7.7.1.m1.1b"><ci id="S6.T3.7.7.1.m1.1.1.cmml" xref="S6.T3.7.7.1.m1.1.1">‚úì</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.7.7.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T3.7.7.1.m1.1d">‚úì</annotation></semantics></math></th><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S6.T3.7.7.3><span class="ltx_text ltx_font_bold" id=S6.T3.7.7.3.1>65.6</span>%</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S6.T3.7.7.4><span class="ltx_text ltx_font_bold" id=S6.T3.7.7.4.1>82.3</span>%</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S6.T3.7.7.5><span class="ltx_text ltx_font_bold" id=S6.T3.7.7.5.1>48.3</span>%</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S6.T3.7.7.6><span class="ltx_text ltx_font_bold" id=S6.T3.7.7.6.1>76.7</span>%</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 3 analyzes how different subsets of the audio-visual supervised fine-tuning (SFT) data impact the model&rsquo;s performance on three different benchmarks: VideoMME, Academic, and StandUp. It compares the full SFT data, data without the reasoning-intensive portion, data with only the reasoning intensive part, and data where the model directly outputs answers without any reasoning steps. The results show the accuracy of each configuration across the three benchmarks, highlighting the importance of reasoning-intensive data and the effect of removing it. Underscores indicate the second-best performance for easier comparison.</p><details><summary>read the caption</summary>Table 3: Effect of different parts of the audio-visual SFT data on VideoMME, Academic and StandUp test sets. Underscore for second-best results. ‚Äúw/o reasoning-intensive part‚Äù means removing the reasoning-intensive SFT data, and ‚Äúw/o any reasoning‚Äù always directly outputting answers during SFT. ‚ÄúReasoning-intensive part only‚Äù always performs reasoning for QA.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S6.T4.1><tbody class=ltx_tbody><tr class=ltx_tr id=S6.T4.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=S6.T4.1.1.1.1>Training Configuration</th><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T4.1.1.1.2>Inference</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T4.1.1.1.3>VideoMME</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T4.1.1.1.4>NExT-QA</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T4.1.1.1.5>StandUp</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T4.1.1.1.6>Academic</td></tr><tr class=ltx_tr id=S6.T4.1.2.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S6.T4.1.2.2.1>SFT</th><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T4.1.2.2.2>1-best</td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T4.1.2.2.3>62.9%</td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T4.1.2.2.4>78.2%</td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T4.1.2.2.5>68.6%</td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T4.1.2.2.6>42.5%</td></tr><tr class=ltx_tr id=S6.T4.1.3.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S6.T4.1.3.3.1>SFT</th><td class="ltx_td ltx_align_center" id=S6.T4.1.3.3.2>Major@20</td><td class="ltx_td ltx_align_center" id=S6.T4.1.3.3.3>63.5%</td><td class="ltx_td ltx_align_center" id=S6.T4.1.3.3.4>81.5%</td><td class="ltx_td ltx_align_center" id=S6.T4.1.3.3.5>73.5%</td><td class="ltx_td ltx_align_center" id=S6.T4.1.3.3.6>45.3%</td></tr><tr class=ltx_tr id=S6.T4.1.4.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S6.T4.1.4.4.1>SFT + ORM</th><td class="ltx_td ltx_align_center" id=S6.T4.1.4.4.2>RM@20</td><td class="ltx_td ltx_align_center" id=S6.T4.1.4.4.3>62.7%</td><td class="ltx_td ltx_align_center" id=S6.T4.1.4.4.4>78.5%</td><td class="ltx_td ltx_align_center" id=S6.T4.1.4.4.5>69.0%</td><td class="ltx_td ltx_align_center" id=S6.T4.1.4.4.6>42.6%</td></tr><tr class=ltx_tr id=S6.T4.1.5.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S6.T4.1.5.5.1>SFT + PRM</th><td class="ltx_td ltx_align_center" id=S6.T4.1.5.5.2>RM@20</td><td class="ltx_td ltx_align_center" id=S6.T4.1.5.5.3>63.5%</td><td class="ltx_td ltx_align_center" id=S6.T4.1.5.5.4>79.3%</td><td class="ltx_td ltx_align_center" id=S6.T4.1.5.5.5>72.1%</td><td class="ltx_td ltx_align_center" id=S6.T4.1.5.5.6>43.9%</td></tr><tr class=ltx_tr id=S6.T4.1.6.6><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S6.T4.1.6.6.1>SFT + pDPO</th><td class="ltx_td ltx_align_center ltx_border_bb" id=S6.T4.1.6.6.2>1-best</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S6.T4.1.6.6.3><span class="ltx_text ltx_font_bold" id=S6.T4.1.6.6.3.1>65.6</span>%</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S6.T4.1.6.6.4><span class="ltx_text ltx_font_bold" id=S6.T4.1.6.6.4.1>82.3</span>%</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S6.T4.1.6.6.5><span class="ltx_text ltx_font_bold" id=S6.T4.1.6.6.5.1>76.7</span>%</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S6.T4.1.6.6.6><span class="ltx_text ltx_font_bold" id=S6.T4.1.6.6.6.1>48.3</span>%</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of different reward modeling methods on four video understanding benchmarks: VideoMME, NEXT-QA, StandUp (from RivaBench), and Academic (from RivaBench). The methods compared are standard supervised fine-tuning (SFT), SFT with outcome reward model (ORM), SFT with process reward model (PRM), and SFT with process direct preference optimization (pDPO). Performance is measured using two metrics: Major@20 (majority voting accuracy over 20 sampled reasoning paths) and RM@20 (best-of-n accuracy over 20 samples). The pDPO method, which uses preference pairs of complete reasoning paths, is highlighted as a key approach in the paper. The results demonstrate how the different reward models influence the ability of the large language model to reason through the video understanding problems.</p><details><summary>read the caption</summary>Table 4: Effect of different reward modelling methods on VideoMME, NExT-QA, the StandUp and Academic split of RivaBench. Major@20 and RM@20 are evaluated following Zhang et¬†al. (2024a), where Major@20 refers to the accuracy under majority voting with 20 sampled paths, and RM@20 is the best-of-n with 20 samples. Samples are all generated from the model after SFT. pDPO with full paths only uses preference pairs of complete reasoning paths.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=A5.T5.6><thead class=ltx_thead><tr class=ltx_tr id=A5.T5.6.7.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=A5.T5.6.7.1.1>Type</th><th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id=A5.T5.6.7.1.2><span class="ltx_inline-block ltx_align_top" id=A5.T5.6.7.1.2.1><span class=ltx_p id=A5.T5.6.7.1.2.1.1 style=width:341.4pt>Prompt content</span></span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=A5.T5.2.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=A5.T5.2.2.3>Direct answer</th><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=A5.T5.2.2.2><span class="ltx_inline-block ltx_align_top" id=A5.T5.2.2.2.2><span class=ltx_p id=A5.T5.2.2.2.2.2 style=width:341.4pt><math alttext="&lt;" class="ltx_Math" display="inline" id="A5.T5.1.1.1.1.1.m1.1"><semantics id="A5.T5.1.1.1.1.1.m1.1a"><mo id="A5.T5.1.1.1.1.1.m1.1.1" xref="A5.T5.1.1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="A5.T5.1.1.1.1.1.m1.1b"><lt id="A5.T5.1.1.1.1.1.m1.1.1.cmml" xref="A5.T5.1.1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="A5.T5.1.1.1.1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="A5.T5.1.1.1.1.1.m1.1d">&lt;</annotation></semantics></math>VIDEO<math alttext="&gt;" class="ltx_Math" display="inline" id="A5.T5.2.2.2.2.2.m2.1"><semantics id="A5.T5.2.2.2.2.2.m2.1a"><mo id="A5.T5.2.2.2.2.2.m2.1.1" xref="A5.T5.2.2.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A5.T5.2.2.2.2.2.m2.1b"><gt id="A5.T5.2.2.2.2.2.m2.1.1.cmml" xref="A5.T5.2.2.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A5.T5.2.2.2.2.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="A5.T5.2.2.2.2.2.m2.1d">&gt;</annotation></semantics></math>Select the best answer to the following question based on the video. Respond with only the letter of the correct option.</span></span></td></tr><tr class=ltx_tr id=A5.T5.6.8.1><th class="ltx_td ltx_th ltx_th_row" id=A5.T5.6.8.1.1></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T5.6.8.1.2><span class="ltx_inline-block ltx_align_top" id=A5.T5.6.8.1.2.1><span class=ltx_p id=A5.T5.6.8.1.2.1.1 style=width:341.4pt><span class=ltx_text id=A5.T5.6.8.1.2.1.1.1>{Question}</span></span></span></td></tr><tr class=ltx_tr id=A5.T5.6.9.2><th class="ltx_td ltx_th ltx_th_row" id=A5.T5.6.9.2.1></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T5.6.9.2.2><span class="ltx_inline-block ltx_align_top" id=A5.T5.6.9.2.2.1><span class=ltx_p id=A5.T5.6.9.2.2.1.1 style=width:341.4pt><span class=ltx_text id=A5.T5.6.9.2.2.1.1.1>Choose from: A. {Option A}, B, {Option B}‚Ä¶</span></span></span></td></tr><tr class=ltx_tr id=A5.T5.4.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T5.4.4.3>Reasoning</th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T5.4.4.2><span class="ltx_inline-block ltx_align_top" id=A5.T5.4.4.2.2><span class=ltx_p id=A5.T5.4.4.2.2.2 style=width:341.4pt><math alttext="&lt;" class="ltx_Math" display="inline" id="A5.T5.3.3.1.1.1.m1.1"><semantics id="A5.T5.3.3.1.1.1.m1.1a"><mo id="A5.T5.3.3.1.1.1.m1.1.1" xref="A5.T5.3.3.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="A5.T5.3.3.1.1.1.m1.1b"><lt id="A5.T5.3.3.1.1.1.m1.1.1.cmml" xref="A5.T5.3.3.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="A5.T5.3.3.1.1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="A5.T5.3.3.1.1.1.m1.1d">&lt;</annotation></semantics></math>VIDEO<math alttext="&gt;" class="ltx_Math" display="inline" id="A5.T5.4.4.2.2.2.m2.1"><semantics id="A5.T5.4.4.2.2.2.m2.1a"><mo id="A5.T5.4.4.2.2.2.m2.1.1" xref="A5.T5.4.4.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A5.T5.4.4.2.2.2.m2.1b"><gt id="A5.T5.4.4.2.2.2.m2.1.1.cmml" xref="A5.T5.4.4.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A5.T5.4.4.2.2.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="A5.T5.4.4.2.2.2.m2.1d">&gt;</annotation></semantics></math> Question:</span></span></td></tr><tr class=ltx_tr id=A5.T5.6.10.3><th class="ltx_td ltx_th ltx_th_row" id=A5.T5.6.10.3.1></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T5.6.10.3.2><span class="ltx_inline-block ltx_align_top" id=A5.T5.6.10.3.2.1><span class=ltx_p id=A5.T5.6.10.3.2.1.1 style=width:341.4pt><span class=ltx_text id=A5.T5.6.10.3.2.1.1.1>{Question}</span></span></span></td></tr><tr class=ltx_tr id=A5.T5.6.11.4><th class="ltx_td ltx_th ltx_th_row" id=A5.T5.6.11.4.1></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T5.6.11.4.2><span class="ltx_inline-block ltx_align_top" id=A5.T5.6.11.4.2.1><span class=ltx_p id=A5.T5.6.11.4.2.1.1 style=width:341.4pt><span class=ltx_text id=A5.T5.6.11.4.2.1.1.1>Choose from: A. {Option A}, B, {Option B}‚Ä¶</span></span></span></td></tr><tr class=ltx_tr id=A5.T5.6.6><th class="ltx_td ltx_th ltx_th_row" id=A5.T5.6.6.3></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T5.6.6.2><span class="ltx_inline-block ltx_align_top" id=A5.T5.6.6.2.2><span class=ltx_p id=A5.T5.6.6.2.2.2 style=width:341.4pt>Answer the question step by step. Output each thinking step. Mark the end of each step with <math alttext="&lt;" class="ltx_Math" display="inline" id="A5.T5.5.5.1.1.1.m1.1"><semantics id="A5.T5.5.5.1.1.1.m1.1a"><mo id="A5.T5.5.5.1.1.1.m1.1.1" xref="A5.T5.5.5.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="A5.T5.5.5.1.1.1.m1.1b"><lt id="A5.T5.5.5.1.1.1.m1.1.1.cmml" xref="A5.T5.5.5.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="A5.T5.5.5.1.1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="A5.T5.5.5.1.1.1.m1.1d">&lt;</annotation></semantics></math>end_of_step<math alttext="&gt;" class="ltx_Math" display="inline" id="A5.T5.6.6.2.2.2.m2.1"><semantics id="A5.T5.6.6.2.2.2.m2.1a"><mo id="A5.T5.6.6.2.2.2.m2.1.1" xref="A5.T5.6.6.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A5.T5.6.6.2.2.2.m2.1b"><gt id="A5.T5.6.6.2.2.2.m2.1.1.cmml" xref="A5.T5.6.6.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A5.T5.6.6.2.2.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="A5.T5.6.6.2.2.2.m2.1d">&gt;</annotation></semantics></math> token.</span></span></td></tr><tr class=ltx_tr id=A5.T5.6.12.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=A5.T5.6.12.5.1>SynthDec</th><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id=A5.T5.6.12.5.2><span class="ltx_inline-block ltx_align_top" id=A5.T5.6.12.5.2.1><span class=ltx_p id=A5.T5.6.12.5.2.1.1 style=width:341.4pt>An AI-generated video contains unnatural distorted things, such as distorted hands or faces. Is the given video AI generated? Answer YES or NO. Answer step by step and output each step clearly.</span></span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the prompt templates used for different task types in the video-SALMONN-01 model. It shows how the prompts are structured for direct answer tasks (requiring a single choice from provided options), reasoning tasks (requiring step-by-step explanations and a final answer), and synthetic video detection tasks (requiring identification of AI-generated video features and a yes/no answer). The different prompt structures illustrate how the model&rsquo;s interaction and expected output vary based on the task demands.</p><details><summary>read the caption</summary>Table 5: Prompt used for different types of tasks.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-ec0a79207c9e9687954c217306610ea4 class=gallery><img src=https://ai-paper-reviewer.com/2502.11775/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.11775/19.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11775/&amp;title=video-SALMONN-o1:%20Reasoning-enhanced%20Audio-visual%20Large%20Language%20Model" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11775/&amp;text=video-SALMONN-o1:%20Reasoning-enhanced%20Audio-visual%20Large%20Language%20Model" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11775/&amp;subject=video-SALMONN-o1:%20Reasoning-enhanced%20Audio-visual%20Large%20Language%20Model" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2502.11775/index.md",oid_likes="likes_paper-reviews/2502.11775/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2502.11275/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-16T00:00:00+00:00>16 February 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2502.13173/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Thinking Preference Optimization</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-17T00:00:00+00:00>17 February 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>