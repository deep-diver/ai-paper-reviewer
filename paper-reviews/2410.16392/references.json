{"references": [{" publication_date": "2020", "fullname_first_author": "Patrick Lewis", "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks", "reason": "This paper is foundational for the RAG (Retrieval Augmented Generation) systems discussed throughout the survey, which are a key type of compound AI system.  It established the core principles of combining LLMs with external knowledge retrieval, making it a highly influential and frequently cited work in this field.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Aman Madaan", "paper_title": "Self-refine: Iterative refinement with self-feedback", "reason": "This paper introduces a novel approach for refining the LLM outputs iteratively, which directly applies to the concept of LLM-based optimization described in the survey.  Its focus on feedback and refinement directly addresses the challenge of improving LLM performance through iterative optimization.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Harsh Trivedi", "paper_title": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions", "reason": "This paper is significant because it contributes to the understanding of multi-hop reasoning in RAG systems, a crucial element in designing efficient compound AI systems. Its exploration of chain-of-thought prompting combined with multi-hop retrieval directly informs the optimization strategies within the survey.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "reason": "This paper is highly relevant as it establishes the core concept of \"chain-of-thought\" prompting, a technique extensively used in LLM-based optimization.  The chain-of-thought approach is crucial for guiding the LLM optimizer towards more effective parameter updates, which is a central theme of the survey.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Xuezhi Wang", "paper_title": "Self-consistency improves chain of thought reasoning in language models", "reason": "The concept of self-consistency, where multiple LLM outputs are generated and compared, is directly relevant to the survey's discussion of LLM-based optimization. The method provides a way to improve reliability and reduce inconsistencies in LLM outputs, which is crucial for effective optimization.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Luyu Gao", "paper_title": "PAL: program-aided language models", "reason": "This paper proposes a novel approach to LLM-based reasoning using program-aided language models (PAL), directly aligning with the survey's focus on optimization strategies.  The use of program-like structures and code interpreters enhances optimization by explicitly structuring the reasoning process.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Justin Chih-Yao Chen", "paper_title": "Magicore: Multi-agent, iterative, coarse-to-fine refinement for reasoning", "reason": "This paper introduces a multi-agent approach to iterative refinement which shows clear connection to the optimization methods discussed in the survey, particularly concerning methods that leverage iterative feedback and refinement to improve the model's performance.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Ching-An Cheng", "paper_title": "Trace is the new autodiff - unlocking efficient optimization of computational workflows", "reason": "This work is highly relevant to the survey's focus on dynamic program analysis, and its methods have direct impact on LLM-based optimization. This paper introduces trace propagation as a novel way to efficiently handle credit assignment in complex systems, a core challenge addressed in the survey.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Mert Y\u00fcksekg\u00f6n\u00fcl", "paper_title": "Textgrad: Automatic \"differentiation\" via text", "reason": "This paper's technique is a novel and direct approach to LLM-based optimization.  The use of textual feedback to guide parameter updates directly addresses the challenges highlighted in the survey, especially in dynamic program analysis. The methodology is innovative and highly relevant.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Tobias Schnabel", "paper_title": "Prompts as programs: A structure-aware approach to efficient compile-time prompt optimization", "reason": "This paper's approach and ideas on prompt optimization have significant relevance to the survey's focus on LLM-based optimization.  The framing of prompts as programs directly aligns with the analogy used in the survey, linking optimization to program analysis.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Denny Zhou", "paper_title": "Least-to-most prompting enables complex reasoning in large language models", "reason": "The \"least-to-most\" prompting strategy introduced in this paper is directly relevant to the optimization techniques discussed in the survey. This method is particularly relevant to refining the prompting strategies for improved LLM performance and better control over the optimization process.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Reid Pryzant", "paper_title": "Automatic prompt optimization with \"gradient descent\" and beam search", "reason": "This paper's approach to automatic prompt optimization directly relates to the challenges and solutions presented in the survey.  The use of gradient descent (albeit in a symbolic way) and beam search shows a systematic method for improving prompts, and thus the optimization process.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Luyu Gao", "paper_title": "PAL: program-aided language models", "reason": "This work's focus on using program-aided language models (PAL) for reasoning aligns strongly with the survey's discussion of LLM-based optimization. The structured approach of PAL directly relates to the concepts of program analysis and offers a systematic way to manage complexity in optimization.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the math dataset", "reason": "The dataset and benchmarks presented in this paper are directly relevant to the applications of LLM-based optimization shown in the survey.  The use of the math dataset provides a standardized way to evaluate the performance of systems involved in solving mathematical problems.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Shunyu Yao", "paper_title": "React: Synergizing reasoning and acting in language models", "reason": "This paper provides a foundation for understanding and optimizing the interaction between LLMs and the environment, directly relevant to the challenges in sequential decision-making covered in the survey. The synergistic approach to reasoning and acting provides a framework for better control and optimization.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zhiheng Xi", "paper_title": "The rise and potential of large language model based agents: A survey", "reason": "While not a direct optimization paper, this survey provides a helpful overview of the broader context of LLM agents, helping to contextualize and strengthen the value and scope of the current survey's focus on LLM-based optimization.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Haotong Zhao", "paper_title": "Empowering large language model agents through action learning", "reason": "This work directly addresses the challenges of optimizing the actions of LLM agents, directly relating to the applications of LLM-based optimization described in the survey.  The focus on action learning provides an approach to improve the effectiveness of agents in complex tasks.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Andrew Zhao", "paper_title": "Expel: LLM agents are experiential learners", "reason": "This paper addresses the challenge of optimizing LLM agents within complex environments, a crucial aspect of the applications of LLM-based optimization discussed in the survey.  Its methodology addresses how the LLM agents can learn from their experiences to improve their performance.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Chengrun Yang", "paper_title": "Large language models as optimizers", "reason": "This is one of the most directly relevant papers to the core theme of the survey.  It explores the use of LLMs as optimizers for other systems, which is the central idea explored in the survey. It provides a conceptual and methodological foundation for the entire work.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Hunter Lightman", "paper_title": "Let's verify step by step", "reason": "This paper directly addresses the safety concerns associated with LLMs, a key discussion point in the survey.  The focus on step-by-step verification and feedback is directly relevant to ensuring the safe and reliable application of LLM-based optimization.", "section_number": 5}]}