{"references": [{" publication_date": "2024", "fullname_first_author": "LangChain AI", "paper_title": "Langchain studio: A platform for building language model applications", "reason": "This paper is foundational as Langchain is a prominent framework used in building and experimenting with various LLM applications.  Its use in the context of compound AI systems makes it critical for reproducibility and practical application of the concepts discussed in the survey.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Lewis", "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks", "reason": "This paper introduces Retrieval Augmented Generation (RAG), a crucial component of various compound AI systems. Understanding RAG is essential to grasping the functionalities and potential of many systems discussed in the survey.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Gao", "paper_title": "PAL: program-aided language models", "reason": "This paper presents a significant advancement in compound AI system design by integrating program-aided models, enhancing reasoning and problem-solving capabilities, and providing a foundation for understanding the advanced techniques presented in the survey.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Trivedi", "paper_title": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions", "reason": "This work significantly contributes to the understanding of multi-hop reasoning in RAG systems, which directly impacts the design of many compound AI systems discussed in the survey.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Chen", "paper_title": "Magicore: Multi-agent, iterative, coarse-to-fine refinement for reasoning", "reason": "This paper introduces a novel multi-agent reasoning approach that is directly relevant to the optimization and operation of compound AI systems explored in the survey.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yao", "paper_title": "Tree of thoughts: Deliberate problem solving with large language models", "reason": "This research introduces the concept of \"Tree of Thoughts,\" a prominent approach for improving the efficiency and effectiveness of LLM-based reasoning, making it critical for understanding many optimization strategies presented in the survey.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zhou", "paper_title": "Least-to-most prompting enables complex reasoning in large language models", "reason": "This paper significantly contributes to improving LLM reasoning abilities, which is crucial for the effective optimization of compound AI systems, as discussed in the survey.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Cheng", "paper_title": "Trace is the new autodiff - unlocking efficient optimization of computational workflows", "reason": "This work presents a novel approach to optimizing compound AI systems using a trace propagation method, which is directly relevant to the optimization strategies described in the survey.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Zhou", "paper_title": "Self-refine: Iterative refinement with self-feedback", "reason": "This paper introduces a self-refinement method relevant to the iterative optimization approaches discussed in the survey. The self-feedback mechanism helps to enhance the performance and efficiency of LLM optimizers.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Pryzant", "paper_title": "Automatic prompt optimization with gradient descent and beam search", "reason": "This paper provides a practical implementation for prompt optimization using gradient descent.  This is crucial for understanding the process of parameter tuning in LLM-based optimizers which the survey discusses.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Wang", "paper_title": "Promptagent: Strategic planning with language models enables expert-level prompt optimization", "reason": "This paper provides a comprehensive approach to prompt optimization through strategic planning, enhancing the overall performance and capabilities of LLM agents.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yang", "paper_title": "Large language models as optimizers", "reason": "This paper introduces a pivotal approach in optimizing compound AI systems using LLMs as optimizers, making it a fundamental work for understanding the core concepts of the survey.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Zhang", "paper_title": "Offline training of language model agents with functions as learnable weights", "reason": "This paper introduces a novel offline training method for LLM agents, making it relevant to the optimization techniques used in compound AI systems and discussed in the survey.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Lu", "paper_title": "Measuring mathematical problem solving with the math dataset", "reason": "This paper presents a significant contribution to the field of mathematical reasoning, directly influencing the understanding of advanced applications of compound AI systems and their optimization in the survey.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zhang", "paper_title": "AgentPoison: Red-teaming LLM agents via poisoning memory or knowledge bases", "reason": "This work highlights important security considerations and vulnerabilities in LLM agents that the survey addresses when discussing broader implications of the technology.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zhao", "paper_title": "Expel: LLM agents are experiential learners", "reason": "This paper's focus on experiential learning is directly relevant to optimizing compound AI systems.  The approach's contribution to improving the efficiency of LLM agents is significant.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Yao", "paper_title": "React: Synergizing reasoning and acting in language models", "reason": "This paper presents the REACT framework, a foundational concept in designing LLM-based agents.  Its impact on the overall design and optimization strategies explored in the survey is significant.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Chen", "paper_title": "Are more llm calls all you need? Towards scaling laws of compound inference systems", "reason": "This research delves into the scaling laws of compound AI systems which is crucial to understand the efficiency and scalability of optimization methods.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Cheng", "paper_title": "Large language models as optimizers", "reason": "This research directly addresses the optimization of compound AI systems using LLMs as optimizers, providing fundamental insights and techniques discussed in the survey.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "McAleese", "paper_title": "LLM critics help catch LLM bugs", "reason": "This research directly addresses the critical issue of debugging and improving the reliability of LLMs which impacts both the design and optimization of compound AI systems, as covered in the survey.", "section_number": 5}]}