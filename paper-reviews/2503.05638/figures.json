[{"figure_path": "https://arxiv.org/html/2503.05638/x2.png", "caption": "Figure 1: \nWe present TrajectoryCrafter, a novel approach to redirect camera trajectories for monocular videos, achieving precise control over the view transformations and coherent 4D content generation.\nPlease refer to the supplementary project page for video results.", "description": "This figure showcases TrajectoryCrafter, a new method for altering the camera path in monocular videos.  It demonstrates the system's ability to precisely control the camera's viewpoint and simultaneously generate consistent and coherent 4D content, ensuring smooth and realistic-looking video transitions and scene changes.  The results shown are still images; to view the full video results, please refer to the project's supplementary materials.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.05638/x3.png", "caption": "Figure 2: Overview of TrajectoryCrafter.\nStarting with a source video, whether casually captured or AI-generated, we first lift it into a dynamic point cloud via depth estimation. Users can then interactively render the point cloud with desired camera trajectories. Finally, the point cloud renders and the source video are jointly processed by our dual-stream conditional video diffusion model, yielding a high-fidelity video that precisely aligns with the specified trajectory and remains 4D consistent with the source video.", "description": "TrajectoryCrafter processes a source video (either recorded or AI-generated) by first converting it into a dynamic point cloud using depth estimation. This point cloud allows users to interactively define new camera trajectories.  The system then uses a dual-stream conditional video diffusion model that combines the point cloud renderings with the original video to create a high-fidelity output video. This output video precisely matches the specified camera path while maintaining temporal consistency (4D consistency) with the source video.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.05638/x4.png", "caption": "Figure 3: Ref-DiT Block. The text and view tokens are first processed through 3D attention, followed by a cross-attention that injects the detailed, yet mis-aligned, reference information into the view tokens, yielding refined view tokens for subsequent layers.", "description": "The Ref-DiT (Reference-conditioned Diffusion Transformer) block refines view tokens by incorporating information from reference tokens.  The process begins with 3D attention applied to both text and view tokens, capturing contextual relationships within the video data and text descriptions.  Then, a cross-attention mechanism injects detailed information from the reference tokens (encoding the source video) into the view tokens. Because the source video and rendered views aren't perfectly aligned spatially, the cross-attention selectively integrates relevant details, improving the accuracy and coherence of the view tokens for subsequent processing steps in the video generation model. This refined information ensures that the generated novel view is both geometrically accurate and visually consistent with the source video.", "section": "3.3 Dual-stream Conditional Video Diffusion"}, {"figure_path": "https://arxiv.org/html/2503.05638/x5.png", "caption": "Figure 4: Double-reprojection. Given a target video, we lift it into a dynamic point cloud to render a novel view \ud835\udc70\u2032superscript\ud835\udc70\u2032\\bm{I}^{\\prime}bold_italic_I start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT via a random view transformation. Then \ud835\udc70\u2032superscript\ud835\udc70\u2032\\bm{I}^{\\prime}bold_italic_I start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is reprojected to the original camera pose, yielding \ud835\udc70\u2032\u2032superscript\ud835\udc70\u2032\u2032\\bm{I}^{\\prime\\prime}bold_italic_I start_POSTSUPERSCRIPT \u2032 \u2032 end_POSTSUPERSCRIPT through the inverse view transformation. \ud835\udc70\u2032\u2032superscript\ud835\udc70\u2032\u2032\\bm{I}^{\\prime\\prime}bold_italic_I start_POSTSUPERSCRIPT \u2032 \u2032 end_POSTSUPERSCRIPT contains occlusions and aligns with the target video, simulating the point cloud renders.", "description": "The figure illustrates the double-reprojection technique used to create training data.  A target video is first converted into a dynamic point cloud representation. A novel view (I') is then rendered from this point cloud using a random view transformation. This novel view is then reprojected back to the original camera's perspective, resulting in a new image (I'') that includes occlusions and aligns with the target video. This I'' image serves as a simulated point cloud render, mimicking the effects of occlusions and view transformations that occur in real-world scenarios.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.05638/x6.png", "caption": "Figure 5: Qualitative comparison of novel trajectory video synthesis.\nWe compare our method with both reconstruction-based method, Shape-of-motion\u00a0[78], and generative methods, GCD\u00a0[75] and ViewCrafter\u00a0[95] on the multi-view dataset, iphone\u00a0[23].", "description": "Figure 5 presents a qualitative comparison of novel trajectory video synthesis methods.  It showcases the results of four different approaches applied to the same video sequence from the iPhone [23] multi-view dataset. These methods include our proposed TrajectoryCrafter, the reconstruction-based method Shape-of-motion [78], and the generative methods GCD [75] and ViewCrafter [95]. The figure allows for visual comparison of the generated videos' quality, accuracy in portraying camera movements, and overall visual fidelity to the source video, highlighting the strengths and weaknesses of each approach.", "section": "4.2 Evaluation on Multi-view Video Benchmark"}, {"figure_path": "https://arxiv.org/html/2503.05638/x7.png", "caption": "Figure 6: \nQualitative comparison on in-the-wild monocular videos.\nWe show results of redirecting the camera trajectory as \u201czoom-in and orbit to the right\u201d from the input videos, produced by our method and the generative baselines, GCD\u00a0[75] and ViewCrafter\u00a0[95].", "description": "Figure 6 presents a qualitative comparison of novel trajectory video synthesis results on real-world monocular videos.  The experiment focuses on the specific camera trajectory of 'zooming in and orbiting to the right'.  The figure displays the input video and the output videos generated by three different methods: the authors' proposed TrajectoryCrafter, GCD [75], and ViewCrafter [95]. This allows for a visual comparison of the quality and accuracy of the trajectory redirection achieved by each method, highlighting strengths and weaknesses in generating realistic and coherent videos with novel camera paths.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.05638/x8.png", "caption": "Figure 7: \nEffectiveness of Ref-DiT blocks.\nWe compare our full model (w/ Ref-DiT) to two alternatives: a baseline without Ref-DiT (w/o Ref-DiT), and a variant that directly concatenates the source video with the point cloud renders (w/ Concat Condition).\nThe yellow box highlights the most prominent differences.", "description": "Figure 7 demonstrates the impact of the Ref-DiT (Reference-conditioned Diffusion Transformer) block on video generation quality within the TrajectoryCrafter model.  Three versions of the model are compared: the complete model with Ref-DiT blocks, a model without Ref-DiT blocks, and a model where the source video is directly concatenated with point cloud renders instead of using Ref-DiT. A yellow box highlights key visual differences between the outputs, showing how the Ref-DiT block improves the coherence and fidelity of the generated video by effectively integrating information from both the point cloud renders and the source video.", "section": "3.3 Dual-stream Conditional Video Diffusion"}, {"figure_path": "https://arxiv.org/html/2503.05638/x9.png", "caption": "Figure 8: \nAblation on the training data.\nWe compare our model trained with mixed data to two alternatives: training without multi-view data and training without dynamic data.\nThe yellow box highlights the most prominent differences of occulusions, geometric distortions, and motion consistency.", "description": "Figure 8 demonstrates the impact of different training data on the model's performance.  It compares the model trained on a combination of static multi-view and dynamic monocular videos (mixed data) against two alternatives: a model trained only on static multi-view data, and a model trained only on dynamic monocular data. The yellow boxes highlight key differences in the results, particularly demonstrating how using both static multi-view and dynamic monocular data improves the model's ability to handle occlusions, geometric distortions, and maintain motion consistency in the generated videos.", "section": "4. Experiments"}]