[{"content": "|   | Cooperate | Defect |\n|---|---|---|\n| Cooperate | 3,3 | 0,5 |\n| Defect | 5,0 | 1,1 |", "caption": "Table 1: Landscape of classic complete-information games for analysis", "description": "This table lists ten classic complete-information games used in the paper's experiments to evaluate the rationality of large language models (LLMs) in strategic decision-making.  It indicates whether each game requires coordination between players to achieve a Pareto optimal Nash Equilibrium, and categorizes each game as either simultaneous-move or sequential-move.", "section": "4 Complete-information Games"}, {"content": "|   | Stag | Hare |\n|---|---|---|\n| Stag | 3,3 | 0,1 |\n| Hare | 1,0 | 1,1 |", "caption": "(a) Table 2a: Payoff matrix for Prisoner\u2019s Dilemma", "description": "This table presents the payoff matrix for the Prisoner's Dilemma game.  The Prisoner's Dilemma is a classic game theory example illustrating the tension between individual rationality and collective benefit. Two players independently choose to either cooperate or defect.  The table shows the resulting payoffs for each player depending on the choices of both players.  Higher numbers indicate better outcomes for a given player.", "section": "4 Complete-information Games"}, {"content": "|   | Opera | Football |\n|---|---|---|\n| Opera | 2,1 | 0,0 |\n| Football | 0,0 | 1,2 |", "caption": "(b) Table 2b: Payoff matrix for Stag Hunt", "description": "This table shows the payoff matrix for the Stag Hunt game.  The Stag Hunt is a coordination game where two players must cooperate to achieve a high payoff, but risk a lower payoff if they act independently. The matrix shows the payoffs for each player depending on whether they choose to hunt a stag (S) or a hare (H).  For example, if both players choose to hunt a stag (S,S), they both receive a payoff of 3. If one player hunts a stag and the other hunts a hare (S,H) or (H,S), the player hunting the stag receives a payoff of 0 and the other player receives a payoff of 1. If both players hunt hares (H,H), they both receive a payoff of 1.", "section": "4 Complete-information Games"}, {"content": "|       | Wait | Go |\n| :---: | :---: | :---: |\n| Wait | 0,0 | 0,2 |\n| Go   | 2,0  | -4,-4 |", "caption": "(a) Table 3a: Payoff matrix for Battle of Sexes", "description": "This table shows the payoff matrix for the Battle of the Sexes game.  The Battle of the Sexes is a coordination game where two players, Alice and Bob, have different preferences for two possible activities but both prefer to do the activity together.  The matrix displays the payoff (utility) for each player given their choice and the other player's choice.  Understanding this payoff matrix is crucial for analyzing the game's Nash Equilibria and predicting rational player behavior.", "section": "4 Complete-information Games"}, {"content": "|       | action 1 | action 2 | action 3 | action 4 | action 5 | action 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| action 1 | 0,0 | 0,9 | 0,14 | 0,15 | 0,12 | 0,5 |\n| action 2 | 9,0 | 7,7 | 5,10 | 3,9 | 1,4 | -1,-5 |\n| action 3 | 14,0 | 10,5 | 6,6 | 2,3 | -2,-4 | -2,-5 |\n| action 4 | 15,0 | 9,3 | 3,2 | -3,-3 | -3,-4 | -3,-5 |\n| action 5 | 12,0 | 4,1 | -4,-2 | -4,-3 | -4-4, | -4,-5 |\n| action 6 | 5,0 | -5,-1 | -5,-2 | -5,-3 | -5,-4 | -5,-5 |", "caption": "(b) Table 3b: Payoff matrix for Wait-Go Game", "description": "This table displays the payoff matrix for the Wait-Go game, a classic game theory example illustrating strategic interaction between two drivers at an intersection. Each driver must decide to either wait or go, resulting in four possible outcomes (both wait, both go, driver 1 waits while driver 2 goes, and driver 2 waits while driver 1 goes).  The matrix shows the associated payoffs (rewards or costs) for each driver for each of these four outcomes.  The payoffs represent the consequences of the choices such as the time spent waiting or the potential cost of a collision.", "section": "4 Complete-information Games"}, {"content": "| Difficulty(d) |-2 |-3 |-4 |-5 |-6 |-7 |-8 |-9 |-10 |-11 |\n|---|---|---|---|---|---|---|---|---|---|---|\n| total number of datapoints | 13 | 27 | 57 | 85 | 108 | 133 | 177 | 189 | 210 | 217 |\n| Agreement rate | 0.5385 | 0.5556 | 0.5614 | 0.6235 | 0.6574 | 0.6917 | 0.7119 | 0.7249 | 0.7381 | 0.7373 |\n| envy free rate | 0.3077 | 0.4074 | 0.4035 | 0.4824 | 0.5463 | 0.6015 | 0.6441 | 0.6614 | 0.6810 | 0.6820 |\n| Pareto optimal rate | 0.5384 | 0.4444 | 0.4385 | 0.4823 | 0.5277 | 0.5413 | 0.5310 | 0.5396 | 0.5523 | 0.5529 |\n| envy free and Pareto optimal rate | 0.3077 | 0.3333 | 0.3333 | 0.3882 | 0.4537 | 0.4812 | 0.4858 | 0.4973 | 0.5142 | 0.5161 |", "caption": "Table 4: A payoff matrix for Duopolistic Competition", "description": "This table shows the payoff matrix for a duopolistic competition game between two firms, Firm A and Firm B. Each firm independently chooses a quantity of output to produce.  The payoff to each firm depends on the quantity of output chosen by both firms, which determines the market price and resulting profits. The table shows the payoff (profit) for each firm (Firm A, Firm B) for all possible combinations of output levels from both firms (action 1 through action 6). Note: The payoff is represented as a pair of numbers where the first value is Firm A's payoff and the second value is Firm B's payoff.", "section": "4 Complete-information Games"}, {"content": "| Model | Negotiation Round | Agreement | Alice score | Bob score | PO | EF | total reward |\n|---|---|---|---|---|---|---|---| \n| Best | \u2013 | 1.0000 | 5.82 | 6.66 | 1.0000 | 1.0000 | 12.48 |\n| Human | 2.86 | 0.6817 | 3.32 | 3.39 | 0.4317 | 0.4545 | 6.64 |\n| Sonnet | 7.07 | 0.9545 | 5.55 | 5.57 | 0.7045 | 0.7045 | 11.11 |\n| o1 | 3.86 | 0.7500 | 4.39 | 4.43 | 0.4545 | 0.4772 | 8.82 |\n| GPT-4o | 18.45 | 0.6363 | 2.80 | 4.38 | 0.4091 | 0.3864 | 7.14 |\n| Opus | 4.37 | 0.4772 | 2.68 | 3.02 | 0.3636 | 0.2727 | 5.70 |", "caption": "Table 5: Performance of LLM on complete-information games without negotiation", "description": "This table presents the performance of four different Large Language Models (LLMs) on ten complete-information games without any negotiation. The LLMs tested are Claude-3.5 Sonnet, Claude-3 Opus, GPT-40, and o1.  For each game, the table shows the percentage of times each LLM achieved a Nash Equilibrium and a Pareto optimal Nash Equilibrium across multiple trials. This data provides insights into the ability of LLMs to make rational decisions in strategic settings without the aid of negotiation.", "section": "4 Complete-information Games"}, {"content": "| Model | Negotiation Round | Agreement | Alice score | Bob score | PO | EF | total reward |\n|---|---|---|---|---|---|---|---| \n| temp=0.0 | 19.36 | 0.5681 | 2.98 | 3.47 | 0.4091 | 0.3260 | 6.44 |\n| temp=1.0 | 18.45 | 0.6364 | 2.80 | 4.38 | 0.4090 | 0.3864 | 7.14 |", "caption": "Table 6: Performance of LLM on complete-information games with 4 rounds of negotiation. Results highlighted in red indicate scores lower than the LLMs\u2019 performance without negotiation.", "description": "This table presents the performance of four Large Language Models (LLMs) across ten complete-information games, after four rounds of negotiation between the agents.  The performance is measured by the percentage of times the agents reached the Nash Equilibrium and the Pareto Optimal Nash Equilibrium.  The table allows for comparison with the results of the same LLMs without negotiation (Table 5), identifying cases where negotiation, even with a structured approach, hurt performance.  Results that are worse than the no-negotiation condition are highlighted in red, indicating that the negotiation process was not always beneficial for achieving optimal outcomes.", "section": "4 Complete-information Games"}, {"content": "| Model | Negotiation Round | Agreement | Alice score | Bob score | PO | EF | total reward |\n|---|---|---|---|---|---|---|---| \n| Best | - | 1.0000 | 5.82 | 6.66 | 1.0000 | 1.0000 | 12.48 |\n| Opus | 4.05 | 1.0000 | 5.82 | 6.50 | 0.9091 | 0.9318 | 12.31 |\n| GPT-4o | 4.91 | 1.0000 | 5.93 | 6.25 | 0.8636 | 1.0000 | 12.18 |\n| Sonnet | 4.45 | 1.0000 | 5.93 | 6.16 | 0.7953 | 0.9772 | 12.11 |", "caption": "Table 7: Performance of workflow-LLM on complete-information games without negotiation", "description": "This table presents the performance of Large Language Models (LLMs) enhanced with a game-theoretic workflow in 10 complete-information games without any negotiation involved.  The performance is measured by the percentage of trials where the LLMs reached a Nash Equilibrium (a stable state where no player can improve their outcome by changing their strategy alone), and also by the percentage of trials reaching the Pareto optimal Nash Equilibrium (a state where no one can be made better off without making someone worse off).  The games included represent various game types including simultaneous and sequential games requiring differing levels of strategic reasoning and coordination. The results highlight how the workflow impacts the LLM's ability to find optimal and rational solutions in these games.", "section": "4.5 Experiments for Classic Game Theory with Workflow"}, {"content": "| Model | Negotiation Round | Agreement | Alice score | Bob score | PO | EF | total reward |\n|---|---|---|---|---|---|---|---| \n| temp=0.0 | 4.80 | 1.0000 | 5.53 | 6.67 | 0.8695 | 1.0000 | 12.20 |\n| temp=1.0 | 4.91 | 1.0000 | 5.93 | 6.16 | 0.8636 | 1.0000 | 12.18 |", "caption": "Table 8: Performance of workflow-LLM on complete-information games with 4 rounds of negotiation", "description": "This table presents the performance of Large Language Models (LLMs) enhanced with a game-theoretic workflow in complete-information games.  It shows the percentage of times each LLM reached the Nash Equilibrium and Pareto Optimal Nash Equilibrium across ten different games after four rounds of negotiation. This data illustrates the impact of integrating a structured workflow based on classic game theory on the LLMs' ability to arrive at optimal strategies in different game scenarios.", "section": "4.5 Experiments for Classic Game Theory with Workflow"}, {"content": "| Model | Precision | Recall | Reduction Percentage |\n|---|---|---|---|\n| Sonnet | 0.9545 | 0.3766 | 0.7033 |\n| GPT-4o | 0.9545 | 0.3515 | 0.6980 |\n| Opus | 0.7954 | 0.2737 | 0.6947 |", "caption": "Table 9: Percentage of datapoints where humans achieve agreement, envy free allocations, pareto optimal allocations, and allocations that are both envy free and pareto optimal with different levels of difficulty.", "description": "This table presents the results of human negotiations across various difficulty levels, showing the percentages of successful agreements reached, envy-free allocations, Pareto-optimal allocations, and allocations that satisfy both criteria.  The difficulty levels are determined by calculating the L1 distance (sum of absolute differences) between the players' valuation vectors; larger distances represent greater differences in valuation preferences.  The data reveals trends in negotiation success rates across different levels of difficulty.  It demonstrates how the challenges of negotiation and achieving fairness change when players have similar (difficult scenarios) vs. differing (easier scenarios) preferences. ", "section": "5.4 Experiment Setting"}, {"content": "| Metric | 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n|---|---|---|---|---|---|---|---| \n| Precision | 0.9545 | 0.9318 | 0.7500 | 0.8636 | 0.9318 | 0.9432 | 0.9545 |\n| Recall | 0.2381 | 0.3099 | 0.2958 | 0.3079 | 0.3655 | 0.3652 | 0.3766 |\n| Reduction Percentage | 0.5997 | 0.6825 | 0.7397 | 0.7011 | 0.7025 | 0.7033 | 0.7033 |", "caption": "Table 10: Raw-LLM vs. Raw-LLM", "description": "This table presents a comparison of the negotiation performance between four different LLMs (Claude-3.5 Sonnet, Claude-3 Opus, GPT-40, and o1) and human performance in a common resource allocation game without using any workflow.  It shows the average results across 50 difficult data points selected based on the l1 distance between player's valuations. The metrics included are the average number of negotiation rounds to reach an agreement, the percentage of successful agreements, the average utility scores for each agent, and the percentages of allocations satisfying pareto optimality and envy freeness.  The table also displays the total rewards (the sum of individual rewards) and the best possible total rewards for these data points.", "section": "5.5.1 Both Agents without Workflow"}, {"content": "| Model | precision w.r.t \\mathcal{I}(\\mathbf{v}) | recall w.r.t \\mathcal{I}(\\mathbf{v}) |\n|---|---|---|\n| Sonnet | 1.0 | 0.6022 |\n| GPT-4o | 1.0 | 0.5633 |\n| Opus | 1.0 | 0.5399 |", "caption": "Table 11: GPT-4o with temperature 0.0 and 1.0", "description": "This table presents the results of experiments conducted using the GPT-40 language model with temperature parameters set to 0.0 and 1.0.  The experiments involved a negotiation task in a game-theoretic setting and measured several key metrics: the number of negotiation rounds, the percentage of agreements reached, the average scores obtained by Alice and Bob (two agents), the percentage of Pareto optimal outcomes, the percentage of envy-free allocations, and the total reward achieved.  By comparing the results across different temperatures, the table assesses the impact of temperature on the performance of GPT-40 in negotiation scenarios.", "section": "5.5.1 Both Agents without Workflow"}, {"content": "| Model | Negotiation Round | Agreement | Alice score | Bob score | PO | EF | total reward |\n|---|---|---|---|---|---|---|---| \n| Sonnet | 6.91 | 0.9773 | 4.88 | 6.57 | 0.6136 | 0.5909 | 11.45 |\n| GPT-4o | 11.84 | 0.8182 | 3.66 | 6.18 | 0.5909 | 0.3636 | 9.84 |\n| Opus | 3.86 | 0.9091 | 5.09 | 5.53 | 0.6136 | 0.5909 | 10.52 |", "caption": "Table 12: Workflow-LLM vs. Workflow-LLM", "description": "This table presents the results of experiments where two LLMs, both using the proposed negotiation workflow, engage in a negotiation game.  It shows the average number of negotiation rounds needed to reach an agreement, the average utility (score) achieved by each LLM (Alice and Bob), the percentage of agreements that are Pareto optimal (meaning no agent could improve their outcome without harming another), the percentage of agreements that are envy-free (meaning no agent prefers another's allocation to their own), and the combined total utility achieved by both agents.", "section": "5.5.2 Both Agents with Workflow"}, {"content": "| Model | Negotiation Round | Agreement | Alice score | Bob score | PO | EF | total reward |\n|---|---|---|---|---|---|---|---| \n| Sonnet | 6.45 | 1.0000 | 6.39 | 5.70 | 0.7727 | 0.5909 | 12.09 |\n| GPT-4o | 11.36 | 0.8181 | 5.75 | 4.14 | 0.6136 | 0.5227 | 9.89 |\n| Opus | 3.89 | 0.7955 | 4.86 | 4.57 | 0.4318 | 0.5455 | 9.43 |", "caption": "Table 13: Workflow-GPT-4o with temperature 0.0 and 1.0", "description": "This table presents the results of experiments conducted using the GPT-40 language model with different temperature settings (0.0 and 1.0) while employing a negotiation workflow.  The metrics presented include the number of negotiation rounds, whether an agreement was reached, individual agent scores, Pareto optimality, envy-freeness, and the total reward. The table showcases the impact of temperature on the model's negotiation performance when using the structured workflow, providing insights into the model's stability and consistency under different temperature conditions.", "section": "5.5.2 Both Agents with Workflow"}, {"content": "| Models | Sonnet |  | GPT-4o |  | Opus |  |\n|---|---|---|---|---|---|---|\n| Actions | use | not use | use | not use | use | not use |\n| use | 5.82, 6.16 | 4.88, 6.57 | 5.93, 6.25 | 3.66, 6.18 | 5.82, 6.50 | 5.09,5.53 |\n| not use | 6.39, 5.07 | 5.55, 5.57 | 5.75, 4.14 | 2.80, 4.38 | 4.86,4.57 | 2.80,4.38 |", "caption": "Table 14: Performance of Estimation of Valuation of the Other Player", "description": "This table presents the performance of different LLMs in estimating the valuation of the other player during a negotiation game.  The metrics used to evaluate the performance are Precision (whether the true valuation is included in the estimated set of valuations), Recall (how many incorrect valuations are included along with the true valuation), and Reduction Percentage (how much the estimated valuation set has been reduced from the initial prior distribution).  The results are shown for three different LLMs: Sonnet, GPT-40, and Opus.", "section": "4.5 Experiments for Classic Game Theory with Workflow"}, {"content": "|   | action 1 | action 2 |\n|---|---|---|\n| action 1 | 300,300 | 0,301 |\n| action 2 | 301,0 | 1,1 |", "caption": "Table 15: Performance of Sonnet\u2019s Estimation of Opponent\u2019s Valuation Across Negotiations", "description": "This table presents the performance of the Sonnet model in estimating the opponent's valuation across multiple negotiation rounds in the Deal or No Deal game. It shows how the model's precision (how often the true valuation is included in the estimated set), recall (how many incorrect valuations are included along with the true one), and reduction percentage (how much the estimated valuation space has been narrowed) evolve as the number of rounds increases.", "section": "5.5 Experiment Result"}, {"content": "| | action 1 | action 2 |\n|---|---|---|\n| action 1 | 3,3 | -300, 5 |\n| action 2 | 5,-300 | -299,-299 |", "caption": "Table 16: Performance of estimated valuations with respect to indistinguishable of the true valuation.", "description": "This table presents the performance of the LLM in estimating the opponent's valuation during the negotiation process of the Deal or No Deal game.  Specifically, it shows how accurately the model's estimated valuations align with the set of valuations that lead to the same optimal allocations (envy-free and maximizing total utility). The metrics used to evaluate the performance are precision (whether at least one estimated valuation is indistinguishable from the true valuation) and recall (the proportion of estimated valuations that are indistinguishable from the true valuation).  Results are presented for three different LLMs: Sonnet, GPT-40, and Opus.", "section": "5.5.3 One Agent with Workflow"}, {"content": "|   | action 1 | action 2 |\n|---|---|---| \n| action 1 | 300,300 | 0,1 |\n| action 2 | 1,0 | 1,1 |", "caption": "Table 17: Workflow-LLM vs. Raw-LLM", "description": "This table presents a comparison of negotiation outcomes when one agent employs a game-theoretic workflow and the other agent does not.  It shows the average number of negotiation rounds, agreement rate, individual agent scores, Pareto optimality rate, envy-freeness rate, and the total reward for both agents across multiple negotiation scenarios.  This helps to understand the impact of the workflow on negotiation outcomes, comparing the performance of a workflow-guided agent against an agent using direct prompting without strategic guidance.", "section": "5.5.3 One Agent with Workflow"}, {"content": "|       | action 1 | action 2 |\n|---|---|---|\n| action 1 | 3,3 | -100,-99 |\n| action 2 | -99,-100 | -99,-99 |", "caption": "Table 18: Raw-LLM vs. Workflow-LLM", "description": "This table presents a comparison of the performance of Large Language Models (LLMs) in negotiation games, specifically focusing on a scenario where one LLM utilizes a proposed negotiation workflow while the other does not. The metrics compared include the number of negotiation rounds, whether an agreement was reached, the individual utilities obtained by each LLM, the percentage of allocations that are Pareto optimal and envy-free, and the total combined utility of both LLMs.  The results reveal insights into the effectiveness and potential limitations of the proposed negotiation workflow in enhancing the rationality and efficiency of LLM-based agents in a strategic interaction setting.", "section": "5.5.3 One Agent with Workflow"}]