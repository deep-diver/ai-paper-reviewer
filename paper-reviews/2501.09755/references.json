{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is fundamental to many modern large language models and is the foundation of the ViTok's architecture"}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-06-01", "reason": "This paper introduced the Vision Transformer (ViT), which is a key component of the ViTok's architecture and enabled the efficient scaling of the model"}, {"fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "publication_date": "2021-06-01", "reason": "This paper demonstrated the effectiveness of using transformers for high-resolution image generation, which is a related task to the ViTok and provided insights into training large autoencoders"}, {"fullname_first_author": "Diederik P. Kingma", "paper_title": "Auto-encoding variational bayes", "publication_date": "2013-05-01", "reason": "This paper introduced the Variational Autoencoder (VAE), a fundamental framework for autoencoding and the basis of the ViTok model"}, {"fullname_first_author": "Aaron van den Oord", "paper_title": "Neural discrete representation learning", "publication_date": "2017-12-01", "reason": "This paper introduced Vector Quantized Variational Autoencoders (VQ-VAEs), a crucial technique for discrete latent representation which is a related approach to the continuous representation used in ViTok"}]}