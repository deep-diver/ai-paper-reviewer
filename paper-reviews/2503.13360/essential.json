{"importance": "This paper addresses the critical issue of visual forgetting in MLLMs, offering a promising solution with TVC. By enhancing visual grounding, it opens avenues for **more robust and reliable multimodal reasoning**, impacting diverse applications from robotics to medical imaging.", "summary": "TVC mitigates visual forgetting in multimodal LLMs, enhancing reasoning by strategically re-introducing and compressing visual information.", "takeaways": ["Multimodal LLMs (MLLMs) tend to 'forget' visual information during long reasoning chains, relying more on text.", "Take-along Visual Conditioning (TVC) method combats this by strategically re-introducing visual inputs during reasoning.", "TVC achieves state-of-the-art results on mathematical reasoning benchmarks, demonstrating its effectiveness."], "tldr": "Large Language Models combined with visual inputs(MLLMs) struggle to maintain focus on visual information as reasoning progresses, leading to text-over-reliance. This \"visual forgetting\" degrades performance, especially in tasks requiring continuous visual grounding like geometry problems. Analysis shows MLLMs diminish attention to images with increased context length, causing hallucinations and limiting full reasoning potential. \n\nTo tackle this, the paper introduces Take-along Visual Conditioning (TVC). It shifts image input to critical reasoning stages and compresses redundant visual tokens. TVC uses Dynamic Visual Reaffirmation (DVR) for training and Periodic Visual Calibration (PVC) at inference. TVC maintains visual attention throughout reasoning, improving performance on mathematical benchmarks by 3.4%.", "affiliation": "Nanjing University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Reasoning"}, "podcast_path": "2503.13360/podcast.wav"}