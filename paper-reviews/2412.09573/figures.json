[{"figure_path": "https://arxiv.org/html/2412.09573/x1.png", "caption": "Figure 1: Given uncalibrated sparse-view images, our FreeSplatter can reconstruct pixel-wise 3D Gaussians, enabling both high-fidelity novel view rendering and instant camera pose estimation in mere seconds. FreeSplatter can deal with both object-centric (up) and scene-level (down) scenarios.", "description": "This figure demonstrates the FreeSplatter model's ability to reconstruct 3D scenes from a small number of images without prior knowledge of camera positions or internal parameters. The top half shows an object-centric example where the model successfully reconstructs a 3D representation of a toy figure from several input views.  The bottom half shows a scene-level example where a more complex scene is similarly reconstructed.  The model produces pixel-wise 3D Gaussians, allowing for high-fidelity novel view rendering and extremely fast (seconds) camera pose estimation.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2412.09573/x2.png", "caption": "Figure 2: FreeSplatter pipeline. Given input views {\ud835\udc70n\u2223n=1,\u2026,N}conditional-setsuperscript\ud835\udc70\ud835\udc5b\ud835\udc5b1\u2026\ud835\udc41\\left\\{{\\bm{I}}^{n}\\mid n=1,\\ldots,N\\right\\}{ bold_italic_I start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2223 italic_n = 1 , \u2026 , italic_N } without any known camera extrinsics or intrinsics, we first patchify them into image tokens, and then feed all tokens into a sequence of self-attention blocks to exchange information among multiple views. Finally, we decode the output image tokens into N\ud835\udc41Nitalic_N Gaussian maps {\ud835\udc6en\u2223n=1,\u2026,N}conditional-setsuperscript\ud835\udc6e\ud835\udc5b\ud835\udc5b1\u2026\ud835\udc41\\left\\{{\\bm{G}}^{n}\\mid n=1,\\ldots,N\\right\\}{ bold_italic_G start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2223 italic_n = 1 , \u2026 , italic_N }, from which we can render novel views, as well as recovering camera focal length f\ud835\udc53fitalic_f and poses {\ud835\udc77n\u2223n=1,\u2026,N}conditional-setsuperscript\ud835\udc77\ud835\udc5b\ud835\udc5b1\u2026\ud835\udc41\\left\\{{\\bm{P}}^{n}\\mid n=1,\\ldots,N\\right\\}{ bold_italic_P start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2223 italic_n = 1 , \u2026 , italic_N } with simple iterative solvers.", "description": "The FreeSplatter pipeline processes uncalibrated multi-view images to reconstruct a 3D scene.  The input images are first divided into patches and converted into image tokens. These tokens are then processed through a series of self-attention transformer blocks, allowing information exchange between different views. The output from the transformer is then decoded into N Gaussian maps (one for each input image). These Gaussian maps represent the 3D scene as a collection of Gaussian primitives. From these Gaussian maps, novel views of the scene can be rendered, and the camera parameters (focal length and pose) for each input image can be estimated using iterative solvers.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2412.09573/x3.png", "caption": "Figure 3: Object-centric Sparse-view Reconstruction. We show 6 samples from the Google Scanned Objects dataset. To be noted, the results of LGM and InstantMesh (\\nth2 and \\nth3 rows) are generated with ground truth camera poses (and intrinsics), while our results (\\nth4 row) are generated in a completely pose-free manner.", "description": "Figure 3 showcases a comparison of object-centric sparse-view 3D reconstruction methods.  Six examples from the Google Scanned Objects dataset are used.  The top two rows display results from LGM and InstantMesh, respectively, which both leverage ground truth camera poses and intrinsic parameters for reconstruction.  The bottom row shows FreeSplatter's results, demonstrating high-fidelity reconstruction achieved without any prior knowledge of camera poses.", "section": "3 Object-centric Sparse-view Reconstruction"}, {"figure_path": "https://arxiv.org/html/2412.09573/x4.png", "caption": "Figure 4: Scene-level Reconstruction on ScanNet++. The results of pixelSplat and MVSplat are obtained with ground truth input poses, while the results of Splat3R and ours are pose-free.", "description": "This figure compares the scene reconstruction capabilities of different methods on the ScanNet++ dataset.  It shows the results of four methods: pixelSplat, MVSplat, Splat3R, and the proposed FreeSplatter. PixelSplat and MVSplat are presented as baselines that rely on accurate camera pose information (ground truth poses). In contrast, Splat3R and FreeSplatter are both pose-free methods; that is, they don't require ground truth camera poses as input.  The image displays a visual comparison of the scene reconstruction quality produced by each approach.", "section": "4.2 Sparse-View Reconstruction"}, {"figure_path": "https://arxiv.org/html/2412.09573/x5.png", "caption": "Figure 5: Scene-level Reconstruction on CO3Dv2.", "description": "This figure displays a comparison of scene-level 3D reconstruction results on the CO3Dv2 dataset.  It showcases the input image, the results of the FreeSplatter model (rendering of input views with predicted poses), the results of Splatt3R (a comparable pose-free model), and additional novel views rendered by both FreeSplatter and Splatt3R.  The goal is to visually demonstrate FreeSplatter's ability to generate high-quality novel views compared to a leading pose-free competitor, highlighting its performance in a challenging real-world scene reconstruction scenario.", "section": "4.2 Sparse-View Reconstruction"}, {"figure_path": "https://arxiv.org/html/2412.09573/x6.png", "caption": "Figure 6: 3D content creation with FreeSplatter. \\nth1 and \\nth2 rows: Image-to-3D results using Zero123++ (input image, Gaussian visualization, two novel views). \\nth3 row: Text-to-3D results using MVDream (prompt shown above; two Gaussian visualizations, two novel views).", "description": "Figure 6 demonstrates the application of FreeSplatter in 3D content creation.  The top two rows showcase image-to-3D results, utilizing Zero123++, a multi-view image generation model.  For each example, the input image is shown, followed by a visualization of the generated 3D Gaussian representation, and finally two novel views rendered from this representation. The bottom row presents text-to-3D results generated with MVDream, where a text prompt is provided, along with visualizations of the generated Gaussian representation and two novel views.", "section": "4.5 Applications"}, {"figure_path": "https://arxiv.org/html/2412.09573/x7.png", "caption": "Figure 7: Comparison with PF-LRM on its evaluation datasets. The test samples in the first 3 rows are from the GSO evaluation set of PF-LRM, while the samples in the last 4 rows are from the OmniObject3D evaluation set of PF-LRM. Our FreeSplatter-O synthesizes significantly better visual details than PF-LRM.", "description": "This figure compares the performance of FreeSplatter-O and PF-LRM on two datasets: Google Scanned Objects (GSO) and OmniObject3D.  The top three rows showcase results from the GSO dataset, while the bottom four rows display results from the OmniObject3D dataset. Each set of images shows the input image, followed by reconstructions generated by FreeSplatter-O and PF-LRM, and finally the ground truth image. The visual comparison demonstrates that FreeSplatter-O produces significantly finer details and more accurate reconstructions than PF-LRM.", "section": "4.2 Sparse-View Reconstruction"}, {"figure_path": "https://arxiv.org/html/2412.09573/x8.png", "caption": "Figure 8: Comparison on image-to-3D generation with Zero123++ v1.2\u00a0(Shi et\u00a0al., 2023).", "description": "This figure compares the image-to-3D generation results of different methods, including FreeSplatter (ours), LGM, and InstantMesh.  All methods use Zero123++ v1.2 to generate multiple views from a single input image.  The comparison highlights the differences in the quality of 3D models produced by each method, showcasing FreeSplatter's ability to generate clearer and more detail-rich views.", "section": "4.2 Sparse-View Reconstruction"}, {"figure_path": "https://arxiv.org/html/2412.09573/x9.png", "caption": "Figure 9: Comparison on image-to-3D generation with Hunyuan3D Std\u00a0(Yang et\u00a0al., 2024b).", "description": "Figure 9 presents a comparison of image-to-3D generation results using different methods.  It shows the results of generating 3D models from a single input image using three approaches:  Hunyuan3D Std (a state-of-the-art multi-view diffusion model used for generating multi-view images as input), a pose-dependent LRM (Large Reconstruction Model) which needs camera pose information for accurate results, and the proposed FreeSplatter method which is pose-free. The figure illustrates the visual quality of the 3D models generated by each method, comparing the generated novel views against the ground truth. This comparison highlights the performance of FreeSplatter in terms of generating visually accurate and detailed 3D models, especially when compared to pose-dependent methods.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09573/x10.png", "caption": "Figure 10: Use input image to enhance the image-to-3D generation results with Zero123++ v1.1\u00a0(Shi et\u00a0al., 2023). The input image is often more high-quality and contains richer visual details than the generated views, but its camera pose is unknown, making it impossible for pose-dependent LRMs to leverage it. The capability of using the input image alongside generated views of our FreeSplatter is particularly valuable for challenging content like human faces, where Zero123++ often struggles to generate.", "description": "This figure demonstrates how incorporating the input image alongside the generated views improves the image-to-3D generation results.  The input image generally has higher quality and richer visual detail than views generated by a multi-view diffusion model like Zero123++ v1.1.  However, pose-dependent Large Reconstruction Models (LRMs) cannot utilize the input image because its camera pose is unknown. FreeSplatter's ability to use both the input image and the generated views is particularly beneficial when reconstructing challenging subjects, such as human faces, where multi-view diffusion models often struggle.", "section": "4.5 Applications"}, {"figure_path": "https://arxiv.org/html/2412.09573/x11.png", "caption": "Figure 11: Our FreeSplatter can faithfully recover the pre-defined camera poses of existing multi-view diffusion models. We use gray pyramids to visualize the ground truth pre-defined camera poses of the diffusion models, and colorful pyramids to visualize the estimated poses. \u03c6\ud835\udf11\\varphiitalic_\u03c6 and \u03b8\ud835\udf03\\thetaitalic_\u03b8 denote the pre-defined azimuth and elevation angles, respectively. Since Zero123++ v1.2 and Hunyuan3D Std generate images at pre-defined fixed Field-of-View (fov), we mark their pre-defined fov angles and corresponding focal lengths (in pixel units) too.", "description": "This figure demonstrates FreeSplatter's ability to accurately estimate camera poses from images generated by multi-view diffusion models.  It compares the ground truth camera poses (shown as gray pyramids) with the poses estimated by FreeSplatter (colorful pyramids).  The ground truth poses are defined by pre-defined azimuth (\u03c6) and elevation (\u03b8) angles, which are specified in the multi-view diffusion model.  For models that use a fixed field of view (FOV), the FOV angles and corresponding focal lengths (in pixels) are also indicated.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09573/x12.png", "caption": "Figure 12: Zero-shot pose-free reconstruction results on ABO and OmniObject3D. Both datasets are unseen for FreeSplatter-O. Our model can faithfully estimate the input camera poses and render high-fidelity novel views.", "description": "This figure showcases the zero-shot, pose-free 3D reconstruction capabilities of the FreeSplatter-O model on the ABO and OmniObject3D datasets.  Importantly, these datasets were not included in the model's training data.  The results demonstrate that FreeSplatter-O accurately estimates the input camera poses, even without prior knowledge, and generates high-fidelity novel views of the 3D scenes.  This highlights the model's robustness and generalization ability.", "section": "4.2 Sparse-View Reconstruction"}, {"figure_path": "https://arxiv.org/html/2412.09573/x13.png", "caption": "Figure 13: Zero-shot pose-free reconstruction and view synthesis results on RealEstate10K. Our FreeSplatter-S model was not trained on RealEstate10K, we directly utilize it for zero-shot view synthesis on this dataset. We can observe that our model can faithfully reconstruct the input views at the estimated input poses, and the rendered novel views align well with the ground truth.", "description": "This figure demonstrates the zero-shot capabilities of FreeSplatter-S on the RealEstate10K dataset, a dataset it was not trained on.  The results show that the model accurately reconstructs the input views using its estimated camera poses.  More importantly, the novel views generated by the model closely match the ground truth views, highlighting the model's ability to generalize well to unseen data.", "section": "4.2 Sparse-View Reconstruction"}, {"figure_path": "https://arxiv.org/html/2412.09573/x14.png", "caption": "Figure 14: Zero-shot generalization of FreeSplatter-S on various datasets. We show 2 examples for DTU, MVImgNet, Tanks & Temples and Sora-generated videos, respectively.", "description": "This figure demonstrates the zero-shot generalization capabilities of the FreeSplatter-S model on diverse datasets.  It showcases two examples each from four distinct datasets: DTU (a dataset of scanned objects), MVImgNet (multi-view images), Tanks & Temples (large-scale outdoor scenes), and Sora-generated videos. For each dataset, the figure displays the input images, the reconstructed images with predicted poses, and the rendered novel views. This visualization highlights the model's ability to generalize across various scene types, scales, and capture conditions.", "section": "4.2 Sparse-View Reconstruction"}, {"figure_path": "https://arxiv.org/html/2412.09573/x15.png", "caption": "Figure 15: Ablation on pixel-alignment loss. We show two samples from the GSO dataset.", "description": "This figure demonstrates the impact of the pixel-alignment loss on the model's performance. It visually compares the results of the model trained with and without the pixel-alignment loss. The comparison is shown on two examples from the Google Scanned Objects (GSO) dataset. The images reveal significantly better visual detail and clarity in the model trained with the pixel-alignment loss, highlighting its importance in improving the overall quality of the 3D reconstruction.", "section": "4.3 ABLATION STUDIES"}, {"figure_path": "https://arxiv.org/html/2412.09573/x16.png", "caption": "Figure 16: Illustration on the influence of input view number. We show the visual comparison of FreeSplatter-O results with varying numbers of input views (n=1\u22126\ud835\udc5b16n=1-6italic_n = 1 - 6). From left to right: input views, reconstructed Gaussians, and rendered target views at 4 fixed viewpoints. Additional input views increase Gaussian density and improve previously uncovered regions, with diminishing returns beyond n=4 when object coverage becomes sufficient.", "description": "This figure shows an ablation study on the number of input views used in FreeSplatter-O, the object-centric variant of the model.  It demonstrates how the reconstruction quality and completeness improve as more views are added.  The figure displays the input views, the resulting 3D Gaussian representation, and renderings from four fixed viewpoints for each number of input views (from 1 to 6). As the number of input views increases, the density of the Gaussian representation increases, filling in gaps and leading to more detailed and complete 3D reconstructions. However, beyond four input views, the improvements become marginal as the scene is already well-captured.", "section": "4.4 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.09573/x17.png", "caption": "Figure 17: Ablation study on view embedding addition. Red/blue boxes indicate views added with reference/source view embeddings respectively. For each case, we visualize the image rendered with identity camera (i.e., reference pose) on the right.", "description": "This ablation study investigates the effect of adding view embeddings to image tokens within the FreeSplatter model.  The experiment uses four input views (V1, V2, V3, V4).  Different combinations of adding a reference view embedding (eref, red boxes) and a source view embedding (esrc, blue boxes) to the image tokens are tested. The resulting image rendered with an identity camera (reference pose) for each case is shown. This helps to understand how the model utilizes view embeddings to differentiate between the reference view and other views, ultimately affecting the reconstruction quality.", "section": "3.2 MODEL ARCHITECTURE"}]