[{"figure_path": "https://arxiv.org/html/2411.05197/x1.png", "caption": "Figure 1: Overview of hardware and software platform inference (HSPI). HSPI aims to identify the underlying hardware and software platform of deep learning models. Engineered requests are sent to a service provider and responses are collected. With only the responses, HSPI predicts information on the hardware and software supply chains of the service provider.", "description": "The figure illustrates the process of Hardware and Software Platform Inference (HSPI).  A user sends engineered requests to a service provider (A or B), which utilizes a deep learning model hosted on a specific hardware and software platform.  The service provider returns responses to the user.  HSPI analyzes these responses alone, without access to the model or the underlying hardware/software details, to infer the platform used by the provider, revealing the hardware and software supply chain involved.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2411.05197/x2.png", "caption": "Figure 2: Splitting an FP32 logit into three INT32 numbers. In case that rounding noise pollutes the bit distribution in FP32 logits, before training SVMs, for each logit, we extract the sign, exponent, and fraction, zero pad each component and view each as an integer.", "description": "This figure illustrates how a single-precision 32-bit floating-point number (FP32), representing a logit from a neural network, is converted into three 32-bit integer numbers (INT32).  The process involves separating the logit's sign, exponent, and fractional parts. Each part is then zero-padded to ensure it's a full 32 bits, and each is treated as a separate integer. This is done to address the issue of rounding noise that may be present in FP32 numbers. By separating the components, the impact of this rounding noise on the overall bit distribution is reduced. This pre-processing step is performed before training Support Vector Machines (SVMs), likely to improve the accuracy of the classification task.", "section": "4.3 HSPI-LD: HSPI with Logits Distributions"}, {"figure_path": "https://arxiv.org/html/2411.05197/x3.png", "caption": "Figure 3: Generating HSPI-LD samples using LLMs. We guide LLMs to generate random words.\nThe logits are flattened to form an input vector for training hardware platform classifiers.", "description": "This figure illustrates the process of generating samples for Hardware and Software Platform Inference using Logits Distributions (HSPI-LD) with Large Language Models (LLMs).  The process involves prompting an LLM to produce a sequence of random words.  The model's output includes the generated words and their associated logits (the pre-softmax probabilities of each word). These logits are then flattened into a single vector, which serves as the input data for training a classifier. This classifier is designed to distinguish between different hardware and software configurations based on the unique patterns in the logit distributions.", "section": "4.3 HSPI-LD: HSPI with Logits Distributions"}, {"figure_path": "https://arxiv.org/html/2411.05197/x4.png", "caption": "Figure 4: Example border images of MobileNet-v3-Small generated by HSPI-BI. The predicted label changes when fed to the same model quantized to different number formats. The subcaption follows the format of model format : predicted label.", "description": "This figure shows example border images generated using the HSPI-BI method for MobileNet-v3-Small model. Each image is an input that causes the model to predict different classes when the model is quantized to different numerical precision formats (FP16, INT8, MXINT8, BF16, FP8-E3, FP8-E4).  The format of each image's subcaption indicates the quantization format used and the resulting prediction (e.g., 'FP16: FROG' denotes that when using FP16 quantization, the model predicts 'FROG'). This demonstrates how subtle differences in numerical precision due to quantization can lead to different model outputs, making it possible to infer hardware and software platform information based on the model's input-output behavior.", "section": "5.2 White-box Attacks"}, {"figure_path": "https://arxiv.org/html/2411.05197/x5.png", "caption": "Figure 5: Kernel density estimate of logit distributions of different quantization classes on the classification of the same 5000 images for CIFAR10 with ResNet18, i.e., 50000 logits.", "description": "This figure displays kernel density estimates of the logit distributions for seven different quantization methods (FP32, BF16, FP16, MXINT8, FP8-E3, FP8-E4, INT8).  Each distribution represents the logits obtained when classifying the same 5000 images from the CIFAR-10 dataset using a ResNet18 model. This results in a total of 50000 logits across all the quantization methods. The purpose of the figure is to visually illustrate the subtle but distinct differences in logit distributions introduced by various quantization techniques, which forms the basis for Hardware and Software Platform Inference (HSPI) in the paper.  The visual comparison shows how these differing distributions can be used to differentiate between various hardware and software configurations.", "section": "5.2 White-box Attacks"}, {"figure_path": "https://arxiv.org/html/2411.05197/x6.png", "caption": "Figure 6: A histogram showing the difference in logit bit distribution for the classification of the same 5000 images for CIFAR10 with ResNet18, i.e., 50000 logits, between Nvidia Quadro RTX 8000 and NVIDIA A100.", "description": "Figure 6 presents a comparative analysis of logit bit distribution between two NVIDIA GPUs: the Quadro RTX 8000 and the A100.  The analysis is based on the classification of 5000 images from the CIFAR-10 dataset using the ResNet18 model, resulting in a total of 50,000 logits. The histogram visualizes the differences in the distribution of bits across the logit values, highlighting how the two GPUs process and represent numerical data differently. This difference in bit distribution is a key aspect of the paper's method for inferring hardware and software platform information solely from a model's input-output behavior.", "section": "5.2 White-box Attacks"}, {"figure_path": "https://arxiv.org/html/2411.05197/x7.png", "caption": "Figure 7: Example border request of DistillGPT2 generated by HPI-BI. When the border request is sent to the same model checkpoint deployed in FP16 and BF16, we observe different responses.", "description": "Figure 7 shows an example of a \"border request\" generated using the HSPI-BI method.  Border requests are specifically designed inputs that cause a model to produce different outputs based on subtle differences in its hardware or software environment (different quantization formats in this case).  The figure demonstrates that when the same border request is sent to a DistillGPT2 model running with FP16 (half-precision floating point) and BF16 (brain floating point 16) quantization, the model's responses (the generated text) differ.  This difference highlights the sensitivity of model outputs to even small variations in underlying hardware and software configurations, demonstrating the feasibility of using these differences to infer details about the platform on which the model is running.", "section": "5.2 White-box Attacks"}, {"figure_path": "https://arxiv.org/html/2411.05197/x8.png", "caption": "Figure 8: The difference of bit distribution between RTXA6000 and A100 (white-box HSPI-LD). We send the same 256 requests to QWen-2.5-3B deployed on RTXA6000 and A100 and compare the bit distribution of FP32 log probabilities generated by the model. Tokens and logits are sampled in the plot but the difference is still obvious. ti\u2062ljsubscripttisubscriptlj\\mathrm{t_{i}l_{j}}roman_t start_POSTSUBSCRIPT roman_i end_POSTSUBSCRIPT roman_l start_POSTSUBSCRIPT roman_j end_POSTSUBSCRIPT denotes the log probability of i\ud835\udc56iitalic_i-th token\u2019s j\ud835\udc57jitalic_j-th logit.", "description": "Figure 8 illustrates the differences in bit distribution between the log probabilities generated by the QWen-2.5-3B language model running on two different GPUs, the RTX A6000 and the A100.  The experiment uses the HSPI-LD (Hardware and Software Platform Inference with Logits Distributions) method. Identical input requests (256 in total) were sent to the model on both GPUs.  The resulting FP32 log probabilities are analyzed for bit-level differences. Although only a sample of tokens and logits are shown in the plot,  the figure clearly shows distinct differences in bit distribution across the two GPU platforms, demonstrating that even subtle hardware differences can manifest in the model's output.", "section": "5.2 White-box Attacks"}, {"figure_path": "https://arxiv.org/html/2411.05197/x9.png", "caption": "(a) Transferability vs batch size", "description": "This figure shows how the transferability of border images is affected by the batch size used during their training.  Transferability refers to the ability of border images, trained on one model, to successfully distinguish between different hardware/software configurations when applied to other models. The x-axis represents the batch size, and the y-axis represents the transferability accuracy (presumably F1-score). The plot shows an improvement in transferability with increasing batch sizes, up to a certain point after which the improvement plateaus or diminishes.  This suggests that larger batch sizes may help generalize the features of the border images, improving their ability to discriminate across different model setups. However, increasing the batch size beyond a certain point may not yield additional benefits or could even lead to reduced performance.", "section": "5.3 Black-box Attacks"}]