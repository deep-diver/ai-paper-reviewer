{"references": [{"fullname_first_author": "Robin Staab", "paper_title": "Beyond Memorization: Violating privacy via inference with large language models", "publication_date": "2024-00-00", "reason": "This paper is highly relevant as it directly addresses the privacy concerns of LLMs, a central theme of the current research."}, {"fullname_first_author": "Niloofar Mireshghallah", "paper_title": "Can LLMs keep a secret? testing privacy implications of language models via contextual integrity theory", "publication_date": "2024-00-00", "reason": "This paper provides a framework for evaluating the privacy implications of LLMs, which is crucial for understanding the risks associated with their deployment."}, {"fullname_first_author": "Aman Priyanshu", "paper_title": "Are chatbots ready for privacy-sensitive applications? an investigation into input regurgitation and prompt-induced sanitization", "publication_date": "2023-05-15", "reason": "This paper investigates the specific privacy challenges of using chatbots for sensitive information, which is directly relevant to the research on private inference."}, {"fullname_first_author": "Wen-jie Lu", "paper_title": "Bumblebee: Secure two-party inference framework for large transformers", "publication_date": "2025-00-00", "reason": "This paper proposes a secure two-party inference framework for LLMs, which is a key technical component of the private inference approach."}, {"fullname_first_author": "Xiaoyang Hou", "paper_title": "Ciphergpt: Secure two-party GPT inference", "publication_date": "2023-00-00", "reason": "This paper introduces a specific secure two-party computation protocol for GPT models, a widely used type of LLM, thus directly contributing to the current research."}]}