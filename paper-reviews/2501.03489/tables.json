[{"content": "Abbreviation|Architectural configuration\n---|---|---\n<span class=\"ltx_text\" style=\"color:#0000FF;\">SM</span> + <span class=\"ltx_text\" style=\"color:#800080;\">LN</span> + <span class=\"ltx_text\" style=\"color:#FF0000;\">G</span>|\\mathbf{X}_{\\text{out}}=\\text{FFN}_{\\text{GELU}}(\\text{LayerNorm}_{2}(\\text{MHA}(\\text{Attn}_{\\text{Softmax}}(\\text{LayerNorm}_{1}(\\mathbf{X}_{\\text{in}})))))\n<span class=\"ltx_text\" style=\"color:#0000FF;\">SM</span> + <span class=\"ltx_text\" style=\"color:#800080;\">LN</span> + <span class=\"ltx_text\" style=\"color:#FF0000;\">R</span>|\\mathbf{X}_{\\text{out}}=\\text{FFN}_{\\text{ReLU}}(\\text{LayerNorm}_{2}(\\text{MHA}(\\text{Attn}_{\\text{Softmax}}(\\text{LayerNorm}_{1}(\\mathbf{X}_{\\text{in}})))))\n<span class=\"ltx_text\" style=\"color:#0000FF;\">SM</span> + <span class=\"ltx_text\" style=\"color:#800080;\">LN</span>|\\mathbf{X}_{\\text{out}}=\\text{FFN}_{\\text{Identity}}(\\text{LayerNorm}_{2}(\\text{MHA}(\\text{Attn}_{\\text{Softmax}}(\\text{LayerNorm}_{1}(\\mathbf{X}_{\\text{in}})))))\n<span class=\"ltx_text\" style=\"color:#0000FF;\">SM</span> + <span class=\"ltx_text\" style=\"color:#FF0000;\">G</span>|\\mathbf{X}_{\\text{out}}=\\text{FFN}_{\\text{GELU}}(\\text{MHA}(\\text{Attn}_{\\text{Softmax}}(\\mathbf{X}_{\\text{in}})))\n<span class=\"ltx_text\" style=\"color:#0000FF;\">SM</span> + <span class=\"ltx_text\" style=\"color:#FF0000;\">R</span>|\\mathbf{X}_{\\text{out}}=\\text{FFN}_{\\text{ReLU}}(\\text{MHA}(\\text{Attn}_{\\text{Softmax}}(\\mathbf{X}_{\\text{in}})))\n<span class=\"ltx_text\" style=\"color:#0000FF;\">SM</span>|\\mathbf{X}_{\\text{out}}=\\text{FFN}_{\\text{Identity}}(\\text{MHA}(\\text{Attn}_{\\text{Softmax}}(\\mathbf{X}_{\\text{in}})))", "caption": "Table 1: Architectural configurations of nonlinearities in LLMs, illustrating the combinations of Softmax (SM), LayerNorm (LN), GELU (G), and ReLU (R) functions (see Eq. 1, 2, 3 and 4).", "description": "This table presents various architectural configurations for Large Language Models (LLMs) focusing on the impact of different combinations of non-linear activation functions on model performance.  It shows how the choice of Softmax (SM), LayerNorm (LN), GELU (G), and ReLU (R) affects the overall architecture, and specifically how these components are combined in equations 1, 2, 3 and 4. This allows researchers to analyze and compare LLMs with various levels of nonlinearity.", "section": "2 Preliminaries"}, {"content": "| Configurations | PPL | +\u0394(%) | \n|---|---|---| \n| SM + LN + G | 2.69 | 0.00 | \n| SM + LN + R | 2.76 | 2.53 | \n| SM + LN | 3.38 | 25.58 | \n| SM + G | 3.20 | 18.92 | \n| SM + R | 2.94 | 9.20 | \n| SM | NaNs | - | ", "caption": "(a) Headwise entropy distribution", "description": "This table visualizes the distribution of entropy values across different attention heads within a language model.  Entropy, a measure of uncertainty, reflects the focus or spread of attention weights assigned to input tokens. A high entropy value indicates a more uniform distribution of attention across tokens (less focus), while a low entropy value indicates more concentrated attention on a few specific tokens. The table likely shows how many heads fall within specific entropy ranges (e.g., low, medium, high), offering insights into the attention mechanism's behavior and potential for improved efficiency and reduced noise.", "section": "Information-Theoretic Analysis of Nonlinearity in LLMs"}, {"content": "|                     | Network Arch. | PPL   | #Nonlinear Ops | FFN  (FLOPs) | Attn. (FLOPs) | Comm. (GB) | Lat. (min.) | Comm. Savings | Lat. Savings |\n|---------------------|-----------------|-------|-----------------|---------------|---------------|-------------|-------------|----------------|----------------|\n| Baseline             | SM+LN+G         | 2.69  | 14.5B           | 7.7B          | 25.32          | 8.21         | 1\u00d7            | 1\u00d7             |\n|                     | LN: 24\u00d7\u211d\u00b9\u00b2\u2078\u02e3\u2077\u2076\u2078  |       |                 |               |               |             |             |                 |                 |\n|                     | G: 12\u00d7\u211d\u00b9\u00b2\u2078\u02e3\u00b3\u2070\u2077\u00b2  |       |                 |               |               |             |             |                 |                 |\n|                     | SM+LN+R         | 2.76  | 14.5B           | 7.7B          | 9.44           | 6.06         | 2.68\u00d7         | 1.35\u00d7          |\n|                     | LN: 24\u00d7\u211d\u00b9\u00b2\u2078\u02e3\u2077\u2076\u2078  |       |                 |               |               |             |             |                 |                 |\n|                     | R: 12\u00d7\u211d\u00b9\u00b2\u2078\u02e3\u00b3\u2070\u2077\u00b2  |       |                 |               |               |             |             |                 |                 |\n|                     | SM+ScFuFFN       | 3.48  | 1.8B            | 7.7B          | 6.43           | 4.76         | 3.94\u00d7         | 1.72\u00d7          |\n| EReg(SM(t)+ScFuFFN) |                 | 3.21  | 1.8B            | 7.7B          | 6.43           | 4.76         | 3.94\u00d7         | 1.72\u00d7          |", "caption": "(b) Loss curve", "description": "The figure shows the training loss curve for various GPT-2 model configurations with different nonlinearities.  It illustrates how the training loss changes over epochs, providing insights into the effectiveness of different model architectures in terms of convergence and stability. The different lines represent different model setups such as those with GELU, ReLU, or no nonlinearities in their feed-forward networks, or different layer normalization strategies.", "section": "Information-Theoretic Analysis of Nonlinearity in LLMs"}, {"content": "| Network Arch. | Eval PPL (1.2B) | Eval PPL (2.4B) | Eval PPL (4.8B) | #Nonlinear Ops | #FLOPs (FFN) | #FLOPs (Attn.) | Comm. (GB) | Lat. (min.) |\n|---|---|---|---|---|---|---|---|---|\n| Baseline<br>SM+LN+G | 25.71 | 23.32 | 21.29 | SM: 144\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u2075\u00b9\u00b2<br>LN: 24\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u2077\u2076\u2078<br>G: 12\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u00b3\u2070\u2077\u00b2 | 58.0B | 36.2B | 145.24 | 30.74 |\n| SM+LN+R | 26.06 | 23.55 | 21.58 | SM: 144\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u2075\u00b9\u00b2<br>LN: 24\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u2077\u2076\u2078<br>R: 12\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u00b3\u2070\u2077\u00b2 | 58.0B | 36.2B | 81.71 | 23.54 |\n| SM+ScFuFFN | 33.77 | 30.82 | 28.59 | SM: 144\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u2075\u00b9\u00b2 | 7.3B | 36.2B | 69.68 | 19.44 |\n| EReg(SM(t)+ScFuFFN) | 31.54 | 28.70 | 26.55 | SM: 144\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u2075\u00b9\u00b2 | 7.3B | 36.2B | 69.68 | 19.44 |", "caption": "Table 2: Results on GPT-2 (L\ud835\udc3fLitalic_L=12, H\ud835\udc3bHitalic_H=12, d\ud835\udc51ditalic_d=768), trained from scratch on the CodeParrot dataset (2.1B tokens, T\ud835\udc47Titalic_T=128).", "description": "This table presents the results of experiments conducted on a GPT-2 language model with 12 layers, 12 attention heads, and an embedding dimension of 768.  The model was trained from scratch using the CodeParrot dataset, which contains 2.1 billion tokens, with a sequence length (context window) of 128 tokens. The table compares the performance of several model variations, each differing in the types of non-linear operations used (e.g., GELU, ReLU, or none), along with the inclusion of entropy regularization.  Metrics reported include perplexity (PPL), the number of nonlinear operations, and the total number of floating point operations (FLOPs), along with communication costs and latency. This allows for an evaluation of the impact of different architectural choices on model efficiency and accuracy.", "section": "Experimental Results"}, {"content": "| Linear layers | Eval PPL(Weight Normalization) | Eval PPL(Spectral Normalization) |\n|---|---|---|\n| QK | 3.89 | 4.25 |\n| FFN | 3.64 | 3.63 |\n| QK+FFN | 3.88 | 4.23 |\n| QKV+FFN | 3.93 | 4.26 |\n| QKVO+FFN | 3.98 | 4.34 |", "caption": "Table 3: Results on GPT-2 (L\ud835\udc3fLitalic_L=12, H\ud835\udc3bHitalic_H=12, d\ud835\udc51ditalic_d=768) model, trained from scratch on Languini [20] (T\ud835\udc47Titalic_T=512)", "description": "Table 3 presents the results of experiments conducted on a GPT-2 language model with 12 layers (L=12), 12 attention heads (H=12), and an embedding dimension of 768 (d=768). The model was trained from scratch using the Languini dataset [20] with a context length of 512 tokens (T=512).  The table likely shows the performance of different model architectures in terms of perplexity (PPL), the number of nonlinear operations, FLOPs (floating-point operations), and communication and latency overheads for private inference. It might also compare different configurations like the inclusion or exclusion of LayerNorm, GELU, and other components, demonstrating the impact of various architectural choices on the efficiency of private inference.", "section": "Experimental Results"}, {"content": "| Weight Normalization | Spectral Normalization | Scaled-FFN |\n|---|---|---|\n| Eval PPL | 3.640 | 3.624 | 3.478 |", "caption": "Table 4: Comparison of weight normalization [17] and spectral normalization [18] when employed in Softmax-only GPT-2 (L\ud835\udc3fLitalic_L=12, H\ud835\udc3bHitalic_H=12, d\ud835\udc51ditalic_d=768) models, and trained from scratch on CodeParrot dataset with 128 input context length. FFN weight normalization yield the similar results; whereas, weight normalization works better in other linear layers.", "description": "This table presents a comparison of the performance of weight normalization and spectral normalization techniques when applied to different linear layers within a Softmax-only GPT-2 language model. The model has 12 layers (L=12), 12 attention heads (H=12), and an embedding dimension of 768 (d=768). It was trained from scratch using the CodeParrot dataset with an input context length of 128 tokens. The table compares the evaluation perplexity (Eval PPL) achieved using each normalization method in different layer combinations: QK (query-key), FFN (feed-forward network), QK+FFN, QKV+FFN (query-key-value + FFN), and QKVO+FFN (query-key-value-output + FFN). The results show that FFN weight normalization yields similar performance across all layer combinations.  In contrast, weight normalization generally performs better than spectral normalization in other linear layers.", "section": "Additional Results"}, {"content": "| Network Arch. | PPL | #Nonlinear Ops | FFN FLOPs | Attn. FLOPs | Comm. (GB) | Lat. (min.) | Comm. Savings | Lat. Savings |\n|---|---|---|---|---|---|---|---|---|\n| Baseline<br>SM+LN+G | 2.35 | 144x\u211d<sup>256x256</sup><br>24x\u211d<sup>256x768</sup><br>12x\u211d<sup>256x3072</sup> | 29.0B | 16.3B | 58.51 | 16.57 | 1x | 1x |\n| SM+LN+R | 2.41 | 144x\u211d<sup>256x256</sup><br>24x\u211d<sup>256x768</sup><br>12x\u211d<sup>256x3072</sup> | 29.0B | 16.3B | 26.73 | 12.59 | 2.19x | 1.32x |\n| SM+ScFuFFN | 3.03 | 144x\u211d<sup>256x256</sup> | 3.6B | 16.3B | 20.72 | 10.45 | 2.82x | 1.59x |\n| EReg(SM(t)+ScFuFFN) | 2.92 | 144x\u211d<sup>256x256</sup> | 3.6B | 16.3B | 20.72 | 10.45 | 2.82x | 1.59x |", "caption": "Table 5: Perplexity comparison of weight normalization, spectral normalization, and learnable scaling employed in FFN of softmax-only GPT-2 model, when trained from scratch on CodeParrot dataset with 128 input context length.", "description": "This table presents a comparison of the perplexity scores achieved by three different methods for regularizing the feed-forward network (FFN) in a softmax-only version of the GPT-2 language model. The methods compared are weight normalization, spectral normalization, and learnable scaling.  The model was trained from scratch on the CodeParrot dataset using an input context length of 128 tokens. The perplexity scores provide a measure of how well each regularization technique generalizes to unseen data.", "section": "C Additional Results"}, {"content": "|   | Network Arch. | PPL | #Nonlinear Ops | #FLOPs (FFN) | #FLOPs (Attn.) | Comm. (GB) | Lat. (min.) | Savings (Comm.) | Savings (Lat.) |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Baseline<br><sup><img src=\"https://arxiv.org/html/2501.03489/fig1.png\"></sup> | SM+LN+G | 2.56 | 21.7B | 11.6B | 37.17 | 10.77 | 1x | 1x |\n|  | SM: 216x\u211d<sup>128x128</sup><br>LN: 36x\u211d<sup>128x768</sup><br>G: 18x\u211d<sup>128x3072</sup> |  |  |  |  |  |  |  |  |\n| SM+LN+R | 2.63 | 21.7B | 11.6B | 13.34 | 8.04 | 2.79x | 1.34x |\n|  | SM: 216x\u211d<sup>128x128</sup><br>LN: 36x\u211d<sup>128x768</sup><br>R: 18x\u211d<sup>128x3072</sup> |  |  |  |  |  |  |  |  |\n| SM+ScFuFFN | 3.24 | 2.7B | 11.6B | 8.83 | 6.07 | 4.21x | 1.77x |\n| EReg(SM(t)+ScFuFFN)<sup><img src=\"https://arxiv.org/html/2501.03489/fig2.png\"></sup> |  3.13 | 2.7B | 11.6B | 8.83 | 6.07 | 4.21x | 1.77x |", "caption": "Table 6: Results on GPT-2 (L\ud835\udc3fLitalic_L=12, H\ud835\udc3bHitalic_H=12, d\ud835\udc51ditalic_d=768), trained from scratch on the CodeParrot dataset (2.1B tokens, T\ud835\udc47Titalic_T=256).", "description": "This table presents the results of experiments conducted on a GPT-2 language model with 12 layers, 12 attention heads, and an embedding dimension of 768. The model was trained from scratch using the CodeParrot dataset, which comprises 2.1 billion tokens and a context length of 256. The table compares the performance of several model configurations, focusing on their perplexity (PPL), the number of nonlinear operations, floating point operations (FLOPS), the communication overhead (Comm.), and latency (Lat.). The configurations evaluated include the baseline model (SM + LN + G), a model with ReLU activation (SM + LN + R), a simplified Softmax-only model (SM + ScFuFFN), and finally, the proposed entropy-regularized Softmax-only model (EReg(SM(t) + ScFuFFN)).  The comparison highlights the performance gains and resource savings achieved by the simplified and entropy-regularized architectures in terms of perplexity, computational cost, communication, and latency.", "section": "5 Experimental Results"}]