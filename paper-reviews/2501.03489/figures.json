[{"figure_path": "https://arxiv.org/html/2501.03489/x1.png", "caption": "Figure 1: An illustration of threat model and cryptographic protocols used for LLM private inference.", "description": "This figure illustrates the threat model and cryptographic protocols employed in private inference (PI) for large language models (LLMs). The threat model depicts a two-party computation scenario where a client (user) sends encrypted input to a server (model provider). The server processes the encrypted data and returns the encrypted output to the client without revealing either party's private information. The figure also showcases the cryptographic protocols used to achieve this secure computation. This ensures that only the final results are obtained, without revealing the client's data or the server's model parameters.", "section": "Threat model for private inference"}, {"figure_path": "https://arxiv.org/html/2501.03489/x4.png", "caption": "(a) SM + LN + G", "description": "This figure shows the layerwise mean entropy distribution for a GPT-2 model (small) with the baseline architecture (Softmax, Layer Normalization, and GELU activation). The y-axis represents the mean entropy, while the x-axis indicates the head index. Each line represents the entropy for a specific layer, providing a visual representation of how entropy changes across layers and heads in this standard model.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.03489/x5.png", "caption": "(b) SM + LN + R", "description": "This figure shows the headwise entropy distribution for a GPT-2 small model (with 12 layers and 12 heads) trained on the CodeParrot dataset.  Specifically, it displays the percentage of attention heads exhibiting different ranges of entropy values, broken down into four entropy intervals: [0, max), [max, max), [max, 3max), and [3max, max].  The figure helps to visualize the impact of using the ReLU activation function within the feed-forward network (FFN) of the transformer architecture. It demonstrates the distribution of attention weights among heads in a model using the softmax activation function, layer normalization and ReLU in the feed-forward network.", "section": "3 Information-Theoretic Analysis of Nonlinearity in LLMs"}, {"figure_path": "https://arxiv.org/html/2501.03489/x6.png", "caption": "(c) SM + LN", "description": "This figure shows the layer-wise entropy distribution across different attention heads for a Softmax + LayerNorm model.  It's part of an analysis comparing entropy distributions across various architectures of Language Models, with and without nonlinearities. Specifically, this sub-figure displays the heatmap visualization of entropy values for the specified model, enabling a direct comparison to other models' behavior shown in other subfigures.  The heatmap reveals the entropy pattern for each head across the model's layers; darker shades generally mean lower entropy while brighter shades mean higher entropy. It illustrates headwise entropy dynamics in a model with standard nonlinearities, providing a baseline for evaluating the effects of removing nonlinearities or using different normalization methods.", "section": "Information-Theoretic Analysis of Nonlinearity in LLMs"}, {"figure_path": "https://arxiv.org/html/2501.03489/x7.png", "caption": "(d) SM + G", "description": "This figure displays the headwise entropy distribution in a language model architecture with reduced nonlinearities. Specifically, it shows the entropy values for each attention head across different layers (indicated by the y-axis) of the model using the softmax activation function and the GELU activation function (SM+G). Each column in the heatmap represents a different attention head, showing how the entropy of each head varies over the layers. The color intensity represents the entropy value, with higher intensity indicating higher entropy.  The figure visualizes the entropy dynamics in the model to demonstrate that removing nonlinearities (particularly LayerNorm) leads to an issue called \"entropic overload\" in early layers, causing some attention heads to use the full representational capacity and underutilize the diversity of the model's attention heads.", "section": "3 Information-Theoretic Analysis of Nonlinearity in LLMs"}, {"figure_path": "https://arxiv.org/html/2501.03489/x8.png", "caption": "(e) SM + R", "description": "This figure shows a heatmap visualization of the entropy distribution across different attention heads in various layers of a language model with a simplified architecture. Specifically, it presents the entropy values for each head in the model that uses only softmax activation function (SM) and ReLU activation function (R) in the feed-forward network (FFN). The heatmap allows for a visual inspection of the entropy distribution across layers and heads and is useful for identifying potential issues such as entropy overload (high entropy values in earlier layers) or entropy collapse (low entropy values in deeper layers), which may negatively affect the model's performance. The y-axis represents the layer index, and the x-axis represents the head index. The color intensity of each cell in the heatmap reflects the entropy value for the corresponding layer and head, with darker colors indicating higher entropy.  The caption (e) SM + R indicates that the figure corresponds to a model architecture without LayerNorm.", "section": "Information-Theoretic Analysis of Nonlinearity in LLMs"}, {"figure_path": "https://arxiv.org/html/2501.03489/x9.png", "caption": "(f)  SM", "description": "This figure shows the headwise entropy distribution across different layers for a Softmax-only model (no LayerNorm or FFN nonlinearities).  It demonstrates the phenomenon of entropy collapse in the deeper layers, where entropy values approach zero, indicating instability.  This contrasts with baseline models that exhibit a well-behaved entropy distribution.  The entropy heatmap visually represents the entropy of attention heads across layers and heads, highlighting the regions of low and high entropy.", "section": "Information-Theoretic Analysis of Nonlinearity in LLMs"}, {"figure_path": "https://arxiv.org/html/2501.03489/x10.png", "caption": "Figure 2: Headwise entropy distribution in LLM architectures with reduced nonlinearities compared to baseline models. Yellow regions indicate high-entropy concentrations, revealing severe entropic overload predominantly in early layers.", "description": "This figure visualizes the distribution of entropy across different attention heads in various layers of several Large Language Models (LLMs).  Each LLM has varying degrees of nonlinearities (nonlinear activation functions), ranging from a full set of nonlinearities to models with most nonlinearities removed.  The color of each cell represents the entropy value for a given attention head in a specific layer. Yellow indicates high entropy values, suggesting that the attention mechanism is not focusing effectively on a subset of features, but rather is distributing attention widely. This phenomenon is called 'entropic overload' and is observed primarily in the earlier layers of the LLMs with fewer nonlinearities. Conversely, models with nonlinearities exhibit a more focused distribution of attention, represented by lower entropy. This demonstrates the importance of nonlinearities in regulating attention distribution and preventing entropic overload.", "section": "Information-Theoretic Analysis of Nonlinearity in LLMs"}, {"figure_path": "https://arxiv.org/html/2501.03489/x11.png", "caption": "Figure 3: Nonlinearity-reduced simplified architecture with entropy-guided attention mechanism.", "description": "This figure illustrates the simplified architecture of a large language model (LLM) designed for efficient private inference.  It shows how the model's architecture has been modified to reduce reliance on non-linear operations, which are computationally expensive in privacy-preserving settings.  The key modification is the incorporation of an 'entropy-guided attention mechanism', which helps control the distribution of attention weights.  This mechanism, which leverages a learnable temperature to regulate entropy, aims to improve model efficiency and stability while maintaining performance by focusing attention on the most relevant parts of the input sequence.  The figure visually details the individual components of this simplified architecture, highlighting the interactions and flow of information between them.", "section": "Entropy-Guided LLM Architecture for Efficient Private Inference"}, {"figure_path": "https://arxiv.org/html/2501.03489/x12.png", "caption": "(a) SM + LN + G", "description": "This figure shows the layer-wise mean entropy for the baseline GPT-2 model with Softmax, Layer Normalization, and GELU activation functions (SM + LN + G).  The x-axis represents the head index (0-11), and the y-axis represents the layer index (0-11). Each cell displays the average entropy value across all query positions for a given head and layer. The colormap visualizes the entropy values, with darker shades indicating lower entropy (more focused attention) and lighter shades indicating higher entropy (more uniform attention). This figure helps visualize the entropy distribution across different attention heads and layers in a well-trained model, serving as a baseline for comparison with models having different architectures.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.03489/x13.png", "caption": "(b) SM", "description": "This figure displays the layer-wise mean entropy for the Softmax-only (SM) model architecture in GPT-2 models, trained from scratch on the CodeParrot dataset.  The x-axis represents the training steps, and the y-axis shows the mean entropy across the different layers. The multiple colored lines represent different layers in the model.  The plot illustrates the entropy dynamics across training steps within each model layer and reveals the entropy collapse and entropic overload issues associated with this simplified architecture.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.03489/x14.png", "caption": "(c) SM + WeightNormalization(FFN)", "description": "This figure shows the layerwise mean entropy patterns in a GPT-2 model (12 layers, 12 heads, dimensionality 768) during training.  The model uses only the softmax activation function (SM) and employs weight normalization specifically on the feed-forward network (FFN) layer. The plot illustrates how entropy changes across different layers (from L0 to L11) over the training steps. It helps visualize the effectiveness of weight normalization in mitigating entropy collapse and potentially improving the stability and performance of the model.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.03489/x15.png", "caption": "(d) SM + SpectralNormalization(FFN)", "description": "This figure shows the layer-wise entropy patterns observed in a GPT-2 model (12 layers, 12 heads, 768 dimensions) trained from scratch on the CodeParrot dataset. This specific visualization focuses on the architecture where only the softmax activation is used, and the feed-forward network (FFN) employs spectral normalization.  The plot displays the average entropy across all heads within each layer of the model, indicating the distribution of attention scores.  The entropy values represent the uncertainty or spread of the attention weights. A low entropy implies that attention is focused on a small set of tokens, while a high entropy suggests more evenly distributed attention.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.03489/x16.png", "caption": "(e) SM + Scaled(FFN)", "description": "This figure shows the layer-wise entropy patterns in a GPT-2 model (with 12 layers, 12 heads, and a dimensionality of 768) trained from scratch on the CodeParrot dataset.  Specifically, it illustrates the entropy distribution across different layers (Layer 0-11) and shows the impact of using a scaled feed-forward network (FFN) within the Softmax-only architecture (SM). The scaled FFN approach is a technique employed to mitigate the training instabilities and issues caused by the removal of nonlinearities typically seen in more complex transformer models. The visualization in this sub-figure allows comparison with other models and techniques explored in the paper for handling entropy dynamics within the transformer architecture.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.03489/x17.png", "caption": "(f) EntropyReg(SM(t)+ScFuFFN)", "description": "This figure shows the layer-wise mean entropy patterns in a GPT-2 model (12 layers, 12 heads, dimensionality 768) trained on the CodeParrot dataset.  Specifically, it displays the entropy dynamics for the architecture denoted as 'EntropyReg(SM(t)+ScFuFFN)', which represents a Softmax-only model (no LayerNorm or FFN nonlinearities) that incorporates an entropy regularization technique and a learnable temperature for each softmax operation. This method aims to mitigate both entropic overload (excessive entropy in early layers) and entropy collapse (near-zero entropy in deeper layers).  The plot helps visualize how effectively this approach manages entropy across layers and heads, promoting more balanced attention distributions.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.03489/x18.png", "caption": "Figure 4: Layerwise entropy patterns in GPT-2 models (L\ud835\udc3fLitalic_L = 12, H\ud835\udc3bHitalic_H = 12, d\ud835\udc51ditalic_d = 768) trained from scratch on CodeParrot dataset. Shown are (a) baseline model, (b) Softmax-only model without normalization, and variants with (c) weight normalization, (d) spectral normalization, and (e) scaled-FFN. While these normalization methods prevent entropy collapse, they fail to address entropic overload in early layers. Our final configuration (f) incorporates entropy regularization within scaled-FFN to effectively manage both issues.", "description": "This figure displays the layerwise entropy patterns observed in various GPT-2 model configurations trained on the CodeParrot dataset.  The models are compared across several variations: a baseline model using softmax, layer normalization, and GELU activation; a softmax-only model without normalization; and variations incorporating weight normalization, spectral normalization, and scaled feed-forward networks (FFN). The figure visualizes how different techniques impact entropy across layers.  While weight, spectral, and scaled FFN normalization methods effectively prevent entropy collapse in later layers, they struggle to address entropic overload, the presence of excessive high-entropy values, in earlier layers.  The final configuration adds entropy regularization to the scaled FFN approach, demonstrating its effectiveness in managing both entropy collapse and overload.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.03489/x19.png", "caption": "(a) Values of learned threshold weights", "description": "This figure visualizes the learned threshold weights (reg_threshold_weights) used in the entropy regularization component of the model.  The values shown represent the learned fraction of the maximum entropy value (Emax) that is used as a threshold for applying the entropy penalty to individual attention heads.  The heatmap shows these weights for each attention head (x-axis) across different layers (y-axis) of the transformer model.  Variations in the weights across layers and heads reflect the model's adaptive adjustment of the regularization strength based on the unique behavior of each head in different layers.", "section": "4 Entropy-Guided LLM Architecture for Efficient Private Inference"}, {"figure_path": "https://arxiv.org/html/2501.03489/x20.png", "caption": "(b) Layerwise mean and variance of threshold weights", "description": "This figure visualizes the distribution of learned threshold weights used in the entropy regularization technique across different layers of the model.  The top panel shows the mean threshold weight values for each attention head within each layer, revealing how the model dynamically adjusts the regularization strength based on the head\u2019s specific role.  The bottom panel displays the layer-wise mean and variance of these weights, emphasizing the non-uniformity of the regularization process across different layers and the significant variability within individual layers.  This non-uniformity is a key aspect of the method; the model adapts its regularization approach depending on the specific needs of different layers.", "section": "4 Entropy-Guided LLM Architecture for Efficient Private Inference"}, {"figure_path": "https://arxiv.org/html/2501.03489/x21.png", "caption": "Figure 5: Analysis of learned threshold weights (\ud835\ude9b\ud835\ude8e\ud835\ude90\u2062_\u2062\ud835\ude9d\ud835\ude91\ud835\ude9b\ud835\ude8e\ud835\ude9c\ud835\ude91\ud835\ude98\ud835\ude95\ud835\ude8d\u2062_\u2062\ud835\udea0\ud835\ude8e\ud835\ude92\ud835\ude90\ud835\ude91\ud835\ude9d\ud835\ude9c\ud835\ude9b\ud835\ude8e\ud835\ude90_\ud835\ude9d\ud835\ude91\ud835\ude9b\ud835\ude8e\ud835\ude9c\ud835\ude91\ud835\ude98\ud835\ude95\ud835\ude8d_\ud835\udea0\ud835\ude8e\ud835\ude92\ud835\ude90\ud835\ude91\ud835\ude9d\ud835\ude9c\\mathtt{reg\\_threshold\\_weights}typewriter_reg _ typewriter_threshold _ typewriter_weights, see Eq. \u2022 \u2023 4) in entropy regularization for softmax-only GPT-2 model: (a) Attention heads adaptively learn non-uniform threshold weights across different heads, setting individualized thresholds for entropy regularization; (b) The non-uniform means and non-zero variances across layers highlight the necessity and effectiveness of headwise learnable thresholds in adapting regularization strength.", "description": "Figure 5 displays the learnable threshold weights, represented by \\texttt{reg_threshold_weights}, used in the entropy regularization method for a softmax-only GPT-2 model.  Panel (a) shows that each attention head learns its own unique threshold weight, allowing for a flexible and adaptive regularization process across different heads.  This is important because some heads naturally exhibit higher entropy than others. Panel (b) demonstrates the non-uniformity in average threshold weights across layers and the presence of non-zero variances. This non-uniformity underscores the importance of allowing each head to have its own dynamic threshold, rather than applying a single global threshold across all heads and layers, thereby ensuring effective regularization strength.", "section": "4 Entropy-Guided LLM Architecture for Efficient Private Inference"}, {"figure_path": "https://arxiv.org/html/2501.03489/x22.png", "caption": "Figure 6: Headwise entropy distribution in the \ud835\ude82\ud835\ude7c\u2062(\ud835\ude9d)+\ud835\ude82\ud835\ude8c\ud835\ude75\ud835\ude9e\ud835\ude75\ud835\ude75\ud835\ude7d\ud835\ude82\ud835\ude7c\ud835\ude9d\ud835\ude82\ud835\ude8c\ud835\ude75\ud835\ude9e\ud835\ude75\ud835\ude75\ud835\ude7d{\\tt SM(t)+ScFuFFN}typewriter_SM ( typewriter_t ) + typewriter_ScFuFFN GPT-2 model (L\ud835\udc3fLitalic_L=12, H\ud835\udc3bHitalic_H=12, d\ud835\udc51ditalic_d=768) when entropy regularization is applied with varying threshold margin, controlled by \u03b3\ud835\udefe\\gammaitalic_\u03b3.", "description": "Figure 6 visualizes the impact of the tolerance margin (\u03b3) hyperparameter on the distribution of entropy across attention heads in a GPT-2 language model (12 layers, 12 heads, hidden dimension 768) using the SM(t)+ScFuFFN architecture.  The x-axis represents the entropy range, categorized into bins, and the y-axis indicates the percentage of attention heads falling within each entropy range. Different colored bars correspond to various values of \u03b3, illustrating how adjusting the tolerance margin affects the concentration of entropy values.  A small tolerance margin leads to more attention heads having high entropy, indicating a less focused attention mechanism. A larger margin results in fewer attention heads exhibiting very high entropy, suggesting a more focused mechanism. This figure demonstrates the effectiveness of the tolerance margin in controlling over-regularization and maintaining attention head diversity.", "section": "Mitigating over-regularization with an appropriate threshold margin"}, {"figure_path": "https://arxiv.org/html/2501.03489/x23.png", "caption": "(a) Tolmargin=0subscriptTolmargin0\\text{Tol}_{\\text{margin}}=0Tol start_POSTSUBSCRIPT margin end_POSTSUBSCRIPT = 0", "description": "Figure 8 visualizes the impact of the tolerance margin hyperparameter (Tolmargin) on layer-wise entropy dynamics during the training process of a Softmax-only model. The figure displays six subplots, each representing a different Tolmargin value (0, 0.05Emax, 0.10Emax, 0.15Emax, 0.20Emax, and 0.25Emax). Each subplot shows the average entropy across all attention heads for each layer (L0 through L11) as training progresses (x-axis represents the training steps).  It illustrates how increasing the tolerance margin allows for a greater proportion of attention heads to exhibit higher entropy values, particularly in the earlier layers, thereby mitigating over-regularization and preserving attention head diversity.  The y-axis represents layer-wise mean entropy while the x-axis represents training steps.", "section": "5 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.03489/x16.png", "caption": "(b) Tolmargin=0.05\u2062EmaxsubscriptTolmargin0.05subscriptEmax\\text{Tol}_{\\text{margin}}=0.05\\text{E}_{\\text{max}}Tol start_POSTSUBSCRIPT margin end_POSTSUBSCRIPT = 0.05 E start_POSTSUBSCRIPT max end_POSTSUBSCRIPT", "description": "This figure shows the layer-wise mean entropy during the training process of a Softmax-only GPT-2 model with entropy regularization.  The x-axis represents the training steps, and the y-axis represents the mean entropy across different layers. Different lines correspond to different layers (L0-L11) in the model. The tolerance margin (Tolmargin) is set to 0.05 times the maximum entropy (Emax), influencing the entropy distribution across the layers during training. This plot visualizes how entropy regularization, with a specific tolerance margin, affects the entropy level in each layer throughout the training process. A lower mean entropy value generally indicates a well-behaved attention mechanism, while high values could point to potential issues like entropic overload or underutilization of attention heads.", "section": "5 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.03489/x24.png", "caption": "(c) Tolmargin=0.10\u2062EmaxsubscriptTolmargin0.10subscriptEmax\\text{Tol}_{\\text{margin}}=0.10\\text{E}_{\\text{max}}Tol start_POSTSUBSCRIPT margin end_POSTSUBSCRIPT = 0.10 E start_POSTSUBSCRIPT max end_POSTSUBSCRIPT", "description": "Figure 8(c) displays the layer-wise mean entropy dynamics during training of the SM(t)+ScFuFFN model when employing entropy regularization with a tolerance margin set to 0.10 times the maximum entropy value (Tolmargin = 0.10*Emax).  The figure visualizes how the average entropy across different layers changes over training steps, demonstrating the impact of this specific tolerance margin setting on the attention mechanism\u2019s entropy distribution.  It shows how the entropy values in different layers evolve throughout training, offering insights into the effect of the chosen tolerance margin on the overall entropy dynamics of the model.", "section": "5 Experimental Results"}]