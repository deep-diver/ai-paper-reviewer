[{"figure_path": "https://arxiv.org/html/2503.14505/x2.png", "caption": "Figure 1: MusicInfuser adapts video diffusion models to music, making them listen and dance according to the music. This adaptation is done in a prior-preserving manner, enabling it to also accept style through the prompt while aligning the movement to the music. Please refer to the project page videos, as the movement appears slower due to the frame sampling rate.", "description": "MusicInfuser modifies pre-trained video diffusion models to generate dance videos synchronized with music.  It does this by adding a lightweight cross-attention module and a low-rank adapter that aligns the model's output to the rhythm and style of the music input. The figure shows four examples of generated dance videos, each conditioned on a specific text prompt and a music track. Note that the movement may appear slower than in the actual videos due to the frame rate used to create the figure.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.14505/extracted/6289646/fig/long.png", "caption": "Figure 2: Due to the conservation of knowledge in video and text modalities, our model generalizes to generate group dance videos by modulating the prompt. To show this, the prompt is set to \u201c[DANCERS] dancing in a studio with a white backdrop, captured from a front view,\u201d where [DANCERS] denotes the description for each number of dancers.", "description": "Figure 2 demonstrates the model's ability to generate group dance videos.  The key is leveraging the model's existing knowledge of dance and text, and modifying only the prompt.  By changing the word \"[DANCERS]\" in the prompt \"\"[DANCERS] dancing in a studio with a white backdrop, captured from a front view\" to specify the number of dancers (e.g., \"a male and female dancer\", \"multiple dancers\", \"a group of dancers\"), the model generates corresponding videos with the correct number of dancers performing synchronized choreography.", "section": "4. MusicInfuser"}, {"figure_path": "https://arxiv.org/html/2503.14505/x3.png", "caption": "Figure 3: We generate longer dance videos that are 2 times longer than the videos used for training. For each row, we use synthetic in-the-wild music tracks with a keyword \u201cK-pop,\u201d a type of music not existing in AIST\u00a0[46], and use a prompt \u201ca professional dancer dancing K-pop \u2026.\u201d This shows our method is highly generalizable, even extending to longer videos with an unheard cateory of the music. The beat and style alignment can be more clearly observed in the videos on the project page.", "description": "This figure demonstrates the model's ability to generate longer dance videos (twice the length of training videos) using unseen music.  Each row shows a different video generated using synthetic K-pop music (a genre not present in the AIST dataset) and the prompt \"a professional dancer dancing K-pop...\".  The consistent synchronization between the dance moves and music beat, along with stylistic consistency, highlights the model's generalizability and robustness to new, unseen music.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.14505/x4.png", "caption": "Figure 4: Overall architecture of MusicInfuser. Our framework adapts a pre-trained diffusion model with audio embedding using ZICA blocks (Sec. 4.1) and HR-LoRA blocks (Sec. 4.2). The placement of ZICA blocks is selected based on layer adaptability (Sec. 4.6).", "description": "MusicInfuser's architecture modifies a pretrained text-to-video diffusion model to incorporate music.  It does so by adding two types of adapter networks: Zero-Initialized Cross-attention (ZICA) blocks and High-Rank LoRA (HR-LoRA) blocks. ZICA blocks introduce music information using cross-attention, while HR-LoRA blocks adapt the attention weights within the diffusion model's transformer layers.  The placement of the ZICA blocks is strategically determined to balance the impact on different aspects of the generated video while minimizing computational cost, using a layer adaptability strategy described in section 4.6. The diagram visually depicts the flow of information (text, audio, and video) through these components, indicating which blocks are trainable and showing the overall process of music-conditioned video generation.", "section": "4. MusicInfuser"}, {"figure_path": "https://arxiv.org/html/2503.14505/x5.png", "caption": "Figure 5: Comparison of audio-driven generation with MM-Diffusion\u00a0[39]. Our method produces fewer artifacts (shown in the first and third rows), while generating more realistic dance videos with more natural movements (first row) and more dynamic motion (second and third rows). Note that we use the same music track for each row, and the spectrogram is stretched for MM-Diffusion since we generate longer videos. For our method, we use the fixed caption \u201ca professional dancer dancing \u2026\u201d across all music tracks.", "description": "Figure 5 compares the dance videos generated by MusicInfuser and MM-Diffusion [39], a prior state-of-the-art method. The figure uses three music tracks as input. For each track, both methods generate dance videos. The first and third rows show that MusicInfuser produces fewer artifacts compared to MM-Diffusion.  The first row demonstrates that MusicInfuser generates videos with more realistic and natural movements. The second and third rows highlight the more dynamic motion produced by MusicInfuser.  Note that the same music track was used for each row, but the spectrogram is stretched for MM-Diffusion because MusicInfuser generates longer videos than MM-Diffusion.  For MusicInfuser, the prompt 'a professional dancer dancing...' was consistently used for all music tracks.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.14505/x6.png", "caption": "Figure 6: Speed control. The audio input is slowed down (the top row) or sped up (the bottom row) by 0.75 times and 1.25 times, respectively. This shows that speeding up generally results in more movements. Also see the change in the dynamicity, as speeding up the audio also increases the tone of the music.", "description": "Figure 6 demonstrates the impact of altering the speed of the input audio on the generated dance video.  The top row shows the dance when the audio is slowed down by a factor of 0.75, the middle row shows the original-speed dance, and the bottom row shows the dance when the audio is sped up by a factor of 1.25.  The figure illustrates that increasing the audio speed leads to a greater number of movements in the generated dance and also changes the overall dynamic intensity and tone of the resulting dance.", "section": "5.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.14505/x7.png", "caption": "Figure 7: Videos generated with three distinct in-the-wild music tracks created with SUNO AI. For each row, we use in-the-wild music tracks generated with a word \u201cK-pop,\u201d an unseen category.", "description": "Figure 7 showcases the model's ability to generalize to unseen music styles.  Three distinct dance videos are generated, each synchronized to a different 'K-pop' music track created using SUNO AI. The 'K-pop' genre was not present in the training data, demonstrating the model's ability to adapt and generate appropriate choreography for new musical styles.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.14505/extracted/6289646/fig/beta_uniform.png", "caption": "Figure 8: By changing the seed, our method can produce diverse results given the same music and text. The generated choreography of each dance is different from each other. We use the fixed prompt \u201ca professional dancer dancing \u2026.\u201d", "description": "Figure 8 showcases the diversity achievable by MusicInfuser.  Using the same music track and the text prompt \u201ca professional dancer dancing\u2026\u201d, altering only the random seed results in several unique dance sequences. Each generated dance video features a distinct choreography, demonstrating the model\u2019s capacity to produce varied creative outputs from the same inputs.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.14505/x8.png", "caption": "Figure 9: Beta distributions.", "description": "This figure shows a set of curves representing beta distributions with varying parameters.  The x-axis represents the values of the beta distribution, and the y-axis shows the probability density. Each curve corresponds to a different beta distribution, with the parameter \u03b2 changing from 3.0 to 1.0 in increments. The curves illustrate how the shape of the beta distribution changes with the parameter \u03b2, going from a distribution concentrated near 0 to a uniform distribution as \u03b2 approaches 1.  This visualization helps to understand the Beta-Uniform scheduling strategy used in the MusicInfuser model, where the noise distribution is gradually transitioned from a Beta distribution to a uniform distribution during the training process. ", "section": "3. Preliminaries"}, {"figure_path": "https://arxiv.org/html/2503.14505/x9.png", "caption": "Figure 10: Changes in the complexity of choreography.", "description": "This figure demonstrates how the model's generated choreography changes in complexity based on the prompt used.  The top row showcases basic dance movements generated with a simple prompt. The middle row shows increased complexity with a more specific style and setting. The bottom row displays the most complex choreography, generated by a detailed and descriptive prompt. This illustrates the model's ability to control the level of detail and sophistication in the generated dance sequences via textual prompts.", "section": "5.2. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.14505/x10.png", "caption": "Figure 11: MusicInfuser infuses listening capability into the text-to-video model (Mochi\u00a0[44]), while preserving the prompt adherence and improving overall consistency and realism (frames 2 and 5 of the top example, and frames 2\u20134 of the bottom example).", "description": "This figure compares the results of generating dance videos using two different methods: MusicInfuser and Mochi.  MusicInfuser, the authors' proposed method, uses the Mochi text-to-video model as a base but adds audio conditioning through their cross-attention mechanism. The figure showcases two examples where each method is prompted to generate a video of a dancer in a specific setting, based on the provided text. The comparisons in this figure highlight how MusicInfuser is able to generate videos that better adhere to the prompt and have higher levels of overall consistency and realism compared to the base Mochi model.  Specifically, the authors point out that frames 2 and 5 in the top example, and frames 2-4 in the bottom example, most clearly illustrate this improvement in adherence and quality.", "section": "4. MusicInfuser"}, {"figure_path": "https://arxiv.org/html/2503.14505/x11.png", "caption": "Figure 12: Ablation study. The prompt is set to \u201ca male dancer dancing in an art gallery with some paintings, captured from a front view\u201d. The seed and music are set the same across all methods.", "description": "Figure 12 presents an ablation study comparing different model variations of MusicInfuser.  All results use the same music track, random seed, and text prompt: \"a male dancer dancing in an art gallery with some paintings, captured from a front view\".  This allows a clear visual comparison of how each component (ZICA layer selection, Beta-Uniform scheduling, higher-rank LoRA, standard LoRA, and the addition of raw audio features) affects the generated dance video.  Differences in dance quality, style adherence, and movement smoothness are easily observable.", "section": "5.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.14505/x12.png", "caption": "Figure 13: Ablation study. The prompt is set to \u201ca male dancer wearing a suit dancing in the middle of a New York City, captured from a front view\u201d. The seed and music are set the same across all methods.", "description": "This ablation study visualizes the impact of different components of the MusicInfuser model on dance video generation.  The experiment uses a consistent prompt ('a male dancer wearing a suit dancing in the middle of a New York City, captured from a front view'), music track, and random seed across all model variations.  Each row shows the results for a specific model variant: the full MusicInfuser model, a model without the zero-initialized cross-attention layer selection, a model without the beta-uniform scheduling, a model without higher-rank LoRA, a model without LoRA, and a model using feature addition instead of the ZICA adapter. The generated video sequences allow for a visual comparison of how each model component affects the final dance generated.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.14505/x13.png", "caption": "Figure 14: Layer adaptability graph from [26], showing imaging and aesthetic quality.", "description": "This figure displays a graph showing the layer adaptability results from the paper [26]. It specifically illustrates how the imaging and aesthetic quality change across different layers of the model. This information is crucial for determining the optimal layer selection strategy within a video generation model.", "section": "4.6. Selecting Layers Based on Adaptability"}, {"figure_path": "https://arxiv.org/html/2503.14505/extracted/6289646/fig/failure.png", "caption": "Figure 15: Videos generated with three distinct in-the-wild music tracks.", "description": "This figure showcases three example videos generated by the MusicInfuser model, each synchronized to a different music track sourced from the internet. This demonstrates the model's capability to generalize to unseen music styles and maintain high-quality dance generation.", "section": "5.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.14505/x14.png", "caption": "Figure 16: Failure cases. Our model inherits some issues from the base model, such as failing to generate fine details (e.g., fingers and faces) and being fooled by the silhouette of the dancers.", "description": "Figure 16 shows examples where the MusicInfuser model fails to generate high-fidelity details, such as fingers and facial features.  These failures are inherited from limitations in the underlying base model.  Additionally, the model demonstrates a susceptibility to errors caused by focusing primarily on the silhouette of the dancers rather than precise details of their pose and movement. In essence, the model struggles with generating fine-grained details and can be misled by overall body shape.", "section": "Limitations"}]