{"references": [{" publication_date": "2022", "fullname_first_author": "E. J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "reason": "This paper introduces LoRA, a prominent low-rank adaptation method for large language models, forming a significant foundation for the proposed LoLDU approach. LoLDU directly addresses limitations inherent in LoRA's random initialization while leveraging its core concept of low-rank updates. The importance is further accentuated by frequent mentions and comparisons throughout the paper, making it a central point of reference and comparison for assessing LoLDU's improvements.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "D. J. Kopiczko", "paper_title": "VeRA: Vector-based random matrix adaptation", "reason": "VeRA, another low-rank adaptation technique, is directly compared to LoLDU, providing a clear benchmark for evaluating performance and parameter efficiency.  The introduction explicitly highlights VeRA alongside LoRA, further establishing its significance as a competing method within the context of parameter-efficient fine-tuning of large language models.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "H. Liu", "paper_title": "Visual instruction tuning", "reason": "This paper is important for demonstrating the application of large language models in diverse domains, including computer vision. The introduction uses the computationally expensive task of visual instruction tuning as one of the motivating examples for the need for efficient fine-tuning techniques like the proposed LoLDU method.  The inclusion of this citation highlights the significance of addressing computational challenges in various applications of large models.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "R. Gal", "paper_title": "An image is worth one word: Personalizing text-to-image generation using textual inversion", "reason": "This work exemplifies the high cost and computational requirements of fine-tuning in specific domains, such as image generation. The introduction uses this research to strengthen the argument for the need for efficient fine-tuning strategies. This citation is important in establishing the relevance and impact of the LoLDU method in diverse application scenarios.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "N. Ruiz", "paper_title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation", "reason": "Similar to the previous citation, DreamBooth also showcases the computationally expensive nature of fine-tuning in image generation, furthering the motivation for more efficient methods.  The introduction uses this research to underscore the need for efficient techniques, highlighting the impact of the LoLDU approach on various application areas.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "A. Wang", "paper_title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding", "reason": "The GLUE benchmark serves as a key dataset in the experimental evaluation of LoLDU for natural language understanding tasks. The paper details the benchmark datasets and tasks, providing essential context for interpreting and evaluating the performance of LoLDU on NLP tasks.  Its importance is highlighted by the extensive experiments reported using the GLUE benchmark.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Y. K. Chia", "paper_title": "InstructEval: Towards holistic evaluation of instruction-tuned large language models", "reason": "InstructEval is a crucial dataset used for evaluating the instruction-following capabilities of large language models.  This paper provides the details of the InstructEval benchmark, essential for understanding and validating the performance of LoLDU on instruction tuning tasks.  The reference indicates the importance placed on a comprehensive evaluation methodology.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "R. Taori", "paper_title": "Stanford alpaca: An instruction-following llama model", "reason": "The Alpaca dataset is a central component in the instruction tuning experiments.  This paper describes the Alpaca dataset in detail, providing crucial context for understanding the experimental setup and results of LoLDU on instruction-following tasks.  This work is directly referenced in the experimental section.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "W.-L. Chiang", "paper_title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality", "reason": "The Vicuna dataset complements the Alpaca dataset in the evaluation of instruction tuning capabilities.  The paper explains the nature of the Vicuna dataset, providing essential information for replicating the experiments and understanding the performance context of LoLDU within the instruction tuning landscape.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "reason": "The ViT model is the backbone for image classification experiments.  This paper introduces the Vision Transformer (ViT) architecture, providing necessary background for understanding the experimental setup and results when evaluating LoLDU's performance on image classification tasks.  The model is directly used in the experiments.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "R. Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "Stable Diffusion, a leading model in image generation, is used for the image generation experiments.  This paper is the primary source for understanding the Stable Diffusion model and its applications, which are fundamental for evaluating LoLDU's performance on image generation tasks.  The model architecture is essential to understanding the experiments.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Y. Liu", "paper_title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "reason": "RoBERTa is a prominent pre-trained language model used in the experiments for natural language understanding. This paper introduces the RoBERTa architecture, providing crucial background for the experimental setup and results when assessing LoLDU's performance on NLP tasks.  The model forms the basis for the NLP experiments.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "H. Touvron", "paper_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "reason": "LLaMA-2 is a significant large language model employed in the instruction tuning experiments. The paper introduces the LLaMA-2 architecture, which is vital for understanding the experimental setup and interpreting the performance results of LoLDU on instruction tuning tasks.  It is explicitly used in the experiments.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "T. Jiang", "paper_title": "MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning", "reason": "This paper discusses another low-rank method for parameter-efficient fine-tuning that addresses some limitations of LoRA, and is therefore relevant to the related work and comparison with LoLDU. This is explicitly mentioned in the related work.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "T. Dettmers", "paper_title": "Qlora: Efficient finetuning of quantized llms", "reason": "QLoRA is another important PEFT method that is mentioned in the related work and implicitly compared against LoLDU.  This paper describes efficient fine-tuning techniques that are relevant to understanding the space of PEFT and the novelty of the LoLDU approach.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "F. Meng", "paper_title": "PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models", "reason": "PiSSA is another PEFT method closely related to LoRA, which is extensively discussed in the paper, and also uses low-rank approximations.  This paper is important in showing the landscape of existing approaches and positioning LoLDU within this context.  It is referenced in the related work section.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "S.-Y. Liu", "paper_title": "DoRA: Weight-Decomposed Low-Rank Adaptation", "reason": "DoRA is another recent low-rank adaptation method that serves as a related work, providing further context for understanding the advancements in PEFT techniques and the comparative advantages of LoLDU. The related work section mentions DoRA, suggesting its relevance to the overall context of PEFT.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "V. Lingam", "paper_title": "SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors", "reason": "SVFT is a parameter-efficient fine-tuning technique using singular vector decomposition that is mentioned in the related work section.  This paper illustrates the variety of methods using SVD in PEFT and shows the advantages of LoLDU over this alternative approach.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "X. Glorot", "paper_title": "Understanding the difficulty of training deep feedforward neural networks", "reason": "This paper is important for understanding the challenges in training deep neural networks, including the role of initialization, which is a key aspect addressed by LoLDU.  The introduction section points out the importance of initialization and addresses the issue of suboptimal convergence in existing methods.  It is referred to in the motivation for addressing the problems of existing methods.", "section_number": 2}, {" publication_date": "2013", "fullname_first_author": "I. Sutskever", "paper_title": "On the importance of initialization and momentum in deep learning", "reason": "This paper is crucial for understanding the importance of proper weight initialization in deep learning, which is a key issue that LoLDU addresses. The introduction highlights the need for more effective parameter-efficient fine-tuning strategies, and this paper directly contributes to the understanding of the challenge of proper weight initialization, which LoLDU improves upon.", "section_number": 2}]}