[{"content": "| Benchmark | Document Categories | BBox | Text | Table | Order | Formula | OCR | DLA | TR | MFR | OCR | ROD | TR | MFR |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Single-Module Eval Benchmark** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Robust Reading [19] | 1 | \u2714 |  |  |  |  | \u2714 |  |  |  |  |  |  |  |\n| PubLayNet [43], DocBank [24], DocLayNet [31], M<sup class=\"ltx_sup\">6</sup>Doc [7] | 1, 1, 5, 6 | \u2714 |  |  |  |  |  | \u2714 |  |  |  |  |  |  |\n| PubTabNet [47],TableX [9], TableBank [23] | 1, 1, 1 |  |  | \u2714 |  |  |  |  | \u2714 |  |  |  |  |  |\n| Im2Latex-100K [8],UniMER-Test [34] | 1 |  |  |  |  | \u2714 |  |  |  |  | \u2714 |  |  |  |  |\n| **End-to-end Eval Benchmarks** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Nougat [5] | 1 |  |  |  | \u2714 |  |  |  |  |  |  | \u2714 | \u2714 | \u2714 | \u2714 |\n| Fox [27] | 2 |  |  |  | \u2714 |  |  |  |  |  |  | \u2714 |  |  |  |\n| GOT OCR 2.0 [39] | 2 |  |  |  | \u2714 |  |  |  |  |  |  | \u2714 |  | \u2714 | \u2714 |\n| READoc [26] | 2 |  |  |  | \u2714 | \u2714 | \u2714 | \u2714 |  |  |  | \u2714 | \u2714 | \u2714 | \u2714 |\n| **OmniDocBench** | 9 | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 |", "caption": "Table 1: A comparison between OmniDocBench and existing DCE benchmarks. OCR: Optical Character Recognition; DLA: Document Layout Analysis; MFR: Math Formula Recognition; TR: Table Recognition; ROD: Reading Order Detection", "description": "This table compares OmniDocBench with other existing document content extraction (DCE) benchmarks.  It highlights key differences in the number of document categories, annotation types available (bounding box, text, table, reading order, formula, etc.) and the types of evaluations supported (single-module or end-to-end). Abbreviations used are explained:  OCR (Optical Character Recognition), DLA (Document Layout Analysis), MFR (Math Formula Recognition), TR (Table Recognition), and ROD (Reading Order Detection). This allows readers to quickly assess the scope and features of OmniDocBench relative to existing benchmarks.", "section": "2.3. Benchmark for Document Content Extraction"}, {"content": "| Method Type | Methods | Text<sup class=\"ltx_sup\">Edit</sup>\u2193 EN | ZH | Formula<sup class=\"ltx_sup\">Edit</sup>\u2193 EN | ZH | Formula<sup class=\"ltx_sup\">CDM</sup>\u2191 EN | ZH | Table<sup class=\"ltx_sup\">TEDS</sup>\u2191 EN | ZH | Table<sup class=\"ltx_sup\">Edit</sup>\u2193 EN | ZH | Read Order<sup class=\"ltx_sup\">Edit</sup>\u2193 EN | ZH | Overall<sup class=\"ltx_sup\">Edit</sup>\u2193 EN | ZH |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Pipeline Tools** | MinerU | **0.058** | **0.211** | **0.278** | 0.577 | 66.9 | 49.5 | **79.4** | 62.7 | **0.305** | <span class=\"ltx_text ltx_framed ltx_framed_underline\">0.461</span> | **0.079** | 0.288 | **0.180** | <span class=\"ltx_text ltx_framed ltx_framed_underline\">0.384</span> |\n|  | Marker | 0.141 | 0.303 | 0.667 | 0.868 | 18.4 | 12.7 | 54.0 | 45.8 | 0.718 | 0.763 | 0.138 | 0.306 | 0.416 | 0.560 |\n|  | Mathpix | <span class=\"ltx_text ltx_framed ltx_framed_underline\">0.101</span> | 0.358 | <span class=\"ltx_text ltx_framed ltx_framed_underline\">0.306</span> | **0.454** | 71.4 | **72.7** | <span class=\"ltx_text ltx_framed ltx_framed_underline\">77.9</span> | **68.2** | <span class=\"ltx_text ltx_framed ltx_framed_underline\">0.322</span> | **0.416** | <span class=\"ltx_text ltx_framed ltx_framed_underline\">0.105</span> | 0.275 | <span class=\"ltx_text ltx_framed ltx_framed_underline\">0.209</span> | **0.376** |\n| **Expert VLMs** | GOT-OCR | 0.187 | 0.315 | 0.360 | <span class=\"ltx_text ltx_framed ltx_framed_underline\">0.528</span> | **81.8** | 51.4 | 53.5 | 48.0 | 0.521 | 0.594 | 0.141 | 0.28 | 0.302 | 0.429 |\n|  | Nougat | 0.365 | 0.998 | 0.488 | 0.941 | 17.4 | 16.9 | 40.3 | 0.0 | 0.622 | 1.000 | 0.382 | 0.954 | 0.464 | 0.973 |\n| **General VLMs** | GPT4o | 0.144 | 0.409 | 0.425 | 0.606 | <span class=\"ltx_text ltx_framed ltx_framed_underline\">76.4</span> | 48.2 | 72.8 | 63.7 | 0.363 | 0.474 | 0.128 | 0.251 | 0.265 | 0.435 |\n|  | Qwen2-VL | 0.252 | <span class=\"ltx_text ltx_framed ltx_framed_underline\">0.251</span> | 0.468 | 0.572 | 54.9 | <span class=\"ltx_text ltx_framed ltx_framed_underline\">60.9</span> | 59.9 | <span class=\"ltx_text ltx_framed ltx_framed_underline\">66.8</span> | 0.591 | 0.587 | 0.255 | **0.223** | 0.392 | 0.408 |\n|  | InternVL2 | 0.353 | 0.290 | 0.543 | 0.701 | 69.8 | 49.6 | 63.8 | 61.1 | 0.616 | 0.638 | 0.317 | <span class=\"ltx_text ltx_framed ltx_framed_underline\">0.228</span> | 0.457 | 0.464 |", "caption": "Table 2: Comprehensive evaluation of document parsing algorithms on OmniDocBench: performance metrics for text, formula, table, and reading order extraction, with overall scores derived from ground truth comparisons.", "description": "This table presents a comprehensive evaluation of various document parsing algorithms using the OmniDocBench benchmark dataset.  It provides performance metrics for four key sub-tasks: text, formula, and table extraction, as well as reading order detection.  For each algorithm and each task, the table shows scores in both English and Chinese, allowing for comparison across languages.  Finally, it displays an overall score derived by comparing the algorithm's output to the ground truth.", "section": "4. OmniDocBench Evaluation Methodology"}, {"content": "| Model Type | Models | Book | Slides | Financial Report | Textbook | Exam Paper | Magazine | Academic Papers | Notes | Newspaper | Average |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| Pipeline Tools | MinerU | 0.044 | 0.124 | 0.033 | 0.102 | 0.159 | 0.072 | 0.025 | 0.984 | 0.148 | 0.188 |\n|  | Marker | 0.188 | 0.327 | 0.087 | 0.292 | 0.423 | 0.134 | 0.102 | 0.470 | 0.270 | 0.255 |\n|  | Mathpix | 0.131 | 0.168 | 0.202 | 0.199 | 0.278 | 0.138 | 0.091 | 0.631 | 0.648 | 0.276 |\n| Expert VLMs | GOT-OCR | 0.105 | 0.222 | 0.067 | 0.132 | 0.204 | 0.198 | 0.179 | 0.388 | 0.771 | 0.252 |\n|  | Nougat | 0.734 | 0.958 | 1.000 | 0.820 | 0.930 | 0.83 | 0.214 | 0.991 | 0.871 | 0.816 |\n| General VLMs | GPT4o | 0.157 | 0.163 | 0.348 | 0.187 | 0.281 | 0.173 | 0.146 | 0.607 | 0.751 | 0.313 |\n|  | Qwen2-VL | 0.094 | 0.08 | 0.145 | 0.148 | 0.219 | 0.065 | 0.315 | 0.298 | 0.79 | 0.239 |\n|  | InternVL2 | 0.216 | 0.098 | 0.162 | 0.184 | 0.247 | 0.150 | 0.419 | 0.226 | 0.903 | 0.289 |", "caption": "Table 3: End-to-end text recognition performance on OmniDocBench: evaluation using edit distance across 9 PDF page types.", "description": "This table presents a comprehensive evaluation of end-to-end text recognition performance across nine diverse document types within the OmniDocBench benchmark.  It utilizes edit distance as the evaluation metric to measure the accuracy of text extraction methods on various document types, offering insights into the strengths and weaknesses of different models in handling diverse document layouts and content styles.", "section": "4. OmniDocBench Evaluation Methodology"}, {"content": "| Models | Fuzzy | Water | Color | Mean | Variance |\n|---|---|---|---|---|---| \n| **Pipeline Tools** |  |  |  |  |  |\n| MinerU | 0.15 | **0.151** | **0.107** | **0.136** | **0.0004** |\n| Marker | 0.286 | 0.436 | 0.290 | 0.337 | 0.0049 |\n| Mathpix | 0.294 | 0.290 | 0.182 | 0.255 | 0.0027 |\n| **Expert VLMs** |  |  |  |  |  |\n| GOT-OCR | 0.175 | 0.190 | 0.186 | 0.184 | **0.0000** |\n| Nougat | 0.934 | 0.915 | 0.873 | 0.907 | 0.0006 |\n| **General VLMs** |  |  |  |  |  |\n| GPT4o | 0.263 | 0.195 | 0.184 | 0.214 | 0.0012 |\n| Qwen2-VL | **0.101** | **0.157** | **0.114** | **0.124** | 0.0006 |\n| InternVL2 | **0.120** | 0.197 | 0.155 | 0.157 | 0.0010 |", "caption": "Table 4: End-to-end text recognition on OmniDocBench: evaluation under various page attributes using the edit distance metric.\nColumns represent: Fuzzy (Fuzzy scan), Water (Watermark), Color (Colorful background).", "description": "This table presents the end-to-end text recognition performance on the OmniDocBench dataset, broken down by various page attributes.  The evaluation metric used is the edit distance. The columns represent different image qualities: Fuzzy (presence of a fuzzy scan), Water (presence of a watermark), and Color (presence of a colorful background).  The results show how well different models perform under various image conditions, indicating their robustness and generalizability.", "section": "4. OmniDocBench Evaluation Methodology"}, {"content": "| Models | Single | Double | Three | Complex | Mean | Variance |\n|---|---|---|---|---|---|---|\n| **Pipeline Tools** |  |  |  |  |  |  |\n| MinerU | 0.311 | **0.101** | **0.117** | **0.376** | **0.226** | 0.0143 |\n| Marker | 0.231 | 0.251 | 0.309 | **0.378** | 0.292 | **0.0033** |\n| Mathpix | 0.189 | **0.175** | **0.225** | 0.413 | 0.250 | **0.0091** |\n| **Expert VLMs** |  |  |  |  |  |  |\n| GOT-OCR | 0.163 | 0.145 | 0.257 | 0.468 | 0.258 | 0.0165 |\n| Nougat | 0.852 | 0.601 | 0.662 | 0.873 | 0.747 | 0.0139 |\n| **General VLMs** |  |  |  |  |  |  |\n| GPT4o | 0.109 | 0.204 | 0.254 | 0.426 | **0.248** | 0.0132 |\n| Qwen2-VL | **0.098** | 0.248 | 0.517 | 0.429 | 0.323 | 0.0263 |\n| InternVL2 | **0.082** | 0.312 | 0.682 | 0.444 | 0.380 | 0.0472 |", "caption": "Table 5: End-to-end reading order evaluation on OmniDocBench: results across different column layout types using Normalized Edit Distance.", "description": "This table presents the performance of various document content extraction models in terms of reading order accuracy, specifically focusing on how well the models handle documents with different numbers of columns. The evaluation metric used is the Normalized Edit Distance, which quantifies the difference between the predicted reading order and the ground truth reading order.  The results provide insights into the models' ability to accurately capture the reading sequence in documents of varying complexity.", "section": "5. Benchmarks"}, {"content": "| Model | Book | Slides | Research | Report | Textbook | Exam | Paper | Academic | Literature | Notes | Newspaper | Average mAP |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| DiT-L | 43.44 | 13.72 | 45.85 | 15.45 | 3.40 | 29.23 | 66.13 | 0.21 | 23.65 | 26.90 |\n| LayoutLMv3 | 42.12 | 13.63 | 43.22 | 21.00 | 5.48 | 31.81 | 64.66 | 0.80 | 30.84 | 28.84 |\n| DOCX-Chain | 30.86 | 11.71 | 39.62 | 19.23 | 10.67 | 23.00 | 41.60 | 1.80 | 16.96 | 21.27 |\n| DocLayout-YOLO | 43.71 | 48.71 | 72.83 | 42.67 | 35.40 | 51.44 | 66.84 | 9.54 | 57.54 | 48.71 |", "caption": "Table 6: Component-level layout detection evaluation on OmniDocBench layout subset: mAP results by PDF page type.", "description": "Table 6 presents a detailed breakdown of the performance of component-level layout detection models across various PDF page types within the OmniDocBench benchmark dataset.  The mean Average Precision (mAP) metric is used to assess the accuracy of layout detection for each document category. This provides insights into the strengths and weaknesses of different models in handling the diverse layout structures found in real-world documents. The table allows for a granular analysis of performance across different document types, facilitating a more comprehensive understanding of the challenges and opportunities in document layout analysis.", "section": "5.1 Component-specific Evaluation Results"}, {"content": "| Model Type | Model | Language EN | Language ZH | Language Mixed | Table Frame Type Full | Table Frame Type Omission | Table Frame Type Three | Table Frame Type Zero | Special Situation Merge Cell (+/-) | Special Situation Formula (+/-) | Special Situation Colorful (+/-) | Special Situation Rotate (+/-) | Overall |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **OCR-based Models** | PaddleOCR | 76.8 | 71.8 | 80.1 | 67.9 | 74.3 | 81.1 | 74.5 | 70.6/75.2 | 71.3/74.1 | 72.7/74.0 | 23.3/74.6 | 73.6 |\n|  | RapidTable | **80.0** | **83.2** | **91.2** | **83.0** | **79.7** | **83.4** | 78.4 | **77.1/85.4** | **76.7/83.9** | **77.6/84.9** | **25.2/83.7** | **82.5** |\n| **Expert VLMs** | StructEqTable | 72.0 | 72.6 | 81.7 | 68.8 | 64.3 | 80.7 | **85.0** | 65.1/76.8 | 69.4/73.5 | 66.8/75.7 | **44.1/73.3** | 72.7 |\n|  | GOT-OCR | 72.2 | **75.5** | **85.4** | **73.1** | 72.7 | 78.2 | 75.7 | 65.0/80.2 | 64.3/77.3 | 70.8/76.9 | 8.5/76.3 | **74.9** |\n| **General VLMs** | Qwen2-VL-7B | 70.2 | 70.7 | 82.4 | 70.2 | 62.8 | 74.5 | **80.3** | 60.8/76.5 | 63.8/72.6 | 71.4/70.8 | 20.0/72.1 | 71.0 |\n|  | InternVL2-8B | 70.9 | 71.5 | 77.4 | 69.5 | 69.2 | 74.8 | 75.8 | 58.7/78.4 | 62.4/73.6 | 68.2/73.1 | 20.4/72.6 | 71.5 |", "caption": "Table 7: Component-level Table Recognition evaluation on OmniDocBench table subset. (+/-) means with/without special situation.", "description": "This table presents a detailed breakdown of the performance of various models on the table recognition task within the OmniDocBench benchmark dataset.  It assesses the accuracy of different models across nine diverse document types. The evaluation considers both standard table scenarios and those with special characteristics (indicated by +/-), such as merged cells, rotated text, and more. This allows for a comprehensive comparison of model performance under various conditions.", "section": "5. Single Algorithm Evaluation Results"}, {"content": "| Model Type | Model | Language EN | Language ZH | Language Mixed | Text background White | Text background Single | Text background Multi | Text Rotate Normal | Text Rotate Rotate90 | Text Rotate Rotate270 | Text Rotate Horizontal |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| **Expert Vision Models** | PaddleOCR | 0.071 | **0.055** | **0.118** | **0.060** | **0.038** | **0.085** | **0.060** | **0.015** | **0.285** | **0.021** |\n|  | Tesseract OCR | 0.179 | 0.553 | 0.553 | 0.453 | 0.463 | 0.394 | 0.448 | 0.369 | 0.979 | 0.982 |\n|  | Surya | 0.057 | 0.123 | 0.164 | 0.093 | 0.186 | 0.235 | 0.104 | 0.634 | 0.767 | 0.255 |\n|  | GOT-OCR | 0.041 | **0.112** | 0.135 | **0.092** | **0.052** | 0.155 | **0.091** | 0.562 | 0.966 | 0.097 |\n|  | Mathpix | **0.033** | 0.240 | 0.261 | 0.185 | 0.121 | 0.166 | 0.180 | **0.038** | **0.185** | 0.638 |\n| **Vision Language Models** | Qwen2-VL | 0.072 | 0.274 | 0.286 | 0.234 | 0.155 | **0.148** | 0.223 | 0.273 | 0.721 | **0.067** |\n|  | InternVL2 | 0.074 | 0.155 | 0.242 | 0.113 | 0.352 | 0.269 | 0.132 | 0.610 | 0.907 | 0.595 |\n|  | GPT4o | **0.020** | 0.224 | **0.125** | 0.167 | 0.140 | 0.220 | 0.168 | 0.115 | 0.718 | 0.132 |", "caption": "Table 8: Component-level evaluation on OmniDocBench OCR subset: results grouped by text attributes using the edit distance metric.", "description": "This table presents a comprehensive evaluation of OCR performance on the OmniDocBench dataset, broken down by various text attributes.  It shows the edit distance results for different OCR models, categorized by language (English, Chinese, mixed), text background color (white, single-colored, multi-colored), and text rotation (normal, rotated 90\u00b0, rotated 270\u00b0, horizontal). This allows for a detailed analysis of OCR accuracy under diverse conditions and provides insights into the strengths and weaknesses of different OCR models.", "section": "5. Single Algorithm Evaluation Results"}, {"content": "| Models | CDM | ExpRate@CDM | BLEU | Norm Edit |\n|---|---|---|---|---|\n| GOT-OCR | 74.1 | 28.0 | 55.07 | 0.290 |\n| Mathpix | 86.6 | 2.8 | 66.56 | 0.322 |\n| Pix2Tex | 73.9 | 39.5 | 46.00 | 0.337 |\n| UniMERNet-B | 85.0 | 60.2 | 60.84 | 0.238 |\n| GPT4o | 86.8 | 65.5 | 45.17 | 0.282 |\n| InternVL2 | 67.4 | 54.5 | 47.63 | 0.308 |\n| Qwen2-VL | 83.8 | 55.4 | 53.71 | 0.285 |", "caption": "Table 9: Component-level formula recognition evaluation on OmniDocBench formula subset.", "description": "Table 9 presents a comprehensive evaluation of formula recognition algorithms on the OmniDocBench dataset, specifically focusing on the formula subset.  It details the performance of various models in accurately recognizing and extracting formula information from diverse document types within the benchmark.", "section": "5.3. Single Algorithm Evaluation Results"}, {"content": "| Model Type | Model | Language EN | Language ZH | Language Mixed | Table Frame Type Full | Table Frame Type Omission | Table Frame Type Three | Table Frame Type Zero | Special Situation Merge Cell (+/-) | Special Situation Formula (+/-) | Special Situation Colorful (+/-) | Special Situation Rotate (+/-) |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Pipeline Tools** | MinerU | 75.7 | 59.9 | **79.6** | 60.0 | **72.8** | **70.1** | **60.4** | 64.1/66.0 | 66.7/65.0 | 59.8/68.1 | 2.9/66.4 |\n|  | Marker | 52.5 | 43.0 | 44.2 | 41.8 | 55.3 | 47.1 | 52.4 | 43.8/47.0 | 42.9/46.6 | 44.3/46.7 | 6.3/46.6 |\n|  | Mathpix | **76.1** | **64.3** | 71.9 | **68.3** | **79.3** | **67.0** | 25.8 | **71.2**/66.4 | **69.8**/67.6 | 60.5/71.8 | **20.7**/68.8 |\n| **Expert Vision Models** | GOT-OCR | 51.9 | 47.0 | 49.4 | 46.2 | 49.3 | 51.6 | 47.2 | 46.5/49.7 | 46.4/49.1 | 40.2/52.7 | 0.0/49.4 |\n|  | Nougat | 36.5 | 0.4 | 0.0 | 6.3 | 3.6 | 22.2 | 0.0 | 15.1/9.1 | 21.2/8.9 | 2.8/15.3 | 0.0/11.4 |\n| **Vision Language Models** | GPT4o | 71.8 | 58.8 | 57.9 | 63.3 | 69.5 | 61.9 | 31.8 | 57.5/65.5 | 61.6/62.9 | **62.0**/63.0 | 14.5/63.5 |\n|  | Qwen2-VL | 57.4 | **62.9** | **72.7** | **70.7** | 64.1 | 48.3 | **57.6** | 49.4/68.2 | 48.5/64.7 | **63.5**/60.7 | **41.6**/61.9 |\n|  | InterVL2 | 61.5 | 59.3 | 65.9 | 59.7 | 66.5 | 58.7 | 56.2 | 49.6/65.9 | 54.4/61.6 | 59.4/60.6 | 7.3/61.1 |", "caption": "Table S1: End-to-End Table TEDS Result grouped by Table Attributes", "description": "This table presents the end-to-end table recognition performance, evaluated using the Tree Edit Distance (TEDS) metric.  Results are broken down by various table attributes such as language (English, Chinese, or mixed), table frame type (full frame, omission line, three lines, or no frame), and presence of special features (merged cells, formulas, colorful background, or rotation). This detailed breakdown allows for a nuanced understanding of how different table characteristics affect model performance.", "section": "III. Model Results Visualization"}, {"content": "| Model Type | Model | Language EN | Language ZH | Language Mixed | Text background White | Text background Single | Text background Multi |\n|---|---|---|---|---|---|---|---| \n| **Pipeline Tools** | MinerU | **0.123** | **0.206** | 0.742 | 0.163 | **0.147** | 0.513 |\n|  | Marker | 0.267 | <ins>0.389</ins> | 0.499 | 0.339 | 0.389 | 0.497 |\n|  | Mathpix | 0.173 | 0.774 | 0.538 | 0.675 | 0.554 | 0.570 |\n| **Expert Vision Models** | GOT-OCR | 0.251 | 0.763 | <ins>0.266</ins> | 0.669 | 0.595 | 0.440 |\n|  | Nougat | 0.587 | 0.991 | 0.983 | 0.874 | 0.935 | 0.972 |\n| **Vision Language Models** | GPT4o | <ins>0.170</ins> | 0.647 | 0.322 | 0.536 | 0.423 | 0.406 |\n|  | Qwen2-VL | 0.337 | 0.575 | 0.310 | 0.537 | 0.400 | <ins>0.233</ins> |\n|  | InternVL2 | 0.418 | 0.606 | **0.251** | 0.589 | <ins>0.366</ins> | **0.221** |", "caption": "Table S2: End-to-End Text Normalized Edit Distance results grouped by Text Attributes. \u201cMixed\u201d represents a mixture of Chinese and English, \u201cSingle\u201d and \u201cMulti\u201d represent single color and multi color.", "description": "Table S2 presents a detailed breakdown of the end-to-end text recognition performance, evaluated using the Normalized Edit Distance metric.  The results are categorized based on three text attributes: language (English, Chinese, or Mixed), text background color (Single or Multi), and text rotation (Normal, Rotate90, Rotate270, or Horizontal). This granular analysis helps to understand how different text characteristics affect the accuracy of the document parsing models.", "section": "Supplementary Material"}, {"content": "| Category | Attribute Name | Count |\n|---|---|---|\n| **PDF Type** | Book | 104 |\n|  | PPT2PDF | 133 |\n|  | Research Report | 81 |\n|  | Colorful Textbook | 96 |\n|  | Exam Paper | 114 |\n|  | Magazine | 97 |\n|  | Academic Literature | 129 |\n|  | Notes | 116 |\n|  | Newspaper | 111 |\n| **Layout Type** | Single Column | 477 |\n|  | Double Column | 126 |\n|  | Three Column | 45 |\n|  | One&More Mixed | 120 |\n|  | Complex Layout | 213 |\n| **Language** | English | 290 |\n|  | Simplified Chinese | 612 |\n|  | Mixed | 79 |\n| **Special Issues** | Fuzzy Scan | 28 |\n|  | Watermark | 65 |\n|  | Colorful Background | 246 |", "caption": "Table S3: The Page Attributes Statistics of OmniDocBench.", "description": "Table S3 presents a detailed breakdown of the statistics for various page attributes within the OmniDocBench dataset.  It shows the count of pages exhibiting specific characteristics like PDF type, layout type, language, and special issues (watermarks, colored backgrounds, fuzzy scans). This provides a comprehensive overview of the dataset's diversity and the distribution of different page attributes.", "section": "3. OmniDocBench Dataset"}, {"content": "| Attribute Category | Category Name | Count |\n|---|---|---|\n| **Language** | English | 5857 |\n|  | Simplified Chinese | 16073 |\n|  | EN&CH Mixed | 1080 |\n| **Text Background** | White | 19465 |\n|  | Single-Colored | 1116 |\n|  | Multi-Colored | 2429 |\n| **Text Rotate** | Normal | 22865 |\n|  | Rotate90 | 14 |\n|  | Rotate270 | 58 |\n|  | Horizontal | 421 |", "caption": "Table S6: Annotation Explanations and Statistics.", "description": "Table S6 provides detailed explanations and statistics for each annotation category in the OmniDocBench dataset.  It lists the category name, a description of what constitutes that category, and the total count of annotations within that category. The categories include various layout elements (titles, text blocks, figures, tables, etc.) and their associated captions and footnotes, as well as structural elements (headers, footers, page numbers), and special annotations (masked regions of the page due to interference).  The table is crucial for understanding the composition and complexity of the OmniDocBench dataset.", "section": "3. Dataset Statistics"}]