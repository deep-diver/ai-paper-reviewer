[{"figure_path": "https://arxiv.org/html/2412.07626/x1.png", "caption": "Figure 1: OmniDocBench Data Diversity. It contains 9 PDF page types, along with Layout Annotations and Recognition Annotations. Furthermore, there are 5 Page Attributes, 3 Text Attributes, and 6 Table Attributes.", "description": "Figure 1 illustrates the diversity of data within the OmniDocBench dataset.  It showcases nine different types of PDF pages included in the benchmark: invoices, academic papers, books, textbooks, magazines, notes, newspapers, financial reports, and slides.  The figure highlights that each page type is annotated with both layout annotations (describing the structural elements of the page like text blocks, tables, figures) and recognition annotations (identifying and classifying content within these elements, such as text, formulas, and tables).  Beyond the page content, the annotations also capture metadata, including 5 page attributes (like language and whether it has a watermark), 3 text attributes (like text language and color), and 6 table attributes (like frame type and cell merging). This comprehensive annotation provides a multi-level assessment capability across various aspects of document parsing.", "section": "3. OmniDocBench Dataset"}, {"figure_path": "https://arxiv.org/html/2412.07626/x2.png", "caption": "Figure 2: Overview of the OmniDocBench dataset construction.", "description": "This figure illustrates the process of creating the OmniDocBench dataset.  It starts with data acquisition from various web sources and internal data, resulting in 200,000 initial PDF documents.  These are then filtered down to 6,000 visually diverse pages through a feature clustering and sampling process.  A manual selection step balances the dataset across page types and attributes to a final 981 pages.  Annotation then involves stages of automated annotation using state-of-the-art vision models, manual corrections by annotators, and finally, expert quality inspection by PhD-level researchers to ensure accuracy.  This multi-stage process generates layout and content annotations, which are then used to build the dataset.", "section": "3. OmniDocBench Dataset"}, {"figure_path": "https://arxiv.org/html/2412.07626/x3.png", "caption": "Figure 3: OmniDocBench Evaluation Pipeline.", "description": "This figure illustrates the detailed evaluation pipeline used in the OmniDocBench benchmark. It shows the process flow, starting from model predictions (markdown, LaTeX, HTML, etc.) that are preprocessed and then matched to the ground truth annotations. The pipeline includes stages for extracting special components (tables, formulas, code blocks), extracting pure text, converting inline formula formats, and handling reading order. Finally, the pipeline calculates several metrics to assess the quality of document content extraction.", "section": "4. OmniDocBench Evaluation Methodology"}, {"figure_path": "https://arxiv.org/html/2412.07626/x4.png", "caption": "Table S4: Text Attributes Statistics of OmniDocBench.", "description": "Table S4 provides a statistical overview of text attributes within the OmniDocBench dataset.  It details the count of each attribute type, offering insights into the diversity of text characteristics in the dataset. Attributes include language (English, Simplified Chinese, and mixed), text background color (white, single-colored, and multi-colored), and text rotation (normal, rotated 90\u00b0, 270\u00b0, and horizontal). This table is crucial for understanding the complexity and diversity of the OmniDocBench dataset and how these attributes influence the performance of different document parsing algorithms.", "section": "3.3 Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2412.07626/x5.png", "caption": "Table S5: Table Attributes Statistics of OmniDocBench.", "description": "Table S5 presents a statistical overview of the Table Attributes within the OmniDocBench dataset.  It details the frequency of different table attributes, such as language (English, Simplified Chinese, or mixed), frame type (full frame, omission line, three lines, or no frame), special situations (merged cells, presence of formulas, colorful background, or rotation), providing insights into the diversity and complexity of tables included in the benchmark dataset.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x6.png", "caption": "Figure S1: The Data Proportion of Pages for each Attribute in OmniDocBench.", "description": "This figure shows the distribution of various attributes across the OmniDocBench dataset.  It visually represents the percentage of pages that possess each attribute, offering insight into the dataset's diversity and the prevalence of specific characteristics in the collected documents. Attributes may include page type (e.g., academic paper, newspaper), layout type (single column, double column), language (English, Chinese), and special characteristics (e.g., watermark, fuzzy scan, colored background). The visualization allows for a quick understanding of the dataset's composition and balance across different document attributes.", "section": "3. Dataset Statistics and Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x7.png", "caption": "Figure S2: The Visualization of vary Annotations in OmniDocBench.", "description": "Figure S2 visualizes the comprehensive annotation framework used in OmniDocBench.  It showcases the diversity of annotations applied across different page types, including bounding boxes for various content elements (text, tables, figures, etc.), layout attributes (columns, frames, rotations), reading order, and text attributes (language, background color). This visualization demonstrates OmniDocBench's rich annotation detail, highlighting the complexity and nuance captured for robust evaluation of document parsing models.", "section": "3. OmniDocBench Dataset"}, {"figure_path": "https://arxiv.org/html/2412.07626/x8.png", "caption": "Figure S3: The Examples of Academic Papers, Books, Textbooks, Notes, and Magazines in OmniDocBench.", "description": "This figure visually showcases the diversity of document types included in the OmniDocBench dataset.  It provides example pages from various sources such as academic papers, textbooks, notes, books, and magazines, to illustrate the wide range of document layouts and content included in the benchmark. The goal is to visually demonstrate the breadth and complexity of document types present in OmniDocBench, highlighting the challenge involved in creating a robust, diverse and fair evaluation standard for document content extraction.", "section": "3. OmniDocBench Dataset"}, {"figure_path": "https://arxiv.org/html/2412.07626/x9.png", "caption": "Figure S4: The Examples of Finacial Reports, Newspapers, Example Papers, and Slides in OmniDocBench.", "description": "Figure S4 presents a visual representation of the diversity within the OmniDocBench dataset.  It showcases examples of four distinct document categories included in the benchmark: financial reports, newspapers, exam papers, and slides. Each category displays several sample pages, highlighting the variety in layout, structure, content type, language, and visual elements found within real-world documents. This diversity is crucial in evaluating the robustness and generalization capabilities of document parsing models.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x10.png", "caption": "Figure S5: The Examples of PDF pages with different Layout Types in OmniDocBench.", "description": "Figure S5 presents various examples of PDF pages from the OmniDocBench dataset, categorized by their layout structures.  Each example visually demonstrates different layout styles including single-column, double-column, three-column, and complex layouts. This showcases the diversity of document layouts encompassed within the OmniDocBench benchmark, highlighting its capacity to evaluate document parsing models' ability to handle diverse page designs.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x11.png", "caption": "Figure S6: The Examples of PDF pages under Special Issues in OmniDocBench.", "description": "Figure S6 presents example PDF pages from OmniDocBench that exhibit various special issues commonly encountered in real-world document processing.  These issues include: pages with fuzzy scans (blurry text), watermarks obscuring content, and pages with colorful backgrounds that can interfere with text extraction and layout analysis.  These examples showcase the challenges that a robust document parsing model must address to achieve high performance on diverse and imperfect document scans.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x12.png", "caption": "Figure S7: The Examples of Tables with different Frame in OmniDocBench.", "description": "This figure showcases various table examples from the OmniDocBench dataset, highlighting the diversity in table frames.  It visually demonstrates the different types of frames present in the dataset, including tables with full frames, tables with omission lines, tables with three lines, and tables without any frames. This variety is crucial for evaluating table recognition models and ensures they are tested against realistic scenarios.", "section": "3. OmniDocBench Dataset"}, {"figure_path": "https://arxiv.org/html/2412.07626/x13.png", "caption": "Figure S8: The Examples of Tables under Special Issues in OmniDocBench.", "description": "This figure showcases examples of tables within the OmniDocBench dataset that present special characteristics or issues. These special cases highlight the challenges in real-world document parsing, such as tables with merged cells, those containing formulas, tables with colorful backgrounds, or tables that have been rotated.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x14.png", "caption": "Figure S9: The Good Model Result and Bad Model Result for Academic Papers.", "description": "This figure in the supplementary material showcases a comparison of results from a good-performing model and a poorly performing model when processing academic papers.  It visually demonstrates the differences in terms of accuracy and completeness of content extraction. The figure highlights the superior performance of the good model in accurately identifying and extracting textual content, formulas, tables, and other key elements within academic papers, compared to the inferior results of the other model, which may miss or incorrectly extract components.", "section": "Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x15.png", "caption": "Figure S10: The Good Model Result and Bad Model Result for Books.", "description": "Figure S10 presents a comparison of the results produced by a high-performing model (good model) and a poorly performing model (bad model) when processing book-type PDF pages.  The figure visually showcases how well each model extracts and presents the content, highlighting the differences in accuracy and completeness of text, table, formula, image extraction, and overall layout interpretation.", "section": "Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x16.png", "caption": "Figure S11: The Good Model Result and Bad Model Result for Exam Papers.", "description": "This figure shows a comparison of how well different models perform on exam papers from the OmniDocBench dataset.  The \"good model result\" side displays examples where the model accurately extracts and formats the text and other elements of the exam paper.  Conversely, the \"bad model result\" side presents instances where the model struggles with accurate extraction and formatting, highlighting common issues like incorrect recognition of text blocks, layout misinterpretations, or missing content. This comparison is crucial for understanding the limitations of different models in handling complex document structures and content variations frequently found in exam papers.", "section": "Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x17.png", "caption": "Figure S12: The Good Model Result and Bad Model Result for Magazines.", "description": "Figure S12 presents a comparison of how well different models perform on magazine pages.  The \"good model\" example shows accurate extraction of text, images, and layout elements with minimal errors. In contrast, the \"bad model\" example highlights common issues in automated magazine parsing such as incomplete text extraction, incorrect layout recognition, and the inability to properly handle complex visual elements. This illustrates the challenges involved in processing visually rich and diverse document layouts and the varying capabilities of different models.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x18.png", "caption": "Figure S13: The Good Model Result and Bad Model Result for Newspaper.", "description": "This figure in the supplementary materials presents a comparison of how well different document parsing models perform on newspaper content.  It visually shows examples of successful extractions (good model results) and examples of incorrect or missing information (bad model results) from the same newspaper page.  This allows for a direct comparison of the accuracy and completeness of different models in handling this specific type of document.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x19.png", "caption": "Figure S14: The Good Model Result and Bad Model Result for Handwriting Notes.", "description": "Figure S14 presents a comparison of how well different models (Mineru and InternVL2) perform on handwritten notes.  The \"Good Model Result\" shows accurate transcription of the handwritten text by Mineru. The \"Bad Model Result\" shows that InternVL2 struggles with accurate transcription and experiences problems, indicated by missing parts of text represented with \"---Handle Writing Text Missing---\". This highlights the challenge of processing handwritten documents, a common problem in document parsing.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x20.png", "caption": "Figure S15: The Good Model Result and Bad Model Result for Financial Reports.", "description": "This figure in the supplementary materials presents a comparison of the financial report parsing results produced by different models.  It showcases examples where models successfully extracted key information (good model results) versus cases where the results contained errors, omissions, or other issues (bad model results).  The visualization likely highlights the differences in accuracy and effectiveness between different methods on a specific document type, namely financial reports.", "section": "Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x21.png", "caption": "Figure S16: The Good Model Result and Bad Model Result for Slides.", "description": "This figure displays a comparison of how well different models perform on slide-type documents.  It shows examples of good and bad model outputs, highlighting the strengths and weaknesses of various algorithms in accurately extracting and representing the visual and textual content of slides.  Specific examples may include issues with layout analysis, text recognition, or the handling of diagrams and other non-textual elements frequently found in presentations.", "section": "Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x22.png", "caption": "Figure S17: The Good Model Result and Bad Model Result for Textbooks.", "description": "This figure in the supplementary materials visually compares the results of document content extraction from textbook pages using a good-performing model versus a poorly performing model.  It highlights the differences in accuracy and completeness of the extracted information, showcasing the challenges associated with parsing complex layouts and formatting commonly found in textbooks. The differences shown help demonstrate the importance of a comprehensive evaluation benchmark such as OmniDocBench.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x23.png", "caption": "Figure S18: The Good Model Result and Bad Model Result for Fuzzy Scan Pages.", "description": "This figure in the supplementary material showcases the results of document parsing on PDF pages with fuzzy scans, comparing the output of well-performing models (good results) against those of poorly-performing models (bad results).  It visually demonstrates the challenges posed by low-quality scans in the context of automated document parsing and the varying capabilities of different methods in handling such images.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x24.png", "caption": "Figure S19: The Good Model Result and Bad Model Result for Pages with Watermark.", "description": "This figure in the supplementary material section visualizes the performance difference between good and bad models in handling PDF pages containing watermarks. It showcases examples of pages with watermarks and how different models either successfully extract the content or fail due to the presence of the watermarks. This helps demonstrate the robustness and limitations of various models in managing challenging real-world scenarios.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x25.png", "caption": "Figure S20: The Good Model Result and Bad Model Result for Colorful Background Pages.", "description": "This figure in the supplementary material showcases examples from OmniDocBench where pages have colorful backgrounds.  It presents a comparison between the results produced by a high-performing model (the \"Good Model\") and a model that struggles with colorful backgrounds (the \"Bad Model\").  The goal is to highlight how well different models handle challenging scenarios, such as complex visual backgrounds that might interfere with accurate document content extraction.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x26.png", "caption": "Figure S21: The Good Model Result and Bad Model Result for Single Column Pages.", "description": "This figure showcases a comparison of the results from good and bad models when processing single-column PDF pages.  It visually demonstrates the differences in accuracy and effectiveness of various models in extracting information and maintaining proper layout from this specific page type.  The visual comparison highlights the strengths and weaknesses of each model in handling text and structural elements within single-column layouts.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x27.png", "caption": "Figure S22: The Good Model Result and Bad Model Result for Double Column Pages.", "description": "This figure showcases a comparison of how well different models handled double-column layouts in PDF documents.  It presents examples of a 'good' model's output (correctly parsing the text and layout) alongside examples from a 'bad' model (failing to accurately represent the document structure).  This visualization helps illustrate the challenges inherent in processing complex document layouts and how different model architectures tackle (or fail to tackle) these challenges.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x28.png", "caption": "Figure S23: The Good Model Result and Bad Model Result for Three Column Pages.", "description": "This figure showcases the results of document parsing on pages with three columns using two different models. The \"Good Model Result\" demonstrates high accuracy in content extraction, maintaining proper column separation and order. In contrast, the \"Bad Model Result\" shows errors in column identification, content merging across columns, and disrupted reading order, highlighting the challenges of three-column layout parsing.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x29.png", "caption": "Figure S24: The Good Model Result and Bad Model Result for Complex Layout Pages.", "description": "Figure S24 showcases a comparison of how well different models parse complex page layouts in the OmniDocBench dataset.  The figure visualizes the output of a model considered to perform well ('good model'), juxtaposed with the output of a model that does not perform as well ('bad model'). This comparison helps illustrate the varying levels of accuracy and effectiveness in handling complex document structures and is part of a larger evaluation within the paper.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x30.png", "caption": "Figure S25: The Good Model Result and Bad Model Result for Text Language in Chinese.", "description": "This figure in the supplementary material showcases a comparison between the results of good and bad models when processing text written in Chinese. The comparison focuses on how well each model can extract and interpret text, highlighting the differences in accuracy and robustness between effective and less effective approaches in handling the nuances of the Chinese language.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x31.png", "caption": "Figure S26: The Good Model Result and Bad Model Result for Text Language in English.", "description": "This figure compares the results of two different models (a good model and a bad model) when processing text in English.  It visually showcases the differences in accuracy and how effectively each model handles English text within the context of document parsing.", "section": "V. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x32.png", "caption": "Figure S27: The Good Model Result and Bad Model Result for Text with Colorful Background.", "description": "This figure in the supplementary material section visualizes the results of text recognition models' performance on PDF pages with colorful backgrounds.  It directly compares the output of a high-performing model (the 'Good Model') against a lower-performing model (the 'Bad Model'). This allows for a visual inspection of how the models handle text extraction when faced with complex backgrounds. Differences in accuracy and ability to correctly interpret text can be observed.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x33.png", "caption": "Figure S28: The Bad Model Result for Text with Rotation.", "description": "This figure in the supplementary material section visualizes the poor performance of a model in recognizing text when it is rotated.  It shows the ground truth (correct text) alongside the model's inaccurate transcription of the rotated text.  This highlights the model's limitations in handling text orientation variations, which is a common challenge in document image analysis.", "section": "III. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x34.png", "caption": "Figure S29: The Good Model Result and Bad Model Result for Three Line Frame Table.", "description": "This figure showcases a comparison between the results of a high-performing model (good model) and a lower-performing model (bad model) for tables with three lines in their frames. It highlights the differences in accuracy and effectiveness of different models in parsing tables with specific characteristics, such as line style or quantity.", "section": "V. Model Results Visualization"}, {"figure_path": "https://arxiv.org/html/2412.07626/x35.png", "caption": "Figure S30: The Good Model Result and Bad Model Result for No Frame Table.", "description": "This figure showcases a comparison of how different models perform on tables without frames. It highlights the discrepancies in table recognition accuracy between a well-performing model (good model) and a poorly performing model (bad model). The visual comparison demonstrates the challenges posed by the absence of frames in accurate table extraction.", "section": "III. Model Results Visualization"}]