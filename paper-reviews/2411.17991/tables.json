[{"content": "|                     | num examples | answers per video | words per ques./ans. | video seg. len (sec) |\n| :------------------ | :------------ | :----------------- | :------------------ | :------------------ |\n| Train                | 36834         | 2.96               | 7.75/12.17           | 4.22                 |\n| Test                 | 2000          | 3.04               | 7.77/12.17           | 4.28                 |", "caption": "Table 1: Dataset statistics of Shot2story-MAGQA-39k.", "description": "This table presents a statistical overview of the Shot2story-MAGQA-39k dataset, which is used for training and evaluating the Multi-Answer Grounded Video Question Answering (MAGQA) task.  The dataset is built upon the Shot2Story dataset. It shows the number of training and testing samples, the average number of answers per video, and the average length (in words and seconds) of the questions and answers, and video segments.", "section": "5. MMDuetIT: Dataset for Training MMDuet"}, {"content": "| Method | QVHighlights mAP/HIT@1 | Charades-STA R@0.5/0.7 |\n|---|---|---|\n| TimeChat | 14.5/23.9 | 32.2/13.4 |\n| VTimeLLM | - | 31.2/11.4 |\n| HawkEye | - | 31.4/14.5 |\n| VTG-LLM | 16.5/33.5 | 33.8/15.7 |\n| LLaVA-OV-TC | 17.6/32.9 | 33.1/12.4 |\n| LLaVA-OV-VT | 19.0/40.0 | 36.5/12.3 |\n| MMDuet (Ours) | **31.3/49.6** | **42.4/18.0** |", "caption": "Table 2: Zero-shot performance on highlight detection and temporal video grounding.", "description": "This table presents a comparison of the zero-shot performance of different video large language models (VideoLLMs) on two time-sensitive video tasks: highlight detection and temporal video grounding.  It shows the performance metrics (mAP and HIT@1 for highlight detection, R@0.5 and R@0.7 for temporal video grounding) achieved by each model.  The models include several baselines (TimeChat, VTimeLLM, HawkEye, and VTG-LLM) and two controlled experiments (LLaVA-OV-TC and LLaVA-OV-VT) using the same initialization model and training data but different interaction formats, as well as the proposed model, MMDuet.  The table highlights the superior performance of MMDuet, which demonstrates significant improvements in both tasks.", "section": "6.1. Highlight Detection and Temporal Video Grounding"}, {"content": "| Dataset | SODAc/CIDEr/F1 |\n|---|---| \n| YouCook2 |  |\n| TimeChat | 1.2/3.4/12.6 |\n| VTG-LLM | 1.5/5.0/17.5 |\n| LLaVA-OV-TC | 1.9/3.3/**21.8** |\n| LLaVA-OV-VT | 2.5/6.7/14.0 |\n| MMDuet (Ours) | 2.4/5.7/19.2 |\n| + rm. prev. resp. | **2.9/8.8**/21.7 |", "caption": "Table 3: Zero-shot performance on dense video captioning.", "description": "This table presents the results of a zero-shot evaluation on the dense video captioning task. It compares the performance of the proposed MMDuet model against several baseline models, including TimeChat, VTimeLLM, LLaVA-OV-TC, and LLaVA-OV-VT.  The performance is measured using three metrics: SODAC, CIDEr, and F1 score.  These metrics assess different aspects of the generated captions, reflecting the model's ability to accurately describe the video content and its temporal structure.", "section": "6.1. Highlight Detection and Temporal Video Grounding"}, {"content": "| Model | Real-Time? | In-Span Score\nLLaMA/GPT | # turns (w/o./ w/. dedup) | time per\nexample |\n|---|---|---|---|---|\n| *Baselines* |  |  |  |  |\n| LLaVA-OV-TC | 2718 | 2.92/2.79 | 3.4/1.9 | **0.76** |\n| LLaVA-OV-VT | 2718 | 2.94/2.78 | 5.4/2.2 | 1.00 |\n| *MMDuet (Ours)* |  |  |  |  |\n| *t*=0.6 | 2714 | 2.46/2.33 | 13.7/4.0 | 1.80 |\n| *t*=0.5 | 2714 | 2.77/2.61 | 18.4/5.3 | 2.23 |\n| *t*=0.4 | 2714 | 3.00/2.81 | 23.0/6.6 | 2.59 |\n| *t*=0.3 | 2714 | **3.13/2.93** | 27.0/7.6 | 2.73 |", "caption": "Table 4: Results on the test set of Shot2Story-MAGQA-39k with the rm. ass. turns method used. For the \u201ctime per example\u201d column, the time used by \u201cLLaVA-OV-VT\u201d is set to 1, and the times for other rows are set as multiples of the time used by \u201cLLaVA-OV-VT\u201d.", "description": "This table presents the results of the Multi-Answer Grounded Video Question Answering (MAGQA) task on the Shot2Story-MAGQA-39k test set.  The \"rm. ass. turns\" method, which removes previously generated assistant turns from the context, was employed. The 'In-Span Score' metric is used to evaluate the model's performance, considering both the accuracy of the text response and the timeliness of the answer relative to the video's content.  The time taken for each example is reported, normalized to the time taken by the LLaVA-OV-VT baseline model (set to 1).  Other models' times are represented as multiples of the baseline time.", "section": "6.3. Multi-Answer Grounded Video QA"}, {"content": "| Model | YouCook2 |\n|---|---| \n| MMDuet | 2.9/8.8/21.7 |\n| w/o rand. resp. pos. | 2.1/7.3/19.0 |\n| w/o multi informative | 2.9/8.0/16.5 |", "caption": "Table 5: Ablation study on training methods.", "description": "This ablation study analyzes the impact of different training methodologies on the performance of the MMDuet model, specifically focusing on the YouCook2 dense video captioning task. It investigates the effects of removing previous responses during training, disabling the random response position selection, and disabling the multi-informative labeling strategy.  The results quantify the contribution of each training method to the model's overall performance.", "section": "6. Experiments"}, {"content": "| Hyper-parameter | value |\n|---|---| \n| `batch_size` | 1 |\n| `gradient_acc_steps` | 8 |\n| `learning_rate` | 2e-5 |\n| `warmup_ratio` | 0.05 |\n| `lora_r` | 16 |\n| `lora_alpha` | 32 |\n| `attn_implementation` | sdpa |", "caption": "Table 6: Hyper-parameters used for training MMDuet.", "description": "This table lists the hyperparameters used during the training of the MMDuet model.  It shows the values assigned to each hyperparameter, including batch size, gradient accumulation steps, learning rate, warmup ratio, LoRA rank, LoRA alpha, attention implementation, and the number of epochs.", "section": "4. MMDuet: Our Proposed VideoLLM"}, {"content": "| Task | Description |\n|---|---| \n| Dense Video Captioning | <https://arxiv.org/html/2411.17991/system.png>system<br> A multimodal AI assistant is helping users with some activities. Below is their conversation, interleaved with the list of video frames received by the assistant. <https://arxiv.org/html/2411.17991/user.png>user<br>(A Dense Video Captioning Query)<br> <https://arxiv.org/html/2411.17991/stream.png>stream<br><frame><frame><frame> \u2026 <https://arxiv.org/html/2411.17991/assistant.png>assistant<br>A person pulls a knife from a black bag.<br><https://arxiv.org/html/2411.17991/stream.png>stream<br><frame><frame><frame> \u2026 <https://arxiv.org/html/2411.17991/assistant.png>assistant<br>A man in a hat and red clothes speaks with a dagger, and a tree behind him.<br><https://arxiv.org/html/2411.17991/stream.png>stream<br><frame><frame><frame> \u2026 (More stream and assistant turns) |\n| MAGQA | <https://arxiv.org/html/2411.17991/system.png>system<br>A multimodal AI assistant is helping users with some activities. Below is their conversation, interleaved with the list of video frames received by the assistant. <https://arxiv.org/html/2411.17991/stream.png>stream<br><frame><frame><frame> \u2026 <https://arxiv.org/html/2411.17991/user.png>user<br>What happens during the basketball game?<br><https://arxiv.org/html/2411.17991/stream.png>stream<br><frame><frame><frame> \u2026 <https://arxiv.org/html/2411.17991/assistant.png>assistant<br>Several players in white jerseys are celebrating by high-fiving each other.<br><https://arxiv.org/html/2411.17991/stream.png>stream<br><frame><frame><frame> \u2026 (More stream and assistant turns) |\n| Temporal Video Grounding | <https://arxiv.org/html/2411.17991/system.png>system<br>A multimodal AI assistant is helping users with some activities. Below is their conversation, interleaved with the list of video frames received by the assistant. <https://arxiv.org/html/2411.17991/user.png>user<br>(A Temporal Video Grounding Query)<br><https://arxiv.org/html/2411.17991/stream.png>stream<br><frame><frame><frame> \u2026 |", "caption": "Table 7: Input examples of different tasks during the training and evaluation phase of MMDuet.", "description": "This table showcases example inputs for three different tasks used in training and evaluating the MMDuet model: dense video captioning, multi-answer grounded video question answering (MAGQA), and temporal video grounding.  Each task example shows the system prompt, user input, video frame sequence, and model's response, illustrating the video-text duet interaction format.", "section": "5. MMDuetIT: Dataset for Training MMDuet"}]