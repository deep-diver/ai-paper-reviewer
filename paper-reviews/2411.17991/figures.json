[{"figure_path": "https://arxiv.org/html/2411.17991/x1.png", "caption": "Figure 1: An example of the common Whole Video Interaction Format and our Video-Text Duet Interaction Format.", "description": "This figure illustrates the difference between the traditional \"Whole Video Interaction Format\" and the novel \"Video-Text Duet Interaction Format\" proposed in the paper.  In the Whole Video format, the user provides a text query and the entire video as input at once. The model processes this information and generates a single response after the input is complete.  This approach is unsuitable for time-sensitive video comprehension tasks, real-time scenarios (like live streams), and long videos. In contrast, the Video-Text Duet format involves the continuous playback of a video while both the user and the model can insert text messages at any point during playback. The video continues to play after each text response, mimicking a musical duet.  This interactive approach is designed to handle real-time responses, time-sensitive tasks, and lengthy video content.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.17991/x2.png", "caption": "Figure 2: Example of reformatting the annotation of a video segment to video-text duet interaction format in MMDuetIT. Information from the original annotation is emphasized with underlines.", "description": "This figure demonstrates how the annotation of a video segment is converted to the video-text duet interaction format used in the MMDuetIT dataset.  The original video segment has timestamps at the start, middle (50%), and end.  The caption/response is inserted at a random time between 50% and 75% of the segment duration.  Informative head labels are also generated to indicate whether each frame contains new information (1) or not (0) relative to the caption.  Specifically, frames from 50% to the insertion point are labeled '1', while all other frames are labeled '0'. This process ensures the model learns to respond at appropriate moments based on the temporal flow of information.", "section": "5. MMDuetIT: Dataset for Training MMDuet"}, {"figure_path": "https://arxiv.org/html/2411.17991/x3.png", "caption": "Figure 3: Data Distribution of MMDuetIT.", "description": "This figure shows the distribution of data samples across different tasks within the MMDuetIT dataset.  It visually represents the number of samples used for each task, namely dense captioning, multi-answer grounded video question answering, and temporal video grounding, along with their respective sub-datasets.", "section": "5. MMDuetIT: Dataset for Training MMDuet"}, {"figure_path": "https://arxiv.org/html/2411.17991/x6.png", "caption": "Figure 4: Performance on temporal video grounding and highlight detection with different w\ud835\udc64witalic_w.", "description": "This figure shows the impact of the smoothing window size (w) on the performance of temporal video grounding and highlight detection tasks.  Different values of w were tested, each representing the number of preceding and following frames considered when calculating a smoothed relevance score. The results illustrate how varying w affects the model's ability to accurately localize relevant video segments for these two tasks. It demonstrates the sensitivity of the performance to the choice of w and potentially indicates an optimal range for achieving best results.", "section": "6.1. Highlight Detection and Temporal Video Grounding"}, {"figure_path": "https://arxiv.org/html/2411.17991/x7.png", "caption": "Figure 5: Performance on dense video captioning with different s\ud835\udc60sitalic_s.", "description": "This figure displays the performance of the model on the dense video captioning task using different threshold values (represented by 's'). It shows how changing the threshold for determining when the model should generate a response affects the overall performance of the task.  The x-axis likely represents different threshold values, while the y-axis likely shows a performance metric, such as CIDEr score or BLEU score, which are commonly used to evaluate the quality of generated captions.  The plot likely compares the model's performance against various baseline methods, allowing a visual assessment of how well the model performs dense captioning under different thresholds.", "section": "6.2. Dense Video Captioning"}, {"figure_path": "https://arxiv.org/html/2411.17991/x8.png", "caption": "Figure 6: An example of dense video captioning with MMDuet, LLaVA-OV-TC and LLaVA-OV-VT.", "description": "This figure showcases a comparison of dense video captioning results across three different models: MMDuet, LLaVA-OV-TC, and LLaVA-OV-VT.  It visually demonstrates the different approaches each model takes to captioning a cooking video, highlighting differences in the granularity, timing, and overall accuracy of the generated captions. The video frames are displayed alongside the corresponding captions generated by each model, allowing for a direct visual comparison of their performance.", "section": "6.1. Highlight Detection and Temporal Video Grounding"}, {"figure_path": "https://arxiv.org/html/2411.17991/x9.png", "caption": "Figure 7: An example of dense video captioning with MMDuet, LLaVA-OV-TC and LLaVA-OV-VT.", "description": "Figure 7 presents a comparative analysis of dense video captioning across three different models: MMDuet, LLaVA-OV-TC, and LLaVA-OV-VT.  It showcases the generated captions for a cooking video, highlighting the differences in the accuracy, detail, and timeliness of the captions produced by each model. The figure visually displays the video frames alongside the corresponding captions, allowing for a direct comparison of the models' performance on this time-sensitive task.  The results show how MMDuet's video-text duet interaction format allows for more precise and detailed descriptions by focusing on smaller video segments, unlike the other two methods.", "section": "6.2. Dense Video Captioning"}, {"figure_path": "https://arxiv.org/html/2411.17991/x10.png", "caption": "Figure 8: An example of dense video captioning with MMDuet, LLaVA-OV-TC and LLaVA-OV-VT.", "description": "This figure shows a comparison of dense video captioning results between three different models: MMDuet, LLaVA-OV-TC, and LLaVA-OV-VT.  The figure displays a sequence of video frames from a cooking video along with the corresponding captions generated by each model. It visually demonstrates the differences in captioning accuracy, detail, and temporal alignment achieved by the three models, highlighting the strengths of MMDuet in generating more comprehensive and temporally precise descriptions.", "section": "6.2. Dense Video Captioning"}, {"figure_path": "https://arxiv.org/html/2411.17991/x11.png", "caption": "Figure 9: An example of multi-answer grounded video question answering with MMDuet, LLaVA-OV-TC and LLaVA-OV-VT.", "description": "Figure 9 presents a comparative analysis of three different video question-answering models: MMDuet, LLaVA-OV-TC, and LLaVA-OV-VT.  A video depicting a man driving a car with a child is used as input. The models are tasked with answering the question: \"What are the people doing in the car?\"  The figure showcases the differences in the models' capabilities regarding the generation of multiple, temporally grounded answers in response to a single question.  MMDuet shows its ability to provide multiple answers accurately placed relative to the video timeline, highlighting its effectiveness in the multi-answer grounded video question answering task.", "section": "6.3. Multi-Answer Grounded Video QA"}, {"figure_path": "https://arxiv.org/html/2411.17991/x12.png", "caption": "Figure 10: An example of multi-answer grounded video question answering with MMDuet, LLaVA-OV-TC and LLaVA-OV-VT.", "description": "This figure showcases a qualitative comparison of three different video-language models: MMDuet, LLaVA-OV-TC, and LLaVA-OV-VT, on a multi-answer grounded video question answering task.  A video clip is shown with timestamps indicating when each model generates an answer.  The question posed is: \"What happens to the rabbit and the duck?\" Each model\u2019s responses are listed below the video.  The responses highlight the differences in the models' abilities to provide temporally grounded answers accurately and completely, and the varying degrees to which they can handle complex events. The comparison demonstrates that MMDuet excels in providing more temporally specific and detailed answers.", "section": "6.3. Multi-Answer Grounded Video QA"}]