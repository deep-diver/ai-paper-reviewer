[{"content": "| Method | Model Condition Input | Model Condition Training-free | Val Seen SR \u2191 | Val Seen SPL \u2191 | Val Seen NQ \u2193 | Val Seen Synonym SR \u2191 | Val Seen Synonym SPL \u2191 | Val Seen Synonym NQ \u2193 | Val Unseen SR \u2191 | Val Unseen SPL \u2191 | Val Unseen NQ \u2193 | Train SR \u2191 | Train SPL \u2191 | Train NQ \u2193 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Monolithic [10] (_CVPR-24_) | d | \u2717 |  | 4.88 | 2.93 | - | 8.33 | 1.54 | - | 0 | 0 | - | 6.58 | 4.08 | - |\n| PSL [35] (_ECCV-24_) | d | \u2717 |  | 10.98 | 5.71 | - | 8.33 | 3.06 | - | 4.26 | 2.67 | - | 4.61 | 1.39 | - |\n| VFLM [44] (_ICRA-24_) | c | \u2714 |  | 0 | 0 | - | 0 | 0 | - | 0 | 0 | - | 0.66 | 0.61 | - |\n| AIUTA | c | \u2714 | 8.64 | 3.2 | 1.57 | 11.11 | 3.32 | 1.50 | 12.77 | 9 | 1.66 | 14.47 | 7.22 | 1.68 |", "caption": "Table 1: Results of the proposed AIUTA compared with baselines on the CoIN-Bench benchmark. We analyzed the SR (main metric, in gray), SPL, and the number of questions NQ. For each model, we indicate whether it operates in a training-free manner. Additionally, we specify the input type: c denotes models that utilize only the object category as input, while d models that use its associated description.", "description": "Table 1 presents a comparison of the proposed AIUTA model against existing state-of-the-art methods on the CoIN-Bench benchmark.  The table evaluates performance across multiple metrics: Success Rate (SR, highlighted as the primary metric), Success Rate weighted by Path Length (SPL), and the average Number of Questions (NQ) asked during the navigation task.  Crucially, the table also indicates whether each model is training-free and specifies the input type it uses: 'c' for models utilizing only the object category and 'd' for models utilizing a detailed description of the object. This detailed breakdown allows for a comprehensive comparison of model performance, considering both accuracy and efficiency, as well as the level of user input required.", "section": "5. COIN-Bench"}, {"content": "| User type | Input | CoIN-Bench subset |  |  |  |\n|---|---|---|---|---|---| \n|  |  |  | **SR \u2191** | **SPL \u2191** | **NQ \u2193** |\n| VLM | c |  | 68.75 | 32.35 | 1.25 |\n| Real Human | c |  | **87.50** | 43.96 | 1.38 |\n| VLM | I* |  | 77.08 | 40.52 | 0.12 |\n| Real Human | I* |  | **81.25** | 41.84 | 0.06 |", "caption": "Table 2: Results on a subset of CoIN-Bench between real human vs our simulated VLM, using as input either category c\ud835\udc50citalic_c or instruction I\u2217superscript\ud835\udc3cI^{*}italic_I start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT containing arbitrary details regarding the target instance.", "description": "This table presents a comparison of the performance of the AIUTA model on a subset of the CoIN-Bench dataset, using two different types of user input: category label only and natural language instruction with detailed description.  The success rate (SR), success rate weighted by path length (SPL), and average number of questions asked (NQ) are reported for both real human users and VLM-simulated users. This allows evaluating the impact of different levels of user input detail on the navigation performance, comparing human users with the simulated ones.", "section": "Experiments"}, {"content": "| Self-Questioner | Skip-Question |  | SR \u2191 | SPL \u2191 | NQ \u2193 |\n|---|---|---|---|---|---|---|\n| \u2717 | \u2717 |  | 9.21 | 5.86 | 3.57 |\n| \u2717 | \u2714 |  | 8.55 | 4.84 | 2.69 |\n| \u2714 | \u2717 |  | 9.87 | 6.5 | 4.6 |\n| \u2714 | \u2714 |  | **14.47** | **7.22** | **1.68** |", "caption": "Table 3: Ablation of components in AIUTA on the Train split.", "description": "This table presents an ablation study analyzing the impact of different components within the AIUTA framework.  It specifically focuses on the 'Train' split of the CoIN-Bench dataset. The results show the success rate (SR), success rate weighted by path length (SPL), and the average number of questions asked (NQ) for various configurations of the AIUTA model, demonstrating the contributions of the Self-Questioner and Interaction Trigger modules.  The table helps to understand which parts of AIUTA are most important for achieving optimal performance.", "section": "4. Proposed Method"}, {"content": "| VLM Model | Selection Function | \n|---|---|---|\n| LLaVA llava-v1.6-mistral-7b-hf | MaxProb | 15.94 |\n|  | LP | 14.01 |\n|  | Energy Score | 20.45 |\n|  | *Normalized Entropy* (ours) | **21.12** |", "caption": "Table 4: Results of different selection functions and their corresponding Effective Reliability rate \u03a6c=1subscript\u03a6\ud835\udc501\\Phi_{c=1}roman_\u03a6 start_POSTSUBSCRIPT italic_c = 1 end_POSTSUBSCRIPT on the IDKVQA dataset.", "description": "This table presents a comparison of different methods for estimating the uncertainty of vision-language models (VLMs) on the IDKVQA dataset.  The Effective Reliability rate (\u03a6c=1) is used as the evaluation metric, measuring the trade-off between correct answers and avoiding incorrect answers. The methods compared include: MaxProb (selecting the highest probability answer), LP (a logistic regression model trained on an answerability task), Energy Score (an energy-based out-of-distribution detection method), and the proposed Normalized Entropy method. The results show the performance of each method in terms of \u03a6c=1, indicating their ability to balance accuracy and reliability in VLM uncertainty estimation.", "section": "Ablation II: VLM uncertainty estimation on IDKVQA"}]