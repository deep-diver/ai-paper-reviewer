[{"figure_path": "https://arxiv.org/html/2412.01250/x2.png", "caption": "Figure 1: \nA sketched episode of the proposed Collaborative Instance Navigation (CoIN) task.\nThe human user (bottom left) provides a request (\u201cFind the picture\u201d) in natural language.\nThe agent has to locate the object within a completely unknown environment, interacting with the user only when needed via template-free, open-ended natural-language dialogue.\nOur method, Agent-user Interaction with UncerTainty Awareness\u00a0(AIUTA), addresses this challenging task,\nminimizing user interactions by equipping the agent with two modules:\u00a0a Self-Questioner and an Interaction Trigger,\nwhose output is shown in the blue boxes along the agent\u2019s path (\u2460 to \u2464), and whose inner working is shown on the right.\nThe Self-Questioner leverages a Large Language Model (LLM) and Vision Language Model (VLM) in a self-dialogue to initially describe the agent\u2019s observation, and then extract additional relevant details, with a novel entropy-based technique to reduce hallucinations and inaccuracies, producing a refined\u00a0detection description.\nThe Interaction Trigger uses this refined description to decide whether to pose a question to the user (\u2460,\u2462,\u2463), continue the navigation (\u2461) or halt the exploration (\u2464).", "description": "This figure illustrates the Collaborative Instance Navigation (COIN) task and the AIUTA method.  A user requests to find a picture. The AIUTA-equipped agent navigates an unknown environment, interacting with the user only when necessary through open-ended dialogue. The agent uses two modules: a Self-Questioner (which uses an LLM and a VLM in a self-dialogue to refine object descriptions) and an Interaction Trigger (which decides whether to ask the user a question, continue navigation, or stop). The figure shows the agent's path (1-5) with blue boxes showing the Self-Questioner and Interaction Trigger outputs at each step. The right side details the inner workings of the modules.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.01250/x3.png", "caption": "Figure 2: Graphical depiction of AIUTA: left shows its interaction cycle with the user, and right provides an exploded view of our method. \u2460\u00a0The agent receives an initial instruction I\ud835\udc3cIitalic_I: \u201cFind a c=<\ud835\udc50c=<italic_c = <object category>>>\u201d. \u2461\u00a0At each timestep t\ud835\udc61titalic_t, a zero-shot policy \u03c0\ud835\udf0b\\piitalic_\u03c0\u00a0[44], comprising a frozen object detection module\u00a0[17], selects the optimal action atsubscript\ud835\udc4e\ud835\udc61a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.\n\u2462\u00a0Upon detection, the agent performs the proposed AIUTA. Specifically, \u2463 the agent first obtains an initial scene description of observation Otsubscript\ud835\udc42\ud835\udc61O_{t}italic_O start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT from a VLM. Then, a Self-Questioner module leverages an LLM to automatically generate attribute-specific questions to the VLM, acquiring more information and refining the scene description with reduced attribute-level uncertainty, producing Sr\u2062e\u2062f\u2062i\u2062n\u2062e\u2062dsubscript\ud835\udc46\ud835\udc5f\ud835\udc52\ud835\udc53\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc51S_{refined}italic_S start_POSTSUBSCRIPT italic_r italic_e italic_f italic_i italic_n italic_e italic_d end_POSTSUBSCRIPT. \u2464\u00a0The Interaction Trigger module then evaluates Sr\u2062e\u2062f\u2062i\u2062n\u2062e\u2062dsubscript\ud835\udc46\ud835\udc5f\ud835\udc52\ud835\udc53\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc51S_{refined}italic_S start_POSTSUBSCRIPT italic_r italic_e italic_f italic_i italic_n italic_e italic_d end_POSTSUBSCRIPT against the \u201cfacts\u201d related to the target, to determine whether to terminate the navigation (if the agent believes it has located the target object\u00a0\u2465), or to pose template-free, natural-language questions to a human\u00a0\u2466, updating the \u201cfacts\u201d based on the response\u00a0\u2467.", "description": "Figure 2 illustrates the AIUTA method's workflow.  The left side shows the interaction between the AIUTA agent and a human user.  The agent receives an initial instruction (1), such as \"Find the picture.\" At each timestep (2), a zero-shot policy, incorporating an object detection module, selects an action. Upon detecting a potential target object (3), AIUTA is activated. The agent uses a VLM to get an initial scene description (4), which is then refined by a Self-Questioner module using an LLM to generate attribute-specific questions for the VLM (4). This process reduces uncertainty, leading to a refined description (S<sub>refined</sub>). The Interaction Trigger module (5) then compares this refined description to known facts to decide whether to stop navigation (6), because the target object is found, or to ask the user a question (7) to obtain more information, updating the known facts (8). The right side of the figure provides a detailed breakdown of the AIUTA architecture and its components.", "section": "4. Proposed Method"}, {"figure_path": "https://arxiv.org/html/2412.01250/x4.png", "caption": "Figure 3: \u03c4\ud835\udf0f\\tauitalic_\u03c4 sensitivity results. For each method, 30303030 new \u03c4\ud835\udf0f\\tauitalic_\u03c4 values are sampled symmetrically around the optimal threshold \u03c4\u2217superscript\ud835\udf0f\\tau^{*}italic_\u03c4 start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT.\nThe x\ud835\udc65xitalic_x-axis shows the set size as a percentage of the original IDKVQA dataset size, while the y\ud835\udc66yitalic_y-axis displays the normalized ER \u03a6c=1subscript\u03a6\ud835\udc501\\Phi_{c=1}roman_\u03a6 start_POSTSUBSCRIPT italic_c = 1 end_POSTSUBSCRIPT.", "description": "This figure shows the sensitivity analysis of the threshold \u03c4 (tau) used in the Normalized Entropy method for VLM uncertainty estimation.  For each of three methods (Normalized Entropy, Energy Score, and LP), 30 different \u03c4 values were tested, symmetrically distributed around the optimal threshold (\u03c4*). The x-axis represents the size of the IDKVQA dataset used (as a percentage of the original dataset size), and the y-axis shows the normalized Effective Reliability (ER \u03a6c=1). The analysis reveals the robustness of the different methods to variations in the threshold \u03c4, indicating how the performance changes as the dataset size and \u03c4 value are modified.", "section": "Ablation II: VLM uncertainty estimation on IDKVQA"}, {"figure_path": "https://arxiv.org/html/2412.01250/x5.png", "caption": "Figure 1: CoIN-Bench can be very challenging when only given the instance category to the agent. We highlight the target instance with red borders, while the distractor instances that exist in the same scene are marked with blue borders.", "description": "The figure shows four examples of scenes from the CoIN-Bench dataset.  Each scene contains multiple instances of the same object category, making it challenging to identify the target object based solely on its category. The target instance in each scene is highlighted with a red border, whereas distractor objects are marked with a blue border. This visualization underscores the difficulty of the Collaborative Instance Navigation (CoIN) task when minimal user input is provided, emphasizing the need for the proposed AIUTA method.", "section": "5. COIN-Bench"}, {"figure_path": "https://arxiv.org/html/2412.01250/x6.png", "caption": "Figure 2: We show the distribution of categories, categorized for each evaluation split.", "description": "This figure presents bar charts illustrating the distribution of object categories across four different evaluation splits within the CoIN-Bench dataset: Train, Val Seen, Val Unseen, and Val Seen Synonyms. Each bar chart represents a specific object category and its frequency in the respective split. The charts visually demonstrate how the number and types of object categories vary across these splits, offering insights into the dataset's composition and the diversity of objects encountered during different stages of evaluation.", "section": "5. COIN-Bench"}, {"figure_path": "https://arxiv.org/html/2412.01250/x7.png", "caption": "Figure 3: (a) Frontier map and (b) value map constructed by VLFM\u00a0[44]. The blue dots in (a) (as well as the red dots in (b)) are the identified frontiers.", "description": "Figure 3 shows the frontier map (a) and value map (b) generated by the VLFM model. The frontier map highlights the boundaries between explored and unexplored areas in the environment, with blue dots representing the frontiers.  The value map assigns a value to each location in the explored area, indicating its relevance to locating the target object. Red dots on the value map highlight the high-value frontiers. These maps work together to guide the agent's navigation towards the target.", "section": "B. Baselines"}]