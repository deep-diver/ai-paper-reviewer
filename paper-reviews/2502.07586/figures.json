[{"figure_path": "https://arxiv.org/html/2502.07586/x1.png", "caption": "Figure 1: Humans and machines conceptualize the world differently from each other. Mismatches in communication occur, which lead to misunderstandings. To understand and control AI, we must bridge this gap by developing new words corresponding to human and machine concepts, and use these words to control machines.", "description": "The figure illustrates the communication challenges between humans and AI stemming from differing conceptualizations of the world.  Humans and machines possess unique sets of concepts, leading to misinterpretations when using standard human language for communication.  To effectively understand and control AI, the proposed solution involves creating new words (neologisms) that precisely represent both human and machine concepts, facilitating more accurate communication and control.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.07586/x2.png", "caption": "Figure 2: Machine and humans may fundamentally understand the world differently, enabling different concepts, knowledge and capabilities. Figure reproduced from Kim (2022); Schut et\u00a0al. (2023) with permission.", "description": "The figure illustrates the conceptual differences between how humans and machines understand the world.  It highlights that humans and machines possess unique sets of concepts (represented by the overlapping and non-overlapping regions of two circles). The non-overlapping parts represent concepts understood only by humans or only by machines, showcasing the limitations of using human language to fully grasp machine intelligence and the need for a shared language bridging this gap. The areas of overlap indicate shared concepts, but even these can differ subtly between humans and machines. This illustrates the communication challenges in AI interpretability and the necessity for new terminology to address this communication gap.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.07586/x3.png", "caption": "Figure 3: Concept-based neologisms sit in-between mechanistic interpretability (which is closer to mechanistic details) and behavioral experiments/capability benchmarking (which is only concerned with the model\u2019s output, not how it arrived there).", "description": "The figure illustrates the position of concept-based neologisms within the spectrum of AI interpretability methods.  Mechanistic interpretability, focusing on low-level details of the model's internal workings, is shown on one end. On the other end is behavioral testing, which only assesses the model's output without considering the internal processes.  Concept-based neologisms are presented as an intermediate approach that bridges this gap by focusing on high-level concepts and their relationship to both the model's internal mechanisms and its observable behavior.", "section": "2. Understanding AI requires neologisms"}, {"figure_path": "https://arxiv.org/html/2502.07586/x4.png", "caption": "Figure 4: Our neologism embedding learning only updates new word embedding, preserving the original model\u2019s responses when the new word is not used.", "description": "The figure illustrates the neologism embedding learning process.  It shows that only the embeddings of newly introduced words are updated during training, leaving the original model's weights unchanged.  This ensures that when the new words aren't used in prompts, the model generates responses identically to how it would before the introduction of these new words. The core idea is to add new words to the model's vocabulary and learn their embeddings to influence model behavior without altering the original model's core functionality.", "section": "5. A proof of concept: Neologism Embedding Learning"}, {"figure_path": "https://arxiv.org/html/2502.07586/extracted/6195662/figures/length3.png", "caption": "Figure 5: Base models prompted for length control fail to generate specified long generations (blue), but with a neologism (orange), they consistently generate longer responses.", "description": "The bar chart visualizes the outcome of a controlled experiment.  Two groups of responses from a language model are compared: one group prompted without a newly-defined word (neologism), and one with the neologism included. Both groups were given the same instructions but the neologism-enhanced prompts were specifically designed to control the length of the generated text.  The chart displays the distribution of response lengths, showing that the baseline model (without the neologism) significantly underperforms in generating responses of the target lengths, while the neologism-augmented prompts successfully guide the model to produce responses at the desired length.", "section": "5. A proof of concept: Neologism Embedding Learning"}, {"figure_path": "https://arxiv.org/html/2502.07586/x5.png", "caption": "Figure 6: Adding a \u201cdiversity neologism\u201d diversehwsuperscriptsubscriptabsent\ud835\udc64\u210e{}_{w}^{h}start_FLOATSUBSCRIPT italic_w end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT to a prompt substantially increases a model\u2019s response variety, as exemplified in a number guessing game. The setup is explained in Section\u00a05.4; higher = more response variety (better).", "description": "The figure displays the results of an experiment using a \"diversity neologism\" to improve the variety of responses from a language model in a number guessing game.  The x-axis represents the number of samples (guesses), and the y-axis shows the probability of finding the correct number. Multiple lines compare different conditions: a baseline model, a model with the diversity neologism, and a theoretical perfect random guesser. The neologism significantly increases the model's probability of success and reduces the bias observed in the baseline model.", "section": "5. A proof of concept: Neologism Embedding Learning"}]