[{"figure_path": "https://arxiv.org/html/2411.19930/x1.png", "caption": "Figure 1: Domain-Specific Performance of AdaMLLM\u00a0and General MLLM. For each of the two domains\u2014biomedicine and food\u2014we conduct post-training to adapt the general MLLM to the target domain and evaluate model performance on various domain-specific tasks. Biomedicine and food tasks are colored gray and orange, respectively.", "description": "Figure 1 presents a comparative analysis of AdaMLLM (an adapted multimodal large language model) and general MLLMs across two distinct domains: biomedicine and food.  The figure uses radar charts to visualize the performance of various MLLMs (including different sizes and sources of pre-trained models) on a range of domain-specific tasks. Each axis of the radar chart represents a unique task, and the distance of each point from the center signifies the model's performance on that specific task.  The gray-shaded regions on the charts indicate tasks specific to the biomedicine domain, while the orange regions denote food-related tasks.  This allows for a direct comparison of the performance improvement achieved by the AdaMLLM through post-training on domain-specific data compared to the original, general-purpose MLLMs. The visual representation helps to quickly assess the effectiveness of domain adaptation techniques for MLLMs in both biomedicine and food applications.", "section": "4. Experiment Settings"}, {"figure_path": "https://arxiv.org/html/2411.19930/x2.png", "caption": "Figure 2: Method Overview. (A) We fine-tune a unified visual instruction synthesizer that generates diverse tasks based on image-caption pairs across various domains. (B) Using this synthesizer, we synthesize tasks based on domain-specific image-caption pairs and then apply a consistency-based data filter. The filtered synthetic tasks, combined with the original image captioning tasks, are employed to train general MLLMs through a single-stage post-training process, MLLM training loss is computed only on the part colored in orange.", "description": "This figure illustrates the two-stage method for domain-specific post-training of multimodal large language models (MLLMs). Stage (A) involves fine-tuning a visual instruction synthesizer using image-caption pairs from diverse domains to generate varied visual instruction tasks.  Stage (B) leverages this trained synthesizer to create domain-specific tasks from image-caption pairs within the target domain, followed by a consistency filter to improve data quality. Finally, the filtered synthetic tasks are combined with original image-caption tasks and used for training MLLMs in a single-stage approach. The orange color highlights the specific section where the MLLM training loss is computed.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.19930/x3.png", "caption": "Figure 3: Task Type Distribution of all our synthetic tasks based on three image-caption sources.", "description": "This figure shows a breakdown of the different types of visual instruction tasks generated by the model, categorized by the three image-caption sources used for training.  The types of tasks are diverse and cover a range of complexities, including image-text matching, object recognition, detailed scene descriptions, and many other task types.  The size of each slice in the pie chart corresponds to the relative frequency of that task type within the overall set of synthetic tasks generated from each source. This visualization allows for assessing the diversity and balance of the generated synthetic tasks, which is important for the effectiveness of the downstream domain adaptation task.", "section": "3.1 Domain-Specific Visual Instruction Synthesis"}, {"figure_path": "https://arxiv.org/html/2411.19930/x4.png", "caption": "Figure 4: Cases of Instruction-Response Pairs synthesized by our method, manual rules, GPT-4, and GPT-4V, the image-caption sources for cases (A), (B), (C) are Recipe1M, PMCR\u2062a\u2062wsuperscriptPMC\ud835\udc45\ud835\udc4e\ud835\udc64\\text{PMC}^{Raw}PMC start_POSTSUPERSCRIPT italic_R italic_a italic_w end_POSTSUPERSCRIPT\u00a0and PMCR\u2062e\u2062f\u2062i\u2062n\u2062e\u2062dsuperscriptPMC\ud835\udc45\ud835\udc52\ud835\udc53\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc51\\text{PMC}^{Refined}PMC start_POSTSUPERSCRIPT italic_R italic_e italic_f italic_i italic_n italic_e italic_d end_POSTSUPERSCRIPT, respectively. Certain portions are omitted and are represented as (\u2026).", "description": "Figure 4 presents a comparison of instruction-response pairs generated using four different methods: the authors' proposed method, manual rules, GPT-4, and GPT-4V.  For each method, an example is shown where an image-caption pair serves as the input.  The generated instruction and response illustrate the style and complexity of the responses produced. Case (A) focuses on a cooking recipe from Recipe1M, Case (B) shows a biomedical procedure explanation using data from PMCRaw, and Case (C) illustrates a medical image analysis task using data from PMCRefined.  The ellipses (...) indicate that portions of the original responses have been omitted for brevity.  This figure demonstrates the diversity and complexity that can be achieved using different data and techniques.", "section": "3.1 Domain-Specific Visual Instruction Synthesis"}, {"figure_path": "https://arxiv.org/html/2411.19930/x5.png", "caption": "Figure 5: Distribution of Image Domains and Task Types in Seed Data.", "description": "This figure shows a pie chart visualization that breaks down the composition of the seed data used to train a visual instruction synthesizer. The chart is divided into two main sections: image domains and task types.  The image domains section displays the relative frequencies of various image categories present in the dataset, such as animals, art, food, medical images, and many more.  The task types section shows the proportion of different kinds of tasks included in the seed data, including image captioning, object recognition, and scene classification, among other visual instruction types. This breakdown illustrates the diversity and scope of the data used to develop the synthesizer, highlighting the balance between different image sources and the variety of tasks it's designed to generate.", "section": "3.1 Domain-Specific Visual Instruction Synthesis"}, {"figure_path": "https://arxiv.org/html/2411.19930/x6.png", "caption": "Figure 6: Prompt Template for Consistency-Based Filter (Part 1), continued in Part 2.", "description": "This figure shows a prompt template used for a consistency-based filter in the paper.  The filter is designed to assess the consistency between an informative response and a precise response generated by a visual instruction synthesizer. The prompt guides the evaluation by providing examples of consistent, inconsistent, and open-ended responses, allowing for the classification of generated responses.  The consistency check helps improve accuracy by reducing the need for human expert annotation.", "section": "3.1. Domain-Specific Visual Instruction Synthesis"}, {"figure_path": "https://arxiv.org/html/2411.19930/x7.png", "caption": "Figure 7: Prompt Template for Consistency-Based Filter (Part 2).", "description": "Figure 7 shows a prompt template used for a consistency-based filter in the visual instruction synthesis process.  The filter helps to improve the accuracy of synthetically generated visual instruction tasks by evaluating the consistency between precise and informative responses generated by the model. The prompt guides an evaluator to determine if the precise response can be reliably inferred from the more detailed informative response, or if the task requires open-ended responses due to ambiguity.", "section": "3.1 Domain-Specific Visual Instruction Synthesis"}, {"figure_path": "https://arxiv.org/html/2411.19930/x10.png", "caption": "Figure 8: Cases of Instruction-Response Pairs (Part 1) synthesized by our method, manual rules, GPT-4, and GPT-4V, the image-caption sources for the cases are Recipe1M, PMCR\u2062a\u2062wsuperscriptPMC\ud835\udc45\ud835\udc4e\ud835\udc64\\text{PMC}^{Raw}PMC start_POSTSUPERSCRIPT italic_R italic_a italic_w end_POSTSUPERSCRIPT\u00a0and PMCR\u2062e\u2062f\u2062i\u2062n\u2062e\u2062dsuperscriptPMC\ud835\udc45\ud835\udc52\ud835\udc53\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc51\\text{PMC}^{Refined}PMC start_POSTSUPERSCRIPT italic_R italic_e italic_f italic_i italic_n italic_e italic_d end_POSTSUPERSCRIPT, respectively. Continued in Part 2. In the first case, the rule-based task simply transforms the recipe caption, ignoring the image content. In contrast, our task involves analyzing the food\u2019s state in the image and applying food-related knowledge to infer its texture, demonstrating a higher level of \\uldomain knowledge utilization. In the second case, the GPT-4 generated task straightforwardly asks about the pointing of the red arrow, while ours requires a detailed analysis and inference, showing greater \\ultask complexity.", "description": "Figure 8 showcases various instruction-response pairs generated using different methods: the authors' method, manual rules, GPT-4, and GPT-4V.  The image-caption pairs used as input originated from three datasets: Recipe1M, PMCRaw, and PMCRefined.  The figure highlights the differences in the complexity and thoroughness of the generated tasks.  In one example, a rule-based approach simply rephrases the recipe caption without considering image information; in contrast, the authors' method produces a task that analyzes the food's visual characteristics (like texture) to generate a more complete response. Another example compares a simple question asked by GPT-4 with a more analytical and inferential task developed by the authors, demonstrating a higher level of task complexity.", "section": "3.1 Domain-Specific Visual Instruction Synthesis"}]