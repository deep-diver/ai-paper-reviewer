[{"content": "| User | Assistant |\n|---|---| \n| <img src=\"https://arxiv.org/html/2411.19930/filename.png\">Describe the image. | {Caption} |\n| **User:** Answer with a precise response. {Instruction} | **Assistant:** {Precise Response} |\n| **User:** Answer with an informative response. {Instruction} | **Assistant:** {Informative Response} |", "caption": "Table 1: Data Format for Synthesizer Tuning. The prefixes \u201cUser\u201d and \u201cAssistant\u201d are determined by synthesizer\u2019s chat template. Tuning loss is computed only on the part colored in orange.", "description": "This table details the data format used to fine-tune the visual instruction synthesizer.  The synthesizer is trained using a multi-turn conversational format. Each training example begins with the system presenting an image to the user. The next turn involves the system providing an instruction, and the user responding with an informative answer and a precise answer. The `User` and `Assistant` prefixes reflect the roles in the conversation. Importantly, the model's training loss is calculated only for the user's precise and informative responses; the caption is used for context but doesn't contribute to the loss calculation.", "section": "3.1 Domain-Specific Visual Instruction Synthesis"}, {"content": "| Role | Content |\n|---|---| \n| User: | <img src=\"https://arxiv.org/html/2411.19930/filename.png\">Describe the image. |\n| Assistant: | {Caption} |\n| User: | Answer with a precise response. {Instruction} |\n| Assistant: | {Precise Response} |\n| User: | Answer with an informative response. {Instruction} |\n| Assistant: | {Informative Response} |", "caption": "Table 2: Biomedicine Task Performance of general MLLMs and MLLMs after domain-adaptive training. We mark the best\nperformance bold and the second-best \\ulunderlined. The image-caption sources for AdaMLLM from\u00a0PMCR\u2062a\u2062wsuperscriptPMC\ud835\udc45\ud835\udc4e\ud835\udc64\\text{PMC}^{Raw}PMC start_POSTSUPERSCRIPT italic_R italic_a italic_w end_POSTSUPERSCRIPT\u00a0and AdaMLLM from\u00a0PMCR\u2062e\u2062f\u2062i\u2062n\u2062e\u2062dsuperscriptPMC\ud835\udc45\ud835\udc52\ud835\udc53\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc51\\text{PMC}^{Refined}PMC start_POSTSUPERSCRIPT italic_R italic_e italic_f italic_i italic_n italic_e italic_d end_POSTSUPERSCRIPT are PMCR\u2062a\u2062wsuperscriptPMC\ud835\udc45\ud835\udc4e\ud835\udc64\\text{PMC}^{Raw}PMC start_POSTSUPERSCRIPT italic_R italic_a italic_w end_POSTSUPERSCRIPT\u00a0and PMCR\u2062e\u2062f\u2062i\u2062n\u2062e\u2062dsuperscriptPMC\ud835\udc45\ud835\udc52\ud835\udc53\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc51\\text{PMC}^{Refined}PMC start_POSTSUPERSCRIPT italic_R italic_e italic_f italic_i italic_n italic_e italic_d end_POSTSUPERSCRIPT, respectively.", "description": "Table 2 presents the results of evaluating various large language models (LLMs) on biomedical tasks. It compares the performance of general-purpose LLMs with those that have undergone domain-adaptive training.  The models are tested on a variety of open and closed biomedical tasks, offering a comprehensive evaluation of their performance. The table highlights the impact of domain-specific post-training on improving the performance of LLMs in biomedical applications.  Specifically, it contrasts results across models post-trained using two different datasets (PMCRaw and PMCRefined), showcasing the influence of the training data on the model's effectiveness. The best performing model for each task is indicated in boldface type, with the second best in underlined type.", "section": "5. Main Results"}, {"content": "| Biomedicine | SLLAKE |  | PathVQA |  | VQA-RAD |  | PMC-VQA | \n|---|---|---|---|---|---|---|---|---|\n|  | OPEN | CLOSED | OPEN | CLOSED | OPEN | CLOSED |  | \n| GPT-4o | 59.1 | 71.6 | 24.1 | 76.0 | 51.6 | 64.0 | 56.7 | \n| *LLaVA-v1.6-8B* | 49.2 | 62.3 | 15.2 | 47.7 | 45.9 | 56.3 | 36.5 | \n| LLaVA-Med-8B | 43.4 | 50.2 | 10.1 | 59.2 | 35.0 | 62.5 | 37.1 | \n| PubMedVision-8B | 50.0 | 68.3 | 17.0 | 67.5 | 43.3 | 67.3 | 40.4 | \n| AdaMLLM-8B from PMC<sup>Raw</sup> | 56.8 | **76.4** | 19.7 | **79.3** | 51.0 | **80.5** | 44.3 | \n| AdaMLLM-8B from PMC<sup>Refined</sup> | **58.0** | 73.3 | **22.9** | 78.6 | **59.8** | **81.3** | **47.9** | \n| *Qwen2-VL-2B* | 50.0 | 52.4 | 17.8 | 38.7 | 37.0 | 46.7 | 45.8 | \n| LLaVA-Med-2B | 43.4 | 55.5 | 11.8 | 60.1 | 37.1 | 58.8 | 41.2 | \n| PubMedVision-2B | 45.2 | 63.2 | 18.2 | **64.7** | 41.3 | 67.3 | 43.2 | \n| AdaMLLM-2B from PMC<sup>Raw</sup> | 53.2 | **75.2** | 20.1 | 63.8 | 49.8 | **74.6** | 43.5 | \n| AdaMLLM-2B from PMC<sup>Refined</sup> | **60.2** | 75.0 | **20.6** | 53.6 | **58.0** | **76.1** | **46.5** | \n| *Llama-3.2-11B* | 56.2 | 63.9 | 22.7 | 72.1 | 46.9 | 63.6 | **51.9** | \n| LLaVA-Med-11B | 47.6 | 58.7 | 14.6 | 69.5 | 38.0 | 69.1 | 47.5 | \n| PubMedVision-11B | 49.1 | 74.3 | 19.3 | 70.9 | 46.2 | 73.9 | 47.1 | \n| AdaMLLM-11B from PMC<sup>Raw</sup> | 56.7 | **77.6** | 22.2 | **87.3** | 55.0 | **76.1** | 49.9 | \n| AdaMLLM-11B from PMC<sup>Refined</sup> | **59.5** | 76.4 | **24.3** | **84.9** | **57.4** | **79.8** | **51.9** |", "caption": "Table 3: Food Task Performance of general MLLMs and MLLMs after domain-adaptive training. We mark the best performance bold and the second-best \\ulunderlined.", "description": "This table presents the performance comparison of various large language models (LLMs) on food-related tasks, both before and after domain-adaptive training.  The models evaluated include general-purpose MLLMs and those adapted specifically for the food domain using the proposed domain-adaptive training method.  Performance is measured across multiple tasks, representing different aspects of food understanding, including recipe generation, nutrition information prediction, and food image classification. The table highlights the improvements in performance achieved through domain adaptation, showing the best and second-best results for each task and model.", "section": "5. Main Results"}, {"content": "| Food | Recipe | Nutrition | Food101 | FoodSeg |\n|---|---|---|---|---|\n| GPT-4o | 26.1 | 46.6 | 89.4 | 61.9 |\n| *LLaVA-v1.6-8B* | 18.6 | 29.6 | 47.9 | 38.9 |\n| LLaVA-Chef-8B | 23.1 | 29.1 | 46.8 | 14.5 |\n| AdaMLLM-8B | 24.8 | 36.1 | 65.3 | 42.0 |\n| *Qwen2-VL-2B* | 18.2 | 36.4 | 73.9 | 19.9 |\n| LLaVA-Chef-2B | 24.1 | 24.5 | 68.8 | 7.7 |\n| AdaMLLM-2B | 24.0 | 41.2 | 72.0 | 23.9 |\n| *Llama-3.2-11B* | 23.7 | 40.0 | 80.8 | 47.6 |\n| LLaVA-Chef-11B | 25.7 | 26.2 | 82.1 | 16.7 |\n| AdaMLLM-11B | 26.1 | 41.0 | 82.2 | 42.0 |", "caption": "Table 4: Domain-Specific Task Performance of MLLMs after Post-Training with different synthetic data and training pipelines. We report the average performance in each domain, with detailed results in Table\u00a014 in Appendix. When the image-caption source and training pipeline are fixed, synthetic data of better performance are marked in bold. When the image-caption source is fixed and our synthetic data are used, numbers marked with \u2191\u00a0indicate that single-stage training outperforms two-stage training, while \u2193\u00a0indicates the opposite.", "description": "This table presents a comparison of the performance of various Multimodal Large Language Models (MLLMs) on domain-specific tasks after post-training.  The models were trained using different types of synthetic data and two training pipelines: a two-stage pipeline and a single-stage pipeline. The table shows the average performance across various tasks within two domains (biomedicine and food).  Bold values indicate better performance when the image-caption source and training pipeline are consistent; \u2191 indicates that single-stage training outperforms two-stage training given the same image-caption source and synthetic data, while \u2193 indicates the opposite.", "section": "3.2 Domain-Specific Single-Stage Post-Training"}, {"content": "| Image-Caption | Recipe1M | PMC<sup>Raw</sup> | PMC<sup>Refined</sup> | \n|---|---|---|---|\n| _Train Pipeline_ | _Two-stage_ | _Single-stage_ | _Two-stage_ | _Single-stage_ |\n| Instruction | Rule | Ours | Rule | Ours | GPT-4 | Ours | GPT-4 | Ours | GPT-4V | Ours | GPT-4V | Ours |\n| LLaVA-v1.6-8B | 28.4 | **29.0** | 34.1 | **42.0 \u2191** | 42.5 | **55.6** | 46.1 | **58.3 \u2191** | 50.5 | **58.6** | 55.5 | **60.3 \u2191** |\n| Qwen2-VL-2B | 31.3 | **38.2** | 31.9 | **40.3 \u2191** | 44.0 | **55.5** | 41.3 | **54.3 \u2193** | 49.0 | **59.5** | 51.6 | **55.7 \u2193** |\n| Llama-3.2-11B | 37.7 | **40.9** | 36.6 | **47.8 \u2191** | 49.3 | **59.2** | 48.8 | **60.7 \u2191** | 54.4 | **60.3** | 53.7 | **62.0 \u2191** |", "caption": "Table 5: Ablation Results. \u201cw/o Blank Image\u201d fine-tunes the synthesizer without replacing 10% of images with blank ones. \u201cw/o Consistency Filter\u201d removes the consistency-based filter and trains with either precise or informative responses. \u201cw/o Synthetic Task\u201d removes synthetic task, and \u201cw/o Image Caption\u201d removes image captioning task. \u201cGeneral Task\u201d trains on seed data processed into our task format, \u201cGeneral Task + Domain Caption\u201d mixes the processed seed data with domain-specific image-caption pairs.", "description": "This table presents the ablation study results on the visual instruction synthesizer and the single-stage post-training method. It shows the impact of removing specific components on the overall performance. The \"w/o Blank Image\" column demonstrates the impact of removing the strategy of replacing 10% of images with blank images during fine-tuning of the visual instruction synthesizer.  The \"w/o Consistency Filter\" column shows the effect of removing the consistency-based filter for improving the accuracy of synthetic tasks. The \"w/o Synthetic Task\" column evaluates the model performance without the generated synthetic tasks. The \"w/o Image Caption\" column examines the model's ability in the absence of image captioning tasks. The \"General Task\" column presents the results of using only seed data processed into the task format. Finally, the \"General Task + Domain Caption\" column demonstrates the performance of combining seed data with domain-specific image-caption pairs.", "section": "3.2 Domain-Specific Single-Stage Post-Training"}, {"content": "|               | Ours | w/o Blank Image | w/o Consistency Filter | Precise | Informative | w/o Synthetic Task | w/o Image Caption | General Task | General Task + Domain Caption |\n| :------------- | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n| BioMed.        | **58.3** | 55.8 | 31.2 | 44.4 | 26.7 | 54.2 | 49.8 | 55.3 |\n| Food           | **42.0** | 35.9 | 37.9 | 37.6 | 25.6 | 36.8 | 36.0 | 38.6 |", "caption": "Table 6: Quality of Synthetic Tasks by Different Visual Instruction Synthesizers, assessed in terms of task diversity, domain knowledge utilization, task complexity, and response accuracy. Column 1 presents results from the MLLM without fine-tuning. Columns 2-5 show results after fine-tuning the MLLM using our seed data to synthesize tasks based on different inputs. Besides, Column 5 replaces 10% of the images with blank images.", "description": "This table presents a quantitative analysis of the quality of synthetic tasks generated by different visual instruction synthesizers.  The quality is assessed across four key dimensions: task diversity (variety of task types generated), domain knowledge utilization (how well the tasks leverage domain-specific knowledge), task complexity (difficulty of the tasks), and response accuracy (correctness of the generated responses). The table compares the performance of the model without any fine-tuning (Column 1) to the performance after fine-tuning it on seed data using different inputs (Columns 2-5). Notably, Column 5 shows the results when 10% of the images used for fine-tuning are replaced with blank images to simulate challenging situations.", "section": "3.1 Domain-Specific Visual Instruction Synthesis"}, {"content": "| Finetune Input | - | Image | Caption | Image + Caption | Image + Caption |\n|---|---|---|---|---|---| \n| _Blank Image_ | - | - | - | \u2717 | \u2713 |\n| Diversity | 52.5 | 68.0 | 75.2 | 81.0 | **85.5** |\n| Knowledge | 72.5 | 95.0 | 93.8 | 97.5 | **98.1** |\n| Complexity | 43.8 | 77.9 | 75.3 | 80.0 | **83.2** |\n| Accuracy | 63.8 | 60.0 | 65.6 | 66.3 | **71.3** |", "caption": "Table 7: Quality of Responses with/without Using Consistency-Based Filter, assessed in terms of consistency between precise and informative responses (Consist.), accuracy of precise responses (Precise Acc), accuracy of informative responses (Info. Acc), and accuracy of combined responses (Acc).", "description": "Table 7 presents a quantitative analysis of response quality before and after applying a consistency-based filter.  It compares the consistency between precise and informative responses, as well as the accuracy of each response type individually, and finally, the accuracy of the combined responses (which uses both the precise and informative components). This helps assess how effective the filter is at improving overall response quality by ensuring alignment between different parts of the answer.", "section": "3.1 Domain-Specific Visual Instruction Synthesis"}, {"content": "|       | w/o Filter       |       | w/ Filter        |\n| :---- | :----------------: | :---- | :----------------: |\n|       | Consist. | Precise Acc | Info. Acc | Consist. | Acc |\n| BioMed. | 30.3       | 64.3        | 61.0        | 92.2        | 75.1        |\n| Food   | 35.7        | 77.2        | 75.5        | 97.1        | 84.3        |", "caption": "Table 8: Quality of Synthetic Tasks by our method, manual rules, GPT-4, and GPT-4V, assessed in terms of task diversity, domain knowledge utilization, task complexity, and response accuracy.", "description": "Table 8 presents a comparative analysis of the quality of synthetic visual instruction tasks generated by four different methods: the authors' proposed method, manual rule-based generation, GPT-4, and GPT-4V.  The quality is evaluated across four key aspects: task diversity (variety of task types generated), domain knowledge utilization (how well the tasks incorporate domain-specific knowledge), task complexity (difficulty of the tasks), and response accuracy (correctness of the model's responses to the tasks). This comparison helps determine which method produces the most effective visual instruction tasks for post-training multimodal large language models (MLLMs).", "section": "3.1 Domain-Specific Visual Instruction Synthesis"}, {"content": "| Image-Caption | Recipe1M | PMC<sup>Raw</sup> | PMC<sup>Refined</sup> | \n|---|---|---|---| \n| Instruction | Rule | Ours | GPT-4 | Ours | GPT-4V | Ours | \n| Diversity | 23.5 | **52.9** | 47.1 | **58.8** | 64.7 | **76.5** | \n| Knowledge | 20.9 | **21.9** | 44.9 | **58.9** | **67.7** | 63.2 | \n| Complexity | 38.4 | **69.9** | 41.7 | **83.2** | 49.6 | **80.5** | \n| Accuracy | **98.7** | 84.3 | **84.4** | 75.1 | **87.5** | 79.6 | ", "caption": "Table 9: Hyper-Parameters for Synthesizer Tuning", "description": "This table details the hyperparameters used to fine-tune the visual instruction synthesizer model.  It shows the base model used for fine-tuning (LLaVa-v1.6-8B), whether the entire model or only specific parts were trained, the number of training epochs, batch size, maximum sequence length, and learning rates for different parts of the model (the projector and LLM, and the visual encoder). It also specifies the learning rate scheduler, weight decay, warm-up ratio, the computing infrastructure used (8 A100-80GB GPUs), and the total training time (13 hours). This information is crucial for understanding the model training process and reproducibility.", "section": "3.1. Domain-Specific Visual Instruction Synthesis"}, {"content": "| Hyper-Parameter | Assignment |\n|---|---| \n| Base Model | LLaVA-v1.6-8B |\n| Trainable | Full Model |\n| Epoch | 2 |\n| Batch Size | 128 |\n| Max Seq Length | 6144 |\n| LR<sub>projector & LLM</sub> | 2e-5 |\n| LR<sub>visual encoder</sub> | 2e-6 |\n| LR Scheduler | Cosine |\n| Weight Decay | 0 |\n| Warm-Up Ratio | 0.03 |\n| Computing Infrastructure | 8 A100-80GB GPUs |\n| Training Time | 13 Hours |", "caption": "Table 10: Hyper-Parameters for MLLM Single-Stage Post-Training.", "description": "This table details the hyperparameters used during the single-stage post-training phase for three different Multimodal Large Language Models (MLLMs): LLaVA-v1.6, Qwen2-VL, and Llama-3.2.  For each MLLM, it lists the values used for parameters such as the training epochs, batch size, maximum sequence length, learning rates for different components (projector and LLM, visual encoder), learning rate scheduler, weight decay, and warm-up ratio. These settings are crucial in fine-tuning the MLLMs for domain-specific tasks, and this table provides a comprehensive overview of the configurations.", "section": "3.2 Domain-Specific Single-Stage Post-Training"}, {"content": "| MLLM | LLaVA-v1.6 | Qwen2-VL | Llama-3.2 |\n|---|---|---|---|\n| Trainable | Full Model | Full Model | Full Model |\n| Epoch | 1 | 1 | 1 |\n| Batch Size | 128 | 128 | 128 |\n| Max Seq Length | 6144 | 6144 | 6144 |\n| LR<sub>projector & LLM</sub> | 2e-5 | 1e-5 | 5e-6 |\n| LR<sub>visual encoder</sub> | 2e-6 | 1e-5 | 5e-6 |\n| LR Scheduler | Cosine | Cosine | Cosine |\n| Weight Decay | 0 | 0.1 | 0.1 |\n| Warm-Up Ratio | 0.03 | 0.1 | 0.1 |", "caption": "Table 11: Training Time (Hours) for MLLM Single-Stage Post-Training on 8 A100-80GB GPUs.", "description": "This table presents the time taken (in hours) to perform single-stage post-training on various Multimodal Large Language Models (MLLMs). The training was conducted using eight NVIDIA A100-80GB GPUs.  Different models are listed, including LLaVA-v1.6-8B, Qwen2-VL-2B, and Llama-3.2-11B. The training times are provided for each MLLM, highlighting the computational resources required for domain-specific adaptation.", "section": "3.2 Domain-Specific Single-Stage Post-Training"}, {"content": "| Image-Caption | PMC<sup>Raw</sup> | PMC<sup>Refined</sup> | Recipe1M |\n|---|---|---|---| \n| LLaVA-v1.6-8B | 21 | 23 | 6 |\n| Qwen2-VL-2B | 3.5 | 4 | 1 |\n| Llama-3.2-11B | 29 | 31 | 9 |", "caption": "Table 12: Specifications of the Evaluated Domain-Specific Task Datasets.", "description": "This table lists the benchmark datasets used to evaluate the performance of multimodal large language models (MLLMs) after domain adaptation. For the biomedicine domain, it includes SLAKE (medical question answering), PathVQA (pathology image QA), VQA-RAD (radiology image QA), and PMC-VQA (medical multi-choice QA). Each dataset is further categorized by question type as open-ended or closed-ended.  For the food domain, it lists Recipe1M (recipe generation), Nutrition5K (food ingredient prediction), Food101 (food category classification), and FoodSeg103 (food multi-label classification). The table specifies the task type, dataset name, evaluation metric (accuracy, recall, F1 score, Rouge-L), and the number of test samples for each dataset.", "section": "4. Experiment Settings"}, {"content": "| Task | Description | Metric | Test Num |\n|---|---|---|---| \n| *BioMed.* |  |  |  |\n| SLAKE OPEN [25] | Medical question answering | Recall | 645 |\n| SLAKE CLOSED [25] | Medical binary classification | Accuracy | 416 |\n| PathVQA OPEN [15] | Medical question answering | Recall | 3357 |\n| PathVQA CLOSED [15] | Medical binary classification | Accuracy | 3362 |\n| VQA-RAD OPEN [19] | Medical question answering | Recall | 179 |\n| VQA-RAD CLOSED [19] | Medical binary classification | Accuracy | 272 |\n| PMC-VQA [57] | Medical multi-chioice QA | Accuracy | 2000 |\n| *Food* |  |  |  |\n| Recipe1M [38] | Recipe generation | Rouge-L | 1000 |\n| Nutrition5K [42] | Ingredient prediction | Recall | 507 |\n| Food101 [4] | Food category classification | Accuracy | 25250 |\n| FoodSeg103 [49] | Food multi-label classification | F1 | 2135 |", "caption": "Table 13: Prompt Templates of the Evaluated Domain-Specific Task Datasets.", "description": "This table presents the prompt templates used for evaluating the performance of multimodal large language models (MLLMs) on various domain-specific tasks.  For each task dataset (SLAKE, PathVQA, VQA-RAD, PMC-VQA for biomedicine; Recipe1M, Nutrition5K, Food101, FoodSeg103 for food), it lists the specific question format used to prompt the MLLM and the expected format of the MLLM's response. The templates are designed to ensure consistent and comparable evaluations across different tasks and domains.", "section": "5. Main Results"}, {"content": "| Task | Instruction | Response |\n|---|---|---|\n| **BioMed.** |  |  |\n| SLAKE | `{question}` | `{answer}` |\n| PathVQA | `{question}` | `{answer}` |\n| VQA-RAD | `{question}` | `{answer}` |\n| PMC-VQA | Question: `{question}`\nThe choices are: `{options}` | `{option}` |\n| **Food** |  |  |\n| Recipe1M | `{question}` | `{recipe}` |\n| Nutrition5K | What ingredients are used to make the dish in the image? | `{ingredients}` |\n| Food101 | What type of food is shown in this image?\nChoose one type from the following options:\n`{food type options}` | `{food type}` |\n| FoodSeg103 | Identify the food categories present in the image.\nThe available categories are: `{options}`\nPlease return a list of the selected food categories, formatted as a list of names like [candy, egg tart, french fries, chocolate]. | `{categories}` |", "caption": "Table 14: Domain-Specific Task Performance of MLLMs after Post-Training with different synthetic data and training pipelines. The image-caption sources are Recipe1M, PMCR\u2062a\u2062wsuperscriptPMC\ud835\udc45\ud835\udc4e\ud835\udc64\\text{PMC}^{Raw}PMC start_POSTSUPERSCRIPT italic_R italic_a italic_w end_POSTSUPERSCRIPT\u00a0and\u00a0PMCR\u2062e\u2062f\u2062i\u2062n\u2062e\u2062dsuperscriptPMC\ud835\udc45\ud835\udc52\ud835\udc53\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc51\\text{PMC}^{Refined}PMC start_POSTSUPERSCRIPT italic_R italic_e italic_f italic_i italic_n italic_e italic_d end_POSTSUPERSCRIPT, respectively.\nIn most cases using our synthetic data, we find that single-stage training outperforms two-stage training on domain-specific tasks, particularly evident in the Recipe generation results for the food domain. Recall that in the two-stage training approach for the food domain, the model first trains on recipe captions and then on our synthetic tasks. We examine the task performance of LLaVA-v1.6-8B on Recipe generation and observe that the model achieves a score of 25.3 after the first stage on recipe captions. However, this score drastically decreases to 16.2 after the second stage. From this, we infer that the two-stage approach causes the model to catastrophically forget the task/knowledge learned in the first stage when transitioning to the second stage\u00a0[30], leading to poorer performance after completing the second-stage training.", "description": "Table 14 presents a detailed comparison of the performance of various Multimodal Large Language Models (MLLMs) on domain-specific tasks after post-training.  The models were trained using different methods for synthesizing training data (manual rules, GPT-4, GPT-4V, and the authors' proposed method) and different training pipelines (two-stage and single-stage). The table shows results for two domains: biomedicine and food, with different image-caption sources used for each domain.  A key finding highlighted in the caption is that single-stage training generally outperforms two-stage training when the authors' synthetic data is used, especially for recipe generation tasks. This is attributed to the two-stage approach causing catastrophic forgetting of knowledge learned in the initial stage during the second stage of training.  The table allows for a comprehensive analysis of various MLLM performance factors.", "section": "5. Main Results"}]