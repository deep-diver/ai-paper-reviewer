{"importance": "This paper is crucial for researchers working on improving instruction-following capabilities of LLMs.  **It introduces a novel data generation method** that is cost-effective and yields high-quality data, addressing the limitations of existing approaches.  The findings offer valuable insights into effective training strategies and open new avenues for research in complex instruction following.", "summary": "Constraint Back-translation enhances complex instruction following in LLMs by leveraging inherent constraints in existing datasets for efficient high-quality data creation.", "takeaways": ["Constraint back-translation, a novel data generation method, effectively creates high-quality complex instruction-following datasets.", "Post-training LLMs on the generated dataset (CRAB) significantly improves their complex instruction-following abilities.", "Constraint back-translation also serves as a beneficial auxiliary training objective, enhancing model understanding of constraints."], "tldr": "Large Language Models (LLMs) struggle with complex instructions, and current instruction-tuning methods using advanced LLMs to generate training data have limitations due to the models' own imperfections. This results in noisy and suboptimal training data. \nThis paper proposes a novel method called \"constraint back-translation.\"  Instead of directly generating complex instruction-response pairs, this method identifies and extracts the implicit constraints already satisfied within high-quality existing datasets and uses them to augment instructions.  **This results in a high-quality, cost-effective complex instruction-response dataset** called CRAB which is used to post-train various LLMs.  The results show significant improvements in complex instruction following ability.", "affiliation": "Tsinghua University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}}