[{"figure_path": "https://arxiv.org/html/2410.24175/x1.png", "caption": "Figure 1: Existing datasets inherently include implicit satisfied complex constraints in the responses.", "description": "The figure illustrates that existing datasets used for training large language models (LLMs) contain implicit complex constraints that are satisfied by the model's responses.  These constraints, although not explicitly stated in the original instructions, are often related to factors like writing style, format, length, and structure of the response.  The example shows how an instruction to 'write a blog on French cuisine' implicitly leads to constraints regarding tone (formal, informative, engaging), hierarchical structure (introduction, four main sections, conclusion), and word count (550-580 words).  This observation is crucial because it highlights that high-quality responses already inherently satisfy complex requirements, a fact that can be leveraged for more efficient data generation.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2410.24175/x2.png", "caption": "Figure 2: The framework of constructing the proposed alignment training dataset.", "description": "This figure illustrates the three-stage process for creating the CRAB dataset.  The first stage, Data Collection, involves gathering high-quality instruction-response pairs from existing datasets.  These pairs are then processed in the Constraint Back-translation stage, where a large language model (LLM) is used to extract implicit constraints satisfied by the existing responses.  Finally, the Constraint Combination stage combines these extracted constraints with the original instructions and responses to create the final training dataset. The figure shows the data flow and transformations at each stage. ", "section": "2 Method"}, {"figure_path": "https://arxiv.org/html/2410.24175/x3.png", "caption": "Figure 3: An example of responses generated with and without constraints by Llama3-70B-Instruct. The evaluator is gpt-4o-0806. For better visualization, we present only a subset of the responses generated without constraints.", "description": "Figure 3 showcases the impact of constraints on the quality of responses generated by Llama-3-70B-Instruct.  It presents an example where the same instruction is given to the model, once with specified constraints and once without. The resulting responses are then evaluated by GPT-4-0806, highlighting the differences in quality metrics.  The figure emphasizes that using constraints significantly improves the quality and structure of responses.", "section": "3.3 Analysis on General Instruction Following"}, {"figure_path": "https://arxiv.org/html/2410.24175/x4.png", "caption": "Figure 4: \nFull-mark rates (%) of the responses generated with and without constraints. The evaluator is gpt-4o-0806, focusing on four widely-used dimensions: Engagingness (Eng.), Understandability (Und.), Fluency (Flu.), and Coherence (Coh.).", "description": "Figure 4 presents a bar chart comparing the quality of responses generated by a language model with and without constraints.  The responses were evaluated by the GPT-4 model (gpt-4o-0806) across four key dimensions: Engagingness, Understandability, Fluency, and Coherence.  Each dimension's score is represented as a percentage of responses receiving a full mark (indicating the highest quality).  The chart allows for a direct comparison of the impact of adding constraints to the prompts on the overall quality of the model's output, as measured by these four dimensions.  This visualization helps to demonstrate the effectiveness of constraint back-translation in enhancing the quality of generated text.", "section": "3.3 Analysis on General Instruction Following"}, {"figure_path": "https://arxiv.org/html/2410.24175/x5.png", "caption": "Figure 5: \nExperimental results on different categories of constraints in FollowBench of\nMistralCrab and ConiferSFT.", "description": "Figure 5 presents a bar chart comparing the performance of MistralCrab and ConiferSFT across various constraint categories within the FollowBench benchmark.  Each bar represents a constraint type (e.g., example, content, situation, style, format, mixed), and the height of the bar indicates the models' success rate for that constraint type. This visualization allows for a direct comparison of the two models' abilities to handle different kinds of constraints in complex instruction following tasks, highlighting strengths and weaknesses of each approach.", "section": "3.5 Analysis on Constraint Category"}, {"figure_path": "https://arxiv.org/html/2410.24175/x6.png", "caption": "Figure 6:  Proportion (%) of data in the Crab by the number of constraints and the source dataset.", "description": "Figure 6 shows the distribution of the 13,500 instances in the CRAB dataset.  The left pie chart displays the percentage of instances containing a specific number of constraints after the combination process. The right pie chart illustrates the percentage of instances originating from each of the four source datasets used to create CRAB: Alpaca GPT4, Orca Chat, Evol Instruct, and OpenAssistant.  The figure helps to visualize the diversity of constraint numbers and the source dataset contributions to the CRAB dataset.", "section": "A Data Collection"}]