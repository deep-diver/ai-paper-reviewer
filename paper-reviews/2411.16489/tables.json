[{"content": "| Model | AIME(2024) Accuracy | AIME(2024) # Average Token | MATH500 Accuracy | MATH500 # Average Token |\n|---|---|---|---|---|\n| Proprietary |  |  |  |  |\n| o1-preview | 12/30 | 9083 | 85.5 | 1501 |\n| o1-mini | 21/30 | 9903 | 90.0 | 944 |\n| Parameter Size: 72B |  |  |  |  |\n| Ours-72B | 13/30 | 8016 | 87.2 | 2235 |", "caption": "Table 1: Comparison of the performance between the distilled O1-mini model and O1-series models on the AIME2024 and MATH500 benchmarks under specific inference cost constraints.", "description": "This table compares the performance of a distilled O1-mini model against the original OpenAI O1-preview and O1-mini models. The comparison is done on two benchmarks, AIME2024 and MATH500, while controlling for the computational cost (average tokens used during inference). It shows the accuracy and the average number of tokens used for each model on each benchmark, demonstrating the performance of the distilled model relative to the original models under similar computational constraints.", "section": "3.3 Performance Analysis"}, {"content": "| Model | Flames | DiaSafety | WildSafety | SimpleQA | C-SimpleQA | CFE-General | CFE-Sycophancy |  | Auto-J | LIMA |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Baseline | 91.0 | 100.0 | **92.0** | **10.58** | **47.08** | **69.08** | 89.70 |  | 81.6 | 77.2 |\n| Ours | **92.5** | 100.0 | 86.5 | 10.41 | 45.76 | 62.65 | **92.65** |  | **88.0** | **87.2** |", "caption": "Table 2: Performance comparison (accuracy) before and after SFT across different evaluation categories. The datasets are grouped into three categories: safety evaluation (Flames, DiaSafety, WildSafety), factuality evaluation (SimpleQA, Chinese SimpleQA, ChineseFactEval-General, ChineseFactEval-Sycophancy, and general evaluation (Auto-J, LIMA). Note: C-SimpleQA, CFE-General, and CFE-Sycophancy stand for Chinese SimpleQA, ChineseFactEval-General, and ChineseFactEval-Sycophancy, respectively.", "description": "Table 2 presents a detailed comparison of model performance before and after supervised fine-tuning (SFT).  It assesses performance across various categories grouped into three main areas: safety, factuality, and general knowledge.  Safety is evaluated using the Flames, DiaSafety, and WildSafety datasets; factuality is assessed with SimpleQA, its Chinese equivalent (C-SimpleQA), and both general and sycophancy versions of the ChineseFactEval dataset (CFE-General and CFE-Sycophancy, respectively).  Finally, general performance is measured using the Auto-J and LIMA datasets. The table allows for a comprehensive understanding of how SFT impacts model capabilities in different aspects, highlighting its strengths and limitations in safety, accuracy and general knowledge reasoning.  The use of diverse datasets ensures a robust and multifaceted evaluation.", "section": "4.1 Safety"}, {"content": "| Evaluation Dimensions | Checklist | Score |\n|---|---|---|\n| Data (14) | Are dataset names, sources, and providers explicitly documented and properly cited? | 3 |\n|  | Is there sufficient documentation of data distributions, formats, and characteristics to enable proper replication? | 3 |\n|  | Are the criteria and methodology for data selection and filtering clearly justified and documented? | 4 |\n|  | For synthetic data generation, is the entire process transparent, including prompting strategies and quality control measures? | 4 |\n| Methodology (33) | Is there a clear and complete description of the base model (including its architecture, size, etc.)? | 4 |\n|  | Is the complete search algorithm implementation (e.g., beam search, MCTS) detailed with all components? | 6 |\n|  | Is the RL algorithm fully specified with its objective function and training procedure? | 6 |\n|  | Is the long thought data curation/generation algorithm thoroughly explained with its complete workflow? | 6 |\n|  | Is the complete training pipeline documented, including all stages and their sequence? | 3 |\n|  | Are the computational requirements and infrastructure details provided? | 2 |\n|  | Is there clear documentation of all training hyperparameters and optimization choices? | 2 |\n|  | Are there comprehensive ablation studies showing the contribution of each major component? | 4 |\n| Evaluation (24) | Is there a clear justification for the selection of evaluation benchmarks? | 4 |\n|  | Is the evaluation dimension clearly specified (e.g., answer-level, step-by-step level)? | 4 |\n|  | Are all evaluation metrics (e.g., pass@k, maj@k) clearly defined? | 4 |\n|  | For any custom metrics (if exists), are they well-justified and clearly documented? | 4 |\n|  | Are the evaluation metrics consistently applied across all baselines? | 4 |\n|  | Are the evaluation conditions (e.g., temperature, top-p) explained for all compared methods? | 4 |\n| Open-Source (29) | Is the post-training data publicly available? | 3 |\n|  | Is the synthetic long thought data publicly available? | 5 |\n|  | Are trained model weights publicly available? | 5 |\n|  | Is the complete training codebase publicly available? | 4 |\n|  | Is the complete evaluation codebase publicly released? | 4 |\n|  | Are there step-by-step guidance and instruction for code usage? | 4 |\n|  | Is there a comprehensive technical paper detailing all research aspects instead of a brief blog post? | 4 |", "caption": "Table 3: Transparency scoring framework for O1 replication efforts. Each evaluation point of the checklist is assigned a score based on their transparency criteria. The total transparency score sums up to 100 points.", "description": "This table presents a scoring framework for evaluating the transparency of OpenAI's O1 model replication attempts.  It breaks down the evaluation into four key dimensions: Data Transparency, Methodology Transparency, Evaluation Transparency, and Open-Source Resources.  Each dimension is further divided into specific criteria, each assigned a point value (detailed in the checklist).  The total score sums up to 100 points, providing a quantitative measure of how reproducible and verifiable each replication attempt is.  Higher scores indicate greater transparency.", "section": "A Framework for Evaluating O1 Replication Claims: The Technical Transparency Index"}, {"content": "| Work | Data (14) | Methodology (33) | Evaluation (24) | Open-Source (29) | Total Score |\n|---|---|---|---|---|---|---|\n| Open O1 | 0 | 8 | 20 | 5 | **33** |\n| O1-Journey (Part1) | 10 | 33 | 24 | 9 | **76** |\n| LLaMA-O1 | 0 | 6 | 0 | 5 | **11** |\n| K0Math | 0 | 0 | 16 | 0 | **16** |\n| Skywork O1 | 0 | 0 | 0 | 0 | **0** |\n| DeepSeek-R1-Lite | 0 | 0 | 20 | 0 | **20** |\n| O1-Journey (Part2) | 10 | 33 | 24 | 12 | **79** |", "caption": "Table 4: Transparency scores of various O1 replication efforts. Each column represents a specific method, with individual scores provided for each evaluation dimension and indicator. The total transparency score is calculated out of 100 points, reflecting the openness and reproducibility of each approach.", "description": "This table presents a comparative analysis of several attempts to replicate OpenAI's O1 model, focusing on their transparency and reproducibility. Each method is assessed across four key dimensions: data transparency (data sources, quality, and documentation), methodology transparency (clarity of techniques, processes, and experimental setup), evaluation transparency (benchmarks and metrics used), and open-source resources (availability of datasets, models, and code).  Each dimension is assigned a score out of a possible 100, providing an overall transparency score for each replication attempt. Higher scores indicate greater openness and reproducibility.", "section": "A Framework for Evaluating O1 Replication Claims: The Technical Transparency Index"}]