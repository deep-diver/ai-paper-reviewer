[{"figure_path": "https://arxiv.org/html/2411.16489/x1.png", "caption": "Figure 1: \nIllustration of our O1 replication journey from September 12 to November 22, 2024.", "description": "This figure illustrates the timeline and key milestones of the authors' journey in replicating OpenAI's O1 model from September 12th to November 22nd, 2024.  It highlights the different approaches explored, including initial assessments, the use of a 'shortcut' method involving knowledge distillation, progress on various benchmarks (like AIME and MATH), and the final results achieved. The visual elements use cartoonish characters to represent different stages and approaches in a clear and intuitive way.", "section": "01 Journey"}, {"figure_path": "https://arxiv.org/html/2411.16489/x2.png", "caption": "Figure 2: The framework of journey learning.", "description": "This figure illustrates the Journey Learning framework, a method for synthesizing long chains of reasoning to solve complex problems.  It involves using tree-search algorithms (like Monte Carlo) to explore different solution paths, selecting promising trajectories, and using LLMs to analyze previous steps, identify errors, and make corrections. This iterative process generates complete trajectories leading to correct answers, which are used for training LLMs. The diagram depicts this process with various stages, including initial assessment, shortcut paths, multi-agent debate, tree search, and human annotations, culminating in the final model.", "section": "2 The \u201cShortcut\u201d Path to O1 Replication"}, {"figure_path": "https://arxiv.org/html/2411.16489/x3.png", "caption": "Figure 3: Different methods of collecting the long thought data. The distillation method offers a cost-effective and reliable approach to obtaining high-quality data.", "description": "Figure 3 illustrates various methods for acquiring long-thought data, crucial for training AI models capable of complex reasoning.  Methods shown include tree search (computationally intensive but thorough), multi-agent debate (involving multiple AI agents to simulate a reasoning process), and human annotation (labor-intensive and costly but providing gold-standard data).  The figure highlights that knowledge distillation from existing advanced models offers a superior balance of cost-effectiveness and reliability for obtaining high-quality data.", "section": "2 The \u201cShortcut\u201d Path to O1 Replication"}, {"figure_path": "https://arxiv.org/html/2411.16489/x4.png", "caption": "Figure 4: Case study on how model-generated long thoughts provide alternatives, resulting in safer responses.", "description": "This figure showcases a comparative analysis of responses generated by the base model and the fine-tuned model to a safety-related query.  The base model, without the benefit of long-thought training data, focuses primarily on anti-theft measures. However, the fine-tuned model, incorporating long-thought chains, exhibits a more comprehensive approach, prioritizing life-threatening risks (fire hazards) before addressing the user's immediate concern (theft). This highlights how the incorporation of long-thought processes leads to more nuanced, safer, and more insightful responses by considering multiple aspects and providing alternative solutions.", "section": "4.1 Safety"}, {"figure_path": "https://arxiv.org/html/2411.16489/x5.png", "caption": "Figure 5: Case study on our model attempting to actively search and leverage external tools to solve a short-form fact-seeking question.", "description": "This figure showcases a case study illustrating how the model actively attempts to utilize external resources (search engines, etc.) to answer a short factual question.  The before-and-after comparison highlights the improvements in the model's approach to problem-solving after undergoing fine-tuning. The 'before' example shows a more simplistic and less thorough approach, whereas the 'after' example depicts a step-by-step, systematic process involving identifying relevant sources, performing searches, verifying information from multiple sources, and presenting a detailed justification for the final answer. This demonstrates how fine-tuning enhances the model's ability to perform complex reasoning tasks.", "section": "4.2 Hallucination"}, {"figure_path": "https://arxiv.org/html/2411.16489/x6.png", "caption": "Figure 6: Case study on how detailed analysis and self-reflection can help prevent hallucination.", "description": "This figure showcases a case study comparing model responses before and after fine-tuning. The before-tuning response is concise and lacks detail, while the after-tuning response demonstrates a thorough step-by-step approach, including active search for information, verification of details, and self-reflection on the process.  This illustrates how detailed analysis and self-reflection, facilitated by the fine-tuning process, can significantly improve response accuracy and reduce hallucinations.", "section": "4.2 Hallucination"}, {"figure_path": "https://arxiv.org/html/2411.16489/x7.png", "caption": "Figure 7: Case study on how self-reflection can help models detect false assumptions.", "description": "This figure showcases a comparative analysis of model responses before and after fine-tuning. It presents two example queries and their respective responses, illustrating how self-reflection during the fine-tuning process helps the model identify and correct a false assumption embedded in the query. The first example shows a query about the second longest river in China, where the model's initial response incorrectly identified the Pearl River. After fine-tuning, the model actively reconsiders this claim, engages in self-reflection, and correctly identifies the Yellow River as the second longest. In the second example, the query concerns the number of times Argentina won the FIFA World Cup. The original model's reasoning is less systematic and results in an incorrect answer. The model after fine-tuning demonstrates significantly improved reasoning and a more rigorous approach, arriving at the correct answer through detailed analysis and verification.", "section": "4.2 Hallucination"}, {"figure_path": "https://arxiv.org/html/2411.16489/x8.png", "caption": "Figure 8: Case study of our model provides helpful insights from different perspectives on answering user questions.", "description": "This figure shows a comparison of how the model before and after fine-tuning (SFT) answers a question about debugging in Python's asyncio library. Before SFT, the response is concise and lacks depth, offering only five basic points and code examples. After SFT, the model's response demonstrates significant improvement in structure, detailed analysis, and helpful insights.  The post-SFT response includes advanced concepts, debugging suggestions, and best practices, showcasing enhanced reasoning and comprehensive understanding.", "section": "4.3 General Scenario"}]