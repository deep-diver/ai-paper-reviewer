{"references": [{" publication_date": "2020", "fullname_first_author": "Tom B. Brown", "paper_title": "Language Models are Few-Shot Learners", "reason": "This paper is foundational to the field of instruction tuning for LLMs, demonstrating the impressive ability of large language models to adapt to new tasks with minimal fine-tuning.  Its impact is evident throughout the paper, justifying the authors' use of instruction-based finetuning for biomedical LLMs and shaping their methodological choices.  The paper's impact extends beyond this specific application, making it highly relevant and significant to the broader AI research community.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Monica Agrawal", "paper_title": "Large Language Models are Few-Shot Clinical Information Extractors", "reason": "This paper directly addresses the application of LLMs to clinical information extraction, a key task within the biomedical domain. Its focus on few-shot learning, similar to the current paper, highlights the potential of efficient adaptation of LLMs to specific medical tasks, making it highly relevant to the research presented in the current paper.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Yizhong Wang", "paper_title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks", "reason": "This work directly tackles the problem of instruction tuning for LLMs and proposes a benchmark dataset with a large number of tasks and instructions. It's particularly relevant to the current paper's emphasis on instruction-based fine-tuning in the biomedical domain and provides a strong foundation for comparison.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This is a seminal paper introducing the concept of aligning language models with human feedback during instruction tuning. It showcases that models fine-tuned using human feedback demonstrate improved performance in following instructions, improving their ability to generalize across various tasks. This directly contributes to the understanding of instruction finetuning methods used in the current work.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Shayne Longpre", "paper_title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning", "reason": "This paper focuses on designing effective instruction tuning datasets and methods.  Its contribution to creating high-quality instruction datasets, with a focus on improving the generalization ability of models, significantly relates to the current research. The design considerations discussed in this paper directly impact the methodology used to develop and evaluate the MEDINST dataset.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Yu Gu", "paper_title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing", "reason": "This paper is highly relevant as it focuses on adapting LLMs for biomedical NLP tasks.  The emphasis on domain-specific pre-training offers a contrasting approach to the current paper's focus on instruction tuning, providing valuable context and highlighting the different strategies used to adapt LLMs for medical applications.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Jason Wei", "paper_title": "Finetuned Language Models Are Zero-Shot Learners", "reason": "This work demonstrates the surprising capability of fine-tuned language models to perform well on tasks they have not been explicitly trained on.  This concept is central to the current paper's exploration of instruction tuning's ability to promote generalization in biomedical LLMs, justifying the approach taken in the study.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Pengcheng Qiu", "paper_title": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMO on Ten Benchmarking Datasets", "reason": "This paper serves as a crucial baseline for understanding the challenges and potential of pre-trained language models in the biomedical field.   Its comprehensive evaluation across various tasks helps to contextualize and strengthen the novelty of the current paper's work which applies instruction tuning to address the limitations highlighted in this reference.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Jason Alan Fries", "paper_title": "BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing", "reason": "This work introduces a framework for data-centric biomedical NLP, focusing on the creation and organization of datasets. Its relevance stems from the emphasis on data quality and accessibility, aligning with the approach and aims of the current research in establishing a comprehensive biomedical instruction dataset.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yanis Labrak", "paper_title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains", "reason": "This paper is directly relevant to the current work, focusing on the development of pretrained large language models for the biomedical domain.  The authors' work shares the aim of improving the capabilities of LLMs for medical applications, but takes a different approach, allowing for a valuable comparison with the current paper's instruction-based fine-tuning method.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Tianyu Han", "paper_title": "MedAlpaca - An Open-Source Collection of Medical Conversational AI Models and Training Data", "reason": "This paper is directly relevant because it focuses on creating medical conversational AI models using instruction-based fine-tuning. The authors' release of an open-source dataset and models offers a valuable comparison point for the current paper's work on MEDINST.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yunxiang Li", "paper_title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge", "reason": "This work shares a similar goal of applying LLMs to medical tasks, focusing on using fine-tuning and medical domain knowledge to improve performance in a specific application (medical chatbots).  This is highly relevant to the current paper's investigation of instruction fine-tuning in the biomedical domain, providing another comparative approach.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Xinlu Zhang", "paper_title": "AlpaCare:Instruction-tuned Large Language Models for Medical Application", "reason": "This work is highly relevant as it directly addresses the use of instruction-tuned large language models for medical applications. The focus on instruction tuning aligns with the current research, providing a strong comparative point for evaluating the effectiveness of the proposed approach.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yizhong Wang", "paper_title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "reason": "This work is closely related to the current research, focusing on the effectiveness of instruction tuning using self-generated data. This methodology contributes to understanding the potential and limitations of instruction tuning, enhancing the validity of the current paper's approach.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "reason": "This paper introduces a benchmark for evaluating multitask language understanding in a broad range of domains. It is significant to the current research because it provides a framework for evaluating and comparing the performance of LLMs across various tasks, including those in the biomedical domain, offering a perspective for evaluating model generalization capabilities.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Asma Ben Abacha", "paper_title": "On the Summarization of Consumer Health Questions", "reason": "This paper's focus on summarization of consumer health questions is directly relevant to biomedical NLP. It helps to understand the challenges and nuances of specific NLP tasks within the biomedical domain, offering a specific example of the broader capabilities of the MEDINST dataset.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Tushar Khot", "paper_title": "SciTail: A Textual Entailment Dataset from Science Question Answering", "reason": "This paper provides a textual entailment dataset from science question answering.  This is relevant to the current paper because it represents a specific task type within the broader category of biomedical NLP tasks included in the MEDINST dataset, offering a concrete example within one of the task categories.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Yao Lu", "paper_title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles", "reason": "This work directly addresses the challenge of multi-document summarization, a complex task that involves integrating information from multiple sources.  This is relevant to the current research as it shows the potential for LLMs to handle complex tasks by using instruction tuning, similar to what is investigated in the current paper.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Clara H. McCreery", "paper_title": "Effective Transfer Learning for Identifying Similar Questions: Matching User Questions to COVID-19 FAQs", "reason": "This paper's focus on applying transfer learning to improve question-answering systems is relevant to the current research because it demonstrates the potential of adapting pre-trained language models for specific tasks, highlighting the effectiveness of instruction-based fine-tuning.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Albert Q. Jiang", "paper_title": "Mistral 7B", "reason": "This paper introduces a new large language model, Mistral 7B. The model's architecture and performance are relevant to the current paper's evaluation of LLMs on the MEDINST32 benchmark. The inclusion of Mistral 7B as a baseline provides a comparison of the proposed method's performance with a state-of-the-art language model.", "section_number": 4}]}