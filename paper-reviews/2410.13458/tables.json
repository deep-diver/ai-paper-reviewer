[{"figure_path": "2410.13458/tables/table_2_0.html", "caption": "Table 1: Comparison of MEDINST to several datasets in biomedical field.", "description": "Table 1 compares MEDINST with other biomedical datasets, highlighting the number of tasks, instructions, and other key characteristics.", "section": "1 Introduction"}, {"figure_path": "2410.13458/tables/table_4_0.html", "caption": "Table 1: Comparison of MEDINST to several datasets in biomedical field.", "description": "The table compares MEDINST with other biomedical datasets across several features, including the presence of task instructions, multi-task datasets, examples, and public availability, as well as the number of tasks, instructions, annotated task types, and average task definition length.", "section": "1 Introduction"}, {"figure_path": "2410.13458/tables/table_4_1.html", "caption": "Table 2: Dataset statistics across various categories.", "description": "Table 2 presents the number of datasets, instructions, and samples across 12 categories in the MEDINST and MEDINST32 datasets.", "section": "3.1 Tasks"}, {"figure_path": "2410.13458/tables/table_8_1.html", "caption": "Table 4: Multiple-choice accuracy evaluation on MMLU-Medicine, a subset of MMLU benchmark. The subjects used are anatomy (An), clinical knowledge (CK), college biology (CB), college medicine (CM), medical genetics (MG) and professional medicine (PM).", "description": "Table 4 presents the multiple-choice accuracy evaluation results of various models on the MMLU-Medicine benchmark, a subset of the MMLU benchmark, across six medical subjects.", "section": "4.4 Evaluation on Public English Benchmarks"}, {"figure_path": "2410.13458/tables/table_13_0.html", "caption": "Table 1: Comparison of MEDINST to several datasets in biomedical field.", "description": "Table 1 compares MEDINST with other datasets in the biomedical field across several key features, such as the presence of task instructions, multi-task datasets, examples, and public availability, along with the number of tasks, instructions, annotated task types and average task definition lengths.", "section": "1 Introduction"}, {"figure_path": "2410.13458/tables/table_14_0.html", "caption": "Table 11: Dataset collection.", "description": "Table 11 lists all the datasets employed in MEDINST, showing the train, dev, and test sizes for each dataset and task.", "section": "3 MEDINST: Meta Dataset of Biomedical Instructions"}, {"figure_path": "2410.13458/tables/table_15_0.html", "caption": "Table 1: Comparison of MEDINST to several datasets in biomedical field.", "description": "Table 1 compares MEDINST to other biomedical datasets across several key features, such as the presence of task instructions, multi-task datasets, examples, and the number of tasks and samples.", "section": "1 Introduction"}, {"figure_path": "2410.13458/tables/table_16_0.html", "caption": "Table 3: Test results of various models on MEDINST32. \u2020 indicates that the training sets of LLaMA3-MI includes the corresponding training sets of the datasets used by MEDINST32, whereas other models have not seen the MEDINST32 dataset. \u2193 represents that a lower score is better, while for other metrics, a higher score is better. The best and second-best results for each row are highlighted in bold and underlined, respectively. For the baselines, we use a few-shot prompt, providing two examples in the instruction. For the fine-tuned models, we use a zero-shot prompt.", "description": "Table 3 presents the evaluation results of different LLMs on MEDINST32, a benchmark dataset of 32 biomedical tasks with varying difficulty levels, comparing their performance with and without fine-tuning on the MEDINST dataset.", "section": "4.2 Results"}, {"figure_path": "2410.13458/tables/table_16_1.html", "caption": "Table 10: TRANSL task: ParaMed results.", "description": "Table 10 presents the performance of various models on the ParaMed dataset using BERTScore and METEOR Score metrics for translation task.", "section": "4.2 Results"}, {"figure_path": "2410.13458/tables/table_17_0.html", "caption": "Table 11: Dataset collection.", "description": "Table 11 shows the train, dev, and test set sizes for each dataset used in the MEDINST dataset collection.", "section": "3 MEDINST: Meta Dataset of Biomedical Instructions"}, {"figure_path": "2410.13458/tables/table_18_0.html", "caption": "Table 11: Dataset collection.", "description": "The table presents the datasets used in MEDINST, categorized by task (e.g., NER, QA, RE), and shows the number of training, development, and test samples for each dataset.", "section": "3 MEDINST: Meta Dataset of Biomedical Instructions"}, {"figure_path": "2410.13458/tables/table_19_0.html", "caption": "Table 11: Dataset collection.", "description": "The table presents the details of the 133 biomedical NLP tasks included in the MEDINST dataset, categorized into 12 categories, showing the number of training, development, and test samples for each task.", "section": "3 MEDINST: Meta Dataset of Biomedical Instructions"}, {"figure_path": "2410.13458/tables/table_20_0.html", "caption": "Table 11: Dataset collection.", "description": "Table 11 presents the dataset employed in MEDINST, showing the number of training, development and test samples for each task.", "section": "3 MEDINST: Meta Dataset of Biomedical Instructions"}]