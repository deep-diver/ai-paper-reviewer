[{"figure_path": "https://arxiv.org/html/2503.02199/x1.png", "caption": "Figure 1: Illustration of the \u201cBlind Faith in Text\u201d phenomenon in Vision-Language Models (VLMs). These models demonstrate a strong tendency to trust textual data, when it is inconsistent with the visual data or even incorrect.", "description": "The figure illustrates a scenario where a Vision-Language Model (VLM) is given an image of a pizza with green broccoli and text stating that the pizza has green pepper. When asked what green vegetable is on the pizza, the VLM incorrectly answers \"pepper\" because it prioritizes the text information over the visual information. This highlights the \"blind faith in text\" phenomenon, where VLMs disproportionately trust textual data even when it contradicts visual evidence or is wrong.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.02199/x2.png", "caption": "Figure 2: Prompt for generating matched and corrupted text given an image, the question and the ground-truth answer. We substitute {question} and {answer} with the specific sample.", "description": "This figure shows the prompt used to generate both correct and incorrect text descriptions for images. Given an image, question, and ground truth answer, the prompt instructs a large language model (LLM) to generate two descriptions. Description 1 is accurate and allows for correct answering of the question without referring to the image. Description 2 is inaccurate and leads to a wrong answer when the question is answered using only the text.", "section": "2. Preliminaries"}, {"figure_path": "https://arxiv.org/html/2503.02199/x3.png", "caption": "Figure 3: Model behaviors over different models when text is corrupted, matched or irrelevant.", "description": "This figure presents a comparison of how different vision-language models (VLMs) behave when presented with visual data and text that is either consistent (matched), inconsistent (corrupted), or unrelated (irrelevant) to the visual data.  It visualizes the model's tendency to favor textual information ('text bias'), even when it contradicts the visual input. Specifically, it displays the proportion of times each model chooses an answer consistent with the image, consistent with the text, or neither, for each of the three text conditions (matched, corrupted, irrelevant). This allows for an analysis of how different models handle inconsistencies between visual and textual information, highlighting the potential for 'blind faith in text'.", "section": "3. Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02199/x4.png", "caption": "Figure 4: Text Preference Ratio (TPR) of all models under different text variations. Most models exhibit high text preference bias when the textual information is relevant even if they are incorrect, especially for open models. Among the proprietary models, Claude-Sonnet exhibits the strongest robustness to corrupted text.", "description": "Figure 4 presents a bar chart visualizing the Text Preference Ratio (TPR) across ten different vision-language models (VLMs) under three text conditions: matching, corrupted, and irrelevant.  The TPR quantifies each model's tendency to favor textual information over visual information when inconsistencies exist. High TPR values (above 50%) indicate a strong preference for text, even when incorrect. The chart reveals a significant text bias in most models, especially the open-source models, which often exhibit TPRs above 80% under both matching and corrupted text.  This illustrates a phenomenon the authors term \"blind faith in text\". In contrast, among proprietary models, Claude-Sonnet shows greater resilience to corrupted text.", "section": "3.2. Blind Faith in Text"}, {"figure_path": "https://arxiv.org/html/2503.02199/x5.png", "caption": "Figure 5:  The effect of different factors (prompting, language model size, text relevance) on text bias. Left: Instructional prompts influence modality preference slightly; text preference drops from 16.8%percent16.816.8\\%16.8 % to 14.2%percent14.214.2\\%14.2 % with \u201cFocus on Image\u201d vs. \u201cFocus on Text\u201d in QwenVL-2-7B. Middle: Scaling the language models (7B, 13B, 34B) in LLaVA-NeXT models decreases text bias but only marginally. Right: Increasing text relevance to the query with BM25 retrieval, raises text bias.", "description": "Figure 5 investigates how prompting, language model size, and text relevance affect the tendency of vision-language models to prioritize text over visual data when inconsistencies exist. The left panel shows that while instructions can slightly influence modality preference (a decrease from 16.8% to 14.2% text preference when prompting for image focus instead of text focus in the QwenVL-2-7B model), the impact is minimal. The middle panel demonstrates that increasing the language model size in LLaVA-NeXT models (from 7B to 34B parameters) modestly reduces this text bias. Finally, the right panel reveals that enhancing the relevance of text to the query (using BM25 retrieval) exacerbates the text bias, highlighting that highly relevant text can disproportionately influence model decisions, even when visual information contradicts it.", "section": "3. Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02199/x6.png", "caption": "Figure 6: Effect of token order on text bias: Placing text tokens before image tokens increases text bias in Phi3.5.", "description": "This figure demonstrates how altering the order of input tokens (text before image vs. image before text) affects the model's tendency to prioritize textual information, even when it contradicts visual evidence.  The experiment focuses on the Phi3.5 model and shows a clear increase in text bias when text tokens precede image tokens across various text conditions (matched, corrupted, irrelevant). This highlights the influence of token order, likely stemming from positional biases inherent in the language model architecture, on the model's ability to handle multi-modal inconsistencies.", "section": "3. Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02199/x7.png", "caption": "Figure 7: \nEffect of uni-modality certainty on model modality preference. Image/Text certainties are divided into three quantile bins, with higher values indicating higher certainty. Models favor visual data when image certainty is high and text certainty is low, and vice versa. When both certainties are low, models often produce Other answers instead of favoring one modality alone.", "description": "This figure analyzes how vision-language models (VLMs) choose between visual and textual data when there are inconsistencies.  It divides image and text certainty into three levels (low, medium, high) and shows how the model's preference (image, text, or other) changes depending on the certainty of each modality.  High image certainty and low text certainty lead models to prefer visual data, while the opposite leads to a text preference. When both are low, the model tends to produce an answer that isn't solely based on either modality.", "section": "3. Influencing Factors"}, {"figure_path": "https://arxiv.org/html/2503.02199/extracted/6247452/figures/llava-next-7b_vs_behave.png", "caption": "Figure 8: Left: The effect of text-only data in SFT. Right: The effect of data volume in SFT.", "description": "Figure 8 presents a dual analysis of supervised fine-tuning (SFT).  The left panel demonstrates how including text-only data during SFT impacts the model's ability to distinguish between correctly and incorrectly phrased text. It shows that text-only data is crucial for maintaining a model's language capabilities while simultaneously reducing reliance on corrupted or irrelevant textual information. The right panel examines how increasing the volume of data used for SFT affects the model's tendency to trust text over images, especially when the text is incorrect. It indicates that increasing the amount of data used for SFT is effective in reducing the model's over-reliance on flawed text information.", "section": "4.2. Supervised Finetuning (SFT)"}, {"figure_path": "https://arxiv.org/html/2503.02199/extracted/6247452/figures/gpt4o_vs_behavior.png", "caption": "(a)", "description": "This figure presents a detailed breakdown of model behavior across various text conditions (match, corruption, irrelevance) for the VQAv2 dataset. It showcases the proportion of model answers consistent with image-based answers, text-based answers, and other cases where neither modality aligns. Each bar represents a different vision-language model, providing a comprehensive view of the 'blind faith in text' phenomenon and its impact on model performance.", "section": "3. Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02199/x8.png", "caption": "(b)", "description": "This figure presents the performance of various vision-language models across different datasets (DocVQA) under various text conditions (match, corruption, irrelevance).  For each model, it displays the base accuracy, accuracy under text corruption, normalized accuracy (considering base accuracy), and the text preference ratio (TPR).  The TPR shows the model's preference for text-based answers over image-based answers, particularly useful for highlighting the 'blind faith in text' phenomenon.  The macro accuracy represents the average accuracy across the three text conditions.", "section": "3. Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02199/x9.png", "caption": "(c)", "description": "This figure displays the performance of various vision-language models across four distinct datasets (VQAv2, DocVQA, MathVista, and Brand Detection) under three different text conditions: matched, corrupted, and irrelevant text.  The bar chart shows the accuracy of each model for each dataset and text condition.  The TPR (Text Preference Ratio) is also shown to indicate the model's preference for trusting text over visual data.  This helps understand the extent of the \"blind faith in text\" phenomenon in different vision-language models under varying conditions.", "section": "3. Empirical Analysis"}]