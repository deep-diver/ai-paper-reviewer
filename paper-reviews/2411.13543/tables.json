[{"content": "| Skills | BabyAI | TextWorld | Crafter | Baba Is AI | MiniHack | NLE |\n|---|---|---|---|---|---|---|\n| Navigation | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 |\n| Exploration | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 |\n| Resource Management | \u2717 | \u2714 | \u2714 | \u2717 | \u2714 | \u2714 |\n| Complex Credit Assignment | \u2717 | \u2717 | \u2714 | \u2714 | \u2714 | \u2714 |\n| Deducing Env. Dynamics | \u2717 | \u2717 | \u2717 | \u2714 | \u2714 | \u2714 |\n| Long-term Planning | \u2717 | \u2717 | \u2717 | \u2714 | \u2714 | \u2714 |\n| Turns to Complete | 10<sup>1</sup> | 10<sup>2</sup> | 10<sup>3</sup> | 10<sup>2</sup> | 10<sup>4</sup>\u201310<sup>5</sup> |\n| Time to Master for Humans | Seconds | Minutes | Hours | Hours | Hours | Years |", "caption": "Table 1: The tested skills, time horizons, and complexities of interactive decision-making tasks evaluated in BALROG. Compared to existing benchmarks, BALROG provides infrastructure for evaluating model reasoning and decision-making on harder, longer time-horizon interactive settings. The evaluated tasks span a range of difficulties.", "description": "Table 1 presents a comparison of six interactive decision-making tasks across various aspects: the skills required (navigation, exploration, resource management, credit assignment, understanding environment dynamics, and long-term planning), the time horizon (seconds to years), and the difficulty for human players to master.  It highlights that BALROG, unlike previous benchmarks, offers a platform to rigorously evaluate AI models' reasoning abilities in complex scenarios demanding more extended interactions and greater difficulty, thus bridging the gap between simpler benchmarks and true real-world application.", "section": "2 BALROG"}, {"content": "| Model | Average Progress (%) |\n|---|---| \n| gpt-4o | 32.34 \u00b1 1.49 |\n| claude-3.5-sonnet | 29.98 \u00b1 1.98 |\n| llama-3.1-70b-it | 27.88 \u00b1 1.43 |\n| llama-3.2-90B-it | 23.66 \u00b1 1.09 |\n| gemini-1.5-pro | 21.00 \u00b1 1.18 |\n| gpt-4o-mini | 17.36 \u00b1 1.35 |\n| llama-3.1-8b-it | 14.14 \u00b1 1.51 |\n| llama-3.2-11B-it | 13.54 \u00b1 1.05 |\n| gemini-1.5-flash | 9.73 \u00b1 0.77 |\n| llama-3.2-3B-it | 8.47 \u00b1 1.12 |\n| llama-3.2-1B-it | 6.32 \u00b1 1.00 |", "caption": "Table 8: LLM Performance on Textworld", "description": "This table presents the average performance of various Large Language Models (LLMs) on the TextWorld environment, specifically focusing on three distinct tasks: Treasure Hunter, Cooking Game, and Coin Collector.  The average progress is measured as a percentage, indicating how well each LLM performed in completing these tasks. The results showcase the relative strengths and weaknesses of different LLMs in handling text-based game environments and complex reasoning tasks.", "section": "C.4 TEXTWORLD RESULTS"}, {"content": "| Model | Average Progress (%) |\n|---|---| \n| claude-3.5-sonnet | 29.08 \u00b1 2.21 |\n| gemini-1.5-pro | 25.76 \u00b1 1.36 |\n| gpt-4o | 22.56 \u00b1 1.44 |\n| gpt-4o-mini | 15.36 \u00b1 1.29 |\n| gemini-1.5-flash | 14.94 \u00b1 1.40 |\n| llama-3.2-90B-it | 13.43 \u00b1 1.16 |\n| llama-3.2-11B-it | 6.91 \u00b1 0.84 |", "caption": "Table 15: Comparison of each LLMs (ability to apply) knowledge in Nethack. We manually grade the responses to each question based on the correctness of the response given (i.e. does the response match information from the Nethack wiki), the correctness of their conclusion (i.e. does the LLM correctly identify that such behaviour should be avoided), and whether an LLM agent\u2019s behaviour during evaluation is consistent with the ground truth (i.e. does the agent successfully avoid the behaviours indicated in the questions). For answers that are partially correct, we award a \n\n\u223csimilar-to\\simbold_\u223c\n. We record behaviour as N/A when the agent does not encounter scenarios where knowledge of the corresponding question should be applied.", "description": "Table 15 assesses the ability of various LLMs to apply their NetHack knowledge.  It evaluates three aspects: the correctness of their answers to NetHack-related questions (compared to the NetHack Wiki), whether they correctly identify actions to avoid, and whether their in-game behavior reflects this understanding. A tilde (~) indicates a partially correct answer.  N/A signifies that the agent did not encounter a situation requiring that specific knowledge.", "section": "4.1 Qualitative Analysis"}]