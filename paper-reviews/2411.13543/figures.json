[{"figure_path": "https://arxiv.org/html/2411.13543/extracted/6011244/images/Diagram_v15.png", "caption": "Figure 1: An overview of the BALROG Benchmark for evaluating LLMs on long-context interactive tasks. Submissions of new inference-time methods for improving the capabilities of an existing model via an \u201cagentic strategy\u201d need only modify the agent.py file.\nSimilarly, benchmarking a new model zero-shot can be done by adjusting a configuration file in client.py. The agent class includes a prompt builder to manage observation history, and a client that abstracts the complexities of various APIs and model-serving frameworks. The env_wrapper.py file standardizes interaction across settings, and the evaluator executes agents and collects performance metrics.", "description": "This figure shows the architecture of the BALROG benchmark, which is designed to evaluate the performance of large language models (LLMs) in long-context interactive tasks.  The benchmark uses a modular design, allowing researchers to easily test new models and inference methods. The main components include: agent.py (where the agent's logic for decision making resides), client.py (handling interactions with various LLMs/APIs), env_wrapper.py (standardizing interactions with different game environments), and evaluator.py (running episodes and collecting performance metrics). The diagram illustrates how these components interact during the evaluation process.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2411.13543/extracted/6011244/images/llm_vlm_results_v13.png", "caption": "Figure 2: Baselines for BALROG. We evaluate the zero-shot performance of seven state-of-the-art and long-context LLMs and VLMs on BALROG. During each timestep of interaction, models are prompted to output the next in-game action conditioned on past interaction history. Standard error is obtained by running multiple replicate seeds, as detailed in the Appendix.", "description": "This figure displays the zero-shot performance of seven state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) on the BALROG benchmark across four different games: BabyAI, Crafter, TextWorld, and MiniHack.  Each model's performance is represented by a bar graph showing the percentage of progress achieved in each game. The x-axis shows the different LLMs and VLMs tested, and the y-axis represents the percentage of game completion.  Error bars indicate the standard error calculated from multiple runs to account for randomness in the games and agents\u2019 behaviors.  The results show significant variations in performance across different models and game types, with some models excelling in simpler games while failing to make progress in more complex ones.", "section": "4 Results"}, {"figure_path": "https://arxiv.org/html/2411.13543/extracted/6011244/images/crafter_map.png", "caption": "Table 2: Language-Only Performance", "description": "This table presents the average progress percentage achieved by different Language Models (LLMs) across various tasks in the BALROG benchmark, specifically focusing on tasks where only textual information (language) is provided as input to the models.  The results illustrate the relative performance of each LLM in terms of successfully completing the given tasks using only language-based decision-making.  Higher percentages indicate better performance.", "section": "4 RESULTS"}, {"figure_path": "https://arxiv.org/html/2411.13543/extracted/6011244/images/textworld_map.png", "caption": "Table 3: Vision-Language Performance", "description": "This table presents the average progress (%) achieved by various Vision-Language Models (VLMs) on the BALROG benchmark.  The VLMs were evaluated using a combination of visual and textual information from the game environments.  The results show the average performance across multiple runs, indicating the effectiveness of each model in integrating both visual and textual cues for decision-making in challenging game scenarios.  The models tested include Claude 3.5-sonnet, GPT-40, Gemini-1.5-Pro, Gemini-1.5-flash, and several versions of the LLaMa model.", "section": "4 RESULTS"}, {"figure_path": "https://arxiv.org/html/2411.13543/extracted/6011244/images/babaisai_map.png", "caption": "Table 4: LLM Performance on BabyAI", "description": "This table presents the average performance of various Large Language Models (LLMs) on the BabyAI tasks.  The performance metric is the average progress percentage across five different BabyAI tasks, calculated using 25 separate runs for each model.  The results show the zero-shot performance of each LLM; that is, the performance achieved by the model without any fine-tuning or additional training on the BabyAI environment.  Higher percentages indicate better performance.", "section": "A.2 BABYAI RESULTS"}, {"figure_path": "https://arxiv.org/html/2411.13543/extracted/6011244/images/minihack_corridor.png", "caption": "Table 5: VLM Performance on BabyAI", "description": "This table presents the average progress percentage achieved by various Vision-Language Models (VLMs) on the BabyAI tasks within the BALROG benchmark.  The results show the average performance across multiple runs, with standard errors indicating the variability of the results. Lower numbers indicate lower performance.  The models tested include: gpt-40, gemini-1.5-pro, claude-3.5-sonnet, gemini-1.5-flash, llama-3.2-11B-it, llama-3.1-8B-it, llama-3.2-3B-it, and llama-3.2-1B-it.", "section": "A.2 BABYAI RESULTS"}, {"figure_path": "https://arxiv.org/html/2411.13543/extracted/6011244/images/minihack_corridor_battle.png", "caption": "Figure 3: Crafter\u2019s examples of unique procedurally generated maps.", "description": "Figure 3 shows three examples of procedurally generated game maps from the Crafter environment.  Each map is unique, featuring different terrain types, resource locations, and overall layouts. The procedural generation ensures that agents playing Crafter cannot rely on memorization of specific map features, requiring them to adapt their strategies to new and unforeseen environments in each playthrough.", "section": "2.1 Environments"}, {"figure_path": "https://arxiv.org/html/2411.13543/extracted/6011244/images/minihack_quest_hard.png", "caption": "Table 6: LLM Performance on Crafter", "description": "This table presents the performance of different Large Language Models (LLMs) on the Crafter game environment.  The performance metric is 'Average Progress (%)', representing the average progress achieved by each LLM in completing the game's tasks.  The results show the average progress and the associated standard error, calculated across multiple runs for each model. This allows for a comparison of how well each LLM performs in terms of the long-term planning, resource management, and exploration skills that are necessary to succeed in the Crafter game.", "section": "4 Results"}, {"figure_path": "https://arxiv.org/html/2411.13543/extracted/6011244/images/minihack_boxoban.png", "caption": "Table 7: VLM Performance on Crafter", "description": "This table presents the average performance of various Vision-Language Models (VLMs) on the Crafter game environment.  The performance is measured as the average progress percentage achieved by each VLM, taking into account standard errors derived from multiple runs.  The table helps to compare the relative effectiveness of different VLMs in this specific game environment, which requires a combination of language understanding and visual perception.", "section": "4 Results"}, {"figure_path": "https://arxiv.org/html/2411.13543/extracted/6011244/images/dungeon_levels.png", "caption": "Figure 4: TextWorld interface along with visualization.", "description": "This figure shows a screenshot of the TextWorld game interface.  The top part displays the game's narrative text, providing instructions and descriptions of the game environment and the player's actions. Below the text is a visual representation of the game world, showing the player's position and the location of various objects, including a carrot, lettuce, a knife, a stove, and a cookbook. On the right-hand side of the interface, the game's inventory and statistics are shown, helping the user to keep track of the player's possessions and progress within the game. The image demonstrates the combination of textual and visual information provided to the agents within the BALROG benchmark.", "section": "2.1 ENVIRONMENTS"}, {"figure_path": "https://arxiv.org/html/2411.13543/extracted/6011244/images/experience_levels.png", "caption": "Figure 5: One of the Baba Is AI puzzles, where the agent has to break the \u201cwall is stop\u201d rule, create new rule \u201cdoor is win\u201d and go to green door to solve the task.", "description": "The image shows a Baba Is You puzzle.  The goal is to reach the green door, which is currently blocked by a \"wall is stop\" rule. To solve the puzzle, the agent must move the text blocks to create a new rule, such as \"door is win\", thereby changing the game's rules and making it possible to reach the goal.", "section": "2.1 Environments"}]