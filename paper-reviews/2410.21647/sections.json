[{"page_end_idx": 1, "page_start_idx": 1, "section_number": 1, "section_title": "Introduction", "details": {"details": "Large language models (LLMs) have shown significant progress in code generation, achieving high accuracy in simple coding tasks. However, this success raises the question of whether LLMs can replace human programmers.  Existing benchmarks, such as HumanEval and MBPP, are limited in their ability to accurately reflect real-world software development because they contain manually-crafted, simplistic tasks. These benchmarks lack the complexities of actual code generation, which often involves file-level and repository-level dependencies.  Therefore, assessing the capabilities of LLMs for real-world applications requires more robust and challenging benchmarks.", "first_cons": "Existing benchmarks (HumanEval, MBPP) are insufficient to assess LLMs' real-world code completion capabilities because they are manually crafted, simple, and lack realistic complexity.", "first_pros": "LLMs demonstrate remarkable code generation capabilities, achieving high accuracy (more than 90 pass@1) in solving Python coding problems from existing benchmarks like HumanEval and MBPP.", "keypoints": ["Current benchmarks are insufficient for evaluating real-world LLM code generation capabilities due to their lack of realistic complexity and context.", "Real-world code generation often involves file-level and repository-level dependencies, unlike in simple benchmarks.", "A new benchmark (REPOCOD) is needed to accurately assess LLMs' ability to handle the complexity of real-world software development tasks.", "REPOCOD will address the limitations of existing benchmarks by using real-world code and providing realistic task complexity and reliable evaluation metrics"], "second_cons": "Manually-crafted code completion tasks do not accurately reflect the complexities and dependencies present in real-world software development projects.", "second_pros": "High accuracy (more than 90 pass@1) in existing benchmarks suggests significant LLM potential in code generation.", "summary": "Current code generation benchmarks are inadequate for evaluating LLMs' ability to replace human programmers because they lack the complexity and real-world context of actual software development tasks."}}, {"page_end_idx": 3, "page_start_idx": 2, "section_number": 2, "section_title": "LLMs for Code", "details": {"details": "Large language models (LLMs) have shown promise in code generation tasks, but existing benchmarks often fall short of reflecting real-world complexities.  While LLMs achieve high accuracy on simple, self-contained coding problems, their performance significantly degrades when dealing with more intricate, project-level tasks that involve multiple files and dependencies. This highlights a crucial gap between current LLM capabilities and the demands of practical software development.  To address this issue, researchers have proposed new benchmarks, some of which focus on multi-line code and project-level context, but even these can suffer from limitations in evaluating the true correctness of generated code, often resorting to metrics such as code similarity rather than functional correctness.  The development of robust, comprehensive benchmarks that accurately assess the performance of LLMs in real-world scenarios is vital to better understanding their applicability in software engineering.", "first_cons": "Existing benchmarks, such as HumanEval and MBPP, use manually crafted, simplistic code generation tasks, failing to reflect the complexities of real-world software development.", "first_pros": "LLMs demonstrate remarkable performance on simple, self-contained code generation tasks.", "keypoints": ["Existing benchmarks are insufficient for evaluating real-world LLM code generation capabilities.", "Real-world code generation tasks involve realistic complexity and project-level dependencies.", "Reliable evaluation metrics must assess functional correctness, not just code similarity."], "second_cons": "Existing project-level benchmarks sometimes use similarity-based metrics that fail to accurately reflect the functional correctness of LLM generated code.", "second_pros": "New benchmarks like ClassEval, CoderEval, and RepoBench incorporate more challenging, multi-line code generation tasks and project-level dependencies.", "summary": "LLMs excel at simple code tasks, but current benchmarks inadequately assess their real-world code generation abilities, emphasizing the need for more comprehensive and functionally accurate evaluations."}}, {"page_end_idx": 4, "page_start_idx": 3, "section_number": 3, "section_title": "REPOCOD Benchmark", "details": {"details": "REPOCOD is a new code generation benchmark dataset designed to evaluate the capabilities of large language models (LLMs) in real-world software development scenarios.  It addresses the limitations of existing benchmarks by including 980 complex, multi-line code completion tasks from 11 popular real-world projects.  REPOCOD's tasks require file-level or repository-level context, have long average canonical solution lengths (331.6 tokens), and high average cyclomatic complexity (9.00).  It uses developer-written test cases for reliable evaluation, overcoming the limitations of previous similarity-based metrics. REPOCOD's comprehensive design and challenging tasks reveal that even state-of-the-art LLMs struggle to achieve high accuracy, highlighting the need for further advancements in LLM technology for real-world code generation.", "first_cons": "Existing benchmarks lack realistic complexity and reliable evaluation metrics.", "first_pros": "REPOCOD includes real-world, complex code completion tasks.", "keypoints": ["**980 problems** from 11 real-world projects", "Requires **file/repository-level context**", "**Longest average solution length** (331.6 tokens)", "**High average cyclomatic complexity** (9.00)", "Uses **developer-written test cases** for evaluation", "Reveals limitations of current LLMs in real-world code generation"], "second_cons": "Existing benchmarks primarily focus on single-line code or simple problems.", "second_pros": "REPOCOD provides a comprehensive and challenging evaluation of LLMs' real-world code generation capabilities.", "summary": "REPOCOD, a new code generation benchmark dataset, features 980 complex, real-world problems requiring file or repository-level context, longer solutions, and developer-written tests, revealing the limitations of current LLMs in real-world scenarios."}}, {"page_end_idx": 5, "page_start_idx": 4, "section_number": 4, "section_title": "Benchmark Statistics", "details": {"details": "REPOCOD, a new code generation benchmark dataset, boasts 980 real-world instances from 11 repositories.  These instances feature an average canonical solution length of **331.6 tokens** and **cyclomatic complexity of 9.00**, significantly exceeding existing benchmarks.  Three context complexity levels are defined: repository-level, file-level, and self-contained.  **REPOCOD's repository-level instances (26.22%)** pose the most significant challenge for LLMs, necessitating a high degree of context understanding.  The dataset includes developer-written test cases for robust evaluation, avoiding the shortcomings of similarity-based metrics used in previous benchmarks.  This data provides a more rigorous and realistic assessment of LLM code generation capabilities than previously available.", "first_cons": "While REPOCOD is more challenging and realistic than previous benchmarks, its size (980 instances) might still be considered limited for extremely comprehensive analysis.  The focus on Python could limit the generalizability of findings to other programming languages.", "first_pros": "The inclusion of developer-written test cases for evaluation is a major strength, providing much more reliable results than previous benchmarks that relied on approximate metrics.  The detailed categorization of problems by context complexity (repository, file, and self-contained) allows for more nuanced analysis of LLM performance.", "keypoints": ["**331.6 tokens average canonical solution length** and **cyclomatic complexity of 9.00** significantly exceed existing benchmarks.", "**26.22% of instances require repository-level context**, highlighting a high degree of difficulty.", "Uses developer-written test cases, offering reliable evaluation, unlike previous similarity-based approaches.", "Includes three context complexity levels (repository, file, self-contained), enabling detailed analysis of model performance across different levels of context reliance.", "REPOCOD dataset is more realistic and challenging than existing benchmarks for assessing LLM code generation capabilities in real-world software development."], "second_cons": "The current dataset is restricted to Python projects and might not fully capture the complexities of other programming languages. Future iterations could benefit from expanding the scope of included languages.", "second_pros": "The high complexity and realism of the REPOCOD benchmark improve the accuracy and reliability of LLM evaluations.  Its use of developer-written test cases improves the robustness of the evaluation, as it directly assesses correctness rather than approximate similarity.", "summary": "REPOCOD is a challenging new code generation benchmark dataset with 980 real-world problems, featuring significantly longer and more complex solutions than previous benchmarks, and utilizing developer-written test cases for reliable evaluation."}}, {"page_end_idx": 6, "page_start_idx": 5, "section_number": 5, "section_title": "Experiment Setup", "details": {"details": "This experiment evaluates the performance of various LLMs on the REPOCOD benchmark using three different retrieval settings: **sparse retrieval**, **dense retrieval**, and **current file**.  The sparse and dense retrieval methods aim to extract relevant context from the repository, while the current file method only uses the file containing the target function.  The goal is to assess the impact of context retrieval on the ability of LLMs to generate code and to determine whether the current file context is sufficient or if additional context is needed to achieve optimal performance. This setup is chosen due to the limitations of LLMs' context windows. The prompting format, including system prompts, file paths, and retrieval context, is rigorously defined to ensure consistent and reliable evaluation.", "first_cons": "The use of three different retrieval methods adds complexity to the analysis. Differences in performance might be attributed to factors beyond the LLM capabilities, such as retrieval technique effectiveness.", "first_pros": "The inclusion of three different retrieval strategies allows for a more comprehensive understanding of LLMs' ability to leverage context, and helps to mitigate potential biases stemming from a single retrieval approach. This comprehensive setup enhances experimental robustness and generalizability.", "keypoints": ["Three retrieval methods (sparse, dense, current file) are used to evaluate the impact of context on LLM code generation performance.", "The experiment focuses on identifying the most effective retrieval method for LLMs.", "The setup addresses the limitations of LLMs' context windows using selective retrieval methods.", "The study rigorously defines the prompting format for consistent evaluation results.", "The results reveal the relative strengths and weaknesses of each retrieval setting."], "second_cons": "The study relies on existing LLMs and a specific benchmark dataset. This limits the generalizability of the findings to other LLMs or code generation tasks beyond the scope of REPOCOD.", "second_pros": "The selection of current state-of-the-art LLMs and a rigorous benchmark dataset allows for a strong basis of comparison and a high standard of code generation evaluation. Using SOTA LLMs ensures the evaluation is relevant and reflects the current capabilities of advanced LLMs. ", "summary": "The experiment uses three context retrieval methods (sparse, dense, and current file) to evaluate the performance of various LLMs on the REPOCOD benchmark, focusing on the impact of context size and retrieval strategy on code generation accuracy."}}, {"page_end_idx": 7, "page_start_idx": 6, "section_number": 6, "section_title": "Model Setup", "details": {"details": "This section describes the experimental setup, focusing on the selection and configuration of large language models (LLMs) used for code generation.  The complexity of the REPOCOD benchmark necessitates a careful selection of models that meet specific requirements, including high performance on existing benchmarks like HumanEval and MBPP, and a context window exceeding 16K tokens to accommodate the length of prompts.  Three different retrieval settings are used to acquire relevant context from the repositories: **sparse retrieval (BM25)**, **dense retrieval (text-embedding-3-small)**, and **current file**. The choice of models and retrieval strategies aims to provide a comprehensive evaluation of LLMs\u2019 abilities to generate code in realistic, complex software development scenarios.  Commercial LLMs, like GPT-4, are used along with open-sourced models like CodeLlama and OpenCodeInterpreter, allowing comparison across different model architectures and sizes.  The prompting format ensures consistent evaluation across all tested LLMs.", "first_cons": "The reliance on a limited set of LLMs, which might not represent the entire landscape of available models, could potentially limit the generalizability of the results. The relatively small number of LLMs may affect the comprehensive evaluation of the performance of different architectures and sizes. The open-sourced models might have slightly different evaluation metrics compared to the commercial LLMs, thus potentially impacting the quality of the results comparison.", "first_pros": "The inclusion of both commercial and open-source LLMs, which allows comparison across different model architectures and capabilities, offers valuable insights and enhances the reliability of the research.  The selection criteria for models ensure that the evaluation incorporates top-performing LLMs and that the models have a sufficient context window size to process the task prompts effectively. Three retrieval methods are employed to ensure a robust and comprehensive evaluation of the models\u2019 capabilities.", "keypoints": ["Selection of LLMs based on performance on existing benchmarks (HumanEval, MBPP) and context window size.", "Three retrieval settings (BM25, dense retrieval, current file) to extract relevant context.", "Inclusion of both commercial and open-source models.", "Focus on LLMs with context window size exceeding 16K tokens to handle complex prompts.", "Rigorous evaluation methodology to ensure consistent and reliable results across all LLMs and retrieval methods.", "Careful evaluation of the effectiveness of various model architectures and sizes in complex code generation tasks."], "second_cons": "The focus on three specific retrieval methods, which might not cover the entire spectrum of potential retrieval techniques, could potentially limit the scope of the findings.  While the methodology includes both commercial and open-source LLMs, variations in the training data and architectures between these models may lead to inconsistencies in performance.", "second_pros": "The use of three different retrieval methods allows for a more robust and comprehensive evaluation of the effect of context on code generation, producing a richer dataset for analysis and offering a more nuanced understanding of the models' capabilities. This diverse approach, encompassing both commercial and open-source LLMs, expands the range of model architectures and sizes considered, enhancing the generalizability and applicability of the findings.", "summary": "The experiment setup in this section meticulously selects and configures LLMs, employing multiple retrieval methods to robustly evaluate their performance on complex code generation tasks within the REPOCOD benchmark."}}, {"page_end_idx": 8, "page_start_idx": 7, "section_number": 7, "section_title": "Result", "details": {"details": "**Commercial LLMs significantly outperform open-source models on REPOCOD**, achieving pass@1 scores exceeding 20%, while open-source models struggle to surpass 20%.  This highlights the current gap between the capabilities of commercial and open-source large language models in handling complex, real-world code generation tasks.  The best-performing model, GPT-40, achieves a remarkable 27.35% pass@1, demonstrating the potential but also the current limitations of LLMs in this domain.  Interestingly, even within the top-performing models, there is considerable diversity in the problems each model can successfully solve.  The evaluation also shows that, in general, retrieving the current file as context produces better results than using sparse or dense retrieval methods for the commercial models but not for the open-source models. The models' success rate decreases as the complexity of code increases, with the most significant drop occurring when the context involves repository-level information. This result is consistent across various retrieval methods, emphasizing the challenge posed by the high context complexity of repository-level code generation.", "first_cons": "Open-source LLMs lag significantly behind commercial models in performance.", "first_pros": "Commercial LLMs show promising results, with GPT-40 reaching 27.35% pass@1.", "keypoints": ["**Commercial LLMs significantly outperform open-source models**", "**GPT-40 achieves a remarkable 27.35% pass@1**", "Context retrieval method significantly impacts performance", "Model performance decreases with increasing code complexity"], "second_cons": "Even the top-performing models show limitations in handling complex, repository-level code.", "second_pros": "The study provides a comprehensive evaluation across different LLMs and retrieval methods.", "summary": "REPOCOD evaluation reveals a significant performance gap between commercial and open-source LLMs in real-world code generation, with commercial models showing much higher success rates, particularly GPT-40, but still struggling with complex, repository-level code."}}, {"page_end_idx": 11, "page_start_idx": 8, "section_number": 8, "section_title": "Limitation", "details": {"details": "The current study has limitations due to the **limited number of repositories** and **LLMs evaluated**.  REPOCOD's dataset, while extensive, only covers a subset of potential Python repositories.  Future iterations should include more repositories to provide a broader representation.  The evaluation is limited to ten models, which don't represent the whole range of LLMs available. Expanding the number of models tested would enhance the generalizability of the study's findings.  These limitations suggest further research is needed to confirm the current findings and to ensure a comprehensive assessment of the performance of LLM for repository-level code generation.", "first_cons": "**Limited number of repositories** used in the study, preventing a comprehensive representation of real-world code generation tasks.", "first_pros": "The study acknowledged its limitations, leading to suggestions for future work to enhance the scope of the study by expanding the data and models considered.", "keypoints": ["The current study only used a **limited number of repositories and LLMs**.", "Future research should involve more diverse datasets and language models for comprehensive results.", "The study acknowledges these limitations, suggesting directions for future improvements."], "second_cons": "**Limited number of LLMs** tested which limits the generalization of the results to other LLMs", "second_pros": "REPOCOD is publicly available to allow future researchers to expand the study.", "summary": "The REPOCOD study has limitations stemming from its limited scope in terms of the number of evaluated repositories and LLMs, indicating a need for further research with more diverse datasets and models to validate the conclusions."}}]