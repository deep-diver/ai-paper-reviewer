[{"figure_path": "https://arxiv.org/html/2411.06208/x1.png", "caption": "Figure 1: Alignment Paradigms (a) Existing DPO Series vs. (b) Proposed IOPO. The green arrow indicates that y\ud835\udc66yitalic_y matches x\ud835\udc65xitalic_x while the red one indicates a mismatch.", "description": "This figure illustrates two different alignment paradigms for large language models (LLMs): Direct Preference Optimization (DPO) and the proposed Input-Output Preference Optimization (IOPO).  Panel (a) shows the DPO approach, where the model generates two responses (Ywin and Yloose) to the same input instruction (X). Green arrows indicate correct alignment between input and output, while red arrows highlight mismatches. Panel (b) presents the IOPO method, which considers both input and output preferences.  The model not only learns to align with preferred outputs (Y) but also explores the input instruction's (X) preferences, refining alignment by considering subtle differences in input constraints that may lead to different desirable outputs. This allows for more nuanced and accurate alignment between the LLM's understanding of complex instructions and the desired responses.  The color-coding of the arrows remains consistent to signify correct and incorrect alignments.", "section": "3 Preliminary"}, {"figure_path": "https://arxiv.org/html/2411.06208/x2.png", "caption": "Figure 2: Construction Pipeline of Trace.", "description": "This figure illustrates the TRACE benchmark's construction pipeline, detailing the five key stages: 1) Taxonomy of Constraint: establishes a comprehensive constraint type system; 2) Constraint Expansion: expands simple instructions into more complex ones; 3) Instruction Structuring: structures instructions into Task Description, Constraints, and Input; 4) Quality Control: ensures validity by checking for redundancy and incompleteness; 5) Response Generation & Evaluation: generates responses and evaluates their compliance with constraints, selecting high-quality data for training and evaluation.", "section": "4.1 Construction Pipeline"}, {"figure_path": "https://arxiv.org/html/2411.06208/x3.png", "caption": "Figure 3: Constraint type distribution over evaluation set in Trace.", "description": "This figure shows a pie chart and a ring chart visualizing the distribution of constraint types in the TRACE benchmark's evaluation dataset. The inner pie chart displays the distribution of five main constraint types: Content, Situation, Style, Format, and Example. The outer ring chart further breaks down each main constraint type into its specific dimensions.  This provides a detailed overview of the types and complexities of constraints present in the evaluation set, illustrating the diversity of the benchmark.", "section": "4 TRACE Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.06208/x4.png", "caption": "Figure 4: Performance comparisons under the same quantity of tokens with Qwen2-7B as the base model.", "description": "This figure displays a comparison of the performance of different instruction following methods (SFT, DPO, and IOPO) using the Qwen2-7B language model. The performance is measured across several metrics (IF-S, IF-M, S-Acc, L-Acc, CSR, ISR, PSR) and datasets (TRACE, IFEval, CFBench).  The key aspect highlighted is that the comparison is done while maintaining the same quantity of tokens used for training, making it easier to understand the impact of each method independently of the training data size.", "section": "6.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.06208/x5.png", "caption": "Figure 5: Performance comparisons under the same quantity of tokens with Llama3.1-8B as the base model.", "description": "This figure compares the performance of different instruction following methods (SFT, DPO, IOPO) using the Llama 3.1-8B language model.  The comparison is performed under the constraint that all methods use the same quantity of tokens during training. The performance is measured across multiple metrics relevant to instruction following tasks, including single-constraint and multi-constraint instruction following, showing improvements made by IOPO.", "section": "6.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.06208/x6.png", "caption": "Figure 6: DPO-series Data Construction.", "description": "This figure illustrates the process of constructing the DPO (Direct Preference Optimization) training dataset.  It shows how a worse response (Yloose) is generated alongside the preferred response (Ywin) for each instruction.  This pair of responses is then used in the DPO training process to refine the model's preference alignment. The example shows a prompt requesting information about Beijing with specific constraints. The model generates both a preferred JSON response and an inferior text-based response.", "section": "4 TRACE Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.06208/x7.png", "caption": "Figure 7: IOPO Data Construction.", "description": "This figure details the construction process of the IOPO (Input-Output Preference Optimization) training dataset.  It illustrates how the dataset is built by first generating modified instructions (x2) with altered constraints from the original instruction (x1). Then, responses (y1 and y2) are generated for both the original and modified instructions.  The process involves using an LLM to generate variations in the instructions' constraints, ensuring that the generated response does not meet the new constraints. This ensures a diverse dataset representing a wider range of instruction complexities for training the IOPO model.", "section": "5 Input-Output Preference Optimization"}]