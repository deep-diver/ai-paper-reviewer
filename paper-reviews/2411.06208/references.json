{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational for RLHF, a crucial technique for aligning LLMs with human preferences, which is directly related to the topic of complex instruction following."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-12-01", "reason": "This paper introduces DPO, a significant advancement in LLM alignment that directly addresses challenges in efficiently optimizing for desired responses, highly relevant to improving complex instruction following."}, {"fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "publication_date": "2022-04-01", "reason": "This work is another highly influential paper on RLHF, providing a strong basis for understanding and improving LLM alignment, a core aspect of enhanced complex instruction following."}, {"fullname_first_author": "Yiwei Qin", "paper_title": "InFoBench: Evaluating instruction following ability in large language models", "publication_date": "2024-07-01", "reason": "This paper introduces a benchmark dataset for evaluating instruction-following abilities, directly relevant to the evaluation of complex instruction-following capabilities."}, {"fullname_first_author": "Xinghua Zhang", "paper_title": "CF-Bench: A multi-constraint instruction following benchmark for evaluating LLMs", "publication_date": "2024-07-01", "reason": "This paper introduces a benchmark dataset specifically designed for evaluating complex instructions with multiple constraints, directly relevant to the paper's focus."}]}