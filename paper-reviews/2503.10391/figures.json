[{"figure_path": "https://arxiv.org/html/2503.10391/x2.png", "caption": "Figure 1: We introduce CINEMA, a video generation framework conditioned on a set of reference images and text prompts. CINEMA enables the generation of videos visually consistent across multiple subjects from diverse scenes, providing enhanced flexibility and precise control over the video synthesis process.", "description": "CINEMA is a novel video generation framework that takes in a set of reference images and text prompts as input.  It generates videos featuring multiple subjects from various scenes, maintaining visual consistency between the subjects.  The framework offers enhanced flexibility in controlling the video synthesis process.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.10391/x3.png", "caption": "Figure 2: Our overall pipeline consists of a Multimodal Large Language Model, a semantic alignment network (AlignerNet), a visual entity encoding network, and a Diffusion Transformer that integrates the embeddings encoded by these two modules. \u2295direct-sum\\oplus\u2295 denotes concatenation.", "description": "Figure 2 illustrates the CINEMA framework architecture.  It begins with user input in the form of reference images and language instructions. These inputs are processed by three key modules: a Multimodal Large Language Model (MLLM) to encode the semantics of the input; AlignerNet, a semantic alignment network, to harmonize the MLLM's output with the format expected by the diffusion model; and a visual entity encoding network based on a 3D Variational Autoencoder (VAE) to capture fine-grained visual details from the reference images.  The outputs from the MLLM and VAE are concatenated and fed into a Multimodal Diffusion Transformer (MM-DiT), a video generation model, to produce the final output video. The figure highlights the flow of information through these modules, emphasizing the role of concatenation (\u2295) in combining different feature representations.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.10391/x4.png", "caption": "Figure 3: Our language instruction template for MLLM.", "description": "This figure shows the specific instructions given to the Multimodal Large Language Model (MLLM) to guide its processing of input data for multi-subject video generation.  The instructions emphasize key aspects of video synthesis, such as maintaining visual consistency across frames, using visual characteristics from the provided reference images, generating natural movements, seamlessly integrating subjects into the scene, and ensuring overall temporal and spatial coherence.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.10391/x5.png", "caption": "Figure 4: An example of reference images, along with the corresponding video and caption, from our training set.", "description": "This figure displays an example from the training dataset used in the CINEMA model. It shows a set of reference images depicting a woman wearing a denim shirt over a light top, smiling and gesturing against a pink background.  Alongside the images is the corresponding video generated by the model based on the images and a text caption that describes the scene. This helps visualize the input data and its relationship to the generated output used in training the model. The aim is to show the quality of the inputs used for training and the overall quality of the model outputs.", "section": "4.1. Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2503.10391/x6.png", "caption": "Figure 5: Qualitative evaluation of our method. The reference images are shown on the left, along with the text prompt at the bottom. In each case, we show four frames uniformly sampled from the generated 45-frame video.", "description": "Figure 5 presents a qualitative assessment of the CINEMA model's video generation capabilities.  For each of four examples, the figure displays the reference images used as input to the model, along with the corresponding text prompt that guides the video generation process.  Four frames, evenly spaced, are extracted from each resulting 45-frame video to visually showcase the model's output. This provides a concise visual demonstration of how well the model integrates reference images and textual prompts to create coherent and visually consistent videos.", "section": "4.2 Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2503.10391/x7.png", "caption": "Figure 6: Qualitative evaluation of our method dealing with multiple concepts. Our model is capable of encoding and understanding multiple subjects based on the reference images.", "description": "Figure 6 presents qualitative results demonstrating the model's ability to generate videos incorporating multiple subjects and concepts.  Each row shows a video generation task:  Multiple reference images are presented (one for each subject), alongside a text prompt describing the scene's context.  The corresponding generated video frames are displayed, showcasing the model's success in maintaining visual consistency of multiple subjects within a coherent scene, demonstrating its understanding of the given relationships between subjects and scene descriptions.", "section": "4.2 Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2503.10391/x8.png", "caption": "Figure 7: Qualitative comparison for ablation studies. The reference images and text prompt are shown on the top. The result of the full model is in the first line, followed by those from different ablation experiments. We show four frames uniformly sampled from the generated 45-frame video of each method.", "description": "This figure presents a qualitative comparison of the results obtained from various ablation experiments conducted on the CINEMA model. The top row displays the input: reference images and the corresponding text prompts. The subsequent rows show the generated video frames from four different ablation experiments:\n(a) The full CINEMA model.\n(b) The model with the MLLM replaced by a T5 encoder.\n(c) The model without visual entity encoding.\n(d) The model with AlignerNet randomly initialized. Each row presents four frames uniformly sampled from a generated 45-frame video. This setup helps visualize and compare the effects of each module in the architecture on the overall video generation process.", "section": "4.3. Ablation Studies"}]