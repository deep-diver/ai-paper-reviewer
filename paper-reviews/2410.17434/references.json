{"references": [{" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is foundational in the field of large language models, introducing the concept of few-shot learning that has significantly impacted the development of advanced LLMs, which are central to the current work on video-language understanding.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper details crucial advancements in training LLMs to effectively follow instructions, a key component in the development of multimodal LLMs capable of understanding and responding to complex video-related instructions.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "CLIP, introduced in this paper, is a seminal work in vision-language modeling.  Its ability to bridge visual and textual representations is foundational to many of the current multimodal models used for video understanding, including LongVU.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Xiaohua Zhai", "paper_title": "Sigmoid loss for language image pre-training", "reason": "This paper proposes SigLIP, a significant improvement in vision-language pre-training, providing a foundation for the improved scalability and performance of subsequent models, including those used for processing video data.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "Flamingo is a notable advancement in visual language models, demonstrating effective few-shot learning capabilities. Its architecture and approach have directly influenced the design of later multimodal models that address video understanding challenges.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Junnan Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "reason": "BLIP-2 represents a significant step in bridging language and image understanding by using a large language model to improve the overall capabilities. This approach has influenced similar work applied to video understanding.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Deyao Zhu", "paper_title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models", "reason": "MiniGPT-4 demonstrates a successful integration of LLMs with vision models for achieving state-of-the-art results in various vision-language tasks.  Its approach to multimodal integration has greatly influenced recent works in the field, particularly video understanding.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Hao Liu", "paper_title": "LLaVA-next: Improved reasoning, OCR, and world knowledge", "reason": "This paper represents a significant advancement in vision-language models demonstrating improved reasoning, OCR, and world knowledge capabilities.  These advancements are directly relevant to the challenging task of long-video understanding.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "KunChang Li", "paper_title": "Videochat: Chat-centric video understanding", "reason": "This paper is among the first to introduce chat-centric video understanding, emphasizing the importance of interactive dialogues in this domain.  It highlights the importance of conversational context for video understanding, which is a major area of focus in the current work.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Bo Li", "paper_title": "LLaVA-OneVision: Easy visual task transfer", "reason": "LLaVA-OneVision is a highly influential model in video understanding and related tasks. Its large-scale dataset and strong performance set a high benchmark against which LongVU is compared.  The work improves the transferability of image and video understanding capabilities to new tasks.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zesen Cheng", "paper_title": "Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-LLMs", "reason": "VideoLLaMA-2 introduces significant advancements in spatial-temporal modeling and audio understanding within video LLMs, directly addressing challenges in video processing and comprehension which LongVU also tackles.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Peiyuan Zhang", "paper_title": "Long context transfer from language to vision", "reason": "This paper addresses the challenge of long-context understanding in vision tasks, a key challenge that LongVU directly addresses. This paper explores the direct transferability of long context models and techniques from language to vision applications.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Muhammad Maaz", "paper_title": "Video-chatgpt: Towards detailed video understanding via large vision and language models", "reason": "This work pioneers chat-based video understanding, creating a new benchmark that showcases the advancements in interactive methods and their implications for the processing of long videos. The interactive and conversational aspects are relevant to the direction of the current work.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Karttikeya Mangalam", "paper_title": "Egoschema: A diagnostic benchmark for very long-form video language understanding", "reason": "This paper introduces a novel benchmark focused on long-form video understanding, offering an important evaluation set against which LongVU's performance is directly measured.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Chaoyou Fu", "paper_title": "Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis", "reason": "VideoMME is a significant benchmark that offers a comprehensive evaluation of multi-modal LLMs in video analysis.  Its inclusion of long videos directly addresses the focus of the current work on long video understanding.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Junjie Zhou", "paper_title": "Mlvu: A comprehensive benchmark for multi-task long video understanding", "reason": "MLVU is a crucial benchmark because of its focus on various multi-task aspects of video understanding, and especially its inclusion of long videos, making it particularly relevant to the evaluation of LongVU.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Maxime Oquab", "paper_title": "Dinov2: Learning robust visual features without supervision", "reason": "DINOv2 is a core component of the proposed method in LongVU. This paper's contribution of a self-supervised training method for robust visual features is crucial to the temporal reduction step of LongVU.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Peng Jin", "paper_title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding", "reason": "This paper is important because it is a relevant contemporary work that also focuses on efficient processing of video data. The comparison against this work helps to highlight the advances of the current work.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "reason": "The Phi-3 model is used as a baseline comparison in this paper. The comparison with this model helps to demonstrate the effectiveness of LongVU, especially on smaller-sized LLMs.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "reason": "Llama 2 serves as a foundation for LongVU, hence its importance. It is a critical component of the current work and this paper provides context and details about this large language model.", "section_number": 4}]}