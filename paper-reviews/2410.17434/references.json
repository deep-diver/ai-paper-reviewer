{"references": [{" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is foundational in establishing the capabilities of large language models (LLMs) and sets the stage for their application in multimodal tasks such as video understanding.  Its impact on the field of natural language processing and its subsequent influence on the development of multimodal LLMs makes it a critical foundational paper for the current work.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is highly influential in improving the alignment and capabilities of LLMs by introducing the concept of training models to follow instructions more effectively using human feedback. This technique is crucial for developing multimodal LLMs that can accurately interpret instructions and respond to complex video understanding tasks.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "CLIP, introduced in this paper, is a seminal work in bridging the gap between vision and language models.  Its success in creating a shared representation space is foundational for many subsequent multimodal LLMs, including those used to process videos and its impact on video understanding is extensive.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Xiaohua Zhai", "paper_title": "Sigmoid loss for language image pre-training", "reason": "SigLIP, introduced in this paper, is a significant advancement in the field of vision-language pre-training.  The authors demonstrate superior scaling capabilities using the proposed sigmoid loss function, making it an important contribution in building large-scale vision-language models used in video understanding.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "Flamingo represents an important step forward in visual language models by demonstrating the potential of efficient few-shot learning in multimodal tasks.  Its architecture and approach are highly influential on subsequent work, especially in the design of multimodal LLMs for video understanding.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Junnan Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "reason": "BLIP-2 is a highly influential multimodal model demonstrating successful integration of large language models with vision models for image understanding. The architecture and methodology used have influenced the development of several subsequent video understanding models, thus highlighting its importance in the field.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Deyao Zhu", "paper_title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models", "reason": "MiniGPT-4 is notable for its innovative approach to integrating visual and language models.  Its efficient and effective architecture for multimodal task completion is a key influence for many subsequent works, including those focused on video understanding. Its ability to process instructions and understand video content makes it highly relevant.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Hao Liu", "paper_title": "LLaVA-next: Improved reasoning, ocr, and world knowledge", "reason": "LLaVA-Next is a highly influential model that has significantly advanced the capabilities of multimodal LLMs in tasks related to image and video understanding. This is an important reference because the paper pushes the boundaries of multimodal LLMs in several areas of research.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Shengbang Tong", "paper_title": "Cambrian-1: A fully open, vision-centric exploration of multimodal LLMs", "reason": "Cambrian, introduced in this paper, is a highly significant multimodal model, which demonstrates the strong potential of integrating multiple vision encoders with a sophisticated aggregation technique. The paper's key contribution lies in its demonstration of the superior performance using SigLIP and DINOv2.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Maxime Oquab", "paper_title": "Dinov2: Learning robust visual features without supervision", "reason": "DINOv2 is a significant advancement in self-supervised visual feature learning.  Its ability to learn robust and effective visual representations without reliance on labeled data is crucial for many multimodal models applied to video understanding, where labeled video data can be scarce. Its use of self-supervision is vital.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "KunChang Li", "paper_title": "Videochat: Chat-centric video understanding", "reason": "VideoChat is highly relevant due to its focus on video understanding within the context of chat-centric interactions. This work directly addresses the core challenge of processing long videos within the limited context length of LLMs, making it a significant contribution and closely related to the current work.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Bo Li", "paper_title": "LLaVA-OneVision: Easy visual task transfer", "reason": "LLaVA-OneVision is a highly relevant model for this paper due to its focus on video understanding.  It serves as a strong baseline for comparison and its design and performance characteristics are critically analyzed in the evaluation section, providing strong context.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Chaoyou Fu", "paper_title": "Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal LLMs in video analysis", "reason": "VideoMME is a crucial benchmark for evaluating the performance of multimodal LLMs in video analysis, and particularly important for long-video understanding. The authors of this paper use this benchmark for a comprehensive comparison with existing state-of-the-art methods.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Junjie Zhou", "paper_title": "Mlvu: A comprehensive benchmark for multi-task long video understanding", "reason": "MLVU is a critical benchmark dataset used in evaluating the capabilities of video understanding models.  Its focus on multi-task performance in long-video scenarios makes it essential for benchmarking LongVU\u2019s effectiveness and scalability.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zesen Cheng", "paper_title": "Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms", "reason": "VideoLLaMA 2 is an important contribution to the field of video large language models, directly relevant due to its efforts in addressing the challenges associated with spatiotemporal modelling in long-form videos.  The comparison with this model highlights LongVU's performance.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Peiyuan Zhang", "paper_title": "Long context transfer from language to vision", "reason": "This paper's focus on transferring long-context information from language to vision is highly relevant, as it tackles a key challenge in video understanding addressed by the authors' work. It provides valuable insight into related techniques and challenges.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Junnan Li", "paper_title": "Videochat2: Chat-centric video understanding", "reason": "VideoChat2 is highly relevant because of its focus on chat-centric video understanding and its use of a large-scale video-text dataset.  The work serves as a critical comparison point for the proposed LongVU model.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Karttikeya Mangalam", "paper_title": "Egoschema: A diagnostic benchmark for very long-form video language understanding", "reason": "EgoSchema is a critical benchmark used to evaluate long-form video understanding capabilities.  Its diagnostic nature and focus on challenging scenarios are highly relevant for evaluating LongVU's performance on difficult video understanding tasks.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Peng Jin", "paper_title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding", "reason": "Chat-UniVi is highly relevant given its focus on unifying image and video understanding within a large language model framework.  It directly addresses a central challenge of this paper, highlighting the model's capabilities and limitations.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "reason": "Phi-3.5 is a significant and highly relevant model to compare against in this research because it is used as a lightweight baseline in order to demonstrate the scalability and strong performance capabilities of the proposed LongVU architecture, especially when using a lighter weight LLM like Llama3.2-3B.", "section_number": 4}]}