{"references": [{" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper introduces CLIP, a foundational vision-language model that has significantly influenced the field.  CLIP's contrastive learning approach and its ability to bridge the gap between image and text representation are fundamental to many subsequent vision-language models, including those focusing on video understanding.  Its impact is evident in the development and evolution of  MLLMs, which LongVU builds upon.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Xiaohua Zhai", "paper_title": "Sigmoid loss for language image pre-training", "reason": "This paper introduces SigLIP, a significant advancement in vision-language model training.  By employing a sigmoid loss instead of the typical contrastive loss, SigLIP enables training at larger batch sizes and achieves superior performance.  This improvement in training efficiency and performance is directly relevant to LongVU's approach to video understanding, as scaling to long videos requires efficient training strategies.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "Flamingo is a highly influential visual language model that demonstrates strong few-shot learning capabilities.  Its use of cross-attention mechanisms and its ability to effectively integrate visual and linguistic information are critical to the design of more advanced multimodal models.  LongVU utilizes similar cross-modal interaction principles, albeit with a focus on handling long video sequences efficiently.", "section_number": 2}, {" publication_date": "2023a", "fullname_first_author": "Junnan Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "reason": "BLIP-2 represents a key advancement in multimodal learning, integrating vision and language effectively.  Its modular design and the use of cross-attention mechanisms are influential in the design of many subsequent models.  LongVU's architecture borrows some key ideas from BLIP-2, such as using cross-modal queries and adapting them to handle temporal information and limited context sizes.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Kirolos Ataallah", "paper_title": "Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens", "reason": "This paper is directly relevant to LongVU as it focuses on video understanding using multimodal LLMs and directly addresses the challenges of processing longer videos, offering a related approach that is relevant to the LongVU methodology. It's important to review and contextualize this work to understand the current state of the art and identify distinct contributions.", "section_number": 2}, {" publication_date": "2024b", "fullname_first_author": "Kunchang Li", "paper_title": "Mvbench: A comprehensive multi-modal video understanding benchmark", "reason": "This paper introduces MVBench, a comprehensive benchmark dataset used for evaluating video understanding models. The fact that LongVU is evaluated on this benchmark makes it a key reference for understanding the context and the scope of the comparisons and the quality of the results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Chaoyou Fu", "paper_title": "Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis", "reason": "VideoMME is one of the main benchmarks used to evaluate the LongVU model. It is specifically designed for assessing long-video understanding capabilities which is LongVU's focus.  Understanding the benchmark's design and scope is essential to interpreting LongVU's performance.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Junjie Zhou", "paper_title": "Mlvu: A comprehensive benchmark for multi-task long video understanding", "reason": "MLVU is another key benchmark used in the evaluation of LongVU.  Its focus on long videos and its multi-task nature make it a particularly relevant reference.   The design of MLVU and the type of tasks included are key aspects of understanding LongVU's performance and capabilities.", "section_number": 4}, {" publication_date": "2024b", "fullname_first_author": "Bo Li", "paper_title": "Llava-onevision: Easy visual task transfer", "reason": "LLaVA-OneVision is a significant model in the field of vision-language understanding, serving as a strong baseline against which LongVU's performance is compared. It is directly relevant to the evaluation context as LongVU frequently compares results to LLaVA-OneVision.", "section_number": 4}, {" publication_date": "2024b", "fullname_first_author": "Junnan Li", "paper_title": "Videochat: Chat-centric video understanding", "reason": "VideoChat is an important reference because it is used as a source of training data in the LongVU experiments. The design, scale, and characteristics of this dataset directly impact the results reported for LongVU, making it a central reference for understanding this work.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "reason": "Phi-3 is a significant model serving as a baseline for comparison when evaluating the smaller LongVU model. The comparison allows us to measure the effectiveness of LongVU even when using a smaller language model, directly highlighting LongVU's ability to scale efficiently.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is foundational in the field of large language models, establishing the capabilities of LLMs for few-shot learning.  This foundational understanding is essential for the context of LongVU, which extends these capabilities to multimodal video understanding.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is highly relevant because it details training methods for aligning large language models to instructions and human feedback. These techniques are crucial for the training process of Multimodal Large Language Models (MLLMs), which are the basis of LongVU.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "This report on GPT-4, while not directly a vision-language model, is crucial for providing context about advancements in large language models (LLMs).  The understanding of these advancements in LLMs is fundamental to understanding the capabilities of MLLMs, which underpin LongVU.", "section_number": 1}, {" publication_date": "2024a", "fullname_first_author": "Hao Liu", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "reason": "This paper introduces Llama 2, a foundational large language model used in the LongVU experiments.  Understanding the architecture and capabilities of Llama 2 is essential for interpreting the results obtained using a smaller version of the model.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Maxime Oquab", "paper_title": "Dinov2: Learning robust visual features without supervision", "reason": "DINOv2 is a crucial component of the LongVU method, playing a central role in its temporal reduction strategy.  The capabilities and features of DINOv2 directly influence LongVU's performance, making it a critical reference.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "reason": "Qwen-VL is a large vision-language model. Its performance is compared against the LongVU model to demonstrate the capabilities of LongVU in comparison with other state-of-the-art models.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Karttikeya Mangalam", "paper_title": "Egoschema: A diagnostic benchmark for very long-form video language understanding", "reason": "EgoSchema is a benchmark dataset used to evaluate LongVU. Its focus on evaluating various aspects of video understanding makes it a valuable reference for contextualizing and interpreting LongVU's results. Understanding the benchmark's design, tasks and metrics is crucial for evaluating the reported performance.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Kirolos Ataallah", "paper_title": "Goldfish: Vision-language understanding of arbitrarily long videos", "reason": "Goldfish is closely related to LongVU, addressing the same problem of processing long videos. The comparison of the approaches and results provides valuable insights and helps identify the unique contributions of LongVU.", "section_number": 2}]}