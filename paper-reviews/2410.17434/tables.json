[{"figure_path": "2410.17434/tables/table_6_0.html", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "Table 1 presents the quantitative results of LongVU and other video understanding models on various benchmarks, including EgoSchema, MVBench, VideoMME, and MLVU, comparing their performance across different video lengths and durations.", "section": "4.2 Benchmarks and metrics"}, {"figure_path": "2410.17434/tables/table_7_0.html", "caption": "Table 2 Results of small-size video language models across video understanding benchmarks.", "description": "Table 2 presents the results of small-size video language models on various video understanding benchmarks, comparing their performance on EgoSchema, MVBench, VideoMME (Overall and Long subsets), and MLVU.", "section": "4.4 Video Understanding"}, {"figure_path": "2410.17434/tables/table_9_0.html", "caption": "Table 3 Ablation studies of number of tokens per frame, different context lengths, and our spatiotemporal compression components.", "description": "Table 3 shows the ablation study of the number of tokens per frame, different context lengths, and the spatiotemporal compression components used in the LongVU model, demonstrating the impact of each component on the performance across different video understanding benchmarks.", "section": "4.5 Ablation Studies"}, {"figure_path": "2410.17434/tables/table_9_1.html", "caption": "Table 4 Ablation study on each subtask in MLVU (Zhou et al., 2024).", "description": "The table presents ablation study results on each subtask in MLVU, showing the impact of different components of the proposed spatiotemporal compression mechanism on the performance of each subtask.", "section": "4.5 Ablation Studies"}, {"figure_path": "2410.17434/tables/table_9_2.html", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "Table 1 presents a quantitative comparison of LongVU's performance against several state-of-the-art video understanding models across various benchmarks, including EgoSchema, MVBench, VideoMME, and MLVU, evaluating metrics like accuracy and overall performance.", "section": "4.2 Benchmarks and metrics"}, {"figure_path": "2410.17434/tables/table_15_0.html", "caption": "Table 6 Training data statistics.", "description": "Table 6 presents the training data statistics, including the modality, task, number of samples, and datasets used for image-text and video-text training.", "section": "4 Experiments"}, {"figure_path": "2410.17434/tables/table_15_1.html", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "Table 1 presents a quantitative comparison of LongVU against various state-of-the-art video understanding models across multiple benchmarks, showcasing its performance in terms of accuracy and efficiency.", "section": "4.2 Benchmarks and metrics"}, {"figure_path": "2410.17434/tables/table_16_0.html", "caption": "Table 8 Ablation study on with or without FPE.", "description": "The table shows the ablation study of adding frame positional encoding (FPE) to the model on EgoSchema, VideoMME and MLVU benchmarks.", "section": "4.5 Ablation Studies"}, {"figure_path": "2410.17434/tables/table_16_1.html", "caption": "Table 9 Strategy ablations on each subtask in MLVU (Zhou et al., 2024).", "description": "Table 9 shows the ablation study on each subtask in MLVU dataset, comparing different strategies of spatial token compression, including DINO, DINO+Query, DINO+Query+STC and DINO+Query+STC+FPE.", "section": "4.5 Ablation Studies"}, {"figure_path": "2410.17434/tables/table_17_0.html", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "Table 1 presents a quantitative comparison of LongVU's performance against various state-of-the-art video understanding models across multiple benchmarks, including metrics for video length, context length, and number of frames.", "section": "4 Experiments"}]