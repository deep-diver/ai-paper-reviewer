[{"figure_path": "2410.17434/tables/table_6_0.html", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "Table 1 presents a quantitative comparison of LongVU against various state-of-the-art video understanding models across multiple benchmarks, showcasing its superior performance.", "section": "4.2 Benchmarks and metrics"}, {"figure_path": "2410.17434/tables/table_7_0.html", "caption": "Table 2 Results of small-size video language models across video understanding benchmarks.", "description": "Table 2 presents the performance comparison of different small-size video language models on various video understanding benchmark datasets, including EgoSchema, MVBench, VideoMME (Overall and Long subsets), and MLVU.", "section": "4.4 Video Understanding"}, {"figure_path": "2410.17434/tables/table_9_0.html", "caption": "Table 3 Ablation studies of number of tokens per frame, different context lengths, and our spatiotemporal compression components.", "description": "Table 3 shows the ablation study results of the number of tokens per frame, different context lengths, and the spatiotemporal compression components on EgoSchema, VideoMME, and MLVU.", "section": "4.5 Ablation Studies"}, {"figure_path": "2410.17434/tables/table_9_1.html", "caption": "Table 4 Ablation study on each subtask in MLVU (Zhou et al., 2024).", "description": "The table shows the ablation study results on each subtask of the MLVU benchmark, comparing different compression strategies.", "section": "4.5 Ablation Studies"}, {"figure_path": "2410.17434/tables/table_9_2.html", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "Table 1 presents a quantitative comparison of LongVU's performance against various state-of-the-art video understanding models across multiple benchmarks, evaluating metrics such as accuracy and showing the effect of video length.", "section": "4 Experiments"}, {"figure_path": "2410.17434/tables/table_15_0.html", "caption": "Table 6 Training data statistics.", "description": "Table 6 presents the training data statistics, including modality, task, number of samples and datasets used for training the LongVU model.", "section": "4 Experiments"}, {"figure_path": "2410.17434/tables/table_15_1.html", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "Table 1 presents a quantitative comparison of various video language models' performance across four video understanding benchmarks, showcasing LongVU's superior performance.", "section": "4.2 Benchmarks and metrics"}, {"figure_path": "2410.17434/tables/table_16_0.html", "caption": "Table 8 Ablation study on with or without FPE.", "description": "Table 8 shows the ablation study of the effect of adding frame position encoding (FPE) on the model's performance across three video understanding benchmarks.", "section": "4.5 Ablation Studies"}, {"figure_path": "2410.17434/tables/table_16_1.html", "caption": "Table 9 Strategy ablations on each subtask in MLVU (Zhou et al., 2024).", "description": "The table presents ablation study results on each subtask in MLVU, comparing different compression strategies (DINO, DINO+Query, DINO+Query+STC, DINO+Query+STC+FPE) in terms of their performance on various subtasks (count, ego, needle, order, plotQA, anomaly, reasoning).", "section": "4.5 Ablation Studies"}, {"figure_path": "2410.17434/tables/table_17_0.html", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "Table 1 presents the performance comparison of LongVU against other video understanding models across four benchmarks (EgoSchema, MVBench, VideoMME, and MLVU), showing LongVU's superior performance, especially in long video understanding tasks.", "section": "4 Experiments"}]