[{"figure_path": "2410.17434/tables/table_6_0.html", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "Table 1 presents a quantitative comparison of LongVU against various state-of-the-art video understanding models across four benchmarks, showcasing LongVU's superior performance.", "section": "4.2 Benchmarks and metrics"}, {"figure_path": "2410.17434/tables/table_7_0.html", "caption": "Table 2 Results of small-size video language models across video understanding benchmarks.", "description": "Table 2 presents the performance comparison of several small-size video language models on various video understanding benchmarks, including EgoSchema, MVBench, VideoMME (Overall and Long subsets), and MLVU.", "section": "4.4 Video Understanding"}, {"figure_path": "2410.17434/tables/table_9_0.html", "caption": "Table 3 Ablation studies of number of tokens per frame, different context lengths, and our spatiotemporal compression components.", "description": "Table 3 shows the ablation study of the number of tokens per frame, different context lengths, and the spatiotemporal compression components of the proposed model LongVU, comparing their performance on EgoSchema, VideoMME, and MLVU benchmarks.", "section": "4.5 Ablation Studies"}, {"figure_path": "2410.17434/tables/table_9_1.html", "caption": "Table 4 Ablation study on each subtask in MLVU (Zhou et al., 2024).", "description": "The table presents ablation study results on each subtask of the MLVU benchmark, comparing different strategies for spatiotemporal compression.", "section": "4.5 Ablation Studies"}, {"figure_path": "2410.17434/tables/table_9_2.html", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "Table 1 presents the quantitative results of LongVU and other video understanding models across various benchmarks, including EgoSchema, MVBench, VideoMME, and MLVU, showing LongVU's superior performance.", "section": "4 Experiments"}, {"figure_path": "2410.17434/tables/table_15_0.html", "caption": "Table 6 Training data statistics.", "description": "Table 6 presents the training data statistics, including the modality, task, number of samples, and datasets used for training the LongVU model.", "section": "4 Experiments"}, {"figure_path": "2410.17434/tables/table_15_1.html", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "Table 1 presents the quantitative results of various video understanding models on four benchmarks, including proprietary and open-source models,  comparing their performance across different video lengths.", "section": "4.2 Benchmarks and metrics"}, {"figure_path": "2410.17434/tables/table_16_0.html", "caption": "Table 8 Ablation study on with or without FPE.", "description": "The table shows the ablation study of the model with or without Frame Positional Encoding (FPE) on EgoSchema, VideoMME, and MLVU datasets.", "section": "4.5 Ablation Studies"}, {"figure_path": "2410.17434/tables/table_16_1.html", "caption": "Table 9 Strategy ablations on each subtask in MLVU (Zhou et al., 2024).", "description": "Table 9 shows the ablation study of each subtask in MLVU (Zhou et al., 2024) using different strategies for spatial token compression.", "section": "4.5 Ablation Studies"}, {"figure_path": "2410.17434/tables/table_17_0.html", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "Table 1 presents the performance comparison of LongVU against various video understanding models across four benchmarks (EgoSchema, MVBench, VideoMME, and MLVU), showing its superior performance, especially in long-video tasks.", "section": "4.2 Benchmarks and metrics"}]