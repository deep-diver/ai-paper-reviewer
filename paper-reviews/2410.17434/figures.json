[{"figure_path": "2410.17434/figures/figures_4_0.png", "caption": "Figure 2. Architecture of LongVU. Given a densely sampled video frames, we first utilize DINOv2 (Oquab et al., 2023) prior to remove redundant frames, and fuse the remaining frame features from both SigLIP (Zhai et al., 2023) and DINOv2 (Oquab et al., 2023), described in Section 3.1. Then we selectively reduce visual tokens via cross-modal query, detailed in Section 3.2. Finally, as demonstrated in Section 3.3, we conduct spatial token compression based on temporal dependencies to further meet the context length of LLMs.", "description": "The figure illustrates the architecture of LongVU, showing its three-step spatiotemporal adaptive compression mechanism for processing long videos.", "section": "3 Method"}, {"figure_path": "2410.17434/figures/figures_8_0.png", "caption": "Figure 3 Examples for various video understanding capabilities of LongVU model. We showcase that our LongVU is able to complete different types of video understanding tasks.", "description": "Figure 3 presents four example video understanding tasks that demonstrate LongVU\u2019s capabilities in spatial-temporal orientation awareness, detailed video description, action counting, and hour-long video understanding.", "section": "4.4 Video Understanding"}, {"figure_path": "2410.17434/figures/figures_8_1.png", "caption": "Figure 3 Examples for various video understanding capabilities of LongVU model. We showcase that our LongVU is able to complete different types of video understanding tasks.", "description": "Figure 3 shows examples of LongVU's video understanding capabilities, demonstrating its ability to perform tasks such as spatial-temporal orientation awareness, detailed video description, action counting, and hour-long video understanding.", "section": "4.4 Video Understanding"}, {"figure_path": "2410.17434/figures/figures_8_2.png", "caption": "Figure 1. Effectiveness of our LongVU over commonly-used uniform sampling and dense sampling. Uniform sampling overlooks critical frames due to its sparse nature. Dense sampling may surpass the maximum context length, leading to truncation of tokens from targeted frames. In contrast, our method can adaptively conduct spatiotemporal compression, accommodating long video sequences while preserving more visual details.", "description": "LongVU adaptively compresses long videos by removing redundant frames using DINOv2 features and cross-modal queries, while preserving visual details within the context length of LLMs.", "section": "1 Introduction"}, {"figure_path": "2410.17434/figures/figures_16_0.png", "caption": "Figure 6. Similarity comparison between SigLIP (Zhai et al., 2023) and DINOv2 (Oquab et al., 2023) features. The similarity is calculated between the first frame and the remainings. DINO concentrating on vision centric task effectively capture subtle frame differences compared with SigLIP (Zhai et al., 2023) which is aligned on semantic space.", "description": "The figure shows the similarity comparison of features extracted from SigLIP and DINOv2, highlighting DINOv2's effectiveness in capturing subtle frame differences compared to SigLIP.", "section": "C DINOv2 v.s. SigLIP"}]