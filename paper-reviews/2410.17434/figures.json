[{"figure_path": "2410.17434/figures/figures_4_0.png", "caption": "Figure 2. Architecture of LongVU. Given a densely sampled video frames, we first utilize DINOv2 (Oquab et al., 2023) prior to remove redundant frames, and fuse the remaining frame features from both SigLIP (Zhai et al., 2023) and DINOv2 (Oquab et al., 2023), described in Section 3.1. Then we selectively reduce visual tokens via cross-modal query, detailed in Section 3.2. Finally, as demonstrated in Section 3.3, we conduct spatial token compression based on temporal dependencies to further meet the context length of LLMs.", "description": "The figure illustrates the LongVU architecture, showcasing its three-step spatiotemporal adaptive compression mechanism for processing long videos within the context length limitations of LLMs.", "section": "3 Method"}, {"figure_path": "2410.17434/figures/figures_8_1.png", "caption": "Figure 3 Examples for various video understanding capabilities of LongVU model. We showcase that our LongVU is able to complete different types of video understanding tasks.", "description": "Figure 3 shows four examples of LongVU's video understanding capabilities, demonstrating its ability to handle spatial-temporal orientation awareness, detailed description, action counting, and hour-long video understanding.", "section": "4.4 Video Understanding"}, {"figure_path": "2410.17434/figures/figures_8_2.png", "caption": "Figure 2. Architecture of LongVU. Given a densely sampled video frames, we first utilize DINOv2 (Oquab et al., 2023) prior to remove redundant frames, and fuse the remaining frame features from both SigLIP (Zhai et al., 2023) and DINOv2 (Oquab et al., 2023), described in Section 3.1. Then we selectively reduce visual tokens via cross-modal query, detailed in Section 3.2. Finally, as demonstrated in Section 3.3, we conduct spatial token compression based on temporal dependencies to further meet the context length of LLMs.", "description": "The figure illustrates the architecture of LongVU, detailing its spatiotemporal adaptive compression mechanism which uses DINOv2 and SigLIP for feature extraction, cross-modal query for selective feature reduction, and spatial token compression based on temporal dependencies to process long videos.", "section": "3 Method"}, {"figure_path": "2410.17434/figures/figures_16_0.png", "caption": "Figure 6. Similarity comparison between SigLIP (Zhai et al., 2023) and DINOv2 (Oquab et al., 2023) features. The similarity is calculated between the first frame and the remainings. DINO concentrating on vision centric task effectively capture subtle frame differences compared with SigLIP (Zhai et al., 2023) which is aligned on semantic space.", "description": "Figure 6 shows that DINOv2 is more effective than SigLIP at capturing subtle frame differences due to its focus on visual features.", "section": "C DINOv2 v.s. SigLIP"}]