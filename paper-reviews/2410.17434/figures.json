[{"figure_path": "2410.17434/figures/figures_4_0.png", "caption": "Figure 2. Architecture of LongVU. Given a densely sampled video frames, we first utilize DINOv2 (Oquab et al., 2023) prior to remove redundant frames, and fuse the remaining frame features from both SigLIP (Zhai et al., 2023) and DINOv2 (Oquab et al., 2023), described in Section 3.1. Then we selectively reduce visual tokens via cross-modal query, detailed in Section 3.2. Finally, as demonstrated in Section 3.3, we conduct spatial token compression based on temporal dependencies to further meet the context length of LLMs.", "description": "The figure illustrates the architecture of LongVU, detailing its spatiotemporal adaptive token compression mechanism which involves temporal reduction using DINOv2, selective feature reduction via cross-modal query, and spatial token compression based on temporal dependencies.", "section": "3 Method"}, {"figure_path": "2410.17434/figures/figures_8_1.png", "caption": "Figure 3 Examples for various video understanding capabilities of LongVU model. We showcase that our LongVU is able to complete different types of video understanding tasks.", "description": "Figure 3 shows examples of LongVU's capabilities in various video understanding tasks, such as spatial-temporal orientation awareness, detailed description, action counting, and hour-long video understanding.", "section": "4.4 Video Understanding"}, {"figure_path": "2410.17434/figures/figures_8_2.png", "caption": "Figure 2. Architecture of LongVU. Given a densely sampled video frames, we first utilize DINOv2 (Oquab et al., 2023) prior to remove redundant frames, and fuse the remaining frame features from both SigLIP (Zhai et al., 2023) and DINOv2 (Oquab et al., 2023), described in Section 3.1. Then we selectively reduce visual tokens via cross-modal query, detailed in Section 3.2. Finally, as demonstrated in Section 3.3, we conduct spatial token compression based on temporal dependencies to further meet the context length of LLMs.", "description": "The figure illustrates the architecture of LongVU, a spatiotemporal adaptive compression mechanism for processing long videos.", "section": "3 Method"}, {"figure_path": "2410.17434/figures/figures_16_0.png", "caption": "Figure 6. Similarity comparison between SigLIP (Zhai et al., 2023) and DINOv2 (Oquab et al., 2023) features. The similarity is calculated between the first frame and the remainings. DINO concentrating on vision centric task effectively capture subtle frame differences compared with SigLIP (Zhai et al., 2023) which is aligned on semantic space.", "description": "The figure shows a comparison of feature similarity between SigLIP and DINOv2, illustrating DINOv2's superior ability to capture subtle frame differences due to its focus on visual-centric tasks.", "section": "C DINOv2 v.s. SigLIP"}]