[{"figure_path": "https://arxiv.org/html/2501.09503/x1.png", "caption": "Figure 1: \nOverview of AnyStory framework.\nAnyStory follows the \u201cencode-then-route\u201d conditional generation paradigm. It first utilizes a simplified ReferenceNet combined with a CLIP vision encoder to encode the subject (Sec.\u00a03.2), and then employs a decoupled instance-aware subject router to guide the subject condition injection (Sec.\u00a03.3). The training process is divided into two stages: the subject encoder training stage and the router training stage (Sec.\u00a03.4). For brevity, we omit the text conditional branch here.", "description": "AnyStory, a unified framework for single and multi-subject personalization in text-to-image generation, is illustrated.  The framework uses a two-stage process: First, it encodes the subject using a simplified ReferenceNet and CLIP vision encoder. Second, it uses a decoupled instance-aware subject router to inject subject conditions into the image generation process, guiding the placement and characteristics of subjects within the image. The training involves separate training of the subject encoder and the router.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2501.09503/x2.png", "caption": "Figure 2: \nEffect of ReferenceNet encoding. The ReferenceNet encoder enhances the preservation of subject details.", "description": "This figure demonstrates the impact of using a ReferenceNet encoder in AnyStory.  The left image shows the output when only a CLIP vision encoder is used, showing loss of fine details. The right image shows how incorporating ReferenceNet into the subject representation enhances the preservation of subject details and fidelity in the generated image.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2501.09503/x3.png", "caption": "Figure 3: \nThe effectiveness of the router. The router restricts the influence areas of the subject conditions, thereby avoiding the blending of characteristics between multiple subjects and improving the quality of the generated images.", "description": "This figure demonstrates the AnyStory model's ability to avoid blending of subject characteristics when generating images with multiple subjects.  The router module, a key component of AnyStory, limits the influence of each subject's conditions to specific areas of the image. By confining these influences, the model produces cleaner, higher-quality images where subjects are distinct and well-defined, without unwanted blending or mixing of features.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2501.09503/x4.png", "caption": "Figure 4: \nVisualization of routing maps.\nWe visualize the routing maps within each cross-attention layer in the U-Net at different diffusion time steps. There are a total of 70 cross-attention layers in the SDXL U-Net, and we sequentially display them in each subfigure in a top-to-bottom and left-to-right order (yellow represents the effective region). We utilize T=25\ud835\udc4725T=25italic_T = 25 steps of EDM sampling. Each complete row corresponds to one entity. The background routing map has been ignored, which is the complement of the routing maps of all subjects. Best viewed in color and zoomed in.", "description": "This figure visualizes the attention maps generated by the model's routing module at various stages of the diffusion process.  Each row represents a different subject or entity in the image, showing how the model focuses on specific areas to condition generation. The maps illustrate the attention weights across the 70 cross-attention layers within the U-Net, revealing how the model progressively refines subject placement and boundaries during image generation. Yellow highlights indicate the areas where the model focuses most strongly. The background map is omitted for clarity; it's essentially the inverse of the combined subject maps.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2501.09503/x7.png", "caption": "Figure 5: Effectiveness of the proposed router structure. For the meaning of each illustration, please refer to Fig.\u00a04.", "description": "Figure 5 demonstrates the effectiveness of AnyStory's decoupled instance-aware subject router in controlling the influence area of subject conditions during image generation.  The figure shows coarse and refined routing maps at different diffusion time steps (t) for various subjects. Each row corresponds to a subject. The yellow regions in the maps highlight where the subject's features are injected into the image. The refined routing maps, generated by masked cross-attention, provide more precise localization of subject features compared to the coarse maps. This precise control prevents the blending of features from different subjects and leads to higher-quality image generation, especially in multi-subject scenarios.", "section": "3.3. Decoupled instance-aware subject routing"}, {"figure_path": "https://arxiv.org/html/2501.09503/x8.png", "caption": "Figure 6: \nExample generations II from AnyStory.", "description": "This figure showcases example image generation results from the AnyStory model.  It demonstrates the model's ability to generate diverse and high-quality images based on text prompts, showcasing a range of art styles, subjects, and compositions. The images highlight AnyStory's capacity for personalization, including the generation of single or multiple subjects with varying levels of detail and complexity.", "section": "4.4. Example generations"}]