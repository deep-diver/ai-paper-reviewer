[{"content": "| Method | Inference Time | KL<sub>*passt*</sub>\u2193 | FD<sub>*openl3*</sub>\u2193 | IB\u2191 | MP | EC | TC | RC |\n|---|---|---|---|---|---|---|---|---| \n| CMT [13] | ~3 min | 52.76 | 269.63 | 8.54 | 3.19 | 2.81 | 2.79 | 3.10 |\n| Video2music [26] | ~1 min | 103.56 | 533.46 | 5.26 | 3.05 | 2.58 | 2.64 | 2.67 |\n| VidMuse [51] | ~13 min | 56.48 | 187.13 | **22.09** | 3.01 | 2.91 | 3.05 | 3.02 |\n| M<sup>2</sup>UGen [34] | ~40 s | 60.41 | 180.72 | 15.58 | 2.84 | 2.32 | 2.37 | 2.71 |\n| VMB (ours) | ~20 s | **48.84** | **105.84** | 21.62 | **3.85** | **3.36** | **3.38** | **3.62 |", "caption": "Table 1: Video-to-music generation performance on SymMV dataset.\nUp/down arrows indicate the desired direction for improvement.", "description": "This table presents a comparison of various video-to-music generation models, evaluated on the SymMV dataset. The metrics used for evaluation include objective measures such as KLpasst, FDopenl3, and ImageBind score (IB), as well as subjective metrics like Musical Pleasantness (MP), Emotional Correspondence (EC), Thematic Correspondence (TC), and Rhythmic Correspondence (RC).  Lower scores for KLpasst and FDopenl3 are better, indicating higher quality and closer alignment to real music. Higher scores are generally preferred for the other metrics.", "section": "4.2. Video-to-music Generation"}, {"content": "| Method | KL<sub>*passt*</sub>\u2193 | FD<sub>*openl3*</sub>\u2193 | CLAPScore\u2191 | IB\u2191 | MP | TMA | \n|---|---|---|---|---|---|---| \n| Stable Audio Open [17] | 42.89 | 183.09 | **40.92** | 24.67 | 3.41 | **3.52** |\n| MusicGen [8] | 46.89 | 181.59 | 33.95 | 22.46 | 3.11 | 3.35 |\n| AudioLDM [33] | 99.85 | 293.86 | 17.61 | 20.01 | 2.34 | 2.71 |\n| M<sup>2</sup>UGen [34] | 49.03 | 188.84 | 28.76 | 16.70 | 3.19 | 3.27 |\n| VMB (ours) | **37.43** | **132.16** | 39.66 | **29.36** | **3.78** | 3.48 |", "caption": "Table 2: Text-to-music generation performance on SongDescriber dataset.", "description": "This table presents a comparison of different text-to-music generation models, including Stable Audio Open, MusicGen, AudioLDM, M2UGen, and the proposed VMB model. The evaluation is performed on the SongDescriber dataset, using both objective metrics (KL Divergence, Fr\u00e9chet distance, CLAPScore, and ImageBind score) and subjective metrics (Musical Pleasantness and Text-Music Alignment). The table shows that VMB outperforms other baseline models by achieving the best scores across multiple evaluation metrics.", "section": "4.3. Text-to-music Generation"}, {"content": "| Method | KL<sub>*passt*&darr; | FD<sub>*openl3*&darr; | IB&uarr; |\n|---|---|---|---| \n| CoDi [50] | 216.48 | 251.52 | 9.60 |\n| M<sup>2</sup>UGen [34] | 128.33 | 247.42 | 2.28 |\n| VMB (ours) | **105.60** | **119.76** | **11.88** |", "caption": "Table 3: Image-to-music generation performance on MUImage dataset.", "description": "This table presents the evaluation results for image-to-music generation using three different models: CoDi, M2UGen, and the proposed VMB model. The evaluation was conducted on the MUImage dataset, a collection of image-music pairs, and uses metrics such as KL divergence (KLpasst), Frechet Audio Distance (FDopenl3), and ImageBind score (IB).  Lower KLpasst and FDopenl3 indicate better music quality and alignment with real music, while a higher IB signifies stronger cross-modal alignment between image and generated music.", "section": "4.4. Image-to-music Generation"}, {"content": "| Model | CLAPScore |\n|---|---| \n| GPT-4V [41] | 44.41 |\n| InternVL [6] | 44.21 |\n| MMDM | 50.88 |", "caption": "Table 6: Ablation of model components on video-to-music generation with SymMV dataset. BR, TR represent broad retrieval and target retrieval respectively.", "description": "This table presents an ablation study conducted on the SymMV dataset, evaluating the impact of removing the Broad Retrieval (BR) and Targeted Retrieval (TR) components from the VMB model on video-to-music generation.  It reports objective metrics (KLpasst, FDopenl3, and ImageBind score) across four different ablation settings: using both BR and TR, using only BR, using only TR, and using neither.  Lower KLpasst and FDopenl3 scores indicate better music quality and alignment with real music distributions, while a higher ImageBind score represents better alignment between the generated music and the input video.", "section": "4.7. Ablation Studies"}, {"content": "| Attribute | Change ($\\Delta$) |\n|---|---| \n| Instrument | +11.46 |\n| Genre | +3.03 |\n| Mood | +4.14 |", "caption": "Table 7: Average BPM of music generated under varying tempo conditions.", "description": "This table presents the average Beats Per Minute (BPM) of music generated by the VMB model under different tempo conditions (\"Fast\", \"Medium\", and \"Slow\") in a controllable generation experiment.  This experiment evaluates the model's ability to adjust the tempo of generated music while maintaining overall coherence.", "section": "4. Experiments"}, {"content": "| BR | TR | KL<sub>*passt*</sub>\u2193 | FD<sub>*openl3*</sub>\u2193 | IB\u2191 |\n|---|---|---|---|---| \n| \u2713 | \u2713 | **75.29** | **177.27** | **24.70** |\n| \u2713 | \u00d7 | 91.89 | 199.74 | 20.73 |\n| \u00d7 | \u2713 | 91.07 | 387.14 | 20.51 |\n| \u00d7 | \u00d7 | 96.42 | 360.29 | 14.67 |", "caption": "Table 8: Samples of visual-to-description generation.", "description": "This table presents examples of how visual inputs, specifically images, are transformed into textual descriptions suitable for music generation. Each row features an image alongside its generated description and an evaluation by GPT-4, providing insights into the effectiveness of capturing the image's essence in the textual description. The evaluation includes a score (out of 5) and a rationale explaining the alignment between the image and the description.", "section": "C. Demos"}]