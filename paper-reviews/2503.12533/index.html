<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills &#183; HF Daily Paper Reviews by AI"><meta name=description content="Being-0: A humanoid robot agent achieves complex tasks by integrating a vision-language model with modular skills, enhancing efficiency and real-time performance."><meta name=keywords content="Multimodal Learning,Embodied AI,üè¢ Peking University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12533/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12533/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills"><meta property="og:description" content="Being-0: A humanoid robot agent achieves complex tasks by integrating a vision-language model with modular skills, enhancing efficiency and real-time performance."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-03-16T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-16T00:00:00+00:00"><meta property="article:tag" content="Multimodal Learning"><meta property="article:tag" content="Embodied AI"><meta property="article:tag" content="üè¢ Peking University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12533/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12533/cover.png"><meta name=twitter:title content="Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills"><meta name=twitter:description content="Being-0: A humanoid robot agent achieves complex tasks by integrating a vision-language model with modular skills, enhancing efficiency and real-time performance."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills","headline":"Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills","abstract":"Being-0: A humanoid robot agent achieves complex tasks by integrating a vision-language model with modular skills, enhancing efficiency and real-time performance.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2503.12533\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-03-16T00:00:00\u002b00:00","datePublished":"2025-03-16T00:00:00\u002b00:00","dateModified":"2025-03-16T00:00:00\u002b00:00","keywords":["Multimodal Learning","Embodied AI","üè¢ Peking University"],"mainEntityOfPage":"true","wordCount":"4598"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-19/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-19</p></a><a href=/ai-paper-reviewer/2025-03-20/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-20</p></a><a href=/ai-paper-reviewer/2025-03-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-21</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-19/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-19</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-20/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-20</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-21</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2503.12533/cover_hu13073376892283681957.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2503.12533/>Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-03-16T00:00:00+00:00>16 March 2025</time><span class="px-2 text-primary-500">&#183;</span><span>4598 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">22 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2503.12533/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2503.12533/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/embodied-ai/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Embodied AI
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-peking-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Peking University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#skill-stitching>Skill Stitching</a></li><li><a href=#vlm-connector>VLM Connector</a></li><li><a href=#active-vision>Active Vision</a></li><li><a href=#fm--skills>FM + Skills</a></li><li><a href=#limited-fm>Limited FM</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#skill-stitching>Skill Stitching</a></li><li><a href=#vlm-connector>VLM Connector</a></li><li><a href=#active-vision>Active Vision</a></li><li><a href=#fm--skills>FM + Skills</a></li><li><a href=#limited-fm>Limited FM</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2503.12533</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Haoqi Yuan et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-03-18</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2503.12533 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2503.12533 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2503.12533/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>The paper addresses challenges in building autonomous agents for humanoid robots, where directly combining Foundation Models (FMs) for high-level cognition with low-level skills often results in poor robustness and efficiency. This is due to compounding errors in long-horizon tasks and varied module latencies. To resolve this, the authors introduce Being-0, a hierarchical agent framework that integrates an FM with a modular skill library.</p><p>Being-0 utilizes a novel Connector module, powered by a lightweight vision-language model (VLM), to bridge the gap between the FM&rsquo;s language-based plans and the execution of low-level skills. <strong>This enhances embodied decision-making</strong> and effectively coordinates locomotion and manipulation. With most components deployable on low-cost onboard devices, Being-0 achieves efficient, real-time performance on a full-sized humanoid robot.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-e37d264579b2745139dcd1fdafdb73c4></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-e37d264579b2745139dcd1fdafdb73c4",{strings:[" A hierarchical agent framework for humanoid robots is introduced, optimizing cloud and onboard deployment. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-c89cea6fe2b1d60ec31910b272cbd7d7></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-c89cea6fe2b1d60ec31910b272cbd7d7",{strings:[" A vision-language model (VLM) based Connector enhances embodied decision-making and skill coordination. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-24a15cf0e306ac08c51128e9f392d9e3></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-24a15cf0e306ac08c51128e9f392d9e3",{strings:[" The agent achieves high success rates in complex, long-horizon tasks with dexterous manipulation and active vision. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>Being-0 offers a practical framework for humanoid robots in real-world tasks, balancing cloud-based planning with on-robot skill execution. It is relevant for researchers by <strong>demonstrating efficient, real-time performance</strong>, and opens up avenues for advancing humanoid dexterity, navigation, and human-robot collaboration.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.12533/x1.png alt></figure></p><blockquote><p>üîº Being-0 is a hierarchical framework for humanoid robots. It consists of three main components: a Foundation Model (FM) for high-level planning and reasoning; a Connector, a vision-language model (VLM), which translates high-level plans into low-level actions; and a Modular Skill Library for locomotion and manipulation. These components work together to enable the robot to perform complex, long-horizon tasks in real-world environments.</p><details><summary>read the caption</summary>Figure 1: Overview of the Being-0 framework. The humanoid agent framework, Being-0, comprises three key components: (1) the Foundation Model (FM) for high-level task planning and reasoning, (2) the Connector, a vision-language model (VLM) that bridges the FM and low-level skills, and (3) the Modular Skill Library for robust locomotion and dexterous manipulation. Together, these components enable Being-0 to effectively control a full-sized humanoid robot equipped with multi-fingered hands and active vision, solving complex, long-horizon embodied tasks in real-world environments.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T1.1><tr class=ltx_tr id=S5.T1.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id=S5.T1.1.1.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.1.1.1><span class=ltx_p id=S5.T1.1.1.1.1.1 style=width:62.6pt><span class="ltx_text ltx_font_bold" id=S5.T1.1.1.1.1.1.1>Task</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id=S5.T1.1.1.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.1.2.1><span class=ltx_p id=S5.T1.1.1.2.1.1 style=width:37pt><span class="ltx_tabular ltx_align_middle" id=S5.T1.1.1.2.1.1.1><span class=ltx_tr id=S5.T1.1.1.2.1.1.1.1><span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T1.1.1.2.1.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T1.1.1.2.1.1.1.1.1.1>w/o Connector</span></span></span></span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id=S5.T1.1.1.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.1.3.1><span class=ltx_p id=S5.T1.1.1.3.1.1 style=width:37pt><span class="ltx_tabular ltx_align_middle" id=S5.T1.1.1.3.1.1.1><span class=ltx_tr id=S5.T1.1.1.3.1.1.1.1><span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T1.1.1.3.1.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T1.1.1.3.1.1.1.1.1.1>Being-0</span></span></span></span></span></span></td></tr><tr class=ltx_tr id=S5.T1.1.2><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id=S5.T1.1.2.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.2.1.1><span class=ltx_p id=S5.T1.1.2.1.1.1 style=width:62.6pt><span class="ltx_text ltx_font_bold" id=S5.T1.1.2.1.1.1.1>Fetch-bottle</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id=S5.T1.1.2.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.2.2.1><span class=ltx_p id=S5.T1.1.2.2.1.1 style=width:37pt>0.00</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id=S5.T1.1.2.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.2.3.1><span class=ltx_p id=S5.T1.1.2.3.1.1 style=width:37pt><span class="ltx_text ltx_font_bold" id=S5.T1.1.2.3.1.1.1>0.90</span></span></span></td></tr><tr class=ltx_tr id=S5.T1.1.3><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T1.1.3.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.3.1.1><span class=ltx_p id=S5.T1.1.3.1.1.1 style=width:62.6pt><span class="ltx_text ltx_font_bold" id=S5.T1.1.3.1.1.1.1>Deliver-basket</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T1.1.3.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.3.2.1><span class=ltx_p id=S5.T1.1.3.2.1.1 style=width:37pt>0.00</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T1.1.3.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.3.3.1><span class=ltx_p id=S5.T1.1.3.3.1.1 style=width:37pt><span class="ltx_text ltx_font_bold" id=S5.T1.1.3.3.1.1.1>0.80</span></span></span></td></tr><tr class=ltx_tr id=S5.T1.1.4><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T1.1.4.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.4.1.1><span class=ltx_p id=S5.T1.1.4.1.1.1 style=width:62.6pt><span class="ltx_text ltx_font_bold" id=S5.T1.1.4.1.1.1.1>Prepare-coffee</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T1.1.4.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.4.2.1><span class=ltx_p id=S5.T1.1.4.2.1.1 style=width:37pt>0.00</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T1.1.4.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.4.3.1><span class=ltx_p id=S5.T1.1.4.3.1.1 style=width:37pt><span class="ltx_text ltx_font_bold" id=S5.T1.1.4.3.1.1.1>0.75</span></span></span></td></tr><tr class=ltx_tr id=S5.T1.1.5><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T1.1.5.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.5.1.1><span class=ltx_p id=S5.T1.1.5.1.1.1 style=width:62.6pt><span class="ltx_text ltx_font_bold" id=S5.T1.1.5.1.1.1.1>Make-coffee</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T1.1.5.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.5.2.1><span class=ltx_p id=S5.T1.1.5.2.1.1 style=width:37pt><span class="ltx_text ltx_font_bold" id=S5.T1.1.5.2.1.1.1>0.90</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T1.1.5.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.5.3.1><span class=ltx_p id=S5.T1.1.5.3.1.1 style=width:37pt><span class="ltx_text ltx_font_bold" id=S5.T1.1.5.3.1.1.1>0.90</span></span></span></td></tr><tr class=ltx_tr id=S5.T1.1.6><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id=S5.T1.1.6.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.6.1.1><span class=ltx_p id=S5.T1.1.6.1.1.1 style=width:62.6pt><span class="ltx_text ltx_font_bold" id=S5.T1.1.6.1.1.1.1>Deliver-coffee</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id=S5.T1.1.6.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.6.2.1><span class=ltx_p id=S5.T1.1.6.2.1.1 style=width:37pt>0.33</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id=S5.T1.1.6.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.6.3.1><span class=ltx_p id=S5.T1.1.6.3.1.1 style=width:37pt><span class="ltx_text ltx_font_bold" id=S5.T1.1.6.3.1.1.1>0.87</span></span></span></td></tr></table></table></figure><blockquote><p>üîº This table presents a comparison of the task completion success rates for the Being-0 robotic agent, both with and without the Connector module, across a range of complex, long-horizon tasks. The results clearly show a substantial increase in successful task completion when the Connector module is utilized, highlighting its significant contribution to the overall performance of the agent.</p><details><summary>read the caption</summary>Table 1: Task completion rates for Being-0 with and without the Connector across various long-horizon tasks. The results demonstrate significant performance improvements when the Connector is used.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Skill Stitching<div id=skill-stitching class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#skill-stitching aria-label=Anchor>#</a></span></h4><p><strong>Skill stitching</strong> is crucial for humanoid robots to perform complex tasks. The paper highlights the challenges in smoothly transitioning between skills like navigation and manipulation, <strong>requiring precise pose adjustments</strong> to avoid failures. A vision-language model, or a similar connector, is proposed to bridge the gap between high-level plans and low-level skill execution. The connector enhances decision-making and coordination to improve task success and efficiency. This integration facilitates <strong>real-time adaptation</strong>, better task sequencing, and improved robustness in dynamic environments. Skill coordination leads to successful completion of complicated tasks in real-world scenarios.</p><h4 class="relative group">VLM Connector<div id=vlm-connector class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#vlm-connector aria-label=Anchor>#</a></span></h4><p>The VLM Connector serves as a crucial bridge, <strong>translating FM plans into actionable skills</strong>. By grounding abstract language plans in real-time visual input, it enhances the robot&rsquo;s adaptability and robustness. <strong>Key functions include grounding FM plans, closed-loop navigation</strong>, and improving transitions between navigation and manipulation. It leverages visual understanding and object detection to inform skill selection. The VLM&rsquo;s rapid inference capabilities and pose adjustment improve the initialization states for subsequent manipulations, <strong>reducing compounding errors</strong> and enhancing overall task success with greater navigation efficiency. This approach allows the robot to perceive surroundings, adapt to unexpected obstacles, and coordinate locomotion and manipulation to tackle complex, long-horizon tasks in dynamic environments.</p><h4 class="relative group">Active Vision<div id=active-vision class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#active-vision aria-label=Anchor>#</a></span></h4><p>The research paper highlights the significant role of <strong>active vision</strong> in enhancing a humanoid robot&rsquo;s capabilities. Traditional fixed camera setups impose limitations on both navigation and manipulation tasks. By contrast, <strong>Being-0, equips the humanoid with an actuated neck and binocular RGB cameras</strong>, enabling it to dynamically adapt its field of view. This dynamic adjustment is crucial for tasks requiring both broad environmental awareness during navigation and precise object localization for manipulation. The system leverages VLM to estimate the relative position and trigger locomotion skills, allowing robot can search the target in 3D space. The <strong>integration of an active camera</strong> consistently achieves high success rates, showcasing superior performance in tasks where visual adaptability is essential. This approach, therefore, significantly contributes to the robot&rsquo;s dexterity and robustness in complex, real-world environments.</p><h4 class="relative group">FM + Skills<div id=fm--skills class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#fm--skills aria-label=Anchor>#</a></span></h4><p>Combining Foundation Models (<strong>FMs</strong>) with robotic skills holds immense potential but faces challenges. <strong>FMs</strong> excel at high-level planning but can lack real-time responsiveness needed for complex robotic tasks. The efficiency and precision of <strong>FMs</strong> directly control robots are often compromised. Directly connecting FMs with skills often leads to issues, particularly for humanoid robots due to their unstable locomotion. Humanoid robots&rsquo; unpredictable movements necessitate continuous adjustments, exceeding the processing capabilities of existing <strong>FMs</strong> and compounding errors. The inherent instability of humanoid robots requires <strong>FM</strong> to continuously react and adjust, adding an additional layer of complexity.</p><h4 class="relative group">Limited FM<div id=limited-fm class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#limited-fm aria-label=Anchor>#</a></span></h4><p><strong>Limited FMs in Robotics</strong>: The paper highlights challenges when directly using large, general-purpose Foundation Models (FMs) like GPT-40 for humanoid robots. <strong>Instability and poor 3D understanding</strong> lead to inefficiency. High inference times make real-time adaptation difficult. This motivates exploring smaller, specialized models, like the VLM-based Connector, which can perform grounded planning. It also calls for finding a lightweight design in model architecture to optimize resource allocation.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.12533/x2.png alt></figure></p><blockquote><p>üîº Figure 2 illustrates the step-by-step workflow of the Being-0 system in accomplishing the task of &lsquo;making a cup of coffee&rsquo;. The process is divided into two rows of images, progressing from left to right. Yellow borders highlight the decision points where the Foundation Model (FM) plans the next step, presented in yellow dialog boxes. The Connector module&rsquo;s decisions are shown in green boxes, while the blue boxes represent the low-level skills from the modular skill library that are executed. This visualization helps to understand the hierarchical interaction between the high-level planning (FM), mid-level coordination (Connector), and low-level skill execution.</p><details><summary>read the caption</summary>Figure 2: Workflow of Being-0 for the task ‚Äúmake a cup of coffee‚Äù. The figure illustrates the step-by-step execution of the task, with images arranged in two rows. The execution order proceeds left to right in the first row, then continues left to right in the second row. Images with yellow borders indicate decision-making points for the Foundation Model (FM). The yellow dialog boxes display the FM‚Äôs plans, the green boxes show decisions made by the Connector, and the blue boxes represent the skills called from the modular skill library.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.12533/x3.png alt></figure></p><blockquote><p>üîº This figure compares the performance of two versions of the Being-0 robotic agent on the &lsquo;Prepare-coffee&rsquo; task. The top row shows the agent without the Connector module, highlighting frequent queries to the Foundation Model (FM) due to poor embodied scene understanding, ultimately failing to complete the task. The bottom row demonstrates the improved performance of the agent with the Connector module, which successfully bridges the high-level planning of the FM and low-level robot control, resulting in task completion with far fewer queries to the FM.</p><details><summary>read the caption</summary>Figure 3: A comparison of Being-0 w/o Connector and Being-0 in the long-horizon task ‚ÄúPrepare-coffee.‚Äù The first row shows recordings of Being-0 without the Connector, while the second row shows recordings of Being-0 with the Connector. Being-0 w/o Connector frequently queries the FM, which often fails to provide correct plans due to its limited embodied scene understanding. In contrast, Being-0 with the Connector completes the task, requiring only a few queries to the FM.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.12533/x4.png alt></figure></p><blockquote><p>üîº This figure presents an ablation study comparing the performance of Being-0 with different camera configurations on navigation and manipulation tasks. The left three images in each row show the navigation phase, where the robot moves toward a target object (coffee machine). The right three images display the subsequent manipulation phase (grasping coffee). The rows represent different setups: fixed cameras with varying pitch angles (0.3, 0.6, and 0.9 radians) and Being-0 with its active camera. The results clearly show that only Being-0, using its active camera, successfully completes both tasks reliably. Fixed camera setups, regardless of angle, fail to achieve consistent success in either navigation or manipulation.</p><details><summary>read the caption</summary>Figure 4: Recordings from the ablation study on the active camera. Each row represents a different camera configuration, with the left three images depicting the navigation task and the right three images depicting the manipulation task. Only Being-0 with an active camera achieves robust performance in both navigation and manipulation.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.12533/x5.png alt></figure></p><blockquote><p>üîº This figure compares the performance of the Being-0 agent with and without the proposed adjustment method for navigation. Two-stage tasks were used, each consisting of navigation followed by manipulation. The left three images in each row show the results of Being-0 without the adjustment, demonstrating that improper navigation poses often led to failed manipulation attempts. The right three images show Being-0 with the adjustment, highlighting its success in producing appropriate poses for successful manipulation.</p><details><summary>read the caption</summary>Figure 5: A comparison of Being-0 with and without the adjustment method in two-stage tasks involving navigation and manipulation. Each row corresponds to a specific task, with the left three images showing results for Being-0 w/o Adjustment and the right three images showing results for Being-0. Without adjustment, the agent may terminate navigation in improper poses, leading to failed manipulations.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.12533/x6.png alt></figure></p><blockquote><p>üîº This figure presents a series of first-person viewpoints demonstrating the humanoid robot&rsquo;s learned manipulation skills. Each row showcases a different skill, with images progressing from left to right to illustrate the sequential steps involved in successfully completing each task. The skills shown highlight the robot&rsquo;s dexterity and precision in manipulating various objects.</p><details><summary>read the caption</summary>Figure 6: First-person view recordings of the learned manipulation skills. Each row corresponds to a specific skill, with images from left to right depicting the progression of the manipulation process.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.12533/x7.png alt></figure></p><blockquote><p>üîº This figure shows the detailed reasoning process of the Foundation Model (FM) in Being-0 while executing the &lsquo;Prepare-coffee&rsquo; task. It breaks down the task into subtasks and illustrates how the FM uses image observation and reasoning to decide on the next action. The process is depicted in three stages: 1. Information gathering, including image description, target identification, and self-reflection. 2. Task Inference, involving subtask identification and reasoning. 3. Action planning, outlining the next action to be taken and the reasoning behind it. Each step demonstrates how the FM reasons about the task, identifies the next step, and plans accordingly, showcasing its hierarchical decision-making process in tackling complex tasks.</p><details><summary>read the caption</summary>Figure 7: Planning traces of the Foundation Model in Being-0 for the task ‚ÄúPrepare-coffee.‚Äù</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.12533/x8.png alt></figure></p><blockquote><p>üîº This figure shows a continuation of the planning traces from Figure 7, illustrating how the Foundation Model (FM) in the Being-0 framework plans and reasons to complete the &lsquo;Prepare-coffee&rsquo; task. It details the FM&rsquo;s image observations, reasoning, task inference, action planning, and self-reflection steps as it guides the robot through various subtasks, like grabbing the cup and interacting with the coffee machine. The traces provide a detailed, step-by-step view of the FM&rsquo;s decision-making process and highlight its ability to break down a complex task into smaller, manageable actions.</p><details><summary>read the caption</summary>Figure 8: (Continued) Planning traces of the Foundation Model in Being-0 for the task ‚ÄúPrepare-coffee.‚Äù</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.12533/x9.png alt></figure></p><blockquote><p>üîº Figure 9 shows a continuation of the detailed planning process of the Foundation Model in the Being-0 framework for the &lsquo;Prepare-coffee&rsquo; task. It illustrates the step-by-step reasoning, decision-making, and action planning of the Foundation Model as it processes visual information and updates its understanding of the task&rsquo;s progress. The figure details the model&rsquo;s internal state, including its understanding of the current situation, the selection of subsequent actions, and its self-reflection on past actions. This detailed trace provides insights into the internal workings of the Foundation Model and its ability to decompose complex tasks into smaller, manageable subtasks.</p><details><summary>read the caption</summary>Figure 9: (Continued) Planning traces of the Foundation Model in Being-0 for the task ‚ÄúPrepare-coffee.‚Äù</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.12533/x10.png alt></figure></p><blockquote><p>üîº This figure shows a continuation of the detailed planning steps taken by the Foundation Model (FM) in the Being-0 system while attempting to complete the &lsquo;Prepare-coffee&rsquo; task. It provides a detailed, step-by-step view of how the FM processes visual information from the robot&rsquo;s cameras, reasons about the task&rsquo;s progress, and plans subsequent actions. The detailed text shows the FM&rsquo;s internal reasoning steps, including image descriptions, subtask inferences, self-reflection analysis, and the selection of actions. This illustrates the complex decision-making process involved in executing even a seemingly simple task like making coffee with a humanoid robot.</p><details><summary>read the caption</summary>Figure 10: (Continued) Planning traces of the Foundation Model in Being-0 for the task ‚ÄúPrepare-coffee.‚Äù</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.12533/x11.png alt></figure></p><blockquote><p>üîº This figure displays the planning traces of the Foundation Model used in the Being-0 system (without the Connector module) while executing the &lsquo;Prepare-coffee&rsquo; task. It shows a step-by-step breakdown of how the model reasons, makes decisions, and selects actions based solely on its high-level understanding and without the benefit of real-time visual feedback from the Connector. Each step includes the model&rsquo;s interpretation of the image data, its reasoning process, the selected action, and a self-reflection evaluating the success of the previous action. The detailed traces highlight the challenges faced by a purely high-level approach in executing complex, real-world tasks, particularly involving navigation and manipulation in dynamic environments.</p><details><summary>read the caption</summary>Figure 11: Planning traces of the Foundation Model in Being-0 w/o Connector for the task ‚ÄúPrepare-coffee.‚Äù</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.12533/x12.png alt></figure></p><blockquote><p>üîº This figure shows the detailed planning traces of the Foundation Model in the Being-0 framework, specifically when the Embodied Connector module is NOT used, for the task of preparing coffee. It provides a step-by-step breakdown of the Foundation Model&rsquo;s reasoning, including image descriptions, self-reflection on previous actions, inference of subtasks, and ultimately, the planned actions for the robot. This detailed view highlights the challenges faced by the Foundation Model when operating without the intermediary assistance of the Connector, revealing its difficulties in consistently achieving successful task completion due to limitations in real-time visual understanding and precise skill execution.</p><details><summary>read the caption</summary>Figure 12: (Continued) Planning traces of the Foundation Model in Being-0 w/o Connector for the task ‚ÄúPrepare-coffee.‚Äù</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.12533/x13.png alt></figure></p><blockquote><p>üîº This figure shows the detailed planning traces of the Foundation Model in the Being-0 system without the Connector module for the &lsquo;Prepare coffee&rsquo; task. It provides a step-by-step breakdown of the model&rsquo;s reasoning, including image descriptions, self-reflection, task inference, and action planning. It illustrates the challenges of using the Foundation Model alone, such as failure to detect the cup on the table and inefficient search strategies, due to a lack of direct perception and embodiment.</p><details><summary>read the caption</summary>Figure 13: (Continued) Planning traces of the Foundation Model in Being-0 w/o Connector for the task ‚ÄúPrepare-coffee.‚Äù</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T2.5><tr class=ltx_tr id=S5.T2.5.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id=S5.T2.5.1.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.1.1.1><span class=ltx_p id=S5.T2.5.1.1.1.1 style=width:68.3pt><span class="ltx_text ltx_font_bold" id=S5.T2.5.1.1.1.1.1>Task</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id=S5.T2.5.1.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.1.2.1><span class=ltx_p id=S5.T2.5.1.2.1.1 style=width:34.1pt><span class="ltx_tabular ltx_align_middle" id=S5.T2.5.1.2.1.1.1><span class=ltx_tr id=S5.T2.5.1.2.1.1.1.1><span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T2.5.1.2.1.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T2.5.1.2.1.1.1.1.1.1>w/o Adjust.</span></span></span></span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id=S5.T2.5.1.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.1.3.1><span class=ltx_p id=S5.T2.5.1.3.1.1 style=width:34.1pt><span class="ltx_tabular ltx_align_middle" id=S5.T2.5.1.3.1.1.1><span class=ltx_tr id=S5.T2.5.1.3.1.1.1.1><span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T2.5.1.3.1.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T2.5.1.3.1.1.1.1.1.1>Being-0</span></span></span></span></span></span></td></tr><tr class=ltx_tr id=S5.T2.5.2><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id=S5.T2.5.2.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.2.1.1><span class=ltx_p id=S5.T2.5.2.1.1.1 style=width:68.3pt><span class="ltx_text ltx_font_bold" id=S5.T2.5.2.1.1.1.1>Grasp-bottle</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id=S5.T2.5.2.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.2.2.1><span class=ltx_p id=S5.T2.5.2.2.1.1 style=width:34.1pt>2 / 5</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id=S5.T2.5.2.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.2.3.1><span class=ltx_p id=S5.T2.5.2.3.1.1 style=width:34.1pt><span class="ltx_text ltx_font_bold" id=S5.T2.5.2.3.1.1.1>4</span> / 5</span></span></td></tr><tr class=ltx_tr id=S5.T2.5.3><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T2.5.3.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.3.1.1><span class=ltx_p id=S5.T2.5.3.1.1.1 style=width:68.3pt><span class="ltx_text ltx_font_bold" id=S5.T2.5.3.1.1.1.1>Place-basket</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T2.5.3.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.3.2.1><span class=ltx_p id=S5.T2.5.3.2.1.1 style=width:34.1pt><span class="ltx_text ltx_font_bold" id=S5.T2.5.3.2.1.1.1>4</span> / 5</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T2.5.3.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.3.3.1><span class=ltx_p id=S5.T2.5.3.3.1.1 style=width:34.1pt>3 / 5</span></span></td></tr><tr class=ltx_tr id=S5.T2.5.4><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T2.5.4.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.4.1.1><span class=ltx_p id=S5.T2.5.4.1.1.1 style=width:68.3pt><span class="ltx_text ltx_font_bold" id=S5.T2.5.4.1.1.1.1>Grasp-coffee</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T2.5.4.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.4.2.1><span class=ltx_p id=S5.T2.5.4.2.1.1 style=width:34.1pt>1 / 5</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T2.5.4.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.4.3.1><span class=ltx_p id=S5.T2.5.4.3.1.1 style=width:34.1pt><span class="ltx_text ltx_font_bold" id=S5.T2.5.4.3.1.1.1>4</span> / 5</span></span></td></tr><tr class=ltx_tr id=S5.T2.5.5><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T2.5.5.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.5.1.1><span class=ltx_p id=S5.T2.5.5.1.1.1 style=width:68.3pt><span class="ltx_text ltx_font_bold" id=S5.T2.5.5.1.1.1.1>Place-coffee <span class="ltx_text ltx_font_italic" id=S5.T2.5.5.1.1.1.1.1>(t)</span></span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T2.5.5.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.5.2.1><span class=ltx_p id=S5.T2.5.5.2.1.1 style=width:34.1pt>4 / 5</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T2.5.5.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.5.3.1><span class=ltx_p id=S5.T2.5.5.3.1.1 style=width:34.1pt><span class="ltx_text ltx_font_bold" id=S5.T2.5.5.3.1.1.1>5</span> / 5</span></span></td></tr><tr class=ltx_tr id=S5.T2.5.6><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id=S5.T2.5.6.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.6.1.1><span class=ltx_p id=S5.T2.5.6.1.1.1 style=width:68.3pt><span class="ltx_text ltx_font_bold" id=S5.T2.5.6.1.1.1.1>Place-coffee <span class="ltx_text ltx_font_italic" id=S5.T2.5.6.1.1.1.1.1>(m)</span></span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id=S5.T2.5.6.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.6.2.1><span class=ltx_p id=S5.T2.5.6.2.1.1 style=width:34.1pt>0 / 5</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id=S5.T2.5.6.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T2.5.6.3.1><span class=ltx_p id=S5.T2.5.6.3.1.1 style=width:34.1pt><span class="ltx_text ltx_font_bold" id=S5.T2.5.6.3.1.1.1>3</span> / 5</span></span></td></tr></table></table></figure><blockquote><p>üîº This ablation study analyzes the impact of the proposed adjustment method within the Connector module on the success rate of manipulation tasks. Five navigation trials were conducted for each task, and the table shows how many of those trials resulted in successful manipulation. The tasks involve grasping and placing objects either on a table (denoted by &rsquo;t&rsquo;) or on a coffee machine (denoted by &rsquo;m&rsquo;). The results highlight the effectiveness of the adjustment method in improving manipulation success rates, particularly in scenarios where precise positioning is crucial.</p><details><summary>read the caption</summary>Table 2: Ablation study on the proposed adjustment method in the Connector module. The results indicate the number of successful manipulations out of 5 navigation trials. (t) denotes ‚Äúon the table‚Äù and (m) denotes ‚Äúon the coffee machine‚Äù.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T3.3><tr class=ltx_tr id=S5.T3.3.1><td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_tt" id=S5.T3.3.1.1 rowspan=2 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.1.1.1>Method</span></td><td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" colspan=2 id=S5.T3.3.1.2 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.1.2.1>Navigation</span></td><td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" colspan=2 id=S5.T3.3.1.3 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.1.3.1>Manipulation</span></td></tr><tr class=ltx_tr id=S5.T3.3.2><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T3.3.2.1 style="padding:1pt 0pt">table</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T3.3.2.2 style="padding:1pt 0pt">coffee machine</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T3.3.2.3 style="padding:1pt 0pt">grasp coffee</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T3.3.2.4 style="padding:1pt 0pt">place coffee</td></tr><tr class=ltx_tr id=S5.T3.3.3><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id=S5.T3.3.3.1 style="padding:1pt 0pt"><table class="ltx_tabular ltx_align_middle" id=S5.T3.3.3.1.1><tr class=ltx_tr id=S5.T3.3.3.1.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T3.3.3.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.3.1.1.1.1.1>Fixed Cam. (0.3)</span></td></tr></table></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T3.3.3.2 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.3.2.1>5</span> / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T3.3.3.3 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.3.3.1>5</span> / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T3.3.3.4 style="padding:1pt 0pt">0 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T3.3.3.5 style="padding:1pt 0pt">0 / 5</td></tr><tr class=ltx_tr id=S5.T3.3.4><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=S5.T3.3.4.1 style="padding:1pt 0pt"><table class="ltx_tabular ltx_align_middle" id=S5.T3.3.4.1.1><tr class=ltx_tr id=S5.T3.3.4.1.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T3.3.4.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.4.1.1.1.1.1>Fixed Cam. (0.6)</span></td></tr></table></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T3.3.4.2 style="padding:1pt 0pt">0 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T3.3.4.3 style="padding:1pt 0pt">0 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T3.3.4.4 style="padding:1pt 0pt">2 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T3.3.4.5 style="padding:1pt 0pt">1 / 5</td></tr><tr class=ltx_tr id=S5.T3.3.5><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=S5.T3.3.5.1 style="padding:1pt 0pt"><table class="ltx_tabular ltx_align_middle" id=S5.T3.3.5.1.1><tr class=ltx_tr id=S5.T3.3.5.1.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T3.3.5.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.5.1.1.1.1.1>Fixed Cam. (0.9)</span></td></tr></table></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T3.3.5.2 style="padding:1pt 0pt">0 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T3.3.5.3 style="padding:1pt 0pt">0 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T3.3.5.4 style="padding:1pt 0pt">4 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T3.3.5.5 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.5.5.1>5</span> / 5</td></tr><tr class=ltx_tr id=S5.T3.3.6><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id=S5.T3.3.6.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.6.1.1>Being-0 (Active Cam.)</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id=S5.T3.3.6.2 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.6.2.1>5</span> / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id=S5.T3.3.6.3 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.6.3.1>5</span> / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id=S5.T3.3.6.4 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.6.4.1>5</span> / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id=S5.T3.3.6.5 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.6.5.1>5</span> / 5</td></tr></table></table></figure><blockquote><p>üîº This table presents the success rates of navigation and manipulation tasks performed by the robot with various camera configurations. Specifically, it compares the performance of the robot using an active camera (that can adjust its pitch angle dynamically) against several configurations using a fixed camera at different pre-set pitch angles (0.3, 0.6, and 0.9). The results demonstrate the impact of active vision on task completion success rates.</p><details><summary>read the caption</summary>Table 3: Success rates of navigation and manipulation tasks with different active camera configurations. The number following Fixed Cam. denotes the pitch angle set for the camera in the absence of active neck movement.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S5.T3.3.3.1.1><tr class=ltx_tr id=S5.T3.3.3.1.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T3.3.3.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.3.1.1.1.1.1>Fixed Cam. (0.3)</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the results of an ablation study evaluating the efficiency of the Being-0 navigation system. It compares three configurations: Being-0 (the complete system), a version without the Connector module, and a version with a fixed camera at a specific pitch angle. For each configuration, the average moving speed (in cm/s) and the success rate (number of successful navigation trials out of 5) are reported. This allows for an assessment of the impact of the Connector module and the use of active vision on navigation performance.</p><details><summary>read the caption</summary>Table 4: Ablation study on the efficiency of Being-0 in navigation. The table reports the average moving speed (cm/s) and success rates for various agent configurations.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S5.T3.3.4.1.1><tr class=ltx_tr id=S5.T3.3.4.1.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T3.3.4.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.4.1.1.1.1.1>Fixed Cam. (0.6)</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the success rates of the Being-0 navigation system across various experimental conditions. It shows how well the robot navigated to different target locations within various indoor environments. The environments tested included simple rooms, rooms with obstacles, and rooms that required crossing between areas. This data demonstrates the robustness and generalizability of the navigation system in complex scenarios.</p><details><summary>read the caption</summary>Table 5: Navigation performance across various scene configurations and target locations.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S5.T3.3.5.1.1><tr class=ltx_tr id=S5.T3.3.5.1.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T3.3.5.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T3.3.5.1.1.1.1.1>Fixed Cam. (0.9)</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the success rates of various manipulation skills performed by the humanoid robot in different scenarios. These scenarios include using seen and unseen objects and situations with visual disturbances. The results demonstrate the robustness and generalizability of the learned manipulation skills. The asterisk (*) indicates that tactile sensors were added to the dexterous hands for those specific trials.</p><details><summary>read the caption</summary>Table 6: Performance of manipulation skills across different scene configurations, including seen objects, unseen objects, and scenarios with visual perturbations. * denotes the use of dexterous hands equipped with tactile sensors.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S5.T4.1.1><tr class=ltx_tr id=S5.T4.1.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id=S5.T4.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T4.1.1.1.1.1>Method</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id=S5.T4.1.1.1.2 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T4.1.1.1.2.1>Avg. Speed</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id=S5.T4.1.1.1.3 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T4.1.1.1.3.1>Success</span></td></tr><tr class=ltx_tr id=S5.T4.1.1.2><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id=S5.T4.1.1.2.1 style="padding:1pt 0pt"><table class="ltx_tabular ltx_align_middle" id=S5.T4.1.1.2.1.1><tr class=ltx_tr id=S5.T4.1.1.2.1.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T4.1.1.2.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T4.1.1.2.1.1.1.1.1>w/o Connector</span></td></tr></table></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T4.1.1.2.2 style="padding:1pt 0pt">2.3</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T4.1.1.2.3 style="padding:1pt 0pt">0 / 5</td></tr><tr class=ltx_tr id=S5.T4.1.1.3><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=S5.T4.1.1.3.1 style="padding:1pt 0pt"><table class="ltx_tabular ltx_align_middle" id=S5.T4.1.1.3.1.1><tr class=ltx_tr id=S5.T4.1.1.3.1.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T4.1.1.3.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T4.1.1.3.1.1.1.1.1>Fixed Cam. (0.3)</span></td></tr></table></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T4.1.1.3.2 style="padding:1pt 0pt">8.5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T4.1.1.3.3 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T4.1.1.3.3.1>5</span> / 5</td></tr><tr class=ltx_tr id=S5.T4.1.1.4><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id=S5.T4.1.1.4.1 style="padding:1pt 0pt"><table class="ltx_tabular ltx_align_middle" id=S5.T4.1.1.4.1.1><tr class=ltx_tr id=S5.T4.1.1.4.1.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T4.1.1.4.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T4.1.1.4.1.1.1.1.1>Being-0</span></td></tr></table></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id=S5.T4.1.1.4.2 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T4.1.1.4.2.1>9.6</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id=S5.T4.1.1.4.3 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T4.1.1.4.3.1>5</span> / 5</td></tr></table></table></figure><blockquote><p>üîº This table presents a breakdown of the sub-processes involved in completing several long-horizon tasks. For each task, it lists the individual steps required, and shows the success rates achieved by the Being-0 system (with the Connector module) and a baseline system (without the Connector). The results highlight the improvement in task completion rates resulting from the addition of the Connector module.</p><details><summary>read the caption</summary>Table 7: Detailed sub-processes required to complete each long-horizon task, along with the success rates of Being-0 and the baseline.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S5.T4.1.1.2.1.1><tr class=ltx_tr id=S5.T4.1.1.2.1.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T4.1.1.2.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T4.1.1.2.1.1.1.1.1>w/o Connector</span></td></tr></table></table></figure><blockquote><p>üîº This table shows the number of successful trajectory demonstrations collected for each manipulation skill during the teleoperation phase of skill acquisition. The data was used to train the manipulation skill policies via imitation learning. The number of trajectories varies based on the complexity of the task and the ease of demonstration.</p><details><summary>read the caption</summary>Table 8: Number of trajectories collected for each manipulation skill.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S5.T4.1.1.3.1.1><tr class=ltx_tr id=S5.T4.1.1.3.1.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T4.1.1.3.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T4.1.1.3.1.1.1.1.1>Fixed Cam. (0.3)</span></td></tr></table></table></figure><blockquote><p>üîº This table lists the hyperparameters used during the training of the Actor-Critic (ACT) policy for acquiring manipulation skills. The ACT policy is a behavior cloning method that uses a Transformer architecture and is trained using teleoperation data. The hyperparameters shown control aspects of the training process, such as the number of training steps, batch size, learning rate, and gradient clipping.</p><details><summary>read the caption</summary>Table 9: Hyperparameters used for training the ACT policy.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S5.T4.1.1.4.1.1><tr class=ltx_tr id=S5.T4.1.1.4.1.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T4.1.1.4.1.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T4.1.1.4.1.1.1.1.1>Being-0</span></td></tr></table></table></figure><blockquote><p>üîº This table presents examples of training data used to teach the vision-language model to perform visual understanding tasks. It illustrates how different question types (bounding box detection, item identification, image description, and ground identification) are phrased and what types of answers are expected. This data helps the model learn to connect visual input with various language-based queries.</p><details><summary>read the caption</summary>Table 10: Examples of the training data for training the vision-language model to acquire the visual understanding ability.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T5.1><tr class=ltx_tr id=S5.T5.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id=S5.T5.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T5.1.1.1.1>Task</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id=S5.T5.1.1.2 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T5.1.1.2.1>Success</span></td></tr><tr class=ltx_tr id=S5.T5.1.2><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id=S5.T5.1.2.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T5.1.2.1.1>In-room</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S5.T5.1.2.2 style="padding:1pt 0pt">1.00</td></tr><tr class=ltx_tr id=S5.T5.1.3><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=S5.T5.1.3.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T5.1.3.1.1>In-room with obstacles</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S5.T5.1.3.2 style="padding:1pt 0pt">0.80</td></tr><tr class=ltx_tr id=S5.T5.1.4><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id=S5.T5.1.4.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=S5.T5.1.4.1.1>Cross-room</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id=S5.T5.1.4.2 style="padding:1pt 0pt">0.83</td></tr></table></table></figure><blockquote><p>üîº This table presents a summary of the dataset used to train the vision-language model (VLM) within the embodied connector module. It breaks down the number of samples collected for each type of visual understanding (VLU) task and action planning (AP) task. The VLU tasks include bounding box detection, yes/no questions, and image description, while the AP tasks involve the planning of actions by the robot.</p><details><summary>read the caption</summary>Table 11: Task Categories and Sample Numbers</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T6.3><tr class=ltx_tr id=S5.T6.3.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id=S5.T6.3.1.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.1.1.1><span class=ltx_p id=S5.T6.3.1.1.1.1 style=width:68.3pt><span class="ltx_text ltx_font_bold" id=S5.T6.3.1.1.1.1.1>Task</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id=S5.T6.3.1.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.1.2.1><span class=ltx_p id=S5.T6.3.1.2.1.1 style=width:14.2pt><span class="ltx_text ltx_font_bold" id=S5.T6.3.1.2.1.1.1>Seen Obj.</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id=S5.T6.3.1.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.1.3.1><span class=ltx_p id=S5.T6.3.1.3.1.1 style=width:14.2pt><span class="ltx_text ltx_font_bold" id=S5.T6.3.1.3.1.1.1>Unseen Obj.</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id=S5.T6.3.1.4 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.1.4.1><span class=ltx_p id=S5.T6.3.1.4.1.1 style=width:37pt><span class="ltx_text ltx_font_bold" id=S5.T6.3.1.4.1.1.1>Perturb.</span></span></span></td></tr><tr class=ltx_tr id=S5.T6.3.2><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id=S5.T6.3.2.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.2.1.1><span class=ltx_p id=S5.T6.3.2.1.1.1 style=width:68.3pt><span class="ltx_text ltx_font_bold" id=S5.T6.3.2.1.1.1.1>Grasp-bottle</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id=S5.T6.3.2.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.2.2.1><span class=ltx_p id=S5.T6.3.2.2.1.1 style=width:14.2pt>0.86</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id=S5.T6.3.2.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.2.3.1><span class=ltx_p id=S5.T6.3.2.3.1.1 style=width:14.2pt>0.63</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id=S5.T6.3.2.4 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.2.4.1><span class=ltx_p id=S5.T6.3.2.4.1.1 style=width:37pt>0.77</span></span></td></tr><tr class=ltx_tr id=S5.T6.3.3><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T6.3.3.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.3.1.1><span class=ltx_p id=S5.T6.3.3.1.1.1 style=width:68.3pt><span class="ltx_text ltx_font_bold" id=S5.T6.3.3.1.1.1.1>Handout-snack</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T6.3.3.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.3.2.1><span class=ltx_p id=S5.T6.3.3.2.1.1 style=width:14.2pt>0.90</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T6.3.3.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.3.3.1><span class=ltx_p id=S5.T6.3.3.3.1.1 style=width:14.2pt>1.00</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T6.3.3.4 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.3.4.1><span class=ltx_p id=S5.T6.3.3.4.1.1 style=width:37pt>0.80</span></span></td></tr><tr class=ltx_tr id=S5.T6.3.4><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T6.3.4.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.4.1.1><span class=ltx_p id=S5.T6.3.4.1.1.1 style=width:68.3pt><span class="ltx_text ltx_font_bold" id=S5.T6.3.4.1.1.1.1>Place-pole</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T6.3.4.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.4.2.1><span class=ltx_p id=S5.T6.3.4.2.1.1 style=width:14.2pt>0.90</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T6.3.4.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.4.3.1><span class=ltx_p id=S5.T6.3.4.3.1.1 style=width:14.2pt>‚Äì</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id=S5.T6.3.4.4 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.4.4.1><span class=ltx_p id=S5.T6.3.4.4.1.1 style=width:37pt>0.80</span></span></td></tr><tr class=ltx_tr id=S5.T6.3.5><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id=S5.T6.3.5.1 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.5.1.1><span class=ltx_p id=S5.T6.3.5.1.1.1 style=width:68.3pt><span class="ltx_text ltx_font_bold" id=S5.T6.3.5.1.1.1.1>Play-chess*</span></span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id=S5.T6.3.5.2 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.5.2.1><span class=ltx_p id=S5.T6.3.5.2.1.1 style=width:14.2pt>0.90</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id=S5.T6.3.5.3 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.5.3.1><span class=ltx_p id=S5.T6.3.5.3.1.1 style=width:14.2pt>‚Äì</span></span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id=S5.T6.3.5.4 style="padding:1pt 0pt"><span class="ltx_inline-block ltx_align_top" id=S5.T6.3.5.4.1><span class=ltx_p id=S5.T6.3.5.4.1.1 style=width:37pt>0.90</span></span></td></tr></table></table></figure><blockquote><p>üîº This table presents a detailed breakdown of the data used for training the bounding box prediction model. Each row represents a category of objects or areas within the dataset, indicating how many images are associated with each category. The categories cover various locations (e.g., Kitchen Area, Robot Laboratory Room) and specific objects within these locations (e.g., coffee machine, espresso coffee button). The &lsquo;Value&rsquo; column shows the frequency of appearance of each category in the dataset, crucial for understanding the class distribution and potential biases in the model&rsquo;s training data. This information is important for assessing the model&rsquo;s performance and identifying areas for potential improvement.</p><details><summary>read the caption</summary>Table 12: Category data overview for the bounding box task.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=A1.T7.1><tr class=ltx_tr id=A1.T7.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id=A1.T7.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=A1.T7.1.1.1.1>Task</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id=A1.T7.1.1.2 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=A1.T7.1.1.2.1>Sub-Process</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id=A1.T7.1.1.3 style="padding:1pt 0pt"><table class="ltx_tabular ltx_align_middle" id=A1.T7.1.1.3.1><tr class=ltx_tr id=A1.T7.1.1.3.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.1.3.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=A1.T7.1.1.3.1.1.1.1>w/o Connector</span></td></tr></table></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id=A1.T7.1.1.4 style="padding:1pt 0pt"><table class="ltx_tabular ltx_align_middle" id=A1.T7.1.1.4.1><tr class=ltx_tr id=A1.T7.1.1.4.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.1.4.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=A1.T7.1.1.4.1.1.1.1>Being-0</span></td></tr></table></td></tr><tr class=ltx_tr id=A1.T7.1.2><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id=A1.T7.1.2.1 rowspan=2 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=A1.T7.1.2.1.1>Fetch-bottle</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id=A1.T7.1.2.2 style="padding:1pt 0pt">Navigate to table.</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=A1.T7.1.2.3 style="padding:1pt 0pt">0 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=A1.T7.1.2.4 style="padding:1pt 0pt">5 / 5</td></tr><tr class=ltx_tr id=A1.T7.1.3><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A1.T7.1.3.1 style="padding:1pt 0pt">Grasp cup.</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.3.2 style="padding:1pt 0pt">0 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.3.3 style="padding:1pt 0pt">4 / 5</td></tr><tr class=ltx_tr id=A1.T7.1.4><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id=A1.T7.1.4.1 rowspan=2 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=A1.T7.1.4.1.1>Deliver-basket</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id=A1.T7.1.4.2 style="padding:1pt 0pt">Navigate to table.</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=A1.T7.1.4.3 style="padding:1pt 0pt">0 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=A1.T7.1.4.4 style="padding:1pt 0pt">5 / 5</td></tr><tr class=ltx_tr id=A1.T7.1.5><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A1.T7.1.5.1 style="padding:1pt 0pt">Place basket.</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.5.2 style="padding:1pt 0pt">0 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.5.3 style="padding:1pt 0pt">3 / 5</td></tr><tr class=ltx_tr id=A1.T7.1.6><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id=A1.T7.1.6.1 rowspan=4 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=A1.T7.1.6.1.1>Prepare-coffee</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id=A1.T7.1.6.2 style="padding:1pt 0pt">Navigate to table.</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=A1.T7.1.6.3 style="padding:1pt 0pt">0 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=A1.T7.1.6.4 style="padding:1pt 0pt">5 / 5</td></tr><tr class=ltx_tr id=A1.T7.1.7><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A1.T7.1.7.1 style="padding:1pt 0pt">Grasp cup.</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.7.2 style="padding:1pt 0pt">0 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.7.3 style="padding:1pt 0pt">4 / 5</td></tr><tr class=ltx_tr id=A1.T7.1.8><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A1.T7.1.8.1 style="padding:1pt 0pt">Navigate to coffee machine.</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.8.2 style="padding:1pt 0pt">0 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.8.3 style="padding:1pt 0pt">3 / 5</td></tr><tr class=ltx_tr id=A1.T7.1.9><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A1.T7.1.9.1 style="padding:1pt 0pt">Place cup.</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.9.2 style="padding:1pt 0pt">0 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.9.3 style="padding:1pt 0pt">3 / 5</td></tr><tr class=ltx_tr id=A1.T7.1.10><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id=A1.T7.1.10.1 rowspan=4 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=A1.T7.1.10.1.1>Make-coffee</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id=A1.T7.1.10.2 style="padding:1pt 0pt">Place cup.</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=A1.T7.1.10.3 style="padding:1pt 0pt">5 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=A1.T7.1.10.4 style="padding:1pt 0pt">5 / 5</td></tr><tr class=ltx_tr id=A1.T7.1.11><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A1.T7.1.11.1 style="padding:1pt 0pt">Select coffee.</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.11.2 style="padding:1pt 0pt">5 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.11.3 style="padding:1pt 0pt">5 / 5</td></tr><tr class=ltx_tr id=A1.T7.1.12><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A1.T7.1.12.1 style="padding:1pt 0pt">Select confirmation.</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.12.2 style="padding:1pt 0pt">4 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.12.3 style="padding:1pt 0pt">4 / 5</td></tr><tr class=ltx_tr id=A1.T7.1.13><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A1.T7.1.13.1 style="padding:1pt 0pt">Grasp cup.</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.13.2 style="padding:1pt 0pt">4 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.13.3 style="padding:1pt 0pt">4 / 5</td></tr><tr class=ltx_tr id=A1.T7.1.14><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t" id=A1.T7.1.14.1 rowspan=3 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=A1.T7.1.14.1.1>Deliver-coffee</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id=A1.T7.1.14.2 style="padding:1pt 0pt">Grasp-cup.</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=A1.T7.1.14.3 style="padding:1pt 0pt">5 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=A1.T7.1.14.4 style="padding:1pt 0pt">5 / 5</td></tr><tr class=ltx_tr id=A1.T7.1.15><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A1.T7.1.15.1 style="padding:1pt 0pt">Navigate to table.</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.15.2 style="padding:1pt 0pt">0 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.15.3 style="padding:1pt 0pt">4 / 5</td></tr><tr class=ltx_tr id=A1.T7.1.16><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id=A1.T7.1.16.1 style="padding:1pt 0pt">Place cup.</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id=A1.T7.1.16.2 style="padding:1pt 0pt">0 / 5</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id=A1.T7.1.16.3 style="padding:1pt 0pt">4 / 5</td></tr></table></table></figure><blockquote><p>üîº This table presents a breakdown of the data used for training the vision-language model&rsquo;s &lsquo;yes/no&rsquo; question answering ability. It lists the different categories of objects or scenarios (e.g., &lsquo;Coffee Machine&rsquo;, &lsquo;Reception Desk&rsquo;, &lsquo;Hallway&rsquo;) and the number of training samples associated with each category. This helps to understand the distribution of data across various aspects of the environment, ensuring a balanced model.</p><details><summary>read the caption</summary>Table 13: Category data overview for the yes/no task.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=A1.T7.1.1.3.1><tr class=ltx_tr id=A1.T7.1.1.3.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.1.3.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=A1.T7.1.1.3.1.1.1.1>w/o Connector</span></td></tr></table></table></figure><blockquote><p>üîº This table details the prompt used in the information gathering process. The prompt instructs the foundation model (a large language model) on how to process visual and textual inputs to understand the robot&rsquo;s environment and task. Specific instructions are given for image description, identifying the robot&rsquo;s current location, determining the cup-holding status, and determining which objects need to be targeted next.</p><details><summary>read the caption</summary>Table 14: The prompt we used for information gathering process.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=A1.T7.1.1.4.1><tr class=ltx_tr id=A1.T7.1.1.4.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=A1.T7.1.1.4.1.1.1 style="padding:1pt 0pt"><span class="ltx_text ltx_font_bold" id=A1.T7.1.1.4.1.1.1.1>Being-0</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the prompt used by the foundation model in the Being-0 framework to summarize the progress of a task and to propose a new subtask. The prompt guides the model through a structured process, incorporating information from previous subtasks, the current image, and a semantic map. It includes sections for summarizing past actions, self-reflection, error analysis, and ultimately proposing a well-reasoned next step for the robot to take toward task completion. The prompt design emphasizes a systematic and logical approach to decision making in complex, real-world scenarios.</p><details><summary>read the caption</summary>Table 15: The prompt for summarizing task progress and proposing a new subtask.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=A2.T8.1><tr class=ltx_tr id=A2.T8.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id=A2.T8.1.1.1 style=padding-left:0pt;padding-right:0pt><span class="ltx_text ltx_font_bold" id=A2.T8.1.1.1.1>Skill</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_tt" id=A2.T8.1.1.2 style=padding-left:0pt;padding-right:0pt><span class="ltx_text ltx_font_bold" id=A2.T8.1.1.2.1>Num. Trajectories</span></td></tr><tr class=ltx_tr id=A2.T8.1.2><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id=A2.T8.1.2.1 style=padding-left:0pt;padding-right:0pt>Carry Basket</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_t" id=A2.T8.1.2.2 style=padding-left:0pt;padding-right:0pt>25</td></tr><tr class=ltx_tr id=A2.T8.1.3><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A2.T8.1.3.1 style=padding-left:0pt;padding-right:0pt>Handout Snack</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id=A2.T8.1.3.2 style=padding-left:0pt;padding-right:0pt>50</td></tr><tr class=ltx_tr id=A2.T8.1.4><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A2.T8.1.4.1 style=padding-left:0pt;padding-right:0pt>Grasp Bottle</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id=A2.T8.1.4.2 style=padding-left:0pt;padding-right:0pt>150</td></tr><tr class=ltx_tr id=A2.T8.1.5><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A2.T8.1.5.1 style=padding-left:0pt;padding-right:0pt>Grasp Cup</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id=A2.T8.1.5.2 style=padding-left:0pt;padding-right:0pt>200</td></tr><tr class=ltx_tr id=A2.T8.1.6><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A2.T8.1.6.1 style=padding-left:0pt;padding-right:0pt>Open Beer</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id=A2.T8.1.6.2 style=padding-left:0pt;padding-right:0pt>50</td></tr><tr class=ltx_tr id=A2.T8.1.7><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A2.T8.1.7.1 style=padding-left:0pt;padding-right:0pt>Place Basket</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id=A2.T8.1.7.2 style=padding-left:0pt;padding-right:0pt>25</td></tr><tr class=ltx_tr id=A2.T8.1.8><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A2.T8.1.8.1 style=padding-left:0pt;padding-right:0pt>Place Cup</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id=A2.T8.1.8.2 style=padding-left:0pt;padding-right:0pt>200</td></tr><tr class=ltx_tr id=A2.T8.1.9><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A2.T8.1.9.1 style=padding-left:0pt;padding-right:0pt>Place Pole</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id=A2.T8.1.9.2 style=padding-left:0pt;padding-right:0pt>50</td></tr><tr class=ltx_tr id=A2.T8.1.10><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A2.T8.1.10.1 style=padding-left:0pt;padding-right:0pt>Play Chess</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id=A2.T8.1.10.2 style=padding-left:0pt;padding-right:0pt>70</td></tr><tr class=ltx_tr id=A2.T8.1.11><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id=A2.T8.1.11.1 style=padding-left:0pt;padding-right:0pt>Play Toy Bricks</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb" id=A2.T8.1.11.2 style=padding-left:0pt;padding-right:0pt>50</td></tr></table></table></figure><blockquote><p>üîº This table presents the prompt used for the self-reflection step in the Being-0 framework. The prompt guides the foundation model to analyze the results of the last action, evaluate its success in achieving the overall goal, and provide a reasoned analysis of why it succeeded or failed. It also includes fields for providing visual information (images) and contextual data (semantic map, robot location). The prompt aims to make the foundation model&rsquo;s reasoning process more transparent and robust, leading to better decision-making and task completion.</p><details><summary>read the caption</summary>Table 16: The prompt for reflecting on the task and evaluating success.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=A2.T9.1><tr class=ltx_tr id=A2.T9.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id=A2.T9.1.1.1 style=padding-left:0pt;padding-right:0pt><span class="ltx_text ltx_font_bold" id=A2.T9.1.1.1.1>Hyperparameter</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_tt" id=A2.T9.1.1.2 style=padding-left:0pt;padding-right:0pt><span class="ltx_text ltx_font_bold" id=A2.T9.1.1.2.1>Value</span></td></tr><tr class=ltx_tr id=A2.T9.1.2><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id=A2.T9.1.2.1 style=padding-left:0pt;padding-right:0pt>Training steps</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_t" id=A2.T9.1.2.2 style=padding-left:0pt;padding-right:0pt>500,000</td></tr><tr class=ltx_tr id=A2.T9.1.3><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A2.T9.1.3.1 style=padding-left:0pt;padding-right:0pt>Batch size</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id=A2.T9.1.3.2 style=padding-left:0pt;padding-right:0pt>90</td></tr><tr class=ltx_tr id=A2.T9.1.4><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A2.T9.1.4.1 style=padding-left:0pt;padding-right:0pt>Learning rate</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id=A2.T9.1.4.2 style=padding-left:0pt;padding-right:0pt>1e-5</td></tr><tr class=ltx_tr id=A2.T9.1.5><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A2.T9.1.5.1 style=padding-left:0pt;padding-right:0pt>Gradient clip norm</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id=A2.T9.1.5.2 style=padding-left:0pt;padding-right:0pt>10</td></tr><tr class=ltx_tr id=A2.T9.1.6><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id=A2.T9.1.6.1 style=padding-left:0pt;padding-right:0pt>Chunk size (train)</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id=A2.T9.1.6.2 style=padding-left:0pt;padding-right:0pt>30</td></tr><tr class=ltx_tr id=A2.T9.1.7><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id=A2.T9.1.7.1 style=padding-left:0pt;padding-right:0pt>Chunk size (test)</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb" id=A2.T9.1.7.2 style=padding-left:0pt;padding-right:0pt>10</td></tr></table></table></figure><blockquote><p>üîº This table displays the prompt used for the Foundation Model (FM) in the Being-0 framework. The prompt instructs the FM on how to select the next action for the humanoid robot. It provides the FM with relevant context, including the current task, subtask, visual input (image description), robot&rsquo;s status (location, whether it holds an object), past actions and their outcomes, a list of available actions (skill library), and a semantic map. The prompt guides the FM in making decisions by asking it to consider success conditions, relevant spatial relationships, and the need to search for targets that might not be immediately visible. The prompt is designed to promote both successful task execution and efficient decision-making by the FM.</p><details><summary>read the caption</summary>Table 17: The prompt for decision-making and action execution in humanoid robot tasks.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=A2.T10.1.1><tr class=ltx_tr id=A2.T10.1.1.1><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_l ltx_border_r ltx_border_tt" id=A2.T10.1.1.1.1 style=padding-left:0pt;padding-right:0pt><span class="ltx_text ltx_font_bold" id=A2.T10.1.1.1.1.1>Task Name</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_tt" id=A2.T10.1.1.1.2 style=padding-left:0pt;padding-right:0pt><span class="ltx_text ltx_font_bold" id=A2.T10.1.1.1.2.1>Samples</span></td></tr><tr class=ltx_tr id=A2.T10.1.1.2><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id=A2.T10.1.1.2.1 style=padding-left:0pt;padding-right:0pt>Bounding Boxes</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id=A2.T10.1.1.2.2 style=padding-left:0pt;padding-right:0pt><span class="ltx_text ltx_font_bold" id=A2.T10.1.1.2.2.1>Text Prompt</span>: Detect the bounding box of target object: Reception desk.</td></tr><tr class=ltx_tr id=A2.T10.1.1.3><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_l ltx_border_r" id=A2.T10.1.1.3.1 style=padding-left:0pt;padding-right:0pt></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id=A2.T10.1.1.3.2 style=padding-left:0pt;padding-right:0pt><span class="ltx_text ltx_font_bold" id=A2.T10.1.1.3.2.1>Answer</span>: [0.471, 0.082, 0.765, 0.951]</td></tr><tr class=ltx_tr id=A2.T10.1.1.4><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id=A2.T10.1.1.4.1 style=padding-left:0pt;padding-right:0pt>Identify items</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id=A2.T10.1.1.4.2 style=padding-left:0pt;padding-right:0pt><span class="ltx_text ltx_font_bold" id=A2.T10.1.1.4.2.1>Text Prompt</span>: Is there an exit in the image? Do not generate anything else except ‚ÄôYes‚Äô or ‚ÄôNo‚Äô.</td></tr><tr class=ltx_tr id=A2.T10.1.1.5><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_l ltx_border_r" id=A2.T10.1.1.5.1 style=padding-left:0pt;padding-right:0pt></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id=A2.T10.1.1.5.2 style=padding-left:0pt;padding-right:0pt><span class="ltx_text ltx_font_bold" id=A2.T10.1.1.5.2.1>Answer</span>: No.</td></tr><tr class=ltx_tr id=A2.T10.1.1.6><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id=A2.T10.1.1.6.1 style=padding-left:0pt;padding-right:0pt>Describe image</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id=A2.T10.1.1.6.2 style=padding-left:0pt;padding-right:0pt><span class="ltx_text ltx_font_bold" id=A2.T10.1.1.6.2.1>Text Prompt</span>: Describe the current image. List all the items you could see.</td></tr><tr class=ltx_tr id=A2.T10.1.1.7><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_l ltx_border_r" id=A2.T10.1.1.7.1 style=padding-left:0pt;padding-right:0pt></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id=A2.T10.1.1.7.2 style=padding-left:0pt;padding-right:0pt><span class="ltx_text ltx_font_bold" id=A2.T10.1.1.7.2.1>Answer</span>: The image shows a corridor or office area with the following items visible:</td></tr><tr class=ltx_tr id=A2.T10.1.1.8><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_l ltx_border_r" id=A2.T10.1.1.8.1 style=padding-left:0pt;padding-right:0pt></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id=A2.T10.1.1.8.2 style=padding-left:0pt;padding-right:0pt>1. A person wearing a light-colored outfit.</td></tr><tr class=ltx_tr id=A2.T10.1.1.9><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_l ltx_border_r" id=A2.T10.1.1.9.1 style=padding-left:0pt;padding-right:0pt></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id=A2.T10.1.1.9.2 style=padding-left:0pt;padding-right:0pt>2. A green-framed glass door.</td></tr><tr class=ltx_tr id=A2.T10.1.1.10><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_l ltx_border_r" id=A2.T10.1.1.10.1 style=padding-left:0pt;padding-right:0pt></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id=A2.T10.1.1.10.2 style=padding-left:0pt;padding-right:0pt>3. A yellow sofa with cushions.</td></tr><tr class=ltx_tr id=A2.T10.1.1.11><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_l ltx_border_r" id=A2.T10.1.1.11.1 style=padding-left:0pt;padding-right:0pt></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id=A2.T10.1.1.11.2 style=padding-left:0pt;padding-right:0pt>4. A wall-mounted tablet or screen.</td></tr><tr class=ltx_tr id=A2.T10.1.1.12><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id=A2.T10.1.1.12.1 style=padding-left:0pt;padding-right:0pt>Describe the ground and identify the transition</td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id=A2.T10.1.1.12.2 style=padding-left:0pt;padding-right:0pt><span class="ltx_text ltx_font_bold" id=A2.T10.1.1.12.2.1>Text Prompt</span>: Describe the ground and identify the transition between different grounds if there are any.</td></tr><tr class=ltx_tr id=A2.T10.1.1.13><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_bb ltx_border_l ltx_border_r" id=A2.T10.1.1.13.1 style=padding-left:0pt;padding-right:0pt></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_r" id=A2.T10.1.1.13.2 style=padding-left:0pt;padding-right:0pt><span class="ltx_text ltx_font_bold" id=A2.T10.1.1.13.2.1>Answer</span>: grey pattern carpet</td></tr></table></table></figure><blockquote><p>üîº This table shows a concise prompt used for guiding a humanoid robot&rsquo;s decision-making process and action execution. It provides essential information like the overall task, subtask details, a map of the environment, available actions, and prompts the model to respond using Python code for action execution.</p><details><summary>read the caption</summary>Table 18: The shorter version of the prompt we used for decision making and action execution of humanoid robot</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-cce0122e47a98c4431b0e215abc820a5 class=gallery><img src=https://ai-paper-reviewer.com/2503.12533/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.12533/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12533/&amp;title=Being-0:%20A%20Humanoid%20Robotic%20Agent%20with%20Vision-Language%20Models%20and%20Modular%20Skills" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12533/&amp;text=Being-0:%20A%20Humanoid%20Robotic%20Agent%20with%20Vision-Language%20Models%20and%20Modular%20Skills" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12533/&amp;subject=Being-0:%20A%20Humanoid%20Robotic%20Agent%20with%20Vision-Language%20Models%20and%20Modular%20Skills" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2503.12533/index.md",oid_likes="likes_paper-reviews/2503.12533/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2503.12528/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Investigating Human-Aligned Large Language Model Uncertainty</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-03-16T00:00:00+00:00>16 March 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2503.12530/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Basic Category Usage in Vision Language Models</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-03-16T00:00:00+00:00>16 March 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>