[{"content": "| Component | Parameter | Value |\n|---|---|---|\n| M-Former | num_queries | 8 |\n|  | num_layers | 4 |\n|  | hidden_size | 768 |\n|  | num_heads | 12 |\n| ViT Decoder | patch_size | 16 |\n|  | num_layers | 12 |\n|  | hidden_size | 768 |\n|  | num_heads | 12 |\n| VQ Codebook | num_codes | 128 |\n|  | latent_dim | 32 |", "caption": "Table 1: Implementation details of the Latent Motion Tokenizer.", "description": "Table 1 provides detailed information about the architecture and hyperparameters of the Latent Motion Tokenizer, a key component of the Moto framework. It breaks down the specific components of the tokenizer (M-Former, ViT Decoder, and VQ Codebook), listing the parameters and their corresponding values. This table is crucial for understanding the design and implementation of the tokenizer, which plays a vital role in converting video data into a latent representation suitable for autoregressive pre-training.", "section": "3. Methodology"}, {"content": "| Parameter | Value |\n|---|---| \n| batch_size | 256 |\n| optimizer | AdamW |\n| lr_max | 1e-4 |\n| lr_schedule | cosine decay |\n| weight_decay | 1e-4 |\n| warmup_steps | 1000 |", "caption": "Table 2: Training hyperparameters for Latent Motion Tokenizer.", "description": "This table details the hyperparameters used during the training of the Latent Motion Tokenizer model.  It includes parameters such as batch size, optimizer type (AdamW), maximum learning rate, learning rate scheduling method (cosine decay), weight decay, and the number of warmup steps.", "section": "3. Methodology"}, {"content": "| Component | Parameter | Value |\n|---|---|---|\n| GPT backbone | num_layers | 12 |\n|  | hidden_size | 768 |\n|  | num_heads | 12 |\n| Action Head | num_layers | 2 |\n|  | hidden_size | 384 |", "caption": "Table 3: Implementation details of Moto-GPT.", "description": "Table 3 provides detailed specifications of the architecture and hyperparameters used in the Moto-GPT model.  It lists the components of the model (GPT backbone and Action Head), along with their respective parameters (such as number of layers, hidden size, and number of heads). This information is crucial for understanding the model's complexity and how it was configured for training and experimentation.", "section": "3. Methodology"}, {"content": "| Parameter | Value |\n|---|---| \n| batch_size | 512 |\n| optimizer | AdamW |\n| lr_max | 1e-4 |\n| lr_schedule | cosine decay |\n| weight_decay | 1e-4 |\n| warmup_epochs | 1 |", "caption": "Table 4: Training hyperparameters for Moto-GPT.", "description": "This table details the hyperparameters used during the training phase of the Moto-GPT model.  It includes settings for batch size, optimizer, learning rate, learning rate scheduling, weight decay, and warmup epochs. These parameters are crucial for effective model training and influence factors like convergence speed and generalization performance.", "section": "3. Methodology"}, {"content": "| Video Representation | Semantic Acc. |\n|---|---| \n| Initial frame | 0.292 |\n| Initial frame repeated by 8 times | 0.283 |\n| Initial frame + 7 subsequent frames | 0.828 |\n| Initial frame + 7 latent motion token chunks | 0.797 |", "caption": "Table 5: \nVideo classification accuracy with varied representations.", "description": "This table presents the results of a video classification experiment.  The goal is to assess how well different video representations can be used to predict semantic labels for 34 tasks from the CALVIN dataset. Four different video representations were used: only the initial frame; the initial frame repeated eight times; the initial frame plus seven subsequent frames; and the initial frame plus seven latent motion token chunks.  The accuracy of a video classifier trained on each of these representations is reported, demonstrating the impact of different input features on video classification performance. This experiment helps assess the effectiveness of using the Latent Motion Tokens to represent visual motions for downstream robot manipulation tasks. ", "section": "5.1 Latent Motion Token as an Interpretable Motion Language"}, {"content": "| Dataset | Top-5 | Top-10 | Top-20 |\n|---|---|---|---| \n| Oepn-X-Embodiment | 0.521 | 0.698 | 0.853 |\n| Calvin (ABC\u27f6D) | 0.298 | 0.518 | 0.768 |", "caption": "Table 6: \nTop-K motion token prediction accuracy of Moto-GPT.", "description": "This table presents the top-K accuracy results for Moto-GPT's motion token prediction task.  It shows the model's ability to accurately predict the next latent motion token in a sequence, given the preceding tokens and contextual information.  The accuracy is evaluated across different datasets, providing insights into Moto-GPT's performance on various video data. Top-K accuracy means the percentage of times the correct token is within the top K predicted tokens.", "section": "5.2. Moto-GPT as a Useful Motion Prior Learner"}, {"content": "| Method | Pick Coke Can |  |  |  | Move Near | Open / Close Drawer |  |  | Overall |\n|---|---|---|---|---|---|---|---|---|---| \n|  | Horizontal | Vertical | Standing | Average | Average | Open | Close | Average | Average |\n| RT-1-X [5] | 0.820 | 0.330 | 0.550 | 0.567 | 0.317 | 0.296 | 0.891 | 0.597 | 0.534 |\n| RT-2-X [62] | 0.740 | **0.740** | 0.880 | **0.787** | **0.779** | 0.157 | 0.343 | 0.250 | **0.607** |\n| Octo-Base [42] | 0.210 | 0.210 | 0.090 | 0.170 | 0.042 | 0.009 | 0.444 | 0.227 | 0.169 |\n| OpenVLA [28] | 0.270 | 0.030 | 0.190 | 0.163 | 0.462 | **0.194** | 0.518 | 0.356 | 0.248 |\n| Moto | **0.820** | **0.500** | **0.900** | **0.740** | **0.604** | 0.130 | 0.732 | **0.431** | **0.614** |\n| Moto w/o Motion Token | 0.600 | 0.190 | 0.740 | 0.503 | 0.554 | 0.000 | **0.796** | 0.398 | 0.480 |", "caption": "Table 7: SIMPLER evaluation results of models pre-trained on Open-X-Embodiment\u00a0[52] datasets. The \u201cOverall\u201d column reports the success rate averaged across the sub-tasks of all task types.", "description": "This table presents a comprehensive comparison of the performance of several robot manipulation models on the SIMPLER benchmark.  Each model was pre-trained using the Open-X-Embodiment dataset. The table shows success rates for three sub-tasks within the benchmark: Pick Coke Can (with horizontal, vertical, and standing variations), Move Near, and Open/Close Drawer.  The 'Overall' column provides the average success rate across all three tasks.  This allows for a direct comparison of model effectiveness in various manipulation scenarios.", "section": "4. Experiment Setup"}, {"content": "| Model | Observation Space | 1 | 2 | 3 | 4 | 5 | Avg. Len. |\n|---|---|---|---|---|---|---|---|---|\n| SuSIE [4] | Static RGB | 0.870 | 0.690 | 0.490 | 0.380 | 0.260 | 2.69 |\n| RoboFlamingo [32] | Static RGB + Gripper RGB | 0.824 | 0.619 | 0.466 | 0.331 | 0.235 | 2.47 |\n| MT-R3M [54] | Static RGB + Gripper RGB + Proprio | 0.529 | 0.234 | 0.105 | 0.043 | 0.018 | 0.93 |\n| GR-1 [54] | Static RGB + Gripper RGB + Proprio | 0.854 | 0.712 | 0.596 | 0.497 | 0.401 | 3.06 |\n| Moto | Static RGB | 0.897 | 0.729 | 0.601 | 0.484 | 0.386 | 3.10 |\n| Moto w/o Motion Token | Static RGB | 0.779 | 0.555 | 0.380 | 0.256 | 0.167 | 2.14 |", "caption": "Table 8: Comparison of models adopting different pre-training techniques on CALVIN (ABC\u27f6\u27f6\\longrightarrow\u27f6D). Avg. Len. is a comprehensive metric indicating the average number of tasks accomplished in a row across 1,000 trial sequences. \u201cStatic RGB\u201d and \u201cGripper RGB\u201d denote the RGB images from a static camera or a gripper view, respectively. \u201cProprio\u201d is short for the proprioceptive robot state.", "description": "Table 8 presents a comparison of various models' performance on the CALVIN benchmark (specifically, the ABC\u2192D setting, where training occurs on environments A, B, and C and testing occurs on environment D).  The models utilize different pre-training techniques.  The table shows the average number of tasks each model successfully completes consecutively across 1000 trial sequences (Avg. Len.).  This metric offers a comprehensive evaluation of performance in complex, multi-step tasks.  The table also specifies the type of sensory data used by each model: Static RGB (RGB images from a fixed camera), Gripper RGB (RGB images from a camera mounted on the robot gripper), and Proprio (proprioceptive robot state data, indicating the robot's internal state such as joint angles and velocities).", "section": "4. Experiment Setup"}]