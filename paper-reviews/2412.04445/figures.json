[{"figure_path": "https://arxiv.org/html/2412.04445/x2.png", "caption": "Figure 1: The overview of Moto, which utilizes Latent Motion Tokens as a bridging \u201clanguage\u201d for autoregressive pretraining on video data. The Moto-GPT pre-trained through next motion token prediction learns a wealth of motion-related prior knowledge from videos, which can be seamlessly transferred to enhance downstream robot manipulation tasks with significant performance gains.", "description": "The figure illustrates the Moto framework, which uses latent motion tokens as an intermediary representation to bridge video data and robot manipulation.  The latent motion tokens are generated from video data using a tokenizer. A model called Moto-GPT is then pre-trained using these tokens to predict the next token in a sequence, learning motion patterns and prior knowledge. This knowledge is then transferred to a robot control policy through co-fine-tuning, enabling robots to perform manipulation tasks more effectively. The figure also shows performance results, indicating significant improvements with the use of the Moto framework.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2412.04445/x3.png", "caption": "Figure 2: \nOverview of Moto\u2019s three training stages: (1) The Latent Motion Tokenizer encodes key visual motions between video frames into compact latent tokens in an unsupervised manner using pure video data. (2) Moto-GPT is pre-trained with autoregressive motion token prediction to learn motion priors from video-instruction pairs. (3) Moto-GPT is co-fine-tuned on action-labeled trajectories to predict robot actions based on the output of learnable action query tokens while maintaining the next-motion-token prediction objective.", "description": "This figure illustrates the three-stage training process of the Moto model. Stage 1 shows the unsupervised training of the Latent Motion Tokenizer, which converts visual motions between video frames into compact latent motion tokens. Stage 2 depicts the autoregressive pre-training of Moto-GPT using these tokens and video instructions to learn motion priors.  Finally, Stage 3 shows the co-fine-tuning of Moto-GPT on action-labeled trajectories to predict robot actions. This stage utilizes learnable action query tokens, incorporating the learned motion priors to achieve precise robot control, while simultaneously maintaining the next-motion-token prediction objective.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2412.04445/x4.png", "caption": "Figure 3: The Latent Motion Tokenizer produces discrete motion tokens from two consecutive video frames. It regularizes the decoder to reconstruct the second frame based on the first one and the discrete tokens, effectively capturing the motion between frames.", "description": "The Latent Motion Tokenizer is an autoencoder that takes two consecutive video frames as input. The encoder compresses these frames into a set of discrete latent motion tokens.  The decoder then uses these tokens and the first frame to reconstruct the second frame. The process regularizes the decoder to ensure it effectively captures and represents the motion that occurred between the two frames.", "section": "3.2. Latent Motion Tokenizer"}, {"figure_path": "https://arxiv.org/html/2412.04445/x5.png", "caption": "Figure 4: \nIllustration of the evaluation tasks in SIMPLER\u00a0[31].", "description": "Figure 4 illustrates the three robot manipulation tasks used to evaluate the performance of different models within the SIMPLER benchmark.  These tasks showcase the robot's ability to perform common manipulation actions.  Specifically, the tasks involve: 1) picking up a Coke can in various orientations (horizontal, vertical, and upright); 2) moving a designated object closer to another target object; and 3) opening and closing a drawer.  These tasks test the model's dexterity, planning ability, and object recognition capabilities in a controlled environment.", "section": "4.1 Benchmarks and Datasets"}, {"figure_path": "https://arxiv.org/html/2412.04445/x6.png", "caption": "Figure 5: \nIllustration of the four different environments in CALVIN, adapted from the original figure in\u00a0Mees et\u00a0al. [40].", "description": "Figure 5 shows the four distinct environments used in the CALVIN benchmark.  Each environment features a table with a sliding door, a drawer, and colored blocks, plus a light bulb controlled by a switch and an LED button.  The image highlights how these environmental elements differ in positioning, texture, and overall layout, offering differing challenges for robot manipulation tasks.", "section": "4. Experiment Setup"}, {"figure_path": "https://arxiv.org/html/2412.04445/x7.png", "caption": "Figure 6: \nQualitative examples of reconstruction results, where discrete motion tokens obtained from the Latent Motion Tokenizer based on the initial and next frame, are fed into the decoder along with the initial frame to reconstruct the target frame.", "description": "This figure shows the reconstruction quality of the Latent Motion Tokenizer.  The tokenizer takes two consecutive frames from a video as input.  It encodes the motion between these frames into a discrete latent motion token. This token, along with the first frame, is fed into the decoder to reconstruct the second frame.  The images in Figure 6 show examples of the original second frame (ground truth), and the frame reconstructed by the decoder, demonstrating the accuracy of the reconstruction.", "section": "3.2. Latent Motion Tokenizer"}, {"figure_path": "https://arxiv.org/html/2412.04445/x8.png", "caption": "Figure 7: \nVisualization of latent motion token interpretability. Each row displays reconstructed frames from the same initial frame using different latent motion tokens, while each column shows frames reconstructed from the same latent motion tokens with varying initial frames. The latent motion tokens exhibit consistent (see columns) and discriminative (see rows) semantics, despite being trained in an unsupervised manner.", "description": "This figure visualizes the interpretability of latent motion tokens.  Each row uses a different set of tokens to reconstruct frames from the same initial image, demonstrating the discriminative nature of the tokens (each row shows a distinct motion). Each column uses the same set of tokens on different initial images, showing consistent results despite the variability in starting points. This consistency and discriminative power highlight the effectiveness of the unsupervised training method in capturing meaningful motion semantics.", "section": "5.1 Latent Motion Token as an Interpretable Motion Language"}, {"figure_path": "https://arxiv.org/html/2412.04445/x9.png", "caption": "Figure 8: \nVideo imitation generation via latent motion tokens, where a sequence of latent motion tokens from a demonstration video are extracted by the Latent Motion Tokenizer and are decoded into a new video. This generated video is based on a different initial frame while preserving the original robot movement semantics.", "description": "This figure visualizes the process of video imitation using latent motion tokens.  A sequence of latent motion tokens is extracted from a demonstration video using the Latent Motion Tokenizer. These tokens are then used to generate a new video sequence with a different initial frame. Importantly, the generated video retains the original robot movements' semantic meaning from the demonstration video, proving that the latent motion tokens effectively capture and transfer the essence of the robot's actions.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2412.04445/x10.png", "caption": "Figure 9: \nVisualization of video trajectories generated from a sequence of latent motion tokens, which are predicted by the pre-trained Moto-GPT given different language instructions.", "description": "This figure visualizes the ability of the pre-trained Moto-GPT model to generate video trajectories from language instructions.  It shows several example videos, each starting with the same initial frame but progressing differently based on different text prompts given to the model. The model uses sequences of latent motion tokens, which capture the essence of motion in a compact form, to predict the future frames of each video.  The variety in the generated videos demonstrates the model's capacity to understand and respond to natural language instructions and its ability to generate diverse and plausible motion sequences.", "section": "5.2. Moto-GPT as a Useful Motion Prior Learner"}, {"figure_path": "https://arxiv.org/html/2412.04445/x11.png", "caption": "Figure 10: \nMoto-GPT distinguishes successful, failed, and random robot trajectories using log-likelihoods, enabling effective assessment of trajectory rationality and potential reward signals.", "description": "This figure shows how Moto-GPT, a model trained to predict motion trajectories using latent motion tokens, can assess the quality of different robot trajectories. By calculating the log-likelihoods of successful, failed, and randomly generated trajectories, Moto-GPT effectively distinguishes between them. This capability demonstrates Moto-GPT's ability to evaluate the rationality of a trajectory, making it potentially useful for generating reward signals in reinforcement learning algorithms.", "section": "5.2. Moto-GPT as a Useful Motion Prior Learner"}, {"figure_path": "https://arxiv.org/html/2412.04445/x12.png", "caption": "Figure 11: Task success rate of models fine-tuned with different proportions of data on CALVIN (ABC\u27f6\u27f6\\longrightarrow\u27f6D).", "description": "This figure shows the success rate achieved by different models on the CALVIN benchmark's D environment.  The models were fine-tuned using varying amounts of labeled data from CALVIN's A, B, and C environments. The x-axis represents the proportion of training data used, ranging from 1% to 100%. The y-axis shows the success rate.  The graph highlights the performance difference between Moto-GPT (the proposed method) and a model trained from scratch without leveraging motion priors.  It demonstrates Moto-GPT's superior data efficiency, achieving a high success rate even with limited training data.", "section": "5.3 Moto-GPT as an Effective Robot Policy"}]