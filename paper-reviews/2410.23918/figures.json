[{"figure_path": "https://arxiv.org/html/2410.23918/x1.png", "caption": "(a)", "description": "This figure demonstrates BitStack's ability to dynamically adjust the model size in response to varying memory availability.  The left panel (a) shows a schematic illustration of how BitStack operates at different memory levels, adjusting its size at a megabyte-level granularity. This allows it to handle different memory constraints on various devices without sacrificing model performance.  The actual caption only states '(a)', without further explanation.", "section": "2 BITSTACK"}, {"figure_path": "https://arxiv.org/html/2410.23918/x2.png", "caption": "(b)", "description": "This figure shows the zero-shot performance of different LLMs at various memory footprints.  BitStack consistently matches or surpasses the performance of GPTQ and AWQ, especially at extreme compression ratios. The x-axis represents memory usage in MB, and the y-axis represents the average zero-shot performance across six different tasks.  The various lines represent different LLMs and compression techniques.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2410.23918/x3.png", "caption": "Figure 1: BitStack enables LLMs to dynamically adjust their size in variable memory environments (1(a)) at a megabyte-level, while still matching or surpassing the performance of practical compression methods such as GPTQ\u00a0(Frantar et\u00a0al., 2022) and AWQ\u00a0(Lin et\u00a0al., 2024) with the same memory footprint(1(b)).", "description": "Figure 1 demonstrates BitStack's ability to dynamically adjust the size of large language models (LLMs) in environments with varying memory constraints.  The left panel (a) shows how BitStack enables fine-grained size control at the megabyte level. The right panel (b) illustrates BitStack's performance, showing that it achieves comparable or superior results to existing state-of-the-art compression methods such as GPTQ and AWQ, even when operating within the same limited memory footprint.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2410.23918/x4.png", "caption": "(a)", "description": "This figure demonstrates BitStack's ability to dynamically adjust the size of LLMs in environments with varying memory availability.  Panel (a) illustrates how BitStack can adapt to low and high memory scenarios by loading a different number of residual blocks (representing different levels of model compression) at a megabyte-level granularity.  This allows the model to seamlessly trade off memory usage and performance as needed.", "section": "2 BITSTACK"}, {"figure_path": "https://arxiv.org/html/2410.23918/x5.png", "caption": "(b)", "description": "The figure shows the zero-shot performance of various LLMs compressed using different methods, including BitStack, GPTQ, and AWQ, across a range of memory footprints.  The x-axis represents the memory in MB, and the y-axis represents the average zero-shot performance across six tasks. Different colored lines correspond to different compression methods. The plot highlights the performance of BitStack at various memory levels, demonstrating its ability to match or surpass the performance of other compression techniques at the same memory footprint. The results indicate BitStack's effectiveness in dynamically adjusting model size and maintaining comparable performance in variable memory environments.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2410.23918/x6.png", "caption": "Figure 2: Overview of BitStack. BitStack dynamically loads and offloads weight residual blocks (Figure\u00a03) between RAM and storage devices based on current memory availability. We can load more weight residuals from storage when available memory increases (2(a)), or offload them otherwise (2(b)). The residual blocks for all weights across all layers are universally stored in the same stack on the storage device (grey blocks denote residual blocks for weights in other layers). Note that we omit positional embeddings, normalization layers, and residual connections in the figure for clarity.", "description": "Figure 2 illustrates BitStack's dynamic memory management.  BitStack uses weight decomposition to create smaller, residual blocks of model weights that can be stored separately on a storage device.  When more RAM is available, BitStack loads additional residual blocks from storage to increase model size and performance. Conversely, if available memory decreases, BitStack offloads blocks back to storage.  All residual blocks from all layers are stored in a single stack on the storage device, allowing for efficient management. For clarity, the figure omits positional embeddings, normalization, and residual connections.", "section": "BITSTACK"}, {"figure_path": "https://arxiv.org/html/2410.23918/x7.png", "caption": "Figure 3: Illustration of a residual block in BitStack. A residual block consists of a sign matrix and singular vectors obtained through absolute value decomposition. The sign matrix can be packed into GPU-supported data types to minimize memory usage.  denotes the sign matrix while  denotes the packed sign matrix.", "description": "Figure 3 illustrates a residual block, the fundamental unit of data in BitStack's compression method.  Each block is generated through the absolute value decomposition (AVD) of a weight matrix.  This process yields two components: a sign matrix containing only +1 or -1 values, and the singular vectors from the singular value decomposition (SVD). The sign matrix is particularly efficient, as its binary nature allows for compact storage using GPU-optimized data types, which reduces memory usage. The figure visually represents these components, showing the original sign matrix and its compressed packed version for storage. The packed sign matrix (denoted by a different symbol) takes up much less memory space than the original sign matrix.", "section": "2 BITSTACK"}, {"figure_path": "https://arxiv.org/html/2410.23918/x8.png", "caption": "(a) Performance with various sizes.", "description": "This figure demonstrates the performance of BitStack on instruction-tuned Llama 3.1 models of different sizes (8B and 70B) across various tasks in the MT-Bench benchmark.  The x-axis represents different memory footprints achieved by loading varying numbers of residual blocks, while the y-axis represents the performance scores. The figure illustrates BitStack's capability to dynamically adjust the model's size based on available memory, showcasing its effectiveness across various scales and tasks.", "section": "3.2 Evaluation on Instruction-Tuned Models"}, {"figure_path": "https://arxiv.org/html/2410.23918/x9.png", "caption": "(b) Pair-wise comparison with AWQ.", "description": "This figure presents a pairwise comparison of BitStack's performance against AWQ (Activation-Aware Weight Quantization) across various model sizes (8B and 70B parameters) and different compression ratios.  The chart likely displays performance metrics, possibly perplexity scores or accuracy on benchmark tasks, to illustrate how BitStack's performance compares to AWQ under different memory constraints.", "section": "3.2 Evaluation on Instruction-Tuned Models"}, {"figure_path": "https://arxiv.org/html/2410.23918/x10.png", "caption": "Figure 4: Evaluation results of BitStack Llama 3.1 Instruct 8B/70B models on MT-Bench, assessed by gpt-4o. (4(a)) demonstrates the single-answer grading results across various sizes of the 8B model loaded by BitStack, while (4(b)) illustrates the pairwise comparison results against AWQ at different compression ratios for both the 8B and 70B models.", "description": "Figure 4 presents a comprehensive evaluation of BitStack's performance on instruction-tuned LLMs.  Specifically, it uses the MT-Bench benchmark, which assesses performance across various tasks like writing, role-playing, reasoning, and coding.  Part (a) shows how the performance of the 8B BitStack model improves as more memory is allocated to it; this demonstrates the fine-grained control BitStack offers. Part (b) provides a direct pairwise comparison of BitStack against AWQ (another compression method) across various compression ratios, for both the 8B and 70B models, highlighting the competitive performance of BitStack.", "section": "3.2 Evaluation on Instruction-Tuned Models"}, {"figure_path": "https://arxiv.org/html/2410.23918/x11.png", "caption": "Figure 5: Perplexity and average zero-shot performance of BitStack Llama 3.1 8B with or without activation-aware scaling and absolute value decomposition(AVD). In the \u201dw/o scaling\u201d experiments, no scaling is applied as in Eq.\u00a04; in the \u201dw/o AVD\u201d experiments, vanilla SVD is used instead of AVD as in Eq.\u00a05. For vanilla SVD, we set k\u2032=k+m\u00d7n16\u00d7(m+n)superscript\ud835\udc58\u2032\ud835\udc58\ud835\udc5a\ud835\udc5b16\ud835\udc5a\ud835\udc5bk^{\\prime}=k+\\frac{m\\times n}{16\\times(m+n)}italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = italic_k + divide start_ARG italic_m \u00d7 italic_n end_ARG start_ARG 16 \u00d7 ( italic_m + italic_n ) end_ARG(for \ud835\udc7e\u2208\u211dm\u00d7n\ud835\udc7esuperscript\u211d\ud835\udc5a\ud835\udc5b{\\bm{W}}\\in\\mathbb{R}^{m\\times n}bold_italic_W \u2208 blackboard_R start_POSTSUPERSCRIPT italic_m \u00d7 italic_n end_POSTSUPERSCRIPT) to ensure the size of each residual block matches that of the main experiments. Solid lines represent average zero-shot performance, while dotted lines represent perplexity scores.", "description": "Figure 5 presents a comprehensive ablation study on the BitStack model (Llama 3.1 8B). It analyzes the impact of two key components: activation-aware scaling and absolute value decomposition (AVD).  The experiments systematically remove one or both of these components, comparing their performance to the full BitStack model.  When scaling is omitted,  the performance significantly degrades.  Similarly, replacing AVD with standard SVD (while adjusting the number of singular values to maintain a comparable residual block size) also causes significant performance drops.  This highlights the crucial role of both scaling and AVD in BitStack's efficiency and accuracy, especially at high compression ratios. The results are shown via perplexity and average zero-shot performance across a range of memory footprints.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2410.23918/x12.png", "caption": "Figure 6: Perplexity and average zero-shot performance of BitStack Llama 3.1 8B with 3 different sorting approaches for residual blocks. Solid lines represent average zero-shot performance, while dotted lines represent perplexity scores.", "description": "Figure 6 compares the performance of three different sorting algorithms for residual blocks in the BitStack model, specifically using Llama 3.1 8B.  The algorithms are Average, Greedy, and Random.  The graph displays both perplexity (dotted lines) and average zero-shot performance (solid lines) across a range of memory footprints. This shows how the choice of sorting algorithm affects the tradeoff between model size and performance.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2410.23918/x13.png", "caption": "(a)", "description": "This figure demonstrates BitStack's ability to dynamically adjust its size in environments with varying memory availability.  The left panel (a) shows how BitStack adapts its size at a megabyte-level granularity, illustrating the flexibility offered by the approach.  Different llama models are shown to have different memory footprints (MB) given their size.  The right panel (b) complements this, showing that BitStack maintains or exceeds the performance of other methods (GPTQ, AWQ) at the same memory footprint.", "section": "2 BITSTACK"}, {"figure_path": "https://arxiv.org/html/2410.23918/x14.png", "caption": "(b)", "description": "The figure shows the zero-shot performance of various compressed language models across different memory footprints.  BitStack consistently matches or surpasses the performance of GPTQ and AWQ, particularly at extreme compression ratios (low memory). This demonstrates BitStack's effectiveness in dynamically adjusting model size for optimal performance within variable memory environments.  Different colors represent different compression methods.", "section": "Experiments"}]