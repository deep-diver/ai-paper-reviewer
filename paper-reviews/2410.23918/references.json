{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "publication_date": "2023-03-08", "reason": "It provides technical details about GPT-4, a significant LLM that is frequently referenced in the paper for comparison and context."}, {"fullname_first_author": "Elias Frantar", "paper_title": "GPTQ: Accurate post-training quantization for generative pre-trained transformers", "publication_date": "2022-10-17", "reason": "This paper introduces GPTQ, a state-of-the-art post-training quantization method for LLMs, which is used as a key baseline in this paper's experiments."}, {"fullname_first_author": "Ji Lin", "paper_title": "Awq: Activation-aware weight quantization for llm compression and acceleration", "publication_date": "2024-06-01", "reason": "This paper introduces AWQ, another strong baseline for comparison, and the paper analyzes how its performance compares to the proposed method."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "publication_date": "2024-07-21", "reason": "This paper introduces Llama 3, a set of large language models which are used as experimental models in this paper's benchmarks, providing important comparative data."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "This paper introduces Llama 2, another significant model used for experimental evaluation and comparison in this paper's benchmarks."}]}