[{"figure_path": "2410.15580/charts/charts_5_0.png", "caption": "Figure 2: Partial products identification accuracy before and after fine-tuning on tasks. Scores are reported on average of Gemma-2-2B and Llama-3.1-8B.", "description": "The bar chart displays the accuracy of partial product identification in four different multiplication calculation methods (Standard, Repetitive addition, Lattice, and Egyptian) before and after fine-tuning.  The chart shows a grouped bar chart for each method comparing the performance before fine-tuning (Before FT) and after fine-tuning (After FT) on a scale from 0 to 1.  The accuracy for Standard, Lattice, and Egyptian methods improved significantly after fine-tuning, while the accuracy for Repetitive Addition remained very low even after fine-tuning.  The chart also includes a final column labeled 'Acc' which is unclear without further context from the paper.", "section": "5 Are Large Language Models Implicit Calculators?"}, {"figure_path": "2410.15580/charts/charts_8_0.png", "caption": "Figure 3: Position-level Accuracy from Gemma-2-2B and Llama-3.1-8B.", "description": "The chart displays the positional accuracy of LLMs (Gemma-2-2B) in 3-digit multiplication tasks across different training set sizes (6.48K, 12.96K, 32.4K, and 64.8K).  The x-axis represents the position of the digit in the output (from left to right), and the y-axis shows the accuracy percentage.  For each training set size, a separate line shows the accuracy at each position. The chart demonstrates a U-shaped curve, with accuracy peaking at the beginning and end positions, and significantly lower accuracy in the middle position, suggesting that LLMs struggle most with the middle digits.", "section": "6.4 Subgroup Selection: Revealing Learning Dynamic in Arithmetic Learning"}, {"figure_path": "2410.15580/charts/charts_8_1.png", "caption": "Figure 3: Position-level Accuracy from Gemma-2-2B and Llama-3.1-8B.", "description": "The chart displays the position-level accuracy of two large language models, Gemma-2-2B and Llama-3.1-8B, across different training set sizes (6.48K, 12.96K, 32.4K, and 64.8K) on 3-digit, 4-digit, and 5-digit multiplication tasks. The x-axis represents the position of the output digit, while the y-axis shows the accuracy percentage.  Each line corresponds to a specific training dataset size. Notably, the accuracy follows a U-shaped pattern across the different multiplication tasks, with highest accuracy at the beginning and end digits, and significantly lower accuracy in the middle digits.", "section": "6.4 Subgroup Selection: Revealing Learning Dynamic in Arithmetic Learning"}, {"figure_path": "2410.15580/charts/charts_8_2.png", "caption": "Figure 3: Position-level Accuracy from Gemma-2-2B and Llama-3.1-8B.", "description": "The chart displays the position-level accuracy for 3, 4, and 5-digit multiplication tasks across different training set sizes (6.48K, 12.96K, 32.4K, and 64.8K) using two language models, Gemma-2-2B and Llama-3.1-8B.  The x-axis represents the position of the output digit, ranging from the least significant digit (rightmost) to the most significant digit (leftmost). The y-axis represents the accuracy in percentage. For both models, the accuracy shows a U-shaped curve, with high accuracy at the beginning and end positions and significantly lower accuracy in the middle positions, irrespective of the training set size. This indicates that LLMs struggle more with the intermediate digits compared to the initial and final ones, which contrasts with typical assumptions about difficulty in arithmetic learning.", "section": "6.3 Subgroup Complexity: Label Space Matters in the Final Stage"}, {"figure_path": "2410.15580/charts/charts_8_3.png", "caption": "Figure 3: Position-level Accuracy from Gemma-2-2B and Llama-3.1-8B.", "description": "The chart displays the position-level accuracy of LLMs (Gemma-2-2B and Llama-3.1-8B) across different multiplication tasks (3-digit, 4-digit, and 5-digit) and training set sizes (6.48K, 12.96K, 32.4K, and 64.8K).  Each line represents a specific training set size, showing accuracy across the positions of the output digits. The x-axis represents the position of the output digits, and the y-axis shows accuracy. Notably, the accuracy shows a U-shaped pattern for all training sizes across various tasks, with higher accuracy at the beginning and end positions and lower accuracy in the middle positions. This is consistent across different tasks and training sizes.", "section": "6.3 Subgroup Complexity: Label Space Matters in the Final Stage"}, {"figure_path": "2410.15580/charts/charts_8_4.png", "caption": "Figure 3: Position-level Accuracy from Gemma-2-2B and Llama-3.1-8B.", "description": "This chart displays the position-level accuracy of two large language models, Gemma-2-2B and Llama-3.1-8B, across different training set sizes (6.48K, 12.96K, 32.4K, and 64.8K) for 3-digit, 4-digit, and 5-digit multiplication tasks. The x-axis represents the position of the digit in the output, while the y-axis shows the accuracy percentage.  The accuracy curves exhibit a U-shaped pattern; accuracy is highest at the beginning and end positions (over 95%), significantly lower in the middle positions (around 10%), especially for the larger multiplication tasks.  The trend indicates that the models struggle more with digits requiring intermediate computations, revealing the models' learning pattern follows an easy-to-hard paradigm in arithmetic learning. ", "section": "6.4 Subgroup Selection: Revealing Learning Dynamic in Arithmetic Learning"}, {"figure_path": "2410.15580/charts/charts_8_5.png", "caption": "Position-level Accuracy from Gemma-2-2B and Llama-3.1-8B", "description": "This chart displays the position-level accuracy of two large language models, Gemma-2-2B and Llama-3.1-8B, across various training set sizes (6.48K, 12.96K, 32.4K, and 64.8K) when performing 5-digit multiplication. The x-axis represents the position of the digit in the output, and the y-axis represents the accuracy in percentage.  The lines in the chart represent different training set sizes.  The results reveal a U-shaped curve for both models, indicating that the accuracy is high at the beginning and end positions but significantly lower in the middle positions, regardless of training data size.", "section": "6.3 Subgroup Complexity: Label Space Matters in the Final Stage"}]