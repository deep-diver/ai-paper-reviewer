[{"figure_path": "2410.15580/tables/table_5_0.html", "caption": "Table 1: Inductive and deductive accuracy difference \u0394.", "description": "This table presents the changes in accuracy on multiplication tasks before and after fine-tuning LLMs on diagnostic sets for four different multiplication calculation methods.", "section": "Examining Partial Product in Arithmetic Learning"}, {"figure_path": "2410.15580/tables/table_5_1.html", "caption": "Table 2: Diagnostic sets with four calculation methods.", "description": "The table presents diagnostic sets for four multiplication calculation methods (standard multiplication, repetitive addition, lattice method, and Egyptian multiplication) used to investigate whether LLMs leverage partial products during arithmetic learning.", "section": "5.2 Examining Partial Product in Arithmetic Learning"}, {"figure_path": "2410.15580/tables/table_7_0.html", "caption": "Table 3: Label space statistics with different rule perturbations. H(L) represents the entropy of the label space, and |L| is the size of the label space. {C}i=1 represents all positions in output digits.", "description": "Table 3 shows the label space entropy and cardinality for various addition and multiplication tasks with different rule perturbations.", "section": "6.3 Subgroup Complexity: Label Space Matters in the Final Stage"}, {"figure_path": "2410.15580/tables/table_7_1.html", "caption": "Table 4: Test Accuracy difference \u0394 on perturbed addition and multiplication.", "description": "The table shows the accuracy difference (\u0394) in percentage for addition and multiplication tasks on Gemma-2-2B and Llama-3.1-8B models with various rule perturbations.", "section": "6.3 Subgroup Complexity: Label Space Matters in the Final Stage"}, {"figure_path": "2410.15580/tables/table_12_1.html", "caption": "Table 6: Test Accuracy difference \u0394 on perturbed addition and multiplication.", "description": "This table presents the accuracy difference (\u0394) in percentage points for addition and multiplication tasks, comparing the performance of Gemma-2-2B and Llama-3.1-8B models under three different input format perturbations (Natural Language, Random String, and Disturbed Digits).", "section": "5.2 Examining Partial Product in Arithmetic Learning"}, {"figure_path": "2410.15580/tables/table_13_0.html", "caption": "Table 1: Inductive and deductive accuracy difference \u0394.", "description": "The table presents the inductive and deductive accuracy differences (\u0394) for four multiplication calculation methods (Standard, Lattice, Repetitive, Egyptian) across two LLMs (Gemma-2-2B and Llama-3.1-8B), showing the impact of fine-tuning on partial products.", "section": "Examining Partial Product in Arithmetic Learning"}]