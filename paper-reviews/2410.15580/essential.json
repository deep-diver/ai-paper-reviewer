{"reason": "This paper investigates how large language models (LLMs) learn arithmetic.  The authors find that LLMs don't use partial products, but instead learn arithmetically in a purely symbolic way by breaking tasks into subgroups, suggesting that LLMs are symbol-level learners. This challenges previous assumptions about how LLMs perform arithmetic and offers insights into their learning dynamics.", "takeaways": ["LLMs do not leverage partial products in arithmetic calculations.", "LLMs learn arithmetic symbolically by selecting subgroups based on complexity, following an easy-to-hard paradigm.", "The difficulty of arithmetic tasks is influenced by the label space entropy and subgroup quality."], "tldr": "Large language models (LLMs) surprisingly don't utilize partial products for arithmetic, instead operating as symbolic learners. They solve arithmetic problems by decomposing them into manageable subgroups, selecting easier patterns first.  The difficulty of these sub-tasks is linked to their complexity and label space entropy."}