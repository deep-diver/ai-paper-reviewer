{"references": [{" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the math dataset", "reason": "This paper introduces the MATH dataset, a benchmark for evaluating mathematical problem-solving abilities in large language models (LLMs).  It is highly relevant because it establishes a crucial context for the current research by highlighting the challenges LLMs face in mathematical reasoning and the need for more rigorous benchmarks. The MATH dataset's influence on the field makes it a foundational reference for this research.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "reason": "This paper introduces the GSM8K dataset, another significant benchmark for evaluating LLMs' mathematical reasoning capabilities. Its relevance stems from the fact that, like MATH, GSM8K provides a standard against which the progress of LLMs in solving mathematical problems can be measured and compared.  This makes it important for establishing the context of the current research.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Nayoung Lee", "paper_title": "Teaching arithmetic to small transformers", "reason": "This work is highly relevant because it directly addresses the challenge of teaching arithmetic to LLMs. The authors' approach of carefully curating data format and using a method termed 'Scratchpad' provides valuable insights into techniques for improving arithmetic capabilities in LLMs, which is directly relevant to the current research's investigation into LLM arithmetic learning mechanisms.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yuntian Deng", "paper_title": "Implicit chain of thought reasoning via knowledge distillation", "reason": "This paper explores the use of chain-of-thought (CoT) prompting in arithmetic learning. While the current study doesn't explicitly use CoT, this reference is important because it highlights a significant technique that has improved LLM performance in mathematical reasoning. Understanding the CoT approach helps contextualize the current research which focuses on a different (symbolic learning) approach to explain LLM arithmetic capabilities. ", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Alessandro Stolfo", "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis", "reason": "This study uses causal mediation analysis to identify components within LLMs responsible for arithmetic learning.  This is directly relevant as the current research addresses a gap in such component-level analyses by exploring the underlying mechanisms of arithmetic learning from a different perspective (symbolic learning) providing a crucial comparison point.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Sasha Boguraev", "paper_title": "Models can and should embrace the communicative nature of human-generated math", "reason": "This paper challenges the common assumption that LLMs struggle with arithmetic due to inherent differences between language and computation. This is highly relevant as it directly addresses the core research question of the current study. By proposing that LLMs might benefit from a more communicative approach to mathematics, it offers an alternative perspective that the current research indirectly investigates.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Yuntian Deng", "paper_title": "From explicit cot to implicit cot: Learning to internalize cot step by step", "reason": "This paper delves into the concept of chain-of-thought (CoT) prompting and its internalization within LLMs, a highly relevant topic given the CoT's success in improving LLM performance on arithmetic tasks. Although the current study doesn't explicitly use CoT, understanding its mechanisms provides a relevant contrast to the current research, which investigates arithmetic learning through a purely symbolic lens.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Zhen Yang", "paper_title": "Gpt can solve mathematical problems without a calculator", "reason": "This paper is highly relevant because it explores the capabilities of LLMs in solving mathematical problems, particularly highlighting the impressive performance of GPT models in 5-digit multiplication. This result directly motivates the current research, which seeks to understand why some arithmetic problems remain challenging for LLMs despite such achievements.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Aitor Lewkowycz", "paper_title": "Solving quantitative reasoning problems with language models", "reason": "This work is highly relevant due to its direct focus on improving LLMs' ability to solve quantitative reasoning problems.  The techniques employed, such as continued pretraining, are relevant to understanding how LLMs can be enhanced for improved mathematical abilities, providing a useful contrast to the symbolic learning approach explored in the current research. ", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces the Llama 3 family of models, one of which (Llama-3.1-8B) is used in the current research.  The description of the model architecture and training methodologies provides essential context for understanding the capabilities and limitations of the LLM used in the experiments, directly impacting the interpretation of the results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Gemma Team", "paper_title": "Gemma 2: Improving open language models at a practical size", "reason": "This paper introduces the Gemma-2-2B model, the other LLM utilized in this research.  Describing its architecture and training methods is essential for interpreting the results and understanding the model's strengths and limitations in the context of the study's findings.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "OpenAI", "paper_title": "Gpt-4 technical report", "reason": "While not directly used, this paper describes GPT-4, a state-of-the-art LLM.  Its relevance comes from the introduction where it's mentioned as a model that, despite its capabilities, still faces challenges with basic arithmetic.  This underscores the gap between current LLM capabilities and human-level arithmetic abilities, forming a key motivation for the current research.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Andrej Karpathy", "paper_title": "Andrej karpathy's lightweight implementation of medium-sized gpts", "reason": "This paper describes NanoGPT, a model used in related work.  It is relevant because it demonstrates a specific attempt at teaching arithmetic to LLMs. This reference is crucial for comparing and contrasting methodologies applied to improving LLM mathematical reasoning capabilities, contextualizing the current study's approach.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Michael Hanna", "paper_title": "How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model", "reason": "This paper investigates the mathematical abilities of a pre-trained language model.  It is highly relevant to the current research as it explores the mechanisms behind LLMs' mathematical capabilities, offering a comparative analysis of the underlying processes.  This perspective helps to contextualize the current work's focus on symbolic learning mechanisms.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Siyuan Guo", "paper_title": "Learning beyond pattern matching? Assaying mathematical understanding in llms", "reason": "This work explores the limitations of LLMs in mathematical understanding.  It is directly related to the central research question of the current study, which investigates the underlying mechanisms of LLMs' arithmetic capabilities. Comparing and contrasting the approaches and findings helps to enrich the current study's analysis and conclusions.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yilun Zhao", "paper_title": "Docmath-eval: Evaluating math reasoning capabilities of llms in understanding long and specialized documents", "reason": "This paper evaluates the math reasoning capabilities of LLMs using a new benchmark called DocMath-Eval. This is directly relevant as it provides a broader perspective on the challenges LLMs face in mathematical reasoning and problem solving, extending beyond the basic arithmetic tasks examined in the current research.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Woosuk Kwon", "paper_title": "Efficient memory management for large language model serving with pagedattention", "reason": "This paper is relevant because it discusses efficient memory management techniques for large language models.  This is indirectly related to the current research as efficient memory management can impact the computational resources available for training LLMs on arithmetic tasks. Efficient training directly affects the performance of LLMs which in turn directly influences this study's results and their interpretation.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "reason": "This paper explores the use of chain-of-thought (CoT) prompting to improve LLMs' reasoning abilities. While not directly used in the current study, it's highly relevant as CoT is a significant technique that has advanced LLM mathematical reasoning.  Understanding this approach helps contextualize the different methodologies for improving LLM arithmetic capabilities explored in the current work.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yiran Wu", "paper_title": "Mathchat: Converse to tackle challenging math problems with llm agents", "reason": "This paper proposes a conversational approach for solving mathematical problems using LLMs.  This methodology contrasts with the focus of the current research, which emphasizes a symbolic, subgroup-level analysis. This contrasting approach helps better define the current research's methodology and its contribution to the field of LLM arithmetic learning.", "section_number": 2}]}