{"references": [{" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the math dataset", "reason": "This paper introduced a benchmark dataset, MATH, for evaluating mathematical problem-solving abilities in large language models (LLMs).  It is highly relevant because it's frequently cited in the introduction to establish the context of LLMs' struggle with arithmetic tasks, and it defines a key problem space explored throughout the current paper.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "reason": "Similar to Hendrycks et al. (2021), this paper presented another benchmark dataset, GSM8K, commonly used to evaluate LLMs' math abilities. The introduction highlights this benchmark's role in pushing the boundaries of LLM capabilities, leading to the exploration of more challenging problems where the limitations of LLMs become more apparent.  This makes it a crucial reference for contextualizing the paper's motivation.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "OpenAI", "paper_title": "GPT-4 technical report", "reason": "This paper introduced GPT-4, a state-of-the-art LLM, and demonstrated its capabilities in various tasks, including arithmetic.  This is relevant because the paper's introduction uses GPT-4 to illustrate the limitations of current LLMs.  Citing this report contextualizes the advancements and limitations of LLMs, thereby clarifying the focus of the paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yang", "paper_title": "GPT can solve mathematical problems without a calculator", "reason": "This paper directly addresses LLMs' struggle with arithmetic, focusing on the challenge of 5-digit multiplication.  It is centrally important in the introduction because it highlights the specific arithmetic task the paper aims to investigate and understand more deeply.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Sasha Boguraev", "paper_title": "Models can and should embrace the communicative nature of human-generated math", "reason": "The introduction mentions this paper to highlight the hypothesis about the fundamental differences between mathematical calculations and language modeling. This is a key motivation for the current paper, making it a critical citation.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Stolfo", "paper_title": "Identifying key attention layers responsible for arithmetic learning using causal mediation analysis", "reason": "This work is cited because the introduction mentions prior research that uses causal attribution to identify the model components involved in arithmetic, such as key attention layers or heads. This paper is an example of such research, thus providing further context for the paper's approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zhang", "paper_title": "Interpreting and improving large language models in arithmetic calculation", "reason": "This paper's citation underscores the importance of examining the internal mechanisms of LLMs during arithmetic learning. The current paper seeks to address the same problem by providing alternative perspectives and methodology.  This makes this reference highly relevant.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Lee", "paper_title": "Teaching arithmetic to small transformers", "reason": "This paper, focusing on arithmetic learning in transformers, is relevant to the current work because it explores a similar topic but with a different approach. The authors use the reference to emphasize that various approaches exist in this field, indicating that deeper explorations are needed, as offered in the current paper.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yang", "paper_title": "GPT can solve mathematical problems without a calculator", "reason": "This study is relevant to the current research because it directly addresses the capacity of LLMs in solving arithmetic problems and demonstrates significant improvements with appropriate training data and techniques.  It thus establishes a baseline and a potential research direction which is then extended in the current paper.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Deng", "paper_title": "Implicit chain of thought reasoning via knowledge distillation", "reason": "This paper also focuses on arithmetic learning in LLMs, showing how to improve performance using knowledge distillation. This reference shows similar explorations in the field, highlighting the need for deeper investigation on how LLMs truly operate during arithmetic tasks.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Deng", "paper_title": "From explicit cot to implicit cot: Learning to internalize cot step by step", "reason": "This paper is relevant as it focuses on chain-of-thought (CoT) reasoning in LLMs. It is mentioned in Section 2 to show prior efforts on improving arithmetic reasoning in LLMs by explicitly expanding calculation steps.  The current study, in contrast, explores the symbolic nature of arithmetic learning, and this reference highlights the different approaches in this area.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Stolfo", "paper_title": "Identifying key attention layers responsible for arithmetic learning using causal mediation analysis", "reason": "This paper is relevant to Section 3 as it directly addresses how causal mediation analysis can be used to understand arithmetic learning in LLMs.  This paper\u2019s method is contrasted against the approach used in the current research which is to explore the symbolic nature of arithmetic learning using a different perspective.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Hanna", "paper_title": "How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model", "reason": "This paper, closely aligned to the theme of interpretability, explores causal abstraction in LLMs focusing on mathematical operations.  This is closely related to the current study\u2019s investigation of the underlying mechanisms and challenges LLMs face in arithmetic, making it an important reference for Section 3.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Zhang", "paper_title": "Interpreting and improving large language models in arithmetic calculation", "reason": "This paper offers insights into improving LLMs' arithmetic capabilities, thus offering a contrasting approach to the main study in Section 3.  The current work, in contrast, focuses on understanding how LLMs intrinsically handle arithmetic calculations from a different viewpoint,  making this reference valuable.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces the Llama-3.1-8B model used in the current research. It is crucial to Section 4 as it provides essential context for the experimental setup; the choice of models is a critical element of the study.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Team", "paper_title": "Gemma 2: Improving open language models at a practical size", "reason": "This paper describes the Gemma-2-2B model, one of the two models employed in the experiments.  Similar to Dubey et al. (2024), it's crucial to Section 4 because it provides the necessary context for the choice of models and the experimental setup used throughout the paper.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Lee", "paper_title": "Teaching arithmetic to small transformers", "reason": "This paper is cited in section 5 because it directly addresses the topic of arithmetic learning in LLMs, specifically highlighting a method termed \"Scratchpad.\" The current paper investigates the question of LLMs' arithmetic capacity from a contrasting perspective, focusing on the symbolic interpretation and addressing limitations in prior work on this topic. ", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Yang", "paper_title": "GPT can solve mathematical problems without a calculator", "reason": "This paper, like Lee et al. (2023), is closely related to the topic of arithmetic learning in LLMs, and is cited in Section 5.  The current paper directly builds upon findings from Yang et al. (2023) by offering a contrasting perspective on how LLMs learn and perform arithmetic tasks; it examines alternative methods for solving arithmetic problems.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Zhang-Li", "paper_title": "Reverse that number! decoding order matters in arithmetic learning", "reason": "This study addresses the common assumption of positional dependence in arithmetic, which is directly relevant to Section 6.  The current paper investigates digit-level accuracy in models, specifically questioning the assumption that accuracy decreases from right to left due to carryover effects.  This makes it an important reference to support the current research findings and the development of a different hypothesis.", "section_number": 6}]}