[{"content": "| Methods | Accuracy (%) |\n|---|---| \n| Random Guess | 25.00 |\n| **Open-source LLMs** |  |\n| Gemma 2-27B [^(Gemma Team, 2024)] | 3.24 |\n| Falcon-40B [^(Almazrouei et al., 2023)] | 4.39 |\n| OLMo-7B [^(Groeneveld et al., 2024)] | 19.00 |\n| Mistral-7B [^(Jiang et al., 2023)] | 22.21 |\n| Qwen 2.5-72B [^(Qwen Team, 2024)] | 35.93 |\n| Mixtral-8x22B-MoE [^(Jiang et al., 2024)] | 37.08 |\n| Llama 3.1-70B [^(MetaAI, 2024)] | 38.13 |\n| **Closed-source LLMs** |  |\n| Gemini 1.5 Pro [^(Anil et al., 2023)] | 34.31 |\n| GPT-4o [^(OpenAI, 2024a)] | 43.18 |\n| GPT-4 [^(OpenAI et al., 2023)] | 49.85 |\n| o1-preview [^(OpenAI, 2024b)] | 59.49 |\n| Claude 3.5 sonnet [^(Anthropic, 2024a)] | 61.10 |", "caption": "Table 1: Various LLMs\u2019 performances on the 1,049 instances of EqInfer\u00a0task.", "description": "This table presents the accuracy scores achieved by various Large Language Models (LLMs) on the Equation Inference (EqInfer) task.  The EqInfer task involves assessing the correctness of equations within the context of a research paper. The table compares the performance of both open-source and closed-source LLMs, providing insights into the strengths and limitations of different models in solving this research-oriented task.  The accuracy is calculated as the percentage of correctly identified equations.", "section": "5.1 EquationInference"}, {"content": "| Methods | S-F<sub>1</sub> | S-Precision | S-Recall | S-Match | ROUGE-L | ROUGE-1 |\n|---|---|---|---|---|---|---|\n| **Experiment Design** |  |  |  |  |  |  |\n| **Experiment Explanation** |  |  |  |  |  |  |\n| **Methods** |  |  |  |  |  |  |\n| Copy Input | 21.13 | 17.94 | 26.76 | 40.32 | 22.06 | 25.28 |\n| **Open-source LLMs** |  |  |  |  |  |  |\n| OLMo-7B (Groeneveld et al., 2024) | 33.94 | 37.25 | 31.79 | 45.78 | 26.30 | 30.38 |\n| Falcon-40B (Almazrouei et al., 2023) | 17.87 | 21.78 | 15.35 | 17.03 | 12.10 | 12.72 |\n| Gemma 2-27B (Gemma Team, 2024) | 34.33 | 39.71 | 30.51 | 42.77 | 26.20 | 29.63 |\n| Mistral-7B (Jiang et al., 2023) | 37.62 | 43.09 | 34.19 | 50.18 | 30.20 | 34.69 |\n| Mixtral-8x22B-MoE (Jiang et al., 2024) | 42.21 | 50.13 | 36.82 | 49.07 | 29.96 | 34.53 |\n| Llama 3.1-70B (MetaAI, 2024) | 40.57 | 48.43 | 35.43 | 50.05 | 29.33 | 34.11 |\n| Qwen 2.5-72B (Qwen Team, 2024) | 43.24 | 51.73 | 37.55 | 51.12 | 29.46 | 34.68 |\n| **Closed-source LLMs** |  |  |  |  |  |  |\n| Gemini 1.5 Pro (Anil et al., 2023) | 51.87 | 50.77 | 53.37 | 52.87 | 28.52 | 33.80 |\n| Claude 3.5 sonnet (Anthropic, 2024a) | 48.74 | 46.49 | 51.53 | 53.03 | 18.75 | 26.15 |\n| GPT-4 (OpenAI et al., 2023) | 43.89 | 42.34 | 45.82 | 55.03 | 22.82 | 30.01 |\n| GPT-4o (OpenAI, 2024a) | 53.00 | 51.24 | 55.12 | 54.79 | 27.54 | 34.31 |\n| o1-preview (OpenAI, 2024b) | 46.67 | 45.04 | 48.70 | 58.55 | 29.11 | 36.70 |", "caption": "Table 2: Various LLMs\u2019 performances on the 100 instances of ExpDesign. The explanation generation is based on the oracle experiments to prevent error propagation. \u201cCopy Input\u201d is a random baseline: for experiment design, randomly select 5 sentences from the input paper; for experiment explanation, directly copy each experiment idea.", "description": "Table 2 presents the performance of various Large Language Models (LLMs) on the task of designing and explaining experiments.  The dataset consists of 100 instances where each instance provides an excerpt of a research paper as input. The LLMs were evaluated on two sub-tasks: (1) generating an experiment design based on the input paper, and (2) generating an explanation for the proposed experiment design.  The results are reported using several metrics, including S-F1, S-Precision, S-Recall, S-Match, and ROUGE-L/ROUGE-1.  A \"Copy Input\" baseline is included where the experiment design consists of 5 randomly selected sentences from the input paper, and the explanation is a direct copy of the experiment idea. This allows comparison against LLMs' ability to synthesize more original and insightful experimental designs and explanations. ", "section": "5.2 EXPERIMENTDESIGN"}, {"content": "| Models | One-by-One | Whole-List |\n|---|---|---|\n| Llama 3.1-70B | 50.05 | 49.36 (\u2193 0.7) |\n| Qwen 2.5-72B | 51.12 | 48.56 (\u2193 2.6) |\n| Gemini 1.5 Pro | 52.87 | 57.48 (\u2191 4.6) |\n| Claude 3.5 sonnet | 53.03 | 59.11 (\u2191 6.1) |\n| GPT-4 | 55.03 | 56.95 (\u2191 1.9) |\n| GPT-4o | 54.79 | 58.54 (\u2191 3.8) |\n| o1-preview | 58.55 | 61.58 (\u2191 3.0) |", "caption": "Table 3: The impact on S-Match\u00a0scores of maintaining the experiment\u2019s self-containment for ExpDesign.", "description": "This table presents the results of an experiment evaluating the impact of maintaining the experiment's self-containment on the S-Match scores in the EXPDESIGN task.  Self-containment refers to the approach of presenting each experiment individually to the LLM for explanation, as opposed to providing the entire experiment list at once. The table compares the performance of various LLMs under both self-contained and non-self-contained scenarios, highlighting the effect of this approach on the quality of the generated explanations.", "section": "5.2 EXPERIMENTDESIGN"}, {"content": "| Models | Acc. ratio |\n|---|---| \n| Llama 3.1-70B | 22.93 |\n| Gemini 1.5 Pro | 55.07 |\n| Claude 3.5 sonnet | 61.46 |\n| GPT-4o | 69.72 |\n| o1-preview | **76.14** |", "caption": "Table 4: The human evaluation results on LLMs\u2019 output explanations of ExpDesign. \u201cAcc. ratio\u201d means how many model outputs are accepted by the annotator.", "description": "This table presents the results of human evaluation on the quality of explanations generated by various Large Language Models (LLMs) for experiment designs.  Human annotators assessed the acceptability of the LLM-generated explanations, and the 'Acc. ratio' column indicates the percentage of LLM explanations deemed acceptable by the annotators. This provides a qualitative measure of the LLM's ability to not only generate experiment designs but also to provide understandable and justifiable rationales for those designs.", "section": "5.2 ExperimentDesign"}, {"content": "| Models | S-F1 | S-Precision | S-Recall | S-Match | ROUGE-L | ROUGE-1 |\n|---|---|---|---|---|---|---|\n| GPT-4o | 53.00 | 51.24 | 55.12 | 58.54 | 29.25 | 35.50 |\n| GPT-4o w/ figures | 50.11 | 48.94 | 51.59 | 58.53 | 27.87 | 34.30 |\n| GPT-4 | 43.89 | 42.34 | 45.82 | 56.95 | 25.98 | 33.37 |\n| GPT-4 w/ figures | 43.54 | 42.56 | 44.85 | 55.03 | 22.82 | 30.01 |\n| InternVL2-26B | 40.52 | 48.95 | 35.20 | 50.03 | 29.13 | 34.26 |\n| InternVL2-26B w/ figures | 38.83 | 46.91 | 33.70 | 50.29 | 29.29 | 34.06 |", "caption": "Table 5: The figure inputs ablation of ExpDesign. For the maximum text input length, same as the setting in Table\u00a02, we use 2,000 and 3,000 words for open- and closed-source models, respectively. For the closed-source GPT-4o and GPT-4, as they have long context window sizes, we use all the figures of each paper. While for InternVL2, we randomly select two figures per input paper.", "description": "This table presents the ablation study on the impact of using figures as input in the experiment design task.  It compares the performance of different large language models (LLMs) in generating experiment plans and their corresponding explanations with and without figure inputs.  The experiment was conducted on 100 instances.  The text input length was held consistent across LLMs (2000 and 3000 words for open- and closed-source models respectively). Closed-source models GPT-4 and GPT-40 used all available figures; InternVL2 used two randomly selected figures per paper. The metrics used to evaluate the performance are S-F1, S-Precision, S-Recall, S-Match, ROUGE-L, and ROUGE-1.", "section": "5.2 ExperimentDesign"}, {"content": "| Methods | SN-F<sup>1</sup> (%) | SN-Precision (%) | SN-Recall (%) | ITF-IDF (\u2191) | \n|---|---|---|---|---| \n| Human Review | \u2014 | \u2014 | \u2014 | 7.69 | \n| **Open-source LLMs** |  |  |  |  | \n| OLMo-7B (Groeneveld et al., 2024) | 43.25 | 40.38 | 47.04 | 2.45 | \n| Falcon-40B (Almazrouei et al., 2023) | 27.34 | 25.13 | 30.88 | 1.06 | \n| Gemma 2-27B (Gemma Team, 2024) | 35.85 | 34.68 | 37.91 | 1.43 | \n| Mistral-7B (Jiang et al., 2023) | 42.03 | 43.80 | 40.77 | 1.17 | \n| Mixtral-8x22B-MoE (Jiang et al., 2024) | 43.23 | 44.59 | 42.23 | 0.98 | \n| Llama 3.1-70B (MetaAI, 2024) | 42.78 | 43.19 | 42.70 | 2.60 | \n| Qwen 2.5-72B (Qwen Team, 2024) | 42.74 | 43.80 | 42.05 | 1.21 | \n| **Closed-source LLMs** |  |  |  |  | \n| Gemini 1.5 Pro (Anil et al., 2023) | 48.75 | 43.97 | 55.08 | 5.88 | \n| Claude 3.5 sonnet (Anthropic, 2024a) | 47.85 | 41.97 | 56.00 | 3.91 | \n| GPT-4 (OpenAI et al., 2023) | 47.66 | 42.15 | 55.19 | 5.31 | \n| GPT-4o (OpenAI, 2024a) | 47.73 | 42.09 | 55.48 | 5.95 | \n| o1-preview (OpenAI, 2024b) | 48.62 | 42.54 | 57.08 | 5.63 | \n| **LLM Agent Framework** |  |  |  |  | \n| AI-SCI (GPT-4o) (Lu et al., 2024) | 45.05 | 40.02 | 51.91 | 2.23 | ", "caption": "Table 6: Various LLMs\u2019 performances on the 993 instances of Weakness.", "description": "This table presents the performance of various Large Language Models (LLMs) on the PAPERWEAKNESS task, a subtask within the AAAR-1.0 benchmark dataset.  The task involves identifying weaknesses in research papers. The table shows the performance metrics for several open-source and closed-source LLMs, including SN-F1 score (a harmonic mean of SN-Precision and SN-Recall), SN-Precision, SN-Recall and ITF-IDF (Inverse Text Frequency-Inverse Document Frequency), a metric measuring weakness diversity.  The results indicate the ability of different LLMs to identify and characterize weaknesses effectively, with closed-source models generally outperforming open-source models.", "section": "3.3 PAPERWEAKNESS"}, {"content": "| Models | Input Context | Processing | Window Size (in words) | SN-F1 | SN-Precision | SN-Recall | ITF-IDF |\n|---|---|---|---|---|---|---|---| \n| GPT-4-Turbo | split-combine | 3,000 | **47.66** | 42.15 | **55.19** | 5.31 |\n|  | no-split | 3,000 | 45.80 | **43.66** | 48.39 | **5.58** |\n|  | no-split | 20,000 | 44.99 | 42.64 | 47.82 | **5.58** |\n| GPT-4o | split-combine | 3,000 | **47.73** | 42.09 | **55.48** | 5.95 |\n|  | no-split | 3,000 | 45.74 | **43.45** | 48.54 | 5.92 |\n|  | no-split | 20,000 | 45.47 | 42.97 | 48.51 | **6.02** |\n| AI-SCI | split-combine | 3,000 | **45.05** | 40.02 | **51.91** | 2.23 |\n|  | no-split | 3,000 | 42.56 | **40.90** | 44.65 | 2.53 |\n|  | no-split | 20,000 | 42.53 | 40.75 | 44.78 | **2.58** |", "caption": "Table 7: The performance comparison of different input processing methods for Weakness. We use GPT-4o and GPT-4-Turbo because both accept a maximum of 128k tokens input. We also put the results of AI-SCI\u00a0in the table for reference. Here, \u201csplit-combine\u201d splits the input paper into several pieces, where each piece\u2019s length is denoted as \u201cwindow size\u201d; \u201cno-split\u201d means the conventional input cutting, for example, if the window size is 3,000, then only the first 3,000 words in the paper are used. According to the data statistics, 20,000 words can cover maximum lengths of more than 95% of the papers in our dataset.", "description": "Table 7 compares the performance of different input processing methods for the WEAKNESS task using GPT-40, GPT-4-Turbo, and AI-SCI.  It contrasts two methods: 'split-combine', which divides the input paper into smaller chunks (specified by a 'window size'), and 'no-split', which uses the entire paper (up to 20,000 words, covering 95% of papers). The table shows how each method's performance varies with different window sizes.  This allows analysis of whether splitting the paper into smaller parts for processing improves model performance on this task.", "section": "3.4 REVIEWCRITIQUE"}, {"content": "| Models |  | SN-F1 | SN-Precision | SN-Recall | ITF-IDF |\n|---|---|---|---|---|---| \n| GPT-4o |  | 47.73 | 42.09 | 55.48 | 5.95 |\n|  | w/ tables | 46.76 | 41.32 | 54.17 | 5.53 |\n|  | w/ figures | 46.62 | 41.20 | 54.04 | 5.48 |\n|  | w/ tables & figures | 46.58 | 41.17 | 53.98 | 5.36 |\n| InternVL2-26B |  | 41.91 | 41.02 | 43.28 | **1.48** |\n|  | w/ tables | 40.55 | 40.37 | 42.91 | 1.46 |\n|  | w/ figures | **42.88** | **42.10** | **43.76** | 1.46 |\n|  | w/ tables & figures | 42.44 | 42.00 | 43.31 | 1.44 |", "caption": "Table 8: The ablation study about the paper tables and figures of Weakness. Based on the conclusion in Table\u00a07, we use the \u201csplit-combine\u201d to process the text input here (2,000 and 3,000 words context window size for open- and closed-source models). For GPT-4o, we use all the table/figure images; while for InternVL2, we randomly select two images per paper, i.e., two random figures, two random tables, or one random figure + table.", "description": "This table presents an ablation study on the impact of using tables and figures as input to the WEAKNESS task.  Building upon the findings from Table 7, which examined different input processing methods, this experiment uses the 'split-combine' method for text processing, with context windows of 2000 and 3000 words for open and closed-source language models, respectively.  For GPT-40, all available table and figure images are used, while InternVL2 uses two randomly selected images per paper (either two figures, two tables, or one of each). The results show the impact of including visual information on the model's performance in identifying weaknesses in research papers.", "section": "3.3 PAPERWEAKNESS"}, {"content": "| Models | Labeling-All | Select-Deficient | Both \u201cNo\u201d | Either \u201cNo\u201d |\n|---|---|---|---|---|\n| **Open-source LLMs** |  |  |  |  |\n| Llama3-8B (AI@Meta, 2024) | 7.73 / 45.95 / 12.22 | 11.47 / 30.29 / 14.88 | 11.37 / 21.27 / 12.46 | 8.19 / 53.61 / 13.35 |\n| Llama3-70B (AI@Meta, 2024) | 13.63 / 42.49 / 18.19 | 13.95 / 31.16 / 17.46 | 16.16 / 23.51 / 16.67 | 12.46 / 50.02 / 18.43 |\n| Qwen2-72B (Bai et al., 2023) | 9.97 / 26.60 / 12.96 | 11.35 / 34.61 / 14.64 | 9.07 / 15.13 / 9.62 | 10.49 / 43.00 / 15.16 |\n| **Closed-source LLMs** |  |  |  |  |\n| Gemini 1.5 (Anil et al., 2023) | 16.58 / 34.13 / 19.76 | 14.71 / 43.60 / 19.72 | 17.01 / 27.05 / 18.28 | 14.46 / 50.37 / 20.34 |\n| GPT-4 (OpenAI et al., 2023) | 14.91 / 34.49 / 18.38 | 17.18 / 34.59 / 20.30 | 18.71 / 21.40 / 16.85 | 14.72 / 47.68 / 20.66 |\n| Claude Opus (Anthropic, 2024b) | 16.86 / 34.26 / 20.35 | 17.69 / 26.61 / 18.71 | 17.14 / 18.70 / 15.78 | 16.94 / 42.12 / 21.99 |", "caption": "Table 9: From (Du et\u00a0al., 2024), various LLMs\u2019 performances on the 11,376 instances of ReviewCritique. The best F1 score among different prompt methods for a single model is underlined. The best F1 score across all models is also bold.", "description": "Table 9 presents the performance evaluation results of various Large Language Models (LLMs) on the ReviewCritique dataset, which comprises 11,376 instances.  The table showcases the F1 scores achieved by different LLMs using two distinct prompting strategies: Labeling-All and Select-Deficient, along with the results of combining these strategies using \"Both No\" and \"Either No\" methods. The best F1-score for each LLM across different prompt methods is underlined, with the overall best F1-score highlighted in bold.", "section": "5.4 REVIEWCRITIQUE"}, {"content": "| Model | ROUGE-1/2/L/BERTScore |\n|---|---| \n| GPT-4 | 17.13 / 2.71 / 14.64 / 55.63 |\n| Claude Opus | **20.18** / **3.69** / **17.52** / **57.28** |\n| Gemini 1.5 | 18.47 / 2.98 / 16.38 / 56.46 |\n| Llama3-8B | 16.49 / 2.22 / 13.65 / 55.23 |\n| Llama3-70B | 15.94 / 1.95 / 13.78 / 57.09 |\n| Qwen2-72B | 17.07 / 3.00 / 14.69 / 56.88 |", "caption": "Table 10: Evaluation of LLMs\u2019 explanations for correctly identified deficient\u00a0segments.", "description": "This table presents the ROUGE-1, ROUGE-2, ROUGE-L, and BERTScore scores for the Large Language Models' (LLMs) explanations of correctly identified deficient review segments.  It evaluates the quality of the LLMs' explanations, comparing them to human-generated explanations. The higher the score, the better the LLM's explanation aligns with human judgments.", "section": "5 EXPERIMENTS AND ANALYSES"}, {"content": "| # of classification instances | 1,049 |\n|---|---|---|\n| # of source papers | 869 |\n| ave. \u201c`left`\u201d input context length (in words) | 4,377 |\n| ave. \u201c`right`\u201d input context length (in words) | 6,362 |\n| max \u201c`left`\u201d input context length (in words) | 24,849 |\n| max \u201c`right`\u201d input context length (in words) | 32,948 |\n| min \u201c`left`\u201d input context length (in words) | 711 |\n| min \u201c`right`\u201d input context length (in words) | 8 |\n| ave. \u201c`pos.`\u201d output equation length (in character) | 55 |\n| ave. \u201c`neg.`\u201d output equation length (in character) | 48 |\n| max \u201c`pos.`\u201d output equation length (in character) | 1,039 |\n| max \u201c`neg.`\u201d output equation length (in character) | 306 |\n| min \u201c`pos.`\u201d output equation length (in character) | 6 |\n| min \u201c`neg.`\u201d output equation length (in character) | 4 |", "caption": "Table 11: The statistics of EqInfer. Here, the \u201cleft\u201d and \u201cright\u201d input context indicates the paper contexts \\ulbefore and  \\ulafter the missed equation; \u201cpos.\u201d means the ground-truth equations (written by the source paper authors), while \u201cneg.\u201d is the GPT4-synthetic wrong equations.", "description": "Table 11 presents a statistical overview of the Equation Inference (EQINFER) dataset used in the AAAR-1.0 benchmark.  It details the average and maximum lengths of the text before and after the equation in the original papers (the input 'context'), as well as the lengths of the correct equations (the 'ground truth' or 'pos.') and the incorrect, synthetically generated equations used as negative examples ('neg.'). This data is crucial in understanding the scale and complexity of the task that the LLMs are expected to complete.", "section": "3.1 EQUATIONINFERENCE"}, {"content": "| # of instances | 100 |\n| # of source papers | 100 |\n| ave. input context length (in words) | 4,288 |\n| max input context length (in words) | 9,799 |\n| min input context length (in words) | 698 |\n| ave. # of input figures | 2.6 |\n| max # of input figures | 16.0 |\n| min # of input figures | 0.0 |\n| ave. length of Experiment&Explanation list | 5.7 |\n| ave. length per experiment (in words) | 34.3 |\n| ave. length per explanation (in words) | 27.1 |\n| max length of Experiment&Explanation list | 13 |\n| max length per experiment (in words) | 135 |\n| max length per explanation (in words) | 89 |\n| min length of Experiment&Explanation list | 2 |\n| min length per experiment (in words) | 9 |\n| min length per explanation (in words) | 9 |", "caption": "Table 12: The statistics of ExpDesign.", "description": "Table 12 presents a statistical overview of the dataset used for the Experiment Design task within the AAAR-1.0 benchmark.  It details the number of instances and source papers, along with the average, maximum, and minimum lengths of the input context (in words), the number of input figures, the average and range of lengths for experiment explanations and descriptions, and the overall lengths of the combined experiment and explanation lists.", "section": "3.2 EXPERIMENTDESIGN"}, {"content": "| # of instances | 993 |\n| # of source papers | 993 |\n| ave. input context length (in words) | 9,811 |\n| max input context length (in words) | 49,195 |\n| min input context length (in words) | 24 |\n| ave. # of input figures | 7.0 |\n| max # of input figures | 37.0 |\n| min # of input figures | 0.0 |\n| ave. # of input tables | 4.3 |\n| max # of input tables | 53.0 |\n| min # of input tables | 0.0 |\n| ave. # of reviewers per paper | 3.8 |\n| max # of reviewers per paper | 9.0 |\n| min # of reviewers per paper | 3.0 |\n| ave. # of weaknesses per reviewer | 4.8 |\n| max # of weaknesses per reviewer | 39.0 |\n| min # of weaknesses per reviewer | 1.0 |\n| ave. length of weakness (in words) | 39.1 |\n| max length of weakness (in words) | 371.0 |\n| min length of weakness (in words) | 2.0 |", "caption": "Table 13: The statistics of Weakness.", "description": "Table 13 presents a detailed statistical overview of the WEAKNESS dataset used in the AAAR-1.0 benchmark.  It includes counts of instances, source papers, and associated data points such as input context length (in words), the number of figures and tables, the number of reviewers per paper, the number of weaknesses identified per reviewer, and the average and maximum length of these weaknesses (in words). These statistics provide insights into the scale and characteristics of the dataset, which is crucial for understanding the complexity and scope of the LLM evaluation task.", "section": "3 AAAR-1.0"}]