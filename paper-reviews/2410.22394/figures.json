[{"figure_path": "https://arxiv.org/html/2410.22394/x1.png", "caption": "Figure 1: The input-output illustration of four tasks in the proposed\u00a0AAAR-1.0\u00a0benchmark.", "description": "This figure illustrates the input and output formats for each of the four tasks in the AAAR-1.0 benchmark dataset.  Each task involves a different aspect of research: Equation Inference (inferring equations from context), Experiment Design (creating experiment plans), Paper Weakness Identification (finding flaws in papers), and Review Critique (evaluating review quality).  For each task, the figure shows the type of input provided to the model (e.g., paper text, incomplete equations, a research idea) and the expected output (e.g., a correct equation, an experiment plan, a list of identified weaknesses, a judgment of the review's reliability).", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2410.22394/x2.png", "caption": "Figure 2: Data construction workflows of the three tasks in \u00a0AAAR-1.0.", "description": "This figure illustrates the data construction pipelines for three of the four tasks in the AAAR-1.0 benchmark dataset.  For each task (Equation Inference, Experiment Design, and Paper Weakness), it shows the steps involved in gathering data, cleaning and preprocessing that data, and using LLMs for synthesis and filtering.  The figure details the role of human experts in ensuring data quality and consistency for each task. The different data sources used (arXiv, OpenReview, etc.) and the various LLMs employed (GPT-4, etc.) in the creation of the dataset are also showcased. The figure visually represents the complex process of creating a high-quality benchmark dataset suitable for evaluating LLMs on AI research-related tasks.", "section": "3 AAAR-1.0"}, {"figure_path": "https://arxiv.org/html/2410.22394/x3.png", "caption": "Figure 3: The input context length scaling trend on the\u00a0EqInfer\u00a0task.", "description": "This figure displays the relationship between the length of the input context and the accuracy of various LLMs on the equation inference task (EqInfer).  The x-axis represents the length of the input context in words, while the y-axis represents the accuracy achieved by different language models. The graph shows how the accuracy changes as the input context length increases.  It helps to understand the impact of context window size on the LLM's performance on this specific task.", "section": "5.1 EQUATIONINFERENCE"}, {"figure_path": "https://arxiv.org/html/2410.22394/x4.png", "caption": "Figure 4: The input context length scaling trend of different LLMs on the\u00a0ExpDesign\u00a0task.", "description": "Figure 4 illustrates how the performance of various Large Language Models (LLMs) on the Experiment Design task changes with varying lengths of input context.  The x-axis represents the length of the input context (in words), while the y-axis shows the performance metric (likely S-F1 score or a similar metric assessing the quality of the generated experiment design).  The plot allows for a comparison of different LLMs' abilities to generate effective experiment plans given different amounts of contextual information. The figure helps to determine if longer contexts are always beneficial, or if there's an optimal length for LLMs to achieve the best performance.", "section": "5.2 ExperimentDesign"}, {"figure_path": "https://arxiv.org/html/2410.22394/x5.png", "caption": "(a) The review score distribution of the papers used in Weakness.", "description": "This figure shows a pie chart illustrating the distribution of review scores for the papers included in the WEAKNESS dataset.  The scores range from 1 to 10, representing a scale of review quality. Each slice of the pie chart corresponds to a specific score range, with its size proportional to the number of papers that received that score.  This visualization helps to understand the overall quality and diversity of the papers used in the benchmark dataset.", "section": "3.3 PAPERWEAKNESS"}, {"figure_path": "https://arxiv.org/html/2410.22394/x6.png", "caption": "(b) The track distribution of the papers used in Weakness.", "description": "The bar chart visualizes the distribution of the 1000 papers used in the WEAKNESS dataset across 13 different research tracks within the ICLR 2023 conference.  Each bar represents a track, and its height corresponds to the number of papers belonging to that track.  The purpose is to show the diversity of research areas represented in the dataset and ensure the sample is not skewed towards any particular area.", "section": "3.3 PAPERWEAKNESS"}, {"figure_path": "https://arxiv.org/html/2410.22394/x7.png", "caption": "Figure 5: The data diversity illustration of Weakness, including the score distribution and track distribution of the papers used in our dataset.", "description": "This figure visualizes the diversity of the WEAKNESS dataset used in the paper.  The left panel (a) shows a pie chart illustrating the distribution of overall scores assigned to papers in the dataset, categorizing papers based on score ranges. The right panel (b) presents a bar chart showing the distribution of papers across different research tracks within the dataset. This dual representation provides a comprehensive view of the dataset's composition, highlighting the balance between score ranges and representation of diverse research topics.  The aim is to demonstrate the breadth and quality of the dataset used to evaluate the performance of Large Language Models.", "section": "3.3 PAPERWEAKNESS"}, {"figure_path": "https://arxiv.org/html/2410.22394/x8.png", "caption": "Figure 6: The annotation platform for collecting the annotation of ExpDesign. We ask annotators to first make comments on the Google Drive PDF, then move all the annotations to the online Google Doc (for further verification and discussion).", "description": "Figure 6 shows the annotation platform used for the Experiment Design task in the AAAR-1.0 benchmark.  The process involves annotators first reviewing a research paper's PDF on Google Drive and adding comments directly to the document.  These comments, which detail suggested experiments and their motivations, are then transcribed into a structured online Google Doc. This two-step process allows for both initial annotations within the context of the paper itself, followed by a structured recording and a later opportunity for review and discussion to improve data quality and consistency.", "section": "3.2 ExperimentDesign"}, {"figure_path": "https://arxiv.org/html/2410.22394/extracted/5963447/figures/case_equation.png", "caption": "Figure 7: A sample case of EqInfer.", "description": "This figure illustrates an example from the Equation Inference task in the AAAR-1.0 benchmark dataset.  The task requires the model to select the correct mathematical equation from four options (A-D), given the surrounding textual context from a research paper. The context consists of 'Context Before' and 'Context After' snippets providing surrounding information, while the actual equation is removed and replaced with the four options. The model's task is to identify the most appropriate equation from the options based on the context, which requires a deep understanding of the algorithm and mathematical concepts in the paper.", "section": "3.1 EquationInference"}, {"figure_path": "https://arxiv.org/html/2410.22394/extracted/5963447/figures/case_exp.png", "caption": "Figure 8: A sample case of ExpDesign.", "description": "This figure shows a sample from the dataset used to evaluate large language models' ability to design experiments.  It illustrates the input and output components of the EXPDESIGN task. The input is a segment of text from a research paper, providing context about a given topic.  The expected output consists of two parts: 1) a list of experiment designs that a researcher would conduct to investigate the topic covered in the input text and 2) a list of explanations justifying the reasons for each proposed experiment. The goal is to assess the model's ability to both conceive of appropriate experiments and articulate their underlying rationales, mirroring a core aspect of research methodology.", "section": "3.2 EXPERIMENTDESIGN"}, {"figure_path": "https://arxiv.org/html/2410.22394/extracted/5963447/figures/case_review.png", "caption": "Figure 9: A sample case of Weakness.", "description": "This figure showcases an example from the PAPERWEAKNESS section of the AAAR-1.0 benchmark dataset.  It illustrates the task of identifying weaknesses in a research paper. The input shows a segment of a research paper describing a Neural Process (NP) model.  The output displays a list of weaknesses identified by human reviewers, demonstrating diverse issues in the paper, such as unclear writing, insufficient experimentation, and lack of comparison with state-of-the-art models. This exemplifies the complexity and nuances involved in evaluating the quality and depth of a research paper.", "section": "3.3 PAPERWEAKNESS"}, {"figure_path": "https://arxiv.org/html/2410.22394/extracted/5963447/figures/prompt_eq.png", "caption": "Figure 10: The prompts used in EqInfer, including both data collection and model prediction.", "description": "This figure displays the prompts used in the Equation Inference task of the AAAR-1.0 benchmark.  It shows three stages: 1) LLM-based Equation Synthesis, where an LLM generates equations based on given context; 2) LLM-based Equation Filtering, where another LLM assesses the correctness of the generated equations; and 3) Model Prediction, where the final task requires an LLM to select the correct equation from provided choices.  The prompts are designed to evaluate the LLM's ability to infer equations based on context.", "section": "3.1 EquationInference"}, {"figure_path": "https://arxiv.org/html/2410.22394/extracted/5963447/figures/prompt_exp.png", "caption": "Figure 11: The prompts used in ExpDesign, including both data collection and model prediction.", "description": "Figure 11 shows the process of data collection and model prediction in the Experiment Design task.  The data collection prompt involves providing a sentence (or a short paragraph) from a paper and a list of its experiments to identify whether the sentence reveals experiment details.  The model prediction prompt involves providing part of a paper with the experiment sections removed.  The model must reconstruct the experiment list, based on understanding the paper's research motivation, and then provide an explanation list corresponding one-to-one with the experiment list to clarify why each experiment is necessary. ", "section": "3.2 EXPERIMENTDESIGN"}, {"figure_path": "https://arxiv.org/html/2410.22394/extracted/5963447/figures/prompt_review.png", "caption": "Figure 12: The prompts used in Weakness.", "description": "Figure 12 shows the prompts used for the WEAKNESS task in the AAAR-1.0 benchmark.  The prompts guide the large language model (LLM) to identify weaknesses in a research paper, given its text and figures.  The prompt instructs the LLM to act as an expert reviewer, carefully reviewing the paper and providing a list of weaknesses, one per line.  If the provided text is not research-related (e.g., an acknowledgement section), the LLM should output \"No research content\".", "section": "3.4 PAPERWEAKNESS"}]