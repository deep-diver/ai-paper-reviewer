[{"figure_path": "https://arxiv.org/html/2412.07589/x2.png", "caption": "Figure 1: Results of DiffSensei. (a) Customized manga generation with controllable character images, panel captions, and layout conditions. Our DiffSensei successfully generates detailed character expressions and states following the panel captions. (b) Manga creation for real human images. The dialogues are post-edited by humans. The continuation is in the Appendix\u00a0Fig.\u00a07. We strongly recommend that the readers see the Appendix for more comprehensive results. Manga reading order: Right to left. Top to bottom.", "description": "This figure showcases the capabilities of DiffSensei in generating manga.  Panel (a) demonstrates customized manga generation where users control character appearances, panel captions, and layout. DiffSensei generates manga panels with accurate character expressions and poses that align with the given captions.  Panel (b) shows a comparison using real human images; note that the dialogue in this panel was post-edited by a human. The full results and continuation of panel (b) can be found in Appendix Figure 7. Manga should be read from right-to-left and top-to-bottom.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.07589/x3.png", "caption": "Figure 2: We construct MangaZero through three steps: 1) Download manga pages from the internet. 2) Annotate manga panels autonomously with pre-trained models. 3) Human calibration for the character ID annotation.", "description": "This figure illustrates the three-step process used to create the MangaZero dataset.  First, manga pages were downloaded from online manga websites. Second, these pages were automatically annotated using pre-trained models to identify panels, character bounding boxes, dialog boxes, and generate panel captions.  Finally, human annotators reviewed and corrected the automatically generated character IDs to ensure accuracy and consistency.", "section": "3 The MangaZero Dataset"}, {"figure_path": "https://arxiv.org/html/2412.07589/x4.png", "caption": "Figure 3: The architecture of DiffSensei. In the first stage, we train a multi-character customized manga image generation model with layout control. The dialog embedding is added to the noised latent after the first convolution layer. All the parameters in the U-Net and feature extractor are trained. In the second stage, we finetune LoRA and resampler weights of an MLLM to adapt the source character features corresponding to the text prompt. We use the model in the first stage as the image generator and freeze its weights.", "description": "DiffSensei's architecture is a two-stage process. Stage 1 trains a multi-character manga generation model with layout control using a diffusion model (U-Net). Dialog embeddings are incorporated early in the process. All U-Net and feature extractor parameters are learned. Stage 2 fine-tunes a multimodal large language model (MLLM) using LoRA and a resampler to adapt source character features to the text prompt. The stage 1 model serves as the image generator with fixed weights.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2412.07589/x5.png", "caption": "Figure 4: Qualitative comparison with baselines. Baselines followed by a \u201c*\u201d use reference images as input rather than character images. Methods marked by \u201c\u2020\u201d means re-trained with dialog embedding. Our model excels at preserving the characters while following the text prompt. Our DiffSensei successively generates highlighted details in panel captions. Better viewed with zoom-in.", "description": "Figure 4 presents a qualitative comparison of DiffSensei against several baseline methods for customized manga generation.  It highlights DiffSensei's superior ability to maintain character consistency across generated panels while adhering to textual prompts.  The figure showcases how DiffSensei successfully generates detailed manga panels that accurately reflect the specified text descriptions and layouts, capturing even subtle details highlighted in the panel captions.  Baseline methods that utilize reference images (indicated by '*') demonstrate limitations in character preservation and text-driven customization. Those re-trained with dialog embedding ('\u2020') show some improvement. The comparison underscores DiffSensei's advantage in preserving character features and incorporating textual information for dynamic and detailed manga generation.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.07589/x6.png", "caption": "Figure 5: Human preference study on MangaZero eval set.", "description": "This bar chart presents the results of a user preference study conducted on the MangaZero evaluation set.  The study compared DiffSensei's generated manga pages against several baseline models across five key aspects: text-image alignment, style consistency, character consistency, image quality, and overall preference. Each bar represents the average score for a specific model on a particular aspect, offering a direct visual comparison of user perception of each model's performance. This allows for easy identification of the model that most successfully fulfills the criteria of effective manga generation.", "section": "5.2 Comparison to Baselines"}, {"figure_path": "https://arxiv.org/html/2412.07589/x7.png", "caption": "Figure 6: Qualitative results. Character images in red boxes are from Manga109 (The rightmost example). Our DiffSensei can generate vivid manga pages in various scenarios. Better viewed with zoom-in. More results can be found in the appendix.", "description": "Figure 6 presents qualitative results demonstrating DiffSensei's ability to generate manga pages across diverse scenarios.  Character images within red boxes are sourced from the Manga109 dataset, highlighting the model's capacity to incorporate external character styles.  The figure showcases DiffSensei's proficiency in generating detailed expressions and poses, aligning with narrative captions.  For a more comprehensive collection of results, refer to the appendix.", "section": "5.3 Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.07589/x8.png", "caption": "Figure 7: A complete long manga story about Hinton, LeCun, and Bengio winning the Nobel Prize.", "description": "This figure shows a long manga story illustrating a fictional narrative about Geoffrey Hinton, Yann LeCun, and Yoshua Bengio's journey to create an AI model surpassing Transformers. The story depicts their challenges, self-doubt, eventual success, and subsequent Nobel Prize win, highlighting the power of perseverance in scientific endeavors.", "section": "Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.07589/x9.png", "caption": "Figure 8: More qualitative comparisons with baselines. Baselines followed by a \u201c*\u201d use reference images as input rather than character images. Methods marked by \u201c\u2020\u201d means re-trained with dialog embedding.", "description": "This figure presents a qualitative comparison of the proposed DiffSensei model against several baseline methods for customized manga generation.  It showcases the strengths of DiffSensei in generating manga panels with multiple characters, each adapting dynamically to the textual prompts. The comparison highlights how DiffSensei outperforms baselines in preserving character identities, controlling layouts, and creating coherent narratives, even when handling unseen characters or adapting to text cues. The asterisk (*) indicates methods that utilize reference images as input instead of individual character images, while the dagger (\u2020) marks methods that were retrained with dialog embedding. ", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.07589/x10.png", "caption": "Figure 9: Qualitative ablation of the proposed modules. CM is character masked attention injection. DM is dialog masked encoding. Magi means using Magi\u00a0[30] image encoder. MLLM means using MLLM for stage 2 training.", "description": "This ablation study investigates the individual contributions of different modules within the DiffSensei framework.  It shows the impact of removing character masked attention injection (CM), dialog masked encoding (DM), the Magi [30] image encoder, or the second-stage Multimodal Large Language Model (MLLM) training. By systematically excluding each component, the figure demonstrates the effect on the model's performance, measured by FID, CLIP, DINO-I, DINO-C, and F1-score. This provides insights into the relative importance of each component for achieving high-quality customized manga generation. ", "section": "5.4 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2412.07589/x11.png", "caption": "Figure 10: Qualitative ablation of \u03b2\ud835\udefd\\betaitalic_\u03b2.", "description": "This figure shows an ablation study on the effect of the hyperparameter \u03b2 (beta) in DiffSensei. Beta controls the weighting between original character features and MLLM-adapted features.  The figure displays several manga panels generated with different values of \u03b2, demonstrating how altering this parameter impacts the balance between character identity preservation (low \u03b2) and text-driven customization (high \u03b2).  Specifically, it illustrates the trade-off between maintaining the original visual characteristics of the character and allowing the model to dynamically adapt the appearance based on textual prompts. Qualitative differences in the generated manga are observed for each \u03b2 value.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.07589/x12.png", "caption": "Figure 11: Failure cases.", "description": "This figure showcases several failure cases encountered by the DiffSensei model.  Specifically, it highlights instances where the model struggles with (a) unclear input character images resulting in identity loss, (b) multiple character inputs causing character fusion, and (c) generating images in a consistent style without character input, leading to a generic manga style. These examples demonstrate the limitations of DiffSensei in handling various input conditions and highlight areas for future improvement.", "section": "5. Limitations and Future Work"}, {"figure_path": "https://arxiv.org/html/2412.07589/x13.png", "caption": "(a) Covers of manga series.", "description": "This figure displays the covers of 48 different manga series included in the MangaZero dataset.  These series were selected for their popularity, diverse art styles, and varied character casts, making them suitable for training a model on customized manga generation.", "section": "3. The MangaZero Dataset"}, {"figure_path": "https://arxiv.org/html/2412.07589/x14.png", "caption": "(b) Examples of character and dialog annotations.", "description": "This figure shows example annotations from the MangaZero dataset.  The image displays several manga panels with bounding boxes drawn around individual characters and dialog bubbles.  Each character bounding box is assigned a unique character ID for consistent tracking throughout a manga story, while dialog boxes highlight speech and thought bubbles within the panels.  This demonstrates the detailed annotation work involved in creating MangaZero to support customized manga generation.", "section": "3. The MangaZero Dataset"}, {"figure_path": "https://arxiv.org/html/2412.07589/x15.png", "caption": "(c) Resolution distribution.", "description": "The figure shows the distribution of panel resolutions in the MangaZero dataset.  The x-axis represents the width of the panel, and the y-axis represents the height. The plot shows a concentration of panels around 512x512 and 1024x1024 pixels, indicating a prevalence of these resolutions in the dataset.", "section": "3 The MangaZero Dataset"}, {"figure_path": "https://arxiv.org/html/2412.07589/x16.png", "caption": "Figure 12: Details of the MangaZero dataset.", "description": "Figure 12 presents details of the MangaZero dataset, a crucial component of the research.  It showcases three key aspects: (a) Cover images of the 48 different manga series included in the dataset, highlighting the diversity of styles and artistic choices. (b) Examples illustrating the annotations for characters and dialogue within the manga panels; this demonstrates the level of detail and precision in the dataset's labeling. (c) A histogram visualizing the distribution of panel resolutions in the dataset. This graph indicates the range of panel sizes and their frequencies, showing whether the dataset has a balance of high- and low-resolution manga pages.", "section": "3 The MangaZero Dataset"}, {"figure_path": "https://arxiv.org/html/2412.07589/x17.png", "caption": "Figure 13: DiffSensei generated results with inputs (Part1).", "description": "Figure 13 presents examples of manga pages generated by the DiffSensei model.  Each example shows the input (story summary, character images, dialog boxes, and layout conditions) alongside the corresponding output generated by the model.  The examples showcase the model's ability to generate manga panels with multiple characters, dynamic expressions and poses that adapt to textual cues, and precise control over dialog and layout.  Part 1 suggests there are additional examples in subsequent figures.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.07589/x18.png", "caption": "Figure 14: DiffSensei generated results with inputs (Part2).", "description": "This figure displays additional examples of manga pages generated by the DiffSensei model.  The input includes story summaries, character images, dialog boxes, and layout conditions.  Each row shows the input elements, the layout designed based on these conditions, and the final output generated by DiffSensei, illustrating the model's ability to generate manga panels according to the given specifications and constraints. It highlights the framework's capacity for generating multi-character scenes with detailed character interaction and expressions.", "section": "5. Experiments"}]