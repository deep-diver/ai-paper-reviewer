[{"content": "| Dataset | Type | Resolution | #Series | #Stories | #Panels | Caption | Character | Dialog | Origin |\n|---|---|---|---|---|---|---|---|---|---| \n| PororoSV [18] | Animation | Fix | 1 | 15,336 | 73,665 | \u2713 | \u00d7 | \u00d7 | 2003-2016 |\n| FlintstonesSV [10] | Animation | Fix | 1 | 25,184 | 122,560 | \u2713 | \u2713 | \u00d7 | 1960-1966 |\n| StorySalon [21] | Animation | Fix | 446 | 18,255 | 159,778 | \u2713 | \u00d7 | \u00d7 | YouTube |\n| StoryStream [45] | Animation | Fix | 3 | 12,614 | 257,850 | \u2713 | \u00d7 | \u00d7 | 1939-2013 |\n| Manga109 [2] | B/W Manga | Vary | 109 | 10,602 | 103,850 | \u00d7 | \u2713 | \u2713 | 1970-2010 |\n| **MangaZero** | **B/W Manga** | **Vary** | **48** | **43,264** | **427,147** | \u2713 | \u2713 | \u2713 | 1974-2024 |", "caption": "Table 1: Comparison between MangaZero and related publically available datasets. A story is defined as a sequence of continuous images annotated consistently with character IDs. In MangaZero, a story means a manga page. A panel means a distinct story image, or called frame\u00a0[45, 21]. Most series in MangaZero are still popular in 2024. Please see the Appendix for the dataset details.", "description": "This table compares the MangaZero dataset with other publicly available datasets used for story visualization and manga generation.  Key characteristics compared include the type of visual media (animation or manga), image resolution, the number of series, stories (manga pages in MangaZero), and panels (individual frames in MangaZero).  Annotation details such as the presence of captions, character IDs, and dialogue annotations are also shown.  The table highlights MangaZero's size and comprehensiveness in terms of annotations and the time period of the included manga series.", "section": "3 The MangaZero Dataset"}, {"content": "| Method | FID \u2193 | CLIP \u2191 | DINO-I \u2191 | DINO-C \u2191 | F1 score \u2191 |\n|---|---|---|---|---|---| \n| AR-LDM* [25] | 0.409 | **0.257** | 0.548 | 0.507 | 0.004 |\n| StoryGen* [21] | 0.411 | 0.219 | 0.536 | 0.488 | 0.012 |\n| SEED-Story* [45] | 0.411 | 0.169 | 0.416 | 0.405 | 0.006 |\n| StoryDiffusion* [52] | 0.409 | 0.244 | 0.461 | 0.362 | 0.002 |\n| MS-Diffusion\u2020 [38] | 0.408 | 0.229 | 0.610 | 0.641 | 0.720 |\n| **DiffSensei** | **0.407** | 0.235 | **0.618** | **0.651** | **0.727** |", "caption": "Table 2: Quantitative comparisons on automatic metrics. Methods followed by \u201c*\u201d use reference images as input rather than characters. Methods marked by \u201c\u2020\u201d means re-trained with dialog embedding.", "description": "This table presents a quantitative comparison of different image generation models using automatic metrics.  The models are evaluated based on their performance in generating manga images.  A key distinction is made between models that use reference images as input (marked with *) and those that do not.  The table also indicates which models were retrained using dialog embedding (marked with \u2020).  Metrics include FID (Fr\u00e9chet Inception Distance), CLIP (CLIP image-text similarity), DINO-I (DINO image similarity), DINO-C (DINO character image similarity), and F1 score (dialog bounding box F1 score).  Higher scores generally indicate better performance.", "section": "5. Experiments"}, {"content": "| Method | FID \u2193 | CLIP \u2191 | DINO-I \u2191 | DINO-C \u2191 | F1 score \u2191 |\n|---|---|---|---|---|---| \n| AR-LDM* [25] | 0.410 | **0.254** | 0.527 | 0.491 | 0.005 |\n| StoryGen* [21] | 0.414 | 0.214 | 0.540 | 0.493 | 0.004 |\n| SEED-Story* [45] | 0.413 | 0.167 | 0.442 | 0.428 | 0.005 |\n| StoryDiffusion* [52] | 0.410 | 0.238 | 0.442 | 0.355 | 0.001 |\n| MS-Diffusion\u2020 [38] | **0.410** | 0.227 | 0.584 | **0.600** | 0.601 |\n| **DiffSensei** | **0.410** | 0.237 | **0.588** | **0.600** | **0.648** |", "caption": "(a) Comparison on MangaZero evaluation set.", "description": "This table presents a quantitative comparison of different models' performance on the MangaZero evaluation dataset.  The comparison uses five key metrics: Fr\u00e9chet Inception Distance (FID), CLIP image-text similarity (CLIP), DINO image similarity (DINO-I), DINO character image similarity (DINO-C), and dialog bounding box F1 score (F1 score).  Each metric provides a quantitative measure of a model's ability to generate manga pages that are visually appealing, semantically relevant, and faithful to the input text.  The models included in this comparison are AR-LDM, StoryGen, SEED-Story, StoryDiffusion, MS-Diffusion, and the proposed model, DiffSensei. The models marked with '*' use reference images as input rather than character images, and those marked with '\u2020' are re-trained with dialog embedding. The results offer insights into the strengths and weaknesses of each model regarding visual quality, text-image alignment, and character consistency.", "section": "5. Experiments"}, {"content": "| CM | DM | Magi | MLLM | FID \u2193 | CLIP \u2191 | DINO-I \u2191 | DINO-C \u2191 | F1 score \u2191 |\n|---|---|---|---|---|---|---|---|---|\n|  |  |  |  | 0.410 | 0.230 | 0.593 | 0.610 | 0.361 |\n| \u2713 |  |  |  | 0.411 | 0.225 | 0.591 | 0.637 | 0.364 |\n| \u2713 | \u2713 |  |  | **0.407** | 0.228 | 0.600 | 0.635 | 0.653 |\n| \u2713 | \u2713 | \u2713 |  | 0.408 | 0.231 | **0.618** | 0.648 | 0.718 |\n| \u2713 | \u2713 | \u2713 | \u2713 | **0.407** | **0.235** | **0.618** | **0.651** | **0.727** |", "caption": "(b) Comparison on Manga109 evaluation set.", "description": "Quantitative comparison results on Manga109 evaluation dataset using automatic metrics.  The table shows the performance of different models (including DiffSensei and baselines) across multiple metrics: FID (Fr\u00e9chet Inception Distance), CLIP (CLIP image-text similarity), DINO-I (DINO image similarity), DINO-C (DINO character image similarity), and F1 score (dialog bounding box F1 score).  These metrics evaluate the quality of generated manga images and their alignment with the input text and character consistency.  Manga109 contains unseen characters for the models during training, testing their generalization capabilities.", "section": "5. Experiments"}, {"content": "| Rate | FID \u2193 | CLIP \u2191 | DINO-I \u2191 | DINO-C \u2191 | F1 score \u2191 |\n|---|---|---|---|---|---| \n| 0.0 | 0.408 | 0.233 | 0.615 | 0.646 | 0.718 |\n| 0.5 | **0.407** | **0.235** | **0.618** | **0.651** | 0.727 |\n| 1.0 | **0.407** | 0.233 | 0.610 | 0.644 | **0.729** |", "caption": "Table 3: Ablation study. CM is character masked attention injection. DM is dialog masked encoding. Magi means using Magi\u00a0[30] image encoder. MLLM means using MLLM for stage 2 training.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different components in the DiffSensei model. The study systematically removes or replaces each component (character masked attention injection (CM), dialog masked encoding (DM), Magi image encoder, and MLLM for stage 2 training) and measures the performance on metrics such as FID, CLIP, DINO-I, DINO-C, and F1 score.  This helps assess the contribution of each component to the overall model performance and understand their relative importance in customized manga generation.", "section": "5.4 Ablation Study"}, {"content": "| \u03b2 | FID \u2193 | CLIP \u2191 | DINO-I \u2191 | DINO-C \u2191 | F1 score \u2191 |\n|---|---|---|---|---|---| \n| 0.0 | 0.408 | 0.231 | 0.618 | 0.648 | 0.718 |\n| 0.2 | 0.407 | 0.231 | 0.620 | 0.653 | 0.722 |\n| 0.4 | 0.407 | 0.235 | 0.618 | 0.651 | 0.727 |\n| 0.6 | 0.406 | 0.237 | 0.608 | 0.637 | 0.728 |\n| 0.8 | 0.407 | 0.237 | 0.604 | 0.629 | 0.727 |\n| 1.0 | 0.407 | 0.236 | 0.601 | 0.618 | 0.731 |", "caption": "Table 4: Quantitative ablations. The first scores are bold. The second scores are underlined.", "description": "This table presents the results of ablation studies performed on the DiffSensei model.  It shows how the model's performance changes when different components (character masked attention, dialog masked encoding, Magi image encoder, and the MLLM) are removed.  The first row for each ablation shows the original performance of the model with all components included (in bold).  The second row shows the model's performance after removing one component (underlined).  The metrics reported are FID, CLIP, DINO-I, DINO-C, and F1 score, which provide a quantitative assessment of the model's performance on various aspects of manga generation.", "section": "5.4. Ablation Study"}]