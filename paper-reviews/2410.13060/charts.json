[{"figure_path": "2410.13060/charts/charts_2_0.png", "caption": "Figure 1: Latency and communication savings through nonlinearity and FLOPs reduction steps when AERO is applied on GPT-2, and trained from scratch on CodeParrot dataset. Further, we benchmark AERO against the SOTA He & Hofmann (2024) at iso-latency points. See Table 4 for a detail analysis.", "description": "The chart displays latency and communication savings achieved through nonlinearity reduction, FLOPs reduction, and entropy regularization when applying AERO to the GPT-2 model, also benchmarking against the state-of-the-art.", "section": "Results"}, {"figure_path": "2410.13060/charts/charts_4_0.png", "caption": "Figure 3: (a) The fraction of attention heads distributed across different entropy ranges, and (b) evaluation loss for GPT-2 (small) models with fewer nonlinearities, when trained from scratch on CodeParrot dataset.", "description": "Figure 3 shows the headwise entropy distribution and the loss curve for GPT-2 small models trained with different nonlinearity configurations.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/charts/charts_5_0.png", "caption": "Figure 1: Latency and communication savings through nonlinearity and FLOPs reduction steps when AERO is applied on GPT-2, and trained from scratch on CodeParrot dataset. Further, we benchmark AERO against the SOTA He & Hofmann (2024) at iso-latency points. See Table 4 for a detail analysis.", "description": "The chart displays the latency and communication reductions achieved by AERO at different stages of optimization, benchmarked against the state-of-the-art.", "section": "Results and implications"}, {"figure_path": "2410.13060/charts/charts_6_0.png", "caption": "Figure 1: Latency and communication savings through nonlinearity and FLOPs reduction steps when AERO is applied on GPT-2, and trained from scratch on CodeParrot dataset. Further, we benchmark AERO against the SOTA He & Hofmann (2024) at iso-latency points. See Table 4 for a detail analysis.", "description": "The chart displays latency and communication savings achieved by applying AERO to GPT-2, and compares its performance against the state-of-the-art.", "section": "Results and implications"}, {"figure_path": "2410.13060/charts/charts_9_0.png", "caption": "Figure 9: While normalizing weights or scaling outputs in the FFN of Softmax-only (GPT-2) model prevents entropy collapse, our proposed entropy regularization effectively mitigates entropic overload.", "description": "The chart displays the percentage of attention heads distributed across different entropy ranges for various Softmax-only model configurations, highlighting the effectiveness of entropy regularization in preventing entropic overload.", "section": "Results"}, {"figure_path": "2410.13060/charts/charts_10_0.png", "caption": "Figure 1: Latency and communication savings through nonlinearity and FLOPs reduction steps when AERO is applied on GPT-2, and trained from scratch on CodeParrot dataset. Further, we benchmark AERO against the SOTA He & Hofmann (2024) at iso-latency points. See Table 4 for a detail analysis.", "description": "The chart displays the latency and communication savings achieved by applying AERO to GPT-2, trained on the CodeParrot dataset, and compares its performance against the state-of-the-art at iso-latency points.", "section": "Results and implications"}, {"figure_path": "2410.13060/charts/charts_10_1.png", "caption": "Figure 10: Headwise entropy distribution in the SM(t) + ScFuFFN GPT-2 model (L=12, H=12, d=768) when entropy regularization is applied with varying threshold margin, controlled by hyperparameter \u03b3.", "description": "The figure shows the headwise entropy distribution in the Softmax-only GPT-2 model with entropy regularization applied using varying threshold margins controlled by the hyperparameter \u03b3.", "section": "4.3 Entropy Regularization"}, {"figure_path": "2410.13060/charts/charts_23_0.png", "caption": "Figure 1: Latency and communication savings through nonlinearity and FLOPs reduction steps when AERO is applied on GPT-2, and trained from scratch on CodeParrot dataset. Further, we benchmark AERO against the SOTA He & Hofmann (2024) at iso-latency points. See Table 4 for a detail analysis.", "description": "The chart illustrates the latency and communication savings achieved by applying AERO to GPT-2, along with a comparison against state-of-the-art methods at iso-latency points.", "section": "Results and implications"}, {"figure_path": "2410.13060/charts/charts_24_0.png", "caption": "Figure 12: Evolution of Layerwise entropy when GPT-2 (L=12, H=12, d=768) models with various nonlinearity configurations are trained from scratch on CodeParrot dataset.", "description": "The chart displays the evolution of layerwise entropy across different GPT-2 model configurations with varying nonlinearities during training.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/charts/charts_24_1.png", "caption": "Figure 12: Evolution of Layerwise entropy when GPT-2 (L=12, H=12, d=768) models with various nonlinearity configurations are trained from scratch on CodeParrot dataset.", "description": "The chart displays the evolution of layer-wise entropy in GPT-2 models with different nonlinearity configurations during training on the CodeParrot dataset.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/charts/charts_24_2.png", "caption": "Figure 12: Evolution of Layerwise entropy when GPT-2 (L=12, H=12, d=768) models with various nonlinearity configurations are trained from scratch on CodeParrot dataset.", "description": "The chart displays the layerwise mean entropy evolution across various GPT-2 model configurations trained on the CodeParrot dataset during different stages of training.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/charts/charts_24_3.png", "caption": "Figure 12: Evolution of Layerwise entropy when GPT-2 (L=12, H=12, d=768) models with various nonlinearity configurations are trained from scratch on CodeParrot dataset.", "description": "The chart displays the evolution of layerwise entropy for different GPT-2 model configurations during training on the CodeParrot dataset.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/charts/charts_24_4.png", "caption": "Figure 12: Evolution of Layerwise entropy when GPT-2 (L=12, H=12, d=768) models with various nonlinearity configurations are trained from scratch on CodeParrot dataset.", "description": "The chart displays the evolution of layerwise entropy across different GPT-2 model configurations with varying nonlinearities during training on the CodeParrot dataset.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/charts/charts_24_5.png", "caption": "Figure 12: Evolution of Layerwise entropy when GPT-2 (L=12, H=12, d=768) models with various nonlinearity configurations are trained from scratch on CodeParrot dataset.", "description": "The chart displays the evolution of layerwise entropy across different GPT-2 model configurations with varying nonlinearities during training.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/charts/charts_25_0.png", "caption": "Figure 1: Latency and communication savings through nonlinearity and FLOPs reduction steps when AERO is applied on GPT-2, and trained from scratch on CodeParrot dataset. Further, we benchmark AERO against the SOTA He & Hofmann (2024) at iso-latency points. See Table 4 for a detail analysis.", "description": "The chart displays the latency and communication savings achieved by applying AERO to GPT-2 model, comparing its performance against the state-of-the-art methods at iso-latency points.", "section": "Results and implications"}, {"figure_path": "2410.13060/charts/charts_25_1.png", "caption": "Figure 1: Latency and communication savings through nonlinearity and FLOPs reduction steps when AERO is applied on GPT-2, and trained from scratch on CodeParrot dataset. Further, we benchmark AERO against the SOTA He & Hofmann (2024) at iso-latency points. See Table 4 for a detail analysis.", "description": "The chart displays the latency and communication savings achieved by applying AERO to GPT-2 model trained on CodeParrot dataset, comparing against the state-of-the-art He & Hofmann (2024) at iso-latency points.", "section": "Results and implications"}, {"figure_path": "2410.13060/charts/charts_27_0.png", "caption": "Figure 1: Latency and communication savings through nonlinearity and FLOPs reduction steps when AERO is applied on GPT-2, and trained from scratch on CodeParrot dataset. Further, we benchmark AERO against the SOTA He & Hofmann (2024) at iso-latency points. See Table 4 for a detail analysis.", "description": "The chart displays the latency and communication reduction achieved by applying AERO on GPT-2 model trained on CodeParrot dataset, along with a comparison against the state-of-the-art.", "section": "Results and implications"}, {"figure_path": "2410.13060/charts/charts_28_0.png", "caption": "Figure 16: Head-wise entropy distribution in the Softmax-only GPT-2 model (L=12, H=12, d=768) with earlier FFNs intact and deeper FFNs pruned, trained from scratch on the CodeParrot dataset.", "description": "The chart displays the headwise entropy distribution in the Softmax-only GPT-2 model with varying numbers of pruned deeper FFNs, illustrating the impact of FFN pruning on entropy.", "section": "D.7 TRAINING DYNAMICS IN SOFTMAX-ONLY MODELS WITH FEWER FFNS"}, {"figure_path": "2410.13060/charts/charts_29_0.png", "caption": "Figure 6: Training collapses in softmax-only GPT-2 model.", "description": "The chart displays the rapid increase in NaN values and the entropy distribution across layers of a Softmax-only GPT-2 model during training, highlighting training instability issues.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/charts/charts_29_1.png", "caption": "Figure 12: Evolution of Layerwise entropy when GPT-2 (L=12, H=12, d=768) models with various nonlinearity configurations are trained from scratch on CodeParrot dataset.", "description": "The chart displays the evolution of layer-wise entropy for different GPT-2 model configurations during training, highlighting the impact of various nonlinearities.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/charts/charts_29_2.png", "caption": "Figure 12: Evolution of Layerwise entropy when GPT-2 (L=12, H=12, d=768) models with various nonlinearity configurations are trained from scratch on CodeParrot dataset.", "description": "The chart displays the evolution of layerwise entropy across various GPT-2 model configurations with different nonlinearities during training.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/charts/charts_29_3.png", "caption": "Figure 12: Evolution of Layerwise entropy when GPT-2 (L=12, H=12, d=768) models with various nonlinearity configurations are trained from scratch on CodeParrot dataset.", "description": "The chart displays the evolution of layerwise entropy across different GPT-2 model configurations during training.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/charts/charts_29_4.png", "caption": "Figure 1: Latency and communication savings through nonlinearity and FLOPs reduction steps when AERO is applied on GPT-2, and trained from scratch on CodeParrot dataset. Further, we benchmark AERO against the SOTA He & Hofmann (2024) at iso-latency points. See Table 4 for a detail analysis.", "description": "The chart displays the latency and communication reductions achieved by AERO at different stages of optimization compared to the state-of-the-art.", "section": "Results and Implications"}, {"figure_path": "2410.13060/charts/charts_30_0.png", "caption": "Figure 18. Headwise entropy distribution in the SM(t) + ScFuFFN GPT-2 model (L=12, H=12, d=768) when entropy regularization is applied with varying threshold margin, controlled by hyperparameter \u03b3.", "description": "The chart shows the impact of the hyperparameter \u03b3 on the headwise entropy distribution in the Softmax-only GPT-2 model with entropy regularization.", "section": "4.3 Entropy Regularization"}, {"figure_path": "2410.13060/charts/charts_30_1.png", "caption": "Figure 12: Evolution of Layerwise entropy when GPT-2 (L=12, H=12, d=768) models with various nonlinearity configurations are trained from scratch on CodeParrot dataset.", "description": "The chart displays the evolution of layer-wise entropy in GPT-2 models with different nonlinearities during training on the CodeParrot dataset.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/charts/charts_30_2.png", "caption": "Figure 12: Evolution of Layerwise entropy when GPT-2 (L=12, H=12, d=768) models with various nonlinearity configurations are trained from scratch on CodeParrot dataset.", "description": "The chart displays the evolution of layer-wise entropy across different GPT-2 model configurations trained without non-linearities on the CodeParrot dataset.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/charts/charts_30_3.png", "caption": "Figure 12: Evolution of Layerwise entropy when GPT-2 (L=12, H=12, d=768) models with various nonlinearity configurations are trained from scratch on CodeParrot dataset.", "description": "The chart displays the mean entropy evolution across layers of GPT-2 models trained from scratch with different nonlinearity configurations on the CodeParrot dataset.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/charts/charts_30_4.png", "caption": "Figure 12: Evolution of Layerwise entropy when GPT-2 (L=12, H=12, d=768) models with various nonlinearity configurations are trained from scratch on CodeParrot dataset.", "description": "The chart displays the evolution of layerwise entropy across different GPT-2 model configurations with varying nonlinearities during training.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/charts/charts_32_0.png", "caption": "Figure 1: Latency and communication savings through nonlinearity and FLOPs reduction steps when AERO is applied on GPT-2, and trained from scratch on CodeParrot dataset. Further, we benchmark AERO against the SOTA He & Hofmann (2024) at iso-latency points. See Table 4 for a detail analysis.", "description": "The chart displays the latency and communication reduction achieved by AERO across various stages of optimization, comparing its performance against the state-of-the-art.", "section": "Results and implications"}, {"figure_path": "2410.13060/charts/charts_33_0.png", "caption": "Figure 21: FLOPs breakdown in Pythia models for a single forward pass: Similar to GPT-2 models (see Figure 20), FLOPs are dominated by FFN operations up to a context length of 4K, except for smaller models where FFN operations dominate up to 2K (Percentage on top of each bar represents the proportion of FFN FLOPs).", "description": "The chart displays the distribution of FLOPs between attention and FFN sub-blocks in Pythia models across different context lengths.", "section": "E.2 DISTRIBUTION OF FLOPS IN PYTHIA MODELS"}]