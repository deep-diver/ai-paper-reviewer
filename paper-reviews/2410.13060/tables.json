[{"figure_path": "2410.13060/tables/table_4_0.html", "caption": "Table 4: Results, and comparison against SOTA (He & Hofmann, 2024), when GPT-2 (L=12, H=12, d=768) model is trained from scratch on CodeParrot (Face) dataset with context length 128.", "description": "Table 4 presents the results of applying the AERO framework to a GPT-2 model and compares its performance against the state-of-the-art (SOTA) in terms of perplexity, number of nonlinear operations, FLOPs, communication, and latency.", "section": "4 AERO"}, {"figure_path": "2410.13060/tables/table_6_0.html", "caption": "Table 4: Results, and comparison against SOTA (He & Hofmann, 2024), when GPT-2 (L=12, H=12, d=768) model is trained from scratch on CodeParrot (Face) dataset with context length 128.", "description": "Table 4 presents the results of applying the AERO framework to the GPT-2 language model, comparing its performance against the state-of-the-art (SOTA) in terms of perplexity (PPL), number of nonlinear operations, FLOPs, communication (Comm.), and latency (Lat.).", "section": "4 AERO"}, {"figure_path": "2410.13060/tables/table_10_0.html", "caption": "Table 4: Results, and comparison against SOTA (He & Hofmann, 2024), when GPT-2 (L=12, H=12, d=768) model is trained from scratch on CodeParrot (Face) dataset with context length 128.", "description": "Table 4 presents the results of applying the AERO framework to a GPT-2 model, comparing its performance against the state-of-the-art (SOTA) in terms of perplexity (PPL), number of nonlinear operations, FLOPs, communication, and latency.", "section": "4 AERO"}, {"figure_path": "2410.13060/tables/table_20_0.html", "caption": "Table 4: Results, and comparison against SOTA (He & Hofmann, 2024), when GPT-2 (L=12, H=12, d=768) model is trained from scratch on CodeParrot (Face) dataset with context length 128.", "description": "Table 4 presents the results achieved by applying AERO on GPT-2 and compares its performance against the state-of-the-art (SOTA) in terms of perplexity (PPL), number of non-linear operations, FLOPs count, communication, and latency.", "section": "4 AERO"}, {"figure_path": "2410.13060/tables/table_26_0.html", "caption": "Table 5: Results, and comparison against SOTA (He & Hofmann, 2024), when GPT-2 (L=12, H=12, d=768) model is trained from scratch on CodeParrot (Face) dataset with context length 256. NaNs indicate training instability in SOTA.", "description": "Table 5 presents a detailed analysis of latency and communication savings achieved by applying AERO to GPT-2 with 256 input tokens, also comparing its performance against SOTA.", "section": "D.3 Additional Results for Latency and Communication Savings using AERO"}, {"figure_path": "2410.13060/tables/table_26_1.html", "caption": "Table 4: Results, and comparison against SOTA (He & Hofmann, 2024), when GPT-2 (L=12, H=12, d=768) model is trained from scratch on CodeParrot (Face) dataset with context length 128.", "description": "Table 4 presents the results of the AERO framework on GPT-2 model with 12 layers, comparing its performance against the state-of-the-art (SOTA) in terms of perplexity (PPL), number of non-linear operations, FLOPs, communication, and latency.", "section": "4 AERO"}, {"figure_path": "2410.13060/tables/table_27_0.html", "caption": "Table 7: Results, and comparison against SOTA (He & Hofmann, 2024), when GPT-2 (L=12, H=12, d=768) model is trained from scratch on Languini (Stani\u0107 et al., 2023) dataset with context length 512. NaNs indicate training instability in SOTA.", "description": "Table 7 presents the results and comparisons of AERO against SOTA on Languini dataset with context length of 512, showing the performance in terms of perplexity, the number of non-linear operations, FLOPs, communication, and latency.", "section": "D.4 Results on Languini Dataset"}, {"figure_path": "2410.13060/tables/table_27_1.html", "caption": "Table 4: Results, and comparison against SOTA (He & Hofmann, 2024), when GPT-2 (L=12, H=12, d=768) model is trained from scratch on CodeParrot (Face) dataset with context length 128.", "description": "Table 4 presents the results of applying AERO optimization techniques to a GPT-2 language model, showing perplexity scores, number of nonlinear operations, FLOPs, communication, and latency, and comparing the results to the state-of-the-art (SOTA).", "section": "4 AERO"}, {"figure_path": "2410.13060/tables/table_28_0.html", "caption": "Table 4: Results, and comparison against SOTA (He & Hofmann, 2024), when GPT-2 (L=12, H=12, d=768) model is trained from scratch on CodeParrot (Face) dataset with context length 128.", "description": "Table 4 presents the results and comparisons of AERO against the state-of-the-art methods on GPT-2 models with 12 layers, 12 heads, and 768 dimensions, trained from scratch on the CodeParrot dataset with a context length of 128 tokens.", "section": "4 AERO"}]