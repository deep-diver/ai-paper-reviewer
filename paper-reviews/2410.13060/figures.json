[{"figure_path": "2410.13060/figures/figures_3_0.png", "caption": "Figure 2: An illustration of threat model and cryptographic protocols used for LLM private inference.", "description": "The figure illustrates the threat model and cryptographic protocols used for large language model private inference, showing the interaction between the client and server.", "section": "2 PRELIMINARIES"}, {"figure_path": "2410.13060/figures/figures_5_0.png", "caption": "Figure 4: Learnable negative slope for leaky ReLU in the FFN of LN-free GPT-2. (a) Layerwise slopes and (b) global slope, both converge toward zero during training, indicating a preference for zero negative slope in LN-free architectures.", "description": "Figure 4 shows the layerwise and global learnable negative slopes for leaky ReLU in the feed-forward network (FFN) of a LayerNorm-free GPT-2 model during training, demonstrating a convergence towards zero.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/figures/figures_7_0.png", "caption": "Figure 7: Mitigating entropy collapse in the deeper layers of a softmax-only GPT-2 model by employing weight or spectral normalization in FFN, or by appropriately scaling FFN block outputs.", "description": "The figure shows the layerwise mean entropy across different layers in a softmax only GPT-2 model trained with weight normalization, spectral normalization, or scaled FFN outputs.", "section": "3 REMOVING NONLINEARITY IN TRANSFORMER-BASED LLMS"}, {"figure_path": "2410.13060/figures/figures_8_0.png", "caption": "Figure 8: Overview of the proposed AERO method for reducing nonlinearities and FLOPs in transformer-based LLMs for efficient PI. The bottom of the figure shows the evolution of entropy in the attention mechanism and its distribution across attention heads.", "description": "Figure 8 illustrates the four-step AERO framework for optimizing transformer-based LLMs for private inference by reducing nonlinearities and FLOPs, and incorporating entropy regularization.", "section": "4 AERO"}, {"figure_path": "2410.13060/figures/figures_21_0.png", "caption": "Figure 1: Latency and communication savings through nonlinearity and FLOPs reduction steps when AERO is applied on GPT-2, and trained from scratch on CodeParrot dataset. Further, we benchmark AERO against the SOTA He & Hofmann (2024) at iso-latency points. See Table 4 for a detail analysis.", "description": "Figure 1 shows the latency and communication savings achieved by applying AERO to GPT-2, trained from scratch on the CodeParrot dataset, and compared against the state-of-the-art.", "section": "Results"}]