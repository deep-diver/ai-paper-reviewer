[{"figure_path": "2410.13276/figures/figures_4_0.png", "caption": "Figure 1: SeerAttention uses a learning-based approach to exploit attention sparsity of LLMs, applicable in both post-training and fine-tuning stages. By incorporating SeerAttention with YaRN (Peng et al., 2024) to extend a Llama-3-8B model from 8k to 32k context length, the loss curves for 50% to 90% sparsity are nearly identical to the dense YaRN baseline (a); For test perplexity, 50% sparsity achieves near-lossless performance, and even at 90% sparsity, the loss remains minimal (b); SeerAttention achieves up to 5.67x inference speedup at 90% sparsity over FlashAttention-2 (Dao, 2023);", "description": "Figure 1 shows the results of applying SeerAttention to extend the context length of a language model, demonstrating near-lossless performance with high sparsity and significant speedup.", "section": "ABSTRACT"}, {"figure_path": "2410.13276/figures/figures_5_0.png", "caption": "Figure 1: SeerAttention uses a learning-based approach to exploit attention sparsity of LLMs, applicable in both post-training and fine-tuning stages. By incorporating SeerAttention with YaRN (Peng et al., 2024) to extend a Llama-3-8B model from 8k to 32k context length, the loss curves for 50% to 90% sparsity are nearly identical to the dense YaRN baseline (a); For test perplexity, 50% sparsity achieves near-lossless performance, and even at 90% sparsity, the loss remains minimal (b); SeerAttention achieves up to 5.67x inference speedup at 90% sparsity over FlashAttention-2 (Dao, 2023);", "description": "This figure shows the results of applying SeerAttention with YaRN to extend a Llama-3-8B model, demonstrating near-lossless performance with high sparsity and significant speedup over FlashAttention-2.", "section": "ABSTRACT"}]