[{"figure_path": "2410.13276/charts/charts_1_0.png", "caption": "Figure 1: SeerAttention uses a learning-based approach to exploit attention sparsity of LLMs, applicable in both post-training and fine-tuning stages. By incorporating SeerAttention with YaRN (Peng et al., 2024) to extend a Llama-3-8B model from 8k to 32k context length, the loss curves for 50% to 90% sparsity are nearly identical to the dense YaRN baseline (a); For test perplexity, 50% sparsity achieves near-lossless performance, and even at 90% sparsity, the loss remains minimal (b); SeerAttention achieves up to 5.67x inference speedup at 90% sparsity over FlashAttention-2 (Dao, 2023);", "description": "Figure 1 shows that SeerAttention, when used with YaRN to extend a Llama-3-8B model, achieves near-lossless performance with 50% sparsity and minimal loss even at 90% sparsity in both fine-tuning loss and test perplexity, while also offering significant inference speedup.", "section": "ABSTRACT"}, {"figure_path": "2410.13276/charts/charts_6_0.png", "caption": "Figure 4: Perplexity results on Proof-pile across various context lengths and sparsity ratios. Note that results on various sparsity ratios comes from the same trained AttnGates by only adjusting the Top-k ratios. Longer context sizes allow for higher sparsity with minimal performance loss.", "description": "Figure 4 shows that SeerAttention only slightly increases perplexity as the sparsity ratio increases across different context lengths, compared to full attention for both Llama-3.1-8B and Mistral-7B-v0.3 models.", "section": "5.1 ACCURACY OF POST-TRAINING"}, {"figure_path": "2410.13276/charts/charts_8_0.png", "caption": "Figure 5: SeerAttention time breakdown compared to FlashAttention-2. At sequence length 128k with 90% sparsity ratio, SeerAttention speeds up attention computation by 5.47x over FlashAttention-2.", "description": "The chart shows the kernel-level latency breakdown of SeerAttention compared to FlashAttention-2 at different sequence lengths and sparsity levels, demonstrating minimal overhead from the AttnGate and Top-k operations and significant speedup from block-sparse FlashAttention.", "section": "5.3.1 KERNEL EVALUATION"}, {"figure_path": "2410.13276/charts/charts_9_0.png", "caption": "Figure 6: SeerAttention block sparse FlashAttention inference kernel speedup.", "description": "The chart shows the speedup of SeerAttention's block-sparse FlashAttention kernel compared to FlashAttention-2 and other sparse attention methods (MoA and MInference) across various sparsity ratios and sequence lengths.", "section": "5.3 EFFICIENCY EVALUATION"}, {"figure_path": "2410.13276/charts/charts_9_1.png", "caption": "Figure 1: SeerAttention uses a learning-based approach to exploit attention sparsity of LLMs, applicable in both post-training and fine-tuning stages. By incorporating SeerAttention with YaRN (Peng et al., 2024) to extend a Llama-3-8B model from 8k to 32k context length, the loss curves for 50% to 90% sparsity are nearly identical to the dense YaRN baseline (a); For test perplexity, 50% sparsity achieves near-lossless performance, and even at 90% sparsity, the loss remains minimal (b); SeerAttention achieves up to 5.67x inference speedup at 90% sparsity over FlashAttention-2 (Dao, 2023);", "description": "The chart displays the fine-tuning loss, test perplexity, and kernel speedup of SeerAttention with YaRN in comparison to baselines, showcasing its effectiveness in exploiting attention sparsity at various sparsity levels.", "section": "ABSTRACT"}, {"figure_path": "2410.13276/charts/charts_10_0.png", "caption": "Figure 8: Memory and latency of customized FlashAttention with max-pooling training kernel.", "description": "The chart compares the GPU memory usage and latency of three different FlashAttention implementations: Flash-Attn-V2, a customized version with max-pooling, and a naive manual implementation using PyTorch, across various sequence lengths.", "section": "4.2 FLASHATTENTION WITH MAX-POOLING: A CUSTOMIZED TRAINING KERNEL"}, {"figure_path": "2410.13276/charts/charts_10_1.png", "caption": "Figure 9: Perplexity with and without RoPE in AttnGate.", "description": "The chart displays the perplexity results on the PG19 dataset for different sparsity ratios (0.5, 0.6, and 0.7) with and without using the RoPE module in the AttnGate across various context lengths.", "section": "ROPE Ablation"}, {"figure_path": "2410.13276/charts/charts_10_2.png", "caption": "Figure 10: Perplexity of SeerAttention with different pooling methods.", "description": "The chart displays the perplexity of SeerAttention on the PG19 dataset at varying sparsity levels (0.5 to 0.9) with different combinations of pooling methods for Q and K tensors (average, max, and min).", "section": "6 ANALYSIS AND ABLATION"}]