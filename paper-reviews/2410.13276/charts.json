[{"figure_path": "2410.13276/charts/charts_1_0.png", "caption": "Figure 1: SeerAttention uses a learning-based approach to exploit attention sparsity of LLMs, applicable in both post-training and fine-tuning stages. By incorporating SeerAttention with YaRN (Peng et al., 2024) to extend a Llama-3-8B model from 8k to 32k context length, the loss curves for 50% to 90% sparsity are nearly identical to the dense YaRN baseline (a); For test perplexity, 50% sparsity achieves near-lossless performance, and even at 90% sparsity, the loss remains minimal (b); SeerAttention achieves up to 5.67x inference speedup at 90% sparsity over FlashAttention-2 (Dao, 2023);", "description": "The chart shows the fine-tuning loss, test perplexity, and kernel speedup of SeerAttention with varying sparsity ratios, demonstrating its effectiveness in improving efficiency and maintaining accuracy in both post-training and fine-tuning scenarios.", "section": "ABSTRACT"}, {"figure_path": "2410.13276/charts/charts_6_0.png", "caption": "Figure 4: Perplexity results on Proof-pile across various context lengths and sparsity ratios. Note that results on various sparsity ratios comes from the same trained AttnGates by only adjusting the Top-k ratios. Longer context sizes allow for higher sparsity with minimal performance loss.", "description": "Figure 4 shows that SeerAttention only slightly increases perplexity as the sparsity ratio increases, compared to full attention, and longer context lengths allow for greater sparsity with minimal accuracy degradation.", "section": "5.1 ACCURACY OF POST-TRAINING"}, {"figure_path": "2410.13276/charts/charts_8_0.png", "caption": "Figure 5: SeerAttention time breakdown compared to FlashAttention-2. At sequence length 128k with 90% sparsity ratio, SeerAttention speeds up attention computation by 5.47x over FlashAttention-2.", "description": "The chart shows the kernel-level latency breakdown of SeerAttention compared to FlashAttention-2 at various sequence lengths and sparsity ratios, demonstrating minimal overhead for AttnGate and Top-k operations and significant speedup with block-sparse attention.", "section": "5.3 Kernel Evaluation"}, {"figure_path": "2410.13276/charts/charts_9_0.png", "caption": "Figure 6: SeerAttention block sparse FlashAttention inference kernel speedup.", "description": "The chart displays the speedup of SeerAttention's block-sparse FlashAttention kernel compared to FlashAttention-2, MoA, and MInference across various sparsity ratios and sequence lengths.", "section": "5.3 EFFICIENCY EVALUATION"}, {"figure_path": "2410.13276/charts/charts_9_1.png", "caption": "Figure 1: SeerAttention uses a learning-based approach to exploit attention sparsity of LLMs, applicable in both post-training and fine-tuning stages. By incorporating SeerAttention with YaRN (Peng et al., 2024) to extend a Llama-3-8B model from 8k to 32k context length, the loss curves for 50% to 90% sparsity are nearly identical to the dense YaRN baseline (a); For test perplexity, 50% sparsity achieves near-lossless performance, and even at 90% sparsity, the loss remains minimal (b); SeerAttention achieves up to 5.67x inference speedup at 90% sparsity over FlashAttention-2 (Dao, 2023);", "description": "The chart displays the fine-tuning loss, test perplexity, and kernel speedup of SeerAttention with YaRN at various sparsity levels, demonstrating its effectiveness in both post-training and fine-tuning stages.", "section": "ABSTRACT"}, {"figure_path": "2410.13276/charts/charts_10_0.png", "caption": "Figure 8: Memory and latency of customized FlashAttention with max-pooling training kernel.", "description": "The chart compares the GPU memory usage and latency of three different FlashAttention implementations: the original FlashAttention-V2, a customized version with max-pooling for training, and a naive manual implementation using PyTorch, across varying sequence lengths.", "section": "4.2 FLASHATTENTION WITH MAX-POOLING: A CUSTOMIZED TRAINING KERNEL"}, {"figure_path": "2410.13276/charts/charts_10_1.png", "caption": "Figure 9: Perplexity with and without RoPE in AttnGate.", "description": "The chart displays the perplexity results on PG19 dataset for Llama-3.1-8B model with different context lengths and sparsity ratios, comparing the performance with and without RoPE (Rotary Position Embedding) in the AttnGate module.", "section": "ROPE Ablation"}, {"figure_path": "2410.13276/charts/charts_10_2.png", "caption": "Figure 10: Perplexity of SeerAttention with different pooling methods.", "description": "Figure 10 shows the perplexity of SeerAttention on the PG19 dataset with different combinations of pooling methods for Q and K at various sparsity levels.", "section": "Pooling Ablation"}]