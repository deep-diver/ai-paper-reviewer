{"references": [{" publication_date": "2017", "fullname_first_author": "Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduced the transformer architecture, which is the foundation for many modern LLMs.  The quadratic complexity of the self-attention mechanism in the transformer is a key challenge addressed by the SeerAttention paper, making this a highly relevant and foundational reference.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Katharopoulos", "paper_title": "Transformers are rnns: Fast autoregressive transformers with linear attention", "reason": "This paper proposes alternative attention mechanisms that reduce the computational complexity from quadratic to linear, directly addressing the efficiency challenges that SeerAttention aims to solve.  It provides a relevant comparison point for assessing the trade-offs between efficiency and accuracy.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Peng", "paper_title": "RWKV: Reinventing rnns for the transformer era", "reason": "This paper introduces a novel architecture that aims to improve efficiency without compromising accuracy by employing a recurrent network structure instead of the standard attention mechanism. It's a relevant reference demonstrating the various ongoing approaches in the field to deal with long-context limitations.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Kitaev", "paper_title": "Reformer: The efficient transformer", "reason": "The Reformer is an example of an efficient transformer architecture that reduces the computational complexity of attention. Its relevance lies in the fact that SeerAttention's approach is directly compared to architectures like the Reformer, which improves efficiency by modifying the way attention is computed.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Jiang", "paper_title": "Minference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention", "reason": "This paper proposes a dynamic sparse attention approach that is directly compared to SeerAttention in the experiments. The comparison is crucial to assess the performance improvement, showcasing the advantages of SeerAttention's approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Fu", "paper_title": "MoA: Mixture of sparse attention for automatic large language model compression", "reason": "This paper introduces a mixture of sparse attention, which is directly compared to SeerAttention. The comparison highlights SeerAttention's ability to achieve better performance while maintaining similar or higher sparsity ratios.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zhu", "paper_title": "Near-lossless acceleration of long context LLM inference with adaptive structured sparse attention", "reason": "This work is closely related to SeerAttention because it explores adaptive structured sparse attention mechanisms, providing a valuable comparison point to evaluate the innovation and effectiveness of the proposed method.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Han", "paper_title": "Hyperattention: Long-context attention in near-linear time", "reason": "This work focuses on improving the efficiency of long-context attention.  Its techniques and performance are relevant benchmarks for evaluating the effectiveness of SeerAttention in long-context settings.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Zaheer", "paper_title": "Big bird: Transformers for longer sequences", "reason": "This paper tackles the problem of long sequences in transformers. Its techniques and solutions are closely related to SeerAttention's approach to dealing with the computational challenges associated with long context lengths.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Liu", "paper_title": "Transformer acceleration with dynamic sparse attention", "reason": "This paper explores dynamic sparse attention, which is highly relevant to SeerAttention's approach. The comparison is essential for evaluating the improvement offered by SeerAttention in terms of adaptability and efficiency.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Wang", "paper_title": "Sparse attention for long sequences", "reason": "This work is important for providing insights into the properties and dynamics of sparse attention. This reference helps to establish the contextual background and relevance of the research problem SeerAttention is addressing. The insights provided are crucial for understanding the motivation behind SeerAttention.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Child", "paper_title": "Generating long sequences with sparse transformers", "reason": "This paper provides a foundational understanding of sparse transformers and their potential to enhance the efficiency of LLMs.  The techniques explored in this paper are directly relevant to SeerAttention and contextualize the specific challenges in long-sequence processing addressed by SeerAttention.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "reason": "The Mixture of Experts (MoE) model is conceptually related to the dynamic sparsity approach used in SeerAttention. This reference illustrates a prior art of using sparse computations for efficiency, providing context and comparison.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Fedus", "paper_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "reason": "This paper presents a different approach to enhance the efficiency of transformers by utilizing sparsity, hence offering a comparison point to analyze the relative merits of different techniques for sparse computation in LLMs.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Dao", "paper_title": "Flashattention: Fast and memory-efficient exact attention with io-awareness", "reason": "FlashAttention is a crucial component of SeerAttention. This paper presents the original algorithm, which is then customized in SeerAttention for training and inference.  Understanding the original work is key to comprehending SeerAttention's enhancements and efficiency gains.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Tillet", "paper_title": "Triton: an intermediate language and compiler for tiled neural network computations", "reason": "Triton is used in SeerAttention's implementation for optimization, therefore, understanding its features and capabilities is fundamental for evaluating the implementation's efficiency and evaluating its impact on the overall performance.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Su", "paper_title": "Enhanced transformer with rotary position embedding", "reason": "Rotary Position Embedding (RoPE) is a technique used in modern LLMs to encode positional information efficiently.  SeerAttention utilizes and modifies this technique, making it a critical reference for understanding the design choices and their implications.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Rae", "paper_title": "Compressive transformers for long-range sequence modelling", "reason": "This paper is important as it offers a different approach to long-range dependency modeling in transformers.  The approaches are relevant to the work on SeerAttention since both aim to resolve the limitations of standard attention mechanisms in handling long sequences.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Peng", "paper_title": "YaRN: Efficient context window extension of large language models", "reason": "This paper introduces the YaRN architecture, which is the framework used in SeerAttention's long-context experiments. Understanding YaRN is essential to fully grasp the context and significance of SeerAttention's performance in long-context settings.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper provides details about the Llama 3 model architecture, a key component of the experimental evaluation conducted within the SeerAttention paper, thereby highlighting its relevance to the work.", "section_number": 5}]}