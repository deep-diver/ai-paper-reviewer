[{"figure_path": "2410.13276/tables/table_7_0.html", "caption": "Table 1: Comparing the perplexity of SeerAttention at post-training with MoA and MInference, using the Llama-3.1-8B-Instruct model on the PG19 dataset.", "description": "Table 1 compares the perplexity of SeerAttention at post-training with MoA and MInference, using the Llama-3.1-8B-Instruct model on the PG19 dataset, showing the impact of different sparsity levels on model performance.", "section": "5.1 ACCURACY OF POST-TRAINING"}, {"figure_path": "2410.13276/tables/table_7_1.html", "caption": "Table 2: Comparing the accuracy of SeerAttention at post-training with MoA and MInference on LongBench.", "description": "Table 2 compares the accuracy of SeerAttention against MoA and MInference on the LongBench benchmark at various sparsity levels.", "section": "5.1 ACCURACY OF POST-TRAINING"}, {"figure_path": "2410.13276/tables/table_8_0.html", "caption": "Table 3: Perplexity of YaRN baseline, SeerAttention after YaRN and YaRN with SeerAttention.", "description": "Table 3 presents the perplexity scores on the PG19 and ProofPile datasets for three different model setups: YaRN baseline, SeerAttention applied after YaRN, and YaRN integrated with SeerAttention, each evaluated at various sparsity ratios (0.0, 0.5, 0.6, 0.7, 0.8, 0.9).", "section": "5.2 ACCURACY OF LONG-CONTEXT EXTENSION FINE-TUNING"}, {"figure_path": "2410.13276/tables/table_9_0.html", "caption": "Table 4: Time to First Token results (s).", "description": "The table compares the time to first token (TTFT) latency in seconds of SeerAttention against FlashAttention-2, MoA, and MInference across different context lengths and sparsity ratios.", "section": "5.3 EFFICIENCY EVALUATION"}]