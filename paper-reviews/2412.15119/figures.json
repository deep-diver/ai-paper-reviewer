[{"figure_path": "https://arxiv.org/html/2412.15119/x1.png", "caption": "Figure 1: Comparison of different parallel generation strategies. Both strategies generate initial tokens [1,2,3,4] sequentially then generate multiple tokens in parallel per step, following the order [5a-5d] to [6a-6d] to [7a-7d], etc. (a) Our approach generates weakly dependent tokens across non-local regions in parallel, preserving coherent patterns and local details. (b) The naive method generates strongly dependent tokens within local regions simultaneously, while independent sampling for strongly correlated tokens can cause inconsistent generation and disrupted patterns, such as distorted tiger faces and fragmented zebra stripes.", "description": "Figure 1 illustrates two different parallel generation strategies for autoregressive visual models. Both methods start by generating the first four tokens sequentially. Then, they proceed in parallel, generating groups of tokens in a specific order ([5a-5d], [6a-6d], etc.).  The key difference lies in the selection of tokens for parallel processing.  (a) shows the proposed approach, where weakly dependent tokens from non-adjacent spatial regions are processed concurrently, preserving image coherence and detail. (b) shows a naive approach, which processes strongly dependent tokens from the same region in parallel.  In this case, independent sampling can lead to inconsistencies and visual artifacts, as evidenced by the distorted tiger face and fragmented zebra stripes shown in the figure.", "section": "3.1. Token Dependencies and Parallel Generation"}, {"figure_path": "https://arxiv.org/html/2412.15119/x2.png", "caption": "Figure 2: Visualization comparison of our parallel generation and traditional autoregressive generation (LlamaGen\u00a0[47]). Our approach (PAR) achieves 3.6-9.5\u00d7\\times\u00d7 speedup over LlamaGen with comparable quality, reducing the generation time from 12.41s to 3.46s (PAR-4\u00d7\\times\u00d7) and 1.31s (PAR-16\u00d7\\times\u00d7) per image. Time measurements are conducted with a batch size of 1 on a single A100 GPU.", "description": "Figure 2 presents a visual comparison of image generation results using the proposed parallelized autoregressive method (PAR) and a traditional autoregressive approach (LlamaGen).  The figure shows generated images side-by-side, highlighting the comparable visual quality achieved by both methods.  Quantitatively, PAR demonstrates significant speed improvements, achieving a 3.6 to 9.5 times speedup over LlamaGen.  Specifically, generation time is reduced from 12.41 seconds per image for LlamaGen to 3.46 seconds (PAR-4x) and 1.31 seconds (PAR-16x) for the proposed method. All timings were measured using a batch size of 1 on a single A100 GPU. The improvements showcase the efficiency gains from PAR's parallelization strategy without compromising image quality.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.15119/x3.png", "caption": "Figure 3: Illustration of our non-local parallel generation process. Stage 1: sequential generation of initial tokens (1-4) for each region (separated by dotted lines) to establish global structure. Stage 2: parallel generation at aligned positions across different regions (e.g., 5a-5d), then moving to next aligned positions (6a-6d, 7a-7d, etc.) for parallel generation. Same numbers indicate tokens generated in the same step, and letter suffix (a,b,c,d) denotes different regions .", "description": "This figure illustrates the two-stage process of the proposed non-local parallel generation method. Stage 1 shows the sequential generation of initial tokens (1-4) in each region, which establishes the global image structure. Stage 2 demonstrates parallel generation of tokens at aligned positions across different regions.  For instance, tokens 5a-5d are generated concurrently, followed by tokens 6a-6d, 7a-7d and so forth. The same numbers denote tokens generated in the same step, while the letter suffixes (a, b, c, d) indicate different regions. This approach aims to balance the speed benefits of parallelism with the accuracy of autoregressive modeling.", "section": "3.2. Non-Local Parallel Generation"}, {"figure_path": "https://arxiv.org/html/2412.15119/x4.png", "caption": "Figure 4: Overview of our parallel autoregressive generation framework. (a) Model implementation. The model first generates initial tokens sequentially [1,2,3,4], then uses learnable tokens [M1,M2,M3] to help transition into parallel prediction mode.\n(b) Comparison of visible context between our parallel prediction approach (left) and traditional single-token prediction (right). The colored cells indicate available context during generation. In traditional AR, when predicting token 6\u2062d6\ud835\udc516d6 italic_d, the model can access all previous tokens including 6\u2062a\u22126\u2062c6\ud835\udc4e6\ud835\udc506a-6c6 italic_a - 6 italic_c. Without full attention, our parallel approach would limit each token (e.g., 6\u2062b6\ud835\udc4f6b6 italic_b) to only see tokens up to the same position in the previous group (e.g., up to 5\u2062b5\ud835\udc4f5b5 italic_b). We enable group-wise full attention to allow access to the entire previous group.", "description": "Figure 4 illustrates the proposed parallel autoregressive generation framework.  Panel (a) shows the model's architecture.  It first generates initial tokens sequentially (1, 2, 3, 4), then uses special \"learnable tokens\" (M1, M2, M3) to smoothly transition to a parallel prediction mode for the remaining tokens.  Panel (b) compares the visible context during generation between the proposed method and the standard autoregressive method.  The standard approach allows access to all previous tokens when predicting a new one (e.g., when predicting 6d, it can see 6a-6c). In contrast, a naive parallel method would limit a token's visibility to tokens at the same position in the previous group (e.g., when predicting 6b, only 5b would be visible). The proposed method uses group-wise full attention to allow each parallel token to see the entire previous group, overcoming this limitation.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.15119/x5.png", "caption": "Figure 5: Qualitative comparison of parallel generation strategies. Top: Our method with sequential initial tokens followed by parallel distant token prediction produces high-quality and coherent images. Middle: Direct parallel prediction without sequential initial tokens leads to inconsistent global structures. Bottom: Parallel prediction of adjacent tokens results in distorted local patterns and broken details.", "description": "This figure compares three different parallel image generation strategies. The top row shows the results of the proposed method, which generates initial tokens sequentially to establish a global structure, followed by parallel generation of distant, weakly dependent tokens. This approach produces high-quality and coherent images. The middle row demonstrates the results of direct parallel prediction without sequential initialization.  This leads to inconsistent global structures, such as repeated patterns or incoherent patches. The bottom row illustrates the outcome of parallel prediction of adjacent tokens.  Due to the strong dependencies between adjacent tokens, independent sampling leads to distorted local patterns and broken details, resulting in poor image quality.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.15119/x6.png", "caption": "Figure 6: Additional image generation results of PAR-4\u00d7\\times\u00d7 across different ImageNet\u00a0[9] categories.", "description": "This figure displays a collection of images generated using the PAR-4x model.  The images showcase the model's ability to generate diverse and visually appealing images across a range of ImageNet object categories.  Each image represents a different category, highlighting the model's versatility and capability for high-quality image synthesis.", "section": "4.2.1. Image Generation"}, {"figure_path": "https://arxiv.org/html/2412.15119/x7.png", "caption": "Figure 7: Additional image generation results of PAR-16\u00d7\\times\u00d7 across different ImageNet\u00a0[9] categories.", "description": "This figure displays a grid of images generated using the PAR-16x model, demonstrating its ability to produce high-quality images across various categories from the ImageNet dataset.  Each image showcases a different object or scene, highlighting the model's versatility in generating diverse visual content. The large-scale parallelization of the PAR-16x model is exemplified in the speed and efficiency with which these images were generated.", "section": "4.2. Main Results"}, {"figure_path": "https://arxiv.org/html/2412.15119/x8.png", "caption": "Figure 8: Video generation results on UCF-101\u00a0[44]. Each row shows sampled frames from a 17-frame sequence at 128\u00d7128 resolution, generated by PAR-1\u00d7\\times\u00d7, PAR-4\u00d7\\times\u00d7, and PAR-16\u00d7\\times\u00d7 respectively across different action categories.", "description": "Figure 8 showcases video generation results from the UCF-101 dataset [44]. Each row displays sample frames extracted from a 17-frame video sequence. The videos have a resolution of 128x128 pixels.  Three different models, PAR-1x, PAR-4x, and PAR-16x, are shown, representing varying degrees of parallelization in the video generation process. The results demonstrate the capability of each model to generate videos from different action categories within the UCF-101 dataset.", "section": "4.2.2. Video Generation"}, {"figure_path": "https://arxiv.org/html/2412.15119/x9.png", "caption": "Figure 9: Visualization of token conditional entropy maps. Each map shows the conditional entropy of all tokens when conditioned on a reference token (blue square). Darker red indicates lower conditional entropy and thus stronger dependency with the reference token. The visualization shows that tokens exhibit strong dependencies with their spatial neighbors and weak dependencies with distant regions.", "description": "This figure visualizes the conditional entropy between tokens in an image. Each image shows the conditional entropy of all tokens given a single reference token (shown as a blue square). The color intensity represents the strength of the dependency; darker red indicates a lower conditional entropy, representing a stronger dependency with the reference token.  The visualization demonstrates that tokens have strong dependencies with their spatial neighbors, while dependencies weaken significantly with distance.", "section": "C. Analysis of Visual Token Dependencies"}, {"figure_path": "https://arxiv.org/html/2412.15119/x10.png", "caption": "Figure 10: Conditional entropy differences between parallel and sequential generation in different orders.\n(a)(d) show parallel (4 tokens) generation strategies and (b)(e) show sequential generation strategies for our proposed order and raster scan order respectively. Numbers indicate generation step in each order. (c)(f) visualize the conditional entropy increase when switching from sequential to parallel generation for each order, where darker red indicates larger entropy increase and thus higher prediction difficulty. Both orders generate the first four tokens sequentially (shown as white regions in entropy maps). Our proposed order that generates tokens from different spatial blocks in parallel shows smaller entropy increases compared to raster scan order that generates consecutive tokens simultaneously, indicating parallel generation across spatial blocks introduces less prediction difficulty than generating adjacent tokens simultaneously.", "description": "Figure 10 compares the conditional entropy increase when switching from sequential to parallel generation for two different token ordering strategies: the authors' proposed strategy and a raster scan.  The proposed strategy generates initial tokens sequentially, then groups spatially distant tokens for parallel generation, while the raster scan generates adjacent tokens in parallel.  The figure visually demonstrates that the proposed strategy yields a smaller entropy increase, indicating less prediction difficulty when parallelizing across spatial blocks compared to parallelizing adjacent tokens.", "section": "3.1 Token Dependencies and Parallel Generation"}]