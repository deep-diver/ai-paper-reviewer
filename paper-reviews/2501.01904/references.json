{"references": [{"fullname_first_author": "OpenAI", "paper_title": "Learning to reason with large language models", "publication_date": "2024-09-01", "reason": "This paper introduces the foundational slow-thinking reasoning system, which is the main focus of the current research."}, {"fullname_first_author": "DeepSeek Team", "paper_title": "DeepSeek-R1-lite-preview is now live: unleashing supercharged reasoning power!", "publication_date": "2024-11-01", "reason": "This paper presents another important slow-thinking reasoning system used as a comparative model in the current research."}, {"fullname_first_author": "Qwen Team", "paper_title": "QWQ: Reflect deeply on the boundaries of the unknown", "publication_date": "2024-11-01", "reason": "This paper introduces a third significant slow-thinking reasoning system, providing another comparative model and data source for the current research."}, {"fullname_first_author": "Wayne Xin Zhao", "paper_title": "A survey of large language models", "publication_date": "2023-03-01", "reason": "This paper provides a comprehensive overview of large language models (LLMs), the foundation upon which the current research builds."}, {"fullname_first_author": "Yingqian Min", "paper_title": "Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems", "publication_date": "2024-12-01", "reason": "This paper offers a detailed replication study of slow-thinking reasoning systems, crucial for validating the findings and methodology of the current research."}]}