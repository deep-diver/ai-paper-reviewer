[{"content": "| Domain | Geometry | Geometry | Geometry | Geometry | Table, Chart, and Figure | Table, Chart, and Figure | Table, Chart, and Figure | Object |\n|---|---|---|---|---|---|---|---|---|\n| Dataset | Geos | GeoQA+ | Geometry3K | UniGeo | TabMWP | FigureQA | ChartQA | CLEVR |\n| # Samples | 279 | 563 | 551 | 555 | 568 | 589 | 509 | 548 |", "caption": "Table 1: The statistics of the selected visual query sources", "description": "Table 1 presents a detailed breakdown of the statistics for four different geometry datasets (Geos, GeoQA+, Geometry3K, UniGeo), three table and figure datasets (TabMWP, FigureQA, ChartQA), and one object dataset (CLEVR).  For each dataset, the table shows the number of samples available.  These datasets were used as sources for visual question answering (VQA) problems to gather multimodal long thought data in the study.", "section": "2.2.1 Visual Long Thought Data Collection"}, {"content": "| Model | Num. Data |  | MathVerse | MathVision |  | MMMU | Average |\n|---|---|---|---|---|---|---|---|---|\n| GPT-4o | - | - | - | 30.4 | 25.9 | 69.1 | - |\n| Gemini-Pro | - | - | 35.3 | 19.2 | 4.2 | 65.8 | 31.13 |\n| Claude-3.5-Sonnet | - | - | - | 38.0 | - | 70.4 | - |\n| OpenAI o1 | - | - | - | - | - | 77.3 | - |\n| QVQ-72B-preview* | - | - | 41.5 | 35.9 | 27.9 | 66.0 | 42.83 |\n| Qwen2-VL-72B-Instruct | - | - | 41.3 | 26.1 | 11.2 | 64.5 | 35.78 |\n| Virgo-72B<sub>DT</sub> | 5K | - | 48.4 | 38.8 | 29.9 | 64.6 | 45.43 |\n| Virgo-72B<sub>D<sub>QVQ</sub></sub> | - | 6.6K | 37.6 | 37.7 | 25.0 | 62.6 | 40.73 |\n| Virgo-72B<sub>D<sub>SD</sub></sub> | - | 7K | 47.4 | 35.0 | 27.2 | 65.8 | 43.85 |\n| Virgo-72B<sub>DT\u222aD<sub>SD</sub></sub> | 5K | 7K | 48.1 | 38.6 | 28.5 | 65.0 | 45.05 |\n| Qwen2-VL-7B-Instruct | - | - | 24.6 | 16.3 | 5.3 | 54.1 | 25.08 |\n| Virgo-7B<sub>DT</sub> | 5K | - | 32.2 | 24.3 | 9.8 | 47.1 | 28.35 |\n| Virgo-7B<sub>D<sub>QVQ</sub></sub> | - | 6.6K | 29.2 | 20.5 | 9.0 | 48.3 | 26.75 |\n| Virgo-7B<sub>D<sub>SD</sub></sub> | - | 7K | 37.5 | 23.1 | 10.3 | 50.7 | 30.40 |\n| Virgo-7B<sub>DT\u222aD<sub>SD</sub></sub> | 5K | 7K | 36.7 | 24.0 | 10.2 | 46.7 | 29.40 |", "caption": "Table 2: Performance comparison of top-tier MLLMs on four representative benchmarks. Here, DTsubscript\ud835\udc37TD_{\\text{T}}italic_D start_POSTSUBSCRIPT T end_POSTSUBSCRIPT denotes the textual long thought data, and DSDsubscript\ud835\udc37SDD_{\\text{SD}}italic_D start_POSTSUBSCRIPT SD end_POSTSUBSCRIPT and DQVQsubscript\ud835\udc37QVQD_{\\text{QVQ}}italic_D start_POSTSUBSCRIPT QVQ end_POSTSUBSCRIPT denote the visual long thought data distilled by our model (the version fine-tuned by DTsubscript\ud835\udc37TD_{\\text{T}}italic_D start_POSTSUBSCRIPT T end_POSTSUBSCRIPT) and QVQ, respectively.\nThe bold fonts denote the best performance among our training variants, while the underline fonts denote the second-best performance. * Since QVQ has not released the evaluation code, we report the evaluation results reproduced by our team.", "description": "Table 2 presents a performance comparison of leading large language models (LLMs) on four challenging benchmarks: MathVerse, MathVision, OlympiadBench, and MMMU.  The LLMs are evaluated based on their performance with different types of training data: textual long thought data (D<sub>T</sub>), visual long thought data distilled from the model fine-tuned on textual data (D<sub>SD</sub>), and visual long thought data distilled from the QVQ model (D<sub>QVQ</sub>).  The table highlights the best and second-best performing models for each benchmark and provides the average performance across all four benchmarks, showcasing the impact of various training datasets on the model's reasoning capabilities. Note that the QVQ results are reproduced by the authors due to the lack of publicly available evaluation code.", "section": "3.2 Main Results"}, {"content": "| Model | Easy | Medium | Hard | Overall |\n|---|---|---|---|---|\n| QVQ-72B-preview | **76.95** | **65.80** | 48.62 | **66.0** |\n| Qwen2-VL-72B-Instruct | 74.58 | 62.26 | 50.28 | 64.5 |\n| Virgo-72B<SUB>D<SUB>T</SUB></SUB> | 72.88 | **62.97** | **54.70** | 64.6 |\n| Virgo-72B<SUB>D<SUB>T</SUB>\u222aD<SUB>SD</SUB></SUB> | **74.58** | 61.79 | **56.91** | **65.0** |", "caption": "Table 3: Performance comparison on samples from different difficulty bins in MMMU.", "description": "This table presents a performance comparison of different models on the MMMU benchmark, broken down by the difficulty level of the questions (easy, medium, hard). It shows the accuracy of each model for each difficulty level and overall. This allows for a more granular analysis of model performance, revealing potential strengths and weaknesses at different levels of problem complexity.", "section": "3.2 Main Results"}, {"content": "| Base Model | Length | MathVerse | MathVision | OlympiadBench | MMMU |\n|---|---|---|---|---|---| \n| Qwen2-VL-7B-Instruct | - | 24.0 | 15.6 | 5.3 | **54.1** |\n|  | (0, 2000] | **28.1** | 22.1 | **8.8** | 41.9 |\n|  | (2000, 4000] | **33.6** | **24.4** | **9.1** | **48.0** |\n|  | (4000, 8000] | 24.8 | **24.9** | 8.3 | 39.8 |", "caption": "Table 4: Performance comparison by tuning with instruction datasets of varying thought length.", "description": "This table presents a performance comparison of a model fine-tuned using instruction datasets with varying lengths of thought processes. It shows the impact of different thought lengths on the model's performance across four benchmarks: MathVerse, MathVision, OlympiadBench, and MMMU. The results are presented for the base model (Qwen2-VL-7B-Instruct) and three different lengths of instruction datasets: (0, 2000], (2000, 4000], and (4000, 8000]. This helps in understanding the relationship between the length of the reasoning process in the training data and the resulting performance of the model.", "section": "3.3 Further Analysis"}, {"content": "| Base Model | Num. | MathVerse | MathVision | OlympiadBench | MMMU |\n|---|---|---|---|---|---| \n| Qwen2-VL-72B-Instruct | - | 41.3 | 26.1 | 11.2 | 64.5 |\n|  | 1K | 42.5 | 39.5 | 26.2 | 61.8 |\n|  | 3K | 44.4 | 40.5 | 26.4 | 58.2 |\n|  | 5K | 48.4 | 38.8 | 29.9 | 64.7 |\n| Qwen2-VL-7B-Instruct | - | 24.0 | 15.6 | 5.3 | 54.1 |\n|  | 1K | 22.5 | 23.7 | 8.6 | 42.8 |\n|  | 3K | 30.2 | 24.9 | 9.6 | 44.6 |\n|  | 5K | 31.9 | 24.6 | 9.2 | 47.1 |", "caption": "Table 5: The scaling effect of instruction data on the base model.", "description": "This table presents the results of an experiment examining the impact of scaling the amount of textual instruction data used to fine-tune a base multimodal large language model (MLLM).  It shows how the model's performance across four different benchmarks (MathVerse, MathVision, OlympiadBench, and MMMU) changes as the number of instructions increases from 1,000 to 5,000.  The base MLLMs used are Qwen2-VL-72B-Instruct and Qwen2-VL-7B-Instruct, allowing for the investigation of scale effects.", "section": "3.2 Main Results"}, {"content": "| Base Model | Difficulty | MathVerse | MathVision | OlympiadBench | MMMU |\n|---|---|---|---|---|---| \n| Qwen2-VL-72B-Instruct | - | 41.3 | 26.1 | 11.2 | 64.5 |\n|  | Medium | 48.1 | 38.6 | 28.5 | 65.0 |\n|  | Hard | 47.4 | 39.1 | 29.7 | 63.0 |\n|  | Random | 47.9 | 38.5 | 29.3 | 64.8 |", "caption": "Table 6: Performance comparison of using self-distilled visual instruction of varying difficulty levels.", "description": "This table presents a performance comparison of a multimodal large language model (MLLM) fine-tuned using self-distilled visual instructions with varying difficulty levels.  It compares the model's performance across four challenging benchmarks: MathVerse, MathVision, OlympiadBench, and MMMU. The difficulty levels of the visual instructions are categorized as 'Medium', 'Hard', and 'Random', reflecting the complexity of the reasoning processes involved.  The table shows the impact of instruction difficulty on the MLLM's ability to perform slow-thinking reasoning tasks.", "section": "3.2 Main Results"}, {"content": "Table showing the problem, ground truth and model answers.\n\n| Problem                                                                     | Ground Truth | Qwen2-VL-72B-Instruct                                                                                                                                                                                                                       | Virgo-72B (ours)                                                                                                                                            |\n|-----------------------------------------------------------------------------|---------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| <img src=\"https://arxiv.org/html/2501.01904/x4.png\" width=498 height=236> | 7\u03c0            | To evaluate the integral of the functions graphed using the formula for circles, we need to identify the equations of the circles and then integrate them over the appropriate intervals.  1. Identify the Equations of the Circles: ... 4. Final Answer: 18\u03c0 | <br> <br> Alright, I\u2019ve got this graph here with three semicircles plotted. The x-axis ranges from 0 to 12, and the y-axis goes up to 5. Each semicircle is labeled with an expression involving x. ... So, this simplifies to ... |", "caption": "Table 7: An example that Virgo-72B outperforms Qwen2-VL-72B-Instruct (part 1).", "description": "This table showcases a comparative analysis of the problem-solving approaches employed by two different large language models (LLMs): Virgo-72B and Qwen2-VL-72B-Instruct.  The models are tasked with evaluating the integral of a function depicted graphically, and the table highlights the step-by-step reasoning processes of each model, demonstrating Virgo-72B's superior performance and more detailed explanation.", "section": "3.4 Case Study"}, {"content": "| Image | Description |\n|---|---| \n| https://arxiv.org/html/2501.01904/S4.T8.1.1.1.1.p1.pic1.png | Let\u2019s calculate the area for each semicircle:\n1. Smallest semicircle: radius  r=1, so area = \u03c0/2.\n2. Middle semicircle:\u2026\u2026\nSo, the total area under all three semicircles is 7\u03c0. |\n", "caption": "Table 8: An example that Virgo-72B outperforms Qwen2-VL-72B-Instruct (part 2)", "description": "Table 8 presents a comparison of the performance of two models, Virgo-72B and Qwen2-VL-72B-Instruct, on a specific visual reasoning task. It shows how Virgo-72B, equipped with slow-thinking reasoning capabilities through textual long-form thought data, not only arrives at the correct answer but also demonstrates a more comprehensive and self-reflective reasoning process compared to Qwen2-VL-72B-Instruct. The table highlights the advantages of the slow-thinking approach by showcasing a more detailed and insightful solution process, indicating improved accuracy and reasoning depth.", "section": "3.4 Case Study"}, {"content": "| Model | Image | Text |\n|---|---|---|\n|  | https://arxiv.org/html/2501.01904/x5.png | <div style=\"font-size:90%;\"><span style=\"font-weight:bold;\">Problem:</span> Graph 1 shows the number of unemployed (in thousands) over a five-month period, organized by level of education completed. The difference between the number of unemployed high school graduates and the number of unemployed workers without a high school diploma was smallest in <span style=\"text-decoration:underline;\"> </span>.</div><div style=\"font-size:90%;\">A. June</div><div style=\"font-size:90%;\">B. July</div><div style=\"font-size:90%;\">C. August</div><div style=\"font-size:90%;\">D. September</div><div style=\"font-size:90%;\">E. October</div><div style=\"font-size:90%;\"><span style=\"font-weight:bold;\">Ground Truth:</span> D. September</div>", "caption": "Table 9: An example that Virgo-72B fails to reflect on perception results (part 1).", "description": "This table showcases a case where the Virgo-72B model demonstrates a failure in its reasoning process due to an incorrect interpretation of the input graph.  The model correctly identifies the question but makes an error in its initial assessment of the number of unemployed individuals in a specific category during a given month. While the model recognizes an inconsistency in its findings and attempts to correct itself, it ultimately fails to question the flawed visual data interpretation and reaches an incorrect final answer. The example illustrates a key limitation: the model's inability to effectively self-correct when a perceptual error affects its reasoning chain.", "section": "3.4 Case Study"}, {"content": "| Month | High School Graduates | No High School Diploma | Difference |\n|---|---|---|---| \n| June |  |  | 4000 |\n| July |  |  | 3500 |\n| August | 8500 | 11500 | 3000 |\n| September | 8000 | 11000 | 3000 |\n| October | 8000 | 12000 | 4000 |", "caption": "Table 10: An example that Virgo-72B fails to reflect on perception results (part 2).", "description": "This table showcases an example where the Virgo-72B model demonstrates a failure to reflect on its initial perception of the data.  Specifically, it incorrectly interprets data from a graph showing unemployment numbers over five months, categorized by educational attainment. The model initially identifies the smallest difference between two unemployment categories (high school graduates and those with less than a high school diploma) and even correctly mentions the months where the difference is smallest. However, it fails to maintain this consistency in its reasoning, leading to a faulty and ultimately incorrect final answer.", "section": "3.4 Case Study"}]