{"references": [{"fullname_first_author": "Gao, L.", "paper_title": "Scaling laws for reward model overoptimization", "publication_date": "2023", "reason": "This paper introduces the concept of reward hacking in RLHF, which is directly analogous to the paper's central concept of teacher hacking in LM distillation."}, {"fullname_first_author": "Hinton, G.", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015", "reason": "This foundational paper introduced the concept of knowledge distillation, which is the core technique that the current paper investigates and expands upon."}, {"fullname_first_author": "Menon, A. K.", "paper_title": "A statistical perspective on distillation", "publication_date": "2021", "reason": "This work highlights the crucial limitation of knowledge distillation where the teacher model itself is an imperfect proxy for the true distribution, which is directly addressed and expanded upon in the current paper."}, {"fullname_first_author": "Sanh, V.", "paper_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter", "publication_date": "2020", "reason": "This paper exemplifies a successful application of knowledge distillation in the context of large language models which is relevant to the current paper's context and methodology."}, {"fullname_first_author": "Zhang, R.", "paper_title": "Do not blindly imitate the teacher: Using perturbed loss for knowledge distillation", "publication_date": "2023", "reason": "This paper explores an alternative method for knowledge distillation which is similar to the current paper and addresses some of the same challenges related to teacher model limitations."}]}