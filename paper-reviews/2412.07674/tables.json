[{"content": "| Attributes | Color | Stroke | Lighting | Dynamic | Focus_and_DoF | Design | Rhythm | Average |\n|---|---|---|---|---|---|---|---|---|\n| Number | 429K | 301K | 370K | 89K | 107K | 66K | 52K | 202K |\n| Accuracy | 0.96 | 0.92 | 0.76 | 0.81 | 0.85 | 0.89 | 0.73 | 0.84 |\n| Standard Deviation | 0.12 | 0.15 | 0.19 | 0.14 | 0.12 | 0.14 | 0.18 | 0.15 |", "caption": "Table 1: Statistics and human validation results. We report the number of individual images containing each specific visual attribute (some images could contain multiple attributes), along with human validation accuracy and cross-agreement measured by standard deviation.", "description": "This table presents a statistical summary of the FiVA dataset, focusing on its visual attributes.  It details the number of images annotated for each attribute (color, stroke, lighting, dynamics, focus and depth of field, design, and rhythm), highlighting that some images possess multiple attributes.  Furthermore, it reports the accuracy of human validation for each attribute, indicating the level of agreement among annotators in assessing the presence of each visual attribute. Finally, it includes the standard deviation for each attribute, quantifying the level of inter-annotator agreement.", "section": "3 Dataset Overview"}, {"content": "| Metrics | DB-Lora | IP-Adapter | DEADiff | StyleAligned | Ours |\n|---|---|---|---|---|---| \n| User-Study |  |  |  |  |  |\n| Sub-Acc | 0.393 | 0.163 | 0.605 | 0.520 | **0.817** |\n| Attr&Sub-Acc | 0.240 | 0.150 | 0.260 | 0.298 | **0.348** |\n| CLIP-Score |  |  |  |  |  |\n| in-domain | 0.180 | 0.161 | 0.211 | 0.196 | **0.228** |\n| out-domain | 0.177 | 0.135 | 0.205 | 0.189 | **0.229** |", "caption": "Table 2: User Study and CLIP Scores. Both quantitative results demonstrate our superior performance over the baseline in terms of both subject and joint accuracy.", "description": "This table presents the results of a user study and CLIP score evaluations comparing the proposed FiVA-Adapter method to several baselines for controllable image generation.  The metrics used are Subject Accuracy (measuring how accurately the generated image matches the intended subject) and Attribute & Subject Accuracy (measuring the joint accuracy of both subject and attribute).  Both in-domain (subjects from the same category as the reference image) and out-of-domain (subjects from different categories) evaluations are included to assess the model's generalization capabilities.  The results demonstrate that FiVA-Adapter outperforms the baselines in terms of both subject and joint accuracy, indicating its superior performance in controllable image generation.", "section": "5 Experiment"}, {"content": "| Methods | Color | Stroke | Lighting | Focus&DoF | Dynamic | Design | Rhythm | Average |\n|---|---|---|---|---|---|---|---|---|\n| DB-Lora | 0.516 | 0.478 | 0.358 | 0.485 | 0.480 | 0.600 | 0.607 | 0.503 |\n| IP-Adapter | 0.323 | 0.403 | 0.340 | 0.364 | 0.520 | 0.440 | 0.500 | 0.413 |\n| DEADiff | 0.161 | 0.209 | 0.245 | 0.485 | 0.400 | 0.080 | 0.357 | 0.277 |\n| Style-Aligned | 0.581 | 0.552 | 0.396 | 0.606 | 0.600 | 0.660 | 0.571 | 0.567 |\n| Ours | 0.780 | 0.647 | 0.396 | 0.727 | 0.560 | 0.510 | 0.521 | 0.592 |", "caption": "Table R1: GPT study results on each attribute type. The Attr&Sub-Acc here denotes the accuracy when both the attribute transferring and target subject are correct.", "description": "This table presents the results of a qualitative user study using GPT-4V, a large language model, to evaluate the performance of different image generation methods on various visual attributes.  Specifically, it assesses the accuracy of each method in transferring a single visual attribute (color, stroke, lighting, focus and depth of field, dynamic, design, rhythm) from a reference image to a target image, while also ensuring that the generated image's subject accurately matches the target. The 'Attr&Sub-Acc' metric combines both attribute transfer accuracy and subject accuracy, indicating whether both aspects were correctly generated.", "section": "5.2 Quantitative Evaluation"}]