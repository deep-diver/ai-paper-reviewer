[{"figure_path": "https://arxiv.org/html/2412.07674/x1.png", "caption": "Figure 1: \nOverview. We propose the FiVA dataset and adapter to learn fine-grained visual attributes for better controllable image generation.", "description": "Figure 1 provides a high-level overview of the FiVA project.  It showcases the dataset's organization around a core set of fine-grained visual attributes (color, stroke, lighting, design, etc.), each illustrated with example images.  The figure also highlights the FiVA-Adapter, a proposed framework built upon the FiVA dataset that facilitates user control over these attributes in image generation models. This control allows users to selectively combine specific attributes from multiple reference images to customize the output of text-to-image models. The ultimate aim is to improve the controllability and quality of images generated using text prompts.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.07674/x2.png", "caption": "Figure 2: \nExamples of visual consistency application range. Some visual attributes, such as \u2018color\u2019 and \u2018stroke,\u2019 are easily transferable across different subjects (left). However, other attributes, like \u2018lighting\u2019 and \u2018dynamics,\u2019 are range-sensitive, meaning they produce varying visual effects depending on the domain (right), resulting in more fine-grained, subject-specific definitions of sub-attributes.", "description": "Figure 2 demonstrates how the application range of visual attributes varies. Attributes like color and stroke maintain consistency across different subjects.  However, attributes such as lighting and dynamics are range-sensitive, producing different visual effects depending on the subject or scene. This necessitates more granular, subject-specific definitions for these range-sensitive attributes.", "section": "3.1 Data Construction Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.07674/x3.png", "caption": "Figure 3: \nFiVA-Adapter architecture and training pipeline. FiVA-Adapter has two key designs: 1) Attribute-specific Visual Prompt Extractor, 2) Multi-image Dual Cross-Attention Module.", "description": "The figure illustrates the architecture and training pipeline of the FiVA-Adapter, a novel framework designed for fine-grained visual attribute adaptation in text-to-image diffusion models.  FiVA-Adapter consists of two main components: 1) an Attribute-specific Visual Prompt Extractor, which extracts relevant visual feature embeddings from input images based on specified attributes; and 2) a Multi-image Dual Cross-Attention Module, which integrates these attribute-specific features along with text prompts into the diffusion model to generate images with precisely controlled attributes. The pipeline shows how image features are extracted, fused with text embeddings, and processed through cross-attention modules before feeding into the U-Net for image generation.", "section": "4 Method"}, {"figure_path": "https://arxiv.org/html/2412.07674/x4.png", "caption": "Figure 4: \nQualitative comparisons on single attribute transferring.", "description": "This figure presents a qualitative comparison of different methods for transferring a single visual attribute from a reference image to a generated image.  It shows the results of several methods including the proposed FiVA-Adapter alongside baselines such as DreamBooth Lora, IP-Adapter, DEADiff, and StyleAlign. For each method, a series of images demonstrates the transfer of attributes like color, stroke, focus, dynamics, design, lighting, and rhythm. The comparison highlights the effectiveness of FiVA-Adapter in accurately transferring the desired attributes while maintaining the overall integrity and consistency of the target image.", "section": "4 Method"}, {"figure_path": "https://arxiv.org/html/2412.07674/x5.png", "caption": "Figure 5: \nThe combination of multiple visual attributes enables the integration of specific characteristics from different reference images into the target subject.", "description": "Figure 5 showcases the capability of the FiVA-Adapter to combine visual attributes from multiple source images.  The figure demonstrates that by using the framework, specific visual characteristics (like color, stroke, lighting, or dynamics) extracted from different reference images can be successfully integrated into a single generated image, creating a customized result that reflects the combined attributes of the sources while adhering to a target subject or theme. This highlights the flexibility and power of the approach in controlling fine-grained aspects of image generation.", "section": "5 User Study and CLIP Scores"}, {"figure_path": "https://arxiv.org/html/2412.07674/x6.png", "caption": "Figure 6: \nAttribute decomposition. One reference image can be decomposed into different attributes via different tags.", "description": "Figure 6 demonstrates that by using different tags in the prompt, various visual attributes can be extracted from a single reference image.  This showcases the fine-grained control offered by the FiVA-Adapter in isolating and manipulating specific visual aspects for subsequent image generation.", "section": "4.2 Fine-grained Visual Attribute Adapter"}, {"figure_path": "https://arxiv.org/html/2412.07674/extracted/6048517/figures/filter_abl.jpg", "caption": "Figure 7: \nAblation on range-sensitive data filter. It helps improve the attribute accuracy and protect the original generation capacity.", "description": "This ablation study demonstrates the effect of a range-sensitive data filter on the quality of image generation. The filter is designed to improve the accuracy of attribute transfer while maintaining the original generation capacity. Removing the filter leads to reduced attribute accuracy and increased generation failures, highlighting the importance of the filter in ensuring high-quality and consistent results.", "section": "3.1 Data Construction Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.07674/x7.png", "caption": "Figure S1: \nMore image examples with different visual attributes.", "description": "This figure displays a collection of images showcasing the diverse range of visual attributes captured in the FiVA dataset.  Each row highlights a specific attribute (e.g., color, stroke, lighting, dynamics, focus & depth of field, design, rhythm), displaying multiple example images representing variations within that attribute. This visually demonstrates the breadth and granularity of visual characteristics included within the dataset used to train the FiVA-Adapter model.", "section": "Additional Experimental Details"}, {"figure_path": "https://arxiv.org/html/2412.07674/x8.png", "caption": "Figure S2: \nStatistics and Analysis. We visualize the rough distribution of visual attributes and subjects on the left. On the right, we show an example pair of images that shares similar lighting condition. We also visualize the attribute alignment accuracy via human validation here.", "description": "Figure S2 presents a comprehensive analysis of the FiVA dataset. The left side displays the overall distribution of visual attributes (e.g., color, lighting, stroke) and subjects (e.g., animals, architecture, cityscapes) using pie charts. This visualization provides a clear overview of the dataset's composition, highlighting the prevalence of different visual aspects and subject categories. The right side showcases an example of an image pair from the dataset exhibiting similar lighting conditions. This example visually demonstrates the core concept of the dataset, where images possess shared visual attributes. Finally, the figure incorporates a chart illustrating the attribute alignment accuracy obtained through human validation. This quantitative measure underscores the quality and reliability of the attribute annotations in the dataset.", "section": "Additional Details on Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2412.07674/x9.png", "caption": "(a) \nSubject Tree of lighting: moonlight.", "description": "This figure shows a hierarchical tree structure representing the organization of images based on the visual attribute \"lighting: moonlight\".  The tree starts with a root node representing the entire set of images.  This is broken down into major subject categories (e.g., City, Nature, Architecture), each further divided into sub-categories (e.g., City -> urban skylines, suburban nightscapes, etc.).  This hierarchical structure is used in a range-sensitive data filtering process to ensure consistency in visual attributes across different image subjects.", "section": "3.1 Data Construction Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.07674/x10.png", "caption": "(b) \nGPT4V based Range-sensitive Data Filtering.", "description": "This figure shows the process of range-sensitive data filtering using GPT-4V.  The process starts with a hierarchical subject tree that organizes images based on their visual attributes. For each attribute, GPT-4V is used to assess the consistency of visual effects across different subjects.  If the consistency is high, then the images are considered to have similar visual effects. If not, the process is repeated at a lower level in the hierarchy. This ensures that only attribute-consistent image pairs are retained in the dataset.", "section": "3.1 Data Construction Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.07674/x11.png", "caption": "Figure S3: Range-sensitive Data Filtering. Taking the attribute lighting: moonlight as an example, (a) demonstrates the hierarchy of Set/Major-subject/Sub-subject. It lists the \"group of suitable subjects\" chosen for generating images with the visual attribute lighting: moonlight, along with sub-subjects under each major-subject. The \"group of suitable subjects\" refers to the pre-defined major-subjects that are applicable for the attribute. Due to space limitations, only 15 sub-subjects are listed for each major-subject. (b) verifies whether the images under major-subject: architecture exhibit consistent lighting traits of moonlight. The result shows that Image 4 exhibits inconsistencies, with the reasons provided.", "description": "Figure S3 illustrates the process of range-sensitive data filtering used to create a high-quality dataset for fine-grained visual attribute control.  Part (a) shows the hierarchical structure used to organize subjects and sub-subjects based on their suitability for specific visual attributes (in this case, 'lighting: moonlight'). The top level ('Set') represents all suitable subjects, which are broken down into major subjects, and finally sub-subjects to ensure fine-grained control.  Due to space constraints, only 15 sub-subjects are shown for each major subject. Part (b) demonstrates a specific example of the validation process performed to check the consistency of the 'moonlight' attribute within the 'architecture' major subject. Nine example images are presented, and it is explained that image 4 shows inconsistent lighting characteristics compared to the others. This inconsistency is highlighted along with the explanation.", "section": "3.1 Data Construction Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.07674/x12.png", "caption": "Figure S4: \nAblation on attribute input augmentation.\nModels trained with tag augmentation handle slight deviations in input text during inference, while those without augmentation would fail in these cases.", "description": "This ablation study investigates the impact of attribute input augmentation on model robustness.  Two sets of models were trained: one with tag augmentation and one without.  The models were then tested on inputs with slight deviations from the standard attribute names used during training. The results show that models trained with tag augmentation were more resilient to these variations and produced successful outputs, while those without augmentation failed to generate correct results.", "section": "Additional Experimental Details"}, {"figure_path": "https://arxiv.org/html/2412.07674/x13.png", "caption": "Figure S5: \nExamples with real-world images. We demonstrate that our adapter can be effectively extended to real-world images, which have a different distribution from generated images.", "description": "Figure S5 presents a qualitative evaluation of the FiVA-Adapter's performance on real-world images.  The figure showcases example images generated by the FiVA-Adapter using real-world images as input, demonstrating its ability to generalize beyond the synthetic dataset used for training. The results highlight the adaptability and versatility of the method for fine-grained attribute adaptation in diverse image domains.", "section": "5.3 Qualitative Evaluation"}]