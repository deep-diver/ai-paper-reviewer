[{"figure_path": "https://arxiv.org/html/2503.12885/x1.png", "caption": "Figure 1: \nThe overview of DreamRenderer. (\u00a7\u00a03.3)\n(a) The pipeline of DreamRenderer.\n(b) Attention maps in Joint Attention, which includes 1) Hard Text Attribute Binding (\u00a7\u00a03.5), 2) Hard Image Attribute Binding (\u00a7\u00a03.6), and 3) Soft Image Attribute Binding (\u00a7\u00a03.6).\nIn the attention maps shown in (b), rows represent queries and columns represent keys.\nWe use different patterns to distinguish between image tokens\u00a0\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\u00a0\n\n\n\n\n\n, text tokens\u00a0\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\u00a0\n\n\n\n\n\n, and bridge image tokens\u00a0\n\n\n\n\n\n\u00a0\n\n\n\n\n\n,\nwhile different colors (\u00a0\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\u00a0\n\n\n\n\n\n for an ice cat, \u00a0\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\u00a0\n\n\n\n\n\n for a fire dog and\u00a0\n\n\n\n\n\n\u00a0\n\n\n\n\n\n for the global text tokens and background image tokens) represent tokens from different instances.", "description": "Figure 1 illustrates the architecture and functionality of DreamRenderer. Panel (a) presents a schematic overview of the DreamRenderer pipeline, showing how depth or canny maps, along with text prompts for individual instances and a global caption, are processed to generate an image.  The process involves encoding via a Variational Autoencoder (VAE) and the T5 text encoder, followed by joint attention mechanisms. Panel (b) details the attention mechanisms within the joint attention module. It highlights three key aspects: Hard Text Attribute Binding, Hard Image Attribute Binding, and Soft Image Attribute Binding.  Different patterns visually distinguish between image, text, and bridge image tokens in the attention maps. Color-coding further differentiates tokens originating from different instances (e.g., an ice cat, a fire dog, or global context).", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.12885/x2.png", "caption": "Figure 2: \nVital Binding Layer Search (\u00a7\u00a03.6).\nWe apply Hard Image Attribute Binding layer by layer and observe that applying it in the FLUX model\u2019s input or output layers degrades performance, whereas applying it in the middle layers yields improvements.", "description": "This figure displays the results of an experiment to determine the optimal layer within the FLUX model to apply Hard Image Attribute Binding.  Hard Image Attribute Binding is a technique used to improve the accuracy of generating images by ensuring that each instance's image tokens only attend to themselves, preventing attribute leakage between instances.  The experiment involved applying this technique layer by layer within the FLUX model, and the results show that applying it to the input or output layers actually decreased performance. However, applying it to the middle layers resulted in performance improvements. This highlights the importance of selecting the right layer for this binding technique for optimal image generation.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.12885/x3.png", "caption": "Figure 3: \nQualitative Comparison on the COCO-POS benchmark (\u00a7\u00a04.2).", "description": "Figure 4 shows a qualitative comparison of the results obtained using FLUX and 3DIS, compared to the results from the proposed method, DreamRenderer, on the COCO-POS benchmark.  The figure visually demonstrates the effectiveness of DreamRenderer in generating images that adhere more closely to the specified depth map and layout than the existing methods (FLUX and 3DIS).  Each row presents a sample image, with the top row showing depth and layout, and the bottom row showing the generated results using FLUX, 3DIS and DreamRenderer respectively. The figure illustrates DreamRenderer's enhanced ability to control multiple instances (e.g., different objects) within a single image, and achieve better alignment with both depth and layout, resulting in better quality generation.", "section": "4.2. Comparison with State-of-the-Art Methods"}, {"figure_path": "https://arxiv.org/html/2503.12885/x4.png", "caption": "Figure 4: Qualitative comparison on the COCO-MIG benchmark (\u00a7\u00a04.2).", "description": "Figure 4 shows a qualitative comparison of the results obtained using the DreamRenderer method on the COCO-MIG benchmark. The COCO-MIG benchmark assesses the model's capacity to generate images matching specified layouts and instance attributes.  The figure visually compares the images generated by different methods, including GLIGEN, InstanceDiffusion, MIGC, and 3DIS, both with and without DreamRenderer integrated. This comparison highlights DreamRenderer's ability to improve the accuracy of instance attributes and enhance the overall image quality when combined with these state-of-the-art layout-to-image models.", "section": "4.2. Comparison with State-of-the-Art Methods"}, {"figure_path": "https://arxiv.org/html/2503.12885/x5.png", "caption": "Figure 5: \nAblation study on Hard Image Attribute Binding (\u00a7\u00a03.6).", "description": "This ablation study analyzes the impact of applying Hard Image Attribute Binding at different layers of the FLUX model. It compares the performance using Hard Image Attribute Binding applied to input, middle, and output layers, against a baseline without Hard Image Attribute Binding.  The results are shown across various metrics, including instance success ratio, mean intersection over union, and image success ratio, for different numbers of instances.  The study aims to identify the optimal layer(s) for applying this technique to balance the accuracy of individual instance rendering with the overall image coherence.", "section": "3.6. Image Attribute Binding"}, {"figure_path": "https://arxiv.org/html/2503.12885/x6.png", "caption": "Figure 6: \nAblation study on Hard Text Attribute Binding (\u00a7\u00a03.5).\nWe use the same layout from Fig.\u00a0LABEL:fig:teaser for testing. Due to space limitations in the main text, additional image results are provided in the Supplementary Materials.", "description": "Figure 6 shows the results of an ablation study on the Hard Text Attribute Binding method (section 3.5).  The study uses the same layout as Figure 1 to compare different versions of the method.  The images demonstrate the effect of the Hard Text Attribute Binding, showing how it improves the accuracy of generating multiple instances within the image. Due to space constraints, only a subset of the images from the ablation study are shown in the main paper; more examples are available in the supplementary materials.", "section": "3.5 Hard Text Attribute Binding"}]