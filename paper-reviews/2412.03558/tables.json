[{"content": "| Method | 3D-Front CD-S \u2193 | 3D-Front F-Score-S \u2191 | 3D-Front CD-O \u2193 | 3D-Front F-Score-O \u2191 | 3D-Front IoU-B \u2191 | BlendSwap CD-S \u2193 | BlendSwap F-Score-S \u2191 | BlendSwap CD-O \u2193 | BlendSwap F-Score-O \u2191 | BlendSwap IoU-B \u2191 | Runtime \u2193 |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| PanoRecon [7] | 0.150 | 40.65 | 0.211 | 35.05 | 0.240 | 0.427 | 19.11 | 0.713 | 13.06 | 0.119 | 32s |\n| Total3D [45] | 0.270 | 32.90 | 0.179 | 36.38 | 0.238 | 0.258 | 37.93 | 0.168 | 38.14 | 0.328 | 39s |\n| InstPIFu [35] | 0.138 | 39.99 | 0.165 | 38.11 | 0.299 | 0.129 | 50.28 | 0.167 | 38.42 | 0.340 | 32s |\n| SSR [4] | 0.140 | 39.76 | 0.170 | 37.79 | 0.311 | 0.132 | 48.72 | 0.173 | 38.11 | 0.336 | 32s |\n| DiffCAD [17] | 0.117 | 43.58 | 0.190 | 37.45 | 0.392 | 0.110 | 52.83 | 0.169 | 38.98 | 0.457 | 64s |\n| Gen3DSR [11] | 0.123 | 40.07 | 0.157 | 38.11 | 0.363 | 0.107 | 60.17 | 0.148 | 40.76 | 0.449 | 9min |\n| REPARO [20] | 0.129 | 41.68 | 0.160 | 40.85 | 0.339 | 0.115 | 62.39 | 0.151 | 42.84 | 0.410 | 4min |\n| Ours | **0.080** | **50.19** | **0.103** | **53.58** | **0.518** | **0.077** | **78.21** | **0.090** | **62.94** | **0.663** | 40s |", "caption": "Table 1: Quantitative comparisons on synthetic datasets\u00a0[15, 1] in scene-level Chamfer Distance (CD-S) and F-Score (F-Score-S), object-level Chamfer Distance (CD-O) and F-Score (F-Score-O), and Volume IoU of object bounding boxes (IoU-B).", "description": "This table presents a quantitative comparison of different methods for 3D scene generation on synthetic datasets (3D-Front [15] and BlendSwap [1]).  The comparison uses four metrics: scene-level Chamfer Distance (CD-S) and F-score (F-Score-S), object-level Chamfer Distance (CD-O) and F-score (F-Score-O), and the Volume IoU of object bounding boxes (IoU-B). Lower values for CD-S and CD-O and higher values for F-Score-S, F-Score-O, and IoU-B indicate better performance.  The table also includes the runtime for each method.", "section": "5. Experiments"}, {"content": "| #K | S. | O. | CD-S \u2193 | F-Score-S \u2191 | CD-O \u2193 | F-Score-O \u2191 | IoU-B \u2191 |\n|---|---|---|---|---|---|---|---| \n| 0 | \u2717 | \u2717 | 0.152 | 41.16 | \u2013 | \u2013 | \u2013 |\n| 0 | \u2713 | \u2713 | 0.145 | 40.94 | 0.096 | 54.16 | 0.327 |\n| 5 | \u2713 | \u2713 | 0.080 | 50.19 | 0.103 | 53.58 | 0.518 |\n| 21 | \u2713 | \u2713 | 0.127 | 44.88 | 0.141 | 48.55 | 0.423 |\n| 5 | \u2717 | \u2713 | 0.134 | 41.49 | 0.102 | 52.91 | 0.459 |\n| 5 | \u2713 | \u2717 | 0.137 | 42.00 | 0.126 | 51.62 | 0.502 |", "caption": "Table 2: Ablation studies. We evaluate the number of multi-instance attention layers (#\u2062K#\ud835\udc3e\\#K# italic_K), the inclusion of global scene image (S.) input, and the use of Objaverse\u00a0[9] (O.) for mixed training.", "description": "This ablation study investigates the impact of key components within the MIDI model on its performance.  It examines three aspects: the number of multi-instance attention layers used, the inclusion or exclusion of global scene image input during training, and the effect of incorporating data from the Objaverse dataset [9] for mixed training. The results are quantitatively evaluated using Chamfer Distance (CD-S), F-Score (F-Score-S), CD-O, F-Score-O, and IoU-B metrics.", "section": "5.5 Ablation Study"}]