[{"figure_path": "https://arxiv.org/html/2412.03558/x2.png", "caption": "Figure 1: MIDI generates compositional 3D scenes from a single image by extending pre-trained image-to-3D object generation models to multi-instance diffusion models, incorporating a novel multi-instance attention mechanism that captures inter-object interactions. (a) shows our generated scenes compared with those reconstructed by existing methods. (b) presents our generated results on synthetic data, real-world images, and stylized images.", "description": "This figure demonstrates the capabilities of the MIDI model in generating compositional 3D scenes from a single input image.  It achieves this by extending pre-trained image-to-3D object generation models and incorporating a novel multi-instance attention mechanism to effectively capture interactions between multiple objects within the scene.  Panel (a) provides a visual comparison of 3D scenes generated by MIDI against those generated by several existing methods.  Panel (b) showcases MIDI's ability to generalize across different types of input images, including synthetic data, real-world photographs, and stylized images created by a text-to-image diffusion model.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.03558/x3.png", "caption": "Figure 2: Comparison between our scene generation pipeline with multi-instance diffusion and existing compositional generation methods.", "description": "Figure 2 illustrates the difference between traditional compositional 3D scene generation methods and the proposed MIDI approach.  Traditional methods (a) involve multiple sequential steps: image segmentation, individual object image inpainting, individual 3D object generation for each object, and finally, layout optimization to arrange the generated objects in a scene. This multi-step process is prone to error accumulation, making it difficult to ensure the final scene is coherent. In contrast, the MIDI pipeline (b) utilizes multi-instance diffusion, directly generating multiple 3D objects simultaneously with a novel multi-instance attention mechanism that captures spatial relationships, thus creating a more efficient and accurate scene generation process.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.03558/x4.png", "caption": "Figure 3: Method overview. Based on 3D object generation models, MIDI denoises the latent representations of multiple 3D instances simultaneously using a weight-shared DiT module. The multi-instance attention layers are introduced to learn cross-instance interaction and enable global awareness, while cross-attention layers integrate the information of object images and global scene context.", "description": "This figure illustrates the architecture of the MIDI model.  It starts with multiple input images, each representing a single object in a scene.  These images are pre-processed by a 3D object generation model to extract latent representations (features encoding the objects' 3D structure). Then, a shared diffusion model, called a DiT module (denoising transformer), processes these latent representations simultaneously. Key to the model are multi-instance attention layers which enable the different object representations to interact, effectively modeling relationships between them (e.g., spatial arrangement, occlusion).  Finally, cross-attention layers integrate information from both the object-specific image features and a global scene context image into the diffusion process. The output is a set of denoised latent representations corresponding to multiple 3D instances that are spatially coherent and consistent with the global scene context.", "section": "4. MIDI: Multi-Instance 3D Generation"}, {"figure_path": "https://arxiv.org/html/2412.03558/x5.png", "caption": "Figure 4: Multi-instance attention. We extend the original object self-attention, where tokens of each object query only themselves, to multi-instance attention, where tokens of each instance query all tokens from all instances in the scene.", "description": "This figure illustrates the mechanism of multi-instance attention used in the MIDI model.  The original object self-attention mechanism only allows tokens within a single object to interact.  In contrast, the enhanced multi-instance attention allows tokens from each object to attend to (query) tokens from all other objects in the scene. This crucial modification enables the model to capture inter-object relationships and spatial coherence during 3D scene generation, leading to more realistic and accurate results.", "section": "4.2. Multi-Instance Attention"}, {"figure_path": "https://arxiv.org/html/2412.03558/x6.png", "caption": "Figure 5: Qualitative comparisons on synthetic datasets, including 3D-Front\u00a0[15] and BlendSwap\u00a0[1].", "description": "Figure 5 presents a qualitative comparison of 3D scene generation results from different methods on synthetic datasets, namely 3D-Front and BlendSwap.  It visually demonstrates the strengths and weaknesses of various approaches, including the proposed MIDI method, by showing the generated 3D scenes side-by-side with the input images. This allows for a direct visual assessment of the accuracy, completeness, and coherence of generated scenes, particularly regarding object shapes, spatial relationships, and overall scene quality.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.03558/x7.png", "caption": "Figure 6: Qualitative comparisons on real-world data, including Matterport3D\u00a0[3] and ScanNet\u00a0[8].", "description": "Figure 6 presents a qualitative comparison of 3D scene generation results from real-world images using different methods.  It shows input images from Matterport3D and ScanNet datasets alongside the 3D reconstructions produced by various methods, including the proposed MIDI model.  This allows for a visual assessment of the accuracy, completeness, and overall quality of the 3D scenes generated by each technique.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.03558/x8.png", "caption": "Figure 7: Qualitative ablation studies on the number of multi-instance attention layers K\ud835\udc3eKitalic_K, and the use of global scene image conditioning, and mixed training with single-object dataset.", "description": "This figure presents a qualitative comparison of ablation studies conducted on the MIDI model.  It shows the impact of varying the number of multi-instance attention layers (K), the inclusion or exclusion of global scene image conditioning, and the use of mixed training with a single-object dataset.  By comparing the generated 3D scenes under different conditions, the figure illustrates how these factors affect the model's ability to generate accurate and coherent 3D scenes with correct spatial relationships between objects.", "section": "5.5. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2412.03558/x9.png", "caption": "Figure 8: Qualitative comparisons on stylized images that are generated by text-to-image diffusion models.", "description": "This figure displays a qualitative comparison of 3D scene generation results from stylized images.  Stylized images, generated using a text-to-image diffusion model, are used as input to different scene generation methods.  The comparison highlights the ability of the proposed MIDI model to generate more coherent and accurate 3D scenes compared to existing methods, particularly when dealing with diverse image styles.", "section": "5.4 Scene Generation from Stylized Images"}]