[{"figure_path": "2410.13924/tables/table_1_0.html", "caption": "Table 1. The size of dataset that is used for training and evaluation in this work. We provide by far the largest real-world labeled training dataset compared to existing real-world datasets. We provide automatically generated dense semantic annotations for 4471 training trajectories and 548 validation trajectories.", "description": "Table 1 presents the sizes of various datasets used for training and evaluation in the paper, highlighting the significantly larger scale of the ARKit LabelMaker dataset compared to existing datasets.", "section": "4.2. Datasets and Metrics for Evaluation"}, {"figure_path": "2410.13924/tables/table_5_0.html", "caption": "Table 1. The size of dataset that is used for training and evaluation in this work. We provide by far the largest real-world labeled training dataset compared to existing real-world datasets. We provide automatically generated dense semantic annotations for 4471 training trajectories and 548 validation trajectories.", "description": "Table 1 shows the number of training, validation, and test data samples in several datasets used for 3D semantic segmentation, including the newly generated ARKit LabelMaker dataset.", "section": "4.2. Datasets and Metrics for Evaluation"}, {"figure_path": "2410.13924/tables/table_6_0.html", "caption": "Table 2. Semantic Segmentation Scores on ScanNet20. We compare different training strategies for two top-performing models (PointTransformerv3 [36] and MinkowskiNet [7]) on the Scan-Net20 dataset. We can show for both models adding ALS200 through pre-training and co-training improves the performance for both models. With PonderV2 [42] and Mix3D [20], we compare large-scale pretraining to two other training strategies. We can show that large-scale pre-training is superior to both, extensive data augmentation (Mix3D) and self-supervised pre-training (PonderV2).", "description": "This table compares the performance of different training strategies for PointTransformerV3 and MinkowskiNet models on the ScanNet20 dataset, highlighting the benefits of large-scale pre-training with automatically generated labels.", "section": "4. Results"}, {"figure_path": "2410.13924/tables/table_6_1.html", "caption": "Table 3. Semantic Segmentation Scores on ScanNet200 [29].", "description": "Table 3 compares different training strategies for two top-performing models (PointTransformerv3 [36] and MinkowskiNet [7]) on the ScanNet200 dataset, showing the performance improvement by adding ALS200 through pre-training and co-training.", "section": "4. Results"}, {"figure_path": "2410.13924/tables/table_6_2.html", "caption": "Table 4. Semantic Segmentation Scores on ScanNet++ [39]. We evaluated the efficacy of our ALC dataset on the ScanNet++ benchmark using both pre-training and joint training methods. \u2020: this number comes from Wu et al.", "description": "Table 4 presents the semantic segmentation scores on the ScanNet++ benchmark, comparing different training strategies (pre-training and joint training) and datasets.", "section": "4. Results"}, {"figure_path": "2410.13924/tables/table_8_0.html", "caption": "Table B1. ScanNet200 validation and test mIoU for head, common and tail classes. For MinkowskiNet, ARKit LabelMaker pre-trained network shows significant improvement on head and common classes. For PTv3, we see improvements across all three splits.", "description": "Table B1 shows the performance of different models on ScanNet200 dataset, categorized by head, common, and tail classes, demonstrating the effectiveness of ARKit LabelMaker pre-training.", "section": "B. Head, common and tail split mIoU scores for ScanNet200"}, {"figure_path": "2410.13924/tables/table_9_0.html", "caption": "Table 1. The size of dataset that is used for training and evaluation in this work. We provide by far the largest real-world labeled training dataset compared to existing real-world datasets. We provide automatically generated dense semantic annotations for 4471 training trajectories and 548 validation trajectories.", "description": "Table 1 shows the size of different datasets used for training and evaluation in the paper, highlighting the significantly larger size of the ARKit LabelMaker dataset compared to existing ones.", "section": "4.2. Datasets and Metrics for Evaluation"}]