[{"figure_path": "https://arxiv.org/html/2412.11258/x1.png", "caption": "Figure 1: \nGaussianProperty\nis a training-free framework, aiming at adding physical properties to 3D Gaussians with the assistance of LMMs. By assigning physical properties to 3D Gaussians, it promotes several downstream tasks such as physical-based generative dynamics and robot grasping in this work.", "description": "GaussianProperty adds physical properties like material, density, etc., to 3D Gaussian models.  This enhances applications such as physics simulations (how objects move and interact) and robot grasping (choosing appropriate grip forces).  The framework uses Large Multimodal Models (LMMs) and doesn't require training.  The figure shows the process: multi-view images as input, processing by SAM, LMM adding properties, and resulting 3D Gaussians used for simulation and robotic grasping.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.11258/x2.png", "caption": "Figure 2: Overall pipeline. Our Gausssian-Property initially leverages SAM to get the segmentation map of the object. Then the original images and the masks are sent to the foundation\nmodels like GPT-4V(ision) to get the corresponding physical properties by inquiring the material candidates. After acquiring physical properties from 2D images, we using a multi-view approach and a voting strategy to add physical properties to the reconstruction 3D Gaussians.", "description": "GaussianProperty begins by employing SAM to segment the object in the input image, creating segmentation masks.  These masks, along with the original images, are then fed into GPT-4V(ision). GPT-4V(ision) leverages a pre-defined material library to predict the physical properties of the segmented regions within the image. This involves matching the visual features of the segments with known material properties. Finally, the estimated physical properties from the 2D images are projected and aggregated onto a 3D Gaussian representation of the object. This utilizes a multi-view approach and a voting strategy to consolidate the 2D estimations into a coherent 3D representation enriched with physical properties.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.11258/x3.png", "caption": "Figure 3: Left: GPT-4V(ision) struggles to recognize the material when directly provided with both global and partial image inputs. Right: Enhanced with combined global-local information and association, the agent accurately characterizes the component\u2019s properties.", "description": "This figure illustrates the effectiveness of the combined global-local reasoning module.  On the left, when provided with both the full image and a masked partial image, GPT-4V struggles to identify the dumbbell handle material. On the right, by first providing a global context image, then the masked partial image, and finally the isolated part image, GPT-4V correctly identifies the handle material as metal (often steel).  This demonstrates the importance of contextual information and part-level association for accurate material recognition by LMMs.", "section": "3.3.2. Combined Global-Local Reasoning Module"}, {"figure_path": "https://arxiv.org/html/2412.11258/x4.png", "caption": "Figure 4: Qualitative results of Material Segmentation. Our model makes boundary-accurate physical material predictions.", "description": "This figure showcases qualitative results of material segmentation. There are four groups of images displayed, each comparing an input RGB image with two corresponding material segmentation outputs: one from a baseline method and one from the proposed method. As shown by the color-coded segmentation outputs, the proposed method delivers more accurate and boundary-precise material predictions compared to the baseline method.  For instance, the proposed method can accurately segment the plastic backrest and the metal frame of a chair, while the baseline method struggles to make these distinctions.", "section": "4.3. Material Segmentation"}, {"figure_path": "https://arxiv.org/html/2412.11258/x5.png", "caption": "Figure 5: Generative Dynamics. We present a potential downstream task of 3D Gaussians with physical property, i.e., the generative dynamics. By imposing force, the 3D Gaussians generate corresponding motion. For example, in the first row, we applied a top-down force, the chair exhibited a movement corresponding to the applied force.", "description": "This figure showcases a downstream application of 3D Gaussians enhanced with physical properties, demonstrating generative dynamics.  By applying forces to the Gaussian representations of objects (a pillow, garbage bin, and chair), the models exhibit realistic movement corresponding to the applied force. In the first row, a chair reacts to a top-down force, realistically illustrating the potential of this physics-based simulation.", "section": "4.4. Generative Dynamics"}, {"figure_path": "https://arxiv.org/html/2412.11258/x6.png", "caption": "Figure 6: Robot Grasping is a downstream application of GaussianProperty. Several sample cases from robot grasping experiments are presented, where we compare our proposed method (right) against three baselines (middle columns), starting from initial configurations (left).", "description": "This figure showcases examples of robot grasping experiments, comparing the proposed GaussianProperty method against three baselines (MinGF, MidGF, and MaxGF) across different objects and materials, starting from initial to final configurations. It highlights the superior performance of GaussianProperty in successfully grasping objects of diverse material properties without causing damage or slippage.", "section": "4.5. Robot Grasping"}, {"figure_path": "https://arxiv.org/html/2412.11258/x7.png", "caption": "Figure 7: The\nrobot platform (left) and the \nrobotic gripper\n(right) \nutilized in robot grasping experiments.", "description": "The figure presents the Jacobi.ai JSR-1 robot platform with a TEK CTAG2F90-C robotic gripper used in the robot grasping experiments. The left image shows the overall robot platform, while the right image focuses on the close-up view of the gripper.", "section": "Supplementary Material, Section B. Robot Grasping Experiment Details"}, {"figure_path": "https://arxiv.org/html/2412.11258/x8.png", "caption": "Figure 8: Calibration curve of robotic gripper grasping force (left) and its 5th-order polynomial smoothings (middle and right).", "description": "This figure presents the calibration curve used to determine the relationship between the normalized input to the robotic gripper (ranging from 0 to 100) and the actual grasping force exerted by the gripper in Newtons. The left plot shows the raw data points from the calibration measurements, while the middle and right plots display smoothed versions of the curve using a 5th-order polynomial fit.  This calibration is essential for accurately controlling the grasping force during the robot experiments, as it allows for mapping the desired force to the appropriate normalized input value for the gripper.", "section": "B. Robot Grasping Experiment Details"}, {"figure_path": "https://arxiv.org/html/2412.11258/x9.png", "caption": "Figure 9: List of selected objects for robot grasping experiments.", "description": "This figure presents sixteen real-world objects selected for robotic grasping experiments. These objects represent common household items made of diverse materials like plastic, wood, glass, etc. Their inclusion aims to test the robustness of a grasping method across varying weights, shapes, and material properties, as a naive approach might struggle with such diversity.", "section": "4.1. Datasets and Evaluation Protocol"}, {"figure_path": "https://arxiv.org/html/2412.11258/x10.png", "caption": "Figure 10: Complete robot grasping experiment results. The 16 test cases along with results in robot grasping experiments are listed. We compare our proposed method (right) against three baselines (middle columns), starting from initial configurations (left). You can view the MP4 videos of the experiments in our project page.", "description": "This figure showcases the results of real-world robotic grasping experiments across 16 test cases, comparing the proposed GaussianProperty method against three baseline grasping strategies (MinGF, MidGF, and MaxGF). Each row displays a different object, with images from left to right showing the initial configuration, the baseline attempts, and the final result using the proposed approach.  The figure highlights successful grasps, failures, and any instances of slippage or damage. Videos of the experiments are available on the project page.", "section": "4.5. Robot Grasping"}, {"figure_path": "https://arxiv.org/html/2412.11258/x11.png", "caption": "Figure 11: Qualitative comparison of hardness prediction. Compared to NeRF2Physics, our method provides more accurate hardness prediction with clear boundaries.", "description": "This figure presents a qualitative comparison of hardness prediction between the proposed method and the baseline method, NeRF2physics, on the same object.  The top row displays the input RGB image and the generated hardness predictions from NeRF2physics. The bottom row shows the segmentation map used by the proposed method and its corresponding hardness prediction. The proposed method generates more accurate predictions, marked by clear boundaries between areas with different hardness levels, in contrast to the baseline method, which produces a more blurred and less accurate prediction.", "section": "C. More Results of Experiments / C.3. Hardness Estimation"}, {"figure_path": "https://arxiv.org/html/2412.11258/x12.png", "caption": "Figure 12: Segmentation process using SAM at different levels of granularity. From left to right: the input image, large-level segmentation, middle-level segmentation, and small-level segmentation. For our model, we selected the middle-level of SAM prediction to balance part-level object understanding and computational efficiency.", "description": "This figure illustrates the process of segmenting an image using the Segment Anything Model (SAM) at three different levels of detail: large, medium, and small.  Each level provides progressively finer segmentations. The authors chose the medium level for their model to balance the trade-off between detailed part-level understanding and computational cost.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.11258/x13.png", "caption": "Figure 13: Examples of data labeling. These objects are sourced from the ABO-500 dataset.", "description": "This figure showcases examples of how different parts of 3D objects are labeled with their corresponding material types. The labeling process involves using an interactive segmentation tool to precisely annotate each part of the object in specific views.  The examples provided include a chair labeled with wood, foam, and metal; a hammer labeled with wood and metal; a hand warmer labeled with leather and metal; a bin labeled with metal; a table labeled with wood and metal; and a folding chair labeled with wood and metal. These labeled objects are selected from the ABO-500 dataset, a subset of the Amazon Berkeley Objects (ABO) dataset, specifically designed for evaluating physical reasoning and robotic manipulation tasks.", "section": "E. Detail of Data Labeling"}, {"figure_path": "https://arxiv.org/html/2412.11258/x14.png", "caption": "Figure 14: Prompt used for proposing materials and other physical properties.", "description": "This figure showcases the prompt employed by GaussianProperty to elicit material proposals and other physical attributes from the Large Multimodal Model (LMM). The prompt instructs the LMM to first offer a concise caption of the part based on the image. Subsequently, the LMM is tasked with describing the material composition of the part, primarily focusing on the major component. Finally, by combining knowledge of the object and its material, the LMM predicts other properties like hardness, density, Young's modulus, and Poisson's Ratio. The prompt also incorporates adaptive hardness scale selection (Shore A or Shore D) depending on the predicted material. The response format is structured as a tuple for parsing: (caption, material, hardness low-high, <Shore A or Shore D>, density, Young's modulus, Poisson's Ratio).  A predefined library of common materials ensures consistent responses. This structured prompting approach aids the LMM in accurately identifying physical properties by providing clear context and specific instructions.", "section": "E.1. Prompting Details"}, {"figure_path": "https://arxiv.org/html/2412.11258/x15.png", "caption": "Figure 15: Effects of Frequency-based Voting Strategy. We provide an example to demonstrate the effectiveness of the frequency-based voting strategy. The result misclassified the \u201caluminum\u201d and \u201cwood\u201d into \u201cplastic\u201d and \u201c\u2019steel\u2019 without voting strategy.", "description": "This figure demonstrates the effectiveness of the frequency-based voting strategy in material segmentation. Without voting, the method misclassifies \"aluminum\" and \"wood\" as \"plastic\" and \"steel.\" With voting across multiple views, accurate material predictions can be obtained. In view 1, voting correctly identifies the dumbbell handle as \"aluminum,\" while view 9 correctly identifies it as \"wood.\" The figure includes images of the original object, property predictions with and without voting, and a segmentation map highlighting the object part.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.11258/x16.png", "caption": "Figure 16: Qualitative comparison of Material Segmentation. These objects are sourced from the ABO-500 dataset.", "description": "This figure visually compares the material segmentation results obtained using NeRF2Physics and the method proposed in the paper.  The objects shown, sourced from the ABO-500 dataset, include a ladder, a clothing rack, a chair, a sofa, a planter, and a bench. For each object, the input RGB image, features extracted using CLIP, and the material segmentation produced by each method are presented side-by-side.  The segmentation maps offer a color-coded visualization of the different materials identified on each object.  The proposed method generally demonstrates more accurate and detailed material segmentation compared to NeRF2Physics, particularly in distinguishing finer object parts and materials.  For instance, the proposed method correctly identifies the wooden steps and metal frame of the ladder, while NeRF2Physics misclassifies some parts.  Similarly, for the chair, the proposed method differentiates the fabric seat and wooden frame, which NeRF2Physics fails to capture accurately.", "section": "4.3. Material Segmentation."}, {"figure_path": "https://arxiv.org/html/2412.11258/x17.png", "caption": "Figure 17: Qualitative results of object material segmentation on MVImgNet. Our model makes reasonable and boundary-accurate material predictions for objects with multiple or single materials.", "description": "Qualitative results of material segmentation on objects from the MVImgNet dataset are presented.  The model accurately segments various materials like wood, fabric, steel, plastic, and glass across different objects, some of which have multiple materials.", "section": "4. Experiments"}]