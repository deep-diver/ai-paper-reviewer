{"references": [{" publication_date": "2023", "fullname_first_author": "Sumyeong Ahn", "paper_title": "Cuda: Curriculum of data augmentation for long-tailed recognition", "reason": "This paper is highly relevant because it also addresses the long-tail problem and proposes a curriculum learning approach.  Comparing and contrasting the CUDA method's engineered augmentations with the DisCL's generative approach provides a valuable comparison and insight into the relative strengths and weaknesses of different strategies for handling data imbalance.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Shekoofeh Azizi", "paper_title": "Synthetic data from diffusion models improves imagenet classification", "reason": "This paper directly addresses the use of synthetic data generated by diffusion models to improve image classification.  It serves as a crucial benchmark and comparison point for evaluating the effectiveness of the DisCL method in generating and utilizing synthetic data for improving classification performance, particularly in the context of diffusion model applications.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Hritik Bansal", "paper_title": "Leaving reality to imagination: Robust classification via generated datasets", "reason": "This paper is highly relevant as it demonstrates the benefits of using synthetic data, generated using different methods than the one in this paper, to improve classification performance. Comparing the results and approaches between this paper and the current work provides valuable insights into the best methodologies for leveraging synthetic data in enhancing classification model robustness and accuracy.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Sara Beery", "paper_title": "Synthetic examples improve generalization for rare classes", "reason": "This work directly addresses the use of synthetic data to improve the performance on rare classes.  It provides a strong foundational context for the current paper's approach to long-tail classification, as both papers focus on improving performance on less-represented classes, but they differ in how they generate and incorporate this data into training.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Sara Beery", "paper_title": "The iwildcam 2021 competition dataset", "reason": "As a key dataset used in the experimental section, this paper is of significant importance.  It establishes the context and characteristics of the iWildCam dataset, providing a benchmark for the DisCL method's ability to enhance the performance of image classification in low-quality data scenarios.  The paper's description of the dataset's characteristics and challenges serves as a critical foundation for the evaluation performed in the experiments.", "section_number": 5}, {" publication_date": "2009", "fullname_first_author": "Yoshua Bengio", "paper_title": "Curriculum learning", "reason": "This seminal paper introduces the concept of curriculum learning, a crucial foundation for the DisCL method. Understanding Bengio's original work on curriculum learning provides the theoretical basis for interpreting and evaluating the DisCL's novel adaptive curriculum learning strategy, allowing for a deeper understanding of its novelty and potential advantages over traditional learning methods.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Kaidi Cao", "paper_title": "Learning imbalanced datasets with label-distribution-aware margin loss", "reason": "This paper is highly relevant due to its focus on handling imbalanced datasets, a core issue in long-tail classification.  The techniques and insights presented in this work provide valuable context for understanding and evaluating the efficacy of DisCL's approach to addressing class imbalances, particularly when comparing results and drawing contrasts between different methods of handling imbalanced data.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Huiwen Chang", "paper_title": "Muse: Text-to-image generation via masked generative transformers", "reason": "This paper showcases the effectiveness of a different generative model in the text-to-image domain.  Including this reference provides a comparison point for evaluating the effectiveness of the diffusion model used in DisCL, allowing for a more nuanced understanding of the choices made and the relative merits of different generative models in the context of synthetic data generation for improving machine learning model performance.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Kevin Clark", "paper_title": "Text-to-image diffusion models are zero shot classifiers", "reason": "This paper directly highlights the connection between text-to-image diffusion models and zero-shot classification, a crucial aspect of the DisCL method's application to long-tail classification.  Understanding how diffusion models inherently allow for zero-shot classification helps clarify and contextualize DisCL's use of generated images for improving the accuracy of models, particularly those trained on imbalanced data.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Lisa Dunlap", "paper_title": "Diversify your vision datasets with automatic diffusion-based augmentation", "reason": "This paper is directly relevant as it explores data augmentation using diffusion models.  Comparing and contrasting the approach in this paper with DisCL's more sophisticated curriculum learning approach illuminates the relative advantages and disadvantages of different methods of using diffusion models to improve the quality and diversity of training data in machine learning.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Sheng Guo", "paper_title": "Curriculumnet: Weakly supervised learning from large-scale web images", "reason": "This paper is important because it demonstrates the use of curriculum learning in weakly supervised settings, providing an additional context to understand how such techniques can be used in generating and using synthetic data.  The insights from this paper on curriculum learning can be used to better understand and compare the approaches used in the DisCL method to create and manage the sequence of synthetic data for improving learning.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Irena Gao", "paper_title": "Out-of-distribution robustness via targeted augmentations", "reason": "This paper directly addresses the issue of out-of-distribution robustness, which is critically important in the context of using synthetic data for training.  By understanding the challenges and strategies related to OOD performance, and how the current paper's method addresses these challenges, we are able to gain a better understanding of DisCL's ability to improve both ID and OOD performance.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Sachin Goyal", "paper_title": "Finetune like you pretrain: Improved finetuning of zero-shot vision models", "reason": "This paper is highly relevant because it directly addresses issues related to finetuning pretrained models. Since DisCL employs pretrained models, this paper provides a valuable perspective on how pretrained models can be adapted and improved upon for specific tasks.  The insights from this work inform the choices and techniques used in fine-tuning the models within the DisCL framework.", "section_number": 5}, {" publication_date": "2004", "fullname_first_author": "Thomas A Halgren", "paper_title": "Glide: a new approach for rapid, accurate docking and scoring. 2. enrichment factors in database screening", "reason": "This paper is referenced because it highlights a very early usage of Glide in the context of molecular docking and database screening.  While not directly related to image generation or curriculum learning, this work is mentioned because of its early application of the model (Glide) and highlights the historical development of generative models used in the current paper.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Ruifei He", "paper_title": "Is synthetic data from generative models ready for image recognition?", "reason": "This paper directly addresses the suitability of synthetic data from generative models for image recognition, providing a valuable comparison point for evaluating the success of DisCL.  It provides a benchmark and context for assessing the effectiveness of the DisCL method in generating synthetic data and improving recognition accuracy.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Jonathan Ho", "paper_title": "Classifier-free diffusion guidance", "reason": "This paper is critical to the methodology of the current paper because it introduces the concept of classifier-free guidance in diffusion models, a key technique used in DisCL's synthetic data generation process. Understanding the principles and advantages of classifier-free guidance is crucial for appreciating the novelty and effectiveness of DisCL's approach to generating synthetic images while avoiding the need for explicit classifiers.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Chengkai Hou", "paper_title": "When to learn what: Model-adaptive data augmentation curriculum", "reason": "This paper is directly relevant to DisCL because it proposes a model-adaptive data augmentation curriculum. Comparing and contrasting this approach with DisCL's adaptive curriculum allows for a nuanced understanding of the relative strengths and weaknesses of different adaptive curriculum strategies and the context of the proposed approach.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Ziwei Liu", "paper_title": "Large-scale long-tailed recognition in an open world", "reason": "This paper is highly significant because it introduces the ImageNet-LT dataset, a key benchmark used in the experimental evaluation of DisCL.  The paper's description of the ImageNet-LT dataset's characteristics and challenges provides a critical context for evaluating the DisCL method's effectiveness in improving the performance of long-tail classification.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Jiaming Song", "paper_title": "Denoising diffusion implicit models", "reason": "This paper introduces the fundamental concept of denoising diffusion implicit models (DDIM), which forms the basis of the diffusion models employed in DisCL.  Understanding the theoretical foundations and workings of DDIM is crucial for understanding how DisCL leverages diffusion models to generate synthetic images and how the image guidance parameter (\u03bb) affects the generated data.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Chitwan Saharia", "paper_title": "Photorealistic text-to-image diffusion models with deep language understanding", "reason": "This paper is highly relevant because it introduces Imagen, a prominent text-to-image diffusion model.  Including this reference provides a comparison point and context for understanding the capabilities and limitations of the diffusion models used in the DisCL approach to generating high-quality synthetic images for training machine learning models.", "section_number": 2}]}