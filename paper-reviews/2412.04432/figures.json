[{"figure_path": "https://arxiv.org/html/2412.04432/x1.png", "caption": "Figure 1: We utilize the diffusion procedure to learn a video tokenizer in a self-supervised manner for unified comprehension and generation, where the spatiotemporal representations serve as the condition of a diffusion model to de-noise video clips. Additionally, the proxy diffusion model functions as a de-tokenizer to decode realistic video clips from the video representations.", "description": "This figure illustrates the Divot framework, which uses a diffusion model for self-supervised video representation learning.  The process begins with a video tokenizer that extracts spatiotemporal features from a video. These features act as conditioning information for a diffusion model, which is trained to remove noise from corrupted video clips.  Crucially, because the diffusion model learns to reconstruct the video from noisy input conditioned on the tokenizer's features, the tokenizer is implicitly learning robust and effective representations of the video. Moreover, the diffusion model itself also acts as a de-tokenizer, enabling the generation of realistic video clips from the learned video representations. This dual functionality of the diffusion model allows for unified video comprehension and generation within a single framework.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2412.04432/x2.png", "caption": "Figure 2: Overview of Divot tokenization and de-tokenization. During training, sparsely sampled video frames are fed into the tokenizer to obtain spatiotemporal representations. These representations serve as the conditions for a U-Net, which is trained to de-noise the noisy VAE latents of densely sampled video frames. During inference, the video representations from the Divot tokenizer can be decoded into realistic video clips with the U-Net.", "description": "Figure 2 illustrates the Divot model's tokenization and de-tokenization processes.  During training, the model takes sparsely sampled video frames as input to its tokenizer. This tokenizer generates spatiotemporal representations of the video. These representations are then used to condition a U-Net, a type of neural network, which is trained using a self-supervised approach. The U-Net's task is to remove noise from noisy representations (VAE latents) of densely sampled video frames. The success of the U-Net in denoising is directly tied to how well the tokenizer captures relevant information from the video.  In essence, the training process teaches the tokenizer to create effective representations by training the network to reconstruct a high-quality version of the original video based on those representations.  Critically, the same U-Net then also functions as a de-tokenizer. During inference (when using the model after training), the tokenizer produces video representations that are passed to the de-tokenizer (which is the same U-Net) to generate realistic video clips.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.04432/x3.png", "caption": "Figure 3:  Overview of Divot-LLM. Video features from the Divot tokenizer are fed into the LLM to perform next-word prediction for video comprehension, while learnable queries are input into the LLM to model the distributions of Divot features using a Gaussian Mixture Model (GMM) for video generation. During inference, video features are sampled from the predicted GMM distribution to decode videos using the de-tokenizer.", "description": "Divot-LLM uses the Divot tokenizer to feed video features into a pre-trained language model (LLM).  For video comprehension, next-word prediction is performed on video-caption data. Video generation is achieved by using learnable queries within the LLM to model the distribution of Divot features using a Gaussian Mixture Model (GMM).  During inference, video features are sampled from this GMM and used by the de-tokenizer to generate video clips. ", "section": "3.2. Video Representation Modeling with LLM"}, {"figure_path": "https://arxiv.org/html/2412.04432/x4.png", "caption": "Figure 4: Paradigms for modeling video representations from the Divot tokenizer with a LLM for video generation. (a) MSE Regression, where the LLM output is trained to minimize its distance with video features using Mean Squared Error (MSE) loss; (b) Diffusion Modeling, where the LLM output is fed into a denoising network as the condition to predict the noise added to video features; (c) GMM Modeling, where the LLM output is trained to predict the parameters of a Gaussian Mixture Model (GMM) for modeling video feature distributions.", "description": "Figure 4 illustrates three different approaches for using a large language model (LLM) to generate videos based on video representations from the Divot tokenizer.  (a) MSE Regression: The LLM directly predicts video features, with training focused on minimizing the difference between the prediction and the actual features using mean squared error. (b) Diffusion Modeling: The LLM output is used as a condition for a denoising network; this network aims to predict the noise added to the video features during a diffusion process. (c) GMM Modeling:  The LLM predicts the parameters (means, variances, and mixture probabilities) of a Gaussian Mixture Model (GMM), which models the probability distribution of the video features.  This probabilistic approach is designed to better capture the diversity of video features.", "section": "3.2. Video Representation Modeling with LLM"}, {"figure_path": "https://arxiv.org/html/2412.04432/x5.png", "caption": "Figure 5: Reconstructed videos, where the Divot tokenizer obtains spatiotemporal representations of sparsely sampled video frames and the de-tokenizer decodes these representations into semantically aligned and temporally coherent video clips.", "description": "Figure 5 showcases the reconstruction capabilities of the Divot model.  The Divot tokenizer processes sparsely sampled video frames (low frame rate) to extract spatiotemporal features, which capture both the spatial information of the individual frames and the temporal dynamics across the sequence.  These features, acting as a compressed representation of the video, are then fed into a de-tokenizer (part of the Divot framework). The de-tokenizer reconstructs a higher frame rate video. The reconstructed video maintains the semantic content of the original, while demonstrating temporal coherence and alignment with the original video's meaning.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.04432/x6.png", "caption": "Figure 6: Qualitative comparison of text-to-video generation with MLLMs that are capable of unified video comprehension and generation. Divot-LLM effectively generates videos that are semantically aligned with text prompts, accurately reflecting temporal changes.", "description": "Figure 6 presents a qualitative comparison of video generation capabilities between Divot-LLM and other state-of-the-art Multimodal Large Language Models (MLLMs).  The figure showcases example video clips generated from text prompts by various models. The key takeaway is that Divot-LLM produces videos which closely match the semantics of the input text, and also accurately reflect the temporal progression and changes described in the prompts. This demonstrates Divot-LLM's ability to seamlessly unify video comprehension and generation.", "section": "4.2. Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2412.04432/x7.png", "caption": "Figure 7: Qualitative examples of video storytelling by Divot-LLM. Given a story instruction, Divot-LLM can generate rich textual narratives along with corresponding video clips that are temporally coherent in an interleaved manner.", "description": "Figure 7 showcases the video storytelling capabilities of the Divot-LLM model.  Given a short story prompt, the model generates a sequence of interleaved text and video segments that tell a cohesive story. The video clips are contextually relevant to the narrative and temporally coherent, demonstrating the model's ability to integrate visual and textual content in a dynamic and meaningful way.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.04432/x8.png", "caption": "Figure 8: More qualitative examples of reconstructed videos, where the Divot tokenizer obtains spatiotemporal representations of sparsely sampled video frames and the de-tokenizer decodes these representations into semantically aligned and temporally coherent video clips.", "description": "Figure 8 showcases the video reconstruction capabilities of the Divot model.  The left column displays sparsely sampled input video frames (at 2 frames per second). The Divot tokenizer processes these frames to extract spatiotemporal representations, which capture both spatial details and temporal dynamics. These representations are then fed into the de-tokenizer (a denoising U-Net), which reconstructs the video at a higher frame rate (8 fps), resulting in the semantically and temporally coherent video clips shown in the right column. The reconstruction demonstrates the model's ability to generate realistic and detailed videos from limited input information.", "section": "B. Qualitative Examples"}, {"figure_path": "https://arxiv.org/html/2412.04432/x9.png", "caption": "Figure 9: More qualitative examples of text-to-video generation by Divot-LLM, which effectively generates videos that are both semantically aligned with text prompts and temporally coherent across frames.", "description": "Figure 9 showcases several examples of videos generated by the Divot-LLM model in response to text prompts.  The model successfully produces videos that accurately reflect the textual descriptions while maintaining temporal consistency throughout each video.  The visual quality of the videos is high, and the scenes depicted change smoothly over time, demonstrating the model's ability to both understand and generate coherent video narratives.", "section": "4.2. Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2412.04432/x10.png", "caption": "Figure 10: Qualitative examples of video comprehension by Divot-LLM.", "description": "Figure 10 showcases Divot-LLM's video comprehension capabilities through qualitative examples.  It demonstrates the model's ability to accurately answer questions about video content, including details about object appearance, actions, and unusual events. The examples highlight the model's nuanced understanding of the temporal sequence and the overall context within video clips.", "section": "4.2. Qualitative Evaluation"}]