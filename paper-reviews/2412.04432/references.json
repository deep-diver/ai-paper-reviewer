{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the field of large language models (LLMs), which are central to the approach taken in this work."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-12-01", "reason": "This paper introduced the core concept of diffusion models, which is the primary technical innovation of this paper."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-26", "reason": "Vision Transformers (ViTs) are fundamental to this paper's video tokenizer architecture, and this paper introduced ViTs."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-06-01", "reason": "This paper advanced diffusion models to high-resolution image generation, an important building block for the approach taken here."}, {"fullname_first_author": "Yang Jin", "paper_title": "Video-LaVIT: Unified video-language pre-training with decoupled visual-motional tokenization", "publication_date": "2024-02-01", "reason": "This is a very recent paper that directly tackles unified video comprehension and generation, a key problem addressed in this work."}]}