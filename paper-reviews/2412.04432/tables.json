[{"content": "| Model | LLM size | Video-Gen | EgoSchema | Perception-Test | MVBench | MSVD | ActivityNet |\n|---|---|---|---|---|---|---|---| \n| Gemini 1.0 Pro [58] | - | \u00d7 | 55.7 | 51.1 | - | - | 49.8 |\n| Gemini 1.5 Pro [59] | - | \u00d7 | 63.2 | - | - | - | 56.7 |\n| GPT4-V [46] | - | \u00d7 | 55.6 | - | 43.7 | - | 59.5 |\n| GPT4-O [47] | - | \u00d7 | **72.2** | - | - | - | **61.9** |\n| LLaMA-VID [35] | 7B | \u00d7 | 38.5 | 44.6 | 41.9 | 69.7 | 47.4 |\n| Video-ChatGPT [43] | 7B | \u00d7 | - | - | - | 64.9 | 35.2 |\n| VideoLLaVA [37] | 7B | \u00d7 | 38.4 | 44.3 | 41.0 | 70.7 | 45.3 |\n| VideoChat2 [31] | 7B | \u00d7 | 42.2 | 47.3 | 51.1 | 70.0 | 49.1 |\n| LLaVA-NeXT-Video [38] | 7B | \u00d7 | 43.9 | 48.8 | 46.5 | 67.8 | 53.5 |\n| LLaVA-NeXT-Video [38] | 32B | \u00d7 | 60.9 | - | - | - | 54.3 |\n| PLLaVA [81] | 34B | \u00d7 | - | 58.1 | - | - | 60.9 |\n| LLaVA-OneVision [30] | 72B | \u00d7 | 62.0 | - | - | - | **62.3** |\n| VideoLLaMA2 [10] | 7B | \u00d7 | 51.7 | 51.4 | **54.6** | 70.9 | 50.2 |\n| VideoLLaMA2 [10] | 72B | \u00d7 | **63.9** | **57.5** | **62.0** | 71.0 | 55.2 |\n| LWM [40] | 7B | \u2713 | - | - | - | 55.9 | - |\n| Video-LaVIT [26] | 7B | \u2713 | 37.3 | 47.9 | - | 73.2 | 50.1 |\n| VILA-U [74] | 7B | \u2713 | - | - | - | **75.3** | 52.7 |\n| Divot-LLM | 7B | \u2713 | 46.5 | **58.3** | 52.1 | **76.4** | 55.8 |", "caption": "Table 2: Datasets used for training the tokenizer and Divot-LLM.", "description": "This table details the datasets used in training the Divot video tokenizer and the Divot-LLM model.  It breaks down the data by training stage (tokenizer training, pre-training of the Divot-LLM, and fine-tuning stages), data type (pure video, video-text, image-text, etc.), and specific dataset names (WebVid-10M, Panda-70M, etc.).  This provides context on the types of data used to build both components of the Divot system.", "section": "3.3 Pre-training and Instruction Tuning"}, {"content": "| Model | Data size | Unified | MSR-VTT CLIPSIM (\u2191) | MSR-VTT FVD (\u2193) |\n|---|---|---|---|---|\n| CogVideo [21] | 5.4M | \u00d7 | 0.2631 | 1294 |\n| Video LDM [5] | 10M | \u00d7 | 0.2929 | - |\n| VideoComposer [66] | 10M | \u00d7 | 0.2932 | 580 |\n| InternVid [68] | 28M | \u00d7 | 0.2951 | - |\n| Make-A-Video [53] | 20M | \u00d7 | 0.3049 | - |\n| VideoPoet [29] | 270M | \u00d7 | 0.3049 | 213 |\n| PYoCo [14] | 22.5M | \u00d7 | - | - |\n| SVD [4] | 152M | \u00d7 | - | - |\n| Video-LavIT [26] | 10M | \u2713 | 0.3012 | 188.36 |\n| Loong [69] | 16M | \u00d7 | 0.2903 | 274 |\n| Snap Video [45] | - | \u00d7 | 0.2793 | 110.4 |\n| VILA-U [74] | 1M | \u2713 | 0.2937 | 499.06 |\n| Divot-LLM | 4.8M | \u2713 | 0.2938 | 301.4 |", "caption": "Table 3: Comparison for video comprehension with MLLMs. \u201cVideo-Gen\u201d denotes whether the model can generate videos besides texts. The evaluation metric is accuracy. The best results are bold and the second best results are underlined.", "description": "Table 3 presents a comparison of video comprehension performance across various Multimodal Large Language Models (MLLMs).  It indicates whether each model can generate videos in addition to text. The key evaluation metric is accuracy, with the highest accuracy scores shown in bold and the second-highest underlined.  The table allows for a direct comparison of different models' capabilities in understanding and responding to video content.", "section": "4.1 Quantitative Evaluation"}]