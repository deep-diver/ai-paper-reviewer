[{"figure_path": "2410.13085/tables/table_8_0.html", "caption": "Table 1: Model performance (%) of different methods based on LLaVA-Med-1.5 on medical VQA task. Notably, we report the accuracy, F1 score and AUROC. The best results and second best results are highlighted in red and blue, respectively.", "description": "Table 1 presents a comparison of different methods' performance on medical visual question answering (VQA) using LLaVA-Med-1.5, showing accuracy, F1-score, and AUROC across multiple datasets.", "section": "5.2 MAIN RESULTS"}, {"figure_path": "2410.13085/tables/table_8_1.html", "caption": "Table 2: Model performance (%) of different methods based on LLaVA-Med-1.5 on report generation task. Notably, we report the average BLEU, ROUGE-L, METEOR.", "description": "Table 2 presents a comparison of the performance of various methods on the report generation task using the LLaVA-Med-1.5 model, showing the average BLEU, ROUGE-L, and METEOR scores.", "section": "5.1 EXPERIMENTAL SETUPS"}, {"figure_path": "2410.13085/tables/table_9_0.html", "caption": "Table 3: Performance comparison with several Med-LVLMs. Rad: Radiology, Opt: Ophthalmology, Pat: Pathology.", "description": "Table 3 presents a comparison of the performance of MMed-RAG against other open-source Med-LVLMs across different medical image modalities, showing that MMed-RAG significantly outperforms other Med-LVLMs.", "section": "5.3 ANALYSIS"}, {"figure_path": "2410.13085/tables/table_9_1.html", "caption": "Table 4: Ablation results on two datasets covering different domains. RG: report generation, FairVLMed: Harvard-FairVLMed.", "description": "Table 4 presents the ablation study results showing the performance gains of each component of MMed-RAG on two datasets, IU-Xray and Harvard-FairVLMed, for both medical VQA and report generation tasks.", "section": "5.3 ANALYSIS"}, {"figure_path": "2410.13085/tables/table_9_2.html", "caption": "Table 4: Ablation results on two datasets covering different domains. RG: report generation, FairVLMed: Harvard-FairVLMed.", "description": "The table presents the ablation study results for both medical VQA and report generation tasks on the IU-Xray and Harvard-FairVLMed datasets, showing the impact of each component in MMed-RAG.", "section": "5.3 ANALYSIS"}, {"figure_path": "2410.13085/tables/table_13_0.html", "caption": "Table 1: Model performance (%) of different methods based on LLaVA-Med-1.5 on medical VQA task. Notably, we report the accuracy, F1 score and AUROC. The best results and second best results are highlighted in red and blue, respectively.", "description": "Table 1 presents the performance comparison of different methods on the medical visual question answering task using the LLaVA-Med-1.5 model, reporting accuracy, F1 score, and AUROC.", "section": "5.2 Main Results"}, {"figure_path": "2410.13085/tables/table_15_0.html", "caption": "Table 6: Data statistics for medical VQA task. \"Train (DR)\" refers to the number of image-text pairs for retriever training, \"All (RAG-PT)\" refers to the total data for RAG-PT, and \"Train (RAG-PT)-a/b/c\" refer to the respective subsets for RAG-PT training.", "description": "Table 6 shows the data statistics used for training the medical visual question answering (VQA) task, including the number of image-text pairs for retriever training and the distribution of data used for RAG-PT training across three subsets.", "section": "A.1.1 DATA STATISTICS"}, {"figure_path": "2410.13085/tables/table_15_1.html", "caption": "Table 7: Data statistics for report generation. \"Train (DR)\" refers to the number of image-text pairs for retriever training, \"All (RAG-PT)\" refers to the total data for RAG-PT, and \"Train (RAG-PT)-a/b/c\" refer to the respective sample categories for RAG-PT training.", "description": "Table 7 presents the data statistics used for the report generation task, showing the number of image-text pairs for retriever training and the distribution of data used in RAG-PT training across three categories.", "section": "5.1 EXPERIMENTAL SETUPS"}, {"figure_path": "2410.13085/tables/table_15_2.html", "caption": "Table 1: Model performance (%) of different methods based on LLaVA-Med-1.5 on medical VQA task. Notably, we report the accuracy, F1 score and AUROC. The best results and second best results are highlighted in red and blue, respectively.", "description": "Table 1 presents the performance comparison of different methods on medical VQA using LLaVA-Med-1.5, showing accuracy, F1-score, and AUROC across multiple datasets.", "section": "5.2 Main Results"}, {"figure_path": "2410.13085/tables/table_18_0.html", "caption": "Table 10: Performance on different backbones.", "description": "Table 10 presents the performance of MMed-RAG on different backbones across various domains.", "section": "5.3 ANALYSIS"}, {"figure_path": "2410.13085/tables/table_19_0.html", "caption": "Table 11: Model performance (%) of different Med-LVLMs based on LLaVA-Med-1.5 on medical VQA task.", "description": "Table 11 presents a comparison of the performance of various Med-LVLMs on a medical visual question answering task, using metrics such as accuracy, F1-score and AUROC across different datasets.", "section": "5.2 MAIN RESULTS"}]