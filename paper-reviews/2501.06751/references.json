{"references": [{"fullname_first_author": "Andrea Banino", "paper_title": "PonderNet: Learning to Ponder", "publication_date": "2021-00-00", "reason": "This paper introduces the concept of 'PonderNet,' a novel architecture that allows for adaptive computation time in neural networks, a concept relevant to the efficient use of padding tokens in T2I models."}, {"fullname_first_author": "Mikhail S Burtsev", "paper_title": "Memory Transformer", "publication_date": "2020-00-00", "reason": "This paper introduces 'Memory Transformer,' a model that uses memory tokens to store and recall information, a mechanism potentially analogous to how padding tokens might function in some T2I models."}, {"fullname_first_author": "Timoth\u00e9e Darcet", "paper_title": "Vision Transformers Need Registers", "publication_date": "2024-00-00", "reason": "This paper introduces the concept of 'registers' in vision transformers, which are special tokens that store and recall information, similar to how padding tokens might function in some T2I models."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "publication_date": "2024-00-00", "reason": "This paper introduces Stable Diffusion 3, a model architecture that uses multi-modal attention mechanisms, which are relevant to understanding how padding tokens might interact with both image and text representations in the diffusion process."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning Transferable Visual Models From Natural Language Supervision", "publication_date": "2021-00-00", "reason": "This paper introduces CLIP, a foundational model for vision-language understanding, which is highly relevant to the functioning of text-to-image models and the role of textual elements like padding tokens."}]}