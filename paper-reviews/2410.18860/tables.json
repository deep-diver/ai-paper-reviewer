[{"figure_path": "2410.18860/tables/table_6_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods on various faithfulness evaluation tasks, highlighting the best and second-best performing methods for each model.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_6_1.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods across multiple faithfulness evaluation tasks, highlighting the best performing model for each task.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_7_0.html", "caption": "Table 3: Performance of different models and decoding methods on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different LLMs and decoding methods on the MuSiQue dataset with and without Chain-of-Thought prompting in closed-book and open-book settings.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_19_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance of different LLMs and decoding methods on several faithfulness evaluation tasks, highlighting the best performing model for each task.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_19_1.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods on various faithfulness evaluation tasks, highlighting the best-performing model and method for each task.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_19_2.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods across multiple faithfulness evaluation tasks, highlighting the best and second-best performance for each model.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_21_0.html", "caption": "Table 7: Performance comparison of Llama3-8B-Instruct with different number of masked retrieval heads on factuality evaluation tasks.", "description": "The table shows the performance comparison of Llama3-8B-Instruct model on factuality evaluation tasks with varying numbers of masked retrieval heads.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_21_1.html", "caption": "Table 7: Performance comparison of Llama3-8B-Instruct with different number of masked retrieval heads on factuality evaluation tasks.", "description": "This table presents the performance comparison of Llama3-8B-Instruct model on factuality evaluation tasks with varying numbers of masked retrieval heads.", "section": "DeCoRe Mitigates Factuality Hallucinations"}, {"figure_path": "2410.18860/tables/table_22_0.html", "caption": "Table 9: Performance comparison of Llama3-8B-Instruct with different number of masked retrieval heads on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings.", "description": "Table 9 shows the performance of Llama3-8B-Instruct model on MuSiQue dataset with different numbers of masked retrieval heads, both with and without Chain-of-Thought prompting, and under both closed-book and open-book settings.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_22_1.html", "caption": "Table 10: Performance comparison of Llama3-8B-Instruct with different numbers of masked random heads on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings.", "description": "This table presents the performance of Llama3-8B-Instruct model on MuSiQue with different numbers of masked random heads, evaluating its performance with and without chain-of-thought prompting in both closed and open book settings.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_23_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods on multiple faithfulness evaluation tasks, highlighting the best-performing model for each task.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_24_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods on various faithfulness evaluation tasks, highlighting the best performing model for each task and base model.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_26_1.html", "caption": "Table 15: Ablation study of DeCoRe entropy on faithfulness hallucination tasks with varying numbers of masked random heads.", "description": "The table presents the performance comparison of Llama3-8B-Instruct with different numbers of masked random heads on faithfulness evaluation tasks.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_27_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods on several faithfulness evaluation tasks, highlighting the best-performing model and method for each task.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_27_1.html", "caption": "Table 17: Ablation study of DeCoRe entropy on factuality hallucination tasks with varying numbers of masked random heads.", "description": "The table presents the performance comparison of Llama3-8B-Instruct with different numbers of masked random heads on factuality evaluation tasks.", "section": "G.3 Factuality"}, {"figure_path": "2410.18860/tables/table_28_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance of various LLMs and decoding methods on several faithfulness evaluation tasks, highlighting the best-performing methods for each model.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_28_1.html", "caption": "Table 19: Performance comparison across different numbers of masked random heads on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings.", "description": "The table presents the performance of Llama3-8B-Instruct model on MuSiQue with DeCoRe entropy across various numbers of masked random heads, in closed-book and open-book settings, with and without Chain-of-Thought prompting.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_29_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods on various faithfulness evaluation tasks, highlighting the best-performing model for each task.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_30_0.html", "caption": "Table 21: Performance comparison of other model families (i.e., Mistral-7B-Instruct-v0.3 and Qwen2-7B-Instruct) with different decoding strategies on factuality evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "Table 21 presents the performance comparison of different LLMs (Mistral-7B-Instruct-v0.3 and Qwen2-7B-Instruct) across various factuality evaluation tasks using different decoding methods.", "section": "H.2 Factuality"}, {"figure_path": "2410.18860/tables/table_30_1.html", "caption": "Table 22: Performance comparison of other model families (i.e., Mistral-7B-Instruct-v0.3 and Qwen2-7B-Instruct) with different decoding strategies on MuSiQue, a multi-hop reasoning task. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different decoding strategies including DeCoRe on MuSiQue, a multi-hop reasoning task, using Mistral-7B-Instruct-v0.3 and Qwen2-7B-Instruct as base models.", "section": "H.3 Chain of Thought"}, {"figure_path": "2410.18860/tables/table_31_0.html", "caption": "Table 23: Performance of Llama3-8b-Instruct with DeCoRestatic on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "Table 23 shows the performance of Llama3-8b-Instruct model with DeCoRestatic decoding strategy on faithfulness evaluation tasks with varying scaling factor (alpha) values.", "section": "I. Ablation of DeCoRestatic"}, {"figure_path": "2410.18860/tables/table_32_0.html", "caption": "Table 24: Performance of Llama3-8b-Instruct with DeCoRestatic on factuality evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance of Llama3-8b-Instruct with DeCoRestatic across different values of the hyperparameter \u03b1 on factuality evaluation tasks.", "section": "I. Ablation of DeCoRestatic"}, {"figure_path": "2410.18860/tables/table_32_1.html", "caption": "Table 18: Performance comparison across different number of masked retrieval heads on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings.", "description": "This table presents the performance comparison of Llama3-8B-Instruct across different numbers of masked retrieval heads on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting, in both closed-book and open-book settings.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_33_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods on various faithfulness evaluation tasks, highlighting the best-performing model for each task.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_34_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "Table 1 presents the performance comparison of different LLMs and decoding methods on various faithfulness evaluation tasks, highlighting the best-performing model for each task.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_34_1.html", "caption": "Table 3: Performance of different models and decoding methods on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different models and decoding methods on the MuSiQue dataset, with and without chain-of-thought prompting, across closed-book and open-book settings.", "section": "4 RESULTS"}]