[{"figure_path": "2410.18860/tables/table_6_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods on various faithfulness evaluation tasks, showing DeCoRe's improvement over baselines.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_6_1.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods on various faithfulness evaluation tasks, highlighting the best performing model for each task.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_7_0.html", "caption": "Table 3: Performance of different models and decoding methods on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different LLMs and decoding methods on the MuSiQue dataset, with and without Chain-of-Thought prompting, across closed-book and open-book settings.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_19_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different models and decoding methods on several faithfulness evaluation tasks, highlighting the best-performing model for each task.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_19_1.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods across several faithfulness evaluation tasks (XSum, MemoTrap, IFEval, NQ-Open, and NQ-Swap).", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_19_2.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods on various faithfulness evaluation tasks, highlighting the best-performing models and methods for each task.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_21_0.html", "caption": "Table 7: Performance comparison of Llama3-8B-Instruct with different number of masked retrieval heads on factuality evaluation tasks.", "description": "The table presents the performance comparison of Llama3-8B-Instruct with varying numbers of masked retrieval heads on factuality evaluation tasks, showing the impact of masked retrieval heads on factuality.", "section": "DeCoRe Mitigates Factuality Hallucinations"}, {"figure_path": "2410.18860/tables/table_21_1.html", "caption": "Table 2: Performance of different models and decoding methods on factuality evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "Table 2 presents the performance comparison of different LLMs and decoding methods across various factuality evaluation tasks, highlighting the best and second-best performances for each model.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_22_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance of various LLMs and decoding methods on several faithfulness evaluation tasks, highlighting the best performing model for each task and model size.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_22_1.html", "caption": "Table 10: Performance comparison of Llama3-8B-Instruct with different numbers of masked random heads on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings.", "description": "This table presents the performance comparison of Llama3-8B-Instruct model on MuSiQue with different numbers of masked random heads, both with and without CoT prompting in closed-book and open-book settings.", "section": "3.4 DeCoRe Variants"}, {"figure_path": "2410.18860/tables/table_23_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents a comparison of different LLMs and decoding methods on several faithfulness evaluation tasks, highlighting the best-performing models for each task.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_24_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different models and decoding methods on several faithfulness evaluation tasks, highlighting the best performing model for each task and model size.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_26_1.html", "caption": "Table 15: Ablation study of DeCoRe entropy on faithfulness hallucination tasks with varying numbers of masked random heads.", "description": "This table presents the results of an ablation study on the DeCoRe entropy method, varying the number of masked random heads on faithfulness evaluation tasks.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_27_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents a comparison of different LLMs and decoding methods on various faithfulness evaluation tasks, highlighting the best-performing model for each task and model.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_27_1.html", "caption": "Table 7: Performance comparison of Llama3-8B-Instruct with different number of masked retrieval heads on factuality evaluation tasks.", "description": "The table shows the performance comparison of Llama3-8B-Instruct model with different numbers of masked retrieval heads on various factuality evaluation tasks, including TruthfulQA, TriviaQA, PopQA, and NQ-Open.", "section": "DeCoRe Mitigates Factuality Hallucinations"}, {"figure_path": "2410.18860/tables/table_28_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods on faithfulness evaluation tasks, highlighting the best-performing model and method for each task.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_28_1.html", "caption": "Table 19: Performance comparison across different numbers of masked random heads on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings.", "description": "This table presents the performance comparison of Llama3-8B-Instruct with different numbers of masked random heads on MuSiQue, a multi-hop reasoning task, with and without CoT prompting in both closed-book and open-book settings.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_29_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods on multiple faithfulness evaluation tasks, highlighting the best-performing models and methods for each task.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_30_0.html", "caption": "Table 2: Performance of different models and decoding methods on factuality evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "Table 2 presents the performance comparison of different LLMs and decoding methods on various factuality evaluation tasks, highlighting the best-performing models and methods for each task.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_30_1.html", "caption": "Table 22: Performance comparison of other model families (i.e., Mistral-7B-Instruct-v0.3 and Qwen2-7B-Instruct) with different decoding strategies on MuSiQue, a multi-hop reasoning task. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different models and decoding strategies on the MuSiQue multi-hop reasoning dataset, showing the impact of different decoding methods on the accuracy of different models in this task.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_31_0.html", "caption": "Table 23: Performance of Llama3-8b-Instruct with DeCoRestatic on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table shows the performance of Llama3-8b-Instruct model with DeCoRestatic decoding method on faithfulness evaluation tasks using different values of hyperparameter alpha.", "section": "I. Ablation of DeCoRestatic"}, {"figure_path": "2410.18860/tables/table_32_0.html", "caption": "Table 24: Performance of Llama3-8b-Instruct with DeCoRestatic on factuality evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance of Llama3-8b-Instruct with DeCoRestatic on factuality evaluation tasks with varying scaling factor (\u03b1).", "section": "I. Ablation of DeCoRestatic"}, {"figure_path": "2410.18860/tables/table_32_1.html", "caption": "Table 1. Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods on various faithfulness evaluation tasks, highlighting the best-performing method for each model.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_33_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance comparison of different LLMs and decoding methods on multiple faithfulness evaluation tasks, highlighting the best-performing model for each task and base model.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_34_0.html", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "The table presents the performance of different LLMs and decoding methods on various faithfulness evaluation tasks, highlighting the best-performing model and method for each task.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_34_1.html", "caption": "Table 3: Performance of different models and decoding methods on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different models and decoding methods on the MuSiQue dataset, a multi-hop reasoning task, with and without Chain-of-Thought (CoT) prompting in both closed-book and open-book settings.", "section": "3 EXPERIMENT SETUP"}]