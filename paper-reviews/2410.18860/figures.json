[{"figure_path": "2410.18860/figures/figures_2_0.png", "caption": "Overview of the DeCoRe workflow. Given the same input, the base LLM (LLMbase) and the variant with masked retrieval heads (LLMmasked) predict the next token. An uncertainty estimation is applied to the base model's output using conditional entropy: higher conditional entropy increases the contrastive factor (a), penalising predictions that align with the LLMmasked. The final prediction is selected based on weighted contrastive decoding of the outputs from both models, leading to a more grounded response.", "description": "The figure illustrates the DeCoRe workflow, showing how it mitigates hallucinations.  It begins with an input consisting of context and a question. This input is fed into two LLMs: a base LLM and a masked LLM (where retrieval heads are masked). Both LLMs generate predictions for the next token. The uncertainty of the base LLM's prediction is estimated using conditional entropy, which dynamically adjusts a contrastive factor (\u03b1). Higher entropy (more uncertainty) leads to a stronger contrastive effect. Finally, contrastive decoding combines the outputs of both LLMs, weighting the base LLM's prediction more heavily when uncertainty is high, thus prioritizing predictions grounded in the context and model parameters over potentially hallucinatory responses from the masked LLM.", "section": "Decoding by Contrasting Retrieval Heads"}, {"figure_path": "2410.18860/figures/figures_8_0.png", "caption": "Correlation between the number of masked retrieval heads and performance of Llama3-8B-Instruct with DeCoReentropy on each task. The correlations are quantified by the Pearson Correlation Coefficient r for each plot. Detailed results are listed in Table 14 and Table 16.", "description": "Figure 3 visualizes the correlation between the number of masked retrieval heads and the performance of the Llama3-8B-Instruct model using DeCoReEntropy across various tasks.  The figure is divided into three subfigures (a, b, c), each representing a category of tasks: (a) Faithfulness Evaluation Tasks (XSum, MemoTrap, IFEval, NQ-Open, and NQ-Swap), (b) Factuality Evaluation Tasks (TruthfulQA, TriviaQA, PopQA, and Closed Book NQ-Open), and (c) Chain-of-Thought Reasoning Evaluation Tasks (MuSiQue with different combinations of closed/open-book and with/without CoT prompting). Each subfigure displays multiple line graphs, one for each task, plotting performance metrics (e.g., ROUGE-L, Macro Accuracy, EM) against the number of masked retrieval heads. The Pearson correlation coefficient (r) is provided for each task, quantifying the strength and direction of the linear relationship between the number of masked heads and performance.  The figure demonstrates that performance generally correlates with the number of masked retrieval heads, often exhibiting a negative correlation for tasks that require contextual faithfulness, such as summarisation and question answering.  This aligns with the hypothesis that masking retrieval heads impairs the ability of the model to retrieve relevant information, leading to hallucinations.", "section": "Effect of Retrieval Head Masking on Task Performance of DeCoRe"}, {"figure_path": "2410.18860/figures/figures_8_1.png", "caption": "Correlation between the number of masked retrieval heads and performance of Llama3-8B-Instruct with DeCoReentropy on each task. The correlations are quantified by the Pearson Correlation Coefficient r for each plot. Detailed results are listed in Table 14 and Table 16.", "description": "Figure 3 visualizes the relationship between the number of masked retrieval heads and the performance of the Llama3-8B-Instruct model using DeCoReentropy across various tasks.  It presents multiple subplots, each representing a different task (Faithfulness, Factuality, and Chain-of-Thought reasoning). Each subplot shows a line graph plotting the performance metric (e.g., EM score, ROUGE-L, Macro Accuracy) on the y-axis against the number of masked retrieval heads on the x-axis.  A trendline is included in each subplot, and the Pearson correlation coefficient (r) is displayed to quantify the strength and direction of the correlation.  The graphs illustrate the impact of masking retrieval heads on model performance for various tasks requiring contextual faithfulness and factual recall.", "section": "Effect of Retrieval Head Masking on Task Performance of DeCoRe"}, {"figure_path": "2410.18860/figures/figures_8_2.png", "caption": "Correlation between the number of masked retrieval heads and performance of Llama3-8B-Instruct with DeCoReentropy on each task. The correlations are quantified by the Pearson Correlation Coefficient r for each plot. Detailed results are listed in Table 14 and Table 16.", "description": "The figure is a grid of 15 plots, each showing the relationship between the number of masked retrieval heads and the performance of a language model on different tasks. The x-axis of each plot represents the number of masked retrieval heads, while the y-axis represents the performance metric for that task, such as ROUGE-L, Macro Accuracy, EM, or %Truth. Each plot shows a line graph of the performance as a function of the number of masked retrieval heads, with a correlation coefficient (r) displayed in the plot. The plots are organised into three groups based on the evaluation task categories: faithfulness, factuality, and chain-of-thought reasoning. Each group has five plots corresponding to different tasks within that category. The plots indicate positive or negative correlations between the number of masked retrieval heads and performance for each specific task, suggesting that specific retrieval heads play a critical role in the performance of the language model on each task.", "section": "Effect of Retrieval Head Masking on Task Performance of DeCoRe"}, {"figure_path": "2410.18860/figures/figures_18_0.png", "caption": "Overview of the DeCoRe workflow. Given the same input, the base LLM (LLMbase) and the variant with masked retrieval heads (LLMmasked) predict the next token. An uncertainty estimation is applied to the base model's output using conditional entropy: higher conditional entropy increases the contrastive factor (a), penalising predictions that align with the LLMmasked. The final prediction is selected based on weighted contrastive decoding of the outputs from both models, leading to a more grounded response.", "description": "This figure illustrates the DeCoRe workflow.  It begins with an input and context provided to both a base LLM and a masked LLM (the latter having its retrieval heads masked). Both LLMs predict the next token. The base LLM's prediction undergoes uncertainty estimation via conditional entropy; higher entropy increases the contrastive factor, 'a', which penalizes predictions similar to the masked LLM's output. Finally, contrastive decoding combines both LLMs' outputs, weighted by 'a', to produce the final, more grounded prediction.", "section": "DeCoRe: Decoding by Contrasting Retrieval Heads"}, {"figure_path": "2410.18860/figures/figures_20_0.png", "caption": "Correlation between the number of masked retrieval heads and performance of Llama3-8B-Instruct with DeCoReentropy on each task. The correlations are quantified by the Pearson Correlation Coefficient r for each plot. Detailed results are listed in Table 14 and Table 16.", "description": "The figure presents a set of line graphs, each displaying the correlation between the number of masked retrieval heads and the model's performance on specific tasks. The tasks are categorized into three groups: faithfulness evaluation, factuality evaluation, and chain-of-thought reasoning evaluation. Each graph shows the performance (e.g., EM score, accuracy) plotted against the number of masked retrieval heads, along with a line representing the baseline performance (i.e., with no retrieval heads masked). The Pearson correlation coefficient (r) is provided for each graph, indicating the strength and direction of the correlation. The graphs show the impact of masking retrieval heads on various tasks, demonstrating the different ways masking retrieval heads affects faithfulness, factual accuracy, and multi-hop reasoning. The results suggest that retrieval heads play a significant role in task performance and their masking (as done by DeCoRe) can influence model accuracy and faithfulness, particularly on tasks requiring complex reasoning and long-range dependencies.", "section": "Effect of Retrieval Head Masking on Task Performance of DeCoRe"}, {"figure_path": "2410.18860/figures/figures_25_0.png", "caption": "Correlation between the number of masked random heads and performance of Llama3-8B-Instruct with DeCoReentropy on each task. The correlations are quantified by the Pearson Correlation Coefficient r for each plot. Detailed results are listed in Table 14 and Table 16.", "description": "Figure 8 presents the correlation between the number of masked random heads and the performance of Llama3-8B-Instruct using DeCoReEntropy across various tasks, including faithfulness, factuality, and chain-of-thought reasoning. Each subplot represents a specific task, displaying a line graph showing the correlation between the number of masked random heads and the corresponding metric. The correlations are quantified using Pearson Correlation Coefficients (r), which are displayed on each subplot.  The shaded areas represent the confidence intervals around the lines, providing a visual representation of the uncertainty in the correlations.", "section": "Effect of Retrieval Head Masking on Task Performance of DeCoRe"}, {"figure_path": "2410.18860/figures/figures_35_0.png", "caption": "Overview of the DeCoRe workflow. Given the same input, the base LLM (LLMbase) and the variant with masked retrieval heads (LLMmasked) predict the next token. An uncertainty estimation is applied to the base model's output using conditional entropy: higher conditional entropy increases the contrastive factor (a), penalising predictions that align with the LLMmasked. The final prediction is selected based on weighted contrastive decoding of the outputs from both models, leading to a more grounded response.", "description": "The figure is a schematic illustration of the DeCoRe (Decoding by Contrasting Retrieval Heads) workflow. It shows that given the same input, both a base LLM and a variant LLM with masked retrieval heads generate predictions for the next token.  An uncertainty measure, specifically conditional entropy, is calculated for the base LLM's prediction. Higher entropy indicates higher uncertainty. This entropy value dynamically adjusts a contrastive factor (\u03b1) that weighs the contributions of the base LLM and the masked LLM in the final prediction.  Higher entropy leads to a larger contrastive factor, thereby penalising predictions similar to those of the masked LLM and boosting the confidence in the predictions of the base LLM.  The final prediction is a weighted combination of the outputs of both models, aiming to improve the accuracy and reduce hallucination.", "section": "DeCoRe: Decoding by Contrasting Retrieval Heads"}]