{"references": [{" publication_date": "2023", "fullname_first_author": "Muhammad Aurangzeb Ahmad", "paper_title": "Creating trustworthy llms: Dealing with hallucinations in healthcare ai", "reason": "This paper is highly relevant to the core theme of the current study, focusing on the critical issue of hallucinations in LLMs, specifically within the healthcare domain. It provides valuable insights into the challenges and potential solutions for mitigating hallucinations, which directly relates to the current paper's objectives.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "reason": "This foundational paper introduces the concept of few-shot learning in large language models, which is highly relevant to the context of the current work, as DeCoRe aims to improve the performance of LLMs without requiring extensive fine-tuning or retraining.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Shiqi Chen", "paper_title": "In-context sharpness as alerts: An inner representation perspective for hallucination mitigation", "reason": "This paper explores the use of in-context sharpness as a method for detecting and mitigating hallucinations in LLMs. The methodology proposed by this paper is relevant to the current research, which also focuses on mitigating hallucinations in LLMs through the examination and utilization of inner model mechanisms such as attention heads.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Yung-Sung Chuang", "paper_title": "Decoding by contrasting layers improves factuality in large language models", "reason": "This paper proposes a decoding method called DoLA that improves the factuality of LLMs by contrasting layers during the decoding process. The approach is relevant to the present research as both papers deal with improving the output quality of LLMs without requiring extensive fine-tuning.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yung-Sung Chuang", "paper_title": "Lookback lens: Detecting and mitigating contextual hallucinations in large language models using only attention maps", "reason": "This paper investigates the issue of contextual hallucinations in LLMs and proposes a method for mitigation using attention maps. The approach is closely related to DeCoRe's focus on improving contextual faithfulness, and the insights gleaned from this paper are directly relevant to understanding the inner workings of LLMs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Matthew Dahl", "paper_title": "Large legal fictions: Profiling legal hallucinations in large language models", "reason": "This research delves into the specific issue of hallucinations in the legal domain, providing a real-world example of the high-stakes situations where inaccurate outputs of LLMs are unacceptable. This highlights the importance of addressing the problem of hallucinations and underpins the motivation for DeCoRe.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces the Llama 3 family of LLMs, which are used in the current work.  It provides context for the specific LLMs used in the experiments, enabling a more accurate interpretation of the experimental results.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Xiang Lisa Li", "paper_title": "Contrastive decoding: Open-ended text generation as optimization", "reason": "This paper introduces the concept of contrastive decoding, which is a key component of the DeCoRe method.  Understanding contrastive decoding is crucial for understanding the core mechanism of DeCoRe, as it is fundamental to how the model contrasts the outputs of the base and masked LLMs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Junyi Li", "paper_title": "Survey of hallucination in natural language generation", "reason": "This paper provides a comprehensive overview of the current research on hallucinations in NLG and offers a classification of hallucinations into different categories.  Understanding this taxonomy of hallucinations is essential for placing the current research within the broader context of this field and understanding the specific challenges addressed by DeCoRe.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Kenneth Li", "paper_title": "Inference-time intervention: Eliciting truthful answers from a language model", "reason": "This paper proposes an intervention method for improving the factuality and reliability of LLM outputs. The approach is relevant to the present study as both papers aim to enhance the accuracy and faithfulness of LLMs, although DeCoRe achieves this without relying on fine-tuning or additional training.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Stephanie Lin", "paper_title": "Truthfulqa: Measuring how models mimic human falsehoods", "reason": "This paper introduces the TruthfulQA dataset, which is used in the experimental evaluation of the DeCoRe method.  Understanding the nature and characteristics of the TruthfulQA dataset is crucial for interpreting and evaluating the experimental results related to factuality and faithfulness.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Shayne Longpre", "paper_title": "Entity-based knowledge conflicts in question answering", "reason": "This paper introduces the NQ-Swap dataset, which is used in the current study to evaluate the model's ability to handle conflicting information sources and produce accurate answers. Understanding the NQ-Swap dataset is essential for evaluating and interpreting the results, which directly relate to the primary research goals.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Nelson F. Liu", "paper_title": "Lost in the middle: How language models use long contexts", "reason": "This study examines the way LLMs utilize long contexts, a key issue related to the problem of hallucinations and the method DeCoRe aims to improve.  It provides crucial context related to attention mechanisms and the processing of longer contexts within the architecture of LLMs. ", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Xiang Lisa Li", "paper_title": "Contrastive decoding: Open-ended text generation as optimization", "reason": "This paper introduced contrastive decoding, a critical technique that forms the foundation of DeCoRe. This foundational paper provides a theoretical framework which underpins the practical implementation and evaluation of DeCoRe.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yung-Sung Chuang", "paper_title": "Decoding by contrasting layers improves factuality in large language models", "reason": "This paper introduced DoLa, a method compared against the DeCoRe method.  Understanding DoLA provides an important baseline for evaluating the relative performance and innovation of DeCoRe. ", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Shiqi Chen", "paper_title": "In-context sharpness as alerts: An inner representation perspective for hallucination mitigation", "reason": "This work is highly relevant to DeCoRe because it focuses on mitigating hallucinations by focusing on the inner representations of the model. It demonstrates an alternative method for achieving better accuracy that DeCoRe also aims to achieve.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "William Merrill", "paper_title": "The Parallelism Tradeoff: Limitations of Log-Precision Transformers", "reason": "This paper discusses a tradeoff between parallelism and log-precision in transformers that is relevant to understanding the limitations and opportunities for improvement in LLMs, providing a broader context for the current research.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This work is highly relevant to the current study due to its focus on training LLMs to follow instructions accurately, and this work highlights the challenges of generating accurate and reliable outputs from LLMs, which is a problem that DeCoRe attempts to address.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "reason": "This influential paper introduced the concept of LLMs as unsupervised multitask learners, providing a foundational understanding for the current research. It's crucial to understand this foundation to appreciate the advancements achieved by DeCoRe in mitigating the challenges of LLMs.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Vipula Rawte", "paper_title": "A survey of hallucination in large foundation models", "reason": "This survey paper provides a broad overview of the research on hallucinations in LLMs. It serves as a crucial background for understanding the context of DeCoRe and the significance of the problem of hallucinations in the field of large language models.", "section_number": 1}]}