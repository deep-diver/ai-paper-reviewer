{"references": [{" publication_date": "2023", "fullname_first_author": "Muhammad Aurangzeb Ahmad", "paper_title": "Creating trustworthy llms: Dealing with hallucinations in healthcare ai", "reason": "This paper directly addresses the core problem of the current research, focusing on hallucinations in LLMs, specifically within the healthcare domain. Its relevance to the current work is undeniable, as it highlights the critical need for reliable and trustworthy LLMs, particularly in high-stakes applications.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "reason": "This foundational paper introduces the concept of few-shot learning in large language models.  Understanding the capabilities and limitations of few-shot learning is crucial for developing effective hallucination mitigation strategies, as DeCoRe is a training-free method that leverages the inherent capabilities of LLMs.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yung-Sung Chuang", "paper_title": "Decoding by contrasting layers improves factuality in large language models", "reason": "This work is highly relevant as it introduces a novel method, DoLa, that also addresses the issue of hallucination using a contrastive decoding strategy. This direct comparison allows for a detailed analysis of the advantages and disadvantages of each approach.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yung-Sung Chuang", "paper_title": "Lookback lens: Detecting and mitigating contextual hallucinations in large language models using only attention maps", "reason": "This study provides further insights into the mechanisms behind contextual hallucinations, specifically focusing on attention maps. This information complements the current research by offering an alternative approach to identifying and mitigating hallucinations, ultimately expanding the scope of the problem and the range of solutions.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Matthew Dahl", "paper_title": "Large legal fictions: Profiling legal hallucinations in large language models", "reason": "This paper focuses on the impact of hallucinations within the legal domain, which is a high-stakes area similar to those highlighted in the introduction. The paper's focus on a specific application of LLMs makes it particularly relevant to the discussion of the implications of hallucination.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Damai Dai", "paper_title": "Knowledge neurons in pretrained transformers", "reason": "This study directly relates to the internal mechanisms of LLMs and the role of neurons in processing information. The analysis of neural networks' internal structure is crucial for understanding the underlying mechanisms that contribute to hallucinations.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces the Llama 3 models, which are used as the foundation for the experiments in this research. Understanding the characteristics and limitations of these models is critical for evaluating the effectiveness of DeCoRe and interpreting the results accurately.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Maha Elbayad", "paper_title": "Depth-adaptive transformer", "reason": "This paper is related to the architecture of transformers, which forms the basis of the LLMs being studied. Understanding architectural aspects such as depth helps in designing and evaluating decoding strategies, making this work relevant to the design and implementation of DeCoRe.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Sebastian Farquhar", "paper_title": "Detecting hallucinations in large language models using semantic entropy", "reason": "This paper proposes using semantic entropy as a method for detecting hallucinations. This complements the current work by offering an alternative uncertainty quantification approach, which can be compared and contrasted with the conditional entropy method used in DeCoRe.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Shangbin Feng", "paper_title": "FactKB: Generalizale factuality evaluation using language models enhanced with factual knowledge", "reason": "This paper introduces a new metric (factKB) for evaluating factuality in generated text. This is directly relevant to the assessment of DeCoRe's performance on factuality-related tasks, providing a benchmark for comparing the effectiveness of DeCoRe in improving factual accuracy.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Giwon Hong", "paper_title": "The hallucinations leaderboard-an open effort to measure hallucinations in large language models", "reason": "This paper provides a comprehensive overview of hallucination in LLMs, highlighting the importance of developing robust evaluation metrics and methods to address this pervasive issue.  Its focus on various benchmark datasets helps contextualize DeCoRe's evaluation.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Ziwei Ji", "paper_title": "Survey of hallucination in natural language generation", "reason": "This survey paper provides a broad overview of the current state of research on hallucinations in natural language generation.  Its comprehensive coverage of existing methods, datasets, and evaluation metrics helps provide context for the current research and highlights the significance of the problem.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "This paper introduces a new family of large language models, Mistral, which was used for testing the proposed method.  Understanding the characteristics of these models helps in evaluating the performance and generalizability of DeCoRe across different LLMs.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "Mandar Joshi", "paper_title": "Triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "reason": "TriviaQA is one of the benchmark datasets used to evaluate the factuality of DeCoRe. Understanding its design and characteristics is necessary for a full interpretation and comparison of the results.  This dataset's importance in evaluating the factual accuracy of LLMs makes it particularly relevant to the current research.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Saurav Kadavath", "paper_title": "Language models (mostly) know what they know", "reason": "This paper focuses on the knowledge capabilities of large language models, which is directly relevant to the problem of hallucination.  The insights into what LLMs know and how they access that knowledge help to inform the design of methods that can effectively reduce hallucinations.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Tom Kwiatkowski", "paper_title": "Natural questions: a benchmark for question answering research", "reason": "Natural Questions is a benchmark dataset used to evaluate the performance of DeCoRe. Understanding its design, evaluation metrics, and the type of questions asked helps in comparing DeCoRe's performance to existing state-of-the-art models and providing a comprehensive assessment of its capabilities.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Junyi Li", "paper_title": "The dawn after the dark: An empirical study on factuality hallucination in large language models", "reason": "This empirical study provides valuable insights into the nature and prevalence of factuality hallucinations in LLMs. Understanding the causes and consequences of these hallucinations is essential for developing effective mitigation strategies, which is a core focus of this research. This makes it highly relevant to the problem's scope and the proposed solution.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Xiang Lisa Li", "paper_title": "Contrastive decoding: Open-ended text generation as optimization", "reason": "This paper introduces contrastive decoding, a technique that is closely related to the core method used in DeCoRe.  Understanding this technique and its performance characteristics is crucial for evaluating the novelty and potential improvements of DeCoRe.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Stephanie Lin", "paper_title": "Truthfulqa: Measuring how models mimic human falsehoods", "reason": "TruthfulQA is a benchmark dataset used to evaluate the factuality of DeCoRe. Understanding its design, evaluation metrics, and the type of questions asked is important for comparing DeCoRe's performance to existing state-of-the-art models and providing a comprehensive assessment of its capabilities in improving factuality.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Nelson F. Liu", "paper_title": "Lost in the middle: How language models use long contexts", "reason": "This paper explores how LLMs use long contexts, providing valuable insights into the mechanisms of information retrieval and the potential impact on hallucination.  Understanding these mechanisms is important for designing effective mitigation strategies, making this paper highly relevant to the current work.", "section_number": 2}]}