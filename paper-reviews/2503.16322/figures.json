[{"figure_path": "https://arxiv.org/html/2503.16322/x2.png", "caption": "Figure 1: High-resolution results by our method.", "description": "This figure showcases the high-resolution image generation capabilities of the proposed method, \"Ultra-Resolution Adaptation with Ease (URAE)\".  It presents several examples of images generated by the model, demonstrating its ability to produce detailed and visually appealing results at higher resolutions than typically seen in text-to-image models. The diverse range of image styles and subject matter highlights the model's versatility and potential across various applications.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.16322/x3.png", "caption": "Figure 2: A toy linear regression case. There are real data with noisy labels and synthetic data generated by a reference model Wr\u2062e\u2062fsubscript\ud835\udc4a\ud835\udc5f\ud835\udc52\ud835\udc53W_{ref}italic_W start_POSTSUBSCRIPT italic_r italic_e italic_f end_POSTSUBSCRIPT. The proportion of synthetic data is p\ud835\udc5dpitalic_p.", "description": "Figure 2 illustrates a simplified linear regression model to demonstrate the impact of using synthetic data during model training.  The model is trained with a mix of real data (containing noise in the labels) and synthetic data generated by a pre-trained reference model (Wref). The graph displays training error as a function of the number of training iterations, for varying proportions (p) of synthetic data in the training dataset.  This shows how the inclusion of synthetic data can impact the convergence speed and accuracy of the model, revealing the trade-off between error due to noise in real labels and error due to the difference between the reference model's parameters and the optimal parameters.", "section": "2. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.16322/x4.png", "caption": "Figure 3: For CFG-distilled models, classifier-free guidance should be disabled in the training time. ztsubscript\ud835\udc67\ud835\udc61z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and t\ud835\udc61titalic_t are omitted from the inputs of \u03f5\u03b8subscriptitalic-\u03f5\ud835\udf03\\epsilon_{\\theta}italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT and \u03f5\u03b8\u2032subscriptitalic-\u03f5superscript\ud835\udf03\u2032\\epsilon_{\\theta^{\\prime}}italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT here for simplicity.", "description": "This figure illustrates the impact of classifier-free guidance (CFG) during the training phase of CFG-distilled diffusion models. CFG introduces an additional 'null-condition' branch during inference, improving the quality of generated samples. However, using CFG during training introduces a mismatch between the training and inference stages. The figure shows that disabling CFG (setting guidance scale to 1) during training yields a consistent target across both stages. Using CFG during training is shown to result in 'improper matching' while disabling CFG results in 'proper matching'.  The variables z<sub>t</sub> (noisy latent representation) and t (timestep) are omitted from the figure for simplicity.", "section": "2. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.16322/x5.png", "caption": "Figure 4: GPT-4o preferred evaluation against current SOTA T2I models. We request GPT-4o to select a better image regarding overall quality, prompt alignment, and visual aesthetics. Our proposed method are preferred against others.", "description": "This figure presents a comparison of different text-to-image (T2I) models, evaluated by GPT-4.  The evaluation focuses on three key aspects: overall image quality, how well the generated image aligns with the given text prompt (prompt alignment), and the visual aesthetics of the image.  For each prompt, two images generated by different methods were compared and GPT-4 selected the preferred image. The results show that the proposed method consistently outperforms existing state-of-the-art T2I models across all three evaluation criteria.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.16322/x6.png", "caption": "Figure 5: Visualizations of our proposed method apply to training-free high-resolution generation pipelines. The prompt is A giraffe stands beneath a tree beside a marina.", "description": "Figure 5 demonstrates the effectiveness of the proposed Ultra-Resolution Adaptation with Ease (URAE) method when integrated with training-free high-resolution generation pipelines.  The input prompt is a simple sentence: \"A giraffe stands beneath a tree beside a marina.\" The figure showcases the results of several different high-resolution generation pipelines, both with and without the URAE method applied, highlighting the improved visual quality and detail achieved by URAE.  This visual comparison shows that URAE significantly enhances the detail and overall quality of the generated images, particularly at higher resolutions.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2503.16322/x7.png", "caption": "Figure 6: Qualitative comparisons with baseline methods. All the images are of 2048 \u00d7\\times\u00d7 2048 size.", "description": "Figure 6 presents a qualitative comparison of image generation results from various methods, including the proposed URAE approach and several baseline models.  Each method is evaluated using the same prompt, and the resulting images (all 2048x2048 pixels) illustrate the differences in visual quality, detail, and adherence to the prompt's description. This comparison helps showcase the advantages of URAE in terms of image generation quality and fidelity.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2503.16322/x8.png", "caption": "Figure 7: Visualization results of ablation studies. The prompt is Imogen Poots portrayed as a D&D Paladin in a fantasy concept art by Tomer Hanuka.", "description": "This figure visualizes the results of ablation studies performed to evaluate the effectiveness of different components within the proposed ultra-resolution adaptation method (URAE).  The image generation was conducted using the prompt: \"Imogen Poots portrayed as a D&D Paladin in a fantasy concept art by Tomer Hanuka.\"", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.16322/x9.png", "caption": "Figure 8: Visualization results for ultra-resolution image generation task. All the images are of 4096 \u00d7\\times\u00d7 4096 size.", "description": "Figure 8 presents a comparison of ultra-high-resolution (4096x4096 pixels) images generated by different methods.  It showcases the visual quality achieved by various approaches for generating 4K images, highlighting the differences in detail, clarity, and overall aesthetic appeal. The methods compared include: Sana-1.6B, FLUX.1-dev*, PixArt-Sigma-XL, and two versions of the proposed method, URAE (Major-4K) and URAE (Minor-4K).  This allows a visual assessment of the effectiveness of the proposed URAE framework in ultra-resolution image generation.", "section": "3. Experiments"}]