[{"figure_path": "2410.11817/tables/table_7_0.html", "caption": "Table 1: R@1 results for 5k text-to-image retrieval using different CLIP-based models.", "description": "Table 1 shows the R@1 retrieval accuracy for 5k text-to-image retrieval using different CLIP-based models, comparing the performance of using the full text embedding, text-relevant embedding, and the average of segment-level embeddings.", "section": "5.2 SEGMENT PREFERENCE MODEL"}, {"figure_path": "2410.11817/tables/table_8_0.html", "caption": "Table 2: FID and Denscore results for 512x512 image generation using different foundation models. PlayG-2 is from Li et al. (2024a), and KanD-2.2 is from Razzhigaev et al. (2023).", "description": "The table presents FID and Denscore results for 512x512 image generation using different foundation models, including the proposed model, demonstrating its improved performance.", "section": "5.5 GENERATION RESULT"}, {"figure_path": "2410.11817/tables/table_10_0.html", "caption": "Table 3: Evaluation for comparison in the P2I diffusion framework.", "description": "Table 3 presents the quantitative comparison of the original P2I diffusion model and its fine-tuned version using the proposed method across multiple evaluation metrics, including FID, Denscore-O, Denscore and GPT-4o.", "section": "5.1 EXPERIMENTAL SETUP"}, {"figure_path": "2410.11817/tables/table_18_0.html", "caption": "Table 4: R@1 results for 5k text-to-image retrieval using different CLIP-based models.", "description": "Table 4 presents the R@1 scores for 5,000 text-to-image retrieval tasks using four different CLIP-based models (CLIP, HPSv2, Pickscore, and Denscore) with varying numbers of sentences in the input text.", "section": "5.2 SEGMENT PREFERENCE MODEL"}]