{"references": [{" publication_date": "2023", "fullname_first_author": "Zhangir Azerbayev", "paper_title": "Llemma: An open language model for mathematics", "reason": "This paper is highly relevant because it introduces Llemma, an open-source language model specifically designed for mathematics.  The availability of open-source models for mathematical reasoning is crucial to the paper's core argument, emphasizing the need for accessible resources and open-source alternatives to expensive proprietary models.  Its focus on mathematical reasoning directly aligns with the ScaleQuest dataset's domain and the overall goal of enhancing LLMs' capabilities in this specific area. The performance comparison with Llemma further supports the paper's claims.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zheng Cai", "paper_title": "Internlm2 technical report", "reason": "This technical report details InternLM2, a powerful language model.  The mention of InternLM2 is relevant because it is used as a reward model in the response generation process of ScaleQuest, which aims at increasing the overall quality and effectiveness of the generated responses.  The strength of the reward model directly impacts the quality of the final dataset, and hence this citation is directly relevant to the method's implementation and evaluation.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Jiaao Chen", "paper_title": "Skills-in-context prompting: Unlocking compositionality in large language models", "reason": "This paper is relevant as it explores prompting techniques to unlock compositionality in LLMs.  The ScaleQuest approach also involves a specific prompt engineering technique for fine-tuning, and the techniques described here offer valuable insights into prompt design and optimization, particularly in the context of mathematical reasoning.  Understanding various prompting strategies is crucial to understanding ScaleQuest's question-tuning process and its potential impact on LLM performance.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Wenhu Chen", "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks", "reason": "This paper is relevant because it introduces the \"Program of Thoughts\" prompting technique, which focuses on separating computation from reasoning in numerical tasks. This is related to ScaleQuest's emphasis on effective prompting methods for mathematical problem-solving, improving the quality and diversity of the generated questions. The techniques presented here offer a useful benchmark and inform ScaleQuest's approaches to eliciting higher-level reasoning from LLMs. ", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yew Ken Chia", "paper_title": "Contrastive chain-of-thought prompting", "reason": "This paper explores contrastive chain-of-thought prompting, a technique that improves the reasoning ability of LLMs.  ScaleQuest utilizes a chain-of-thought prompting strategy for response generation, making this paper directly relevant to the quality and effectiveness of the response generation part of ScaleQuest. The insights from contrastive chain-of-thought prompting can inform the optimization of the response generation process and help achieve better overall results.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "reason": "This paper is essential because it introduces the GSM8K dataset, which is one of the datasets used in the training phase of ScaleQuest's question fine-tuning (QFT) process.  The GSM8K dataset helps activate the question generation capabilities of the open-source models by providing a well-structured and diverse set of mathematical problems. This direct involvement makes it highly relevant to the experimental setup and the subsequent success of ScaleQuest.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Aniket Didolkar", "paper_title": "Metacognitive capabilities of llms: An exploration in mathematical problem solving", "reason": "This paper explores the metacognitive capabilities of LLMs in mathematical problem-solving, which directly relates to ScaleQuest's focus on enhancing LLMs' reasoning abilities. The paper delves into the complexities of mathematical reasoning and provides insights into the challenges involved in designing effective datasets for improving LLM performance. This provides context and supports the claims made about the improvement gained by the ScaleQuest dataset.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces Llama 3, one of the open-source LLMs used in the evaluation of ScaleQuest. The comparative results on the Llama 3 model are crucial in demonstrating the effectiveness and scalability of the ScaleQuest dataset. The inclusion of this model underscores the generality and applicability of ScaleQuest across different language model architectures.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Run-Ze Fan", "paper_title": "Reformatted alignment", "reason": "This paper proposes reformatted alignment, which is a technique to improve the quality of LLM responses.   As ScaleQuest employs a response generation and filtering process using InternLM2-7B-Reward, this paper offers valuable insight into the intricacies of refining and improving LLM outputs. Its direct relevance lies in contextualizing and understanding the response generation and refinement steps within ScaleQuest's data creation process.  Improvements in response quality directly contribute to the overall quality of the final dataset.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Luyu Gao", "paper_title": "Pal: Program-aided language models", "reason": "This paper introduces PAL, a program-aided language model, relevant to ScaleQuest because ScaleQuest aims to enhance the reasoning abilities of LLMs. While PAL focuses on program understanding, its underlying approach of combining language models with symbolic reasoning techniques is relevant to improving mathematical reasoning, the core focus of ScaleQuest's dataset. Understanding program-aided models provides broader context to the research in improving LLM reasoning abilities.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Zhibin Gou", "paper_title": "Tora: A tool-integrated reasoning agent for mathematical problem solving", "reason": "This work is highly relevant because Tora, a tool-integrated reasoning agent, focuses on mathematical problem-solving.   ScaleQuest similarly addresses improving mathematical reasoning capabilities. This direct alignment makes the understanding of Tora's approach beneficial in understanding the context and progress in the field of improving LLMs' mathematical reasoning abilities, thus informing the design and evaluation of ScaleQuest.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Chaoqun He", "paper_title": "Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems", "reason": "This work is relevant because it introduces OlympiadBench, a challenging benchmark dataset used in evaluating ScaleQuest's performance.  The inclusion of OlympiadBench in the evaluation demonstrates the robustness of the ScaleQuest dataset's improvement in a challenging high-level mathematical reasoning domain. Its challenging nature strengthens the argument about the effectiveness of ScaleQuest's synthesized data.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the math dataset", "reason": "This paper is highly significant because it introduces the MATH dataset, which is a key benchmark dataset used to evaluate the performance of ScaleQuest.  The MATH dataset is a widely recognized and challenging benchmark for mathematical reasoning capabilities in LLMs, making the results on this benchmark crucial in demonstrating ScaleQuest's effectiveness. The direct use of the MATH dataset makes this paper highly relevant to the experimental design and evaluation of ScaleQuest.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Hakan Inan", "paper_title": "Llama guard: Llm-based input-output safeguard for human-ai conversations", "reason": "This work introduces Llama Guard, a safety model employed to detect unsafe elements in the generated dataset, contributing to the robustness and safety of ScaleQuest.   The application of Llama Guard is crucial for ensuring that the generated dataset is not only high-quality but also safe and reliable for use in training or other applications. The focus on safety and reliability of the data is an important aspect of the overall methodology.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "This paper introduces Mistral 7B, a prominent open-source language model used for evaluating the ScaleQuest dataset.  The use of Mistral 7B in the experimental section is vital in demonstrating the generality and effectiveness of ScaleQuest across different language model architectures, thereby increasing the significance and reliability of the obtained results.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Mario Michael Krell", "paper_title": "Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance", "reason": "This paper is relevant as it explores efficient sequence packing techniques for accelerating large language models.  ScaleQuest utilizes sequence packing to speed up the training process, making the techniques discussed here directly relevant to the efficiency of ScaleQuest's experimental setup.   Increasing training efficiency is a critical factor in making large-scale data synthesis more feasible and cost-effective.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Woosuk Kwon", "paper_title": "Efficient memory management for large language model serving with pagedattention", "reason": "This paper presents efficient memory management techniques for large language models, relevant to ScaleQuest given that the ScaleQuest approach involves the use of multiple LLMs. The focus on efficient memory management is directly relevant to the cost and scalability aspects of ScaleQuest, as efficient memory usage can significantly reduce the computational cost of the process. This enhances the overall efficiency and cost-effectiveness of ScaleQuest's data synthesis pipeline.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Xin Lai", "paper_title": "Step-dpo: Stepwise preference optimization for long-chain reasoning of llms", "reason": "This paper introduces Step-DPO, a stepwise preference optimization method, aligning with ScaleQuest's use of Question Preference Optimization (QPO).  Both approaches aim at optimizing question generation, and this work provides valuable insight into the optimization techniques, specifically in the context of long-chain reasoning.  Understanding Step-DPO contributes to a more comprehensive understanding of the QPO aspect of ScaleQuest.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Aitor Lewkowycz", "paper_title": "Solving quantitative reasoning problems with language models", "reason": "This paper is relevant to ScaleQuest as it addresses the challenge of solving quantitative reasoning problems with language models, a central theme of ScaleQuest. The paper explores various aspects of quantitative reasoning and LLMs, providing valuable context and insights into the challenges involved in improving LLM's capabilities in this area.  This helps contextualize and support ScaleQuest's efforts in creating a high-quality dataset for improving mathematical reasoning in LLMs.", "section_number": 4}]}