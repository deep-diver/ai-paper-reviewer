{"references": [{" publication_date": "2023", "fullname_first_author": "Zhangir Azerbayev", "paper_title": "Llemma: An open language model for mathematics", "reason": "This paper is highly relevant due to its introduction of Llemma, an open-source language model specifically designed for mathematical reasoning.  Its focus on mathematical capabilities directly addresses the core problem that ScaleQuest aims to solve, making it a crucial benchmark for evaluating the proposed method's performance and establishing its position within the existing mathematical reasoning literature.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zheng Cai", "paper_title": "Internlm2 technical report", "reason": "The InternLM2 model serves as a strong baseline for comparison in the experiments, especially because it's used as the reward model in the response generation phase of ScaleQuest. Understanding its characteristics and performance is essential for interpreting the results of the proposed method, demonstrating its effectiveness relative to a contemporary, high-performing model.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Jiaao Chen", "paper_title": "Skills-in-context prompting: Unlocking compositionality in large language models", "reason": "This paper explores techniques for improving the compositional abilities of language models, which is relevant to the question generation and response evaluation stages of ScaleQuest.  Improving compositionality can lead to more coherent and contextually appropriate questions and answers, enhancing the overall quality of the synthetic dataset produced by ScaleQuest.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Wenhu Chen", "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks", "reason": "The \"Program of Thoughts\" prompting technique investigated in this paper is directly related to the chain-of-thought prompting used in response generation within ScaleQuest.  Understanding the effectiveness of this technique for improving reasoning capabilities in LLMs is crucial to evaluating the overall quality and effectiveness of ScaleQuest's response generation.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yew Ken Chia", "paper_title": "Contrastive chain-of-thought prompting", "reason": "This paper's focus on contrastive chain-of-thought prompting is highly relevant to the response generation aspect of ScaleQuest, especially in light of using a reward model. Contrasting different reasoning paths or solutions would be directly beneficial for selecting the most appropriate response, leading to higher quality data.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "reason": "The GSM8K dataset, introduced in this paper, is one of the key datasets used for training in the ScaleQuest methodology.  Understanding the characteristics and limitations of GSM8K is essential for interpreting the results and assessing the impact of the proposed QFT and QPO stages on dataset diversity and quality.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Aniket Didolkar", "paper_title": "Metacognitive capabilities of LLMs: An exploration in mathematical problem solving", "reason": "This paper explores metacognitive capabilities in LLMs, which are crucial for the higher-level reasoning aspects of ScaleQuest's question-generation and optimization.  Understanding the potential and limitations of metacognition in LLMs is relevant to the overall design and evaluation of the ScaleQuest method.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "The Llama 3 models are used as a comparison benchmark for evaluating the performance improvements resulting from ScaleQuest.  Understanding the capabilities and limitations of Llama 3 allows for a better assessment of ScaleQuest's success in enhancing mathematical reasoning capabilities in open-source models.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Run-Ze Fan", "paper_title": "Reformatted alignment", "reason": "This paper introduces a novel approach to data reformatting, which is highly relevant to the filtering and optimization stages within ScaleQuest.  The method of reformatting may positively influence the clarity and coherence of questions and responses, contributing to a higher quality dataset.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Luyu Gao", "paper_title": "Pal: Program-aided language models", "reason": "This paper discusses program-aided language models, a concept potentially relevant to ScaleQuest's use of smaller models for question generation.  ScaleQuest uses smaller models to generate questions; understanding the impact of program assistance on model performance could offer potential avenues for improvement in ScaleQuest's approach.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Zhibin Gou", "paper_title": "Tora: A tool-integrated reasoning agent for mathematical problem solving", "reason": "This paper directly addresses the issue of tool integration in mathematical reasoning, which is relevant to future extensions of ScaleQuest.  Though ScaleQuest does not focus on tool integration, this paper may offer potential avenues for enhancing the ScaleQuest method by incorporating additional tools and techniques for problem-solving.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Chaoqun He", "paper_title": "Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems", "reason": "The Olympiad Bench dataset is used for evaluating the generalization capabilities of the models fine-tuned on ScaleQuest's synthetic dataset.  The paper's introduction of this challenging benchmark provides valuable context for interpreting ScaleQuest's performance beyond standard mathematical reasoning tasks.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the math dataset", "reason": "The MATH dataset, introduced in this paper, is a central benchmark for evaluating the effectiveness of ScaleQuest.  Understanding the characteristics and difficulty of the MATH dataset is essential for correctly interpreting and contextualizing the performance gains reported using ScaleQuest's dataset.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Hakan Inan", "paper_title": "Llama guard: Llm-based input-output safeguard for human-ai conversations", "reason": "The Llama Guard model is used in ScaleQuest's safety analysis, providing a critical assessment of the generated dataset's safety.  The paper is relevant because it shows how unsafe elements are detected, validating the robustness and safety of ScaleQuest's dataset.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "The Mistral 7B model is one of the models evaluated using ScaleQuest's synthetic dataset. This paper provides valuable information about the model's architectural characteristics and performance capabilities which allows for accurate contextualization and interpretation of the performance improvements achieved with the ScaleQuest dataset.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Mario Michael Krell", "paper_title": "Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance", "reason": "This paper introduces an efficient sequence packing technique relevant to the training phase of ScaleQuest.  Efficient packing methods enhance the training process, reducing training time and computational costs which would be crucial for generating large datasets efficiently.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Woosuk Kwon", "paper_title": "Efficient memory management for large language model serving with pagedattention", "reason": "This paper's focus on efficient memory management is highly relevant to the practical aspects of large-scale data synthesis. ScaleQuest involves generating and processing large amounts of data; efficient memory management is critical for cost-effectiveness and scalability.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Aitor Lewkowycz", "paper_title": "Solving quantitative reasoning problems with language models", "reason": "This paper delves into quantitative reasoning problems, directly addressing the core task that ScaleQuest focuses on.  Understanding different techniques for tackling these problems would directly benefit ScaleQuest by potentially informing more efficient question generation and solution methods.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Haoran Li", "paper_title": "(almost) from scratch: Generalized instruction tuning for language models", "reason": "This paper is highly relevant due to its focus on generalized instruction tuning, a technique directly related to ScaleQuest\u2019s approach to question generation.  ScaleQuest leverages a similar idea to activate the question-generation capabilities of smaller models, making this paper a strong contextual reference for understanding the underlying principles of ScaleQuest's approach.", "section_number": 4}]}