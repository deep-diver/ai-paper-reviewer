[{"figure_path": "2410.18693/figures/figures_3_0.png", "caption": "Figure 2: Overview of our ScaleQuest method.", "description": "This figure presents a flowchart illustrating the ScaleQuest method for scalable question synthesis. It consists of three main stages: training question generators, question generation, and final data construction.  The training stage involves two steps: Query Fine-Tuning (QFT) using a base problem writer and Query Preference Optimization (QPO) using an expert and advanced problem designer.  The question generation stage uses prompts to generate synthetic questions that are then filtered for language, solvability, and difficulty, and finally sampled based on difficulty. The final data construction stage involves answer generation and reward filtering to produce a high-quality dataset of 1M question-answer pairs.", "section": "2 SCALEQUEST: SCALING QUESTION SYNTHESIS FROM SCRATCH"}, {"figure_path": "2410.18693/figures/figures_15_0.png", "caption": "Figure 2: Overview of our ScaleQuest method.", "description": "This figure presents a flowchart illustrating the ScaleQuest method for scalable question synthesis.  It begins with training question generators using a professional problem solver and a base problem writer, progressing to question generation which incorporates diversity, solvability, and difficulty considerations. The generated questions are then processed through language, solvability, and difficulty filtering, followed by answer generation and reward filtering, culminating in a dataset of 1 million high-quality question-answer pairs.  The figure depicts each stage of the process, including the percentage of questions filtered out at each stage.", "section": "2 SCALEQUEST: SCALING QUESTION SYNTHESIS FROM SCRATCH"}, {"figure_path": "2410.18693/figures/figures_18_0.png", "caption": "Figure 1: Left: Results of different models on MATH, where -ScaleQuest denotes ours. Right: Results of Llama3-8B fine-tuned on publicly available datasets constructed by different methods.", "description": "This figure contains two bar charts, side-by-side, visualizing the performance of various large language models (LLMs) on a mathematical reasoning benchmark (MATH). The left chart compares the performance of several LLMs including both open-source and proprietary models (GPT-4-Turbo, Claude 3.5 Sonnet), highlighting the significant improvement achieved by fine-tuning the Qwen2-Math-7B-Base model with the ScaleQuest dataset.  The right chart shows the performance gains of Llama3-8B when fine-tuned using different publicly available datasets compared to its baseline performance.  The charts illustrate the effectiveness of the ScaleQuest dataset in improving the reasoning capabilities of LLMs relative to other data synthesis methods.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18693/figures/figures_20_0.png", "caption": "Figure 1: Left: Results of different models on MATH, where -ScaleQuest denotes ours. Right: Results of Llama3-8B fine-tuned on publicly available datasets constructed by different methods.", "description": "This figure presents a comparison of different LLMs' performance on the MATH benchmark.  The left panel shows the accuracy of various models, including proprietary models (GPT-4-Turbo and Claude 3.5 Sonnet) and open-source models fine-tuned with the ScaleQuest dataset, highlighting the significant improvement achieved by ScaleQuest. The right panel displays the accuracy gains of Llama3-8B after fine-tuning on several publicly available datasets, illustrating the superior performance of the ScaleQuest dataset.  The figures use bar charts and line graphs to represent the performance of different models and datasets.", "section": "1 INTRODUCTION"}]