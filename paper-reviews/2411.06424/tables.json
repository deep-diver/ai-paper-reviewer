[{"content": "| Model | Intervention | Toxicity | PPL | F1 |\n|---|---|---|---|---|\n| GPT2 | None | 0.453 | 21.70 | 0.193 |\n| GPT2 | Ablate top 100 toxic neurons | 0.403 | 21.99 | 0.192 |\n| GPT2 | Ablate top 200 toxic neurons | 0.405 | 22.41 | 0.192 |\n| GPT2 | Ablate top 1000 toxic neurons | 0.436 | 27.34 | 0.184 |\n| GPT2 | Ablate top 100 positively activated toxic neurons | 0.384 | 21.78 | 0.193 |\n| GPT2 | Ablate top 200 positively activated toxic neurons | 0.366 | 21.83 | 0.193 |\n| GPT2 | Ablate top 1000 positively activated toxic neurons | 0.320 | 30.04 | 0.191 |\n| GPT2 | Ablate top 2000 positively activated toxic neurons | 0.319 | 29.07 | 0.189 |\n| GPT2 | Patch all dampened toxic neurons to post-DPO levels | 0.335 | 21.69 | 0.190 |\n| DPO | None | 0.208 | 23.34 | 0.195 |\n| DPO | Scale the key vectors on top 7 toxic neurons (x2) | 0.487 | 21.72 | 0.192 |\n| DPO | Scale the key vectors on top 7 toxic neurons (x5) | 0.555 | 23.36 | 0.188 |\n| DPO | Scale the key vectors on top 7 toxic neurons (x10) | 0.458 | 37.33 | 0.183 |", "caption": "Table 1: Toxicity, Perplexity (PPL) and F1 scores after ablating and patching most toxic neurons. Ablating the most toxic neurons or patching all dampened toxic neurons to post-DPO levels yields some toxicity reduction, but the effects are limited compared to DPO\u2019s impact.", "description": "This table presents the results of experiments evaluating the impact of ablating (removing) and patching (adjusting to post-DPO levels) the most toxic neurons in a GPT-2 language model on toxicity, perplexity, and F1 scores.  Different numbers of toxic neurons were ablated and patched (including only those with positive activations). The results are compared against a control (no intervention) and a direct preference optimization (DPO) fine-tuned model. The goal was to test the hypothesis that DPO reduces toxicity primarily by dampening the most toxic neurons. The table shows that while ablating or patching toxic neurons reduces toxicity to some extent, the effect is significantly smaller than that achieved by DPO, demonstrating the limitations of this hypothesis.", "section": "4 Tracking toxic feature reduction across neurons"}, {"content": "| Model | Intervention | Toxicity | PPL | F1 |\n|---|---|---|---|---|\n| GPT2 | None | 0.453 | 21.70 | 0.193 |\n| GPT2 | Patch $\\rm TP_{-}$ neurons to post-DPO activations | 0.335 | 21.69 | 0.190 |\n| GPT2 | Patch $\\rm TN_{+}$ neurons to post-DPO activations | 0.413 | 21.71 | 0.190 |\n| GPT2 | Patch $\\rm AN_{-}$ neurons to post-DPO activations | 0.410 | 21.80 | 0.193 |\n| GPT2 | Patch $\\rm AP_{+}$ neurons to post-DPO activations | 0.455 | 21.72 | 0.193 |\n| GPT2 | Patch $\\rm TP_{-}$ and $\\rm AN_{-}$ to post-DPO activations | 0.239 | 21.78 | 0.189 |\n| GPT2 | Patch $\\rm TP_{-}$, $\\rm AN_{-}$, $\\rm TN_{+}$ to post-DPO activations | 0.193 | 21.76 | 0.174 |\n| GPT2 | Patch all four groups to post-DPO activations | 0.114 | 21.76 | 0.171 |\n| DPO | None | 0.208 | 23.34 | 0.195 |", "caption": "Table 2: Toxicity, Perplexity (PPL) and F1 scores after patching on each neuron group.\nPatching each top three contributing neuron groups individually reduces toxicity, while patching the top three or all four groups together achieves a toxicity reduction surpassing DPO.", "description": "This table presents the results of an experiment where different groups of neurons in a language model were modified using activation patching. Activation patching adjusts the activation values of neurons to match those observed after applying Direct Preference Optimization (DPO), a safety fine-tuning algorithm.  The table shows the toxicity, perplexity (PPL), and F1 scores of the model after patching each of the top three contributing neuron groups individually (TP, AN, TN+), and also after patching combinations of groups. The results indicate that patching the top two groups, TP and AN, comes close to the toxicity reduction achieved by DPO, and patching more groups leads to even greater toxicity reduction.  The findings suggest that DPO's effect on toxicity reduction is not solely dependent on individual neurons, but rather a complex interplay of multiple neuron groups working together.", "section": "5 Activation patching"}]