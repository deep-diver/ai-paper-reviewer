{"references": [{"fullname_first_author": "Trenton Bricken", "paper_title": "Towards monosemanticity: Decomposing language models with dictionary learning", "publication_date": "2023-00-00", "reason": "This paper provides a method for decomposing language models, which is relevant to understanding the internal mechanisms of how language models work and how they can be improved."}, {"fullname_first_author": "Samyak Jain", "paper_title": "Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks", "publication_date": "2023-11-11", "reason": "This paper provides mechanistic insights into how fine-tuning algorithms alter the capabilities of pre-trained models, relevant to understanding how safety fine-tuning methods work."}, {"fullname_first_author": "Andrew Lee", "paper_title": "A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity", "publication_date": "2024-01-01", "reason": "This paper is the most directly related to the current paper, as it also studies direct preference optimization (DPO) for toxicity reduction."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-05-18", "reason": "This paper introduces the concept of direct preference optimization (DPO), which is the focus of the current paper."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-06", "reason": "This paper introduces the proximal policy optimization (PPO) algorithm, which is a related safety fine-tuning algorithm that is mentioned in the current paper."}]}