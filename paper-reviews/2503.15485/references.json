{"references": [{"fullname_first_author": "Mahmoud Assran", "paper_title": "Self-supervised learning from images with a joint-embedding predictive architecture", "publication_date": "2023-01-01", "reason": "This paper is significant because it introduces a self-supervised learning approach using a joint-embedding predictive architecture, which is relevant to TULIP's method of learning visual representations."}, {"fullname_first_author": "Max Bain", "paper_title": "Frozen in time: A joint video and image encoder for end-to-end retrieval", "publication_date": "2021-01-01", "reason": "Bain et al. developed a joint video and image encoder which is relevant because TULIP incorporates video data as an image augmentation method to improve its generative contrastive learning."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper is important because it introduces CLIP, a foundational image-text contrastive model that TULIP aims to improve upon and serves as a baseline for comparison."}, {"fullname_first_author": "Kaiming He", "paper_title": "Momentum contrast for unsupervised visual representation learning", "publication_date": "2020-01-01", "reason": "This paper is significant because it introduces MoCo, an important contrastive learning approach that may be used as a comparison when studying contrastive image-text learning."}, {"fullname_first_author": "Jia Deng", "paper_title": "ImageNet: A large-scale hierarchical image database", "publication_date": "2009-01-01", "reason": "This paper describes ImageNet, which is an important dataset used for zero-shot classification in TULIP, and a very common benchmark dataset in computer vision."}]}