[{"figure_path": "2410.18538/figures/figures_1_0.png", "caption": "Figure 1: SMITE. Using only one or few segmentation references with fine granularity (left), our method learns to segment different unseen videos respecting the segmentation references.", "description": "The figure illustrates the SMITE method, showing how a few annotated images are used to train a model that can then segment different unseen videos while maintaining consistency with the original annotations.", "section": "INTRODUCTION"}, {"figure_path": "2410.18538/figures/figures_4_0.png", "caption": "Figure 2: SMITE pipeline. During inference (a), we invert a given video into a noisy latent by iteratively adding noise. We then use an inflated U-Net denoiser (b) along with the trained text embedding as input to denoise the segments. A tracking module ensures that the generated segments are spatially and temporally consistent via spatio-temporal guidance. The video latent zt is updated by a tracking energy Etrack (c) that makes the segments temporally consistent and also a low-frequency regularizer (d) Ereg which guides the model towards better spatial consistency.", "description": "The figure illustrates the SMITE pipeline, detailing the process of video segmentation using an inflated U-Net, tracking modules, and a low-frequency regularizer to ensure temporal and spatial consistency.", "section": "4 METHOD"}, {"figure_path": "2410.18538/figures/figures_6_0.png", "caption": "Figure 4: Segment tracking module ensures that segments are consistent across time. It uses co-tracker to track each point of the object's segment (here it is nose) and then finds point correspondence of this segment (denoted by blue dots) across timesteps. When the tracked point is of a different class (e.g,. face) then it is recovered by using temporal voting. The misclassified pixel is then replaced by the average of the neighbouring pixels of adjacent frames. This results are temporally consistent segments without visible flickers.", "description": "The figure illustrates how the segment tracking module maintains temporal consistency by tracking segments across frames and using temporal voting to correct misclassified pixels.", "section": "4.3 TEMPORAL CONSISTENCY"}, {"figure_path": "2410.18538/figures/figures_6_1.png", "caption": "Figure 3: Best viewed in Adobe Acrobat.", "description": "The figure compares the video segmentation results using frame-by-frame processing, without tracking and low-pass regularization, and with SMITE's proposed approach.", "section": "4 METHOD"}, {"figure_path": "2410.18538/figures/figures_7_0.png", "caption": "Figure 5: SMITE-50 Dataset sample.", "description": "The figure shows sample images from the SMITE-50 dataset, showcasing different object categories (horses, faces, cars, and non-text) with varying levels of segmentation granularity.", "section": "5 RESULTS AND EXPERIMENTS"}, {"figure_path": "2410.18538/figures/figures_8_0.png", "caption": "Figure 6: Visual comparisons with other methods demonstrate that SMITE maintains better motion consistency of segments and delivers cleaner, more accurate segmentations. Both GSAM2 and Baseline-I struggle to accurately capture the horse\u2019s mane, and GSAM2 misses one leg (Left), whereas our method yields more precise results. Additionally, both alternative techniques create artifacts around the chin (Right), while SMITE produces a cleaner segmentation.", "description": "Figure 6 presents a visual comparison of video segmentation results between SMITE and other methods, highlighting SMITE\u2019s superior performance in maintaining motion consistency and producing cleaner segmentations.", "section": "5 RESULTS AND EXPERIMENTS"}, {"figure_path": "2410.18538/figures/figures_10_0.png", "caption": "Figure 7: Additional results. We visualize the generalization capability of SMITE model (trained on the reference images) in various challenging poses, shape, and even in cut-shapes.", "description": "Figure 7 shows additional results demonstrating SMITE\u2019s ability to generalize segmentation to unseen videos with objects in various poses and shapes, even when cut.", "section": "5 RESULTS AND EXPERIMENTS"}, {"figure_path": "2410.18538/figures/figures_10_1.png", "caption": "Figure 8: Segmentation results in challenging scenarios . SMITE accurately segments out the objects under occlusion (\"ice-cream\") or camouflage (\"turtle\") highlighting the robustness of our segmentation technique.", "description": "Figure 8 shows examples of SMITE's accurate segmentation results in challenging scenarios with object occlusion and camouflage.", "section": "5 RESULTS AND EXPERIMENTS"}]