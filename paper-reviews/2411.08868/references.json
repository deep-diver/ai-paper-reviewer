{"references": [{"fullname_first_author": "Julien Abadji", "paper_title": "Towards a cleaner document-oriented multilingual crawled corpus", "publication_date": "2022-00-00", "reason": "This paper introduces CulturaX, a multilingual dataset that is the primary source for the pre-training data of CamemBERTv2 and CamemBERTav2, significantly impacting the models' performance."}, {"fullname_first_author": "Wissam Antoun", "paper_title": "Data-efficient french language modeling with camemberta", "publication_date": "2023-00-00", "reason": "CamemBERTa, the predecessor model, is directly built upon in this work, making it foundational to understanding the improvements made in CamemBERTv2 and CamemBERTav2."}, {"fullname_first_author": "Kevin Clark", "paper_title": "ELECTRA: Pre-training text encoders as discriminators rather than generators", "publication_date": "2020-00-00", "reason": "The Replaced Token Detection (RTD) objective used in CamemBERTav2 is based on this paper, contributing to the enhanced contextual understanding of the model."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-00-00", "reason": "BERT's architecture is the basis of both CamemBERTv2 and CamemBERTav2's underlying design, impacting their functionality and performance."}, {"fullname_first_author": "Yinhan Liu", "paper_title": "Roberta: A robustly optimized bert pretraining approach", "publication_date": "2019-00-00", "reason": "RoBERTa's architecture and Masked Language Modeling (MLM) objective are adopted by CamemBERTv2, and it significantly influences the model's training and performance."}]}