[{"content": "| Model | F1 | EM |\n|---|---|---|\n| CamemBERT | 80.98 \u00b1 0.48 | 62.51 \u00b1 0.54 |\n| CamemBERTa | 81.15 \u00b1 0.38 | 62.01 \u00b1 0.45 |\n| CamemBERTv2 | 80.39 \u00b1 0.36 | 61.35 \u00b1 0.39 |\n| CamemBERTav2 | **83.04 \u00b1 0.19** | **64.29 \u00b1 0.31** |", "caption": "Table 1: POS tagging, dependency parsing and NER results on the test sets of our French datasets. UPOS (Universal Part-of-Speech) refers here to POS tagging accuracy, and LAS measures the overall accuracy of labeled dependencies in a parsed sentence.", "description": "This table presents the results of experiments evaluating Part-of-Speech (POS) tagging, dependency parsing, and Named Entity Recognition (NER) performance on four different French datasets (GSD, RHAPSODIE, SEQUOIA, FSMB, FTB-NER).  For each task and dataset, the table shows the UPOS (Universal Part-of-Speech) tagging accuracy and the Labelled Attachment Score (LAS) for dependency parsing.  For NER, the F1 score is reported.  The table compares the performance of four different models: CamemBERT, CamemBERTa, CamemBERTv2, and CamemBERTav2, highlighting the improvements achieved by the updated versions.", "section": "4 Experiments and Results"}, {"content": "| Model | CLS | PAWS-X | XNLI |\n|---|---|---|---|\n| CamemBERT | 94.62 \u00b1 0.04 | 91.36 \u00b1 0.38 | 81.95 \u00b1 0.51 |\n| CamemBERTa | 94.92 \u00b1 0.13 | 91.67 \u00b1 0.17 | 82.00 \u00b1 0.17 |\n| CamemBERTv2 | 95.07 \u00b1 0.11 | 92.00 \u00b1 0.24 | 81.75 \u00b1 0.62 |\n| CamemBERTav2 | **95.63 \u00b1 0.16** | **93.06 \u00b1 0.45** | **84.82 \u00b1 0.54** |", "caption": "Table 2: Question Answering results on FQuAD 1.0.", "description": "This table presents the results of the Question Answering task, evaluated using the FQuAD 1.0 dataset.  It shows the F1 score (harmonic mean of precision and recall) and the Exact Match (EM) score (the percentage of questions where the model's answer exactly matches the ground truth answer) for each of the four different language models being compared: CamemBERT, CamemBERTa, CamemBERTv2, and CamemBERTav2.", "section": "4.2 General Domain Results"}, {"content": "| Model | Medical-NER | Counter-NER |\n|---|---|---|\n| CamemBERT | 70.96 \u00b1 0.13 | 84.18 \u00b1 1.23 |\n| CamemBERTa | 71.86 \u00b1 0.11 | 87.37 \u00b1 0.73 |\n| CamemBERT-bio | **73.96 \u00b1 0.12** | - |\n| CamemBERTv2 | 72.77 \u00b1 0.11 | 87.46 \u00b1 0.62 |\n| CamemBERTav2 | **73.98 \u00b1 0.11** | **89.53 \u00b1 0.73** |", "caption": "Table 3: Text classification results (Accuracy) on the FLUE benchmark.", "description": "This table presents the accuracy scores achieved by four different French language models (CamemBERT, CamemBERTa, CamemBERTv2, and CamemBERTav2) on three text classification tasks within the FLUE benchmark: CLS (sentence classification), PAWS-X (paraphrase detection), and XNLI (natural language inference).  It allows comparison of model performance across various tasks to highlight the relative strengths and weaknesses of each model.", "section": "4.3 Domain Specific Results"}, {"content": "| Dataset | Model | F1 |\n|---|---|---|\n| CAS1 | CamemBERT | 70.72 \u00b1 1.47 |\n|  | CamemBERTa | 71.96 \u00b1 1.38 |\n|  | Dr-BERT | 62.76 \u00b1 1.55 |\n|  | CamemBERT-Bio | 72.28 \u00b1 1.46 |\n|  | CamemBERTv2 | 71.18 \u00b1 1.62 |\n|  | CamemBERTav2 | **72.87 \u00b1 2.29** |\n| CAS2 | CamemBERT | 78.43 \u00b1 1.78 |\n|  | CamemBERTa | 79.06 \u00b1 0.68 |\n|  | Dr-BERT | 76.43 \u00b1 0.49 |\n|  | CamemBERT-Bio | **82.50 \u00b1 0.56** |\n|  | CamemBERTv2 | 81.87 \u00b1 0.58 |\n|  | CamemBERTav2 | 81.85 \u00b1 0.49 |\n| E3C | CamemBERT | 67.01 \u00b1 2.13 |\n|  | CamemBERTa | 67.01 \u00b1 1.85 |\n|  | Dr-BERT | 56.99 \u00b1 2.40 |\n|  | CamemBERT-Bio | 69.87 \u00b1 1.21 |\n|  | CamemBERTv2 | 69.27 \u00b1 0.90 |\n|  | CamemBERTav2 | **70.12 \u00b1 0.87** |\n| EMEA | CamemBERT | 73.53 \u00b1 2.04 |\n|  | CamemBERTa | 75.99 \u00b1 0.51 |\n|  | Dr-BERT | 71.33 \u00b1 0.84 |\n|  | CamemBERT-Bio | 76.96 \u00b1 2.00 |\n|  | CamemBERTv2 | 76.30 \u00b1 1.00 |\n|  | CamemBERTav2 | **77.28 \u00b1 0.57** |\n| MEDLINE | CamemBERT | 65.11 \u00b1 0.56 |\n|  | CamemBERTa | 65.33 \u00b1 0.30 |\n|  | Dr-BERT | 58.90 \u00b1 0.51 |\n|  | CamemBERT-Bio | **68.21 \u00b1 0.91** |\n|  | CamemBERTv2 | 65.26 \u00b1 0.33 |\n|  | CamemBERTav2 | 67.77 \u00b1 0.44 |\n| Counter-NER | CamemBERT | 84.18 \u00b1 1.23 |\n|  | CamemBERTa | 87.37 \u00b1 0.73 |\n|  | CamemBERTv2 | 87.46 \u00b1 0.62 |\n|  | CamemBERTav2 | **89.53 \u00b1 0.73** |", "caption": "Table 4: Summary of NER F1 scores on the domain-specific downstream tasks. Full scores are available in Table\u00a05.", "description": "This table summarizes the F1 scores achieved by various CamemBERT models on several Named Entity Recognition (NER) tasks within specific domains.  It presents a concise overview of the performance, showing how the updated CamemBERT models (CamemBERTv2 and CamemBERTav2) compare to previous versions and a specialized biomedical NER model (CamemBERT-bio) across different datasets. The full detailed results with individual scores for each task and model are provided in Table 5.", "section": "4.3 Domain Specific Results"}, {"content": "| Hyper-parameter | CamemBERTav2<sub>base</sub> | CamemBERTv2<sub>base</sub> |\n|---|---|---|\n| Number of Layers | 12 | 12 |\n| Hidden size | 768 | 768 |\n| Generator Hidden size | 256 | - |\n| FNN inner Hidden size | 3072 | 3072 |\n| Attention Heads | 12 | 12 |\n| Attention Head size | 64 | 64 |\n| Dropout | 0.1 | 0.1 |\n| Warmup Steps (p1/p2) | 10k/1k | 10k/1k |\n| Learning Rates (p1/p2) | 7e-4/3e-4 | 7e-4/3e-4 |\n| End Learning Rates (p1/p2) | 1e-5 | 1e-5 |\n| Batch Size | 8k | 8k |\n| Weight Decay | 0.01 | 0.01 |\n| Max Steps (p1/p2) | 91k/17k | 273k/17k |\n| Learning Rate Decay | Polynomial p=0.5 | Polynomial p=0.5 |\n| Adam \u03f5 | 1e-6 | 1e-6 |\n| Adam \u03b2<sub>1</sub> | 0.878 | 0.878 |\n| Adam \u03b2<sub>2</sub> | 0.974 | 0.974 |\n| Gradient Clipping | 1.0 | 1.0 |\n| Masking Probability | 20% | 40% |\n| Seq. Length (p1/p2) | 512/1024 | 512/1024 |\n| Precision | BF16 | BF16 |", "caption": "Table 5: NER F1 scores on the domain-specific downstream tasks.", "description": "This table presents the NER F1 scores achieved by various models on several domain-specific downstream tasks.  These tasks are categorized into different domains like medical (EMEA, MEDLINE, CAS1, CAS2, E3C) and radicalization (Counter-NER). The models compared include CamemBERT, CamemBERTa, DrBERT, CamemBERT-bio, CamemBERTv2, and CamemBERTav2, allowing for a comprehensive analysis of performance across different models and specific domains.", "section": "4.3 Domain Specific Results"}, {"content": "| Task | Learning Rate | LR Sch. | Epochs | Max Len. | Batch Size | Warmup |\n|---|---|---|---|---|---|---|\n| **FQuAD** | {3, 5, 7}e-5 | cosine | 6 | 1024 | {32,64} | {0,0.1} |\n| **CLS** | {3, 5, 7}e-5 | cosine<br>linear | 6 | 1024 | {32,64} | 0 |\n| **PAWS-X** | {3, 5, 7}e-5 | cosine<br>linear | 6 | 148 | {32,64} | 0 |\n| **FTB NER** | {3, 5, 7}e-5 | cosine<br>linear | 8 | 192 | {16,32} | {0,0.1} |\n| **XNLI** | {3, 5, 7}e-5 | cosine | 10 | 160 | 32 | 0.1 |\n| **POS** | 3e-05 | linear | 64 | 1024 | 8 | 100 steps |\n| **Dep. Pars.** | 3e-05 | linear | 64 | 1024 | 8 | 100 steps |\n| **Counter-NER** | {3, 5, 7}e-5 | cosine<br>linear | 8 | 512 | {16,32} | {0,0.1} |\n| **Med-NER** | 5e-5 | linear | 3 | 20 | 8 | 0.224 |", "caption": "Table 6: Hyper-parameters for pre-training CamemBERTa and CamemBERT 2.0.", "description": "This table lists the hyperparameters used during the pre-training phase for both CamemBERTa and the two new CamemBERT 2.0 models (CamemBERTav2 and CamemBERTv2).  It details settings for various aspects of the training process, including network architecture (number of layers, hidden size, attention heads), optimization (learning rate, weight decay, Adam parameters), and training data specifics (batch size, sequence length, masking probability). These hyperparameters significantly influence the models' performance and characteristics.", "section": "3 Pre-Training"}, {"content": "|Method    |\n|------------|\n|cosine     |\n|linear     |", "caption": "Table 7: Hyperparameter Search During Fine-tuning of CamemBERTv2. All models were trained with FP32", "description": "This table details the hyperparameters explored during the fine-tuning process for CamemBERTv2 on various downstream tasks.  It shows the learning rate schedule, number of epochs, maximum sequence length, batch size, and warmup steps used for each task (FQuAD, CLS, PAWS-X, FTB NER, XNLI, POS, Dependency Parsing, Counter-NER, and Med-NER). All models were trained using FP32 precision.", "section": "B.2 Fine-Tuning Hyper-parameters"}]