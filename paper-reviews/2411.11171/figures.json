[{"figure_path": "https://arxiv.org/html/2411.11171/x1.png", "caption": "(a) Token count distribution for the entire dataset (pink), the combination of all unique data (blue) and duplicate partition (gray).", "description": "This figure shows the distribution of token counts in the RedPajama dataset. The pink curve represents the distribution for the entire dataset, combining unique and duplicate entries. The blue curve shows the distribution if only the unique data points are considered. Finally, the gray curve shows the distribution of only the duplicate data points.", "section": "2.1 Dataset"}, {"figure_path": "https://arxiv.org/html/2411.11171/x2.png", "caption": "(b) Token count distribution for each partition separately: head unique, middle Unique, head duplicate and middle duplicate", "description": "This figure shows the distribution of token counts within four distinct subsets of the RedPajama dataset.  The dataset has been partitioned based on token count and duplicate status. The four subsets are: tokens from the 'head' portion of the dataset that are unique; tokens from the 'middle' portion that are unique; tokens from the 'head' portion that are duplicates; and tokens from the 'middle' portion that are duplicates. Each subset's distribution is displayed separately to reveal variations in token length across the data quality levels.", "section": "2.1 Dataset"}, {"figure_path": "https://arxiv.org/html/2411.11171/x3.png", "caption": "Figure 1: Redpajama statistics based on gbert-large tokenizer", "description": "This figure presents a statistical analysis of the RedPajama dataset, specifically focusing on the token count distribution.  Subfigure (a) shows the overall distribution, differentiating between all data, unique data, and duplicate data. Subfigure (b) further breaks down the unique and duplicate data into 'head' and 'middle' sections, which represent different quality levels within the dataset, based on a perplexity score. The tokenizer used for this analysis is gbert-large.", "section": "2.1 Dataset"}, {"figure_path": "https://arxiv.org/html/2411.11171/extracted/6005823/pics/120M/loss_120m.png", "caption": "Figure 2: Top 20 most frequent domains across the full dataset in gray with frequencies in head and middle partitions separately.", "description": "This bar chart visualizes the top 20 most frequent domains found within a large dataset used for training a German language model. The dataset is divided into 'head' and 'middle' partitions based on data quality, with the full dataset's distribution shown in gray for comparison.  The chart displays the frequency of each domain in the full dataset, as well as the frequencies specific to the head and middle partitions, allowing for a comparison of domain distribution across different data quality levels. This helps understand the composition of the training data and its potential biases.", "section": "2.1 Dataset Analysis"}, {"figure_path": "https://arxiv.org/html/2411.11171/extracted/6005823/pics/1B/loss_1b.png", "caption": "Figure 3: Loss curve of LL\u00e4Mmlein 120M model. Each color indicates a run, resumed after a training interruption.", "description": "This figure displays the training loss curve for the LL\u00e4Mmlein 120M language model.  The x-axis represents the training step, and the y-axis shows the loss value. Multiple lines are shown, each representing a separate training run.  Each run was interrupted at some point and then resumed from the latest checkpoint, with each interruption and subsequent resumption represented by a different color.  The plot allows visualization of the model's training progress and highlights the impact of training interruptions on the overall training dynamics.", "section": "3.2.1 LL\u00e4Mmlein 120M"}, {"figure_path": "https://arxiv.org/html/2411.11171/x4.png", "caption": "Figure 4: Loss curve of LL\u00e4Mmlein 1B model. Each color indicates a run, resumed after a training interruption.", "description": "This figure displays the training loss curve for the LL\u00e4Mmlein 1B language model.  Multiple lines represent separate training runs, each a different color.  The training was interrupted multiple times, and each interruption and subsequent resumption is shown as a separate colored line.  Examining the graph allows for the analysis of training dynamics and the impact of interruptions.", "section": "3.2 Model Pretraining"}, {"figure_path": "https://arxiv.org/html/2411.11171/x5.png", "caption": "snapshot from middle", "description": "The figure shows the token count distribution for a sample of the dataset's middle partition, comparing counts generated by different tokenizers.  This helps illustrate how different tokenizers process the text differently and produce varying token counts for the same dataset.", "section": "2.1 Dataset"}, {"figure_path": "https://arxiv.org/html/2411.11171/x6.png", "caption": "snapshot from head", "description": "This figure displays the token count distribution for a sample of the German dataset from the 'head' partition.  The head partition is a subset of the RedPajama V2 dataset containing high-quality German text, as determined by a perplexity score based on a language model trained on Wikipedia. The token counts are generated using the gbert-large tokenizer. The distribution shows how many tokens each document contains, providing insights into the dataset's characteristics.", "section": "2.1 Dataset"}, {"figure_path": "https://arxiv.org/html/2411.11171/x7.png", "caption": "(a)", "description": "This figure shows the token count distribution for the entire RedPajama dataset, highlighting the proportion of unique and duplicate data.  The combination of unique and duplicate data is displayed to show the distribution of all combined data. The graph aids in understanding the dataset's composition and potential redundancy during preprocessing.", "section": "2.1 Dataset"}, {"figure_path": "https://arxiv.org/html/2411.11171/x8.png", "caption": "(b)", "description": "This figure shows the token count distribution for each partition of the RedPajama dataset separately.  These partitions are categorized by the quality and duplication status of the text data: head unique, middle unique, head duplicate, and middle duplicate.  The x-axis represents the token count, and the y-axis represents the frequency of documents with that token count. The chart helps visualize how the dataset is distributed across different token lengths and duplication levels.", "section": "2.1 Dataset"}, {"figure_path": "https://arxiv.org/html/2411.11171/x9.png", "caption": "(c)", "description": "This figure shows the comparison of LL\u00e4Mmlein 120M across the full SuperGLEBer benchmark with bert-base-german-cased.  The asterisks represent the statistical significance of the differences.  \"ns\" means not significant (p > 0.05);  * indicates p < 0.05; ** indicates p < 0.01; *** indicates p < 0.001; and **** indicates p < 0.0001.", "section": "4.2.1 LL\u00e4Mmlein 120M"}, {"figure_path": "https://arxiv.org/html/2411.11171/x10.png", "caption": "Figure 5: Comparison of LL\u00e4Mmlein 120M across the full SuperGLEBer benchmark with: (5(a)) german-gpt2, (5(b)) gbert-base and (5(c)) bert-base-german-cased.\nThe asterisks indicate the level of statistical significance: \u201cns\u201d denotes not significant (p>0.05\ud835\udc5d0.05p>0.05italic_p > 0.05), while increasing significance is represented as follows: * (p\u22640.05\ud835\udc5d0.05p\\leq 0.05italic_p \u2264 0.05), ** (p\u22640.01\ud835\udc5d0.01p\\leq 0.01italic_p \u2264 0.01), *** (p\u22640.001\ud835\udc5d0.001p\\leq 0.001italic_p \u2264 0.001), and **** (p\u22640.0001\ud835\udc5d0.0001p\\leq 0.0001italic_p \u2264 0.0001).", "description": "Figure 5 presents a comparative analysis of LL\u00e4Mmlein 120M's performance against three other German Language Models (GLMs): german-gpt2, gbert-base, and bert-base-german-cased.  The evaluation is conducted across the complete SuperGLEBer benchmark, a comprehensive suite of tasks designed to assess various aspects of GLM capabilities.  The figure uses bar graphs to visually represent the performance scores of each model on each task within the benchmark. Asterisks above the bars indicate the statistical significance of performance differences, with 'ns' representing no significant difference (p>0.05), and increasing numbers of asterisks denoting progressively higher levels of significance (p\u22640.05, p\u22640.01, p\u22640.001, p\u22640.0001). This allows for a direct visual comparison of LL\u00e4Mmlein 120M's strengths and weaknesses against established models in the German NLP landscape.", "section": "4.3.1 LL\u00e4Mmlein 120M"}, {"figure_path": "https://arxiv.org/html/2411.11171/x11.png", "caption": "(a)", "description": "Token count distribution across the entire dataset, unique data, and duplicate data partitions.  It shows the frequency of documents containing a given number of tokens. This helps to understand the data distribution and the relative proportions of unique versus duplicate content in the dataset. The graph shows that most samples have around 1,000 tokens, and the distribution of token counts is heavily right skewed (long tail).", "section": "2.1 Dataset"}, {"figure_path": "https://arxiv.org/html/2411.11171/x12.png", "caption": "(b)", "description": "The figure shows the token count distribution for each partition of the RedPajama dataset separately.  These partitions are: head unique, middle unique, head duplicate, and middle duplicate. This visualization helps to understand the distribution of unique and duplicate text segments within the different quality levels (head and middle) of the dataset.  The x-axis represents the token count, and the y-axis represents the frequency of document lengths.", "section": "2.1 Dataset"}]