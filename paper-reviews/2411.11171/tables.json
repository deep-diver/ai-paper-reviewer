[{"content": "| Tokenizer | Token Count |\n|---|---| \n| word count | 80,782,685 |\n| german-gpt2 | 138,976,962 |\n| gbert-large | 140,757,764 |\n| ours 1TB | 183,720,038 |\n| ours 2023-2021 | 169,298,221 |\n| ours 2023_14 | 145,359,306 |", "caption": "Table 1: Comparison of our three created tokenizers with different training data sizes and other German tokenizers on two unseen training data samples: one from the head partition and one from the middle.", "description": "This table compares the performance of four different German tokenizers: two existing tokenizers (german-gpt2 and gbert-large) and two newly trained tokenizers created by the authors of the paper, each trained on different amounts of data (1TB, 2023-2021, and 2023_14). The comparison is based on the token counts generated by each tokenizer when applied to two unseen samples from the RedPajama dataset (one from the 'head' partition and one from the 'middle' partition).  The table shows how the choice of tokenizer and the amount of training data affect the token count, offering insights into the efficiency and performance of each.", "section": "2.2 Tokenizer Training"}, {"content": "| Tokenizer | Token Count |\n|---|---| \n| word count | 46,509,357 |\n| german-gpt2 | 78,151,205 |\n| gbert-large | 79,969,101 |\n| ours 1TB | 105,481,995 |\n| ours 2023-2021 | 96,459,503 |\n| ours 2023_14 | 81,993,239 |", "caption": "Table 2: Results of different checkpoints of LL\u00e4Mmlein 120M on six SuperGLEBer tasks compared to german_gpt2, gbert_base and bert-base-german-cased", "description": "This table presents the performance of LL\u00e4Mmlein 120M, a German language model, at various checkpoints during its training.  The results are compared against three other German models: german_gpt2, gbert_base, and bert-base-german-cased.  The comparison is made across six different tasks from the SuperGLEBer benchmark, illustrating the model's progress and its performance relative to established baselines.", "section": "2.4.1 Intermediate Checkpoint Evaluation"}, {"content": "| Model | FactClaiming | EuroParl | Pawsx | NLI | DB Aspect | WebCAGe |\n|---|---|---|---|---|---|---|\n| 010000 | 0.711 | 0.531 | 0.427 | 0.549 | 0.454 | 0.689 |\n| 050000 | 0.717 | 0.536 | 0.428 | 0.549 | 0.452 | 0.688 |\n| 100000 | 0.708 | 0.532 | 0.464 | 0.559 | 0.479 | 0.700 |\n| 150000 | 0.702 | 0.516 | 0.474 | 0.575 | 0.474 | 0.692 |\n| 200000 | 0.705 | 0.497 | 0.497 | 0.575 | 0.464 | 0.703 |\n| 210000 | 0.715 | 0.493 | 0.489 | 0.578 | 0.475 | 0.685 |\n| 250000 | 0.723 | 0.536 | 0.478 | 0.560 | 0.479 | 0.684 |\n| 300000 | 0.712 | 0.525 | 0.497 | 0.615 | 0.498 | 0.682 |\n| 350000 | 0.705 | 0.547 | 0.492 | 0.624 | 0.511 | 0.678 |\n| 400000 | 0.713 | 0.522 | 0.488 | 0.627 | 0.511 | 0.695 |\n| 450000 | 0.693 | 0.511 | 0.479 | 0.638 | 0.504 | 0.694 |\n| 466509 | 0.711 | 0.538 | 0.489 | 0.629 | 0.517 | 0.687 |\n| german_gpt2 | 0.707 | 0.533 | 0.394 | 0.479 | 0.429 | 0.645 |\n| gbert_base | 0.751 | 0.616 | 0.561 | 0.436 | 0.478 | 0.693 |\n| bert-base-german-cased | 0.721 | 0.607 | 0.537 | 0.490 | 0.480 | 0.679 |", "caption": "Table 3: Performance of LL\u00e4Mmlein 1B across multiple training checkpoints on six SuperGLEBer tasks, with comparison to the best-performing models for each task in the benchmark.", "description": "This table presents the performance of the LL\u00e4Mmlein 1B language model at various checkpoints during its training.  It shows the model's scores on six different tasks from the SuperGLEBer benchmark, comparing its performance at different training stages.  The comparison is made against the best performing models for each task reported in the benchmark. This allows for an assessment of the model's progress throughout training and its overall capabilities compared to existing state-of-the-art models.", "section": "4.2.2 LL\u00e4Mmlein 1B"}, {"content": "| Model | FactClaiming | EuroParl | Pawsx | NLI | DB Aspect | WebCAGe |\n|---|---|---|---|---|---|---|\n| 010000 | 0.735 | 0.708 | 0.461 | 0.642 | 0.563 | 0.677 |\n| 100000 | 0.734 | 0.662 | 0.511 | 0.709 | 0.607 | 0.699 |\n| 190000 | 0.736 | 0.701 | 0.525 | 0.721 | 0.614 | 0.719 |\n| 310000 | 0.744 | 0.656 | 0.521 | 0.725 | 0.611 | 0.720 |\n| 400000 | 0.716 | 0.665 | 0.517 | 0.722 | 0.623 | 0.719 |\n| 500000 | 0.733 | 0.712 | 0.539 | 0.734 | 0.613 | 0.720 |\n| 600000 | 0.712 | 0.724 | 0.541 | 0.725 | 0.608 | 0.722 |\n| 700000 | 0.737 | 0.676 | 0.529 | 0.727 | 0.630 | 0.722 |\n| 800000 | 0.718 | 0.727 | 0.528 | 0.743 | 0.613 | 0.742 |\n| 900000 | 0.732 | 0.718 | 0.542 | 0.748 | 0.634 | 0.733 |\n| 950000 | 0.747 | 0.732 | 0.556 | 0.746 | 0.622 | 0.755 |\n| 1000000 | 0.750 | 0.697 | 0.540 | 0.740 | 0.629 | 0.756 |\n| 1100000 | 0.740 | 0.710 | 0.550 | 0.744 | 0.623 | 0.762 |\n| 1200000 | 0.726 | 0.679 | 0.545 | 0.746 | 0.629 | 0.755 |\n| 1300000 | 0.725 | 0.695 | 0.533 | 0.751 | 0.624 | 0.764 |\n| 1350000 | 0.748 | 0.712 | 0.528 | 0.752 | 0.633 | 0.763 |\n| 1400000 | 0.729 | 0.702 | 0.536 | 0.741 | 0.629 | 0.756 |\n| 1420000 | 0.745 | 0.702 | 0.530 | 0.345 | 0.643 | 0.759 |\n| 1430512 | 0.736 | 0.713 | 0.526 | 0.749 | 0.623 | 0.765 |\n| gbert_base | 0.751 | 0.616 | 0.561 | 0.436 | 0.478 | 0.693 |\n| mbart_large_50 | 0.723 | 0.727 | 0.358 | 0.336 | 0.471 | 0.651 |\n| gbert_large | 0.747 | 0.636 | 0.654 | 0.736 | 0.550 | 0.716 |\n| leo-mistral-7b | 0.741 | 0.649 | - | 0.807 | 0.664 | - |\n| leo-hessian-7b | 0.747 | - | - | - | 0.669 | 0.781 |", "caption": "Table 4: Performance comparison of LL\u00e4Mmlein 120M to other language models on the lm-evaluation-harness-de including the four translated tasks: TruthfulQA, ARC-Challenge, HellaSwag and MMLU.", "description": "This table presents a performance comparison of the LL\u00e4Mmlein 120M language model against other models on the `lm-evaluation-harness-de` benchmark.  The benchmark includes four translated tasks: TruthfulQA, ARC-Challenge, HellaSwag, and MMLU. The table allows for a quantitative assessment of LL\u00e4Mmlein 120M's capabilities relative to existing models across diverse question answering, common sense reasoning, and factual knowledge tasks.", "section": "4.3 Final Model Evaluation"}, {"content": "| Model | TruthfulQA | ARC-Challenge | HellaSwag | MMLU |\n|---|---|---|---|---|\ngerman gpt2 | 0.261|0.432 | 0.195|0.236 | 0.262|0.268 | 0.238|0.263 |\nLL\u00e4Mmlein 120M | 0.247|0.404 | 0.194|0.238 | 0.291|0.320 | 0.245|0.276 |\nLL\u00e4Mmlein 120M Alpaka | 0.266|0.439 | 0.178|0.235 | 0.269|0.277 | 0.231|0.268 |", "caption": "Table 5: Performance comparison of LL\u00e4Mmlein 1B and its Instruction tuned variants as well as the similar sized Llama 3.2 1B and various larger models on the lm-evaluation-harness-de including the four tasks: TruthfulQA, ARC-Challenge, HellaSwag and MMLU.", "description": "This table presents a performance comparison of various large language models (LLMs) on a German language evaluation benchmark.  The models compared include LL\u00e4Mmlein 1B (and instruction-tuned variants), Llama 3.2 1B, and several larger models. The benchmark used is `lm-evaluation-harness-de`, and the specific tasks assessed are TruthfulQA, ARC-Challenge, HellaSwag, and MMLU. The results show the accuracy of each model on each task, allowing for a comparison of performance across different model sizes and training methodologies (base vs. instruction-tuned). This helps evaluate the effectiveness of LL\u00e4Mmlein 1B relative to other state-of-the-art models, particularly considering its smaller size and training approach.", "section": "4.3 Final Model Evaluation"}]