{"references": [{"fullname_first_author": "Stella Biderman", "paper_title": "Pythia: a suite for analyzing large language models across training and scaling", "publication_date": "2023-00-00", "reason": "This paper provides a benchmark suite used for analyzing LLMs across training and scaling, which is directly relevant to the methodology and experiments performed in the target paper."}, {"fullname_first_author": "Together Computer", "paper_title": "RedPajama: an open dataset for training large language models", "publication_date": "2023-00-00", "reason": "This paper introduces the RedPajama dataset, a crucial resource used for training the German-only language models discussed in the main paper."}, {"fullname_first_author": "Peiyuan Zhang", "paper_title": "Tinyllama: An open-source small language model", "publication_date": "2024-01-00", "reason": "This paper describes the TinyLlama framework, which is used in the target paper for model pretraining and is key to their model training methodology."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "publication_date": "2024-07-00", "reason": "This paper discusses Llama 3 models, providing a comparative context for the German-only models by highlighting the limitations of non-English language models."}, {"fullname_first_author": "Jan Pfister", "paper_title": "SuperGLEBer: German language understanding evaluation benchmark", "publication_date": "2024-00-00", "reason": "This paper presents the SuperGLEBer benchmark, a key evaluation tool used to assess the performance of the German-only LLMs developed in the main study."}]}