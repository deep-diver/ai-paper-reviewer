[{"figure_path": "https://arxiv.org/html/2501.03916/x1.png", "caption": "Figure 1: Comparisons of the four steps in the evolutionary trajectory towards auto-research including (a) Entirely human-driven research, (b) AI-assisted research, (c) Semi-automatic research, and (d) Auto-research.", "description": "Figure 1 illustrates the evolution of scientific research methods, progressing from fully manual processes to complete automation.  (a) shows the traditional human-driven approach where all steps, from idea generation to experimentation and analysis, are performed by researchers. (b) depicts AI-assisted research, where AI tools enhance specific steps such as data analysis or computation.  (c) represents semi-automatic research, characterized by partial automation of the research workflow. Finally, (d) showcases the ultimate goal of fully automated scientific research, where AI autonomously handles the entire research process, from idea generation and experimentation to result analysis.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.03916/x2.png", "caption": "Figure 2: The overview of Dolphin, an closed-loop open-ended auto-research framework. Dolphin first generates a set of ideas based on the retrieved papers. After filtering ideas, experimental plans will be generated for these filtered ideas. Then, codes can be generated and debugged using the proposed error-traceback-guided debugging process. Finally, the results of successfully executed experiments will be auto-analyzed and reflected into the next round of ideas generation.", "description": "The figure illustrates the DOLPHIN framework, a closed-loop, open-ended auto-research system.  It details the iterative process of scientific research automation.  First, DOLPHIN retrieves relevant papers and generates research ideas based on those papers.  These ideas then undergo a filtering process to select the most promising ones.  For each selected idea, an experimental plan is created and subsequently used to generate the necessary code.  The generated code is then automatically debugged using an error-traceback-guided approach.  Finally, DOLPHIN automatically analyzes the results of successful experiments, feeding that information back into the system to improve the quality of future idea generation.  This iterative cycle repeats to continuously refine the research process.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2501.03916/x3.png", "caption": "Figure 3: Debugging with traceback-guided local code structure.", "description": "This figure illustrates DOLPHIN's exception-traceback-guided debugging process.  When an error occurs during code execution, DOLPHIN extracts information from the exception traceback, including the function name, line number, and code snippet causing the error.  This information is used to guide the LLM in analyzing the local code structure and identifying the source of the error. The LLM then suggests code modifications to resolve the issue. The figure displays an example of an error (RuntimeError), the relevant code section, and the proposed modification to the code, demonstrating how DOLPHIN uses the traceback to pinpoint and correct errors automatically.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2501.03916/x4.png", "caption": "Figure 4: Case studies for the ideas and codes generated by Dolphin (Left) and human researcher (Right).", "description": "This figure presents a comparative case study of the idea generation and code implementation process between DOLPHIN and a human researcher.  The left panel showcases DOLPHIN's output: a novel idea for enhancing point cloud classification using contextual semantic reasoning, alongside the automatically generated code. The right panel shows an equivalent example from a human researcher, highlighting a similar task (dynamic graph CNN for point cloud processing) and associated code.  The comparison illustrates DOLPHIN's capacity to generate comparable research ideas and code relative to a human expert.", "section": "4.4 Case Studies"}, {"figure_path": "https://arxiv.org/html/2501.03916/x5.png", "caption": "Figure 5: Prompts of paper retrieval, paper ranking, and ideas generation.", "description": "Figure 5 shows the three prompts used in the DOLPHIN framework for idea generation. The first prompt focuses on extracting relevant task attributes from a given topic description to guide the paper search process. The second prompt instructs the system to retrieve relevant papers using the Semantic Scholar API based on specified keywords. The third prompt guides the system to generate novel and non-redundant research ideas based on retrieved papers, taking into consideration existing knowledge and previous ideas.  Each prompt specifies the required input format (e.g., JSON) for the response and guides the system towards generating high-quality ideas.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2501.03916/x6.png", "caption": "Figure 6: An example of independence check.", "description": "This figure displays an example of the independence check performed by DOLPHIN, a closed-loop open-ended auto-research framework.  DOLPHIN aims to generate novel and non-redundant research ideas. The independence check ensures that newly generated ideas are distinct from those already considered.  The figure shows two ideas, both aiming to improve point cloud classification using PointNet. Although the ideas have different titles and brief descriptions, a closer examination reveals a substantial overlap in their underlying methodologies.  The independence check flags one as independent and the other as non-independent due to this similarity, preventing redundant experimentation.", "section": "3.1. Ideas Generation Process"}, {"figure_path": "https://arxiv.org/html/2501.03916/x7.png", "caption": "Figure 7: Prompts of local code structure and debugging.", "description": "This figure shows the two prompts used in the DOLPHIN framework for debugging automatically generated code.  The first prompt, \"Code Structure Prompt\", instructs a large language model (LLM) to analyze error messages and traceback information to identify and extract the relevant code structure associated with the error. This structure is then provided to the second prompt, \"Debugging Prompt,\" which guides the LLM to generate a plan to fix the error within the extracted code, making the necessary modifications and ensuring code functionality.  This two-step process uses the LLM to debug automatically generated code, improving efficiency in the auto-research cycle.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2501.03916/x8.png", "caption": "Figure 8: Code structure without extracted traceback information.", "description": "Figure 8 shows the code structure before applying the exception-traceback-guided debugging process.  The image displays sections of Python code related to a `PointNetEncoder` class and a `Model` class, illustrating the code's architecture and function calls involved in the forward pass of a neural network. This visualization is used to highlight the complexity of the codebase prior to the application of the debugging technique, emphasizing the challenges of automated debugging in this context.", "section": "3.2. Experimental Verification Process"}, {"figure_path": "https://arxiv.org/html/2501.03916/x9.png", "caption": "Figure 9: Idea and codes generated by Dolphin which achieves 92.34% OA and 89.54% mAcc. on ModelNet40 (+1.34% OA and +1.94% mAcc. compared to our baseline).", "description": "Figure 9 showcases an example of an idea and its corresponding code generated by the DOLPHIN framework.  The idea focuses on enhancing point cloud classification by incorporating a latent space exploration mechanism using an autoencoder.  The code implements this mechanism, mapping point cloud data to a latent space for exploration before combining latent features with the original features for improved classification. The results demonstrate a significant performance improvement on the ModelNet40 dataset, achieving 92.34% overall accuracy (OA) and 89.54% mean accuracy (mAcc), representing improvements of +1.34% OA and +1.94% mAcc compared to the baseline PointNet model. This highlights DOLPHIN's ability to generate effective and novel ideas within the scientific research process.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2501.03916/x10.png", "caption": "Figure 10: Idea and codes generated by Dolphin which achieves 92.30% OA and 88.96% mAcc. on ModelNet40 (+1.30% OA and +1.36% mAcc. compared to our baseline).", "description": "Figure 10 presents a case study showcasing DOLPHIN's capabilities.  DOLPHIN automatically generated a novel idea and corresponding code for enhancing 3D point cloud classification.  This involved creating a \"Contrast Enhancement Module\" within the PointNet architecture, designed to amplify the contrast between features to improve classification accuracy. The results demonstrate that this DOLPHIN-generated approach achieves 92.30% overall accuracy (OA) and 88.96% mean accuracy (mAcc) on the ModelNet40 dataset. This represents a significant improvement of +1.30% OA and +1.36% mAcc compared to the baseline PointNet model.", "section": "4. Case Studies"}, {"figure_path": "https://arxiv.org/html/2501.03916/x11.png", "caption": "Figure 11: Idea and codes generated by Dolphin which achieves 82.05% Acc. on CIFAR-100 (+0.85% Acc. compared to our baseline).", "description": "Figure 11 presents a case study from the DOLPHIN system, showcasing its ability to generate novel research ideas and corresponding code.  Specifically, it details an idea for enhancing image classification accuracy on the CIFAR-100 benchmark dataset by integrating a bio-inspired filter module into the WideResNet architecture. This module mimics the functionality of the human visual cortex, using Gabor filters to detect edges and patterns.  The generated code implements this module, demonstrating DOLPHIN's capacity for both conceptual innovation and functional code generation. The result shows a performance improvement of +0.85% in accuracy compared to the baseline WideResNet.", "section": "4. Case Studies"}]