[{"content": "| Tasks |  | CIFAR-100 |  | ModelNet40 |  | SST-2 |\n|---|---|---|---|---|---|---|\n|  |  | Top-1 Acc. (%) |  | OA (%) | mAcc. (%) | Acc. (%) |\n| Baseline |  | 79.5 (WRN-28-10 [49]) |  | 89.2 (PointNet [28]) | 86.2 (PointNet [28]) |  | - |\n| Baseline\u2020 |  | 81.2 (WRN-28-10) |  | 91.0 (PointNet [28]) | 87.6 (PointNet [28]) |  | 91.0 (BERT-base [8]) |\n| Avg. Improvement |  | 81.8 (+0.6) |  | 92.0 (+1.0) | 88.7 (+1.1) |  | 91.8 (+0.8) |\n| Max Improvement |  | 82.0 (+0.8) |  | 93.9 (+2.9) | 91.1 (+3.5) |  | 92.5 (+1.5) |\n| Human designed |  | 82.2 (ResNeXt [22]) |  | 93.8 (GPSFormer [40]) | 91.8 (GPSFormer [40]) |  | 93.1 (BERT-large [8]) |\n| Number ideas |  | 6 / 40 |  | 5 / 40 | 5 / 40 |  | 6 / 40 |", "caption": "Table 1: Experimental verifications on 2D image classification, 3D point classification, and sentiment classification tasks. Number ideas refers to the number of ideas that can achieve performance gains. \u2020\u2020\\dagger\u2020 denotes the results of our implementation. Avg. Improvement and Max Improvement represent the average and maximum improvement of all ideas that can improve the baseline performance.", "description": "This table presents the results of experiments conducted to evaluate the effectiveness of the DOLPHIN framework on three different tasks: 2D image classification, 3D point cloud classification, and sentiment classification.  For each task, the table shows the top-1 accuracy or overall accuracy achieved by a baseline model.  It then shows the average and maximum improvement in accuracy achieved by DOLPHIN across multiple automatically generated ideas.  The 'Number of ideas' column indicates how many of the generated ideas resulted in performance gains compared to the baseline. The results marked with \u2020 denote those obtained using the DOLPHIN implementation.", "section": "4. Experimental Results"}, {"content": "| Method | Novelty | Cost (Avg.) |\n|---|---|---|\n| Naive generation | 8 / 20 | $0.106 |\n| Generation with naive retrieval | 13 / 20 | $0.187 |\n| Ours (task attribute filtering) | 19 / 20 | $0.184 |", "caption": "Table 2: Results of ideas generation process. The novelty is evaluated by gpt-4o-2024-08-06. Cost (Avg.) is the cost per idea including paper retrieval, ideas generation, and novelty check.", "description": "This table presents a comparison of three different approaches to idea generation within the DOLPHIN framework.  The first approach is a naive generation method, the second incorporates naive retrieval of papers, and the third is the proposed DOLPHIN method which uses task-attribute filtering to improve relevance.  For each approach, the table shows the number of novel ideas generated (as assessed by GPT-4-2024-08-06) out of a total of 20,  alongside the average cost per idea, including the costs of paper retrieval, idea generation, and novelty checks. This allows for analysis of the cost-effectiveness and idea generation quality of each approach.  The goal is to demonstrate the effectiveness of the proposed task-attribute filtering in producing more novel ideas at a comparable or lower cost.", "section": "4. Results"}, {"content": "| Keywords | Classification | Detection | Segmentation | Completion |\n|---|---|---|---|---|\n| Naive | 82 | 17 | 38 | 16 |\n| Filter (ours) | 109 | 4 | 43 | 0 |", "caption": "Table 3: For the 3D classification task, the frequency of each keyword is determined from the retrieved papers, focusing only on those words that appear in the abstracts and titles of papers scoring above 8 points in the ranking process. \u201cNaive\u201d and \u201cFilter\u201d refer to naive retrieval and filtering based on task attributes.", "description": "This table presents a quantitative analysis of keywords extracted from research papers related to 3D classification.  Only papers with a score above 8 (indicating high relevance) in a pre-processing ranking step were included in the analysis.  The keyword frequencies are compared between two approaches: a 'naive' method (direct keyword search), and a method employing 'filtering' based on task attributes. This comparison highlights the impact of attribute-based filtering on retrieving relevant papers for the 3D classification task.", "section": "4.2. Main Results"}, {"content": "| L.C.S. | Traceback | Successful execution |  |  |  |\n|---|---|---|---|---|---|\n| \\usym2717 | \\usym2717 | 4 / 15 | 5 / 13 | 5 / 14 |  |\n| \\usym2713 | \\usym2717 | 3 / 15 | 5 / 13 | 6 / 14 |  |\n| \\usym2713 | \\usym2713 | 7 / 15 | 6 / 13 | 8 / 14 |  |", "caption": "Table 4: Results of successful execution rate. L.C.S. represents local code structure. Traceback denotes using information extracted from exception traceback. The denominator is the number of ideas after the novelty and independence check.", "description": "This table presents the success rates of code execution in the experimental verification process of the DOLPHIN framework. Three different approaches are compared: using only the exception traceback, incorporating the local code structure (L.C.S.) along with the traceback, and extracting the L.C.S. from the traceback before feeding it to the LLM. The success rate is calculated as the number of successfully executed ideas divided by the total number of ideas after novelty and independence checks. The results show a significant improvement in execution rate when using local code structure information.", "section": "4. Experimental Verification Process"}, {"content": "| Loop | Loop 1 | Loop 2 | Loop 3 | Total |\n|---|---|---|---|---|\n| Improvement rate | 2 / 7 | 3 / 6 | 4 / 8 | 9 / 21 |\n| Cost (Avg.) | 0.184 | 0.203 | 0.218 | 0.201 |", "caption": "Table 5: Performance in different loops. The denominator is the number of successfully executed ideas.", "description": "This table presents the performance of DOLPHIN across three different loops of the research process.  It shows the improvement rate, calculated as the number of successful experiments that resulted in performance gains divided by the total number of successfully executed ideas.  The average cost per idea is also given for each loop. This data illustrates how the efficiency and effectiveness of DOLPHIN evolves across iterations, demonstrating improvement in performance over time.", "section": "4.2. Main Results"}, {"content": "| Method | Accuracy (Avg. class) | Overall accuracy |\n|---|---|---|\n| **Human designed methods** |  |  |\n| PointNet [28] | 86.2 | 89.2 |\n| PointNet++ [29] | - | 91.9 |\n| DGCNN [43] | 90.2 | 92.9 |\n| PointNeXt [30] | 90.8 | 93.2 |\n| OctFormer [41] | - | 92.7 |\n| GPSFormer [40] | **91.8** | 93.8 |\n| **Methods obtained by Dolphin (auto-research)** |  |  |\n| PointNet-CSR | 91.1 | **93.9** |", "caption": "Table 6: Classification on ModelNet40\u00a0[45]. The results are obtained from 1024 points without voting.", "description": "Table 6 presents the classification results on the ModelNet40 dataset [45], a widely used benchmark in 3D point cloud analysis.  The model's performance is evaluated using 1024 points per sample, without employing any voting mechanisms to combine predictions from multiple parts of the model. This setup provides a clear and direct evaluation of the model's inherent ability to classify 3D shapes based on their point cloud representations.", "section": "4.2. Main Results"}, {"content": "| Diff. | DGCNN | PointNet-CSR (Completed by Dolphin) |\n|---|---|---|\n| Idea | 1) Architecture-level | 1) Module-level |\n|  | 2) With learnable parameters | 2) Without learnable parameters |\n|  | 3) Repeated blocks | 3) Single module |\n| Impl. | Multi-layer Edge with high complexity | Single contextual semantic reasoning module with low complexity |\n| Results | 1) 90.2% mAcc., 92.9% OA | 1) 91.1% mAcc., 93.9% OA |\n|  | 2) ~ 20.86s per epoch | 2) ~ 6.12s per epoch (> 3x faster) |", "caption": "Table 7: The differences between DGCNN\u00a0[43] proposed by human and PointNet-CSR proposed using Dolphin.", "description": "This table compares the human-designed Dynamic Graph CNN (DGCNN) model with the PointNet-CSR model generated by the DOLPHIN framework.  It highlights key architectural differences, including the level of design (architecture vs. module), the use of learnable parameters, the number of repeated blocks used, and the overall complexity of the model.  The table also presents a comparison of the model's performance (mAcc and OA), as well as training efficiency in terms of time per epoch.", "section": "4.3. Further Analyses"}]