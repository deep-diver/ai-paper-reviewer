{"references": [{"fullname_first_author": "Tri Dao", "paper_title": "Flashattention: Fast and memory-efficient exact attention with io-awareness", "publication_date": "2022-01-01", "reason": "This paper is important because the MOBA model incorporates optimization techniques from FlashAttention, leading to efficient attention computation."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-01-01", "reason": "This paper is important because the MOBA architecture builds upon the principles of Mixture of Experts (MoE), which are detailed in this reference."}, {"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-01-01", "reason": "This paper is important because the MOBA experiments follow the Chinchilla scaling law described here to train language models and assess their validation loss."}, {"fullname_first_author": "Iz Beltagy", "paper_title": "Longformer: The long-document transformer", "publication_date": "2020-01-01", "reason": "This paper is important because it introduces the Longformer, which is related to MOBA as it is another type of sparse attention architecture and it provides a baseline for comparison."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-01-01", "reason": "This paper is important because it introduces Mamba, a linear attention model, which is discussed as a promising alternative to traditional attention mechanisms and provides a point of comparison for MOBA."}]}