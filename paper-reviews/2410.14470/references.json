{"references": [{" publication_date": "1989b", "fullname_first_author": "LeCun", "paper_title": "Optimal Brain Damage", "reason": "This paper is foundational in the field of neural network pruning and weight decay, influencing many subsequent studies on neural network efficiency and resource optimization.  Its exploration of removing less important weights and connections in networks directly relates to the core concept of this paper on layer criticality, establishing an important connection between weight optimization and layer utility.", "section_number": 1}, {" publication_date": "1993", "fullname_first_author": "Hassibi", "paper_title": "Optimal Brain Surgeon and general network pruning", "reason": "This work introduces a systematic approach to pruning neural networks, significantly contributing to the understanding of which network components contribute most to the final output. Its relevance to this work lies in the direct connection between the identification of less important neurons and the concept of identifying auxiliary (non-critical) layers.", "section_number": 1}, {" publication_date": "2015", "fullname_first_author": "Hinton", "paper_title": "Distilling the Knowledge in a Neural Network", "reason": "The concept of distilling large networks into smaller, efficient ones directly addresses the research question of utilizing fewer parts of networks. This paper presents an efficient and scalable knowledge transfer technique in neural networks, allowing for the creation of smaller models without a significant loss in performance. This technique is closely related to the main topic of the paper about efficient neural network utilization and layers.", "section_number": 1}, {" publication_date": "2015", "fullname_first_author": "He", "paper_title": "Deep Residual Learning for Image Recognition", "reason": "This paper introduced the ResNet architecture which is used extensively in this work as the model architecture of choice and is a fundamental element in this study. The study uses the ResNet-50 model extensively to test their methodology and analysis, establishing the importance of this paper as the foundation of the experiments performed.", "section_number": 3}, {" publication_date": "2015", "fullname_first_author": "Ioffe", "paper_title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "reason": "Batch normalization is a key technique used in training modern neural networks, impacting the efficiency and stability of training.  This paper presents a method which is directly related to the training methods in the current study, which heavily relies on batch normalization layers in the ResNet-50 architecture. The techniques in this paper are essential for the study's findings.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Madry", "paper_title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "reason": "This work introduced adversarial training, a critical training technique investigated in this research.  The exploration of adversarial training directly contributes to this study because it is a significant factor in the layer criticality determination.  By testing adversarial training methods, this study contributes to the better understanding of its impact on network structure and efficiency.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Cubuk", "paper_title": "AutoAugment: Learning Augmentation Strategies From Data", "reason": "This paper presents a methodology for automatically learning data augmentation strategies, which is a key element of training modern neural networks and is directly relevant to this study.  The methodology is important because data augmentation methods heavily impact the outcome and efficiency of neural network training. ", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Cubuk", "paper_title": "RandAugment: Practical Automated Data Augmentation with a Reduced Search Space", "reason": "RandAugment is a practical and efficient data augmentation technique that directly relates to training methodologies investigated in this study. This paper improves the efficiency of data augmentation strategies, allowing for wider experimentation and optimization which is directly linked to the goals of improving training efficiency.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Caron", "paper_title": "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments", "reason": "This work is foundational for self-supervised learning and its influence on the learning processes in this study. It provides a robust methodology for self-supervised learning using contrasting cluster assignments and is directly related to the self-supervised learning components of the current study. It makes significant contributions in understanding the impact of self-supervised learning on network efficiency.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Chen", "paper_title": "Big Self-Supervised Models are Strong Semi-Supervised Learners", "reason": "This paper introduced a significant advance in self-supervised learning, demonstrating that large self-supervised models can act as strong semi-supervised learners. This is highly relevant to the current research because self-supervised learning and semi-supervised learning are closely related and can be used in various contexts of neural network training. Understanding the contribution of this paper is vital for understanding the methodology of this study.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Chen", "paper_title": "An Empirical Study of Training Self-Supervised Vision Transformers", "reason": "This work provides insights into training self-supervised vision transformers, which are directly related to the models used in this study and its methodology. The focus on training self-supervised vision transformers makes it closely related to the core methodology. By investigating these models, the study increases its overall credibility and provides a richer context.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Caron", "paper_title": "Emerging Properties in Self-Supervised Vision Transformers", "reason": "This paper investigates the properties of self-supervised vision transformers and is closely related to the models investigated in this study. The models' performance is critically dependent on the training methods which directly influence layer criticality. This paper contributes important findings for better understanding of the training methods in the current study.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Hendrycks", "paper_title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization", "reason": "This paper directly addresses the robustness of different training methods and is highly relevant to this study which investigates the influence of various training methods on layer criticality. Understanding robustness and how it relates to the distribution of criticality across layers is essential for this study, improving overall credibility.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Modas", "paper_title": "PRIME: A Few Primitives Can Boost Robustness to Common Corruptions", "reason": "This paper introduces a technique for boosting robustness against corruptions, which is a significant factor that influences the training method's impact on neural networks. This paper is critically important to the study because robustness and the overall reliability of the networks is influenced by the training methods and it is a core topic being explored.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Zhang", "paper_title": "Are All Layers Created Equal?", "reason": "This paper is crucial as it directly investigates the concept of layer criticality, serving as a foundational work for the current study. This paper makes a significant contribution to the methodology of this study and helps to establish a clear connection between past research and future work. The current study expands upon the findings of this paper, making it a keystone reference.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Chatterji", "paper_title": "The Intriguing Role of Module Criticality in the Generalization of Deep Networks", "reason": "This work is directly related to the concept of layer criticality and its influence on the generalization of deep neural networks. The study's methodology is informed by the findings of this work, indicating a clear and critical link between the two research pieces. Understanding this paper is a crucial step in understanding the overall methodology and findings of the current study.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Wightman", "paper_title": "PyTorch Image Models", "reason": "This work provides the foundation for the models investigated in this study and their training methods. The use of pre-trained models in the study is critically dependent on this work, making it a vital piece of the methodology and findings in this paper. This is highly relevant to the overall findings and conclusions in this study.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Wightman", "paper_title": "ResNet strikes back: An improved training procedure in timm", "reason": "This paper introduces improved training procedures for ResNet models which are directly utilized in this study. The improved training procedures heavily influence the performance and outcome of this study because its core component revolves around testing these training procedures in order to evaluate the influence of training methods on the resulting network efficiency and layer criticality.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Vryniotis", "paper_title": "How to Train State-Of-The-Art Models Using TorchVision's Latest Primitives", "reason": "This work provides the most up-to-date training methods for state-of-the-art models, which is directly relevant to this study.  The use of the latest training methods is highly important for evaluating the impact of training strategies on network efficiency and layer criticality.  Understanding these training methods is critical for interpreting the findings in this study.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Agnihotri", "paper_title": "On the unreasonable vulnerability of transformers for image restoration-and an easy fix", "reason": "This work investigates vulnerabilities of transformers, which are directly relevant to the robustness of neural networks being investigated in this study. By understanding vulnerabilities and weaknesses, the study can better address the challenges of neural network training and optimization. It helps to build a stronger foundation and context for interpreting the results of this study.", "section_number": 3}]}