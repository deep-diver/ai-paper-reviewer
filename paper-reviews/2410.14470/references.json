{"references": [{" publication_date": "2022", "fullname_first_author": "Zhang", "paper_title": "Are All Layers Created Equal?", "reason": "This paper is foundational to the current work, introducing the concept of critical and auxiliary layers within neural networks. The authors' methodology of randomizing layer parameters and measuring performance changes serves as a basis for the current study's methodology, which builds upon and refines this approach.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Chatterji", "paper_title": "The intriguing role of module criticality in the generalization of deep networks", "reason": "This paper significantly contributes to the understanding of layer criticality by showing a strong correlation between the average module criticality and the generalization performance of neural networks. The current research expands on these findings by exploring how different training strategies affect this relationship.", "section_number": 1}, {" publication_date": "2015", "fullname_first_author": "He", "paper_title": "Deep Residual Learning for Image Recognition", "reason": "The ResNet-50 architecture, used extensively in the current study, is introduced in this highly influential paper. ResNets have become a cornerstone of computer vision, and understanding their behavior under various training conditions is crucial.", "section_number": 1}, {" publication_date": "2015", "fullname_first_author": "Russakovsky", "paper_title": "ImageNet Large Scale Visual Recognition Challenge", "reason": "The ImageNet dataset, the backbone of the experimental evaluations in this study, is described in detail in this paper.  Understanding the properties and characteristics of ImageNet is paramount to interpreting the results of the current research.", "section_number": 1}, {" publication_date": "1989", "fullname_first_author": "LeCun", "paper_title": "Handwritten Digit Recognition with a Back-Propagation Network", "reason": "This pioneering work significantly contributed to the development of deep learning techniques and is considered a seminal paper in the field. The use of backpropagation for training neural networks is foundational to most deep learning models, making this reference highly relevant.", "section_number": 1}, {" publication_date": "1993", "fullname_first_author": "Hassibi", "paper_title": "Optimal Brain Surgeon and general network pruning", "reason": "The concept of 'parameter pruning' which this paper is foundational to is important for the current work and provides crucial background on the under-utilization of neural network parameters.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Zhang", "paper_title": "Are All Layers Created Equal?", "reason": "This paper is foundational to the current work, introducing the concept of critical and auxiliary layers within neural networks.  The authors' methodology of randomizing layer parameters and measuring performance changes serves as a basis for the current study's methodology, which builds upon and refines this approach.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "Ioffe", "paper_title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "reason": "Batch Normalization is a widely used technique in training deep neural networks.  Understanding its role and influence on layer criticality is crucial, hence the reference.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "Goodfellow", "paper_title": "Explaining and Harnessing Adversarial Examples", "reason": "Adversarial training is a key aspect of the current study, and this seminal paper introduces the concept of adversarial examples and provides the foundation for adversarial training methods.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "Russakovsky", "paper_title": "ImageNet Large Scale Visual Recognition Challenge", "reason": "The ImageNet dataset, the backbone of the experimental evaluations in this study, is described in detail in this paper. Understanding the properties and characteristics of ImageNet is paramount to interpreting the results of the current research.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Madry", "paper_title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "reason": "This paper introduces the Projected Gradient Descent (PGD) attack, which is used in the current study's adversarial training experiments.  A deep understanding of this attack is essential for analyzing the results.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Hendrycks", "paper_title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization", "reason": "PixMix, a data augmentation technique, is used in the current study and its effectiveness is detailed in this highly relevant paper.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Caron", "paper_title": "Emerging Properties in Self-Supervised Vision Transformers", "reason": "DINO, a self-supervised learning method, is used in the current study's experiments. This paper introduces DINO and its properties, making it a significant reference.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Chen", "paper_title": "An Empirical Study of Training Self-Supervised Vision Transformers", "reason": "MoCo v3, a self-supervised learning method, is used in this study's experiments. This paper introduces MoCo v3 and its properties, making it a significant reference.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Caron", "paper_title": "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments", "reason": "SwAV, a self-supervised learning method used in the current study, is introduced in this paper.  Understanding SwAV and its properties is crucial for interpreting the results.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Chen", "paper_title": "Big Self-Supervised Models are Strong Semi-Supervised Learners", "reason": "SimCLR v2, a self-supervised learning method used in this study, is introduced in this paper.  Understanding SimCLR v2 and its properties is crucial for interpreting the results.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Wightman", "paper_title": "PyTorch Image Models", "reason": "The timm library, used in the current study's experiments, is introduced in this paper.  Understanding the timm library and its properties is crucial for interpreting the results.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Vryniotis", "paper_title": "How to Train State-of-the-Art Models Using TorchVision's Latest Primitives", "reason": "This paper provides important details regarding the improved training recipes used in this study. These advanced techniques are critical for understanding the results and their implications.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Hendrycks", "paper_title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization", "reason": "AugMix, a data augmentation technique used in the current study, is introduced and discussed in this paper. The effectiveness of AugMix is particularly relevant for interpreting the results of this research.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Cubuk", "paper_title": "AutoAugment: Learning Augmentation Strategies From Data", "reason": "AutoAugment, another data augmentation technique used in the current research, is introduced and discussed in this paper. Its application is significant for interpreting the results of the study.", "section_number": 3}]}