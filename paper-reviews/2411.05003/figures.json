[{"figure_path": "https://arxiv.org/html/2411.05003/x2.png", "caption": "Figure 1: \nGiven a user-provided source video, using ReCapture, we are able to generate a new version of the video with a new customized camera trajectory. Notice that the motion of the subject and scene in the video is preserved, and the scene is observed from angles that are not present in the source video.", "description": "ReCapture takes a user-provided video as input and generates a new video with a different camera trajectory. The generated video maintains the original video's scene motion and subject movements, but shows the scene from novel viewpoints not present in the original.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.05003/x3.png", "caption": "Figure 2: ReCapture consists, at setup time, of (a) Anchor video generation (b) Masked video fine-tuning using spatial and temporal LoRAs. To generate the clean output video with the new camera trajectory we simply perform inference of the video model.", "description": "This figure illustrates the two-stage ReCapture process. Stage (a) shows the generation of a noisy 'anchor' video using either point cloud rendering or multiview diffusion modeling.  This anchor video incorporates the desired new camera trajectory but contains artifacts and inconsistencies. Stage (b) depicts the masked video fine-tuning stage.  Here, spatial and temporal Low-Rank Adaptation (LoRA) modules are trained on the known parts of the anchor video and source video.  The spatial LoRA learns spatial context from the source video, while the temporal LoRA learns temporal consistency from the anchor video. During inference, only the fine-tuned model is used to generate a temporally consistent, clean video with the new camera path, filling in any missing information from the anchor video. The masked loss ensures that the model primarily focuses on the known areas during the fine-tuning process. The final output is a clean re-angled video.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.05003/x4.png", "caption": "Figure 3: Anchor video generation using image-level multiview-diffusion models to generate new views frame-by-frame.", "description": "This figure illustrates the process of creating an \"anchor video\" which is a noisy intermediate video that serves as the input for the next stage of the ReCapture method.  It uses a multiview image diffusion model to generate new views frame by frame. The model takes a source video frame and its corresponding camera parameters as input and produces a new view based on a novel camera trajectory.  The process is repeated for every frame to create a complete anchor video. The anchor video will have artifacts (missing information) due to the new camera viewpoints, and it's not temporally consistent; these artifacts will be corrected in a later stage.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.05003/x5.png", "caption": "Figure 4: Anchor video generation using depth estimation to turn each frame into a point cloud and then generating new views by controlling the camera pose.", "description": "This figure illustrates the first stage of the ReCapture method, specifically the point cloud approach for generating anchor videos. Depth estimation is first performed on each frame of the input video to create a 3D point cloud representation of the scene.  The user-specified camera trajectory (including zoom, pan, tilt, etc.) is then applied to these point clouds. Finally, the modified point clouds are projected back onto the image plane from the new camera viewpoints to generate the anchor video.  This process produces a noisy anchor video containing missing information, artifacts, and inconsistencies, which will be refined in the subsequent masked video fine-tuning stage.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.05003/x6.png", "caption": "Figure 5: Comparisons with generative camera dolly\u00a0[82] using an orbit camera trajectory.", "description": "Figure 5 displays a qualitative comparison between ReCapture and Generative Camera Dolly [82], a prior method, using an orbit camera trajectory. The comparison focuses on the visual quality of videos generated using both methods.  The figure shows source videos with different subjects, videos generated by Generative Camera Dolly, and videos generated by ReCapture.  The results demonstrate that ReCapture produces sharper and clearer results than Generative Camera Dolly, especially concerning motion blur, and more accurately follows the requested orbit camera trajectory.", "section": "4.2. Qualitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2411.05003/x7.png", "caption": "Figure 6: Gallery of generated videos with novel and unseen user-provided camera trajectories using ReCapture.", "description": "This figure showcases several example videos generated using the ReCapture model. Each row presents a source video alongside its corresponding ReCapture outputs under various novel camera trajectories.  These trajectories include zooming, panning, tilting, and orbiting, demonstrating ReCapture's ability to generate new video perspectives while maintaining the original scene's content and subject motion. The examples highlight ReCapture\u2019s capability to generate plausible views even from angles that were not originally captured.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.05003/x8.png", "caption": "Figure 7: Visualization of the effectiveness of masked video fine-tuning (Stage 2) for generating spatially and temporally coherent outputs from noisy anchor videos.", "description": "Figure 7 shows the effectiveness of the masked video fine-tuning stage (Stage 2) in ReCapture. The top row displays noisy anchor videos, which contain artifacts and are incomplete due to the camera movement. The bottom row shows the results after masked video fine-tuning. The masked video fine-tuning process effectively cleans and completes the noisy anchor videos. This results in a spatially and temporally coherent output video, demonstrating the effectiveness of the method in removing artifacts and ensuring consistency. ", "section": "3.3. Masked Video Fine-tuning"}]