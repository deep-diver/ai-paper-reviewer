[{"content": "| Type | Model | Single Obj. | Two Obj. | Counting | Colors | Position | Color Attri. | Overall |\n|---|---|---|---|---|---|---|---|---|\n| Diffusion | LDM [57] | 0.92 | 0.29 | 0.23 | 0.58 | 0.02 | 0.05 | 0.37 |\n|  | SD-1.5 [57] | 0.97 | 0.38 | 0.35 | 0.76 | 0.04 | 0.06 | 0.43 |\n|  | SD-2.1 [57] | 0.98 | 0.51 | 0.44 | 0.85 | 0.07 | 0.17 | 0.50 |\n|  | DALL-E 2 [56] | 0.94 | 0.66 | 0.49 | 0.77 | 0.10 | 0.19 | 0.52 |\n|  | Show-o [81] | 0.95 | 0.52 | 0.49 | 0.82 | 0.11 | 0.28 | 0.53 |\n|  | SDXL [50] | 0.98 | 0.74 | 0.39 | 0.85 | 0.15 | 0.23 | 0.55 |\n|  | DALLE 3 [6] | 0.96 | 0.87 | 0.47 | 0.83 | 0.43 | 0.45 | 0.67 |\n| Auto-regressive | LLamaGen [63] | 0.71 | 0.34 | 0.21 | 0.58 | 0.07 | 0.04 | 0.32 |\n|  | Emu3Gen [75] | 0.98 | 0.71 | 0.34 | 0.81 | 0.17 | 0.21 | 0.54 |\n|  | Chameleon [68] | - | - | - | - | - | - | 0.39 |\n|  | Ours | 0.97 | 0.69 | 0.28 | 0.71 | 0.14 | 0.15 | 0.49 (+0.10) |\n|  | Ours (+text pred.) | 0.98 | 0.73 | 0.33 | 0.85 | 0.26 | 0.28 | 0.57 |\n|  | \u0394 | +0.01 | +0.04 | +0.05 | +0.14 | +0.12 | +0.04 | +0.08 |", "caption": "Table 1: Evaluation of text-to-image generation ability on GenEval\u00a0[26] benchmark. Unifying image dense description task through next text token prediction can significantly improve the text-image alignment of images generated by Chameleon\u00a0[68].", "description": "Table 1 presents a quantitative comparison of the text-to-image generation capabilities of various models on the GenEval benchmark [26].  The table assesses performance across different aspects of image generation, including the accuracy of object depiction, color representation, and text-image alignment.  A key focus is on the impact of incorporating an image dense description task through next token prediction within an autoregressive model architecture. The results highlight the substantial performance gains achieved by this method, particularly in improving the text-image alignment of images generated by the Chameleon [68] model.", "section": "4. Experiments"}, {"content": "| Type | Methods | Depth Est. (RMSE\u2193) | Semantic Seg. (mIoU\u2191) | Surface Normal Est. (Mean Angle Error\u2193) | Lowlight Enhans. (LOL) (PSNR\u2191) | Lowlight Enhans. (LOL) (SSIM\u2191) | Deblur (GoPro) (PSNR\u2191) | Deblur (GoPro) (SSIM\u2191) | Derain (Rain100L) (PSNR\u2191) | Derain (Rain100L) (SSIM\u2191) |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Domain Specific Model | DepthAnything [83] | 0.206 |  |  |  |  |  |  |  |  |\n|  | Marigold [32] | 0.224 |  |  |  |  |  |  |  |  |\n|  | Mask DINO [36] | 60.80 |  |  |  |  |  |  |  |  |\n|  | Mask2Former [13] | 56.10 |  |  |  |  |  |  |  |  |\n|  | Bae et al. [3] |  |  |  | 14.90 |  |  |  |  |  |\n|  | InvPT [86] |  |  |  | 19.04 |  |  |  |  |  |\n|  | AirNet [35] |  |  |  | 18.18 | 0.735 | 24.35 | 0.781 | 32.98 | 0.951 |\n|  | InstructIR [15] |  |  |  | 23.00 | 0.836 | 29.40 | 0.886 | 36.84 | 0.937 |\n| Unified Model (continuous) | Painter [73] | 0.288 | 49.90 | \u00d7 | 22.40 | 0.872 | \u00d7 | \u00d7 | 29.87 | 0.882 |\n|  | InstructCV [22] | 0.297 | 47.23 | \u00d7 | \u00d7 | \u00d7 | \u00d7 | \u00d7 | \u00d7 | \u00d7 |\n|  | InstructDiffusion [25] | \u00d7 | \u00d7 | \u00d7 | \u00d7 | \u00d7 | 23.58 | - | 19.82 | 0.741 |\n|  | OmniGen [80] | 0.480 | \u00d7 | \u00d7 | 13.38 | 0.392 | 13.39 | 0.321 | 12.02 | 0.233 |\n| Unified Model (discrete) | Unified-IO [43] | 0.387 | 25.71 | - | \u00d7 | \u00d7 | \u00d7 | \u00d7 | \u00d7 | \u00d7 |\n|  | Lumina-mGPT [41] | \u00d7 | 20.87 | 22.10 | \u00d7 | \u00d7 | 17.59 | 0.536 | 16.61 | 0.365 |\n|  | Ours | 0.277 | 31.21 | 19.17 | 19.71 | 0.810 | 21.04 | 0.761 | 25.53 | 0.843 |", "caption": "Table 2: Comparison of X-Prompt with task-specific and vision generalist baselines across six representative tasks, covering both high-level visual understanding and low-level image processing. \u2019\u00d7\\times\u00d7\u2019 indicates that the method is incapable of performing the task.", "description": "Table 2 presents a comparative analysis of X-Prompt against specialized models designed for individual tasks and general-purpose vision models. The comparison focuses on six representative tasks categorized into high-level visual understanding and low-level image processing.  High-level tasks involve complex visual interpretations, while low-level tasks focus on fundamental image manipulations.  The table highlights the performance of each model on each task, indicating whether a model could successfully complete the task or not using '\u00d7' to represent failure.", "section": "4. Experiments"}, {"content": "| Type | Methods | CLIPdir \u2191 | CLIPout \u2191 | CLIPimg \u2191 | DINO \u2191 |\n|---|---|---|---|---|---| \n| Continuous | InstructPix2Pix [7] | 0.081 | 0.276 | 0.852 | 0.750 |\n|  | MagicBrush [89] | 0.106 | 0.278 | 0.933 | 0.899 |\n|  | UltraEdit [93] | 0.093 | 0.274 | 0.899 | 0.848 |\n| Discrete | Lumina-mGPT [41] | 0.025 | 0.253 | 0.810 | 0.751 |\n|  | Ours (w/o text pred.) | 0.067 | 0.263 | 0.823 | 0.785 |\n|  | Ours (w/ text pred.) | 0.083 | 0.271 | 0.857 | 0.781 |\n|  | Ours + RAIE | 0.097 | 0.279 | 0.862 | 0.792 |", "caption": "Table 3: Image Editing Results. Comparison of different methods on the MagicBrush\u00a0[89] testset.", "description": "Table 3 presents a comparative analysis of various image editing methods using the MagicBrush [89] benchmark dataset. It showcases the performance of different models in terms of image editing quality, using metrics like CLIP direction and CLIP output, and CLIP image scores, alongside DINO scores. This allows for a quantitative assessment of how effectively each method executes the image editing tasks defined in the benchmark.", "section": "4.3. Image Editing with RAIE"}, {"content": "| Settings | Low Light Enhancement |  | Derain |  | Object Addition |  | Object Removal |  | Depth Estimation | \n|---|---|---|---|---|---|---|---|---|---| \n|  | LOL |  | Rain100H |  | InstructP2P [7] |  |  |  | NYU-v2 | \n|  | PSNR\u2191 | SSIM\u2191 | PSNR\u2191 | SSIM\u2191 | CLIP<sub>dir</sub>\u2191 | CLIP<sub>out</sub>\u2191 | CLIP<sub>dir</sub>\u2191 | CLIP<sub>out</sub>\u2191 | RMSE\u2193 | \n| OmniGen [80] (in-context) | 8.923 | 0.243 | 13.14 | 0.411 | 0.054 | 0.243 | 0.031 | 0.233 | \u00d7 | \n| Full training | <span style=\"color:#A6A6A6;\">19.71</span> | <span style=\"color:#A6A6A6;\">0.770</span> | <span style=\"color:#A6A6A6;\">21.55</span> | <span style=\"color:#A6A6A6;\">0.633</span> | <span style=\"color:#A6A6A6;\">0.112</span> | <span style=\"color:#A6A6A6;\">0.283</span> | <span style=\"color:#A6A6A6;\">0.103</span> | <span style=\"color:#A6A6A6;\">0.265</span> | <span style=\"color:#A6A6A6;\">0.279</span> | \n| No In-context | 9.140 | 0.253 | 7.924 | 0.212 | -0.031 | 0.252 | 0.023 | 0.244 | 0.745 | \n| In-context w/o <span style=\"color:#723A6B; font-weight:bold;\">X-Prompt</span> | 17.00 | 0.633 | 18.10 | 0.509 | 0.092 | 0.262 | 0.069 | 0.246 | 0.390 | \n| In-context w/ <span style=\"color:#723A6B; font-weight:bold;\">X-Prompt</span> | 17.22 | 0.653 | 18.91 | 0.512 | 0.092 | 0.274 | 0.073 | 0.251 | 0.352 | ", "caption": "Table 4: Results of in-context learning in novel task settings. \u201cFull training\u201d denotes for model trained with corresponding training set. While the other settings evaluate performance on tasks not encountered during training.", "description": "Table 4 presents the results of an experiment on in-context learning using a model trained on a variety of image generation tasks. The table compares the model's performance on novel tasks (tasks not seen during training) under different conditions: with full training data, with in-context examples (but without full training), and with in-context examples using the proposed X-Prompt method.  This allows for evaluating the effectiveness of the in-context learning approach and the proposed X-Prompt enhancement on unseen tasks.", "section": "4. Experiments"}, {"content": "| Model | Resolution | PSNR | SSIM |\n|---|---|---|---| \n| SDXL-VAE (16x) | 512 | 27.51 | 0.810 |\n| SDXL-VAE (16x) | 1024 | 32.13 | 0.922 |\n| Chemeleon-VQVAE (16x) | 512 | 26.34 | 0.805 |\n| Chemeleon-VQVAE (16x) | 1024 | 29.77 | 0.906 |\n| Emu3-VQVAE (8x) | 512 | 27.78 | 0.833 |", "caption": "Table 5: Reconstruction quality tested on Rain-100L. Increasing resolution can greatly enhance reconstruction quality.", "description": "Table 5 presents a comparison of image reconstruction quality using different models on the Rain-100L dataset.  The table shows that increasing the input image resolution from 512x512 to 1024x1024 significantly improves the reconstruction quality, as measured by PSNR and SSIM scores.  This improvement is attributed to the fact that higher resolutions provide more detail for the model to reconstruct, reducing information loss during compression.  The table includes results from three autoregressive models and one diffusion model (SDXL-VAE).", "section": "4. Experiments"}, {"content": "|           | DFWB | GoPro | Rain13k | mit5k | LoL | Laion_Aesthetic | Ultra-Edit | MagicBrush | NYU-v2-depth | ADE20K | ScannNet-Norm | dep/seg/norm/hed/mlsd2img |\n| :--------- | :---- | :----- | :------ | :---- | :-- | :--------------- | :---------- | :--------- | :------------ | :----- | :------------- | :----------------------- |\n| Ori_data   | 72K   | 17K    | 13K     | 5K    | 6K  | 500K             | 500K        | 1.7K       | 48K           | 20K    | 260K           | 100K \u00d7 5                    |\n| Augmentation | 288K  | 68K    | 52K     | 20K   | 24K | 1000K            | 2000K       | 6.8K       | 192K          | 80K    | 1040K          | 100K \u00d7 20                   |", "caption": "Table 6: Detailed statistics of training data with augmentation. For each pair, we use reverse task and difference description task to augment the data.", "description": "Table 6 presents a detailed breakdown of the training data used in the study, highlighting the impact of data augmentation techniques.  It shows the original dataset sizes for various tasks (e.g., GoPro, Rain13K, etc.), then indicates how these were significantly expanded through data augmentation strategies. These strategies involved creating \"reverse tasks\" (e.g., if the original task was removing rain, the reverse task would be adding rain) and generating descriptive text explaining the differences between input and output images for each task. The table provides a clear picture of the substantial increase in the training dataset size after incorporating these augmentation methods.", "section": "4. Experiments"}, {"content": "|           | RB-Modulation | IP-Adapter | Viton-Try-On | Pose&Action | MimicBrush |\n|------------|-----------------|-------------|---------------|-------------|------------|\n| Ori_data   | 10K             | 50K         | 120K          | 10K         | 50K        |", "caption": "Table 7: Detailed statistics of training data without augmentation.", "description": "Table 7 presents a detailed breakdown of the dataset used for training the model, specifically focusing on data *without* augmentation. It shows the original number of data points for each task before any data augmentation techniques were applied. This helps clarify the initial size of the training data used to establish the model's foundation, separate from the effects of data augmentation.", "section": "3.2 Task Augmentation Pipeline"}, {"content": "| Rendering | Sketch | Poster |\n|---|---|---|\n| Mosaic | Cyberpunk | Line Drawing |\n| Impressionism | Cubism | Surrealism |\n| Pop Art | Minimalism | Abstract Expressionism |\n| Pointillism | Graffiti | Neo-Futurism |\n| Baroque | Melting Gold | Melting Silver |\n| Gold Material | Made by Gold | Made by Bronze |\n| Fantasy Glow Rendering | Low-Poly |  |", "caption": "Table 8: Style key words for FLUX to generate stylised images.", "description": "This table lists keywords used as input prompts for the FLUX model to generate stylized images.  These keywords represent various artistic styles, rendering techniques, and material properties to control the visual characteristics of the generated images.", "section": "3.2 Task Augmentation Pipeline"}]