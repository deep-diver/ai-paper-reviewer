[{"figure_path": "https://arxiv.org/html/2412.01824/x1.png", "caption": "Figure 1: X-Prompt can perform multi-modal generation based on in-content examples in a pure auto-regressive foundation model.", "description": "Figure 1 showcases X-Prompt's ability to generate diverse image outputs based on multi-modal in-context learning.  The figure presents several examples of image generation tasks, each with an in-context prompt including example image pairs.  X-Prompt processes these examples and the input prompt to produce a new image that reflects the instructions, showing its capacity for complex multi-modal generation and in-context learning in a purely auto-regressive framework.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.01824/x2.png", "caption": "Figure 2: Attention masking of X-Prompt for context feature compression and unified text and image next token prediction training.", "description": "This figure illustrates the attention mechanism employed in X-Prompt. It shows how the model compresses contextual information from in-context examples into a fixed-length sequence of tokens (X-Prompt Tokens).  The masking prevents direct interaction between the in-context examples (In-context Example Tokens) and the prediction target (TODO Tokens), forcing the model to rely on the compressed tokens for contextual information. This setup facilitates the unified training of text and image prediction, as well as the handling of longer in-context sequences, ultimately improving generalization to unseen tasks.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.01824/x3.png", "caption": "Figure 3: Training data pair augmentation and list of training prototype tasks and subtasks. We introduce reverse task and difference description task through next text token prediction to improve the performance and generalizibility.", "description": "This figure illustrates the data augmentation strategies and the task pipeline used in the training process.  The left side shows how training data pairs are augmented by introducing reverse tasks and difference description tasks. A reverse task reverses the input and output images of a task, while a difference description task requires the model to describe the changes between input and output images using text.  This augmentation is aimed at improving model performance and generalization. The right side of the figure details the prototype tasks and subtasks employed, which comprise image editing, controlled image generation, image composition, and low-level vision tasks.  These varied tasks are integrated to allow the model to learn a broader range of image manipulation techniques.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.01824/x4.png", "caption": "Figure 4: Qualitative Results on MagicBrush\u00a0[89] testset comparing with MagicBrush results w/ and w/o context examples.", "description": "Figure 4 presents a qualitative comparison of image editing results obtained using the X-Prompt model with and without in-context examples, evaluated on the MagicBrush [89] dataset.  It visually demonstrates the model's improved performance when provided with in-context examples, showcasing its ability to better understand and execute complex image editing tasks. The figure includes several image editing examples, each displaying the input image, the output generated by X-Prompt with context, the output of X-Prompt without context, and the ground truth. This allows for a direct visual comparison of the model's performance under different conditions.", "section": "4.3. Image Editing with RAIE"}, {"figure_path": "https://arxiv.org/html/2412.01824/x5.png", "caption": "Figure 5: Novel task in-context testing compared to OmniGen\u00a0[80]. X-Prompt can achieve novel task generalization with a given example. While OmniGen\u00a0[80] fall short in in-context learning (such as adapting to new color spectrum or preserve details when adding object to the image).", "description": "Figure 5 presents a comparison of X-Prompt and OmniGen [80] on novel, unseen tasks using in-context learning.  X-Prompt demonstrates successful generalization to these new tasks, adapting to diverse requests like changing color palettes or adding details to images.  Conversely, OmniGen shows limitations in in-context learning, struggling to handle requests requiring such adaptations.", "section": "4.4. In-Context Learning on Novel Tasks"}, {"figure_path": "https://arxiv.org/html/2412.01824/x6.png", "caption": "Figure 6: X-Prompt can support diversified context to achieve style personalization and action preservation.", "description": "This figure shows examples of X-Prompt's ability to handle diverse contexts for image generation.  The top row demonstrates style transfer, where the model applies a specific artistic style to an image based on a style example. The bottom row showcases action preservation; using an example image pair of a person performing an action, the model replicates that action with different characters, preserving the action while changing the character's appearance.", "section": "4.5. Other In-context Form"}, {"figure_path": "https://arxiv.org/html/2412.01824/x7.png", "caption": "Figure 7: Qualitative results of text-to-image generation. High-quality text-to-image generation cases with high aesthetic qualities after training on Laion-Aesthetics\u00a0[59].", "description": "Figure 7 showcases examples of high-quality images generated by the X-Prompt model. These images demonstrate the model's ability to produce visually appealing and aesthetically pleasing results, highlighting its effectiveness in text-to-image generation. The model was trained on a subset of the LAION-Aesthetics dataset, which is known for its high-quality images, contributing to the model's ability to generate similar high-quality outputs.  The figure visually supports the claim that X-Prompt excels at producing aesthetically pleasing images, which is a key aspect of its superior performance in text-to-image generation.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.01824/x8.png", "caption": "Figure 8: Qualitative results of diversed tasks, such as semantic segmentation, norm estimation, image deblur, denoise and derain.", "description": "Figure 8 showcases the model's performance on diverse, low-level vision tasks.  The top row displays semantic segmentation results where the model segments different objects within the image into distinct classes. The second row demonstrates normal estimation, predicting the surface normals at each pixel.  The next rows illustrate the model's ability to deblur, denoise, and derain images.  For each task, the figure presents a comparison between the input image, the model's output, and the ground truth.  This provides a visual assessment of the model's effectiveness in these tasks.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.01824/extracted/6038096/figure/T2I.jpg", "caption": "Figure 9: An example of conversation with GPT-4o to annotate the relationship between input images and output image produced by IP-Adapter\u00a0[87]", "description": "Figure 9 shows a conversation with GPT-4, a large language model, to describe the relationship between input and output images generated by the IP-Adapter model [87].  The IP-Adapter model combines two input images to create a new image.  GPT-4 is used to provide a detailed textual description that explains how the features (style, layout, color, texture, etc.) from each of the input images are used and combined in the final output image. This detailed description serves as training data to enhance the model's understanding of image manipulation tasks.", "section": "3.2 Task Augmentation Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.01824/x9.png", "caption": "Figure 10: Qualitative results of text-to-image generation. Compared to Janus\u00a0[77] and Emu3\u00a0[75], our model presents marked improvement in both quality and textual alignment.", "description": "Figure 10 presents a qualitative comparison of text-to-image generation results between the proposed X-Prompt model and two existing models, Janus [77] and Emu3 [75].  For several prompts, the generated images from each model are displayed side-by-side. This allows for a visual assessment of the image quality and how well the generated images match the textual description in each prompt. The caption highlights that X-Prompt demonstrates superior performance in both the quality of the generated images and the alignment between the image content and the corresponding text prompt.", "section": "4. Experiments"}]