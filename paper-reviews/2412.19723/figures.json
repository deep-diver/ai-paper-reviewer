[{"figure_path": "https://arxiv.org/html/2412.19723/x1.png", "caption": "Figure 1: Ideal GUI trajectory format, including High-Level Instructions, States (visual + textual representation), Low-Level Instructions, and Actions.", "description": "This figure illustrates the ideal format of a GUI agent trajectory.  It showcases the four key components: a high-level instruction that describes the overall goal of the task (e.g., \"Mark the 'Avocado Toast with Egg' recipe as a favorite in the Broccoli app.\"); the environment's state, including both visual (screenshot) and textual representations of the UI; low-level instructions that break down the task into smaller, actionable steps (e.g., \"I need to click 'Avocado Toast with Egg' to view more details and find the option to mark it as a favorite.\"); and finally, the specific actions the agent takes (e.g., CLICK [Avocado Toast with Egg] (698, 528)).  This detailed structure is crucial for training high-performing GUI agents.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.19723/x2.png", "caption": "Figure 2: An overview of how we generate instruction data without relying on predefined tasks or human annotations.\nOS-Genesis\u00a0begins with a model-free, interaction-driven traversal in online environments (e.g., a web browser).\nThis process produces massive triples consisting of actions and their corresponding pre- and post-interaction screenshots.\nReverse task synthesis leverages these triples to generate low-level instructions and associates them with broader objectives to construct high-level instructions.", "description": "This figure illustrates the OS-Genesis data generation process.  It starts with an agent freely exploring a GUI environment (like a web browser) without any predefined tasks or human input. This exploration generates a large dataset of triples: an action performed by the agent, the screen's appearance before the action, and the screen's appearance after the action.  A technique called 'reverse task synthesis' then analyzes these triples. It uses them to create low-level instructions describing individual actions and combines those low-level instructions into high-level instructions that represent broader tasks.", "section": "3 OS-Genesis"}, {"figure_path": "https://arxiv.org/html/2412.19723/x3.png", "caption": "Figure 3: An overview of collecting complete trajectories through exploring high-level instructions generated by reverse task synthesis. Low-level instructions and the last three states of the trajectory (indicated in  light blue) are used by the Trajectory Reward Model (TRM) to assign reward scores.", "description": "This figure illustrates the OS-Genesis pipeline for generating GUI agent trajectories.  It begins with high-level instructions, which are then broken down into a series of low-level instructions. The agent interacts with the GUI environment according to these instructions, generating a trajectory.  The trajectory is a sequence of actions and corresponding screenshots, showing the state of the GUI at each step.  The final three states (shown in light blue) are fed, along with the low-level instructions, into a Trajectory Reward Model (TRM). The TRM assigns a reward score to the entire trajectory based on the quality and completeness of the task completion.  This score is then used to guide future trajectory generation, improving the data quality and diversity.", "section": "3 OS-Genesis"}, {"figure_path": "https://arxiv.org/html/2412.19723/x4.png", "caption": "Figure 4: Comparison of instruction diversity and trajectory diversity between different synthetic data and human data, measured by average cosine distance.", "description": "This figure compares the diversity of instructions and trajectories generated by different methods (OS-Genesis, task-driven, self-instruction) against human-generated data.  Diversity is measured using the average cosine distance of embeddings, showing how semantically different the instructions and actions are within each dataset.  The graph helps to illustrate whether OS-Genesis produces more diverse and varied trajectories compared to task-driven baselines and how these compare to human data.", "section": "5 Analysis"}, {"figure_path": "https://arxiv.org/html/2412.19723/x5.png", "caption": "Figure 5: Comparison of different reward modeling strategies.", "description": "This figure compares the performance of three different reward modeling strategies: using a trajectory reward model (TRM), using a labeler to filter trajectories, and training without any reward model. The results show that TRM significantly improves performance on high-level tasks, while the labeler-based approach leads to some gains in high-level tasks but at the cost of performance in low-level tasks. Training without a reward model provides consistent improvement across both low and high-level tasks, indicating the value of using any reward model.  The y-axis represents the success rate (SR) and action type accuracy (Type) for each task. The x-axis represents different reward modeling strategies.", "section": "5.2 How TRM Impacts Performance?"}, {"figure_path": "https://arxiv.org/html/2412.19723/x6.png", "caption": "Figure 6: Performance of GUI agents trained on datasets of varying scales.", "description": "This figure shows the performance of GUI agents trained on datasets with varying sizes.  The x-axis represents the number of trajectories used for training, while the y-axis represents the success rate of the GUI agents on a specific task (likely the AndroidWorld benchmark).  Multiple lines are shown, each representing a different model architecture, demonstrating how different models perform with increasing amounts of training data. The graph helps illustrate the relationship between the amount of training data and the accuracy of the trained agents.", "section": "5.3 How Scaling Trajectory Data Improves Agentic Ability?"}, {"figure_path": "https://arxiv.org/html/2412.19723/x7.png", "caption": "(a) InternVL2-8B", "description": "This figure shows a comparison of the training effectiveness between trajectories generated using OS-Genesis and those from human-annotated data.  The results are broken down by high-level and low-level tasks, revealing how each method performs in comparison. The graph visually displays the success rates of agents trained with different data sources across different task complexities.", "section": "5 Analysis"}, {"figure_path": "https://arxiv.org/html/2412.19723/x8.png", "caption": "(b) Qwen2-VL-7B-Instruct", "description": "This figure shows the performance of the Qwen2-VL-7B-Instruct model on the AndroidControl benchmark.  The graph displays the success rate (SR) and action type accuracy for both high-level and low-level tasks.  Success rate measures the percentage of tasks completed successfully, while action type accuracy reflects how precisely the agent's actions match the ground truth. The results help evaluate the model's ability to perform both high-level planning and low-level execution in a challenging GUI interaction environment.", "section": "4.3 Main Results"}, {"figure_path": "https://arxiv.org/html/2412.19723/x9.png", "caption": "Figure 7: Comparison of training effectiveness between trajectories constructed from human-written and OS-Genesis high-level instructions.", "description": "This figure compares the performance of models trained using high-level instructions generated in two different ways: human-written instructions and OS-Genesis generated instructions.  The x-axis represents the type of task (High-Level or Low-Level). The y-axis shows the success rate of the tasks. The bars represent the performance of models trained on data generated with human-written high-level instructions and OS-Genesis-generated high-level instructions for InternVL2-8B and Qwen2-VL-7B models. The goal is to demonstrate that OS-Genesis's method of generating instructions produces data leading to comparable or better model performance than using human-written instructions.", "section": "5 Analysis"}, {"figure_path": "https://arxiv.org/html/2412.19723/x10.png", "caption": "(a) InternVL2-8B", "description": "This figure (Figure 7) shows a comparison of the training effectiveness between trajectories constructed from human-written instructions and OS-Genesis high-level instructions.  The chart displays the success rate (SR) for both high-level and low-level tasks for models trained with each type of data.  InternVL2-8B model performance is shown for both instructions types, illustrating the impact of training data quality on model accuracy in various task settings.", "section": "5 Analysis"}, {"figure_path": "https://arxiv.org/html/2412.19723/x11.png", "caption": "(b) Qwen2-VL-7B-Instruct", "description": "This figure shows the results of the AndroidWorld benchmark for the Qwen2-VL-7B-Instruct model.  The benchmark measures the success rate (SR) and action type accuracy of GUI agents performing tasks. The figure likely displays a bar chart or similar visualization comparing the performance of Qwen2-VL-7B-Instruct to other models or baselines (such as Zero-Shot, Task-Driven, Self-Instruction, or other versions of the Qwen2 model) on the AndroidWorld tasks, potentially showing significant improvement from the Qwen2 model trained with the OS-Genesis data synthesis method. The y-axis would represent performance metrics (SR and/or Type accuracy), and the x-axis lists different models/methods.", "section": "4.3 Main Results"}, {"figure_path": "https://arxiv.org/html/2412.19723/x12.png", "caption": "Figure 8: Comparison of training effectiveness between OS-Genesis trajectories and human-annotated trajectories.", "description": "This figure compares the effectiveness of training GUI agents using trajectories generated by OS-Genesis versus human-annotated trajectories.  It shows that models trained with OS-Genesis data achieve significantly higher performance in both high-level and low-level tasks compared to models trained with human-annotated data.  The improved performance highlights OS-Genesis's ability to produce high-quality, diverse training data that is more effectively utilized by the model.", "section": "5 Analysis"}, {"figure_path": "https://arxiv.org/html/2412.19723/x13.png", "caption": "Figure 9: Visualization of the instruction embeddings across various synthetic datasets.", "description": "This figure visualizes the embeddings of instructions generated by different methods for synthesizing GUI agent trajectories.  It uses t-SNE to reduce the dimensionality of the instruction embeddings to two dimensions, allowing for visualization. Each point represents an instruction, and the proximity of points suggests semantic similarity.  The figure aims to show the diversity of instructions generated by each method (Task-Driven, Self-Instruct, OS-Genesis).  The clustering patterns show the degree to which each method generates similar or diverse instructions.  This helps demonstrate the impact of different methods on the quality and diversity of training data.", "section": "5 Analysis"}, {"figure_path": "https://arxiv.org/html/2412.19723/extracted/6096018/ARR-Dec/appendices/figures/Screenshot_1731336368.png", "caption": "(a) Contacts", "description": "This figure shows a screenshot of the Contacts app's initial screen on a mobile device. The screenshot displays various features of the Contacts app interface, including options for viewing contacts, accessing voicemail, and managing favorites.  The image is used to illustrate the diversity of the GUI environments explored by the OS-Genesis system during its interaction-driven functional discovery phase. It helps contextualize the reverse task synthesis process by showing a typical starting point for agent exploration.", "section": "3 OS-Genesis"}, {"figure_path": "https://arxiv.org/html/2412.19723/extracted/6096018/ARR-Dec/appendices/figures/Screenshot_1731336457.png", "caption": "(b) Files", "description": "The figure shows a screenshot of the \"Files\" app on a mobile device.  It displays a list of files and folders, categorized by type (Images, Audio, Videos, Documents).  The interface includes a search bar, options to access Favorites and Recents, and a count of files in the Downloads folder. The visual emphasizes a minimalist design with a focus on file organization.", "section": "3 OS-Genesis"}, {"figure_path": "https://arxiv.org/html/2412.19723/extracted/6096018/ARR-Dec/appendices/figures/Screenshot_1731336435.png", "caption": "(c) Markor", "description": "This image shows a screenshot of the \"Markor\" mobile application's interface. The screenshot displays a simple, clean interface with a search bar at the top and several options underneath, including \"Files\", \"To-Do\", \"QuickNote\", and \"More\". This suggests a basic note-taking or text-editing application.", "section": "3 OS-Genesis"}, {"figure_path": "https://arxiv.org/html/2412.19723/extracted/6096018/ARR-Dec/appendices/figures/web_init_1.png", "caption": "Figure 10: Examples of initial screens employed in building task-driven baselines for mobile tasks.", "description": "This figure displays example screenshots of initial screens from three different mobile applications used to establish task-driven baselines.  These starting points are used to instruct agents in completing tasks within the applications, representing the starting point for the task-driven approach where pre-defined tasks direct the agent's behavior. The image shows three examples of what an agent starts with: A screen showing a list of contacts, one showing a list of files, and a simpler note-taking app.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.19723/extracted/6096018/ARR-Dec/appendices/figures/web_init_2.png", "caption": "(a) CMS", "description": "This figure shows an example of an initial screen from the CMS (Content Management System) used in building task-driven baselines for web tasks in the OS-Genesis experiment. The image displays a portion of a CMS interface, illustrating the kind of visual input used to generate high-level instructions and explore the environment.", "section": "3 OS-Genesis"}, {"figure_path": "https://arxiv.org/html/2412.19723/extracted/6096018/ARR-Dec/appendices/figures/web_init_3.png", "caption": "(b) GitLab", "description": "This image shows a screenshot of the GitLab web interface, used as an example of initial screens employed in building task-driven baselines for web tasks in the OS-Genesis paper.  The screenshot displays a typical GitLab project view, likely showing project information, activity, and possibly code repositories.", "section": "3 OS-Genesis"}]