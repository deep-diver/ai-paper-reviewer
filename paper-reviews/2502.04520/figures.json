[{"figure_path": "https://arxiv.org/html/2502.04520/x1.png", "caption": "Figure 1: \nDemonstration of our main discoveries. 1) We can fit a linear transformation between the output of source and target knowledge prompts, which is resilient against fine-tuning. 2) Updating the source knowledge will generalize to the target one via resilient linearity, causing compositional generalization/hallucination.", "description": "This figure demonstrates two key findings about how language models (LMs) handle knowledge composition.  First, it shows that a linear transformation can accurately map the prediction logits (the LM's internal probabilities for the next word) from one type of knowledge prompt to another. For example, logits predicting the country someone lives in can be approximated linearly from the logits predicting the city they live in.  Importantly, this linear relationship remains even after substantial further training of the LM (fine-tuning).  Second, the figure illustrates how updating knowledge in one domain (e.g., correcting the LM's knowledge about a city) propagates through this linear transformation to other related domains (e.g., updating knowledge about the country).  This propagation can lead to either accurate generalizations (correctly updated information) or hallucinations (incorrectly inferred information) depending on whether the linear transformation accurately captures the underlying relationship between the knowledge domains.", "section": "3. Discovering Linear Correlation"}, {"figure_path": "https://arxiv.org/html/2502.04520/", "caption": "Figure 2: \nOur hypothesis and questions about how LMs compose knowledge by learning (W,b)\ud835\udc4a\ud835\udc4f(W,b)( italic_W , italic_b ).", "description": "This figure illustrates the hypothesis that Language Models (LMs) learn a linear transformation (W, b) to compose knowledge.  The core idea is that the model's output logits for related knowledge prompts (e.g.,  'X lives in the city of' and 'X lives in the country of') are linearly correlated. The figure poses four key questions. 1) Can a linear transformation (W, b) be fit between the logits of these related prompts? 2) Does this linear relationship hold for arbitrary inputs X, not just those that are explicitly paired in the training data? 3) Does fine-tuning the LM significantly alter the learned linear transformation (W, b)? 4) Which LM parameters contribute to the formation of this linear transformation?", "section": "3. Discovering Linear Correlation"}, {"figure_path": "https://arxiv.org/html/2502.04520/x3.png", "caption": "Figure 3: The linear correlation between NTP logits of llama-3-8b.", "description": "This figure displays a heatmap visualizing the linear correlation between the next-token prediction (NTP) logits from the LLaMA-3-8B language model.  The heatmap shows the correlation between pairs of related knowledge prompts.  Warmer colors (red) represent stronger positive correlations, while cooler colors (blue) indicate weaker or negative correlations. The figure helps illustrate the concept of resilient linearity in the model's knowledge composition, revealing relationships between different knowledge subdomains.", "section": "3. Experiment Results"}, {"figure_path": "https://arxiv.org/html/2502.04520/x4.png", "caption": "Figure 4: The scaling-up of the precision of W\ud835\udc4aWitalic_W with model size.", "description": "This figure shows how the precision of the linear transformation matrix W increases with the size of the language model.  The precision is measured by the Hit@Top-N metric (where N=1 and 5 are shown), which indicates how frequently the model correctly identifies the most influential (or influenced) token pairs when composing knowledge. The figure uses the City-Country and CEO-Company knowledge pairs as examples, illustrating that larger models achieve higher precision in inferring relationships, particularly for the City-Country task.", "section": "3.4 Experiment Results"}, {"figure_path": "https://arxiv.org/html/2502.04520/x5.png", "caption": "Figure 5: The effect of W\ud835\udc4aWitalic_W weights on generalization.", "description": "This figure illustrates the relationship between the weights of the linear transformation matrix W and the success of compositional generalization.  The x-axis represents the weight assigned to a specific (City, Country) pair in matrix W. The y-axis represents the proportion of times this pair correctly generalizes (successful generalization) or results in a hallucination (hallucination).  The bars show that high weights generally lead to successful generalization, while low weights are more likely to result in hallucinations.  This indicates that the precision of W is critical for successful compositional generalization.", "section": "3.5 W can Reflect Real-world Knowledge"}, {"figure_path": "https://arxiv.org/html/2502.04520/x6.png", "caption": "Figure 6: We replace the deep intermediate layers of LMs with an initialized shallow bag-of-word network.", "description": "This figure illustrates an experiment to investigate the origin of linear correlations in Language Models (LMs). The researchers simplified the LM architecture by replacing its complex internal layers (e.g., self-attention, positional embeddings, etc.) with a simpler structure consisting only of a mean-pooling layer and a single feedforward network.  This simplified model was trained on a small set of paired texts exhibiting the knowledge composition relations studied in the paper. The goal was to determine whether the simplified model could still learn and generalize the compositional knowledge relationships observed in larger, more complex LMs, thereby helping to isolate the factors responsible for the linear correlations found in those LMs. The simplified structure helps to show that the linear correlation is not caused by the complex inner workings of the transformer models.", "section": "6. What Causes the Correlation?"}, {"figure_path": "https://arxiv.org/html/2502.04520/x7.png", "caption": "Figure 7: The linear correlation between NTP logits of llama-3-8b in math operations.", "description": "This heatmap visualizes the linear correlation between next-token prediction (NTP) logits in the LLaMA-3-8B language model when performing mathematical operations.  Each cell represents the correlation between logits from two different prompts involving math problems. The intensity of color corresponds to the strength of the correlation; darker red signifies stronger positive correlation, while darker blue represents stronger negative correlation.  The diagonal represents perfect correlation (1.0). This figure helps to illustrate the extent to which the model uses linear relationships in solving mathematical problems and generating sequential tokens.", "section": "3. Experiment Results"}, {"figure_path": "https://arxiv.org/html/2502.04520/x8.png", "caption": "Figure 8: The linear correlation between NTP logits of llama-3-8b before and after large-scale post-training.", "description": "This figure visualizes the correlation matrices of next token prediction (NTP) logits from the LLaMA-3-8B language model.  It compares the correlations before and after large-scale post-training.  The color intensity in the matrix represents the strength of the correlation between logits; warmer colors indicate stronger correlations, while cooler colors represent weaker correlations.  Comparing the two matrices allows for an assessment of how the model's understanding of relationships between concepts changes after extensive fine-tuning, highlighting the resilience (or lack thereof) of the learned correlations to further training.", "section": "3. Experiment Results"}, {"figure_path": "https://arxiv.org/html/2502.04520/x9.png", "caption": "Figure 9: The linear correlation between NTP logits in math operations before and after large-scale post-training.", "description": "This figure visualizes the correlation matrices of next token prediction (NTP) logits for mathematical operations before and after large-scale post-training.  It shows the correlation coefficients between the logits generated by the language model for different mathematical expressions, both before any post-training fine-tuning and after a large-scale fine-tuning process. This allows for assessment of how robust the linear relationships between these logits are to significant model adjustments, and how the model's understanding of mathematical composition changes.", "section": "3.4 Experiment Results"}, {"figure_path": "https://arxiv.org/html/2502.04520/x10.png", "caption": "Figure 10: The instance-wise correlation between NTP logits of llama3-8b (attribute\nas an example).", "description": "This figure displays the instance-wise correlation matrix for the next token prediction (NTP) logits of the LLaMA3-8b model, focusing on the 'attribute' knowledge family as an example.  The matrix visualizes the Pearson correlation coefficients between the logits of different input-output word pairs within the attribute family.  Higher correlation (redder colors) indicates a stronger linear relationship between the corresponding logits, suggesting that the model is more likely to generalize knowledge composition for those pairs.  Conversely, lower correlation (bluer colors) indicates a weaker linear relationship and a higher likelihood of compositional generalization failures or hallucinations.", "section": "3. Discovering Linear Correlation"}, {"figure_path": "https://arxiv.org/html/2502.04520/x11.png", "caption": "Figure 11: The attribute correlation between NTP logits of gpt2-medium.", "description": "This figure displays a heatmap visualizing the correlation between next-token prediction (NTP) logits in the GPT-2-medium language model.  The heatmap shows the correlation strength between different attributes, such as the correlation between 'city' and 'country' or 'job' and 'company'. Warmer colors (red) indicate a stronger positive correlation, while cooler colors (blue) indicate a weaker or negative correlation. The diagonal line represents perfect correlation (1.0) between identical attributes. This visualization helps understand how the model associates different attributes in its internal representation of knowledge.", "section": "3. Discovering Linear Correlation"}, {"figure_path": "https://arxiv.org/html/2502.04520/x12.png", "caption": "Figure 12: The attribute correlation between NTP logits of llama-3.2-1b.", "description": "This figure shows a heatmap representing the correlation between the next-token prediction (NTP) logits of the LLaMA-3-1B language model for different attribute pairs.  Each cell's color intensity indicates the strength of the linear correlation between the logits of two attributes.  Darker red indicates a strong positive correlation, while dark blue indicates a strong negative correlation. The diagonal shows perfect correlation, and the off-diagonal elements reveal the relationships between various attributes (e.g., city, country, job, personality, etc.). The figure helps visualize how the model associates and generalizes knowledge between related attributes.", "section": "3. Experiment Results"}, {"figure_path": "https://arxiv.org/html/2502.04520/x13.png", "caption": "Figure 13: The attribute correlation between NTP logits of llama-3.2-3b.", "description": "This heatmap visualizes the correlation between next-token prediction (NTP) logits from different attribute-related prompts in the LLaMA-3-3B language model. Each cell's color intensity represents the correlation coefficient between the logits of two prompts, indicating the strength of their semantic relationship. Warmer colors suggest a stronger positive correlation, while cooler colors represent a negative or weaker correlation. The diagonal shows perfect correlation (1.0) as it compares a prompt's logits to itself. This figure helps understand how the model relates different attributes in its knowledge representation.", "section": "3. Experiment Results"}, {"figure_path": "https://arxiv.org/html/2502.04520/x14.png", "caption": "Figure 14: The attribute correlation between NTP logits of llama-3-8b.", "description": "This heatmap visualizes the correlation between the next-token prediction (NTP) logits of the LLaMA-3-8B language model for different attribute pairs.  Each cell's color intensity represents the correlation coefficient between the logits of two related attributes, indicating how strongly the model associates them.  Darker red indicates a strong positive correlation, while dark blue represents a strong negative correlation.  The attributes considered are likely related to various semantic fields like geography, occupation, family, etc. The figure aids in understanding the model's implicit knowledge structures by highlighting strongly correlated attributes.", "section": "3. Experiment Results"}, {"figure_path": "https://arxiv.org/html/2502.04520/x15.png", "caption": "Figure 15: The attribute correlation between NTP logits of llama-3-70b.", "description": "This heatmap visualizes the correlation between the next-token prediction logits of the LLaMA-3-70B language model for different attribute pairs.  Each cell represents the correlation coefficient between the logits of two attributes (e.g., city and country).  Warmer colors (red) indicate a higher positive correlation, while cooler colors (blue) show a negative correlation. The diagonal line represents the perfect correlation of an attribute with itself. The figure provides insights into how the model relates different attributes in its internal knowledge representation, revealing potential linear relationships between certain attributes which might explain compositional generalization or hallucination.", "section": "3. Experiment Results"}, {"figure_path": "https://arxiv.org/html/2502.04520/x16.png", "caption": "Figure 16: The attribute correlation between NTP logits of deepseek-r1-distll-qwen-7B.", "description": "This heatmap visualizes the correlation between the next-token prediction (NTP) logits from different attribute prompts in the Deepseek-r1-distll-qwen-7B language model.  Each row and column represents a specific attribute prompt (e.g., 'X lives in the city of', 'X works for the company').  The color intensity at the intersection of a row and column indicates the strength of the correlation between the corresponding attribute pairs.  Darker red shades signify high positive correlation, while darker blue shades indicate high negative correlation. The diagonal line shows perfect correlation because an attribute is perfectly correlated with itself. This visualization helps to understand how different attributes are related and interconnected within the language model's knowledge representation.", "section": "3. Experiment Results"}, {"figure_path": "https://arxiv.org/html/2502.04520/x17.png", "caption": "Figure 17: The attribute correlation between NTP logits of mistral-7b-v0.3.", "description": "This heatmap visualizes the correlation between the next-token prediction (NTP) logits of the Mistral-7b-v0.3 language model for various attribute pairs. Each cell represents the correlation between the logits of two attributes.  Strong positive correlations (red) indicate that the model strongly associates the two attributes, suggesting a strong learned relationship in the model.  Negative correlations (blue) mean the model associates the attributes inversely.  The diagonal line shows perfect correlation between an attribute and itself. This figure helps understand the nature of knowledge composition within the model.", "section": "3. Experiment Results"}, {"figure_path": "https://arxiv.org/html/2502.04520/x18.png", "caption": "Figure 18: The linear correlation between NTP logits of llama-3.2-3b.", "description": "This heatmap visualizes the linear correlation between next-token prediction (NTP) logits from different knowledge prompts in the LLaMA-3-8B language model.  The rows and columns represent source and target knowledge prompts, respectively, and the color intensity of each cell reflects the strength of the linear correlation between the logits of the corresponding prompts.  Darker red indicates a strong positive correlation, while darker blue indicates a strong negative correlation.  The figure is used to illustrate the existence of linear correlations between related knowledge pairs and the resilience of these correlations to large-scale fine-tuning. The specific knowledge categories are indicated within the heatmap.", "section": "3. Experiment Results"}, {"figure_path": "https://arxiv.org/html/2502.04520/x19.png", "caption": "Figure 19: The linear correlation between NTP logits of llama-3.2-3b before and after large-scale post-training.", "description": "This figure visualizes the correlation matrices of next token prediction (NTP) logits from the LLaMA-3 3B parameter model.  The left panel shows the correlation before large-scale post-training, while the right panel shows the correlation after the post-training.  Comparing these two matrices reveals the resilience of linear correlations in the model's parameters to significant fine-tuning.  The color intensity in each matrix represents the strength of the correlation between different logits.  Darker reds indicate strong positive correlations, while darker blues indicate strong negative correlations.  The figure provides visual evidence that relationships between logits captured before fine-tuning are largely maintained after the model undergoes substantial post-training.", "section": "3.4. Experiment Results"}, {"figure_path": "https://arxiv.org/html/2502.04520/x20.png", "caption": "Figure 20: The correlation becomes more resilient in larger LMs.", "description": "This figure displays the correlation matrices for three different sizes of LLAMA language models (1B, 3B, and 8B parameters) before and after large-scale post-training.  It visually demonstrates how the linear correlation between next-token prediction (NTP) logits of related knowledge pairs, as established prior to fine-tuning, remains consistent even after extensive post-training. The increased robustness of the correlation in larger models suggests that the effect is more prominent and resilient in models with greater scale and capacity.", "section": "4. Resilient Correlation against Training"}, {"figure_path": "https://arxiv.org/html/2502.04520/x21.png", "caption": "Figure 21: The correlation between logits from mistral-7b-v0.3 before and after post-training.", "description": "This figure displays heatmaps visualizing the correlation between next-token prediction (NTP) logits from the Mistral-7b-v0.3 language model.  It compares the correlation matrices before and after large-scale post-training. The heatmaps allow for a visual comparison of how the relationships between different word tokens change before and after the model undergoes further training.  A strong correlation between logits suggests a stronger relationship between corresponding words, highlighting the impact of post-training on the model's understanding of word relationships.", "section": "4.2. Correlation after Large-scale Post-training"}, {"figure_path": "https://arxiv.org/html/2502.04520/x22.png", "caption": "Figure 22: The comparison between Aya and LLaMA in cross-lingual correlation.", "description": "This figure compares the cross-lingual correlation of language models between Aya and LLaMA.  Specifically, it visualizes the correlation between language pairs' output logits.  The heatmaps illustrate the strength of linear correlation discovered in the study.  Higher correlation indicates a stronger association between the logits of related concepts in different languages.  The comparison highlights how the multilingual nature of Aya impacts its ability to correlate concepts in diverse languages, particularly when compared to the primarily English-centric training of LLaMA.  Note that this comparison focuses on the consistency of correlated concepts, not the accuracy or completeness of the concepts themselves.", "section": "H. Multilingual LM"}, {"figure_path": "https://arxiv.org/html/2502.04520/x23.png", "caption": "Figure 23: The std of correlation distribution between logits.", "description": "This figure visualizes the standard deviation (std) of the label-wise correlation distribution between next token prediction (NTP) logits.  It provides insight into how consistent the linear correlations are across different knowledge pairs and addresses the concern that the correlation might primarily reflect the majority property of labels or be biased by highly correlated pairs. The heatmap shows the standard deviations, with lower values indicating more consistent correlations.", "section": "J. Low Dispersion in Label-wise Correlation"}, {"figure_path": "https://arxiv.org/html/2502.04520/x24.png", "caption": "Figure 24: The std of correlation distribution between logits before and after large-scale post-training.", "description": "This figure shows the standard deviation (std) of the label-wise Pearson correlation coefficients before and after large-scale post-training.  It visualizes the dispersion or spread of the correlation values across different pairs of next-token prediction (NTP) logits. A low standard deviation indicates that the correlations are clustered around a central value, implying a more consistent relationship between the logits, whereas a higher standard deviation indicates more variability in the strength of the linear relationships. This analysis helps assess the robustness of the linear correlation against the large-scale fine-tuning process, demonstrating its resilience in capturing consistent patterns in the data.", "section": "4.2. Correlation after Large-scale Post-training"}]