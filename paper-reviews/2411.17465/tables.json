[{"content": "| Usage | Device | Source | #Sample | #Ele. | #Cls. (len.) | Highlights |\n|---|---|---|---|---|---|---| \n| **Grounding** | Web | Self-collected | 22K | 576K | N/A | Visual-based |\n|  | Mobile | AMEX [8] | 97K | 926K | N/A | Functionality |\n|  | Desktop | OmniAct [22] | 100 | 8K | N/A | Diverse query |\n| **Navigation** | Web | GUIAct [10] | 72K | 569K | 9 (7.9) | One / Multi-step |\n|  | Mobile | GUIAct [10] | 65K | 585K | 5 (9.0) | Multi-step |\n| **Total** | Diverse |  | 256K | 2.7M |  |  |", "caption": "Table 1: \nOverview of our instruction-tuning data.\n#Sample indicates the number of the task instance (screenshot in grounding, task in navigation);\n#Ele. indicates the number of the element (i.e., bbox in grounding);\n#Cls. represents the number of action classes, and len. indicates the average trajectory length per task.", "description": "This table summarizes the datasets used for instruction tuning in the ShowUI model.  It details the source of each dataset (Web, Mobile, Desktop), the number of samples (task instances; screenshots for grounding, tasks for navigation), the number of elements (bounding boxes for grounding), the number of action classes, and the average length of action trajectories per task.  The dataset is split into grounding and navigation tasks, with different metrics relevant to each.", "section": "2.3 GUI Instructional Tuning"}, {"content": "| Method | Size | #Train | Mobile Text | Mobile Icon | Desktop Text | Desktop Icon | Web Text | Web Icon | Avg. |\n|---|---|---|---|---|---|---|---|---|---| \n| Qwen2-VL-2B [41] | 2B | \u2013 | 24.2 | 10.0 | 1.4 | 9.3 | 8.7 | 2.4 | 9.3 |\n| Fuyu [5] | 8B | \u2013 | 41.0 | 1.3 | 33.0 | 3.6 | 33.9 | 4.4 | 19.5 |\n| CogAgent [17] | 18B | 400K | 67.0 | 24.0 | 74.2 | 20.0 | 70.4 | 28.6 | 47.4 |\n| SeeClick [11] | 9.6B | 364K | 78.0 | 52.0 | 72.2 | 30.0 | 55.7 | 32.5 | 53.4 |\n| OmniParser [31] | * | \u2013 | 93.9 | 57.0 | 91.3 | 63.6 | 81.3 | 51.0 | 73.0 |\n| UGround [15] | 7B | 1.3M | 82.8 | 60.3 | 82.5 | 63.6 | 80.4 | 70.4 | 73.3 |\n| ShowUI-G | 2B | 119K | 91.6 | 69.0 | 81.8 | 59.0 | 83.0 | 65.5 | 74.9 |\n| ShowUI | 2B | 256K | 92.3 | 75.5 | 76.3 | 61.1 | 81.7 | 63.6 | 75.1 |", "caption": "Table 2: \nZero-shot grounding on Screenspot.\n* means Omniparser use GPT-4V. \u201cSize\u201d refers to model size.\nShowUI-G: trained solely on grounding data, excluding navigation data.\nShowUI, delivers strong grounding results with a lightweight model size and minimal training data.", "description": "This table presents the results of a zero-shot grounding experiment using the Screenspot benchmark dataset.  It compares the performance of ShowUI, a lightweight vision-language-action model, against other state-of-the-art models.  The comparison considers model size, training data size, and grounding accuracy across different device types (Web, Mobile, Desktop).  The asterisk (*) indicates that the Omniparser model utilized GPT-4V.  ShowUI-G represents a variant of ShowUI trained exclusively on grounding data, without using navigation data. The results show that ShowUI achieves strong performance with a small model size and relatively little training data.", "section": "3.2 Main Results"}, {"content": "| Method | FT? | General | Install | G.Apps | Single | WebShop. | Overall |\n|---|---|---|---|---|---|---|---| \n| ChatGPT-CoT [53] | \u2013 | 5.9 | 4.4 | 10.5 | 9.4 | 8.4 | 7.7 |\n| PaLM2-CoT [37] | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | 39.6 |\n| OmniParser [31] | * | 48.3 | 57.8 | 51.6 | 77.4 | 52.9 | 57.7 |\n| SeeClick [11] | \u2713 | 54.0 | 66.4 | 54.9 | 63.5 | 57.6 | 59.3 |\n| Qwen2-VL-2B [41] | \u2713 | 61.4 | 71.8 | 62.6 | 73.7 | 66.7 | 67.2 |\n| ShowUI \u2020 | \u2713 | 63.5 | 72.3 | 66.0 | 72.3 | 65.8 | 68.3 |\n| ShowUI | \u2713 | 63.9 | 72.5 | 69.7 | 77.5 | 66.6 | 70.0 |\n| ShowUI-ZS | \u2717 | 32.1 | 47.7 | 42.0 | 20.1 | 37.4 | 35.9 |", "caption": "Table 3: Performance of Mobile Navigation\u00a0[36],\nwhere gray color indicates these methods either using HTML inputs or rely on close-source GPT-4V (marked with *).\nShowUI \u2020\u2020\\dagger\u2020 denotes our variant without interleaved visual-action streaming, utilizing only action history.", "description": "Table 3 presents a comparison of different models' performance on mobile navigation tasks from the AITW benchmark [36].  The table shows the accuracy of each model in terms of element accuracy and operational F1-score,  as well as average steps to reach the goal in navigation tasks.  Models marked with an asterisk (*) use either HTML inputs or rely on the closed-source GPT-4V language model. The entry for ShowUI\u2020 indicates the performance of a variant of the ShowUI model that does not use interleaved vision-action streaming, relying solely on action history.", "section": "3.2 Main Results"}, {"content": "| Method | FT? | Cross-Task Ele.Acc | Cross-Task Op.F1 | Cross-Task Step.SR | Cross-Website Ele.Acc | Cross-Website Op.F1 | Cross-Website Step.SR | Cross-Domain Ele.Acc | Cross-Domain Op.F1 | Cross-Domain Step.SR |\n|---|---|---|---|---|---|---|---|---|---|---|\n| MindAct [12] | \u2013 | 55.1 | 75.7 | 52.0 | 42.0 | 65.2 | 38.9 | 42.1 | 66.5 | 39.6 |\n| GPT-4 [32] | \u2013 | 41.6 | 60.6 | 36.2 | 35.8 | 51.1 | 30.1 | 37.1 | 46.5 | 26.4 |\n| OmniParser [31] | * | 42.4 | 87.6 | 39.4 | 41.0 | 84.8 | 36.5 | 45.5 | 85.7 | 42.0 |\n| CogAgent [17] | \u2713 | 22.4 | 53.0 | 17.6 | 18.4 | 42.4 | 13.4 | 20.6 | 42.0 | 15.5 |\n| Qwen-VL [4] | \u2713 | 15.9 | 86.7 | 13.3 | 13.2 | 83.5 | 9.2 | 14.1 | 84.3 | 12.0 |\n| SeeClick [11] | \u2713 | 28.3 | 87.0 | 25.5 | 21.4 | 80.6 | 16.4 | 23.2 | 84.8 | 20.8 |\n| Qwen2-VL-2B [41] | \u2713 | 37.7 | 86.4 | 33.2 | 36.0 | 79.2 | 27.6 | 36.3 | 81.8 | 30.7 |\n| ShowUI \u2020 | \u2713 | 39.7 | 88.0 | 36.9 | 41.0 | 83.6 | 34.2 | 38.9 | 85.3 | 34.1 |\n| ShowUI | \u2713 | 39.9 | 88.6 | 37.2 | 41.6 | 83.5 | 35.1 | 39.4 | 86.8 | 35.2 |\n| ShowUI-ZS | \u2717 | 21.4 | 85.2 | 18.6 | 21.9 | 81.9 | 16.8 | 24.4 | 83.9 | 21.4 |", "caption": "Table 6: Effect by individual training data on Screenspot.", "description": "This table presents the results of an ablation study, evaluating the impact of individual training datasets on the Screenspot benchmark's zero-shot grounding performance.  It shows the performance gains achieved by using different combinations of training datasets (AMEX, Web, OmniAct, and GUIAct) with various data augmentation methods. The data demonstrates the effects of each data source and the benefit of balanced data sampling strategies for improved model generalization.", "section": "3.2 Main Results"}]