[{"figure_path": "https://arxiv.org/html/2412.07338/extracted/6058375/img/contextual-counterspeech.png", "caption": "Figure 1. Current AI-generated counterspeech only leverages the content of the toxic message. Here, we generate contextualized counterspeech that also leverages information about the community, the conversation, and the moderated user to craft more persuasive responses.", "description": "The figure illustrates the difference between traditional AI-generated counterspeech and the contextualized approach proposed in the paper. Traditional methods only use the toxic message itself to generate a response.  In contrast, the authors' method incorporates additional context, such as information about the online community where the interaction is occurring, the broader conversation thread, and characteristics of the user who posted the toxic message. This additional information aims to create more persuasive and effective counterspeech.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2412.07338/extracted/6058375/img/factors.png", "caption": "Figure 2. Algorithmic evaluation results for each factor. For each factor (y axis) and indicator (panels), the teal dot shows the mean value of the indicator when the factor is used in the evaluated configurations, while the sand dot indicates the mean value of the indicator when the factor is not used. Arrows specify whether larger \u2191\u2191\\uparrow\u2191 or smaller \u2193\u2193\\downarrow\u2193 scores are better.", "description": "Figure 2 presents a detailed analysis of how different factors influence the performance of the contextualized counterspeech model. For each factor (e.g., fine-tuning with a specific dataset, incorporating conversation history), the figure displays teal and sand dots representing the average performance when the factor is present or absent, respectively.  The performance is measured across multiple indicators (relevance, diversity, readability, toxicity, adaptation, personalization), showing the impact of each factor on each metric. Upward arrows indicate higher values are preferable for the given metric, while downward arrows indicate lower values are better.", "section": "4.2 Algorithmic evaluation"}, {"figure_path": "https://arxiv.org/html/2412.07338/x1.png", "caption": "Figure 3. Human evaluation results (non-contextual condition). Effect sizes and confidence intervals of the scores assigned to several configurations compared to the baseline. Statistical significance: ***: p<0.01\ud835\udc5d0.01p<0.01italic_p < 0.01.", "description": "Figure 3 presents the results of a human evaluation comparing different configurations of an AI-generated counterspeech system to a baseline model. The evaluation was performed under non-contextual conditions, meaning participants only saw the toxic message and the generated counterspeech, not additional context.  The figure displays effect sizes and confidence intervals for several key aspects of the counterspeech: relevance, adequacy, truthfulness, persuasiveness towards the toxic user, persuasiveness in steering the conversation, and artificiality.  Statistical significance (p-values) are indicated by asterisks, with *** denoting p < 0.01.", "section": "4.2 Human evaluation"}, {"figure_path": "https://arxiv.org/html/2412.07338/x2.png", "caption": "Figure 4. Human evaluation results (contextual condition). Effect sizes and confidence intervals of the scores assigned to several configurations compared to the baseline. Statistical significance: ***: p<0.01\ud835\udc5d0.01p<0.01italic_p < 0.01, **: p<0.05\ud835\udc5d0.05p<0.05italic_p < 0.05, *: p<0.1\ud835\udc5d0.1p<0.1italic_p < 0.1.", "description": "Figure 4 presents the results of a human evaluation of AI-generated counterspeech, focusing on the impact of providing contextual information.  It compares seven different configurations of the AI model, including a baseline (Ba) and variations incorporating adaptation and personalization strategies. The graph displays effect sizes and confidence intervals for six key metrics: relevance, adequacy, truthfulness, persuasiveness (towards the toxic user), persuasiveness (towards the overall conversation), and artificiality.  Statistical significance levels (***, **, *) are indicated for comparisons against the baseline, indicating the statistical strength of observed differences. The contextual condition refers to experiments where additional context about the conversation and user was provided to the AI. The figure highlights the relative effectiveness of different model configurations under contextual conditions. ", "section": "4.2 Human evaluation"}, {"figure_path": "https://arxiv.org/html/2412.07338/", "caption": "Figure 5. Differences in human evaluation results between the contextual and non-contextual conditions. Statistical significance: ***: p<0.01\ud835\udc5d0.01p<0.01italic_p < 0.01, **: p<0.05\ud835\udc5d0.05p<0.05italic_p < 0.05, *: p<0.1\ud835\udc5d0.1p<0.1italic_p < 0.1.", "description": "Figure 5 presents a comparison of human evaluation results obtained under two conditions: one where participants received contextual information alongside the toxic message and generated counterspeech (contextual condition), and another where they only received the toxic message and counterspeech (non-contextual condition).  The figure displays effect sizes and confidence intervals for several key aspects of the counterspeech, including relevance, adequacy, truthfulness, persuasiveness towards the toxic user and the overall conversation, and the perceived artificiality of the generated response.  Statistical significance levels are indicated using asterisks: *** for p<0.01, ** for p<0.05, and * for p<0.1, to show the differences between the contextual and non-contextual conditions for each configuration.", "section": "6.2 Human evaluation"}, {"figure_path": "https://arxiv.org/html/2412.07338/extracted/6058375/img/evaluation_rankings.png", "caption": "Figure 6. Aggregated rankings of the selected configurations, based on algorithmic and human evaluations.", "description": "This figure displays the aggregated rankings of seven configurations of a large language model (LLM) for generating counterspeech.  The rankings are based on two types of evaluations: algorithmic (using quantitative indicators such as relevance, diversity, readability, toxicity, adaptation, and personalization) and human (using crowd-sourced ratings on the same aspects, with and without providing the context surrounding the toxic message). The figure visually represents how the configurations performed across various aspects of counterspeech generation in both the algorithmic and human evaluations, allowing for a comparison of the two evaluation methods and highlighting any discrepancies between them. The configurations incorporate various contextual factors such as community style, conversation history, and user-specific information to generate more persuasive and tailored responses.", "section": "6 Results"}, {"figure_path": "https://arxiv.org/html/2412.07338/x4.png", "caption": "Figure 7. Human evaluation results (non-contextual condition) based on answers from those participants who reported using social media \u201cvery often\u201d. Statistical significance: ***: p<0.01\ud835\udc5d0.01p<0.01italic_p < 0.01, **: p<0.05\ud835\udc5d0.05p<0.05italic_p < 0.05.", "description": "Figure 7 presents the results of a human evaluation focusing on participants who use social media very frequently. The evaluation was conducted under non-contextual conditions, meaning participants only saw the toxic messages and the generated counterspeech responses.  The figure shows statistical comparisons of seven different configurations of a large language model (LLM) used for generating counterspeech, comparing each configuration to a baseline.  Statistical significance is indicated using asterisks: *** represents p<0.01 (highly significant), and ** represents p<0.05 (significant). The figure visualizes the effect sizes and confidence intervals for several metrics, including relevance, adequacy, truthfulness, persuasiveness towards the toxic user and the conversation, and artificiality of the generated counterspeech. ", "section": "6.2 Human evaluation"}]