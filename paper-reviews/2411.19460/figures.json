[{"figure_path": "https://arxiv.org/html/2411.19460/x1.png", "caption": "Figure 1: Memory usage comparison across sequence lengths for Mamba-2-2.7B with different checkpointing methods, demonstrating the memory-saving capability of Multi-Axis Gradient Checkpointing (MA-GC).", "description": "This figure compares the memory usage of the Mamba-2-2.7B model across various sequence lengths when using different gradient checkpointing methods.  The x-axis represents the sequence length (in number of tokens), and the y-axis shows the actual memory used in gigabytes (GB).  Different lines represent different checkpointing strategies: no checkpointing (GC off), standard gradient checkpointing (GC on), square root gradient checkpointing (Sqrt GC), and the proposed Multi-Axis Gradient Checkpointing (MA-GC).  The figure demonstrates that MA-GC significantly reduces memory consumption compared to the other methods, especially as sequence length increases. This highlights the memory efficiency improvements achieved by the novel MA-GC method.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.19460/x2.png", "caption": "Figure 2: Overview of MA-GC grid structure. Checkpoints are stored every l\ud835\udc59litalic_l layers and s\ud835\udc60sitalic_s steps. The blue, red, and green arrows indicate forward propagation, activation restoration, and gradient propagation, respectively. This grid design optimizes memory by selectively restoring activations as needed. The below table shows comparison of checkpointing usage, maximum sequence length on 80GB VRAM, and peak activation memory in BFloat16 at sequence length 16384.", "description": "Figure 2 illustrates the Multi-Axis Gradient Checkpointing (MA-GC) method.  It uses a grid structure to checkpoint activations at regular intervals along both the layer (every 'l' layers) and sequence (every 's' steps) dimensions.  The grid allows for selective restoration of activations only when needed during backpropagation, reducing memory usage. Arrows show the flow of forward propagation, activation restoration, and gradient propagation. The table compares MA-GC to other checkpointing methods, showing its advantages in terms of memory usage and maximum sequence length achievable with 80GB of VRAM.  Peak activation memory at sequence length 16384 is also compared.", "section": "3. Video-Ma\u00b2mba"}, {"figure_path": "https://arxiv.org/html/2411.19460/x3.png", "caption": "Figure 3: The overall summarization for the training stages of Video-Ma2mba.", "description": "This figure shows the three main stages of training for the Video-Ma2mba model. Stage 1 focuses on cross-modal alignment using image-text and video-text pairs to align visual and textual features. Stage 1.5 emphasizes long-video knowledge learning using the SceneWalk dataset to train the model on longer video sequences with detailed descriptions, enhancing its temporal understanding. Stage 2 involves supervised fine-tuning on a diverse video question-answering dataset to refine the model's ability to respond accurately to various queries about video content. The figure highlights the architecture of the model at each stage, demonstrating its progression through different training phases. It also indicates the input modalities (image, video, and text) and the output (caption or detailed descriptions) at each stage.", "section": "3. Video-Ma\u00b2mba"}, {"figure_path": "https://arxiv.org/html/2411.19460/x4.png", "caption": "(a) Experimental results on Video-MME", "description": "This figure presents a table showing the experimental results of Video-Ma\u00b2mba on the Video Multimodal Evaluation (Video-MME) benchmark. It compares the performance of Video-Ma\u00b2mba with different model sizes (0.7B, 1.8B, and 3.1B parameters) against other state-of-the-art models on short, medium, and long video clips.  The table likely displays metrics such as accuracy or F1-score, allowing for a comparison of Video-Ma\u00b2mba's performance across various video lengths and against competing methods.", "section": "4. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.19460/x5.png", "caption": "(b) Experimental results on LongVideoBench", "description": "This figure presents a table showing the experimental results of Video-Ma\u00b2mba and other models on the LongVideoBench benchmark. It compares performance across different video lengths (8-15s, 15-60s, 180-600s, 900-3600s) and across different model sizes. The results are presented as scores on the LongVideoBench dataset, indicating the models' ability to understand long-form video content.", "section": "4.2. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.19460/x6.png", "caption": "Figure 4: Qualitative examples on Video-MME\u00a0[13] with Video-Ma2mba-3.1B.", "description": "Figure 4 showcases qualitative examples from the Video Multimodal Evaluation benchmark (Video-MME).  It presents three example video questions, their options, the correct answer, and the prediction made by the Video-Ma2mba-3.1B model. This visualization demonstrates the model's capacity for accurate and nuanced understanding of long-form video content within Video-MME's diverse question types and contextual scenarios. Each example includes a series of video frames depicting the relevant portion of the video clip.", "section": "4.2. Experimental Results"}]