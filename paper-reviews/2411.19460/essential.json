{"importance": "This paper is crucial for researchers working with long-form videos and large language models.  It directly addresses the critical challenge of quadratic memory growth in processing long video sequences, offering a scalable solution for video understanding tasks.  The proposed MA-GC technique and the use of the Mamba-2 architecture open up exciting avenues for improved efficiency and model scalability, significantly impacting future research in this area. The findings have implications for numerous video applications requiring extensive context understanding.", "summary": "Video-Ma\u00b2mba efficiently handles long videos by using State Space Models, achieving linear scaling in memory and time, and employing a novel Multi-Axis Gradient Checkpointing (MA-GC) for significant memory reduction.", "takeaways": ["Video-Ma\u00b2mba uses State Space Models in the Mamba-2 framework to process long videos linearly, resolving the quadratic complexity of attention mechanisms.", "A novel Multi-Axis Gradient Checkpointing (MA-GC) method drastically reduces memory footprint compared to standard approaches.", "Video-Ma\u00b2mba demonstrates significant performance improvements on various long-form video understanding benchmarks."], "tldr": "Current long-form video understanding models struggle with high computational and memory costs due to the quadratic complexity of transformer-based architectures.  These models often resort to sparse sampling, losing crucial temporal information.  This results in suboptimal performance in tasks requiring a comprehensive understanding of long video content. \nThe Video-Ma\u00b2mba model tackles these issues by replacing the transformer architecture with State Space Models (SSMs) within the Mamba-2 framework.  **This allows for linear scaling in terms of memory and computation**, significantly reducing resource demands. Further enhanced by the innovative **Multi-Axis Gradient Checkpointing (MA-GC)** method, Video-Ma\u00b2mba demonstrates impressive results on various benchmark datasets, proving its capability to efficiently handle long video sequences and maintain high accuracy.", "affiliation": "Integrated Vision and Language Lab, KAIST, South Korea", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2411.19460/podcast.wav"}