[{"content": "|---|---|---|---|---|---|\n|---|---|---|---|---|---| \n| **Video-MME** |  |  |  |  |  |\n|---|---|---|---|---|---| \n| **Model** | **Size** | **Short** | **Medium** | **Long** | **Avg.** |\n|---|---|---|---|---|---| \n| GPT-4V [32] | - | 70.5 | 55.8 | 53.5 | 59.9 |\n| GPT-4o [33] | - | 80.0 | 70.3 | 65.3 | 71.9 |\n| Gemini 1.5 Pro [38] | - | 81.7 | 74.3 | 67.4 | 75.0 |\n| ST-LLM [28] | 7B | 45.7 | 36.8 | 31.3 | 37.9 |\n| VideoChat2-Mistral [21] | 7B | 48.3 | 37.0 | 33.2 | 39.5 |\n| Video-LLaVA [23] | 7B | 45.3 | 38.0 | 36.2 | 39.9 |\n| ShareGPT4Video [4] | 8B | 48.3 | 36.3 | 35.0 | 39.9 |\n| Chat-UniVi-V1.5 [18] | 7B | 45.7 | 40.3 | 35.8 | 40.6 |\n| Qwen-VL-Chat [1] | 7B | 46.9 | 38.7 | 37.8 | 41.1 |\n| SliME [50] | 8B | 53.3 | 42.7 | 39.8 | 45.3 |\n| Video-Ma<sup>2</sup>mba-0.7B | 0.7B | 37.4 | 35.0 | 26.8 | 33.1 |\n| Video-Ma<sup>2</sup>mba-1.8B | 1.8B | 49.4 | 39.2 | 31.9 | 40.3 |\n| Video-Ma<sup>2</sup>mba-3.1B | 3.1B | 57.6 | 42.7 | 35.4 | 45.2 |", "caption": "Table 1: Performance comparison across video length categories in Video-MME and LongVideoBench benchmarks.", "description": "This table presents a comparison of different models' performance on video understanding tasks, categorized by video length.  It shows the accuracy of various models (including the authors' Video-Ma\u00b2mba model in three different sizes) on short, medium, and long videos using two benchmark datasets: Video-MME and LongVideoBench.  The results highlight the ability of each model to handle videos of varying lengths, demonstrating the strengths and weaknesses of different architectures in processing long-form video content.", "section": "4.2. Experimental Results"}, {"content": "| Model | Size | 8-15s | 15-60s | 180-600s | 900-3600s | test set | val set |\n|---|---|---|---|---|---|---|---| \n| GPT-4o [33] | - | 71.6 | 76.8 | 66.7 | 61.6 | 66.7 | 66.7 |\n| Gemini 1.5 Pro [38] | - | 70.2 | 75.3 | 65.0 | 59.1 | 64.4 | 64.0 |\n| GPT-4-Turbo [31] | - | 66.4 | 71.1 | 61.7 | 54.5 | 60.7 | 59.1 |\n| VideoChat2 [21] | 7B | 38.1 | 40.5 | 33.5 | 33.6 | 35.1 | 36.0 |\n| VideoLLaVA [23] | 8B | 43.1 | 44.6 | 36.4 | 34.4 | 37.6 | 39.1 |\n| PLLaVA [45] | 7B | 45.3 | 47.3 | 38.5 | 35.2 | 39.2 | 40.2 |\n| LLaVA-1.5 [25] | 7B | 45.0 | 47.4 | 40.1 | 37.0 | 40.4 | 40.3 |\n| ShareGPT4Video [4] | 7B | 46.9 | 50.1 | 40.0 | 38.7 | 41.8 | 39.7 |\n| Video-Ma<sup>2</sup>mba-0.7B | 0.7B | 43.3 | 45.4 | 33.3 | 28.5 | 34.2 | 34.0 |\n| Video-Ma<sup>2</sup>mba-1.8B | 1.8B | 48.4 | 49.5 | 39.6 | 34.1 | 39.8 | 38.0 |\n| Video-Ma<sup>2</sup>mba-3.1B | 3.1B | 55.4 | 55.6 | 42.4 | 38.5 | 44.2 | 43.0 |", "caption": "Table 2: Benchmark results for ActivityNetQA, VideoChatGPT, and MVBench, comparing Video-Ma2mba and baselines.", "description": "Table 2 presents a performance comparison of Video-Ma2mba against various baseline models on three distinct video question answering benchmarks: ActivityNetQA, VideoChatGPT, and MVBench.  For each benchmark, the table shows the model size (in billions of parameters) and the accuracy score achieved by each model. This allows for a direct assessment of Video-Ma2mba's performance relative to other models, highlighting its capabilities in handling various video understanding tasks.", "section": "4.2. Experimental Results"}, {"content": "| Model | Size | ActNet-QA Acc. | ActNet-QA Score | VCG Acc. | MVBench Acc. |\n|---|---|---|---|---|---| \n| GPT4V [32] | - | 57.0 | - | 4.06 | 43.5 |\n| GPT-4o [33] | - | 61.9 | - | - | - |\n| Gemini 1.5 Pro [38] | - | 57.5 | - | - | - |\n| VideoLLaMA [47] | 7B | 12.4 | 1.1 | 2.16 | 34.1 |\n| Video-ChatGPT [29] | 7B | 35.2 | 2.7 | 2.42 | 32.7 |\n| MovieChat [39] | 7B | 45.7 | - | 2.67 | - |\n| Chat-UniVi [18] | 7B | 46.1 | 3.2 | 2.99 | - |\n| LLaMA-VID [22] | 7B | 47.4 | 3.3 | 2.89 | 41.3 |\n| VideoChat2-Mistral [21] | 7B | 49.1 | 3.3 | 2.98 | 62.3 |\n| ShareGPT4Video [4] | 8B | 50.8 | - | - | 51.2 |\n| VideoLLaMA2 [7] | 7B | 53.0 | 3.3 | 3.13 | 54.6 |\n| Video-Ma<sup class=\"ltx_sup\">2</sup>mba-0.7B | 0.7B | 43.8 | 3.2 | 2.69 | 41.1 |\n| Video-Ma<sup class=\"ltx_sup\">2</sup>mba-1.8B | 1.8B | 50.0 | 3.1 | 2.76 | 44.4 |\n| Video-Ma<sup class=\"ltx_sup\">2</sup>mba-3.1B | 3.1B | 51.7 | 3.4 | 3.03 | 48.3 |", "caption": "Table 3: Memory overhead (GB) for GC methods in Mamba-2-2.7B across sequence lengths (S=2n\ud835\udc46superscript2\ud835\udc5bS=2^{n}italic_S = 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT). \u201cGC off\u201d indicates no checkpointing; \u201cGC on\u201d applies checkpointing per layer; \u201cSqrt GC\u201d groups layers by L\ud835\udc3f\\sqrt{L}square-root start_ARG italic_L end_ARG; and \u201cMA-GC\u201d optimizes based on sequence length. Each cell show peak memory during activation and backpropagation (BF16 precision), excluding model weights and gradients.", "description": "This table presents a comparison of memory usage (in gigabytes) for different gradient checkpointing methods applied to the Mamba-2-2.7B model across various sequence lengths.  The sequence lengths are powers of 2 (from 2<sup>10</sup> to 2<sup>21</sup>). The methods compared are: no checkpointing (\"GC off\"), checkpointing per layer (\"GC on\"), checkpointing layers in groups of the square root of the total number of layers (\"Sqrt GC\"), and a multi-axis gradient checkpointing method optimized for sequence length (\"MA-GC\"). The memory overhead shown for each method and sequence length represents the peak memory consumption during both the forward and backward passes, using BF16 precision, excluding the model weights and gradients themselves.", "section": "3.2 Multi-Axis Gradient Checkpointing"}, {"content": "| Method | Model | Sequence Length (S=2<sup>n</sup>) | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | 21 | \n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| GC off: O(L\u00b7S) | 350M, L=48, d=1024 | 0.9 | 1.7 | 3.3 | 6.6 | 13.3 | 26.5 | 52.9 | - | - | - | - | - | - |\n|  | 1.3B, L=48, d=2048 | 1.7 | 3.3 | 6.5 | 13.1 | 26.0 | 52.1 | - | - | - | - | - | - | - |\n|  | 2.7B, L=64, d=2560 | 2.7 | 5.4 | 10.7 | 21.4 | 42.6 | - | - | - | - | - | - | - | - |\n| GC on: O(L\u00b7S) | 350M, L=48, d=1024 | 0.4 | 0.7 | 1.3 | 2.7 | 5.5 | 10.9 | 21.9 | 43.7 | - | - | - | - | - |\n|  | 1.3B, L=48, d=2048 | 0.7 | 1.4 | 2.7 | 5.5 | 10.9 | 21.8 | 43.5 | - | - | - | - | - | - |\n|  | 2.7B, L=64, d=2560 | 1.1 | 2.2 | 4.4 | 8.7 | 17.4 | 34.7 | 69.3 | - | - | - | - | - | - |\n| Sqrt GC:  O(\u221aL\u00b7S) | 350M, L=48, d=1024 | 0.2 | 0.4 | 0.8 | 1.6 | 3.1 | 6.2 | 12.3 | 24.6 | 49.3 | - | - | - | - |\n|  | 1.3B, L=48, d=2048 | 0.4 | 0.8 | 1.5 | 3.1 | 6.1 | 12.1 | 24.3 | 48.5 | - | - | - | - | - |\n|  | 2.7B, L=64, d=2560 | 0.6 | 1.1 | 2.2 | 4.4 | 8.7 | 17.3 | 34.6 | 69.3 | - | - | - | - | - |\n| MA-GC: O(S) | 350M, L=48, d=1024 | 0.3 | 0.5 | 0.6 | 1.1 | 1.6 | 2.4 | 3.8 | 5.5 | 8.8 | 15.4 | 23.1 | 40.2 | \n|  | 1.3B, L=48, d=2048 | 0.5 | 0.9 | 1.2 | 2.1 | 3.7 | 4.8 | 7.4 | 11.3 | 17.7 | 30.8 | 45.8 | - |\n|  | 2.7B, L=64, d=2560 | 0.7 | 1.1 | 1.8 | 2.7 | 4.2 | 6.9 | 10.5 | 17.2 | 25.9 | 42.2 | - | - | -", "caption": "Table 4: Ablation study on frame size and Stage 1.5 effects in Video-MME using Video-Ma2mba-3.1B.", "description": "This ablation study investigates the impact of frame size and the inclusion of Stage 1.5 (Long Video Knowledge Learning) on the performance of Video-Ma2mba-3.1B using the Video Multi-modal Evaluation (Video-MME) benchmark.  It compares different frame sampling rates (8 frames, 16 frames, and 32 frames) and the presence or absence of Stage 1.5 training.  Results show performance across short, medium, and long video lengths, providing insights into the effect of temporal context on model accuracy.", "section": "4.3. Additional Analyses on Video-Ma\u00b2mba"}, {"content": "| Tr Stage | Frame Limit | Frame Limit | Video-MME | Video-MME | Video-MME | Video-MME |\n|---|---|---|---|---|---|---|\n|  | train | infer | Short: \u22642m | Mid: 4-15m | Long: 30-60m | Overall |\n| 1/ 1.5 /2 | 16 frm | 8 frm | 49.0 | 38.7 | 33.8 | 40.5 |\n| \u2713\u00a0\u00a0\u2717\u00a0\u00a0\u2713 | 16 frm | 16 frm | **50.0** | **40.7** | **34.6** | **41.7** |\n| \u2713\u00a0\u00a0\u2717\u00a0\u00a0\u2713 | 1 fps | 8 frm | 47.7 | 37.9 | 32.2 | 39.3 |\n|  | 1 fps | 16 frm | 50.6 | 39.4 | 33.2 | 41.1 |\n|  | 1 fps | 32 frm | 52.7 | 40.8 | 33.9 | 42.4 |\n|  | 1 fps | 1 fps | **54.4** | **41.4** | **34.4** | **43.4** |\n| \u2713\u00a0\u00a0\u2713\u00a0\u00a0\u2713 | 1 fps | 8 frm | 53.3 | 39.3 | 32.2 | 41.6 |\n|  | 1 fps | 16 frm | 55.9 | 41.3 | 33.9 | 43.7 |\n|  | 1 fps | 32 frm | **57.9** | 41.9 | 33.9 | 44.6 |\n|  | 1 fps | 1 fps | 57.6 | **42.7** | **35.4** | **45.2** |", "caption": "Table 6: \nHyperparameters for Training Stages.", "description": "This table details the hyperparameters used during the three training stages of the Video-Ma\u00b2mba model.  It includes specifications for the input modalities (video and image in Stage 1, video in Stages 1.5 and 2), frame rates, input resolution, the number of trainable parameters in different model sizes, learning rates for the language model (LLM) and vision components, optimizer used (AdamW), global batch sizes for each stage, training epochs, warmup ratio, weight decay, gradient clipping, precision, deepspeed stages, and the gradient checkpointing method used.", "section": "3.5 Training Stages"}, {"content": "| config | Stage1 | Stage1.5 | Stage2 |\n|---|---|---|---|\n| input modality | Vid + Img | Video | Video |\n| FPS for video | 1 FPS | 1 FPS | 1 FPS |\n| input resolution | 336x336 | 336x336 | 336x336 |\n| trainable params | Projector | Full Model | Full Model |\n| LLM lr | 1e-3 | 4e-5 | 4e-5 |\n| Vision lr | - | 4e-6 | 4e-6 |\n| lr scheduler | Cosine Decay | Cosine Decay | Cosine Decay |\n| optimizer | AdamW (\u03b2\u2081=0.9,\u03b2\u2082=0.95) | AdamW (\u03b2\u2081=0.9,\u03b2\u2082=0.95) | AdamW (\u03b2\u2081=0.9,\u03b2\u2082=0.95) |\n| global batch size | 512 | 32 | 32 |\n| train epochs | 2 | 2 | 2 |\n| warmup ratio | 0.1 | 0.1 | 0.1 |\n| weight decay | 0.05 | 0.05 | 0.05 |\n| gradient clipping | 1.0 | 1.0 | 1.0 |\n| training precision | BFloat16 | BFloat16 | BFloat16 |\n| DeepSpeed stage | ZeRO-1 | ZeRO-1 | ZeRO-1 |\n| GC | Multi-Axis Gradient Checkpointing | Multi-Axis Gradient Checkpointing | Multi-Axis Gradient Checkpointing |", "caption": "Table 7: Model-specific constants for memory estimation under BFloat16 precision. Constants reflect relative element counts, where SSM states in Float32 are equivalent to two BFloat16 elements.", "description": "This table presents model-specific constants used in calculating memory usage for Video-Ma\u00b2mba, a model for long video understanding.  The constants are crucial for the memory optimization formula presented in the paper (Equation 12).  Each constant represents the memory consumption of different parts of the model's architecture under BFloat16 precision (except for SSM states, which use Float32 and thus count as two BFloat16 elements).  The table breaks down these constants for three different sizes of the Mamba-2 model (370M, 1.3B, and 2.7B parameters), reflecting how memory usage scales with model size.  It shows the memory requirements for layer-wise checkpoints (CL-ckpt), sequence-wise checkpoints (CS-ckpt), grid cells (Cgrid), and SSM states (Cstate).", "section": "3.3 Analysis of Upper Bound of Memory Reduction"}, {"content": "| Model | $C_{L\\text{-ckpt}}$ | $C_{S\\text{-ckpt}}$ | $C_{\\text{grid}}$ | $C_{\\text{state}}$ |\n|---|---|---|---|---|\n| Mamba-2-370m | 1,024 | 269,056 | 6,432 | 264,448 |\n| Mamba-2-1.3b | 2,048 | 537,344 | 12,608 | 528,640 |\n| Mamba-2-2.7b | 2,560 | 671,488 | 15,696 | 660,736 |", "caption": "Table 8: Computational analysis of throughput and per-token processing time among gradient checkpointing methods. Results are measured using the Mamba-2-2.7b model on an A100 80GB GPU. The notation @bold-@@bold_@ \ud835\udfd0\ud835\udc27superscript2\ud835\udc27\\mathbf{2^{n}}bold_2 start_POSTSUPERSCRIPT bold_n end_POSTSUPERSCRIPT specifies the sequence length (in tokens) used for measurement.", "description": "This table presents a computational analysis comparing the throughput (tokens processed per second) and per-token processing time (milliseconds per token) across various gradient checkpointing methods.  The experiments were conducted using the Mamba-2-2.7B model on an NVIDIA A100 GPU with 80GB of memory.  Sequence lengths are denoted using the notation 2<sup>n</sup>, where n represents the exponent, indicating the number of tokens processed. The results help to demonstrate the trade-offs between memory efficiency and processing speed for different gradient checkpointing techniques.", "section": "D. Gradient Checkpointing Time Analysis"}]