[{"figure_path": "https://arxiv.org/html/2412.17747/x1.png", "caption": "Figure 1: Overview of the proposed architecture. The input sequence is processed by a frozen LLM, generating a kv-cache. This cache is then passed to a coprocessor, along with trainable soft tokens. The coprocessor outputs latent embeddings which are used to augment the original kv-cache before being fed back into the LLM for output generation.", "description": "The figure illustrates the architecture of a system that augments a frozen large language model (LLM) with an offline coprocessor to improve its reasoning capabilities.  The input sequence is first processed by the frozen LLM, which generates a key-value (kv) cache representing the LLM's internal representation of the input. This kv-cache is then passed to a separate coprocessor module.  The coprocessor receives the kv-cache along with trainable soft tokens (which serve as prompts or additional context) and produces latent embeddings. These embeddings are added to the original kv-cache. This augmented cache is then fed back into the frozen LLM, allowing it to leverage the coprocessor's additional computations without changing the main LLM's weights.  The LLM then generates the final output, incorporating the enriched context from the coprocessor. The key advantage is that the coprocessor operates asynchronously and can augment the kv-cache to improve prediction of many tokens beyond the augmentation point.", "section": "2.2. Model architecture"}, {"figure_path": "https://arxiv.org/html/2412.17747/x2.png", "caption": "Figure 2: Our coprocessor training framework. (a) Illustration of multi-position augmentation and ahead token prediction. For each selected augmentation position, latent embeddings are generated by the coprocessor and inserted after the corresponding token\u2019s embedding. The target tokens for prediction (\"ahead tokens\") are then appended. A causal mask is applied to all sequences following these insertion points. (b) Structure of the modified input and attention mask for model training. We show an example of 1 latent embedding and 1 ahead token here for simplicity.", "description": "This figure illustrates the coprocessor training framework.  Panel (a) shows how latent embeddings are generated at multiple points within an input sequence. For each selected position, the coprocessor generates embeddings which are inserted after the corresponding token's embedding. Ahead tokens are appended after the embeddings, and a causal mask ensures that subsequent tokens only attend to preceding tokens, creating a sequential prediction problem. Panel (b) provides a simplified visual representation of how the input sequence, latent embeddings, ahead tokens, and the causal attention mask are structured for training.", "section": "2. Methodology"}, {"figure_path": "https://arxiv.org/html/2412.17747/x3.png", "caption": "Figure 3: Validation perplexity of the baseline frozen Gemma-2 2B model and augmented models with varying numbers of latents (8, 16, 32, 64), when predicting the 1st and 32nd tokens following latent augmentation. Lower perplexity indicates better performance.", "description": "This figure displays the validation perplexity results for the Gemma-2 2B model, both with and without cache augmentation.  Two scenarios are presented: predicting the first token and predicting the 32nd token after latent augmentation. Different curves represent the model's performance with varying numbers of latent embeddings (8, 16, 32, and 64). Lower perplexity values indicate better model performance.  The graph illustrates how adding latent embeddings through cache augmentation consistently improves the model's predictive accuracy, even when predicting tokens far beyond the immediate vicinity of the augmentation.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.17747/x4.png", "caption": "Figure 4: Finetuning the coprocessor from Gemma-2 2B pretrained weights significantly improves GSM8K accuracy compared to training from scratch. Lines represent the mean and shaded areas represent the 95% confidence interval, both estimated from the last 5 checkpoints.", "description": "This figure displays the impact of different training methods on the GSM8K 8-shot accuracy.  Specifically, it compares the performance of a coprocessor trained from scratch against one fine-tuned from pre-trained Gemma-2 2B weights. The x-axis represents the number of latent embeddings used, and the y-axis shows the corresponding GSM8K accuracy.  The lines illustrate the mean accuracy, while the shaded areas depict the 95% confidence intervals calculated using data from the final five checkpoints of each training run. The results clearly demonstrate that fine-tuning the coprocessor with pre-trained weights leads to significantly better performance than training from scratch, showcasing the benefits of leveraging pre-existing knowledge.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.17747/x5.png", "caption": "Figure 5: Scaling of GSM8K accuracy and validation perplexity with increasing training steps for the coprocessor (using 32 latent embeddings). The baseline performance of the frozen Gemma-2 2B model is shown for reference.", "description": "This figure displays the impact of increasing the training steps on the coprocessor's performance, specifically focusing on GSM8K accuracy and validation perplexity. The coprocessor uses 32 latent embeddings.  The x-axis represents the total number of training steps, and the y-axis shows the corresponding GSM8K accuracy and validation perplexity.  A clear trend is observed: as training steps increase (more data is seen by the coprocessor), GSM8K accuracy consistently improves and validation perplexity decreases.  The baseline performance of the frozen Gemma-2 2B model (without coprocessor augmentation) is shown for comparison, highlighting the significant improvement achieved through coprocessor training.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.17747/x6.png", "caption": "Figure 6: Accuracy on GSM8K\u2019s test set after LoRA finetuning. Our augmented model shows a significant improvement compared to the baseline.", "description": "This figure displays the results of applying LoRA finetuning to both a baseline model and an augmented model on the GSM8K test set. The augmented model incorporates the coprocessor for generating latent embeddings to enhance the context provided to the LLM. The x-axis represents the number of training steps, and the y-axis represents the accuracy achieved on the GSM8K benchmark.  The plot clearly demonstrates that the augmented model, with its coprocessor, significantly outperforms the baseline model after LoRA finetuning, highlighting the effectiveness of the proposed cache augmentation technique.", "section": "3. Experiments"}]