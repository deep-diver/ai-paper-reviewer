[{"content": "| Benchmark | Contamination |\n|---|---| \n| cais/mmlu | 1.34% |\n| openai/openai_humaneval | 0.00% |\n| openai/gsm8k | 0.08% |\n| ucinlp/drop | 0.20% |\n| lighteval/MATH | 0.06% |\n| google/IFEval | 0.00% |\n| akariasai/PopQA | 7.21% |\n| tatsu-lab/alpaca_eval | 1.37% |\n| lukaemon/bbh | 0.02% |\n| truthfulqa/truthful_qa | 1.47% |\n| allenai/wildguardmix | 0.06% |\n| allenai/wildjailbreak | 0.00% |\n| TIGER-Lab/MMLU-Pro | 0.93% |\n| Idavidrein/gpqa | 0.00% |\n| lighteval/agi_eval_en | 0.00% |\n| bigcode/bigcodebench | 0.00% |\n| deepmind/math_dataset | 0.00% |", "caption": "Table 1: Contamination of benchmarks in the SFT dataset used allenai/tulu-3-sft-mixture", "description": "This table presents the contamination levels of various benchmarks used in the Supervised Fine-tuning (SFT) dataset.  Contamination refers to the presence of test data in the training set, which can inflate evaluation metrics. Each benchmark is listed with its corresponding percentage of contamination.", "section": "3. Supervised Finetuning"}, {"content": "| Hyperparameter | SmolTulu | SmolTulu | Tulu 3 | Tulu 3 |\n|---|---|---|---|---| \n|  | **SFT-1130** | **SFT-1207** | **SFT 8b** | **SFT 70b** |\n| Learning Rate (LR) | 9.0 \u00d7 10\u207b\u2075 | 3.1 \u00d7 10\u207b\u2076 | 5.0 \u00d7 10\u207b\u2076 | 2.0 \u00d7 10\u207b\u2076 |\n| Batch Size (BS) | 8 | 32 | 128 | 128 |\n| LR/BS \u00d7 10\u2076 | 11.25 | 0.097 | 0.039 | 0.016 |", "caption": "Table 2: SFT hyperparameter selection", "description": "This table shows the selected hyperparameters for supervised finetuning (SFT), including learning rate (LR), batch size (BS), and the ratio of LR to BS, for different model sizes during the supervised finetuning stage. Effective Batch Size is calculated to match Tulu-3 and to represent the true batch size used. SmolTulu and Tulu 3 utilize different LR/BS ratios, with SmolTulu employing higher ratios, especially at smaller scales.", "section": "3 Supervised Finetuning"}, {"content": "| Metric | SmolTulu<br>SFT-1130 | SmolTulu<br>SFT-1207 | SmolLM2<br>1.7B-Instruct |\n|---|---|---|---| \n| ARC (Average) | 51.0 | **55.6** | 51.7 |\n| BBH (3-shot) | **34.7** | 34.0 | 32.2 |\n| GSM8K (5-shot) | **49.0** | 42.8 | 48.2 |\n| HellaSwag | 61.5 | **67.5** | 66.1 |\n| IFEval (Average) | **61.0** | 47.8 | 56.7 |\n| MMLU-Pro (MCF) | 17.6 | 17.9 | **19.3** |\n| PIQA | 72.7 | **76.9** | 74.4 |", "caption": "Table 3: Performance comparison of SFT models", "description": "This table presents a comparison of the performance of different Supervised Fine-Tuning (SFT) models, including two versions of SmolTulu (SFT-1130 and SFT-1207), and the SmolLM2 1.7B-Instruct model.  The models are evaluated on a variety of benchmarks including ARC, BBH, GSM8K, HellaSwag, IFEval, MMLU-Pro, and PIQA.  The table shows the scores achieved by each model on these benchmarks, allowing for a direct comparison of their performance after SFT.", "section": "Supervised Finetuning"}, {"content": "| Benchmark | Contamination |\n|---|---| \n| cais/mmlu | 0.69% |\n| openai/openai_humaneval | 0.00% |\n| openai/gsm8k | 0.00% |\n| ucinlp/drop | 0.07% |\n| lighteval/MATH | 0.02% |\n| google/IFEval | 0.00% |\n| akariasai/PopQA | 2.72% |\n| tatsu-lab/alpaca_eval | 1.24% |\n| lukaemon/bbh | 0.00% |\n| truthfulqa/truthful_qa | 0.61% |\n| allenai/wildguardmix | 0.06% |\n| allenai/wildjailbreak | 0.00% |\n| TIGER-Lab/MMLU-Pro | 0.36% |\n| Idavidrein/gpqa | 0.00% |\n| lighteval/agi_eval_en | 0.00% |\n| bigcode/bigcodebench | 0.00% |\n| deepmind/math_dataset | 0.00% |", "caption": "Table 4: Contamination of benchmarks in the DPO dataset used allenai/llama-3.1-tulu-3-8b-preference-mixture", "description": "This table presents the contamination levels of various evaluation benchmarks within the Direct Preference Optimization (DPO) dataset, which is a mixture of preference data derived from various sources.  Contamination refers to the presence of training data within the evaluation set, which can inflate performance metrics. Lower contamination percentages indicate a cleaner evaluation set. This analysis is crucial for ensuring a fair and accurate assessment of the model's performance improvements after undergoing preference optimization. The table lists the benchmark dataset name and its corresponding contamination rate.", "section": "4.1 Dataset"}, {"content": "| Hyperparameter | SmolTulu\nDPO-1130 | SmolTulu\nDPO-1207 | Tulu 3\nDPO 8b | Tulu 3\nDPO 70b |\n|---|---|---|---|---|\n| Learning Rate (LR) | 8.0e-7 | 5e-7 | 5.0e-7 | 2.0e-7 |\n| Batch Size (BS) | 12 | 32 | 128 | 128 |\n| LR/BS x 10^7 | 0.667 | 0.156 | 0.039 | 0.016 |", "caption": "Table 5: DPO hyperparameter selection", "description": "Hyperparameters used for Direct Preference Optimization (DPO) training across different model sizes, including learning rate, batch size, and the derived ratio between them.", "section": "4 Direct Preference Optimization"}, {"content": "| Metric | SmolTulu<br>DPO-1130 | SmolTulu<br>DPO-1207 | SmolLM2<br>1.7B-Instruct |\n|---|---|---|---| \n| ARC (Average) | 51.5 | **57.1** | 51.7 |\n| BBH (3-shot) | **33.8** | **34.2** | 32.2 |\n| GSM8K (5-shot) | **51.6** | 44.7 | 48.2 |\n| HellaSwag | 61.1 | 64.2 | **66.1** |\n| IFEval (Average) | **67.7** | 56.6 | 56.7 |\n| MMLU-Pro (MCF) | 17.4 | 19.1 | **19.3** |\n| PIQA | 72.2 | **76.4** | 74.4 |", "caption": "Table 6: Performance comparison of DPO models", "description": "This table compares the performance of different Direct Preference Optimization (DPO) models, including two SmolTulu variants (DPO-1130 and DPO-1207) and a baseline SmolLM2 1.7B-Instruct model, across a range of evaluation metrics (ARC, BBH, GSM8K, HellaSwag, IFEval, MMLU-Pro, and PIQA).  The table presents the scores achieved by each model on these benchmarks, allowing for direct comparison and analysis of the impact of different DPO hyperparameter settings on the performance of smaller vs. larger language models.", "section": "4 Direct Preference Optimization"}, {"content": "| Hyperparameter | SmolTulu | SmolTulu | Tulu 3 | \n|---|---|---|---| \n|  | **RM-1130** | **RM-1207** | **DPO 8b** | \n| Learning Rate (LR) | 4.0 \u00d7 10\u207b\u2075 | 7.5 \u00d7 10\u207b\u2077 | 5.0 \u00d7 10\u207b\u2077 | \n| Batch Size (BS) | 4 | 8 | 128 | \n| LR/BS \u00d7 10\u2077 | 100 | 0.938 | 0.039 |", "caption": "Table 7: Reward model hyperparameter selection", "description": "This table details the hyperparameters used for training the reward model, including learning rate and batch size. Two configurations are shown for the SmolTulu models and one for the Tulu 3 8b model, allowing for comparison. The key difference is the learning rate to batch size ratio, which is significantly higher for the smaller SmolTulu models. This highlights the exploration of different optimization strategies tailored to the model scale.", "section": "5.2 Training"}, {"content": "| Metric | SmolTulu<br>RM-1130 | SmolTulu<br>RM-1207 | Tulu 3<br>8b RM |\n|---|---|---|---|\n| RB Chat | *94.13* | 83.52 | **96.27** |\n| RB Chat Hard | 43.64 | *44.74* | **55.92** |\n| RB Safety | *75.54* | 64.59 | **84.05** |\n| RB Reasoning | *68.01* | 54.71 | **76.50** |\n| RB Average | *72.43* | 58.59 | **81.34** |\n| UFB | *73.17* | 61.66 | **77.34** |", "caption": "Table 8: Performance comparison of reward models, where UFB is the test_prefs split of allenai/ultrafeedback_binarized_cleaned and RB is RewardBench.", "description": "This table presents a comparison of the performance of different reward models (RMs). It includes two variants of SmolTulu RMs, and a Tulu 3 RM for comparison.  The table uses two key metrics: UltraFeedback (UFB) and RewardBench (RB),  RB is further categorized into RB Chat, RB Chat Hard, RB Safety, and RB Reasoning, providing a more comprehensive evaluation across different aspects of reward modeling.", "section": "5 Reward Modelling"}, {"content": "| Metric | SmolTulu<br>DPO-1130 | SmolTulu<br>DPO-1207 | SmolTulu<br>SFT-1130 | SmolTulu<br>SFT-1207 | SmolLM2<br>1.7B-Instruct | Llama-3.2<br>1B-Instruct | Qwen2.5<br>1.5B-Instruct |\n|---|---|---|---|---|---|---|---|\n| ARC (Average) | 51.5 | **57.1** | 51.0 | 55.6 | 51.7 | 41.6 | 46.2 |\n| BBH (3-shot) | 33.8 | 34.2 | 34.7 | 34.0 | 32.2 | 27.6 | **35.3** |\n| GSM8K (5-shot) | **51.6** | 44.7 | 49.0 | 42.8 | 48.2 | 26.8 | 42.8 |\n| HellaSwag | 61.1 | 64.2 | 61.5 | **67.5** | 66.1 | 56.1 | 60.9 |\n| IFEval (Average) | **67.7** | 56.6 | 61.0 | 47.8 | 56.7 | 53.5 | 47.4 |\n| MMLU-Pro (MCF) | 17.4 | 19.1 | 17.6 | 17.9 | 19.3 | 12.7 | **24.2** |\n| PIQA | 72.2 | 76.4 | 72.7 | **76.9** | 74.4 | 72.3 | 73.2 |", "caption": "Table 9: A comparison against a wider selection of models", "description": "This table presents a comprehensive comparison of the performance of different SmolTulu models against a wider selection of prominent language models, including SmolLM2, Llama 3.2, and Qwen 2.5. The evaluation spans a variety of tasks, such as ARC, BBH, GSM8K, HellaSwag, IFEval, MMLU-Pro, and PIQA, providing a holistic view of the models' capabilities across different domains.", "section": "8 Conclusion"}, {"content": "| Language | Presence (%) |\n|---|---| \n| English | 83.13 |\n| Hindi | 3.79 |\n| Swahili | 2.02 |\n| Russian | 2.00 |\n| Spanish | 1.15 |\n| Arabic | 0.98 |\n| Chinese | 0.94 |\n| Turkish | 0.87 |\n| Urdu | 0.78 |\n| Portuguese | 0.77 |\n| Vietnamese | 0.64 |\n| Japanese | 0.63 |\n| French | 0.66 |\n| Bulgarian | 0.33 |\n| Italian | 0.32 |\n| Dutch | 0.31 |\n| Polish | 0.25 |\n| German | 0.23 |\n| Thai | 0.10 |\n| Greek | 0.09 |", "caption": "Table 10: Language distribution in SFT dataset.", "description": "This table presents the distribution of different languages within the Supervised Fine-tuning (SFT) dataset used for training the SmolTulu language model. It lists various languages and their corresponding percentage presence in the dataset, providing insights into the linguistic diversity of the training data.", "section": "3 Supervised Finetuning"}, {"content": "| Language | Presence (%) |\n|---|---| \n| English | 86.24 |\n| Hindi | 2.23 |\n| Russian | 2.03 |\n| French | 1.42 |\n| Spanish | 1.40 |\n| Chinese | 1.37 |\n| Urdu | 0.68 |\n| Swahili | 0.65 |\n| German | 0.58 |\n| Japanese | 0.57 |\n| Portuguese | 0.54 |\n| Arabic | 0.51 |\n| Turkish | 0.42 |\n| Vietnamese | 0.33 |\n| Italian | 0.32 |\n| Polish | 0.22 |\n| Dutch | 0.18 |\n| Bulgarian | 0.18 |\n| Thai | 0.10 |\n| Greek | 0.04 |", "caption": "Table 11: Language distribution in DPO / RM dataset.", "description": "This table presents the language distribution within the dataset used for Direct Preference Optimization (DPO) and Reward Modeling (RM). It lists various languages and their corresponding percentage presence in the dataset.", "section": "4 Direct Preference Optimization"}, {"content": "| Language | Presence (%) |\n|---|---| \n| English | 94.80 |\n| French | 1.29 |\n| Spanish | 1.04 |\n| Chinese | 0.66 |\n| German | 0.55 |\n| Russian | 0.48 |\n| Japanese | 0.40 |\n| Hindi | 0.23 |\n| Polish | 0.10 |\n| Portuguese | 0.10 |\n| Dutch | 0.08 |\n| Urdu | 0.07 |\n| Bulgarian | 0.07 |\n| Italian | 0.05 |\n| Turkish | 0.03 |\n| Arabic | 0.03 |\n| Vietnamese | 0.02 |\n| Swahili | 0.00 |", "caption": "Table 12: Language distribution in RLVR dataset.", "description": "This table shows the language distribution of the  Reinforcement Learning with Verifiable Rewards (RLVR) dataset used for training the model. Most of the dataset consists of English text (94.8%), followed by French (1.29%), and Spanish (1.04%). Other languages are present in smaller amounts.", "section": "5 Reward Modelling"}, {"content": "| Benchmark | Contamination |\n| -------- | ----------- |\n| cais/mmlu | 0.65% |\n| openai/openai_humaneval | 0.00% |\n| openai/gsm8k | 0.00% |\n| ucinlp/drop | 0.00% |\n| lighteval/MATH | 0.24% |\n| google/IFEval | 0.00% |\n| akariasai/PopQA | 0.45% |\n| tatsu-lab/alpaca_eval | 0.12% |\n| lukaemon/bbh | 0.00% |\n| truthfulqa/truthful_qa | 0.12% |\n| allenai/wildguardmix | 0.00% |\n| allenai/wildjailbreak | 0.00% |\n| TIGER-Lab/MMLU-Pro | 0.66% |\n| Idavidrein/gpqa | 0.00% |\n| lighteval/agi_eval_en | 0.00% |\n| bigcode/bigcodebench | 0.00% |\n| deepmind/math_dataset | 0.00% |", "caption": "Table 13: Contamination of benchmarks in the RLVR dataset allenai/RLVR-GSM-MATH-IF-Mixed-Constraints", "description": "This table presents the contamination levels of various evaluation benchmarks within the RLVR dataset, specifically the `allenai/RLVR-GSM-MATH-IF-Mixed-Constraints` version. Contamination refers to the presence of test data within the training set, which can inflate evaluation metrics and provide an unrealistic assessment of model performance. By quantifying the contamination rate for each benchmark, this table offers insights into the reliability and trustworthiness of the evaluation results obtained using this dataset.", "section": "5 Reward Modelling"}]