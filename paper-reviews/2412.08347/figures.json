[{"figure_path": "https://arxiv.org/html/2412.08347/extracted/6061800/assets/arc_contour.png", "caption": "(a) Effect of learning rate and batch size on ARC score.", "description": "This contour plot visually represents how different learning rates and batch sizes impact the performance of a 135 million parameter language model on the ARC Challenge, a reasoning task.  The x-axis represents the learning rate, and the y-axis represents the effective batch size. The darker areas indicate higher ARC scores, meaning better performance. The contours show how the performance changes with different combinations of learning rate and batch size, revealing the optimal ratio between these hyperparameters for this specific task and model size.", "section": "3 Supervised Finetuning"}, {"figure_path": "https://arxiv.org/html/2412.08347/extracted/6061800/assets/gsm8k_contour.png", "caption": "(b) Effect of learning rate and batch size on GSM8K score.", "description": "This contour plot visualizes the performance of a 135M parameter language model on the GSM8K benchmark as a function of learning rate and effective batch size during supervised fine-tuning.  The x-axis represents the learning rate, and the y-axis represents the effective batch size. The color gradient reflects the model's performance, with darker shades indicating higher GSM8K scores. The plot reveals that higher learning rate to batch size ratios generally lead to better performance on this mathematical reasoning task.", "section": "3 Supervised Finetuning"}, {"figure_path": "https://arxiv.org/html/2412.08347/extracted/6061800/assets/hellaswag_contour.png", "caption": "(c) Effect of learning rate and batch size on HellaSwag score.", "description": "This contour plot analyzes the effects of learning rate and effective batch size on the HellaSwag score during supervised finetuning of the SmolLM2-135M model. The x-axis represents the learning rate, and the y-axis is the effective batch size. The contour lines and color gradients represent the HellaSwag score, with darker shades indicating higher performance. The plot reveals that HellaSwag, a pattern recognition task, achieves optimal performance with lower learning rate to batch size ratios.", "section": "3 Supervised Finetuning"}, {"figure_path": "https://arxiv.org/html/2412.08347/extracted/6061800/assets/ifeval_contour.png", "caption": "(d) Effect of learning rate and batch size on IFEval score.", "description": "This contour plot shows the effect of varying learning rate and effective batch size on the IFEval score during supervised finetuning. It visualizes the performance of a 135 million parameter language model (SmollM2-135M) across different learning rate and effective batch size combinations.  The x-axis represents the learning rate, and the y-axis represents the effective batch size. The color gradient represents the IFEval score, where darker shades indicate higher scores. The plot reveals an optimal region for learning rate and batch size settings that yield the best performance on the IFEval benchmark.", "section": "3 Supervised Finetuning"}]