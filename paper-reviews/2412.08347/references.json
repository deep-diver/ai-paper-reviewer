{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This paper introduced InstructGPT, which established the core supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF) workflow for post-training language models, making it highly influential in the field."}, {"fullname_first_author": "Nathan Lambert", "paper_title": "T\u00fclu 3: Pushing frontiers in open language model post-training", "publication_date": "2024-00-00", "reason": "This work provides the core pipeline and methodology adapted and evaluated in the present study."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-00-00", "reason": "This paper introduced Direct Preference Optimization (DPO), a simplified approach to preference learning that is central to the present work's post-training strategy."}, {"fullname_first_author": "Loubna Ben Allal", "paper_title": "Smollm2 - with great data, comes great performance", "publication_date": "2024-00-00", "reason": "This work introduces the SmolLM2 family of language models, which serves as the base model for the SmolTulu models developed and evaluated in the present study."}, {"fullname_first_author": "Priya Goyal", "paper_title": "Accurate, large minibatch sgd: Training imagenet in 1 hour", "publication_date": "2018-00-00", "reason": "This work establishes key principles for large-batch training, specifically demonstrating how learning rate should be scaled with batch size, which is relevant to understanding optimization dynamics in the present study."}]}