<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Identifying Sensitive Weights via Post-quantization Integral &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="Identifying Sensitive Weights via Post-quantization Integral &#183; HF Daily Paper Reviews by AI"><meta name=description content="PQI: Accurately identify sensitive weights in post-quantization to enhance LLM compression & performance!"><meta name=keywords content="Machine Learning,Deep Learning,üè¢ Tsinghua University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01901/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01901/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="Identifying Sensitive Weights via Post-quantization Integral"><meta property="og:description" content="PQI: Accurately identify sensitive weights in post-quantization to enhance LLM compression & performance!"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-02-28T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-28T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="üè¢ Tsinghua University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01901/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01901/cover.png"><meta name=twitter:title content="Identifying Sensitive Weights via Post-quantization Integral"><meta name=twitter:description content="PQI: Accurately identify sensitive weights in post-quantization to enhance LLM compression & performance!"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Identifying Sensitive Weights via Post-quantization Integral","headline":"Identifying Sensitive Weights via Post-quantization Integral","abstract":"PQI: Accurately identify sensitive weights in post-quantization to enhance LLM compression \u0026amp; performance!","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2503.01901\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-02-28T00:00:00\u002b00:00","datePublished":"2025-02-28T00:00:00\u002b00:00","dateModified":"2025-02-28T00:00:00\u002b00:00","keywords":["Machine Learning","Deep Learning","üè¢ Tsinghua University"],"mainEntityOfPage":"true","wordCount":"2603"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-28</p></a><a href=/ai-paper-reviewer/2025-03-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-31</p></a><a href=/ai-paper-reviewer/2025-04-01/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-04-01</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-28</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-31</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-04-01/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-04-01</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2503.01901/cover_hu11222128876599474787.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2503.01901/>Identifying Sensitive Weights via Post-quantization Integral</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Identifying Sensitive Weights via Post-quantization Integral</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-02-28T00:00:00+00:00>28 February 2025</time><span class="px-2 text-primary-500">&#183;</span><span>2603 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">13 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2503.01901/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2503.01901/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/machine-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Machine Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/deep-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Deep Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-tsinghua-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Tsinghua University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#pqi-accuracy>PQI: Accuracy++</a></li><li><a href=#requant-key-idea>ReQuant: Key idea</a></li><li><a href=#sparse-detach>Sparse Detach</a></li><li><a href=#llm-metric-flaws>LLM Metric Flaws</a></li><li><a href=#limited-radius>Limited Radius</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#pqi-accuracy>PQI: Accuracy++</a></li><li><a href=#requant-key-idea>ReQuant: Key idea</a></li><li><a href=#sparse-detach>Sparse Detach</a></li><li><a href=#llm-metric-flaws>LLM Metric Flaws</a></li><li><a href=#limited-radius>Limited Radius</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2503.01901</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Yuezhou Hu et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-03-07</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2503.01901 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2503.01901 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2503.01901/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Serving LLMs is difficult due to their large size. Post-training quantization (PTQ) helps by compressing LLMs but relies on sensitivity metrics to identify important weights. Existing metrics are inaccurate due to the LLM‚Äôs complicated loss landscape. They underestimate the impact of quantization, as the quantized weights fall outside the convergence radius. Moreover, the sensitivity might change after quantization.</p><p>To solve these issues, this work introduces Post-quantization Integral (<strong>PQI</strong>), a new sensitivity metric that accurately estimates the influence of each quantized weight. PQI considers both original and quantized weights. The research also proposes ReQuant, a framework with two components: outlier selection and step-wise significant weights detach. Experiments show ReQuant improves PTQ, enhancing perplexity gain on Llama 3.2 1B with QTIP.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-10985520166aaf2b20437632d7f77c53></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-10985520166aaf2b20437632d7f77c53",{strings:[" Post-Quantization Integral (PQI) is an accurate metric for estimating the sensitivity of weights after quantization. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-bdfc2b0cd257e8801f9d0687d60900d4></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-bdfc2b0cd257e8801f9d0687d60900d4",{strings:[" The ReQuant pipeline, leveraging PQI, significantly enhances the quality of quantized models through Dense-and-Sparse detach. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-ad9eb31731ca2e9089418b1cd8bc7a14></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-ad9eb31731ca2e9089418b1cd8bc7a14",{strings:[" ReQuant achieves substantial perplexity reduction and performance gains on LLMs when combined with existing quantization methods. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is important because it <strong>introduces a novel approach to enhancing post-training quantization (PTQ) methods for LLMs.</strong> It can significantly boost the performance of existing PTQ techniques, making LLMs more accessible for deployment on resource-constrained devices and opening new research directions for quantization techniques.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.01901/x1.png alt></figure></p><blockquote><p>üîº The ReQuant pipeline consists of six steps. First, the weights are pre-quantized using a traditional method and its sensitivity metric. Second, the optimal outlier ratio for each layer is determined using Algorithm 1. Third, outliers are selected based on this ratio. Fourth, the weights are re-quantized after removing the outliers. Fifth, significant weights are recovered using Algorithm 2. Finally, the low-precision dense weights, sparse outliers and significant weights are summed to form the final quantized weight.</p><details><summary>read the caption</summary>Figure 1: ReQuant pipeline.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S4.T1.3.1><tr class=ltx_tr id=S4.T1.3.1.1><td class="ltx_td ltx_align_left ltx_border_tt" id=S4.T1.3.1.1.2><span class=ltx_text id=S4.T1.3.1.1.2.1></span><span class=ltx_text id=S4.T1.3.1.1.2.2 style=font-size:90%> </span><span class=ltx_text id=S4.T1.3.1.1.2.3 style=font-size:90%><span class="ltx_tabular ltx_align_middle" id=S4.T1.3.1.1.2.3.1><span class=ltx_tr id=S4.T1.3.1.1.2.3.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S4.T1.3.1.1.2.3.1.1.1><span class="ltx_text ltx_font_bold" id=S4.T1.3.1.1.2.3.1.1.1.1>Quan-</span></span></span>
<span class=ltx_tr id=S4.T1.3.1.1.2.3.1.2><span class="ltx_td ltx_nopad_r ltx_align_center" id=S4.T1.3.1.1.2.3.1.2.1><span class="ltx_text ltx_font_bold" id=S4.T1.3.1.1.2.3.1.2.1.1>tized</span></span></span>
<span class=ltx_tr id=S4.T1.3.1.1.2.3.1.3><span class="ltx_td ltx_nopad_r ltx_align_center" id=S4.T1.3.1.1.2.3.1.3.1><span class="ltx_text ltx_font_bold" id=S4.T1.3.1.1.2.3.1.3.1.1>Layer</span></span></span>
</span></span><span class=ltx_text id=S4.T1.3.1.1.2.4></span><span class=ltx_text id=S4.T1.3.1.1.2.5 style=font-size:90%></span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T1.3.1.1.3><span class="ltx_text ltx_font_bold" id=S4.T1.3.1.1.3.1 style=font-size:90%>First-order</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T1.3.1.1.4><span class="ltx_text ltx_font_bold" id=S4.T1.3.1.1.4.1 style=font-size:90%>Second-order</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T1.3.1.1.1><span class="ltx_text ltx_font_bold" id=S4.T1.3.1.1.1.1 style=font-size:90%>Actual</span><span class=ltx_text id=S4.T1.3.1.1.1.2 style=font-size:90%></span><math alttext="\Delta F" class="ltx_Math" display="inline" id="S4.T1.3.1.1.1.m1.1"><semantics id="S4.T1.3.1.1.1.m1.1a"><mrow id="S4.T1.3.1.1.1.m1.1.1" xref="S4.T1.3.1.1.1.m1.1.1.cmml"><mi id="S4.T1.3.1.1.1.m1.1.1.2" mathsize="90%" mathvariant="normal" xref="S4.T1.3.1.1.1.m1.1.1.2.cmml">Œî</mi><mo id="S4.T1.3.1.1.1.m1.1.1.1" xref="S4.T1.3.1.1.1.m1.1.1.1.cmml">‚Å¢</mo><mi id="S4.T1.3.1.1.1.m1.1.1.3" mathsize="90%" xref="S4.T1.3.1.1.1.m1.1.1.3.cmml">F</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.3.1.1.1.m1.1b"><apply id="S4.T1.3.1.1.1.m1.1.1.cmml" xref="S4.T1.3.1.1.1.m1.1.1"><times id="S4.T1.3.1.1.1.m1.1.1.1.cmml" xref="S4.T1.3.1.1.1.m1.1.1.1"></times><ci id="S4.T1.3.1.1.1.m1.1.1.2.cmml" xref="S4.T1.3.1.1.1.m1.1.1.2">Œî</ci><ci id="S4.T1.3.1.1.1.m1.1.1.3.cmml" xref="S4.T1.3.1.1.1.m1.1.1.3">ùêπ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.1.1.1.m1.1c">\Delta F</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.1.1.1.m1.1d">roman_Œî italic_F</annotation></semantics></math></td></tr><tr class=ltx_tr id=S4.T1.3.1.2><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T1.3.1.2.1><span class=ltx_text id=S4.T1.3.1.2.1.1 style=font-size:90%>1</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.3.1.2.2><span class=ltx_text id=S4.T1.3.1.2.2.1 style=font-size:90%>7.10E-04</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.3.1.2.3><span class=ltx_text id=S4.T1.3.1.2.3.1 style=font-size:90%>-5.98E-06</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.3.1.2.4><span class=ltx_text id=S4.T1.3.1.2.4.1 style=font-size:90%>6.88E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.3><td class="ltx_td ltx_align_left" id=S4.T1.3.1.3.1><span class=ltx_text id=S4.T1.3.1.3.1.1 style=font-size:90%>2</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.3.2><span class=ltx_text id=S4.T1.3.1.3.2.1 style=font-size:90%>-6.58E-05</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.3.3><span class=ltx_text id=S4.T1.3.1.3.3.1 style=font-size:90%>-4.54E-06</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.3.4><span class=ltx_text id=S4.T1.3.1.3.4.1 style=font-size:90%>4.45E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.4><td class="ltx_td ltx_align_left" id=S4.T1.3.1.4.1><span class=ltx_text id=S4.T1.3.1.4.1.1 style=font-size:90%>3</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.4.2><span class=ltx_text id=S4.T1.3.1.4.2.1 style=font-size:90%>-3.21E-04</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.4.3><span class=ltx_text id=S4.T1.3.1.4.3.1 style=font-size:90%>-3.66E-06</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.4.4><span class=ltx_text id=S4.T1.3.1.4.4.1 style=font-size:90%>3.67E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.5><td class="ltx_td ltx_align_left" id=S4.T1.3.1.5.1><span class=ltx_text id=S4.T1.3.1.5.1.1 style=font-size:90%>4</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.5.2><span class=ltx_text id=S4.T1.3.1.5.2.1 style=font-size:90%>-5.04E-04</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.5.3><span class=ltx_text id=S4.T1.3.1.5.3.1 style=font-size:90%>-3.68E-06</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.5.4><span class=ltx_text id=S4.T1.3.1.5.4.1 style=font-size:90%>3.82E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.6><td class="ltx_td ltx_align_left" id=S4.T1.3.1.6.1><span class=ltx_text id=S4.T1.3.1.6.1.1 style=font-size:90%>5</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.6.2><span class=ltx_text id=S4.T1.3.1.6.2.1 style=font-size:90%>-7.00E-04</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.6.3><span class=ltx_text id=S4.T1.3.1.6.3.1 style=font-size:90%>-3.75E-06</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.6.4><span class=ltx_text id=S4.T1.3.1.6.4.1 style=font-size:90%>3.72E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.7><td class="ltx_td ltx_align_left" id=S4.T1.3.1.7.1><span class=ltx_text id=S4.T1.3.1.7.1.1 style=font-size:90%>6</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.7.2><span class=ltx_text id=S4.T1.3.1.7.2.1 style=font-size:90%>-6.29E-04</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.7.3><span class=ltx_text id=S4.T1.3.1.7.3.1 style=font-size:90%>-3.61E-06</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.7.4><span class=ltx_text id=S4.T1.3.1.7.4.1 style=font-size:90%>4.27E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.8><td class="ltx_td ltx_align_left" id=S4.T1.3.1.8.1><span class=ltx_text id=S4.T1.3.1.8.1.1 style=font-size:90%>7</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.8.2><span class=ltx_text id=S4.T1.3.1.8.2.1 style=font-size:90%>-2.04E-04</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.8.3><span class=ltx_text id=S4.T1.3.1.8.3.1 style=font-size:90%>-3.63E-06</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.8.4><span class=ltx_text id=S4.T1.3.1.8.4.1 style=font-size:90%>5.06E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.9><td class="ltx_td ltx_align_left" id=S4.T1.3.1.9.1><span class=ltx_text id=S4.T1.3.1.9.1.1 style=font-size:90%>8</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.9.2><span class=ltx_text id=S4.T1.3.1.9.2.1 style=font-size:90%>6.82E-05</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.9.3><span class=ltx_text id=S4.T1.3.1.9.3.1 style=font-size:90%>-3.60E-06</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.9.4><span class=ltx_text id=S4.T1.3.1.9.4.1 style=font-size:90%>5.59E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.10><td class="ltx_td ltx_align_left" id=S4.T1.3.1.10.1><span class=ltx_text id=S4.T1.3.1.10.1.1 style=font-size:90%>9</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.10.2><span class=ltx_text id=S4.T1.3.1.10.2.1 style=font-size:90%>5.75E-05</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.10.3><span class=ltx_text id=S4.T1.3.1.10.3.1 style=font-size:90%>-3.97E-06</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.10.4><span class=ltx_text id=S4.T1.3.1.10.4.1 style=font-size:90%>6.85E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.11><td class="ltx_td ltx_align_left" id=S4.T1.3.1.11.1><span class=ltx_text id=S4.T1.3.1.11.1.1 style=font-size:90%>10</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.11.2><span class=ltx_text id=S4.T1.3.1.11.2.1 style=font-size:90%>2.86E-04</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.11.3><span class=ltx_text id=S4.T1.3.1.11.3.1 style=font-size:90%>-4.10E-06</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.11.4><span class=ltx_text id=S4.T1.3.1.11.4.1 style=font-size:90%>7.78E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.12><td class="ltx_td ltx_align_left" id=S4.T1.3.1.12.1><span class=ltx_text id=S4.T1.3.1.12.1.1 style=font-size:90%>11</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.12.2><span class=ltx_text id=S4.T1.3.1.12.2.1 style=font-size:90%>-6.43E-04</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.12.3><span class=ltx_text id=S4.T1.3.1.12.3.1 style=font-size:90%>-3.66E-06</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.12.4><span class=ltx_text id=S4.T1.3.1.12.4.1 style=font-size:90%>6.57E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.13><td class="ltx_td ltx_align_left" id=S4.T1.3.1.13.1><span class=ltx_text id=S4.T1.3.1.13.1.1 style=font-size:90%>12</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.13.2><span class=ltx_text id=S4.T1.3.1.13.2.1 style=font-size:90%>8.29E-04</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.13.3><span class=ltx_text id=S4.T1.3.1.13.3.1 style=font-size:90%>-2.95E-06</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.13.4><span class=ltx_text id=S4.T1.3.1.13.4.1 style=font-size:90%>6.81E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.14><td class="ltx_td ltx_align_left" id=S4.T1.3.1.14.1><span class=ltx_text id=S4.T1.3.1.14.1.1 style=font-size:90%>13</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.14.2><span class=ltx_text id=S4.T1.3.1.14.2.1 style=font-size:90%>6.14E-04</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.14.3><span class=ltx_text id=S4.T1.3.1.14.3.1 style=font-size:90%>-2.80E-06</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.14.4><span class=ltx_text id=S4.T1.3.1.14.4.1 style=font-size:90%>5.83E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.15><td class="ltx_td ltx_align_left" id=S4.T1.3.1.15.1><span class=ltx_text id=S4.T1.3.1.15.1.1 style=font-size:90%>14</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.15.2><span class=ltx_text id=S4.T1.3.1.15.2.1 style=font-size:90%>1.30E-03</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.15.3><span class=ltx_text id=S4.T1.3.1.15.3.1 style=font-size:90%>-2.65E-06</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.15.4><span class=ltx_text id=S4.T1.3.1.15.4.1 style=font-size:90%>6.57E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.16><td class="ltx_td ltx_align_left" id=S4.T1.3.1.16.1><span class=ltx_text id=S4.T1.3.1.16.1.1 style=font-size:90%>15</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.16.2><span class=ltx_text id=S4.T1.3.1.16.2.1 style=font-size:90%>-2.52E-04</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.16.3><span class=ltx_text id=S4.T1.3.1.16.3.1 style=font-size:90%>-2.84E-06</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.16.4><span class=ltx_text id=S4.T1.3.1.16.4.1 style=font-size:90%>5.30E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.17><td class="ltx_td ltx_align_left" id=S4.T1.3.1.17.1><span class=ltx_text id=S4.T1.3.1.17.1.1 style=font-size:90%>16</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.17.2><span class=ltx_text id=S4.T1.3.1.17.2.1 style=font-size:90%>3.47E-04</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.17.3><span class=ltx_text id=S4.T1.3.1.17.3.1 style=font-size:90%>-5.05E-06</span></td><td class="ltx_td ltx_align_center" id=S4.T1.3.1.17.4><span class=ltx_text id=S4.T1.3.1.17.4.1 style=font-size:90%>9.79E-03</span></td></tr><tr class=ltx_tr id=S4.T1.3.1.18><td class="ltx_td ltx_align_left ltx_border_bb" id=S4.T1.3.1.18.1><span class=ltx_text id=S4.T1.3.1.18.1.1 style=font-size:90%>All</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.3.1.18.2><span class=ltx_text id=S4.T1.3.1.18.2.1 style=font-size:90%>8.92E-04</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.3.1.18.3><span class=ltx_text id=S4.T1.3.1.18.3.1 style=font-size:90%>-6.05E-05</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.3.1.18.4><span class=ltx_text id=S4.T1.3.1.18.4.1 style=font-size:90%>1.00E-01</span></td></tr></table></table></figure><blockquote><p>üîº Table 1 presents a detailed breakdown of the accuracy of first-order and second-order Taylor expansion approximations in predicting the actual change in loss function (ŒîF) after weight quantization. It compares the calculated first-order and second-order terms from Equation 1 with the actual observed ŒîF for each layer of a 16-layer Llama 3.2 1B language model. This comparison reveals the significant discrepancy between the approximation and reality, especially concerning the underestimation of the actual loss function change by orders of magnitude.</p><details><summary>read the caption</summary>Table 1: First-order, second-order term and actual Œî‚Å¢FŒîùêπ\Delta Froman_Œî italic_F in Equation¬†1.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">PQI: Accuracy++<div id=pqi-accuracy class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#pqi-accuracy aria-label=Anchor>#</a></span></h4><p>While the title &ldquo;PQI: Accuracy++&rdquo; is speculative, it suggests a significant leap in accuracy attributable to the Post-quantization Integral (PQI). The &ldquo;++&rdquo; implies PQI isn&rsquo;t merely an incremental improvement, but a substantial enhancement. This leap could stem from PQI&rsquo;s ability to <strong>more accurately estimate the impact of quantization</strong> on individual weight dimensions, overcoming limitations of gradient/Hessian metrics. A central component of PQI&rsquo;s accuracy is its <strong>fine-grained approach</strong>, estimating posterior sensitivity meticulously. It should also be noted that it can be combined with quantization methods. It is important to state that its accuracy lies in <strong>decomposing the path into numerous small fragments</strong>. As a result, Taylor&rsquo;s formula can accurately approximate each fragment.</p><h4 class="relative group">ReQuant: Key idea<div id=requant-key-idea class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#requant-key-idea aria-label=Anchor>#</a></span></h4><p>ReQuant introduces a novel approach to post-quantization by employing a <strong>Dense-and-Sparse detach</strong> strategy, which distinguishes it from traditional quantization methods. The core idea revolves around <strong>intelligently separating weights into dense, outlier, and significant components.</strong> The method first quantizes most of the weights using standard techniques (dense component), then preserves a small subset of <strong>outlier weights in high precision</strong> to mitigate accuracy loss. Critically, ReQuant identifies and detaches weights that, while not necessarily outliers, have a <strong>disproportionate impact on the model&rsquo;s performance post-quantization (significant weights)</strong>. By treating these crucial weights separately, ReQuant aims to strike a balance between aggressive compression and maintaining model fidelity. This allows for more effective quantization without sacrificing accuracy, as demonstrated by its performance improvements on LLMs.</p><h4 class="relative group">Sparse Detach<div id=sparse-detach class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#sparse-detach aria-label=Anchor>#</a></span></h4><p>The <strong>sparse detach</strong> component is likely a crucial step in optimizing the quantization process for Large Language Models (LLMs). It probably involves selectively removing or isolating a subset of weights deemed less critical, or outliers, to improve overall model performance after quantization. This approach is based on the idea that not all weights contribute equally. By focusing quantization efforts on the most sensitive weights and detaching or handling outliers differently, the impact of reduced precision can be minimized. This is often achieved by maintaining a certain percentage of weights in higher precision. It is important as improper selection would degrade performance.</p><h4 class="relative group">LLM Metric Flaws<div id=llm-metric-flaws class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#llm-metric-flaws aria-label=Anchor>#</a></span></h4><p><strong>Existing metrics</strong> for evaluating weight quantization sensitivity in LLMs, such as gradient or Hessian-based approaches, <strong>suffer from inaccuracies</strong>. These metrics <strong>underestimate the impact of quantization</strong> on the loss function by orders of magnitude, mainly due to the <strong>small convergence radius</strong> of local second-order approximations. The complicated loss landscape of LLMs invalidates these approximations outside a tiny region around the original weights. Furthermore, the sensitivity calculated on original weights may not align with the actual sensitivity of quantized weights, as previously important weights may lose significance after quantization, and vice-versa, thus, these metrics <strong>fail to accurately predict the change in loss</strong> caused by weight quantization. Therefore, a new metric is needed.</p><h4 class="relative group">Limited Radius<div id=limited-radius class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#limited-radius aria-label=Anchor>#</a></span></h4><p>The text discusses the <strong>convergence radius of Taylor&rsquo;s expansion</strong> and how it affects the accuracy of sensitivity metrics used in post-training quantization (PTQ) for Large Language Models (LLMs). It argues that existing gradient and Hessian-based metrics are inaccurate due to the small convergence radius, meaning the local approximations they use are only valid in a very small region around the original weights. Quantization introduces significant changes, pushing the weights outside this radius. The Taylor series expansion becomes unreliable when the quantized weights fall outside the convergence radius of the original weights. The result is inaccurate estimation of the loss function change, hindering effective weight quantization.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S4.T2.6.2><tr class=ltx_tr id=S4.T2.6.2.2><td class="ltx_td ltx_align_left ltx_border_tt" id=S4.T2.5.1.1.1><math alttext="\lambda" class="ltx_Math" display="inline" id="S4.T2.5.1.1.1.m1.1"><semantics id="S4.T2.5.1.1.1.m1.1a"><mi id="S4.T2.5.1.1.1.m1.1.1" mathsize="90%" xref="S4.T2.5.1.1.1.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S4.T2.5.1.1.1.m1.1b"><ci id="S4.T2.5.1.1.1.m1.1.1.cmml" xref="S4.T2.5.1.1.1.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.1.1.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.1.1.1.m1.1d">italic_Œª</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T2.6.2.2.3><span class="ltx_text ltx_font_bold" id=S4.T2.6.2.2.3.1 style=font-size:90%>First-order</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T2.6.2.2.4><span class="ltx_text ltx_font_bold" id=S4.T2.6.2.2.4.1 style=font-size:90%>Second-order</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T2.6.2.2.2><span class="ltx_text ltx_font_bold" id=S4.T2.6.2.2.2.1 style=font-size:90%>Actual</span><span class=ltx_text id=S4.T2.6.2.2.2.2 style=font-size:90%></span><math alttext="\Delta F" class="ltx_Math" display="inline" id="S4.T2.6.2.2.2.m1.1"><semantics id="S4.T2.6.2.2.2.m1.1a"><mrow id="S4.T2.6.2.2.2.m1.1.1" xref="S4.T2.6.2.2.2.m1.1.1.cmml"><mi id="S4.T2.6.2.2.2.m1.1.1.2" mathsize="90%" mathvariant="normal" xref="S4.T2.6.2.2.2.m1.1.1.2.cmml">Œî</mi><mo id="S4.T2.6.2.2.2.m1.1.1.1" xref="S4.T2.6.2.2.2.m1.1.1.1.cmml">‚Å¢</mo><mi id="S4.T2.6.2.2.2.m1.1.1.3" mathsize="90%" xref="S4.T2.6.2.2.2.m1.1.1.3.cmml">F</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.2.2.2.m1.1b"><apply id="S4.T2.6.2.2.2.m1.1.1.cmml" xref="S4.T2.6.2.2.2.m1.1.1"><times id="S4.T2.6.2.2.2.m1.1.1.1.cmml" xref="S4.T2.6.2.2.2.m1.1.1.1"></times><ci id="S4.T2.6.2.2.2.m1.1.1.2.cmml" xref="S4.T2.6.2.2.2.m1.1.1.2">Œî</ci><ci id="S4.T2.6.2.2.2.m1.1.1.3.cmml" xref="S4.T2.6.2.2.2.m1.1.1.3">ùêπ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.2.2.2.m1.1c">\Delta F</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.2.2.2.m1.1d">roman_Œî italic_F</annotation></semantics></math></td></tr><tr class=ltx_tr id=S4.T2.6.2.3><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.6.2.3.1><span class=ltx_text id=S4.T2.6.2.3.1.1 style=font-size:90%>1E-1</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T2.6.2.3.2><span class=ltx_text id=S4.T2.6.2.3.2.1 style=font-size:90%>8.92E-5</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T2.6.2.3.3><span class=ltx_text id=S4.T2.6.2.3.3.1 style=font-size:90%>-6.05E-7</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T2.6.2.3.4><span class=ltx_text id=S4.T2.6.2.3.4.1 style=font-size:90%>1.00E-3</span></td></tr><tr class=ltx_tr id=S4.T2.6.2.4><td class="ltx_td ltx_align_left" id=S4.T2.6.2.4.1><span class=ltx_text id=S4.T2.6.2.4.1.1 style=font-size:90%>5E-2</span></td><td class="ltx_td ltx_align_center" id=S4.T2.6.2.4.2><span class=ltx_text id=S4.T2.6.2.4.2.1 style=font-size:90%>4.46E-5</span></td><td class="ltx_td ltx_align_center" id=S4.T2.6.2.4.3><span class=ltx_text id=S4.T2.6.2.4.3.1 style=font-size:90%>-1.51E-7</span></td><td class="ltx_td ltx_align_center" id=S4.T2.6.2.4.4><span class=ltx_text id=S4.T2.6.2.4.4.1 style=font-size:90%>2.73E-4</span></td></tr><tr class=ltx_tr id=S4.T2.6.2.5><td class="ltx_td ltx_align_left" id=S4.T2.6.2.5.1><span class=ltx_text id=S4.T2.6.2.5.1.1 style=font-size:90%>1E-2</span></td><td class="ltx_td ltx_align_center" id=S4.T2.6.2.5.2><span class=ltx_text id=S4.T2.6.2.5.2.1 style=font-size:90%>8.92E-6</span></td><td class="ltx_td ltx_align_center" id=S4.T2.6.2.5.3><span class=ltx_text id=S4.T2.6.2.5.3.1 style=font-size:90%>-6.05E-9</span></td><td class="ltx_td ltx_align_center" id=S4.T2.6.2.5.4><span class=ltx_text id=S4.T2.6.2.5.4.1 style=font-size:90%>1.81E-5</span></td></tr><tr class=ltx_tr id=S4.T2.6.2.6><td class="ltx_td ltx_align_left" id=S4.T2.6.2.6.1><span class=ltx_text id=S4.T2.6.2.6.1.1 style=font-size:90%>5E-3</span></td><td class="ltx_td ltx_align_center" id=S4.T2.6.2.6.2><span class=ltx_text id=S4.T2.6.2.6.2.1 style=font-size:90%>4.46E-6</span></td><td class="ltx_td ltx_align_center" id=S4.T2.6.2.6.3><span class=ltx_text id=S4.T2.6.2.6.3.1 style=font-size:90%>-1.51E-9</span></td><td class="ltx_td ltx_align_center" id=S4.T2.6.2.6.4><span class=ltx_text id=S4.T2.6.2.6.4.1 style=font-size:90%>6.68E-6</span></td></tr><tr class=ltx_tr id=S4.T2.6.2.7><td class="ltx_td ltx_align_left ltx_border_bb" id=S4.T2.6.2.7.1><span class=ltx_text id=S4.T2.6.2.7.1.1 style=font-size:90%>1E-3</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T2.6.2.7.2><span class=ltx_text id=S4.T2.6.2.7.2.1 style=font-size:90%>8.92E-7</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T2.6.2.7.3><span class=ltx_text id=S4.T2.6.2.7.3.1 style=font-size:90%>-6.05E-11</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T2.6.2.7.4><span class=ltx_text id=S4.T2.6.2.7.4.1 style=font-size:90%>9.54E-7</span></td></tr></table></table></figure><blockquote><p>üîº This table shows how the accuracy of the Taylor expansion approximation for the change in loss function (ŒîF) varies with different values of Œª (lambda). Œª controls how close the interpolated weight w&rsquo; is to the original weight w. As Œª approaches 0, w&rsquo; gets closer to w, making the Taylor expansion more accurate. The table compares the actual change in loss (Actual ŒîF) to the values predicted by the first-order and second-order terms of the Taylor expansion for various layers of a 16-layer Llama 3.2 1B model.</p><details><summary>read the caption</summary>Table 2: Actual Œî‚Å¢FŒîùêπ\Delta Froman_Œî italic_F with different ŒªùúÜ\lambdaitalic_Œª.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T3.5.1><tr class=ltx_tr id=S5.T3.5.1.1><td class="ltx_td ltx_align_left ltx_border_tt" id=S5.T3.5.1.1.2><span class="ltx_text ltx_font_bold" id=S5.T3.5.1.1.2.1 style=font-size:90%>Intervals</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.5.1.1.1><span class="ltx_text ltx_font_bold" id=S5.T3.5.1.1.1.1 style=font-size:90%>Predicted</span><span class=ltx_text id=S5.T3.5.1.1.1.2 style=font-size:90%></span><math alttext="\Delta F" class="ltx_Math" display="inline" id="S5.T3.5.1.1.1.m1.1"><semantics id="S5.T3.5.1.1.1.m1.1a"><mrow id="S5.T3.5.1.1.1.m1.1.1" xref="S5.T3.5.1.1.1.m1.1.1.cmml"><mi id="S5.T3.5.1.1.1.m1.1.1.2" mathsize="90%" mathvariant="normal" xref="S5.T3.5.1.1.1.m1.1.1.2.cmml">Œî</mi><mo id="S5.T3.5.1.1.1.m1.1.1.1" xref="S5.T3.5.1.1.1.m1.1.1.1.cmml">‚Å¢</mo><mi id="S5.T3.5.1.1.1.m1.1.1.3" mathsize="90%" xref="S5.T3.5.1.1.1.m1.1.1.3.cmml">F</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.5.1.1.1.m1.1b"><apply id="S5.T3.5.1.1.1.m1.1.1.cmml" xref="S5.T3.5.1.1.1.m1.1.1"><times id="S5.T3.5.1.1.1.m1.1.1.1.cmml" xref="S5.T3.5.1.1.1.m1.1.1.1"></times><ci id="S5.T3.5.1.1.1.m1.1.1.2.cmml" xref="S5.T3.5.1.1.1.m1.1.1.2">Œî</ci><ci id="S5.T3.5.1.1.1.m1.1.1.3.cmml" xref="S5.T3.5.1.1.1.m1.1.1.3">ùêπ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.1.1.1.m1.1c">\Delta F</annotation><annotation encoding="application/x-llamapun" id="S5.T3.5.1.1.1.m1.1d">roman_Œî italic_F</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.5.1.1.3><span class="ltx_text ltx_font_bold" id=S5.T3.5.1.1.3.1 style=font-size:90%>Error</span></td></tr><tr class=ltx_tr id=S5.T3.5.1.2><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T3.5.1.2.1><span class=ltx_text id=S5.T3.5.1.2.1.1 style=font-size:90%>4</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.5.1.2.2><span class=ltx_text id=S5.T3.5.1.2.2.1 style=font-size:90%>1.042E-1</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.5.1.2.3><span class=ltx_text id=S5.T3.5.1.2.3.1 style=font-size:90%>1.72E-2</span></td></tr><tr class=ltx_tr id=S5.T3.5.1.3><td class="ltx_td ltx_align_left" id=S5.T3.5.1.3.1><span class=ltx_text id=S5.T3.5.1.3.1.1 style=font-size:90%>8</span></td><td class="ltx_td ltx_align_center" id=S5.T3.5.1.3.2><span class=ltx_text id=S5.T3.5.1.3.2.1 style=font-size:90%>1.032E-1</span></td><td class="ltx_td ltx_align_center" id=S5.T3.5.1.3.3><span class=ltx_text id=S5.T3.5.1.3.3.1 style=font-size:90%>8.39E-4</span></td></tr><tr class=ltx_tr id=S5.T3.5.1.4><td class="ltx_td ltx_align_left" id=S5.T3.5.1.4.1><span class=ltx_text id=S5.T3.5.1.4.1.1 style=font-size:90%>16</span></td><td class="ltx_td ltx_align_center" id=S5.T3.5.1.4.2><span class=ltx_text id=S5.T3.5.1.4.2.1 style=font-size:90%>1.028E-1</span></td><td class="ltx_td ltx_align_center" id=S5.T3.5.1.4.3><span class=ltx_text id=S5.T3.5.1.4.3.1 style=font-size:90%>3.90E-4</span></td></tr><tr class=ltx_tr id=S5.T3.5.1.5><td class="ltx_td ltx_align_left ltx_border_bb" id=S5.T3.5.1.5.1><span class=ltx_text id=S5.T3.5.1.5.1.1 style=font-size:90%>32</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.5.1.5.2><span class=ltx_text id=S5.T3.5.1.5.2.1 style=font-size:90%>1.026E-1</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.5.1.5.3><span class=ltx_text id=S5.T3.5.1.5.3.1 style=font-size:90%>1.62E-4</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the results of an experiment designed to evaluate the accuracy of the Post-quantization Integral (PQI) method, a novel sensitivity metric. The experiment quantifies the change in the loss function (ŒîF) using PQI with varying numbers of intervals used in the numerical integration process. The goal is to demonstrate how well PQI can predict the actual change in the loss function. The actual ŒîF for the dataset is provided for comparison, allowing assessment of PQI&rsquo;s accuracy with different levels of granularity in the approximation.</p><details><summary>read the caption</summary>Table 3: Predicted Œî‚Å¢FŒîùêπ\Delta Froman_Œî italic_F with intervals we split. For reference, the actual Œî‚Å¢F‚Å¢(ùê∞)Œîùêπùê∞\Delta F(\bm{\mathbf{w}})roman_Œî italic_F ( bold_w ) on this dataset should be 0.1024.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T4.3><tr class=ltx_tr id=S5.T4.3.1><td class="ltx_td ltx_align_left ltx_border_tt" id=S5.T4.3.1.1><span class="ltx_text ltx_font_bold" id=S5.T4.3.1.1.1 style=font-size:90%>Layer</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.3.1.2><span class="ltx_text ltx_font_bold" id=S5.T4.3.1.2.1 style=font-size:90%>Q</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.3.1.3><span class="ltx_text ltx_font_bold" id=S5.T4.3.1.3.1 style=font-size:90%>K</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.3.1.4><span class="ltx_text ltx_font_bold" id=S5.T4.3.1.4.1 style=font-size:90%>V</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.3.1.5><span class="ltx_text ltx_font_bold" id=S5.T4.3.1.5.1 style=font-size:90%>O</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.3.1.6><span class="ltx_text ltx_font_bold" id=S5.T4.3.1.6.1 style=font-size:90%>Gate</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.3.1.7><span class="ltx_text ltx_font_bold" id=S5.T4.3.1.7.1 style=font-size:90%>Up</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.3.1.8><span class="ltx_text ltx_font_bold" id=S5.T4.3.1.8.1 style=font-size:90%>Down</span></td></tr><tr class=ltx_tr id=S5.T4.3.2><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T4.3.2.1><span class=ltx_text id=S5.T4.3.2.1.1 style=font-size:90%>1</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.3.2.2><span class=ltx_text id=S5.T4.3.2.2.1 style=font-size:90%>4.53E-08</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.3.2.3><span class=ltx_text id=S5.T4.3.2.3.1 style=font-size:90%>9.93E-08</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.3.2.4><span class=ltx_text id=S5.T4.3.2.4.1 style=font-size:90%>1.59E-07</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.3.2.5><span class=ltx_text id=S5.T4.3.2.5.1 style=font-size:90%>9.13E-08</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.3.2.6><span class=ltx_text id=S5.T4.3.2.6.1 style=font-size:90%>4.22E-08</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.3.2.7><span class=ltx_text id=S5.T4.3.2.7.1 style=font-size:90%>4.99E-08</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.3.2.8><span class=ltx_text id=S5.T4.3.2.8.1 style=font-size:90%>5.31E-08</span></td></tr><tr class=ltx_tr id=S5.T4.3.3><td class="ltx_td ltx_align_left" id=S5.T4.3.3.1><span class=ltx_text id=S5.T4.3.3.1.1 style=font-size:90%>5</span></td><td class="ltx_td ltx_align_center" id=S5.T4.3.3.2><span class=ltx_text id=S5.T4.3.3.2.1 style=font-size:90%>4.16E-08</span></td><td class="ltx_td ltx_align_center" id=S5.T4.3.3.3><span class=ltx_text id=S5.T4.3.3.3.1 style=font-size:90%>6.66E-08</span></td><td class="ltx_td ltx_align_center" id=S5.T4.3.3.4><span class=ltx_text id=S5.T4.3.3.4.1 style=font-size:90%>1.07E-07</span></td><td class="ltx_td ltx_align_center" id=S5.T4.3.3.5><span class=ltx_text id=S5.T4.3.3.5.1 style=font-size:90%>7.37E-08</span></td><td class="ltx_td ltx_align_center" id=S5.T4.3.3.6><span class=ltx_text id=S5.T4.3.3.6.1 style=font-size:90%>2.57E-08</span></td><td class="ltx_td ltx_align_center" id=S5.T4.3.3.7><span class=ltx_text id=S5.T4.3.3.7.1 style=font-size:90%>4.14E-08</span></td><td class="ltx_td ltx_align_center" id=S5.T4.3.3.8><span class=ltx_text id=S5.T4.3.3.8.1 style=font-size:90%>4.37E-08</span></td></tr><tr class=ltx_tr id=S5.T4.3.4><td class="ltx_td ltx_align_left" id=S5.T4.3.4.1><span class=ltx_text id=S5.T4.3.4.1.1 style=font-size:90%>8</span></td><td class="ltx_td ltx_align_center" id=S5.T4.3.4.2><span class=ltx_text id=S5.T4.3.4.2.1 style=font-size:90%>3.83E-08</span></td><td class="ltx_td ltx_align_center" id=S5.T4.3.4.3><span class=ltx_text id=S5.T4.3.4.3.1 style=font-size:90%>6.11E-08</span></td><td class="ltx_td ltx_align_center" id=S5.T4.3.4.4><span class=ltx_text id=S5.T4.3.4.4.1 style=font-size:90%>9.94E-08</span></td><td class="ltx_td ltx_align_center" id=S5.T4.3.4.5><span class=ltx_text id=S5.T4.3.4.5.1 style=font-size:90%>8.83E-08</span></td><td class="ltx_td ltx_align_center" id=S5.T4.3.4.6><span class=ltx_text id=S5.T4.3.4.6.1 style=font-size:90%>2.46E-08</span></td><td class="ltx_td ltx_align_center" id=S5.T4.3.4.7><span class=ltx_text id=S5.T4.3.4.7.1 style=font-size:90%>4.01E-08</span></td><td class="ltx_td ltx_align_center" id=S5.T4.3.4.8><span class=ltx_text id=S5.T4.3.4.8.1 style=font-size:90%>4.72E-08</span></td></tr><tr class=ltx_tr id=S5.T4.3.5><td class="ltx_td ltx_align_left ltx_border_bb" id=S5.T4.3.5.1><span class=ltx_text id=S5.T4.3.5.1.1 style=font-size:90%>11</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.3.5.2><span class=ltx_text id=S5.T4.3.5.2.1 style=font-size:90%>2.63E-08</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.3.5.3><span class=ltx_text id=S5.T4.3.5.3.1 style=font-size:90%>4.53E-08</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.3.5.4><span class=ltx_text id=S5.T4.3.5.4.1 style=font-size:90%>7.88E-08</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.3.5.5><span class=ltx_text id=S5.T4.3.5.5.1 style=font-size:90%>4.67E-08</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.3.5.6><span class=ltx_text id=S5.T4.3.5.6.1 style=font-size:90%>2.90E-08</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.3.5.7><span class=ltx_text id=S5.T4.3.5.7.1 style=font-size:90%>3.78E-08</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.3.5.8><span class=ltx_text id=S5.T4.3.5.8.1 style=font-size:90%>4.61E-08</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the element-wise average of the Post-quantization Integral (PQI) sensitivity metric for different layers and sub-layers within the Llama language model. The PQI metric quantifies the sensitivity of each weight to quantization, indicating its importance in maintaining model accuracy. Higher values suggest greater sensitivity and thus a larger impact on the model&rsquo;s performance if that weight is quantized. The table helps to understand the varying sensitivity across different model components, informing strategies for more effective quantization.</p><details><summary>read the caption</summary>Table 4: Element-wise average Œî‚Å¢FP‚Å¢Q‚Å¢IŒîsubscriptùêπùëÉùëÑùêº\Delta F_{PQI}roman_Œî italic_F start_POSTSUBSCRIPT italic_P italic_Q italic_I end_POSTSUBSCRIPT of different layers and sublayers.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S6.T5.3.1><tr class=ltx_tr id=S6.T5.3.1.1><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T5.3.1.1.2><span class=ltx_text id=S6.T5.3.1.1.2.1></span> <span class=ltx_text id=S6.T5.3.1.1.2.2><span class="ltx_tabular ltx_align_middle" id=S6.T5.3.1.1.2.2.1><span class=ltx_tr id=S6.T5.3.1.1.2.2.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S6.T5.3.1.1.2.2.1.1.1><span class="ltx_text ltx_font_bold" id=S6.T5.3.1.1.2.2.1.1.1.1>Proportion of</span></span></span>
<span class=ltx_tr id=S6.T5.3.1.1.2.2.1.2><span class="ltx_td ltx_nopad_r ltx_align_center" id=S6.T5.3.1.1.2.2.1.2.1><span class="ltx_text ltx_font_bold" id=S6.T5.3.1.1.2.2.1.2.1.1>Significant Weights</span></span></span>
</span></span><span class=ltx_text id=S6.T5.3.1.1.2.3></span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T5.3.1.1.1><span class=ltx_text id=S6.T5.3.1.1.1.2></span> <span class=ltx_text id=S6.T5.3.1.1.1.1><span class="ltx_tabular ltx_align_middle" id=S6.T5.3.1.1.1.1.1><span class=ltx_tr id=S6.T5.3.1.1.1.1.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S6.T5.3.1.1.1.1.1.1.1><math alttext="\Delta F_{PQI}" class="ltx_Math" display="inline" id="S6.T5.3.1.1.1.1.1.1.1.m1.1"><semantics id="S6.T5.3.1.1.1.1.1.1.1.m1.1a"><mrow id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.2" mathvariant="normal" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.2.cmml">Œî</mi><mo id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.1" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.1.cmml">‚Å¢</mo><msub id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mi id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">F</mi><mrow id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.cmml"><mi id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.2" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.2.cmml">P</mi><mo id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.1" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.3" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.3.cmml">Q</mi><mo id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.1a" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.4" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.4.cmml">I</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.T5.3.1.1.1.1.1.1.1.m1.1b"><apply id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1"><times id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.1"></times><ci id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.2">Œî</ci><apply id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.2">ùêπ</ci><apply id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3"><times id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.1.cmml" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.1"></times><ci id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.2.cmml" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.2">ùëÉ</ci><ci id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.3.cmml" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.3">ùëÑ</ci><ci id="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.4.cmml" xref="S6.T5.3.1.1.1.1.1.1.1.m1.1.1.3.3.4">ùêº</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.3.1.1.1.1.1.1.1.m1.1c">\Delta F_{PQI}</annotation><annotation encoding="application/x-llamapun" id="S6.T5.3.1.1.1.1.1.1.1.m1.1d">roman_Œî italic_F start_POSTSUBSCRIPT italic_P italic_Q italic_I end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id=S6.T5.3.1.1.1.1.1.1.1.1>percentage</span></span></span>
</span></span><span class=ltx_text id=S6.T5.3.1.1.1.3></span></td></tr><tr class=ltx_tr id=S6.T5.3.1.2><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T5.3.1.2.1>0.15%</td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T5.3.1.2.2>4.53%</td></tr><tr class=ltx_tr id=S6.T5.3.1.3><td class="ltx_td ltx_align_center" id=S6.T5.3.1.3.1>0.71%</td><td class="ltx_td ltx_align_center" id=S6.T5.3.1.3.2>11.29%</td></tr><tr class=ltx_tr id=S6.T5.3.1.4><td class="ltx_td ltx_align_center ltx_border_bb" id=S6.T5.3.1.4.1>5.25%</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S6.T5.3.1.4.2>34.06%</td></tr></table></table></figure><blockquote><p>üîº This table shows the relationship between the percentage of weights considered &lsquo;significant&rsquo; and their cumulative contribution to the total post-quantization integral (PQI) sensitivity metric. The PQI sensitivity metric quantifies how much each weight&rsquo;s quantization affects the model&rsquo;s loss. Higher ŒîFPQI values indicate greater sensitivity. The table helps illustrate the impact of focusing on a smaller subset of most sensitive weights in the model&rsquo;s quantization.</p><details><summary>read the caption</summary>Table 5: The proportion of significant weights we choose and how much they can cover in total Œî‚Å¢FP‚Å¢Q‚Å¢IŒîsubscriptùêπùëÉùëÑùêº\Delta F_{PQI}roman_Œî italic_F start_POSTSUBSCRIPT italic_P italic_Q italic_I end_POSTSUBSCRIPT.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id=S6.T6.4><tr class=ltx_tr id=S6.T6.4.5><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S6.T6.4.5.1 rowspan=2><span class="ltx_text ltx_font_bold" id=S6.T6.4.5.1.1 style=font-size:80%>Precision</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S6.T6.4.5.2 rowspan=2><span class="ltx_text ltx_font_bold" id=S6.T6.4.5.2.1 style=font-size:80%>Method</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S6.T6.4.5.3 rowspan=2><span class="ltx_text ltx_font_bold" id=S6.T6.4.5.3.1 style=font-size:80%>Calib Set</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan=2 id=S6.T6.4.5.4><span class="ltx_text ltx_font_bold" id=S6.T6.4.5.4.1 style=font-size:80%>Sparsity</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S6.T6.4.5.5 rowspan=2><span class="ltx_text ltx_font_bold" id=S6.T6.4.5.5.1 style=font-size:80%>Bits</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S6.T6.4.5.6><span class="ltx_text ltx_font_bold" id=S6.T6.4.5.6.1 style=font-size:80%>Mem</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S6.T6.4.5.7><span class="ltx_text ltx_font_bold" id=S6.T6.4.5.7.1 style=font-size:80%>Base</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T6.4.5.8><span class="ltx_text ltx_font_bold" id=S6.T6.4.5.8.1 style=font-size:80%>Instruct</span></td></tr><tr class=ltx_tr id=S6.T6.4.4><td class="ltx_td ltx_align_center" id=S6.T6.1.1.1><math alttext="r_{o}" class="ltx_Math" display="inline" id="S6.T6.1.1.1.m1.1"><semantics id="S6.T6.1.1.1.m1.1a"><msub id="S6.T6.1.1.1.m1.1.1" xref="S6.T6.1.1.1.m1.1.1.cmml"><mi id="S6.T6.1.1.1.m1.1.1.2" mathsize="80%" xref="S6.T6.1.1.1.m1.1.1.2.cmml">r</mi><mi id="S6.T6.1.1.1.m1.1.1.3" mathsize="80%" xref="S6.T6.1.1.1.m1.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S6.T6.1.1.1.m1.1b"><apply id="S6.T6.1.1.1.m1.1.1.cmml" xref="S6.T6.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.T6.1.1.1.m1.1.1.1.cmml" xref="S6.T6.1.1.1.m1.1.1">subscript</csymbol><ci id="S6.T6.1.1.1.m1.1.1.2.cmml" xref="S6.T6.1.1.1.m1.1.1.2">ùëü</ci><ci id="S6.T6.1.1.1.m1.1.1.3.cmml" xref="S6.T6.1.1.1.m1.1.1.3">ùëú</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.1.1.1.m1.1c">r_{o}</annotation><annotation encoding="application/x-llamapun" id="S6.T6.1.1.1.m1.1d">italic_r start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T6.2.2.2><math alttext="r_{s}" class="ltx_Math" display="inline" id="S6.T6.2.2.2.m1.1"><semantics id="S6.T6.2.2.2.m1.1a"><msub id="S6.T6.2.2.2.m1.1.1" xref="S6.T6.2.2.2.m1.1.1.cmml"><mi id="S6.T6.2.2.2.m1.1.1.2" mathsize="80%" xref="S6.T6.2.2.2.m1.1.1.2.cmml">r</mi><mi id="S6.T6.2.2.2.m1.1.1.3" mathsize="80%" xref="S6.T6.2.2.2.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S6.T6.2.2.2.m1.1b"><apply id="S6.T6.2.2.2.m1.1.1.cmml" xref="S6.T6.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S6.T6.2.2.2.m1.1.1.1.cmml" xref="S6.T6.2.2.2.m1.1.1">subscript</csymbol><ci id="S6.T6.2.2.2.m1.1.1.2.cmml" xref="S6.T6.2.2.2.m1.1.1.2">ùëü</ci><ci id="S6.T6.2.2.2.m1.1.1.3.cmml" xref="S6.T6.2.2.2.m1.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.2.2.2.m1.1c">r_{s}</annotation><annotation encoding="application/x-llamapun" id="S6.T6.2.2.2.m1.1d">italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T6.4.4.5><span class=ltx_text id=S6.T6.4.4.5.1 style=font-size:80%>(GB)</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T6.3.3.3><span class=ltx_text id=S6.T6.3.3.3.1 style=font-size:80%>Wiki2</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T6.3.3.3.m1.1"><semantics id="S6.T6.3.3.3.m1.1a"><mo id="S6.T6.3.3.3.m1.1.1" mathsize="80%" stretchy="false" xref="S6.T6.3.3.3.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S6.T6.3.3.3.m1.1b"><ci id="S6.T6.3.3.3.m1.1.1.cmml" xref="S6.T6.3.3.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T6.3.3.3.m1.1d">‚Üì</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=S6.T6.4.4.4><span class=ltx_text id=S6.T6.4.4.4.1 style=font-size:80%>MATH</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T6.4.4.4.m1.1"><semantics id="S6.T6.4.4.4.m1.1a"><mo id="S6.T6.4.4.4.m1.1.1" mathsize="80%" stretchy="false" xref="S6.T6.4.4.4.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S6.T6.4.4.4.m1.1b"><ci id="S6.T6.4.4.4.m1.1.1.cmml" xref="S6.T6.4.4.4.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T6.4.4.4.m1.1d">‚Üë</annotation></semantics></math></td></tr><tr class=ltx_tr id=S6.T6.4.6><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S6.T6.4.6.1><span class=ltx_text id=S6.T6.4.6.1.1 style=font-size:80%>full</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S6.T6.4.6.2><span class=ltx_text id=S6.T6.4.6.2.1 style=font-size:80%>Baseline</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S6.T6.4.6.3><span class=ltx_text id=S6.T6.4.6.3.1 style=font-size:80%>-</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T6.4.6.4><span class=ltx_text id=S6.T6.4.6.4.1 style=font-size:80%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S6.T6.4.6.5><span class=ltx_text id=S6.T6.4.6.5.1 style=font-size:80%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S6.T6.4.6.6><span class=ltx_text id=S6.T6.4.6.6.1 style=font-size:80%>16</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S6.T6.4.6.7><span class=ltx_text id=S6.T6.4.6.7.1 style=font-size:80%>2.30</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S6.T6.4.6.8><span class=ltx_text id=S6.T6.4.6.8.1 style=font-size:80%>9.75</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S6.T6.4.6.9><span class=ltx_text id=S6.T6.4.6.9.1 style=font-size:80%>29.30</span></td></tr><tr class=ltx_tr id=S6.T6.4.7><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.7.1 rowspan=2><span class=ltx_text id=S6.T6.4.7.1.1 style=font-size:80%>2-bit</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.7.2><span class=ltx_text id=S6.T6.4.7.2.1 style=font-size:80%>QTIP</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.7.3><span class=ltx_text id=S6.T6.4.7.3.1 style=font-size:80%>RedPajama</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T6.4.7.4><span class=ltx_text id=S6.T6.4.7.4.1 style=font-size:80%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.7.5><span class=ltx_text id=S6.T6.4.7.5.1 style=font-size:80%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.7.6><span class=ltx_text id=S6.T6.4.7.6.1 style=font-size:80%>2.02</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.7.7><span class=ltx_text id=S6.T6.4.7.7.1 style=font-size:80%>1.40</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.7.8><span class=ltx_text id=S6.T6.4.7.8.1 style=font-size:80%>18.67</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T6.4.7.9><span class=ltx_text id=S6.T6.4.7.9.1 style=font-size:80%>0.78</span></td></tr><tr class=ltx_tr id=S6.T6.4.8><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T6.4.8.1><span class=ltx_text id=S6.T6.4.8.1.1 style=font-size:80%>QTIP+ReQuant</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T6.4.8.2><span class=ltx_text id=S6.T6.4.8.2.1 style=font-size:80%>RedPajama/WikiText-2/Tulu 3</span></td><td class="ltx_td ltx_align_center" id=S6.T6.4.8.3><span class=ltx_text id=S6.T6.4.8.3.1 style=font-size:80%>0</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T6.4.8.4><span class=ltx_text id=S6.T6.4.8.4.1 style=font-size:80%>0.5</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T6.4.8.5><span class=ltx_text id=S6.T6.4.8.5.1 style=font-size:80%>2.26</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T6.4.8.6><span class=ltx_text id=S6.T6.4.8.6.1 style=font-size:80%>1.47</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T6.4.8.7><span class="ltx_text ltx_font_bold" id=S6.T6.4.8.7.1 style=font-size:80%>16.01</span></td><td class="ltx_td ltx_align_center" id=S6.T6.4.8.8><span class="ltx_text ltx_font_bold" id=S6.T6.4.8.8.1 style=font-size:80%>2.68</span></td></tr><tr class=ltx_tr id=S6.T6.4.9><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.9.1 rowspan=2><span class=ltx_text id=S6.T6.4.9.1.1 style=font-size:80%>3-bit</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.9.2><span class=ltx_text id=S6.T6.4.9.2.1 style=font-size:80%>QTIP</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.9.3><span class=ltx_text id=S6.T6.4.9.3.1 style=font-size:80%>RedPajama-</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T6.4.9.4><span class=ltx_text id=S6.T6.4.9.4.1 style=font-size:80%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.9.5><span class=ltx_text id=S6.T6.4.9.5.1 style=font-size:80%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.9.6><span class=ltx_text id=S6.T6.4.9.6.1 style=font-size:80%>3.02</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.9.7><span class=ltx_text id=S6.T6.4.9.7.1 style=font-size:80%>1.72</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.9.8><span class=ltx_text id=S6.T6.4.9.8.1 style=font-size:80%>11.17</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T6.4.9.9><span class=ltx_text id=S6.T6.4.9.9.1 style=font-size:80%>18.78</span></td></tr><tr class=ltx_tr id=S6.T6.4.10><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T6.4.10.1><span class=ltx_text id=S6.T6.4.10.1.1 style=font-size:80%>QTIP+ReQuant</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T6.4.10.2><span class=ltx_text id=S6.T6.4.10.2.1 style=font-size:80%>RedPajama/WikiText-2/Tulu 3</span></td><td class="ltx_td ltx_align_center" id=S6.T6.4.10.3><span class=ltx_text id=S6.T6.4.10.3.1 style=font-size:80%>0</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T6.4.10.4><span class=ltx_text id=S6.T6.4.10.4.1 style=font-size:80%>0.5</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T6.4.10.5><span class=ltx_text id=S6.T6.4.10.5.1 style=font-size:80%>3.26</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T6.4.10.6><span class=ltx_text id=S6.T6.4.10.6.1 style=font-size:80%>1.80</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T6.4.10.7><span class="ltx_text ltx_font_bold" id=S6.T6.4.10.7.1 style=font-size:80%>10.83</span></td><td class="ltx_td ltx_align_center" id=S6.T6.4.10.8><span class="ltx_text ltx_font_bold" id=S6.T6.4.10.8.1 style=font-size:80%>20.06</span></td></tr><tr class=ltx_tr id=S6.T6.4.11><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id=S6.T6.4.11.1 rowspan=2><span class=ltx_text id=S6.T6.4.11.1.1 style=font-size:80%>4-bit</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.11.2><span class=ltx_text id=S6.T6.4.11.2.1 style=font-size:80%>QTIP</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.11.3><span class=ltx_text id=S6.T6.4.11.3.1 style=font-size:80%>RedPajama</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T6.4.11.4><span class=ltx_text id=S6.T6.4.11.4.1 style=font-size:80%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.11.5><span class=ltx_text id=S6.T6.4.11.5.1 style=font-size:80%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.11.6><span class=ltx_text id=S6.T6.4.11.6.1 style=font-size:80%>4.02</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.11.7><span class=ltx_text id=S6.T6.4.11.7.1 style=font-size:80%>2.05</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T6.4.11.8><span class=ltx_text id=S6.T6.4.11.8.1 style=font-size:80%>10.12</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S6.T6.4.11.9><span class=ltx_text id=S6.T6.4.11.9.1 style=font-size:80%>26.38</span></td></tr><tr class=ltx_tr id=S6.T6.4.12><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S6.T6.4.12.1><span class=ltx_text id=S6.T6.4.12.1.1 style=font-size:80%>QTIP+ReQuant</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S6.T6.4.12.2><span class=ltx_text id=S6.T6.4.12.2.1 style=font-size:80%>RedPajama/WikiText-2/Tulu 3</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S6.T6.4.12.3><span class=ltx_text id=S6.T6.4.12.3.1 style=font-size:80%>0</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S6.T6.4.12.4><span class=ltx_text id=S6.T6.4.12.4.1 style=font-size:80%>0.5</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S6.T6.4.12.5><span class=ltx_text id=S6.T6.4.12.5.1 style=font-size:80%>4.26</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S6.T6.4.12.6><span class=ltx_text id=S6.T6.4.12.6.1 style=font-size:80%>2.13</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S6.T6.4.12.7><span class="ltx_text ltx_font_bold" id=S6.T6.4.12.7.1 style=font-size:80%>10.06</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S6.T6.4.12.8><span class="ltx_text ltx_font_bold" id=S6.T6.4.12.8.1 style=font-size:80%>27.36</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the results of applying the QTIP (Quantization with Trellises and Incoherence Processing) method to the Llama 3.2 1B Base and Instruct models. It shows the performance metrics achieved at different precision levels (2-bit, 3-bit, 4-bit), using various calibration sets, and with the addition of the ReQuant method. Metrics include perplexity on the WikiText-2 benchmark and the MATH score, indicating performance on mathematical reasoning tasks. Sparsity refers to the proportion of weights that are sparsely stored instead of being fully quantized.</p><details><summary>read the caption</summary>Table 6: QTIP results for Llama 3.2 1B Base/Instruct models. The entries share the same meaning as Table¬†7.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id=S7.T7.6><tr class=ltx_tr id=S7.T7.6.7><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.6.7.1 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.7.1.1 style=font-size:70%>Llama 3.2 1B Base/Instruct</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan=3 id=S7.T7.6.7.2 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.7.2.1 style=font-size:70%>Hyperparameters</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan=4 id=S7.T7.6.7.3 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.7.3.1 style=font-size:70%>3-bit</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=4 id=S7.T7.6.7.4 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.7.4.1 style=font-size:70%>4-bit</span></td></tr><tr class=ltx_tr id=S7.T7.6.8><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.8.1 rowspan=2 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.8.1.1 style=font-size:70%>Method</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.8.2 rowspan=2 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.8.2.1 style=font-size:70%>Calib Set</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan=2 id=S7.T7.6.8.3 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.8.3.1 style=font-size:70%>Sparsity</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.8.4 rowspan=2 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.8.4.1 style=font-size:70%>Bits</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.8.5 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.8.5.1 style=font-size:70%>Mem</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.8.6 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.8.6.1 style=font-size:70%>Base</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.8.7 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.8.7.1 style=font-size:70%>Instruct</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.8.8 rowspan=2 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.8.8.1 style=font-size:70%>Bits</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.8.9 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.8.9.1 style=font-size:70%>Mem</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.8.10 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.8.10.1 style=font-size:70%>Base</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T7.6.8.11 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.8.11.1 style=font-size:70%>Instruct</span></td></tr><tr class=ltx_tr id=S7.T7.6.6><td class="ltx_td ltx_align_center" id=S7.T7.1.1.1 style=padding-left:7pt;padding-right:7pt><math alttext="r_{o}" class="ltx_Math" display="inline" id="S7.T7.1.1.1.m1.1"><semantics id="S7.T7.1.1.1.m1.1a"><msub id="S7.T7.1.1.1.m1.1.1" xref="S7.T7.1.1.1.m1.1.1.cmml"><mi id="S7.T7.1.1.1.m1.1.1.2" mathsize="70%" xref="S7.T7.1.1.1.m1.1.1.2.cmml">r</mi><mi id="S7.T7.1.1.1.m1.1.1.3" mathsize="70%" xref="S7.T7.1.1.1.m1.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S7.T7.1.1.1.m1.1b"><apply id="S7.T7.1.1.1.m1.1.1.cmml" xref="S7.T7.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.T7.1.1.1.m1.1.1.1.cmml" xref="S7.T7.1.1.1.m1.1.1">subscript</csymbol><ci id="S7.T7.1.1.1.m1.1.1.2.cmml" xref="S7.T7.1.1.1.m1.1.1.2">ùëü</ci><ci id="S7.T7.1.1.1.m1.1.1.3.cmml" xref="S7.T7.1.1.1.m1.1.1.3">ùëú</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T7.1.1.1.m1.1c">r_{o}</annotation><annotation encoding="application/x-llamapun" id="S7.T7.1.1.1.m1.1d">italic_r start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.2.2.2 style=padding-left:7pt;padding-right:7pt><math alttext="r_{s}" class="ltx_Math" display="inline" id="S7.T7.2.2.2.m1.1"><semantics id="S7.T7.2.2.2.m1.1a"><msub id="S7.T7.2.2.2.m1.1.1" xref="S7.T7.2.2.2.m1.1.1.cmml"><mi id="S7.T7.2.2.2.m1.1.1.2" mathsize="70%" xref="S7.T7.2.2.2.m1.1.1.2.cmml">r</mi><mi id="S7.T7.2.2.2.m1.1.1.3" mathsize="70%" xref="S7.T7.2.2.2.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S7.T7.2.2.2.m1.1b"><apply id="S7.T7.2.2.2.m1.1.1.cmml" xref="S7.T7.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S7.T7.2.2.2.m1.1.1.1.cmml" xref="S7.T7.2.2.2.m1.1.1">subscript</csymbol><ci id="S7.T7.2.2.2.m1.1.1.2.cmml" xref="S7.T7.2.2.2.m1.1.1.2">ùëü</ci><ci id="S7.T7.2.2.2.m1.1.1.3.cmml" xref="S7.T7.2.2.2.m1.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T7.2.2.2.m1.1c">r_{s}</annotation><annotation encoding="application/x-llamapun" id="S7.T7.2.2.2.m1.1d">italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.6.6.7 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.6.7.1 style=font-size:70%>(GB)</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.3.3.3 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.3.3.3.1 style=font-size:70%>Wiki2</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S7.T7.3.3.3.m1.1"><semantics id="S7.T7.3.3.3.m1.1a"><mo id="S7.T7.3.3.3.m1.1.1" mathsize="70%" stretchy="false" xref="S7.T7.3.3.3.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S7.T7.3.3.3.m1.1b"><ci id="S7.T7.3.3.3.m1.1.1.cmml" xref="S7.T7.3.3.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T7.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S7.T7.3.3.3.m1.1d">‚Üì</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.4.4.4 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.4.4.4.1 style=font-size:70%>MATH</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S7.T7.4.4.4.m1.1"><semantics id="S7.T7.4.4.4.m1.1a"><mo id="S7.T7.4.4.4.m1.1.1" mathsize="70%" stretchy="false" xref="S7.T7.4.4.4.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S7.T7.4.4.4.m1.1b"><ci id="S7.T7.4.4.4.m1.1.1.cmml" xref="S7.T7.4.4.4.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T7.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S7.T7.4.4.4.m1.1d">‚Üë</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.6.6.8 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.6.8.1 style=font-size:70%>(GB)</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.5.5.5 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.5.5.5.1 style=font-size:70%>Wiki2</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S7.T7.5.5.5.m1.1"><semantics id="S7.T7.5.5.5.m1.1a"><mo id="S7.T7.5.5.5.m1.1.1" mathsize="70%" stretchy="false" xref="S7.T7.5.5.5.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S7.T7.5.5.5.m1.1b"><ci id="S7.T7.5.5.5.m1.1.1.cmml" xref="S7.T7.5.5.5.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T7.5.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S7.T7.5.5.5.m1.1d">‚Üì</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=S7.T7.6.6.6 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.6.6.1 style=font-size:70%>MATH</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S7.T7.6.6.6.m1.1"><semantics id="S7.T7.6.6.6.m1.1a"><mo id="S7.T7.6.6.6.m1.1.1" mathsize="70%" stretchy="false" xref="S7.T7.6.6.6.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S7.T7.6.6.6.m1.1b"><ci id="S7.T7.6.6.6.m1.1.1.cmml" xref="S7.T7.6.6.6.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T7.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S7.T7.6.6.6.m1.1d">‚Üë</annotation></semantics></math></td></tr><tr class=ltx_tr id=S7.T7.6.9><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.6.9.1 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.9.1.1 style=font-size:70%>Baseline</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.6.9.2 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.9.2.1 style=font-size:70%>-</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S7.T7.6.9.3 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.9.3.1 style=font-size:70%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.6.9.4 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.9.4.1 style=font-size:70%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.6.9.5 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.9.5.1 style=font-size:70%>16</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.6.9.6 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.9.6.1 style=font-size:70%>2.30</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.6.9.7 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.9.7.1 style=font-size:70%>9.75</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.6.9.8 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.9.8.1 style=font-size:70%>29.30</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.6.9.9 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.9.9.1 style=font-size:70%>16</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.6.9.10 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.9.10.1 style=font-size:70%>2.30</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.6.9.11 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.9.11.1 style=font-size:70%>9.75</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S7.T7.6.9.12 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.9.12.1 style=font-size:70%>29.30</span></td></tr><tr class=ltx_tr id=S7.T7.6.10><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.10.1 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.10.1.1 style=font-size:70%>AWQ (g128)</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.10.2 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.10.2.1 style=font-size:70%>Pile</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T7.6.10.3 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.10.3.1 style=font-size:70%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.10.4 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.10.4.1 style=font-size:70%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.10.5 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.10.5.1 style=font-size:70%>3.25</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.10.6 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.10.6.1 style=font-size:70%>0.86</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.10.7 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.10.7.1 style=font-size:70%>16.74</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.10.8 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.10.8.1 style=font-size:70%>fail</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.10.9 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.10.9.1 style=font-size:70%>4.25</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.10.10 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.10.10.1 style=font-size:70%>0.97</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.10.11 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.10.11.1 style=font-size:70%>10.84</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T7.6.10.12 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.10.12.1 style=font-size:70%>22.82</span></td></tr><tr class=ltx_tr id=S7.T7.6.11><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.6.11.1 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.11.1.1 style=font-size:70%>AWQ (g256)+ReQuant</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.6.11.2 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.11.2.1 style=font-size:70%>Pile</span></td><td class="ltx_td ltx_align_center" id=S7.T7.6.11.3 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.11.3.1 style=font-size:70%>0.25</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.6.11.4 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.11.4.1 style=font-size:70%>0</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.6.11.5 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.11.5.1 style=font-size:70%>3.25</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.6.11.6 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.11.6.1 style=font-size:70%>0.86</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.6.11.7 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.11.7.1 style=font-size:70%>15.36</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.6.11.8 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.11.8.1 style=font-size:70%>fail</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.6.11.9 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.11.9.1 style=font-size:70%>4.25</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.6.11.10 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.11.10.1 style=font-size:70%>0.97</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.6.11.11 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.11.11.1 style=font-size:70%>10.65</span></td><td class="ltx_td ltx_align_center" id=S7.T7.6.11.12 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.11.12.1 style=font-size:70%>24.32</span></td></tr><tr class=ltx_tr id=S7.T7.6.12><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.12.1 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.12.1.1 style=font-size:70%>SqueezeLLM</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.12.2 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.12.2.1 style=font-size:70%>WikiText-2</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T7.6.12.3 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.12.3.1 style=font-size:70%>0.45</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.12.4 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.12.4.1 style=font-size:70%>0.05</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.12.5 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.12.5.1 style=font-size:70%>3.25</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.12.6 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.12.6.1 style=font-size:70%>0.86</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.12.7 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.12.7.1 style=font-size:70%>13.86</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.12.8 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.12.8.1 style=font-size:70%>11.28</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.12.9 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.12.9.1 style=font-size:70%>4.25</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.12.10 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.12.10.1 style=font-size:70%>0.97</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.6.12.11 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.12.11.1 style=font-size:70%>10.51</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T7.6.12.12 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.12.12.1 style=font-size:70%>fail</span></td></tr><tr class=ltx_tr id=S7.T7.6.13><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.6.13.1 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.13.1.1 style=font-size:70%>SqueezeLLM+ReQuant</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.6.13.2 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.13.2.1 style=font-size:70%>WikiText-2</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S7.T7.6.13.3 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.13.3.1 style=font-size:70%>0.45</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.6.13.4 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.13.4.1 style=font-size:70%>0.05</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.6.13.5 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.13.5.1 style=font-size:70%>3.25</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.6.13.6 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.13.6.1 style=font-size:70%>0.86</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.6.13.7 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.13.7.1 style=font-size:70%>13.30</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.6.13.8 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.13.8.1 style=font-size:70%>14.18</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.6.13.9 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.13.9.1 style=font-size:70%>4.25</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.6.13.10 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.6.13.10.1 style=font-size:70%>0.97</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.6.13.11 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.13.11.1 style=font-size:70%>10.43</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S7.T7.6.13.12 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.6.13.12.1 style=font-size:70%>24.74</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the results of evaluating various quantization methods (AWQ, SqueezeLLM, and QTIP) on Llama language models (3.2B and 3B). It shows the WikiText-2 perplexity for base models and the 4-shot MATH (Mathematical Problem Solving) evaluation score for instruction following models. The table compares baseline performance with the results obtained after applying the proposed ReQuant method. &lsquo;Fail&rsquo; indicates instances where the model&rsquo;s output could not be properly parsed due to errors.</p><details><summary>read the caption</summary>Table 7: WikiText-2 perplexity for base models and 4-shot MATH evaluation for instruction following models. ‚ÄúFail‚Äù means failure to parse model‚Äôs output due to garbled characters.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id=S7.T7.12><tr class=ltx_tr id=S7.T7.12.7><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.12.7.1 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.7.1.1 style=font-size:70%>Llama 3.2 3B Base/Instruct</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan=3 id=S7.T7.12.7.2 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.7.2.1 style=font-size:70%>Hyperparameters</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan=4 id=S7.T7.12.7.3 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.7.3.1 style=font-size:70%>3-bit</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=4 id=S7.T7.12.7.4 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.7.4.1 style=font-size:70%>4-bit</span></td></tr><tr class=ltx_tr id=S7.T7.12.8><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.8.1 rowspan=2 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.8.1.1 style=font-size:70%>Method</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.8.2 rowspan=2 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.8.2.1 style=font-size:70%>Calib Set</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan=2 id=S7.T7.12.8.3 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.8.3.1 style=font-size:70%>Sparsity</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.8.4 rowspan=2 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.8.4.1 style=font-size:70%>Bits</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.8.5 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.8.5.1 style=font-size:70%>Mem</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.8.6 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.8.6.1 style=font-size:70%>Base</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.8.7 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.8.7.1 style=font-size:70%>Instruct</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.8.8 rowspan=2 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.8.8.1 style=font-size:70%>Bits</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.8.9 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.8.9.1 style=font-size:70%>Mem</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.8.10 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.8.10.1 style=font-size:70%>Base</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T7.12.8.11 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.8.11.1 style=font-size:70%>Instruct</span></td></tr><tr class=ltx_tr id=S7.T7.12.6><td class="ltx_td ltx_align_center" id=S7.T7.7.1.1 style=padding-left:7pt;padding-right:7pt><math alttext="r_{o}" class="ltx_Math" display="inline" id="S7.T7.7.1.1.m1.1"><semantics id="S7.T7.7.1.1.m1.1a"><msub id="S7.T7.7.1.1.m1.1.1" xref="S7.T7.7.1.1.m1.1.1.cmml"><mi id="S7.T7.7.1.1.m1.1.1.2" mathsize="70%" xref="S7.T7.7.1.1.m1.1.1.2.cmml">r</mi><mi id="S7.T7.7.1.1.m1.1.1.3" mathsize="70%" xref="S7.T7.7.1.1.m1.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S7.T7.7.1.1.m1.1b"><apply id="S7.T7.7.1.1.m1.1.1.cmml" xref="S7.T7.7.1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.T7.7.1.1.m1.1.1.1.cmml" xref="S7.T7.7.1.1.m1.1.1">subscript</csymbol><ci id="S7.T7.7.1.1.m1.1.1.2.cmml" xref="S7.T7.7.1.1.m1.1.1.2">ùëü</ci><ci id="S7.T7.7.1.1.m1.1.1.3.cmml" xref="S7.T7.7.1.1.m1.1.1.3">ùëú</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T7.7.1.1.m1.1c">r_{o}</annotation><annotation encoding="application/x-llamapun" id="S7.T7.7.1.1.m1.1d">italic_r start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.8.2.2 style=padding-left:7pt;padding-right:7pt><math alttext="r_{s}" class="ltx_Math" display="inline" id="S7.T7.8.2.2.m1.1"><semantics id="S7.T7.8.2.2.m1.1a"><msub id="S7.T7.8.2.2.m1.1.1" xref="S7.T7.8.2.2.m1.1.1.cmml"><mi id="S7.T7.8.2.2.m1.1.1.2" mathsize="70%" xref="S7.T7.8.2.2.m1.1.1.2.cmml">r</mi><mi id="S7.T7.8.2.2.m1.1.1.3" mathsize="70%" xref="S7.T7.8.2.2.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S7.T7.8.2.2.m1.1b"><apply id="S7.T7.8.2.2.m1.1.1.cmml" xref="S7.T7.8.2.2.m1.1.1"><csymbol cd="ambiguous" id="S7.T7.8.2.2.m1.1.1.1.cmml" xref="S7.T7.8.2.2.m1.1.1">subscript</csymbol><ci id="S7.T7.8.2.2.m1.1.1.2.cmml" xref="S7.T7.8.2.2.m1.1.1.2">ùëü</ci><ci id="S7.T7.8.2.2.m1.1.1.3.cmml" xref="S7.T7.8.2.2.m1.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T7.8.2.2.m1.1c">r_{s}</annotation><annotation encoding="application/x-llamapun" id="S7.T7.8.2.2.m1.1d">italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.12.6.7 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.6.7.1 style=font-size:70%>(GB)</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.9.3.3 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.9.3.3.1 style=font-size:70%>Wiki2</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S7.T7.9.3.3.m1.1"><semantics id="S7.T7.9.3.3.m1.1a"><mo id="S7.T7.9.3.3.m1.1.1" mathsize="70%" stretchy="false" xref="S7.T7.9.3.3.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S7.T7.9.3.3.m1.1b"><ci id="S7.T7.9.3.3.m1.1.1.cmml" xref="S7.T7.9.3.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T7.9.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S7.T7.9.3.3.m1.1d">‚Üì</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.10.4.4 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.10.4.4.1 style=font-size:70%>MATH</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S7.T7.10.4.4.m1.1"><semantics id="S7.T7.10.4.4.m1.1a"><mo id="S7.T7.10.4.4.m1.1.1" mathsize="70%" stretchy="false" xref="S7.T7.10.4.4.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S7.T7.10.4.4.m1.1b"><ci id="S7.T7.10.4.4.m1.1.1.cmml" xref="S7.T7.10.4.4.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T7.10.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S7.T7.10.4.4.m1.1d">‚Üë</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.12.6.8 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.6.8.1 style=font-size:70%>(GB)</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.11.5.5 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.11.5.5.1 style=font-size:70%>Wiki2</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S7.T7.11.5.5.m1.1"><semantics id="S7.T7.11.5.5.m1.1a"><mo id="S7.T7.11.5.5.m1.1.1" mathsize="70%" stretchy="false" xref="S7.T7.11.5.5.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S7.T7.11.5.5.m1.1b"><ci id="S7.T7.11.5.5.m1.1.1.cmml" xref="S7.T7.11.5.5.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T7.11.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S7.T7.11.5.5.m1.1d">‚Üì</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=S7.T7.12.6.6 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.6.6.1 style=font-size:70%>MATH</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S7.T7.12.6.6.m1.1"><semantics id="S7.T7.12.6.6.m1.1a"><mo id="S7.T7.12.6.6.m1.1.1" mathsize="70%" stretchy="false" xref="S7.T7.12.6.6.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S7.T7.12.6.6.m1.1b"><ci id="S7.T7.12.6.6.m1.1.1.cmml" xref="S7.T7.12.6.6.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T7.12.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S7.T7.12.6.6.m1.1d">‚Üë</annotation></semantics></math></td></tr><tr class=ltx_tr id=S7.T7.12.9><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.12.9.1 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.9.1.1 style=font-size:70%>Baseline</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.12.9.2 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.9.2.1 style=font-size:70%>-</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S7.T7.12.9.3 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.9.3.1 style=font-size:70%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.12.9.4 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.9.4.1 style=font-size:70%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.12.9.5 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.9.5.1 style=font-size:70%>16</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.12.9.6 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.9.6.1 style=font-size:70%>5.98</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.12.9.7 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.9.7.1 style=font-size:70%>7.81</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.12.9.8 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.9.8.1 style=font-size:70%>44.92</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.12.9.9 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.9.9.1 style=font-size:70%>16</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.12.9.10 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.9.10.1 style=font-size:70%>5.98</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S7.T7.12.9.11 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.9.11.1 style=font-size:70%>7.81</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S7.T7.12.9.12 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.9.12.1 style=font-size:70%>44.92</span></td></tr><tr class=ltx_tr id=S7.T7.12.10><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.10.1 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.10.1.1 style=font-size:70%>AWQ (g128)</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.10.2 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.10.2.1 style=font-size:70%>Pile</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T7.12.10.3 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.10.3.1 style=font-size:70%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.10.4 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.10.4.1 style=font-size:70%>-</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.10.5 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.10.5.1 style=font-size:70%>3.25</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.10.6 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.10.6.1 style=font-size:70%>1.80</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.10.7 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.10.7.1 style=font-size:70%>10.30</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.10.8 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.10.8.1 style=font-size:70%>29.64</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.10.9 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.10.9.1 style=font-size:70%>4.25</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.10.10 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.10.10.1 style=font-size:70%>2.13</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.10.11 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.10.11.1 style=font-size:70%>8.22</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T7.12.10.12 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.10.12.1 style=font-size:70%>42.88</span></td></tr><tr class=ltx_tr id=S7.T7.12.11><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.12.11.1 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.11.1.1 style=font-size:70%>AWQ (g256)+ReQuant</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.12.11.2 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.11.2.1 style=font-size:70%>Pile</span></td><td class="ltx_td ltx_align_center" id=S7.T7.12.11.3 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.11.3.1 style=font-size:70%>0.25</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.12.11.4 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.11.4.1 style=font-size:70%>0</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.12.11.5 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.11.5.1 style=font-size:70%>3.24</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.12.11.6 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.11.6.1 style=font-size:70%>1.80</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.12.11.7 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.11.7.1 style=font-size:70%>9.98</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.12.11.8 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.11.8.1 style=font-size:70%>35.08</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.12.11.9 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.11.9.1 style=font-size:70%>4.24</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.12.11.10 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.11.10.1 style=font-size:70%>2.13</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S7.T7.12.11.11 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.11.11.1 style=font-size:70%>8.20</span></td><td class="ltx_td ltx_align_center" id=S7.T7.12.11.12 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.11.12.1 style=font-size:70%>42.20</span></td></tr><tr class=ltx_tr id=S7.T7.12.12><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.12.1 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.12.1.1 style=font-size:70%>SqueezeLLM</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.12.2 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.12.2.1 style=font-size:70%>WikiText-2</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T7.12.12.3 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.12.3.1 style=font-size:70%>0.45</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.12.4 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.12.4.1 style=font-size:70%>0.05</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.12.5 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.12.5.1 style=font-size:70%>3.24</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.12.6 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.12.6.1 style=font-size:70%>1.80</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.12.7 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.12.7.1 style=font-size:70%>9.39</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.12.8 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.12.8.1 style=font-size:70%>33.80</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.12.9 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.12.9.1 style=font-size:70%>4.24</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.12.10 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.12.10.1 style=font-size:70%>2.13</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S7.T7.12.12.11 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.12.11.1 style=font-size:70%>8.12</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T7.12.12.12 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.12.12.1 style=font-size:70%>43.06</span></td></tr><tr class=ltx_tr id=S7.T7.12.13><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.12.13.1 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.13.1.1 style=font-size:70%>SqueezeLLM+ReQuant</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.12.13.2 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.13.2.1 style=font-size:70%>WikiText-2</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S7.T7.12.13.3 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.13.3.1 style=font-size:70%>0.45</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.12.13.4 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.13.4.1 style=font-size:70%>0.05</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.12.13.5 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.13.5.1 style=font-size:70%>3.24</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.12.13.6 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.13.6.1 style=font-size:70%>1.80</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.12.13.7 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.13.7.1 style=font-size:70%>9.47</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.12.13.8 style=padding-left:7pt;padding-right:7pt><span class="ltx_text ltx_font_bold" id=S7.T7.12.13.8.1 style=font-size:70%>35.34</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.12.13.9 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.13.9.1 style=font-size:70%>4.24</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.12.13.10 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.13.10.1 style=font-size:70%>2.13</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S7.T7.12.13.11 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.13.11.1 style=font-size:70%>8.14</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S7.T7.12.13.12 style=padding-left:7pt;padding-right:7pt><span class=ltx_text id=S7.T7.12.13.12.1 style=font-size:70%>42.24</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the ablation study results on the WikiText-2 dataset, focusing on the impact of outlier selection and significant weight detach on perplexity. It compares the performance of the proposed ReQuant method against baselines, evaluating different settings for outlier selection and gradual weight detachment. The &lsquo;rand&rsquo; row provides a control, where outliers and significant weights are randomly selected, highlighting the effectiveness of the proposed selection strategy.</p><details><summary>read the caption</summary>Table 8: Ablation results on WikiText-2 perplexity. The ‚Äúrand‚Äù line indicates that ùê∞osubscriptùê∞ùëú\bm{\mathbf{w}}_{o}bold_w start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT and ùê∞ssubscriptùê∞ùë†\bm{\mathbf{w}}_{s}bold_w start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT are picked out randomly from the weights.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S7.T8.9.5><tr class=ltx_tr id=S7.T8.6.2.2><td class="ltx_td ltx_align_center ltx_border_tt" id=S7.T8.5.1.1.1><math alttext="r_{o}" class="ltx_Math" display="inline" id="S7.T8.5.1.1.1.m1.1"><semantics id="S7.T8.5.1.1.1.m1.1a"><msub id="S7.T8.5.1.1.1.m1.1.1" xref="S7.T8.5.1.1.1.m1.1.1.cmml"><mi id="S7.T8.5.1.1.1.m1.1.1.2" xref="S7.T8.5.1.1.1.m1.1.1.2.cmml">r</mi><mi id="S7.T8.5.1.1.1.m1.1.1.3" xref="S7.T8.5.1.1.1.m1.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S7.T8.5.1.1.1.m1.1b"><apply id="S7.T8.5.1.1.1.m1.1.1.cmml" xref="S7.T8.5.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.T8.5.1.1.1.m1.1.1.1.cmml" xref="S7.T8.5.1.1.1.m1.1.1">subscript</csymbol><ci id="S7.T8.5.1.1.1.m1.1.1.2.cmml" xref="S7.T8.5.1.1.1.m1.1.1.2">ùëü</ci><ci id="S7.T8.5.1.1.1.m1.1.1.3.cmml" xref="S7.T8.5.1.1.1.m1.1.1.3">ùëú</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T8.5.1.1.1.m1.1c">r_{o}</annotation><annotation encoding="application/x-llamapun" id="S7.T8.5.1.1.1.m1.1d">italic_r start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S7.T8.6.2.2.2><math alttext="r_{s}" class="ltx_Math" display="inline" id="S7.T8.6.2.2.2.m1.1"><semantics id="S7.T8.6.2.2.2.m1.1a"><msub id="S7.T8.6.2.2.2.m1.1.1" xref="S7.T8.6.2.2.2.m1.1.1.cmml"><mi id="S7.T8.6.2.2.2.m1.1.1.2" xref="S7.T8.6.2.2.2.m1.1.1.2.cmml">r</mi><mi id="S7.T8.6.2.2.2.m1.1.1.3" xref="S7.T8.6.2.2.2.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S7.T8.6.2.2.2.m1.1b"><apply id="S7.T8.6.2.2.2.m1.1.1.cmml" xref="S7.T8.6.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S7.T8.6.2.2.2.m1.1.1.1.cmml" xref="S7.T8.6.2.2.2.m1.1.1">subscript</csymbol><ci id="S7.T8.6.2.2.2.m1.1.1.2.cmml" xref="S7.T8.6.2.2.2.m1.1.1.2">ùëü</ci><ci id="S7.T8.6.2.2.2.m1.1.1.3.cmml" xref="S7.T8.6.2.2.2.m1.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T8.6.2.2.2.m1.1c">r_{s}</annotation><annotation encoding="application/x-llamapun" id="S7.T8.6.2.2.2.m1.1d">italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math></td><td class="ltx_td ltx_align_left ltx_border_tt" id=S7.T8.6.2.2.3><span class="ltx_text ltx_font_bold" id=S7.T8.6.2.2.3.1>Comment</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S7.T8.6.2.2.4><span class="ltx_text ltx_font_bold" id=S7.T8.6.2.2.4.1>Train PPL</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S7.T8.6.2.2.5><span class="ltx_text ltx_font_bold" id=S7.T8.6.2.2.5.1>Test PPL</span></td></tr><tr class=ltx_tr id=S7.T8.9.5.6><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.9.5.6.1>-</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.9.5.6.2>-</td><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T8.9.5.6.3>bfloat16</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.9.5.6.4>10.20</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.9.5.6.5>9.75</td></tr><tr class=ltx_tr id=S7.T8.9.5.7><td class="ltx_td ltx_align_center" id=S7.T8.9.5.7.1>0.45</td><td class="ltx_td ltx_align_center" id=S7.T8.9.5.7.2>0.05</td><td class=ltx_td id=S7.T8.9.5.7.3></td><td class="ltx_td ltx_align_center" id=S7.T8.9.5.7.4>10.95</td><td class="ltx_td ltx_align_center" id=S7.T8.9.5.7.5>10.45</td></tr><tr class=ltx_tr id=S7.T8.9.5.8><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.9.5.8.1>0.45</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.9.5.8.2>0</td><td class="ltx_td ltx_border_t" id=S7.T8.9.5.8.3></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.9.5.8.4>11.02</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.9.5.8.5>10.52</td></tr><tr class=ltx_tr id=S7.T8.9.5.9><td class="ltx_td ltx_align_center" id=S7.T8.9.5.9.1>0.45</td><td class="ltx_td ltx_align_center" id=S7.T8.9.5.9.2>0</td><td class="ltx_td ltx_align_left" id=S7.T8.9.5.9.3>SqueezeLLM</td><td class="ltx_td ltx_align_center" id=S7.T8.9.5.9.4>11.15</td><td class="ltx_td ltx_align_center" id=S7.T8.9.5.9.5>10.62</td></tr><tr class=ltx_tr id=S7.T8.9.5.10><td class="ltx_td ltx_align_center" id=S7.T8.9.5.10.1>0</td><td class="ltx_td ltx_align_center" id=S7.T8.9.5.10.2>0.05</td><td class=ltx_td id=S7.T8.9.5.10.3></td><td class="ltx_td ltx_align_center" id=S7.T8.9.5.10.4>11.15</td><td class="ltx_td ltx_align_center" id=S7.T8.9.5.10.5>10.65</td></tr><tr class=ltx_tr id=S7.T8.9.5.11><td class="ltx_td ltx_align_center" id=S7.T8.9.5.11.1>0</td><td class="ltx_td ltx_align_center" id=S7.T8.9.5.11.2>0</td><td class=ltx_td id=S7.T8.9.5.11.3></td><td class="ltx_td ltx_align_center" id=S7.T8.9.5.11.4>11.30</td><td class="ltx_td ltx_align_center" id=S7.T8.9.5.11.5>10.80</td></tr><tr class=ltx_tr id=S7.T8.9.5.12><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.9.5.12.1>0.45</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.9.5.12.2>0.05</td><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T8.9.5.12.3>rand</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.9.5.12.4>11.28</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.9.5.12.5>10.77</td></tr><tr class=ltx_tr id=S7.T8.7.3.3><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.7.3.3.2>0.45</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.7.3.3.3>0.05</td><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T8.7.3.3.1><math alttext="\beta=0.0125" class="ltx_Math" display="inline" id="S7.T8.7.3.3.1.m1.1"><semantics id="S7.T8.7.3.3.1.m1.1a"><mrow id="S7.T8.7.3.3.1.m1.1.1" xref="S7.T8.7.3.3.1.m1.1.1.cmml"><mi id="S7.T8.7.3.3.1.m1.1.1.2" xref="S7.T8.7.3.3.1.m1.1.1.2.cmml">Œ≤</mi><mo id="S7.T8.7.3.3.1.m1.1.1.1" xref="S7.T8.7.3.3.1.m1.1.1.1.cmml">=</mo><mn id="S7.T8.7.3.3.1.m1.1.1.3" xref="S7.T8.7.3.3.1.m1.1.1.3.cmml">0.0125</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T8.7.3.3.1.m1.1b"><apply id="S7.T8.7.3.3.1.m1.1.1.cmml" xref="S7.T8.7.3.3.1.m1.1.1"><eq id="S7.T8.7.3.3.1.m1.1.1.1.cmml" xref="S7.T8.7.3.3.1.m1.1.1.1"></eq><ci id="S7.T8.7.3.3.1.m1.1.1.2.cmml" xref="S7.T8.7.3.3.1.m1.1.1.2">ùõΩ</ci><cn id="S7.T8.7.3.3.1.m1.1.1.3.cmml" type="float" xref="S7.T8.7.3.3.1.m1.1.1.3">0.0125</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T8.7.3.3.1.m1.1c">\beta=0.0125</annotation><annotation encoding="application/x-llamapun" id="S7.T8.7.3.3.1.m1.1d">italic_Œ≤ = 0.0125</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.7.3.3.4>10.94</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T8.7.3.3.5>10.42</td></tr><tr class=ltx_tr id=S7.T8.8.4.4><td class="ltx_td ltx_align_center" id=S7.T8.8.4.4.2>0.45</td><td class="ltx_td ltx_align_center" id=S7.T8.8.4.4.3>0.05</td><td class="ltx_td ltx_align_left" id=S7.T8.8.4.4.1><math alttext="\beta=0.025" class="ltx_Math" display="inline" id="S7.T8.8.4.4.1.m1.1"><semantics id="S7.T8.8.4.4.1.m1.1a"><mrow id="S7.T8.8.4.4.1.m1.1.1" xref="S7.T8.8.4.4.1.m1.1.1.cmml"><mi id="S7.T8.8.4.4.1.m1.1.1.2" xref="S7.T8.8.4.4.1.m1.1.1.2.cmml">Œ≤</mi><mo id="S7.T8.8.4.4.1.m1.1.1.1" xref="S7.T8.8.4.4.1.m1.1.1.1.cmml">=</mo><mn id="S7.T8.8.4.4.1.m1.1.1.3" xref="S7.T8.8.4.4.1.m1.1.1.3.cmml">0.025</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T8.8.4.4.1.m1.1b"><apply id="S7.T8.8.4.4.1.m1.1.1.cmml" xref="S7.T8.8.4.4.1.m1.1.1"><eq id="S7.T8.8.4.4.1.m1.1.1.1.cmml" xref="S7.T8.8.4.4.1.m1.1.1.1"></eq><ci id="S7.T8.8.4.4.1.m1.1.1.2.cmml" xref="S7.T8.8.4.4.1.m1.1.1.2">ùõΩ</ci><cn id="S7.T8.8.4.4.1.m1.1.1.3.cmml" type="float" xref="S7.T8.8.4.4.1.m1.1.1.3">0.025</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T8.8.4.4.1.m1.1c">\beta=0.025</annotation><annotation encoding="application/x-llamapun" id="S7.T8.8.4.4.1.m1.1d">italic_Œ≤ = 0.025</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=S7.T8.8.4.4.4>10.94</td><td class="ltx_td ltx_align_center" id=S7.T8.8.4.4.5>10.42</td></tr><tr class=ltx_tr id=S7.T8.9.5.5><td class="ltx_td ltx_align_center ltx_border_bb" id=S7.T8.9.5.5.2>0.45</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S7.T8.9.5.5.3>0.05</td><td class="ltx_td ltx_align_left ltx_border_bb" id=S7.T8.9.5.5.1><math alttext="\beta=0.05" class="ltx_Math" display="inline" id="S7.T8.9.5.5.1.m1.1"><semantics id="S7.T8.9.5.5.1.m1.1a"><mrow id="S7.T8.9.5.5.1.m1.1.1" xref="S7.T8.9.5.5.1.m1.1.1.cmml"><mi id="S7.T8.9.5.5.1.m1.1.1.2" xref="S7.T8.9.5.5.1.m1.1.1.2.cmml">Œ≤</mi><mo id="S7.T8.9.5.5.1.m1.1.1.1" xref="S7.T8.9.5.5.1.m1.1.1.1.cmml">=</mo><mn id="S7.T8.9.5.5.1.m1.1.1.3" xref="S7.T8.9.5.5.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T8.9.5.5.1.m1.1b"><apply id="S7.T8.9.5.5.1.m1.1.1.cmml" xref="S7.T8.9.5.5.1.m1.1.1"><eq id="S7.T8.9.5.5.1.m1.1.1.1.cmml" xref="S7.T8.9.5.5.1.m1.1.1.1"></eq><ci id="S7.T8.9.5.5.1.m1.1.1.2.cmml" xref="S7.T8.9.5.5.1.m1.1.1.2">ùõΩ</ci><cn id="S7.T8.9.5.5.1.m1.1.1.3.cmml" type="float" xref="S7.T8.9.5.5.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T8.9.5.5.1.m1.1c">\beta=0.05</annotation><annotation encoding="application/x-llamapun" id="S7.T8.9.5.5.1.m1.1d">italic_Œ≤ = 0.05</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S7.T8.9.5.5.4>10.95</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S7.T8.9.5.5.5>10.45</td></tr></table></table></figure><blockquote><p>üîº This table presents the inference speed comparison between different quantization methods with and without the proposed ReQuant technique. It shows the time taken for pre-filling (preparing data), decoding (processing the data), and the total inference time for various model sizes and precision levels. The results are useful for assessing the performance impact of ReQuant on inference speed and for understanding the tradeoff between accuracy and speed.</p><details><summary>read the caption</summary>Table 9: Inference speed of Dense-and-Sparse decomposition.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S7.T9.1.1><tr class=ltx_tr id=S7.T9.1.1.1><td class="ltx_td ltx_align_left ltx_border_tt" id=S7.T9.1.1.1.1 rowspan=2><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.1.1.1>Model</span></td><td class="ltx_td ltx_align_left ltx_border_tt" id=S7.T9.1.1.1.2 rowspan=2><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.1.2.1>Precision</span></td><td class="ltx_td ltx_align_left ltx_border_tt" id=S7.T9.1.1.1.3 rowspan=2><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.1.3.1>Method</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S7.T9.1.1.1.4><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.1.4.1>Prefilling</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S7.T9.1.1.1.5><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.1.5.1>Decoding</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S7.T9.1.1.1.6><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.1.6.1>Total</span></td></tr><tr class=ltx_tr id=S7.T9.1.1.2><td class="ltx_td ltx_align_center" id=S7.T9.1.1.2.1>(ms)</td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.2.2>(ms)</td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.2.3>(ms)</td></tr><tr class=ltx_tr id=S7.T9.1.1.3><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.3.1>1B</td><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.3.2>4-bit</td><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.3.3>AWQ</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.3.4><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.3.4.1>13</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.3.5><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.3.5.1>23768</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.3.6><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.3.6.1>23781</span></td></tr><tr class=ltx_tr id=S7.T9.1.1.4><td class="ltx_td ltx_align_left" id=S7.T9.1.1.4.1>1B</td><td class="ltx_td ltx_align_left" id=S7.T9.1.1.4.2>4-bit</td><td class="ltx_td ltx_align_left" id=S7.T9.1.1.4.3>AWQ+PQI</td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.4.4>18</td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.4.5>35204</td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.4.6>35222</td></tr><tr class=ltx_tr id=S7.T9.1.1.5><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.5.1>1B</td><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.5.2>4-bit</td><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.5.3>SqueezeLLM</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.5.4>86</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.5.5>47151</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.5.6>47237</td></tr><tr class=ltx_tr id=S7.T9.1.1.6><td class="ltx_td ltx_align_left" id=S7.T9.1.1.6.1>1B</td><td class="ltx_td ltx_align_left" id=S7.T9.1.1.6.2>4-bit</td><td class="ltx_td ltx_align_left" id=S7.T9.1.1.6.3>SqueezeLLM+ReQuant</td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.6.4><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.6.4.1>29</span></td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.6.5><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.6.5.1>45266</span></td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.6.6><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.6.6.1>45295</span></td></tr><tr class=ltx_tr id=S7.T9.1.1.7><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.7.1>1B</td><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.7.2>3-bit</td><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.7.3>SqueezeLLM</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.7.4><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.7.4.1>85</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.7.5>33657</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.7.6>33742</td></tr><tr class=ltx_tr id=S7.T9.1.1.8><td class="ltx_td ltx_align_left" id=S7.T9.1.1.8.1>1B</td><td class="ltx_td ltx_align_left" id=S7.T9.1.1.8.2>3-bit</td><td class="ltx_td ltx_align_left" id=S7.T9.1.1.8.3>SqueezeLLM+ReQuant</td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.8.4>86</td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.8.5><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.8.5.1>32568</span></td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.8.6><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.8.6.1>32654</span></td></tr><tr class=ltx_tr id=S7.T9.1.1.9><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.9.1>3B</td><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.9.2>4-bit</td><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.9.3>AWQ</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.9.4>31</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.9.5>59631</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.9.6>59662</td></tr><tr class=ltx_tr id=S7.T9.1.1.10><td class="ltx_td ltx_align_left" id=S7.T9.1.1.10.1>3B</td><td class="ltx_td ltx_align_left" id=S7.T9.1.1.10.2>4-bit</td><td class="ltx_td ltx_align_left" id=S7.T9.1.1.10.3>AWQ+ReQuant</td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.10.4>31</td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.10.5><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.10.5.1>59174</span></td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.10.6><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.10.6.1>59205</span></td></tr><tr class=ltx_tr id=S7.T9.1.1.11><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.11.1>3B</td><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.11.2>4-bit</td><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.11.3>SqueezeLLM</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.11.4>230</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.11.5>56882</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.11.6>57112</td></tr><tr class=ltx_tr id=S7.T9.1.1.12><td class="ltx_td ltx_align_left" id=S7.T9.1.1.12.1>3B</td><td class="ltx_td ltx_align_left" id=S7.T9.1.1.12.2>4-bit</td><td class="ltx_td ltx_align_left" id=S7.T9.1.1.12.3>SqueezeLLM+ReQuant</td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.12.4><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.12.4.1>68</span></td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.12.5><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.12.5.1>56372</span></td><td class="ltx_td ltx_align_center" id=S7.T9.1.1.12.6><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.12.6.1>56440</span></td></tr><tr class=ltx_tr id=S7.T9.1.1.13><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.13.1>3B</td><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.13.2>3-bit</td><td class="ltx_td ltx_align_left ltx_border_t" id=S7.T9.1.1.13.3>SqueezeLLM</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.13.4>229</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.13.5>56343</td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T9.1.1.13.6>56572</td></tr><tr class=ltx_tr id=S7.T9.1.1.14><td class="ltx_td ltx_align_left ltx_border_bb" id=S7.T9.1.1.14.1>3B</td><td class="ltx_td ltx_align_left ltx_border_bb" id=S7.T9.1.1.14.2>3-bit</td><td class="ltx_td ltx_align_left ltx_border_bb" id=S7.T9.1.1.14.3>SqueezeLLM+ReQuant</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S7.T9.1.1.14.4>229</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S7.T9.1.1.14.5><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.14.5.1>54640</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S7.T9.1.1.14.6><span class="ltx_text ltx_font_bold" id=S7.T9.1.1.14.6.1>54869</span></td></tr></table></table></figure><blockquote><p>üîº This table lists the hyperparameters used in the experiments for three different post-training quantization methods: AWQ, SqueezeLLM, and QTIP. For each method, it specifies the calibration dataset used, the sequence length of the calibration data, the number of intervals (N) used for the numerical integration in PQI, and the number of times (n) the dataset was sampled for calculations. It also gives the parameters rs and Œ≤ used in the ReQuant method, representing the percentage of weights detached and the step size for that detachment respectively.</p><details><summary>read the caption</summary>Table 10: Experimental hyperparameters.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=A2.T10.8.8><tr class=ltx_tr id=A2.T10.8.8.9><td class="ltx_td ltx_align_left ltx_border_tt" id=A2.T10.8.8.9.1><span class="ltx_text ltx_font_bold" id=A2.T10.8.8.9.1.1>Setting</span></td><td class="ltx_td ltx_align_left ltx_border_tt" id=A2.T10.8.8.9.2><span class="ltx_text ltx_font_bold" id=A2.T10.8.8.9.2.1>Hyperparameter</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=A2.T10.8.8.9.3><span class="ltx_text ltx_font_bold" id=A2.T10.8.8.9.3.1>Value</span></td></tr><tr class=ltx_tr id=A2.T10.8.8.10><td class="ltx_td ltx_align_left ltx_border_t" id=A2.T10.8.8.10.1 rowspan=5><span class=ltx_text id=A2.T10.8.8.10.1.1>AWQ</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=A2.T10.8.8.10.2>calib set (all)</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T10.8.8.10.3>Pile</td></tr><tr class=ltx_tr id=A2.T10.8.8.11><td class="ltx_td ltx_align_left" id=A2.T10.8.8.11.1>calib sequence length</td><td class="ltx_td ltx_align_center" id=A2.T10.8.8.11.2>2048</td></tr><tr class=ltx_tr id=A2.T10.1.1.1><td class="ltx_td ltx_align_left" id=A2.T10.1.1.1.1><math alttext="N" class="ltx_Math" display="inline" id="A2.T10.1.1.1.1.m1.1"><semantics id="A2.T10.1.1.1.1.m1.1a"><mi id="A2.T10.1.1.1.1.m1.1.1" xref="A2.T10.1.1.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A2.T10.1.1.1.1.m1.1b"><ci id="A2.T10.1.1.1.1.m1.1.1.cmml" xref="A2.T10.1.1.1.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.1.1.1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="A2.T10.1.1.1.1.m1.1d">italic_N</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=A2.T10.1.1.1.2>32</td></tr><tr class=ltx_tr id=A2.T10.2.2.2><td class="ltx_td ltx_align_left" id=A2.T10.2.2.2.1><math alttext="n" class="ltx_Math" display="inline" id="A2.T10.2.2.2.1.m1.1"><semantics id="A2.T10.2.2.2.1.m1.1a"><mi id="A2.T10.2.2.2.1.m1.1.1" xref="A2.T10.2.2.2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A2.T10.2.2.2.1.m1.1b"><ci id="A2.T10.2.2.2.1.m1.1.1.cmml" xref="A2.T10.2.2.2.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.2.2.2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="A2.T10.2.2.2.1.m1.1d">italic_n</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=A2.T10.2.2.2.2>100</td></tr><tr class=ltx_tr id=A2.T10.3.3.3><td class="ltx_td ltx_align_left" id=A2.T10.3.3.3.1><math alttext="r_{s}/\beta" class="ltx_Math" display="inline" id="A2.T10.3.3.3.1.m1.1"><semantics id="A2.T10.3.3.3.1.m1.1a"><mrow id="A2.T10.3.3.3.1.m1.1.1" xref="A2.T10.3.3.3.1.m1.1.1.cmml"><msub id="A2.T10.3.3.3.1.m1.1.1.2" xref="A2.T10.3.3.3.1.m1.1.1.2.cmml"><mi id="A2.T10.3.3.3.1.m1.1.1.2.2" xref="A2.T10.3.3.3.1.m1.1.1.2.2.cmml">r</mi><mi id="A2.T10.3.3.3.1.m1.1.1.2.3" xref="A2.T10.3.3.3.1.m1.1.1.2.3.cmml">s</mi></msub><mo id="A2.T10.3.3.3.1.m1.1.1.1" xref="A2.T10.3.3.3.1.m1.1.1.1.cmml">/</mo><mi id="A2.T10.3.3.3.1.m1.1.1.3" xref="A2.T10.3.3.3.1.m1.1.1.3.cmml">Œ≤</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T10.3.3.3.1.m1.1b"><apply id="A2.T10.3.3.3.1.m1.1.1.cmml" xref="A2.T10.3.3.3.1.m1.1.1"><divide id="A2.T10.3.3.3.1.m1.1.1.1.cmml" xref="A2.T10.3.3.3.1.m1.1.1.1"></divide><apply id="A2.T10.3.3.3.1.m1.1.1.2.cmml" xref="A2.T10.3.3.3.1.m1.1.1.2"><csymbol cd="ambiguous" id="A2.T10.3.3.3.1.m1.1.1.2.1.cmml" xref="A2.T10.3.3.3.1.m1.1.1.2">subscript</csymbol><ci id="A2.T10.3.3.3.1.m1.1.1.2.2.cmml" xref="A2.T10.3.3.3.1.m1.1.1.2.2">ùëü</ci><ci id="A2.T10.3.3.3.1.m1.1.1.2.3.cmml" xref="A2.T10.3.3.3.1.m1.1.1.2.3">ùë†</ci></apply><ci id="A2.T10.3.3.3.1.m1.1.1.3.cmml" xref="A2.T10.3.3.3.1.m1.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.3.3.3.1.m1.1c">r_{s}/\beta</annotation><annotation encoding="application/x-llamapun" id="A2.T10.3.3.3.1.m1.1d">italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT / italic_Œ≤</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=A2.T10.3.3.3.2>2</td></tr><tr class=ltx_tr id=A2.T10.8.8.12><td class="ltx_td ltx_align_left ltx_border_t" id=A2.T10.8.8.12.1 rowspan=5><span class=ltx_text id=A2.T10.8.8.12.1.1>SqueezeLLM</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=A2.T10.8.8.12.2>calib set (all)</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T10.8.8.12.3>WikiText-2</td></tr><tr class=ltx_tr id=A2.T10.8.8.13><td class="ltx_td ltx_align_left" id=A2.T10.8.8.13.1>calib sequence length</td><td class="ltx_td ltx_align_center" id=A2.T10.8.8.13.2>2048</td></tr><tr class=ltx_tr id=A2.T10.4.4.4><td class="ltx_td ltx_align_left" id=A2.T10.4.4.4.1><math alttext="N" class="ltx_Math" display="inline" id="A2.T10.4.4.4.1.m1.1"><semantics id="A2.T10.4.4.4.1.m1.1a"><mi id="A2.T10.4.4.4.1.m1.1.1" xref="A2.T10.4.4.4.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A2.T10.4.4.4.1.m1.1b"><ci id="A2.T10.4.4.4.1.m1.1.1.cmml" xref="A2.T10.4.4.4.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.4.4.4.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="A2.T10.4.4.4.1.m1.1d">italic_N</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=A2.T10.4.4.4.2>32</td></tr><tr class=ltx_tr id=A2.T10.5.5.5><td class="ltx_td ltx_align_left" id=A2.T10.5.5.5.1><math alttext="n" class="ltx_Math" display="inline" id="A2.T10.5.5.5.1.m1.1"><semantics id="A2.T10.5.5.5.1.m1.1a"><mi id="A2.T10.5.5.5.1.m1.1.1" xref="A2.T10.5.5.5.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A2.T10.5.5.5.1.m1.1b"><ci id="A2.T10.5.5.5.1.m1.1.1.cmml" xref="A2.T10.5.5.5.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.5.5.5.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="A2.T10.5.5.5.1.m1.1d">italic_n</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=A2.T10.5.5.5.2>100</td></tr><tr class=ltx_tr id=A2.T10.6.6.6><td class="ltx_td ltx_align_left" id=A2.T10.6.6.6.1><math alttext="r_{s}/\beta" class="ltx_Math" display="inline" id="A2.T10.6.6.6.1.m1.1"><semantics id="A2.T10.6.6.6.1.m1.1a"><mrow id="A2.T10.6.6.6.1.m1.1.1" xref="A2.T10.6.6.6.1.m1.1.1.cmml"><msub id="A2.T10.6.6.6.1.m1.1.1.2" xref="A2.T10.6.6.6.1.m1.1.1.2.cmml"><mi id="A2.T10.6.6.6.1.m1.1.1.2.2" xref="A2.T10.6.6.6.1.m1.1.1.2.2.cmml">r</mi><mi id="A2.T10.6.6.6.1.m1.1.1.2.3" xref="A2.T10.6.6.6.1.m1.1.1.2.3.cmml">s</mi></msub><mo id="A2.T10.6.6.6.1.m1.1.1.1" xref="A2.T10.6.6.6.1.m1.1.1.1.cmml">/</mo><mi id="A2.T10.6.6.6.1.m1.1.1.3" xref="A2.T10.6.6.6.1.m1.1.1.3.cmml">Œ≤</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T10.6.6.6.1.m1.1b"><apply id="A2.T10.6.6.6.1.m1.1.1.cmml" xref="A2.T10.6.6.6.1.m1.1.1"><divide id="A2.T10.6.6.6.1.m1.1.1.1.cmml" xref="A2.T10.6.6.6.1.m1.1.1.1"></divide><apply id="A2.T10.6.6.6.1.m1.1.1.2.cmml" xref="A2.T10.6.6.6.1.m1.1.1.2"><csymbol cd="ambiguous" id="A2.T10.6.6.6.1.m1.1.1.2.1.cmml" xref="A2.T10.6.6.6.1.m1.1.1.2">subscript</csymbol><ci id="A2.T10.6.6.6.1.m1.1.1.2.2.cmml" xref="A2.T10.6.6.6.1.m1.1.1.2.2">ùëü</ci><ci id="A2.T10.6.6.6.1.m1.1.1.2.3.cmml" xref="A2.T10.6.6.6.1.m1.1.1.2.3">ùë†</ci></apply><ci id="A2.T10.6.6.6.1.m1.1.1.3.cmml" xref="A2.T10.6.6.6.1.m1.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.6.6.6.1.m1.1c">r_{s}/\beta</annotation><annotation encoding="application/x-llamapun" id="A2.T10.6.6.6.1.m1.1d">italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT / italic_Œ≤</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=A2.T10.6.6.6.2>2</td></tr><tr class=ltx_tr id=A2.T10.8.8.14><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=A2.T10.8.8.14.1 rowspan=5><span class=ltx_text id=A2.T10.8.8.14.1.1>QTIP</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=A2.T10.8.8.14.2>calib set (Hessian)</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T10.8.8.14.3>RedPajama</td></tr><tr class=ltx_tr id=A2.T10.8.8.15><td class="ltx_td ltx_align_left" id=A2.T10.8.8.15.1>calib set (ReQuant, base models)</td><td class="ltx_td ltx_align_center" id=A2.T10.8.8.15.2>WikiText-2</td></tr><tr class=ltx_tr id=A2.T10.8.8.16><td class="ltx_td ltx_align_left" id=A2.T10.8.8.16.1>calib set (ReQuant, instruction following models)</td><td class="ltx_td ltx_align_center" id=A2.T10.8.8.16.2>Tulu 3</td></tr><tr class=ltx_tr id=A2.T10.7.7.7><td class="ltx_td ltx_align_left" id=A2.T10.7.7.7.1><math alttext="N" class="ltx_Math" display="inline" id="A2.T10.7.7.7.1.m1.1"><semantics id="A2.T10.7.7.7.1.m1.1a"><mi id="A2.T10.7.7.7.1.m1.1.1" xref="A2.T10.7.7.7.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A2.T10.7.7.7.1.m1.1b"><ci id="A2.T10.7.7.7.1.m1.1.1.cmml" xref="A2.T10.7.7.7.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.7.7.7.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="A2.T10.7.7.7.1.m1.1d">italic_N</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=A2.T10.7.7.7.2>32</td></tr><tr class=ltx_tr id=A2.T10.8.8.8><td class="ltx_td ltx_align_left ltx_border_bb" id=A2.T10.8.8.8.1><math alttext="r_{s}/\beta" class="ltx_Math" display="inline" id="A2.T10.8.8.8.1.m1.1"><semantics id="A2.T10.8.8.8.1.m1.1a"><mrow id="A2.T10.8.8.8.1.m1.1.1" xref="A2.T10.8.8.8.1.m1.1.1.cmml"><msub id="A2.T10.8.8.8.1.m1.1.1.2" xref="A2.T10.8.8.8.1.m1.1.1.2.cmml"><mi id="A2.T10.8.8.8.1.m1.1.1.2.2" xref="A2.T10.8.8.8.1.m1.1.1.2.2.cmml">r</mi><mi id="A2.T10.8.8.8.1.m1.1.1.2.3" xref="A2.T10.8.8.8.1.m1.1.1.2.3.cmml">s</mi></msub><mo id="A2.T10.8.8.8.1.m1.1.1.1" xref="A2.T10.8.8.8.1.m1.1.1.1.cmml">/</mo><mi id="A2.T10.8.8.8.1.m1.1.1.3" xref="A2.T10.8.8.8.1.m1.1.1.3.cmml">Œ≤</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T10.8.8.8.1.m1.1b"><apply id="A2.T10.8.8.8.1.m1.1.1.cmml" xref="A2.T10.8.8.8.1.m1.1.1"><divide id="A2.T10.8.8.8.1.m1.1.1.1.cmml" xref="A2.T10.8.8.8.1.m1.1.1.1"></divide><apply id="A2.T10.8.8.8.1.m1.1.1.2.cmml" xref="A2.T10.8.8.8.1.m1.1.1.2"><csymbol cd="ambiguous" id="A2.T10.8.8.8.1.m1.1.1.2.1.cmml" xref="A2.T10.8.8.8.1.m1.1.1.2">subscript</csymbol><ci id="A2.T10.8.8.8.1.m1.1.1.2.2.cmml" xref="A2.T10.8.8.8.1.m1.1.1.2.2">ùëü</ci><ci id="A2.T10.8.8.8.1.m1.1.1.2.3.cmml" xref="A2.T10.8.8.8.1.m1.1.1.2.3">ùë†</ci></apply><ci id="A2.T10.8.8.8.1.m1.1.1.3.cmml" xref="A2.T10.8.8.8.1.m1.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.8.8.8.1.m1.1c">r_{s}/\beta</annotation><annotation encoding="application/x-llamapun" id="A2.T10.8.8.8.1.m1.1d">italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT / italic_Œ≤</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_bb" id=A2.T10.8.8.8.2>2</td></tr></table></table></figure><blockquote><p>üîº This table shows the estimated GPU time (in hours) required for calculating the Post-quantization Integral (PQI) on an A100 GPU. It details the computation time for different model sizes (Llama 3.2 1B and 3.2 3B) and calibration datasets (WikiText-2 and Tulu 3) with varying numbers of samples (n). The size of the dataset used to calculate PQI significantly impacts the computational cost.</p><details><summary>read the caption</summary>Table 11: GPU Hours of doing integral on A100.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-340e188c917f92c1583102dbae9fc95b class=gallery><img src=https://ai-paper-reviewer.com/2503.01901/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.01901/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.01901/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.01901/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.01901/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.01901/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.01901/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.01901/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.01901/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.01901/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.01901/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.01901/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.01901/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.01901/14.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01901/&amp;title=Identifying%20Sensitive%20Weights%20via%20Post-quantization%20Integral" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01901/&amp;text=Identifying%20Sensitive%20Weights%20via%20Post-quantization%20Integral" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01901/&amp;subject=Identifying%20Sensitive%20Weights%20via%20Post-quantization%20Integral" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2503.01901/index.md",oid_likes="likes_paper-reviews/2503.01901/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2502.21263/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">RuCCoD: Towards Automated ICD Coding in Russian</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-28T00:00:00+00:00>28 February 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2502.20811/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-28T00:00:00+00:00>28 February 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>