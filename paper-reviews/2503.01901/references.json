{"references": [{"fullname_first_author": "Touvron, H.", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-01", "reason": "This paper introduces Llama 2, a popular open-source large language model, making it a crucial baseline for many quantization methods."}, {"fullname_first_author": "Lin, J.", "paper_title": "Awq: Activation-aware weight quantization for on-device ilm compression and acceleration", "publication_date": "2024-01-01", "reason": "This work presents AWQ, a popular activation-aware weight quantization method, and is relevant as it serves as a common comparison point."}, {"fullname_first_author": "Dettmers, T.", "paper_title": "Gptq: Accurate post-training quantization for generative pre-trained transformers", "publication_date": "2023-01-01", "reason": "This research introduces GPTQ, a common, accurate post-training quantization method."}, {"fullname_first_author": "Xiao, G.", "paper_title": "SmoothQuant: Accurate and efficient post-training quantization for large language models", "publication_date": "2023-01-01", "reason": "This work presents SmoothQuant, a method for post-training quantization that addresses the challenge of outliers, making it an important reference in the field."}, {"fullname_first_author": "Kim, S.", "paper_title": "SqueezeLLM: Dense-and-sparse quantization", "publication_date": "2024-07-01", "reason": "SqueezeLLM is a dense-and-sparse quantization method that uses a hessian based method, used for comparative and improvement purposes within this study."}]}