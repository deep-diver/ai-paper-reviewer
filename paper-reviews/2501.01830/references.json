{"references": [{"fullname_first_author": "Achiam, J.", "paper_title": "Constrained policy optimization", "publication_date": "2017-00-00", "reason": "This paper introduces constrained policy optimization, a fundamental concept in reinforcement learning that is crucial for handling safety constraints in the AUTO-RT framework."}, {"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-03-02", "reason": "This paper is foundational to the field of aligning LLMs with human values and is highly relevant to the context of red-teaming for LLM safety."}, {"fullname_first_author": "Schulman, J.", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-06", "reason": "The Proximal Policy Optimization (PPO) algorithm is used for optimizing the attack model in AUTO-RT, making this paper critical to the framework's functionality."}, {"fullname_first_author": "Ng, A. Y.", "paper_title": "Policy invariance under reward transformations: Theory and application to reward shaping", "publication_date": "1999-00-00", "reason": "This paper introduces reward shaping, a core concept in reinforcement learning used in AUTO-RT to improve the exploration process, particularly valuable in scenarios with sparse rewards."}, {"fullname_first_author": "Altman, E.", "paper_title": "Constrained Markov decision processes", "publication_date": "1999-00-00", "reason": "This paper introduces Constrained Markov Decision Processes (CMDPs), which serve as a fundamental theoretical foundation for the problem formulation of automated red-teaming in the AUTO-RT framework."}]}