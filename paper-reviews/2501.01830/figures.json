[{"figure_path": "https://arxiv.org/html/2501.01830/x1.png", "caption": "Figure 1: Comparison between previous red-teaming approaches and Auto-RT. Previous works focused on identifying safety flaws of the target model under given attack strategies, whereas Auto-RT directly explores systematic safety flaws in target models starting from searching strategies itself, enabling a fully automated process.", "description": "The figure illustrates the key difference between Auto-RT and prior red-teaming techniques.  Traditional methods evaluate the safety of large language models (LLMs) using pre-defined attack strategies.  They focus on finding existing vulnerabilities within the model's responses to these known attacks. In contrast, Auto-RT takes a more proactive and comprehensive approach. It begins by automatically exploring and generating a wide variety of attack strategies. Then, it uses these novel strategies to assess the LLM's safety, enabling the identification of previously unknown vulnerabilities. This automated exploration process makes Auto-RT more efficient and scalable than existing methods.", "section": "1. Preliminary: Red-Teaming Aligned LLMs"}, {"figure_path": "https://arxiv.org/html/2501.01830/x2.png", "caption": "Figure 2: The framework of Auto-RT, comprising two key components: 1) Early-terminated Exploration, which assesses the diversity of the generated strategies and the consistency of the rephrased prompt with the initial toxic behavior to determine the necessity of safety evaluation. If either constraint is unmet, the process immediately terminate, and a penalty is applied; 2) Progressive Reward\nTracking, which enhances the density of safety rewards by utilizing a degrade model derived from the target model, thereby improving the efficiency and effectiveness of the exploration process.", "description": "AUTO-RT's framework is composed of two main parts: Early-terminated Exploration and Progressive Reward Tracking.  Early-terminated Exploration assesses the diversity of generated attack strategies and the consistency between the rephrased prompt and the original toxic behavior. If either fails, the process stops early, applying a penalty.  Progressive Reward Tracking improves exploration efficiency by using a degraded model (a less capable version of the main language model) to generate denser safety reward signals, guiding the search toward successful attacks more effectively. This method refines the search trajectory and helps to identify vulnerabilities faster.", "section": "2. Auto Red-Teaming"}, {"figure_path": "https://arxiv.org/html/2501.01830/x3.png", "caption": "Figure 3: Conceptual diagram of the safety distribution \ud835\udca5\u2062(s)\ud835\udca5s\\mathcal{J}(\\textbf{s})caligraphic_J ( s ) across the state space s, illustrating the principle of our proposed reward shaping process. The red curve represents the safer model m\ud835\udc5a{\\color[rgb]{1,0.23,0.13}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.23,0.13}%\n\\pgfsys@color@cmyk@stroke{0}{0.77}{0.87}{0}\\pgfsys@color@cmyk@fill{0}{0.77}{0.%\n87}{0}m}italic_m, while the blue curve represents the less safe model m\u2032superscript\ud835\udc5a\u2032{\\color[rgb]{0.0600000000000001,0.46,1}\\definecolor[named]{pgfstrokecolor}{rgb%\n}{0.0600000000000001,0.46,1}\\pgfsys@color@cmyk@stroke{0.94}{0.54}{0}{0}%\n\\pgfsys@color@cmyk@fill{0.94}{0.54}{0}{0}m^{\\prime}}italic_m start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT. \u03b8\ud835\udf03\\thetaitalic_\u03b8 denotes the safety-danger threshold, with \u03b4\ud835\udeff{\\color[rgb]{1,0.23,0.13}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.23,0.13}%\n\\pgfsys@color@cmyk@stroke{0}{0.77}{0.87}{0}\\pgfsys@color@cmyk@fill{0}{0.77}{0.%\n87}{0}\\delta}italic_\u03b4 and \u03b4\u2032superscript\ud835\udeff\u2032{\\color[rgb]{0.0600000000000001,0.46,1}\\definecolor[named]{pgfstrokecolor}{rgb%\n}{0.0600000000000001,0.46,1}\\pgfsys@color@cmyk@stroke{0.94}{0.54}{0}{0}%\n\\pgfsys@color@cmyk@fill{0.94}{0.54}{0}{0}\\delta^{\\prime}}italic_\u03b4 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT marking the respective dangerous subspaces. The safer model, m\ud835\udc5a{\\color[rgb]{1,0.23,0.13}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.23,0.13}%\n\\pgfsys@color@cmyk@stroke{0}{0.77}{0.87}{0}\\pgfsys@color@cmyk@fill{0}{0.77}{0.%\n87}{0}m}italic_m, demonstrates higher safety across most states, with its dangerous subspace, \u03b4\ud835\udeff{\\color[rgb]{1,0.23,0.13}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.23,0.13}%\n\\pgfsys@color@cmyk@stroke{0}{0.77}{0.87}{0}\\pgfsys@color@cmyk@fill{0}{0.77}{0.%\n87}{0}\\delta}italic_\u03b4, being sparse and minimally interconnected. In contrast, the less safe model, m\u2032{\\color[rgb]{0.0600000000000001,0.46,1}\\definecolor[named]{pgfstrokecolor}{rgb%\n}{0.0600000000000001,0.46,1}\\pgfsys@color@cmyk@stroke{0.94}{0.54}{0}{0}%\n\\pgfsys@color@cmyk@fill{0.94}{0.54}{0}{0}m{\\prime}}italic_m \u2032, exhibits larger and more connected dangerous subspaces, increasing the probability of encountering unsafe regions. Notably, the dangerous subspace of m\ud835\udc5a{\\color[rgb]{1,0.23,0.13}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.23,0.13}%\n\\pgfsys@color@cmyk@stroke{0}{0.77}{0.87}{0}\\pgfsys@color@cmyk@fill{0}{0.77}{0.%\n87}{0}m}italic_m is entirely encompassed by that of m\u2032{\\color[rgb]{0.0600000000000001,0.46,1}\\definecolor[named]{pgfstrokecolor}{rgb%\n}{0.0600000000000001,0.46,1}\\pgfsys@color@cmyk@stroke{0.94}{0.54}{0}{0}%\n\\pgfsys@color@cmyk@fill{0.94}{0.54}{0}{0}m{\\prime}}italic_m \u2032. This relationship allows for the strategic use of m\u2032{\\color[rgb]{0.0600000000000001,0.46,1}\\definecolor[named]{pgfstrokecolor}{rgb%\n}{0.0600000000000001,0.46,1}\\pgfsys@color@cmyk@stroke{0.94}{0.54}{0}{0}%\n\\pgfsys@color@cmyk@fill{0.94}{0.54}{0}{0}m{\\prime}}italic_m \u2032 to efficiently focus the exploration process on identifying the dangerous subspaces of m\ud835\udc5a{\\color[rgb]{1,0.23,0.13}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.23,0.13}%\n\\pgfsys@color@cmyk@stroke{0}{0.77}{0.87}{0}\\pgfsys@color@cmyk@fill{0}{0.77}{0.%\n87}{0}m}italic_m.", "description": "This figure illustrates the reward shaping process using two models: a safer model (red curve) and a less safe model (blue curve). The x-axis represents the state space, and the y-axis represents the safety distribution.  The safer model has a smaller and less connected dangerous subspace (\u03b4), while the less safe model has a larger and more interconnected dangerous subspace (\u03b4'). Using the less safe model allows focusing the exploration on the dangerous subspaces of the safer model.", "section": "2. Auto Red-Teaming"}, {"figure_path": "https://arxiv.org/html/2501.01830/x4.png", "caption": "Figure 4: Comparison of attack efficiency between Auto-RT and RL. The violin plots represent the distribution of attack success rates for every 1k sampled strategies, with lighter colors indicating Auto-RT and darker colors representing RL. Auto-RT achieves higher attack success rates than RL under the same number of samples, and with larger variance, indicating that it achieves more comprehensive exploration.", "description": "Figure 4 presents a comparison of the attack efficiency between Auto-RT and RL.  The violin plots illustrate the distribution of attack success rates for sets of 1,000 sampled strategies. Lighter colors represent Auto-RT, while darker colors represent RL.  The figure demonstrates that Auto-RT consistently achieves higher attack success rates than RL using the same number of samples.  Furthermore, the larger variance in Auto-RT's success rates suggests a broader and more thorough exploration of the strategy space, leading to the discovery of a wider range of vulnerabilities.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.01830/extracted/6109106/resources/sampling.png", "caption": "Figure 5: The relationship between the red-teaming outcomes (Attack ASR) following reward shaping with a series of intermediate models (M1 to M6), the safety levels of these models (Weaken ASR), and their first inverse rate for additional toxic behavior (Weaken FIR). These intermediate models are derived by fine-tuning on six target models using varying amounts of toxic data.The optimal red-teaming results are achieved by selecting the last intermediate model before a sudden spike in FIR (represented by the dark-colored bar in the figure) as the degrade model for reward shaping.", "description": "Figure 5 illustrates the impact of using different degraded models on the red-teaming process.  Six intermediate models (M1-M6) were created by fine-tuning six target models with increasing amounts of toxic data. The figure plots three key metrics against the intermediate models: Attack ASR (the success rate of attacks on the original target model after using reward shaping with the intermediate model), Weaken ASR (the safety level of the intermediate model itself), and Weaken FIR (the rate at which the intermediate model produces unsafe responses). The optimal results are achieved when the final degraded model is selected just before a sharp increase in the Weaken FIR, which is indicated by the dark-colored bars in the graph. This suggests that selecting a model with sufficiently degraded safety capabilities but not excessively weak is crucial for effective reward shaping.", "section": "2. Auto Red-Teaming"}, {"figure_path": "https://arxiv.org/html/2501.01830/x5.png", "caption": "Figure 6: Complete prompt for new strategies exploration. seed examples are demonstrations selected from existing strategies based on different settings.", "description": "Figure 6 shows the prompt used in the AUTO-RT framework for generating new attack strategies.  The prompt instructs the model to create a novel rephrasing strategy that modifies user queries while preserving their intent.  It specifies constraints to ensure diversity and maintain the original query meaning, and requests strategies that are easy to understand and apply. The prompt includes example strategies to guide the model's generation process, thus allowing for a more targeted and effective search for vulnerabilities.", "section": "2. Auto Red-Teaming"}, {"figure_path": "https://arxiv.org/html/2501.01830/x6.png", "caption": "Figure 7: Complete the prompt for attack query rephrasing using the provided attack strategy. The attack strategy is sampled from the attack model, and the toxic query represents the initial toxic behavior.", "description": "This figure displays the prompt template used in the AUTO-RT framework for the query rephrasing stage.  The prompt instructs the model to rephrase a given toxic query according to a specific attack strategy.  This attack strategy is dynamically generated by the attack model within the AUTO-RT system, and is fed as input to the rephrasing process.  The goal is to modify the query while maintaining its original intent, but in a way that is more likely to elicit a harmful response from the target language model. The prompt includes example rephrased queries to guide the model in generating consistent and effective rephrased queries. This ensures the model produces variations of the original toxic query that could still trigger unsafe behaviors, even if the original phrasing was blocked or filtered by the target model.", "section": "2. Auto Red-Teaming"}, {"figure_path": "https://arxiv.org/html/2501.01830/x7.png", "caption": "Figure 8: Complete the prompt for judging query intent. Verify that the original query and the rephrased query, modified with the attack strategy, share a similar intent by assessing their purposes.", "description": "This figure shows the prompt used for the consistency judge module in the AUTO-RT framework.  This module assesses whether the original query and its rephrased version (modified using a generated attack strategy) maintain the same intent. The prompt guides the judge to determine similarity based on several criteria:  If both queries can be answered by the same response, share similar key terms, or represent the same underlying request, then their intent is considered the same. Conversely, different intent would be assigned if the queries yield different responses or lack similar key terms.  The prompt includes example scenarios for better understanding.", "section": "2. Auto Red-Teaming"}]