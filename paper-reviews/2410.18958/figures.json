[{"figure_path": "2410.18958/figures/figures_2_0.png", "caption": "Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models.", "description": "The figure illustrates the Stable Consistency Tuning (SCT) framework and its relationship to other consistency model training methods.  It shows the process of solving the Probability Flow ODE (PF-ODE) in diffusion models, where the initial state (X0) is noised to an intermediate state (Xt). Consistency Distillation (CD) solves the ODE using the expectation of (Xt, t), which has a low upper bound but high stability.  Consistency Training (CT) solves the ODE using the expectation of (Xt, t; X0),  resulting in high training variance.  SCT is positioned as a unifying approach, approximating the expectation E(xt, t) and offering a way to reduce variance, bridging the gap between CD and CT.  The diagram visually represents the different training strategies and how SCT incorporates multiple reference samples to improve training stability.", "section": "3 UNDERSTANDING CONSISTENCY MODELS"}, {"figure_path": "2410.18958/figures/figures_5_0.png", "caption": "Figure 2: Phasing the ODE path along the time axis for consistency training. We visualize both training and inference techniques in discrete form for easier understanding.", "description": "This figure compares one-step inference and multistep inference in consistency models.  The top half illustrates one-step inference, where a consistency model directly predicts the initial state (x0) from a noisy sample (x1) using a single step. The bottom half demonstrates the multistep approach, where multiple intermediate states are generated by the consistency model using multiple ODE steps. The edge-skipping inference strategy is highlighted; this skips certain intermediate stages to improve efficiency and avoid instability in training. The figure uses a visual representation of a dog image, which is progressively denoised from x1 to x0, to depict the process.", "section": "4 STABLE CONSISTENCY TUNING"}, {"figure_path": "2410.18958/figures/figures_18_0.png", "caption": "Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models.", "description": "The figure is a diagram illustrating Stable Consistency Tuning (SCT) and its relation to other training strategies for consistency models.  It shows how SCT, using variance-reduced learning via the score identity, acts as a unifying framework, incorporating and improving upon aspects of both consistency distillation (CD) and consistency training/tuning (CT). The diagram depicts CD as using a pretrained diffusion model to solve an ODE with a low upper bound, CT as having high training variance and using the ODE solution with a single reference sample, and SCT as incorporating multiple i.i.d. samples to obtain a variance-reduced training target for improved stability and performance.", "section": "3 UNDERSTANDING CONSISTENCY MODELS"}, {"figure_path": "2410.18958/figures/figures_19_0.png", "caption": "Figure 7: 1-step samples from class-conditional SCT trained on CIFAR-10. Each row corresponds to a different class.", "description": "This figure displays 1-step samples generated by the Stable Consistency Tuning (SCT) model trained on the CIFAR-10 dataset.  The samples are organized into rows, with each row representing a different class from the CIFAR-10 dataset. Each row shows multiple sample images belonging to that specific class, demonstrating the model's ability to generate diverse and representative images for each class. The figure visually illustrates the quality and diversity of generated samples from a class-conditional model.", "section": "III Qualitative Results"}, {"figure_path": "2410.18958/figures/figures_20_0.png", "caption": "Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models.", "description": "The figure illustrates the Stable Consistency Tuning (SCT) framework, highlighting its variance-reduced training target. It presents a comparison of different consistency model training strategies, including consistency distillation (CD) and consistency training (CT), which are shown as special cases of SCT.  CD uses a pretrained diffusion model to solve the ODE (Ordinary Differential Equation), resulting in a low performance upper bound but low training variance. CT, on the other hand, directly solves the ODE from raw data, offering a higher performance upper bound but with high training variance. SCT incorporates variance-reduced learning to overcome the limitations of CT by utilizing multiple reference samples to approximate E(xt,t), which helps improve the training stability and performance. The figure uses a diagrammatic representation to visually show the process of ODE solving through approximating E(xt,t) for different methods.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18958/figures/figures_21_0.png", "caption": "Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models.", "description": "This figure is a diagram illustrating the Stable Consistency Tuning (SCT) method and how it relates to other consistency model training strategies.  It shows a unifying perspective by modeling the denoising process as a Markov Decision Process (MDP) and framing consistency model training as value estimation through Temporal Difference (TD) learning.  The diagram visually compares consistency distillation (CD), which uses a pretrained diffusion model as a teacher, and consistency training/tuning (CT), which learns directly from data, highlighting their differences in terms of training variance and upper performance bounds. SCT is presented as a generalized framework that encompasses both CD and CT, incorporating variance-reduced learning for improved performance and stability.", "section": "3 UNDERSTANDING CONSISTENCY MODELS"}, {"figure_path": "2410.18958/figures/figures_22_0.png", "caption": "Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models.", "description": "The figure is a flowchart illustrating Stable Consistency Tuning (SCT).  It shows how SCT unifies different training strategies for consistency models, including consistency distillation (CD) and consistency training/tuning (CT). CD uses a pretrained diffusion model to estimate a training target, represented as solving an ODE with E(Xt,t). CT, shown as a special case of SCT, directly trains the consistency model using a single reference sample x0, leading to high variance during training and represented as solving an ODE with E(Xt,t;x0).  SCT improves on both by utilizing variance-reduced learning using the score identity, obtaining a lower upper bound for training and better performance than CD and CT, ultimately leading to a more stable and effective training process. The visual shows that the initial state of the MDP is randomly sampled from Gaussian. The intermediate state consists of the denoised sample xt and the corresponding conditional information, including the timestep t. The policy function of the MDP corresponds to the action of applying the ODE solver to perform single-step denoising, resulting in the transition to the new state.", "section": "3 UNDERSTANDING CONSISTENCY MODELS"}, {"figure_path": "2410.18958/figures/figures_23_0.png", "caption": "Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models.", "description": "The figure is a diagram illustrating Stable Consistency Tuning (SCT) and how it relates to other training strategies for consistency models.  It depicts three main approaches: Consistency Distillation (CD), Consistency Training (CT), and SCT itself.  CD is shown as solving an Ordinary Differential Equation (ODE) using the expectation of a diffusion model, resulting in a low upper bound on performance but high stability. CT, depicted as a special case of SCT, solves the ODE using only one reference sample, leading to higher variance and instability.  SCT is presented as a unifying framework that addresses these limitations by using multiple i.i.d. samples to approximate the ODE solution, achieving variance reduction. The diagram visually represents the different approaches, highlighting their strengths and weaknesses in terms of variance and performance upper bounds.", "section": "3 UNDERSTANDING CONSISTENCY MODELS"}, {"figure_path": "2410.18958/figures/figures_24_0.png", "caption": "Figure 13: 1-step samples from class-conditional SCT trained on ImageNet-64 (FID 2.23). Each row corresponds to a different class.", "description": "This figure shows 1-step samples generated from a class-conditional Stable Consistency Tuning (SCT) model trained on the ImageNet-64 dataset. The figure is arranged as a grid, with each row representing a different class from the dataset.  Each cell within a row displays a single image sample generated by the model for that specific class.  The caption indicates that the model achieved a Fr\u00e9chet Inception Distance (FID) score of 2.23, a metric used to evaluate the quality of generated images, lower scores indicating better image quality.", "section": "5.2 RESULTS AND ANALYSIS"}, {"figure_path": "2410.18958/figures/figures_25_0.png", "caption": "Figure 13: 1-step samples from class-conditional SCT trained on ImageNet-64 (FID 2.23). Each row corresponds to a different class.", "description": "This figure displays a grid of 1-step samples generated from a class-conditional Stable Consistency Tuning (SCT) model trained on the ImageNet-64 dataset. The FID score, a metric for evaluating image quality, is reported as 2.23. Each row in the grid represents a different class from the ImageNet-64 dataset. The images are arranged in a grid format, making it easy to visually compare the generated samples across different classes. This visualization aims to showcase the model's ability to generate high-quality images that accurately reflect the characteristics of the corresponding classes.", "section": "5 EXPERIMENTS"}, {"figure_path": "2410.18958/figures/figures_26_0.png", "caption": "Figure 13: 1-step samples from class-conditional SCT trained on ImageNet-64 (FID 2.23). Each row corresponds to a different class.", "description": "The figure displays 1-step samples generated by the Stable Consistency Tuning (SCT) model trained on the ImageNet-64 dataset. The images are organized in a grid, with each row representing a different class from the dataset.  The FID (Fr\u00e9chet Inception Distance) score of 2.23 indicates a relatively high quality of the generated images compared to other models for this task. The figure visually demonstrates the model's ability to generate high-quality, class-conditional images in a single step.", "section": "5 EXPERIMENTS"}]