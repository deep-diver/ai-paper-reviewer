{"references": [{" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This paper is foundational in introducing diffusion probabilistic models, a significant advancement in generative modeling that underpins much of the work on consistency models.  The concepts and techniques introduced in this paper are directly relevant to understanding and improving the efficiency and performance of the newer consistency models, which are the focus of this paper.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Prafulla Dhariwal", "paper_title": "Diffusion models beat gans on image synthesis", "reason": "This paper showcases the superior image generation quality achievable by diffusion models, setting a high benchmark for consistency models to match or surpass.  It's a key reference for understanding the strengths and limitations of the baseline approach that consistency models aim to improve upon.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Tero Karras", "paper_title": "Elucidating the design space of diffusion-based generative models", "reason": "This paper provides a comprehensive overview of the design space and training dynamics of diffusion models, a crucial reference for understanding the context within which consistency models operate and the challenges they seek to address.  It helps highlight the tradeoffs associated with diffusion models, further emphasizing the significance of the proposed consistency model advancements.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yang Song", "paper_title": "Consistency models", "reason": "This paper introduces consistency models and forms the core foundation for this research. It establishes the fundamental concepts, training methods, and the self-consistency condition that are central to the understanding and development of faster generative models.  Its implications are deeply integrated throughout the current paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yang Song", "paper_title": "Improved techniques for training consistency models", "reason": "This paper builds upon the initial work on consistency models, offering significant insights and improvements in training techniques.  The advancements in training methodologies are relevant and inform the approach taken in the current work to further refine and enhance consistency model performance.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Jiaming Song", "paper_title": "Denoising diffusion implicit models", "reason": "This paper makes significant contributions to the theory and practice of diffusion models, especially regarding sampling techniques. Its influence is seen in the understanding of probability flow ODE and the design of consistency models.  The improved sampling methods are directly relevant to the goals of this work.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Yang Song", "paper_title": "Score-based generative modeling through stochastic differential equations", "reason": "This paper establishes the theoretical foundation for score-based generative models, a crucial aspect of understanding both diffusion and consistency models.  The detailed mathematical formulation provides context for the current research and facilitates the development of a novel MDP framework for analyzing consistency model training.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yang Song", "paper_title": "Consistency models", "reason": "This paper provides a comprehensive introduction to consistency models, their training objectives, and the two key training methods.  It explains the foundational concepts and lays the groundwork for the proposed improvements and analysis presented in the current work.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Kevin Black", "paper_title": "Training diffusion models with reinforcement learning", "reason": "This paper explores the intersection of reinforcement learning and diffusion models, providing a new perspective on training diffusion models. This alternative perspective complements the work presented, offering a broader context for the application of reinforcement learning principles to generative modeling.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Yang Song", "paper_title": "Consistency models", "reason": "This paper, fundamental to the field, introduces consistency models and provides a basis for understanding their strengths and weaknesses.  The current work directly builds upon this foundation, extending the understanding and proposing novel methods for improvement.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Zhengyang Geng", "paper_title": "Consistency models made easy", "reason": "This paper introduces Easy Consistency Tuning (ECT), the foundation upon which the novel method Stable Consistency Tuning (SCT) is built.  This demonstrates a clear progression of work and allows for a direct comparison between the original ECT method and the improved SCT method proposed in this work.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yang Song", "paper_title": "Improved techniques for training consistency models", "reason": "This paper offers key insights into improving the training techniques of consistency models.  The improvements discussed in this paper, such as variance reduction and smoother progressive training schedules, directly relate to and motivate the improvements in SCT.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Tero Karras", "paper_title": "Guiding a diffusion model with a bad version of itself", "reason": "This paper presents classifier-free guidance, a technique that is adapted and applied in the SCT method to further enhance performance.  It demonstrates the use of a sub-optimal model version for guiding the generation process, a concept directly integrated into the SCT framework.", "section_number": 4}, {" publication_date": "2011", "fullname_first_author": "Pascal Vincent", "paper_title": "A connection between score matching and denoising autoencoders", "reason": "This paper establishes a theoretical connection between score matching and denoising autoencoders, providing a foundation for the score identity which SCT leverages for variance reduction. The score identity plays a crucial role in the variance reduction strategies of SCT, leading to improved training stability and better performance.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "CLIP is mentioned in the paper as a potential method for class-conditional generation. This paper's introduction of CLIP as a powerful tool for image-text understanding is relevant because it provides a method to acquire class labels from text descriptions, which could be directly used in class-conditional image generation with consistency models.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This paper introduces diffusion probabilistic models, which are the foundation for many subsequent generative models, including the consistency models discussed in this work.  Understanding the fundamentals of diffusion models is crucial to understanding the improvements and advancements proposed in this paper.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Yang Song", "paper_title": "Improved techniques for training consistency models", "reason": "This paper provides valuable insights into improving the training of consistency models, and several techniques mentioned are directly applied and extended within the SCT framework.  The work highlights the importance of training stability and efficiency in achieving high-quality generation, which motivates several aspects of SCT.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Zhengyang Geng", "paper_title": "Consistency models made easy", "reason": "ECT is the foundation for SCT, and this paper explains its core concepts and training methods.   Understanding ECT is critical to appreciating the novelty and advancements introduced by SCT, as SCT builds upon and improves upon the ECT approach.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Yang Song", "paper_title": "Consistency models", "reason": "This paper serves as the primary reference for the theoretical understanding of consistency models.  The work's framework, training methods, and fundamental concepts are integral to this paper's contribution.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Tero Karras", "paper_title": "Elucidating the design space of diffusion-based generative models", "reason": "This work provides a strong background on the design space and limitations of diffusion models which consistency models are meant to address. Understanding the strengths and weaknesses of diffusion models is necessary to understand the contributions of consistency models, which are faster and more efficient.", "section_number": 5}]}