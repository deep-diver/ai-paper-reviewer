{"references": [{" publication_date": "2021", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This paper is foundational for diffusion models, introducing the core concept and framework that many subsequent works, including this paper, build upon.  Its introduction of the denoising diffusion process laid the groundwork for a significant advancement in generative modeling.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Yang Song", "paper_title": "Score-based generative modeling through stochastic differential equations", "reason": "This work provides a theoretical framework for score-based generative models using stochastic differential equations (SDEs), which is crucial for understanding the underlying mathematics of diffusion and consistency models. It significantly advanced the field by providing a robust and efficient framework for training and sampling.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Yang Song", "paper_title": "Consistency models", "reason": "This paper introduces consistency models, the core focus of the current paper.  It is highly influential in shaping the research area and setting the stage for future improvements, such as those proposed in this paper.  The introduction of two training strategies (distillation and direct training) significantly expands the field.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yang Song", "paper_title": "Improved techniques for training consistency models", "reason": "This paper directly addresses some of the limitations of consistency model training, making it a highly relevant reference to the current paper.  By improving training techniques, this work sets the stage for further research.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Tero Karras", "paper_title": "Elucidating the design space of diffusion-based generative models", "reason": "This paper provides a deep analysis of the design choices available for diffusion models, offering a broad understanding of the theoretical landscape that informs practical improvements. This analysis is important for improving model training and sampling efficiency, which is a key focus of the current paper.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zhengyang Geng", "paper_title": "Consistency models made easy", "reason": "This paper is directly related to the current paper, presenting Easy Consistency Tuning (ECT), which serves as the foundation for the proposed Stable Consistency Tuning (SCT).  SCT builds upon and improves upon the techniques presented in this paper.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Kevin Black", "paper_title": "Training diffusion models with reinforcement learning", "reason": "This paper explores a novel approach to training diffusion models using reinforcement learning. It offers an alternative perspective to traditional training methods and suggests potential directions for enhancing training efficiency and stability.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Fan Bao", "paper_title": "Vidu: a highly consistent, dynamic and skilled text-to-video generator with diffusion models", "reason": "This paper is highly relevant as it showcases a high-performance video generation method using diffusion models, highlighting the need for efficient and high-quality generation methods in high-dimensional scenarios such as video.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Uriel Singer", "paper_title": "Make-a-video: Text-to-video generation without text-video data", "reason": "This work demonstrates high-quality video generation, which highlights the importance of efficient training methods in such high-dimensional settings.  The challenges addressed in this paper align directly with the slow generation speeds associated with diffusion models, which consistency models strive to improve.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Ian Goodfellow", "paper_title": "Generative adversarial networks", "reason": "Generative Adversarial Networks (GANs) are a significant class of generative models, and understanding their relationship with diffusion models and consistency models provides a broader context for evaluating the proposed improvements.  This provides a valuable reference point for assessing the advancements in generative models.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Diederik P. Kingma", "paper_title": "Variational diffusion models", "reason": "This paper introduced variational diffusion models, providing an alternative approach to diffusion models, which provides valuable context for the current paper's focus on consistency models and their advantages in terms of efficiency and speed.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Tero Karras", "paper_title": "Analyzing and improving the training dynamics of diffusion models", "reason": "This research delves into the training dynamics of diffusion models and proposes strategies for improving stability and training efficiency.  These techniques are highly relevant to the current paper's goal of enhancing consistency model training through variance reduction.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "David Berthelot", "paper_title": "TRACT: Denoising diffusion models with transitive closure time-distillation", "reason": "This work explores transitive closure time-distillation, a specific method for improving diffusion models.  The techniques used here, while different from consistency modeling, offer an alternative strategy for addressing issues such as slow sampling, making it a valuable comparison point.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Chenlin Meng", "paper_title": "On distillation of guided diffusion models", "reason": "This paper directly addresses distillation techniques for diffusion models, a highly relevant aspect of the current paper. By comparing and contrasting various distillation methods, this work highlights potential approaches to improving the consistency model training.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This work demonstrates the effectiveness of combining textual and visual information for improved image generation, which is important context for this paper. The use of CLIP for assessing model performance and understanding class-conditional generation in image generation is a vital aspect of the current paper's work on conditional image generation and variance reduction.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Ricky TQ Chen", "paper_title": "Riemannian flow matching on general geometries", "reason": "This paper provides a more general and mathematically rigorous framework for understanding flows in data spaces.  This is highly relevant to the current paper because consistency models rely on the probability flow ODE, which is directly related to the concept of flows.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Xiaoyu Shi", "paper_title": "Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling", "reason": "This paper addresses the challenge of image-to-video generation, a closely related problem to image generation.  Addressing high-dimensional data like video generation requires efficient methods, making this paper relevant.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Tero Karras", "paper_title": "Analyzing and improving the image quality of stylegan", "reason": "This paper explores the nuances of StyleGAN, a successful GAN-based image generation model.  Understanding the strengths and weaknesses of GANs offers valuable context for evaluating the current paper's focus on diffusion models and consistency models as alternative approaches.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Pascal Vincent", "paper_title": "A connection between score matching and denoising autoencoders", "reason": "This paper establishes a connection between score matching and denoising autoencoders, which underpins many diffusion models.  Understanding the mathematical relationships between these concepts is vital for developing and improving generative models.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jiaming Song", "paper_title": "Improved techniques for training consistency models", "reason": "This paper directly improves consistency model training, a critical aspect that is also improved in the current paper. Its impact on training stability and efficiency makes it relevant to the current paper's enhancements.", "section_number": 4}]}