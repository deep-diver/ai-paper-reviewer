[{"figure_path": "2410.13824/figures/figures_1_0.png", "caption": "Figure 1: Overview of MultiUI, a 7M multimodal instruction-tuning dataset built from a diverse collection of Webpage UIs. The model UIX, trained on MultiUI, generalizes effectively to a broad range of unseen scenarios, including GUI understanding (web and mobile interfaces) and, surprisingly, non-GUI tasks such as document and chart understanding.", "description": "The figure shows an overview of the MultiUI dataset, highlighting its size, diversity, and the generalization capabilities of a model trained on it across various tasks, including GUI and non-GUI tasks.", "section": "ABSTRACT"}, {"figure_path": "2410.13824/figures/figures_2_0.png", "caption": "Figure 2: MultiUI compared with previous methods. Our proposed MultiUI construction approach synthesizes full structured webpage UIs into multimodal instruction samples of versatile tasks by harnessing powerful LLMs, which leads to more generalizable training samples.", "description": "This figure compares MultiUI with prior methods for constructing multimodal instruction samples from web UIs, highlighting MultiUI's use of powerful LLMs to synthesize diverse and generalizable training samples.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13824/figures/figures_3_0.png", "caption": "Figure 3: Construction pipeline of MultiUI. The process consists of four main stages: (1) Website Scraping; (2) Website Curation with Llama-3-70b-Instruct; (3) Task Extraction utilizing Llama-3-70b-Instruct, GPT-40 mini, and rule-based approaches to generate Web UI tasks across three categories: visual understanding and reasoning, text recognition, and grounding; (4) For each task, generate tasks samples by applying the diverse instruction templates paraphrased by GPT-40.", "description": "The figure illustrates the four main stages of the MultiUI dataset construction pipeline: website scraping, curation, task extraction, and generalization.", "section": "2 Dataset Construction"}, {"figure_path": "2410.13824/figures/figures_4_0.png", "caption": "Figure 1: Overview of MultiUI, a 7M multimodal instruction-tuning dataset built from a diverse collection of Webpage UIs. The model UIX, trained on MultiUI, generalizes effectively to a broad range of unseen scenarios, including GUI understanding (web and mobile interfaces) and, surprisingly, non-GUI tasks such as document and chart understanding.", "description": "The figure illustrates the MultiUI dataset, highlighting its size, diversity of tasks and UI types, and the model's generalization capabilities across various scenarios.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13824/figures/figures_19_0.png", "caption": "Figure 1: Overview of MultiUI, a 7M multimodal instruction-tuning dataset built from a diverse collection of Webpage UIs. The model UIX, trained on MultiUI, generalizes effectively to a broad range of unseen scenarios, including GUI understanding (web and mobile interfaces) and, surprisingly, non-GUI tasks such as document and chart understanding.", "description": "The figure illustrates the MultiUI dataset, showing its size, sources, tasks covered, and the generalization capabilities of a model trained on it, highlighting its effectiveness for text-rich visual understanding.", "section": "ABSTRACT"}, {"figure_path": "2410.13824/figures/figures_25_0.png", "caption": "Figure 1: Overview of MultiUI, a 7M multimodal instruction-tuning dataset built from a diverse collection of Webpage UIs. The model UIX, trained on MultiUI, generalizes effectively to a broad range of unseen scenarios, including GUI understanding (web and mobile interfaces) and, surprisingly, non-GUI tasks such as document and chart understanding.", "description": "Figure 1 is an overview of the MultiUI dataset, showing its composition, the model trained on it (UIX), and the model's generalization capabilities across diverse tasks.", "section": "1 Introduction"}, {"figure_path": "2410.13824/figures/figures_26_0.png", "caption": "Figure 1: Overview of MultiUI, a 7M multimodal instruction-tuning dataset built from a diverse collection of Webpage UIs. The model UIX, trained on MultiUI, generalizes effectively to a broad range of unseen scenarios, including GUI understanding (web and mobile interfaces) and, surprisingly, non-GUI tasks such as document and chart understanding.", "description": "Figure 1 provides a high-level overview of MultiUI, a large-scale multimodal instruction-tuning dataset derived from web UIs, and demonstrates the model's generalization capabilities across various tasks.", "section": "ABSTRACT"}, {"figure_path": "2410.13824/figures/figures_26_1.png", "caption": "Figure 1: Overview of MultiUI, a 7M multimodal instruction-tuning dataset built from a diverse collection of Webpage UIs. The model UIX, trained on MultiUI, generalizes effectively to a broad range of unseen scenarios, including GUI understanding (web and mobile interfaces) and, surprisingly, non-GUI tasks such as document and chart understanding.", "description": "The figure illustrates the MultiUI dataset, its construction, and the generalization ability of a model trained on it to various tasks including GUI and non-GUI tasks.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13824/figures/figures_27_0.png", "caption": "Figure 1: Overview of MultiUI, a 7M multimodal instruction-tuning dataset built from a diverse collection of Webpage UIs. The model UIX, trained on MultiUI, generalizes effectively to a broad range of unseen scenarios, including GUI understanding (web and mobile interfaces) and, surprisingly, non-GUI tasks such as document and chart understanding.", "description": "The figure illustrates the MultiUI dataset, showing its composition, the training process, and the generalization ability of the model trained on it to various tasks, including GUI and non-GUI tasks.", "section": "1 Introduction"}]