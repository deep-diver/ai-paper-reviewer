[{"figure_path": "2410.13824/tables/table_6_0.html", "caption": "Table 1: Statistics of our dataset MultiUI.", "description": "The table presents the statistics of the MultiUI dataset, breaking down the number of samples by platform (desktop and mobile), task category (visual understanding and reasoning, grounding, and text recognition), and specific task.", "section": "2 DATASET CONSTRUCTION"}, {"figure_path": "2410.13824/tables/table_8_0.html", "caption": "Table 2: Results on GUI understanding and grounding benchmarks. Bold text and underlined indicate the best-performing and the second-best models in each group, respectively. * indicates specific fine-tuning on the corresponding training set. \u2020 denotes our re-implementation with the same backbone model architecture of UIX. ScreenQA-short, VisualWebBench Element-Ground (bbox generation), VisualWebBench Action-Ground (bbox generation), ScreenSpot are abbreviated as SQA short, VWB Ele-G, VWB Act-G, SSpot, respectively.", "description": "Table 2 presents the performance comparison of different models on GUI understanding and grounding benchmarks, highlighting the improvement achieved by the proposed model.", "section": "4.1 RESULTS ON GUI-RELATED TASKS"}, {"figure_path": "2410.13824/tables/table_9_0.html", "caption": "Table 3: Results on general OCR/Doc/Chart related QA and grounding benchmarks. Bold text and underlined indicate the best-performing and the second-best models in each group, respectively.", "description": "Table 3 presents the performance of various models on general OCR, document question answering (DocQA), chart question answering (ChartQA), and grounding tasks, highlighting the best-performing models in each category.", "section": "4.3 RESULTS ON GENERAL DOCUMENT UNDERSTANDING AND GROUNDING TASKS"}, {"figure_path": "2410.13824/tables/table_9_1.html", "caption": "Table 1: Statistics of our dataset MultiUI.", "description": "The table presents the statistics of the MultiUI dataset, showing the number of samples for each task category and platform (desktop and mobile).", "section": "2 DATASET CONSTRUCTION"}, {"figure_path": "2410.13824/tables/table_10_0.html", "caption": "Table 5: Performance on other general multimodal benchmarks.", "description": "Table 5 presents the performance of different models on three general multimodal benchmarks: MMMU, MMBench, and VQA-V2, showcasing the models' capabilities in various multimodal tasks.", "section": "4.4 RESULTS ON GENERAL MULTIMODAL TASKS"}, {"figure_path": "2410.13824/tables/table_25_0.html", "caption": "Table 1: Statistics of our dataset MultiUI.", "description": "The table presents the statistical distribution of 7.3 million samples across various tasks and platforms within the MultiUI dataset.", "section": "2 DATASET CONSTRUCTION"}, {"figure_path": "2410.13824/tables/table_26_0.html", "caption": "Table 1: Statistics of our dataset MultiUI.", "description": "The table presents the statistics of the MultiUI dataset, showing the number of samples for each task and platform.", "section": "2 DATASET CONSTRUCTION"}, {"figure_path": "2410.13824/tables/table_27_0.html", "caption": "Table 1: Statistics of our dataset MultiUI.", "description": "The table presents the statistics of the MultiUI dataset, showing the number of samples for each task and platform (desktop and mobile).", "section": "2 DATASET CONSTRUCTION"}, {"figure_path": "2410.13824/tables/table_27_1.html", "caption": "Table 1: Statistics of our dataset MultiUI.", "description": "The table presents the statistics of the MultiUI dataset, showing the number of samples for each platform (desktop and mobile) and task category (visual understanding and reasoning, grounding, and text recognition).", "section": "2 DATASET CONSTRUCTION"}, {"figure_path": "2410.13824/tables/table_27_2.html", "caption": "Table 1: Statistics of our dataset MultiUI.", "description": "The table presents the statistics of the MultiUI dataset, showing the number of samples for each task and platform (desktop and mobile).", "section": "2 DATASET CONSTRUCTION"}, {"figure_path": "2410.13824/tables/table_28_0.html", "caption": "Table 1: Statistics of our dataset MultiUI.", "description": "The table presents the statistics of the MultiUI dataset, breaking down the number of samples across different platforms (desktop and mobile), tasks (visual understanding and reasoning, grounding, and text recognition), and subtasks.", "section": "2 DATASET CONSTRUCTION"}, {"figure_path": "2410.13824/tables/table_31_0.html", "caption": "Table 1: Statistics of our dataset MultiUI.", "description": "The table presents the statistics of the MultiUI dataset, showing the number of samples for various tasks and platforms.", "section": "2 DATASET CONSTRUCTION"}, {"figure_path": "2410.13824/tables/table_31_1.html", "caption": "Table 1: Statistics of our dataset MultiUI.", "description": "The table presents the statistics of the MultiUI dataset, showing the number of samples for each task and platform.", "section": "2 DATASET CONSTRUCTION"}, {"figure_path": "2410.13824/tables/table_32_0.html", "caption": "Table 9: Full experimental results of our models compared to three different backbones.", "description": "This table presents the complete performance results of the three UIX models (UIX-Vicuna, UIX-Llama3.1, UIX-Qwen2) and their corresponding LLaVA baselines across various benchmarks, showcasing the impact of different model backbones on performance.", "section": "4.1 Results on GUI-Related Tasks"}]