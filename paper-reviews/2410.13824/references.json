{"references": [{" publication_date": "2023", "fullname_first_author": "Gilles Baechler", "paper_title": "ScreenAI: A vision-language model for UI and infographics understanding", "reason": "This paper introduces ScreenAI, a vision-language model specifically designed for UI and infographics understanding.  Its focus on UI understanding, a key aspect of the current work, makes it highly relevant.  The model's performance and approach offer a valuable comparative baseline for evaluating the effectiveness of the proposed MultiUI dataset and the resulting UIX model.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Youngmin Baek", "paper_title": "Character region awareness for text detection", "reason": "This paper focuses on text detection, a crucial component of text-rich visual understanding.  Its contribution to accurate text extraction from images directly impacts the quality and reliability of the data used for training multimodal models.  Improved text detection techniques are essential for the success of the proposed MultiUI dataset.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Chongyang Bai", "paper_title": "Uibert: Learning generic multimodal representations for UI understanding", "reason": "Uibert directly addresses the task of UI understanding, making it highly relevant to the current work.  Its focus on generic multimodal representations provides a valuable comparison point for evaluating the generalization capabilities of the MultiUI dataset and the UIX model.  The paper's methodology and findings can inform the design and evaluation of the proposed MultiUI dataset.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Lin Chen", "paper_title": "ShareGPT4V: Improving large multi-modal models with better captions", "reason": "This paper explores the impact of improved captions on multi-modal models. Captions are an important element in MultiUI, and this paper's findings on caption quality and model performance are directly relevant to evaluating the effectiveness of the MultiUI dataset.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Xingyu Chen", "paper_title": "WebSRC: A dataset for web-based structural reading comprehension", "reason": "This paper is highly relevant because it introduces WebSRC, a dataset specifically designed for evaluating structural reading comprehension on web pages, a core aspect of the proposed MultiUI dataset.  The design and methodology of WebSRC offer valuable insights into creating and evaluating similar datasets focused on text-rich visual understanding.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Kanzhi Cheng", "paper_title": "SeeClick: Harnessing GUI grounding for advanced visual GUI agents", "reason": "This paper is highly relevant because it focuses on GUI grounding, a central task in the MultiUI dataset.  The methods and results of SeeClick provide a critical comparative analysis of different approaches to GUI grounding, thus offering a valuable comparative baseline for evaluating the effectiveness of the MultiUI dataset in improving GUI grounding performance.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Xiang Deng", "paper_title": "Mind2Web: Towards a generalist agent for the web", "reason": "This paper introduces Mind2Web, a benchmark for evaluating generalist web agents. Its focus on agent capabilities and the detailed task design makes it crucial for the proposed methodology.  The comparison of UIX to models trained on Mind2Web demonstrates the generalizability of the proposed MultiUI dataset and the resulting UIX model in real-world web interaction scenarios.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The LLaMa 3 herd of models", "reason": "This paper describes the LLaMa 3 family of models, one of which (Llama-3-70B-Instruct) is a core component of the MultiUI dataset creation pipeline. Understanding the capabilities and limitations of Llama-3-70B-Instruct is crucial for assessing the quality and reliability of the MultiUI dataset itself.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuan Gao", "paper_title": "Enhancing Vision-Language Pre-training with Rich Supervisions", "reason": "This paper explores rich supervision methods for vision-language pre-training.  The insights gained from this paper directly contribute to the design and evaluation of multimodal instruction synthesis in the MultiUI dataset creation pipeline.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Wenyi Hong", "paper_title": "CogAgent: A visual language model for GUI agents", "reason": "CogAgent is a strong baseline for GUI agent tasks.  Comparing the proposed UIX model's performance on Mind2Web against CogAgent's results is crucial to assess the effectiveness of MultiUI.  The architecture and capabilities of CogAgent offer valuable insights into designing and evaluating multimodal agents for UI interactions.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yu-Chung Hsiao", "paper_title": "ScreenQA-Short: A short question answering dataset on mobile app screenshots", "reason": "ScreenQA-Short is a benchmark used for evaluating the models' performance on short question answering related to mobile app screens.  This provides insights into the model's ability to handle concise questions, and its performance serves as a comparative baseline to show the impact of the MultiUI dataset.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Yang Li", "paper_title": "Widget captioning: Generating natural language description for mobile user interface elements", "reason": "This paper directly relates to the task of generating captions for UI elements.  Understanding the challenges and methods used in Widget captioning directly contributes to enhancing the quality and diversity of the multimodal instruction samples generated in MultiUI.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Haotian Liu", "paper_title": "LLaVA-Next: Improved reasoning, OCR, and world knowledge", "reason": "This paper introduces LLaVA-NeXT, a significant improvement over the original LLaVA model.  It is used as a strong baseline in the experimental results, providing a crucial reference point for assessing the impact and effectiveness of the proposed MultiUI dataset and the resulting UIX model. The model's design choices also influence the design choices made for the UIX model.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Junpeng Liu", "paper_title": "VisualWebBench: How far have multimodal LLMs evolved in web page understanding and grounding?", "reason": "This paper introduces VisualWebBench, the primary benchmark used to evaluate the performance of the proposed MultiUI dataset and the resulting UIX model. VisualWebBench focuses on web-related tasks, aligning directly with the focus of this paper.   Its design and evaluation methodology directly impact the design and evaluation of MultiUI and UIX.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Ahmed Masry", "paper_title": "ChartQA: A benchmark for question answering about charts with visual and logical reasoning", "reason": "ChartQA is a crucial benchmark for evaluating the models' ability to answer questions about charts. Its inclusion in the study highlights the broad applicability of the MultiUI dataset, and its evaluation results demonstrate the dataset's impact on model performance in non-UI domains.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Minesh Mathew", "paper_title": "DocVQA: A dataset for VQA on document images", "reason": "DocVQA is a key benchmark that evaluates the models' ability to answer questions about document images. Its inclusion highlights the generalization capabilities of models trained on MultiUI to non-UI domains.  The results on DocVQA demonstrate the broader applicability of the MultiUI dataset beyond web UIs.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Amanpreet Singh", "paper_title": "Towards VQA models that can read", "reason": "This paper focuses on VQA models that can process textual information within images, a task directly related to text-rich visual understanding.  Its contributions to improving text processing within images are highly relevant to the creation and evaluation of MultiUI and the UIX model.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Ryota Tanaka", "paper_title": "VisualMRC: Machine reading comprehension on document images", "reason": "VisualMRC is a benchmark focused on machine reading comprehension on document images.  Its inclusion in the evaluation demonstrates the broad applicability of the proposed approach beyond web-based scenarios and highlights the dataset's ability to improve model performance on tasks involving text extraction and reasoning from images.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "An Yang", "paper_title": "Qwen2 technical report", "reason": "This paper introduces the Qwen-2 model, a large language model (LLM) used as the backbone for one of the UIX models.  The architecture and training details of Qwen-2 directly impact the design and evaluation of the UIX model, making this paper crucial for understanding the experimental setup and results.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Xiang Yue", "paper_title": "MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI", "reason": "MMMU is a comprehensive multimodal benchmark assessing models on college-level tasks. Its inclusion shows the generality of the MultiUI dataset and UIX model, extending beyond web UIs to various multimodal tasks. The results on MMMU provide evidence for broader applicability and demonstrate the ability of MultiUI to improve performance in diverse domains.", "section_number": 3}]}