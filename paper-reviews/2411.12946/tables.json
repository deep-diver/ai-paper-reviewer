[{"content": "| Approach | Model | ROC-AUC | F1 | Precision | Recall |\n|---|---|---|---|---|---| \n| Fine-tuned cross-encoder classifier | stsb-roberta-base | 0.99 | 0.99 | 0.99 | 0.99 |\n| Fine-tuned bi-encoder classifier | jina-embeddings-v2-small-en | 0.99 | 0.97 | 0.99 | 0.95 |\n| Cosine similarity | bge-large-en-v1.5 | 0.89 | 0.59 | 0.97 | 0.42 |\n| KNN | bge-large-en-v1 | 0.90 | 0.75 | 0.94 | 0.63 |\n| Pre-trained cross-encoder | stsb-roberta-base | 0.73 | 0.68 | 0.53 | 0.93 |\n| Pre-trained colbert | Colbert v2 | 0.78 | 0.72 | 0.72 | 0.73 |\n| Prompt engineering | GPT 4o (2024-08-06) | - | 0.95 | 0.94 | 0.97 |\n| Prompt engineering | GPT 4o Mini (2024-07-18) | - | 0.91 | 0.85 | 0.91 |\n| Zero-shot classifier | GPT 4o Mini (2024-07-18) | 0.99 | 0.97 | 0.95 | 0.99 |", "caption": "Table 1. Performance on Synthetic Dataset Generated by GPT 4o (2024-08-06) (N=17,201)", "description": "This table presents the performance of different models and baselines on a synthetic dataset generated by GPT 40 (on August 6th, 2024). It shows the results of evaluating various approaches to off-topic prompt detection, including fine-tuned and pre-trained models, as well as simpler heuristics such as cosine similarity. The metrics reported are ROC-AUC, F1 score, precision, and recall.  The large dataset size (N=17,201) allows for robust comparisons, and the inclusion of baselines facilitates the assessment of the improvement provided by the fine-tuned models.  The purpose of the table is to compare the performance of the proposed methodology and models against common approaches and to highlight the advantage gained by using a synthetic dataset.", "section": "4. Experiments and Results"}, {"content": "| Approach | Model | ROC-AUC | F1 | Precision | Recall |\n|---|---|---|---|---|---| \n| Fine-tuned cross-encoder classifier | stsb-roberta-base | 0.80 | 0.72 | 0.76 | 0.68 |\n| Fine-tuned bi-encoder classifier | jina-embeddings-v2-small-en | 0.92 | 0.83 | 0.84 | 0.82 |", "caption": "Table 2. Binary Classification Performance on JailbreakBench", "description": "This table presents the results of a binary classification task using the JailbreakBench dataset.  It shows the performance of fine-tuned cross-encoder and bi-encoder classifiers, comparing their performance metrics such as ROC-AUC, F1 score, precision, and recall. The table allows readers to compare the effectiveness of different model architectures (cross-encoder vs. bi-encoder) in detecting jailbreak attempts on LLMs.", "section": "4.3 External Datasets"}, {"content": "| Benchmark | Approach | Model | Recall |\n|---|---|---|---| \n| HarmBench | Fine-tuned cross-encoder classifier | stsb-roberta-base | 0.83 |\n|  | Fine-tuned bi-encoder classifier | jina-embeddings-v2-small-en | 0.99 |\n| TrustLLM | Fine-tuned cross-encoder classifier | stsb-roberta-base | 0.78 |\n|  | Fine-tuned bi-encoder classifier | jina-embeddings-v2-small-en | 0.97 |\n| Localised harmful prompts | Fine-tuned cross-encoder classifier | stsb-roberta-base | 0.74 |\n|  | Fine-tuned bi-encoder classifier | jina-embeddings-v2-small-en | 0.86 |", "caption": "Table 3. Recall for HarmBench, TrustLLM, and internal dataset on localised harmful prompts", "description": "This table presents the recall scores achieved by two fine-tuned models (fine-tuned cross-encoder classifier and fine-tuned bi-encoder classifier) when evaluated against three different datasets containing harmful or inappropriate prompts: HarmBench, TrustLLM, and an internal dataset specific to Singapore.  The internal dataset contains localized harmful prompts, reflecting the nuances of the Singaporean context.  The table highlights the models' performance in identifying harmful content within each dataset, showcasing their ability to generalize across diverse resources.", "section": "4.3 External Datasets"}, {"content": "| Approach | Model | Processed Pairs Per Minute | Latency Per Pair (s) |\n|---|---|---|---| \n| Fine-tuned bi-encoder classifier | jina-embeddings-v2-small-en | 2,216 | 0.027 |\n| Fine-tuned cross-encoder classifier | stsb-roberta-base | 1,919 | 0.031 |", "caption": "Table 4. Inference Speed Benchmarking", "description": "This table presents the inference speed of two fine-tuned models (fine-tuned bi-encoder classifier and fine-tuned cross-encoder classifier) used for off-topic prompt detection.  It shows the number of prompt pairs each model can process per minute and the latency (in seconds) for processing a single pair.  This data is important for evaluating the real-time performance and suitability of these models for deployment as guardrails.", "section": "4.4 Inference Speed Benchmarking"}]