{"references": [{"fullname_first_author": "Paul F Christiano", "paper_title": "Deep Reinforcement Learning from Human Preferences", "publication_date": "2017-12-01", "reason": "This paper is foundational to the field of AI alignment, which is crucial to the paper's focus on LLM safety and responsible use."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper details a key method for aligning LLMs to human preferences, a technique directly relevant to mitigating risks associated with off-topic prompts."}, {"fullname_first_author": "Patrick Chao", "paper_title": "Jailbreak-Bench: An Open Robustness Benchmark for Jailbreaking Large Language Models", "publication_date": "2024-04-01", "reason": "This paper introduces a benchmark specifically designed to evaluate the robustness of LLMs against jailbreaking, an important aspect of LLM safety which relates to the paper's discussion of misuse categories."}, {"fullname_first_author": "Mantas Mazeika", "paper_title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "publication_date": "2024-02-01", "reason": "This paper provides a standardized framework for evaluating the safety of LLMs with respect to harmful outputs, directly relevant to the paper's interest in developing guardrails that prevent the generation of undesired content."}, {"fullname_first_author": "Traian Rebedea", "paper_title": "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails", "publication_date": "2023-10-01", "reason": "This paper introduces a relevant toolkit for developing guardrails, providing valuable context for the paper's proposed methodology and open-source contributions."}]}