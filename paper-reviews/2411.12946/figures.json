[{"figure_path": "https://arxiv.org/html/2411.12946/extracted/6011583/example.png", "caption": "Figure 1. Example of on- and off-topic user prompts: The goal is to correctly classify if a prompt is off-topic or not, with respect to the system prompt", "description": "This figure shows two examples of user prompts given to a large language model (LLM). The first example is an on-topic prompt, meaning it is relevant to the task the LLM was designed to perform. The second example is an off-topic prompt, meaning it is irrelevant to the task. The system prompt defines the LLM's intended task, and the goal is to develop a guardrail that can correctly classify whether a user prompt is on-topic or off-topic with respect to the system prompt.  This is crucial for ensuring the LLM is used appropriately and does not produce unexpected or harmful outputs.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2411.12946/extracted/6011583/overview.png", "caption": "Figure 2. Our Guardrail Development Methdology", "description": "This figure illustrates a three-step methodology for developing guardrails to enhance the safety and reliability of LLMs.  Step 1 involves a qualitative analysis of potential vulnerabilities and the definition of acceptable and unacceptable behaviors. Step 2 leverages LLMs to generate synthetic datasets representing diverse use cases, which enhances realism by incorporating few-shot examples and randomizing input parameters. Step 3 focuses on training transformer-based classifiers using this synthetic data, specifically designed to accurately identify and effectively mitigate undesirable inputs. The entire process facilitates the creation of operational guardrails deployable before full deployment of the LLM, making it a pre-deployment approach.", "section": "3 METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2411.12946/extracted/6011583/architecture.png", "caption": "Figure 3. Summary of the two modelling approaches for the off-topic prompt detection", "description": "This figure illustrates the architectures of two different models used for off-topic prompt detection: a fine-tuned bi-encoder classifier and a fine-tuned cross-encoder classifier.  The bi-encoder model processes the system prompt and user prompt separately through embedding models, applies cross-attention to allow interaction between the two representations, and then uses a projection and classification layer to predict whether the prompt is on-topic or off-topic. The cross-encoder model concatenates the system and user prompts before feeding them into a pre-trained cross-encoder model, followed by a classification layer for prediction. Both models provide a probability score from 0 to 1 (0 = on-topic, 1 = off-topic).", "section": "3.2.3 Modelling"}, {"figure_path": "https://arxiv.org/html/2411.12946/extracted/6011583/prompt_length.png", "caption": "Figure 4. ROC-AUC Score by User and System Prompt Length", "description": "This figure shows the relationship between the length of user prompts and system prompts and the ROC-AUC score achieved by the fine-tuned bi-encoder and cross-encoder models for off-topic prompt detection.  It visualizes how the model's performance (as measured by ROC-AUC) varies depending on the lengths of both prompt types.  The heatmaps allow for easy visual comparison of ROC-AUC across various lengths of system prompts and user prompts.", "section": "4.2 Performance on Synthetic Data"}, {"figure_path": "https://arxiv.org/html/2411.12946/extracted/6011583/calibration.png", "caption": "Figure 5. Calibration Plot", "description": "This calibration plot displays the reliability of the probability scores outputted by the fine-tuned bi-encoder and cross-encoder models.  It shows the relationship between the predicted probability (the model's confidence in its prediction) and the actual probability (the true rate of correct predictions). A perfectly calibrated model would show a diagonal line, indicating that a predicted probability of 0.8 means that the model is correct 80% of the time. Deviations from this line indicate areas where the model is either overconfident (predicting higher probabilities than it should) or underconfident (predicting lower probabilities than it should).  This plot helps to evaluate the trustworthiness of the probability scores produced by the models, which is important for decision making, as developers may choose to apply different thresholds for flagging or taking actions based on confidence levels.", "section": "4.2 Performance on Synthetic Data"}]