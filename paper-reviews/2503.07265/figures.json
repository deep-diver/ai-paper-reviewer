[{"figure_path": "https://arxiv.org/html/2503.07265/x1.png", "caption": "Figure 1: Comparison of previous straightforward benchmarks and our proposed WISE. (a) Previous benchmarks typically use simple prompts, such as \u201cA photo of two bananas\u201d in GenEval [9], which only require shallow text-image alignment. (b) WISE, in contrast, uses prompts that demand world knowledge and reasoning, such as \u201cEinstein\u2019s favorite musical instrument,\u201d to evaluate a model\u2019s ability to generate images based on deeper understanding.", "description": "Figure 1 contrasts the simplicity of previous text-to-image generation benchmarks (like GenEval, which uses prompts like \"A photo of two bananas\") with the more sophisticated approach of WISE.  While existing benchmarks mainly assess basic visual-text alignment, WISE challenges models with prompts requiring deeper semantic understanding and world knowledge, such as \"Einstein's favorite musical instrument.\" This allows for a more comprehensive evaluation of the model's ability to generate images that accurately reflect nuanced prompts and real-world knowledge.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.07265/x3.png", "caption": "Figure 2: Illustrative samples of WISE from 3 core dimensions with 25 subdomains. By employing non-straightforward semantic prompts, it requires T2I models to perform logical inference grounded in world knowledge for accurate generation of target entities.", "description": "Figure 2 showcases examples from the WISE benchmark dataset, illustrating its breadth and complexity.  WISE evaluates Text-to-Image (T2I) models' ability to generate images based on complex semantic prompts requiring world knowledge and logical reasoning. The figure presents example prompts and corresponding images across three main categories: Cultural Common Sense, Spatio-Temporal Reasoning, and Natural Science. Each category is further divided into 25 sub-domains, demonstrating the diverse range of prompts that test a model's understanding of the world. The prompts intentionally go beyond simple keyword matching; instead, they require models to perform logical inference and integrate their world knowledge to generate accurate and relevant images.", "section": "3. Our benchmark: WISE"}, {"figure_path": "https://arxiv.org/html/2503.07265/x4.png", "caption": "Figure 3: Detailed composition of WISE, consisting of 3 categories and 25 subdomains.", "description": "The figure shows a detailed breakdown of the WISE benchmark dataset, which is composed of three main categories: Cultural Common Sense, Spatio-Temporal Reasoning, and Natural Science.  Each category is further divided into several subdomains (25 in total), providing a comprehensive evaluation of the models' understanding of diverse aspects of world knowledge during image generation.  The subdomains within each category are visually represented, offering a clear overview of the benchmark's structure and scope.", "section": "3. Our benchmark: WISE"}, {"figure_path": "https://arxiv.org/html/2503.07265/x5.png", "caption": "Figure 4: \nIllustration of the WISE framework, which employs a four-phase verification process (Panel\u00a0I to\u00a0IV) to systematically evaluate generated content across three core dimensions. The two representative cases, science-domain input \u201ccandle in space\u201d violates oxygen-dependent combustion principles, while spatiotemporal-domain \u201cclose-up of summer maple leaf\u201d contradicts botanical seasonal patterns, both receiving 0 in consistency (see Evaluation Metrics in Panel\u00a0III), confirming the benchmark\u2019s sensitivity in world knowledge conflicts.", "description": "This figure illustrates the WISE (World Knowledge-Informed Semantic Evaluation) framework's four-stage evaluation process.  It highlights how WISE assesses generated images' alignment with world knowledge across three core dimensions (Cultural Common Sense, Spatio-temporal Reasoning, and Natural Science).  Two example prompts are shown: \"a candle in space\" (Natural Science) and \"a close-up of a maple leaf in summer\" (Spatio-temporal Reasoning).  Both prompts reveal limitations in the models' understanding of fundamental scientific and seasonal facts, respectively, resulting in a consistency score of 0.  This demonstrates WISE's ability to effectively identify knowledge-related conflicts in generated images.", "section": "3.2 Evaluation Framework"}]