[{"content": "| Method | FID \u2193 | FVD \u2193 | PSNR \u2191 | SSIM \u2191 | LPIPS \u2191 |  | FID \u2193 | AED \u2193 | APD \u2193 |\n|---|---|---|---|---|---|---|---|---|---| \n| Liveportrait[5] | 43.84 | 262.19 | 30.66 | 0.649 | 0.228 |  | 313.09 | 1.02 | 0.204 |\n| Aniportrait[19] | 38.34 | 384.98 | 30.78 | 0.695 | 0.147 |  | 309.52 | 0.96 | 0.068 |\n| FollowyourEmoji[11] | 39.11 | 301.71 | 30.91 | 0.695 | 0.152 |  | 312.46 | 0.97 | 0.071 |\n| **Ours** | **37.69** | **231.55** | **31.08** | **0.704** | **0.143** |  | **304.35** | **0.81** | **0.051** |", "caption": "Table 1: In comparing our method with the open-source SOTA, it\u2019s important to note that during FVD evaluation, 25 continuous frames are randomly selected from each sample video to calculate the metrics. This leads to variations in the absolute values of test results each time; however, after multiple validations, we found that their relative rankings remain consistent with the values presented in the table.", "description": "This table compares the performance of the proposed method with state-of-the-art (SOTA) open-source methods for both self-reenactment and cross-reenactment tasks.  Self-reenactment uses a video of a subject as both reference and driving input, while cross-reenactment uses a separate reference image and a driving video.  The metrics used include Fr\u00e9chet Inception Distance (FID), Fr\u00e9chet Video Distance (FVD), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), Average Expression Distance (AED), and Average Pose Distance (APD). Note that FVD calculations are based on randomly selecting 25 continuous frames from each video, leading to some variation in the absolute values but consistent relative rankings across multiple evaluations.", "section": "4. Experiments"}, {"content": "| Method | Sample A | Sample B | Sample C |\n|---|---|---|---|\n| SD_EXP txt2im | [https://arxiv.org/html/2410.22901/a_sd_t2i.png](https://arxiv.org/html/2410.22901/a_sd_t2i.png) | [https://arxiv.org/html/2410.22901/b_sd_t2i.png](https://arxiv.org/html/2410.22901/b_sd_t2i.png) | [https://arxiv.org/html/2410.22901/c_sd_t2i.png](https://arxiv.org/html/2410.22901/c_sd_t2i.png) |\n| SK_EXP txt2im | [https://arxiv.org/html/2410.22901/a_sk_t2i.png](https://arxiv.org/html/2410.22901/a_sk_t2i.png) | [https://arxiv.org/html/2410.22901/b_sk_t2i.png](https://arxiv.org/html/2410.22901/b_sk_t2i.png) | [https://arxiv.org/html/2410.22901/c_sk_t2i.png](https://arxiv.org/html/2410.22901/c_sk_t2i.png) |\n| SD_EXP im2im | [https://arxiv.org/html/2410.22901/a_sd_i2i.png](https://arxiv.org/html/2410.22901/a_sd_i2i.png) | [https://arxiv.org/html/2410.22901/b_sd_i2i.png](https://arxiv.org/html/2410.22901/b_sd_i2i.png) | [https://arxiv.org/html/2410.22901/c_sd_i2i.png](https://arxiv.org/html/2410.22901/c_sd_i2i.png) |\n| SK_EXP im2im | [https://arxiv.org/html/2410.22901/a_sk_i2i.png](https://arxiv.org/html/2410.22901/a_sk_i2i.png) | [https://arxiv.org/html/2410.22901/b_sk_i2i.png](https://arxiv.org/html/2410.22901/b_sk_i2i.png) | [https://arxiv.org/html/2410.22901/c_sk_i2i.png](https://arxiv.org/html/2410.22901/c_sk_i2i.png) |\n| GT | [https://arxiv.org/html/2410.22901/a.png](https://arxiv.org/html/2410.22901/a.png) | [https://arxiv.org/html/2410.22901/b.png](https://arxiv.org/html/2410.22901/b.png) | [https://arxiv.org/html/2410.22901/c.png](https://arxiv.org/html/2410.22901/c.png) |", "caption": "Table 2: Evaluation results for the SKAttentions-related experiments, where the \u201dsim\u201d metric represents the similarity between the faces in the reference image and the generated output.", "description": "Table 2 presents a quantitative analysis of experiments conducted to evaluate the effectiveness of Spatial Knitting Attention (SKA).  The experiments compare different model configurations using various metrics on two datasets: MomoFaceTest1_1W and FFHQTest1_3K. Metrics include FID (Fr\u00e9chet Inception Distance), PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index), and a custom similarity metric, 'sim', which quantifies the similarity between faces in the reference image and those generated. The table allows for a comparison of SKA's performance against baseline models and variations with different input modalities (text-to-image and image-to-image).", "section": "A. Experiments on Spatial Knitting Attentions"}]