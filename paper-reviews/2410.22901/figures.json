[{"figure_path": "https://arxiv.org/html/2410.22901/x1.png", "caption": "Figure 1: Our solution consists of three modules. HMReferenceNet is used to extract Fidelity-Rich features from the reference image, while HMControlNet extracts high-level features such as head pose and facial expression information. HMDenoisingNet receives both sets of features and performs the core denoising function. It can also integrate a fine-tuned Animatediff module to generate continuous video frames.", "description": "The figure illustrates the architecture of the proposed HelloMeme model, which consists of three main modules: HMReferenceNet, HMControlNet, and HMDenoisingNet. HMReferenceNet extracts detailed features from a reference image, capturing high-fidelity information. HMControlNet extracts high-level features, such as head pose and facial expression, from driving images. These two feature sets are then fed into HMDenoisingNet, which performs the core denoising process to generate a new image or video frame. Optionally, a fine-tuned Animatediff module can be integrated into HMDenoisingNet for generating continuous video frames.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2410.22901/x2.png", "caption": "Figure 2: This is the structural diagram of SKCrossAttention, which utilizes the Spatial Knitting Attention mechanism to fuse 2D feature maps with linear features. It performs cross-attention first row by row, then column by column.", "description": "The figure shows the architecture of SKCrossAttention, a mechanism that fuses 2D feature maps with linear features.  Unlike standard cross-attention which flattens the 2D feature map before processing, SKCrossAttention performs cross-attention in two stages: first row-wise, then column-wise. This approach, inspired by the way threads are interwoven in knitting, preserves the spatial structure of the 2D feature map, leading to improved performance, especially when dealing with high-level conditions like exaggerated facial expressions.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2410.22901/x3.png", "caption": "Figure 3: This is the structural diagram of SKReferenceAttention, which uses the Spatial Knitting Attention mechanism to fuse two 2D feature maps. Specifically, the two feature maps are first concatenated row by row, followed by performing self-attention along the rows. Afterward, only the first half of each row is retained. A similar operation is then performed column by column.", "description": "The figure shows the architecture of the SKReferenceAttention module.  This module takes two 2D feature maps as input.  First, it concatenates these maps row-wise. Then, it performs self-attention on each row, which allows the model to capture relationships between features within each row. After the self-attention, only the first half of each row is kept. This process is then repeated column-wise: the remaining feature maps are concatenated column-wise, self-attention is applied to each column, and only the first half of each column is retained. The output is a refined 2D feature map that incorporates information from both input maps.", "section": "3.1. Spatial Knitting Attentions"}, {"figure_path": "https://arxiv.org/html/2410.22901/extracted/5965254/show/gt1.jpg", "caption": "(a) Ground Truth", "description": "This figure displays a comparison of self-reenactment performance across five different methods: ground truth, Liveportrait, Aniportrait, FollowYourEmoji, and the proposed method.  Each method is represented by five frames sampled from a generated video to illustrate the visual results.  The first row shows the ground truth video, with the initial frame outlined in red dashed lines to highlight its use as the reference image.", "section": "4.3 Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2410.22901/extracted/5965254/show/gt2.jpg", "caption": "(b) Liveportrait", "description": "This figure shows a visual comparison of meme video generation results from the Liveportrait method.  The image displays five frames from a video sequence, showcasing the method's ability to generate talking head videos. This allows for a direct visual assessment of the video quality and the method's performance on the task.  The specific frames shown likely highlight key aspects of the video generation process, such as facial expressions, head movements and overall visual fidelity.", "section": "4.3 Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2410.22901/extracted/5965254/show/gt3.jpg", "caption": "(c) Aniportrait", "description": "The figure shows a comparison of self-reenactment performance between different methods. Specifically, it displays five frames sampled from a video generated by the Aniportrait method, where the first frame of the video serves as the reference image. This visual comparison helps to illustrate the quality of video generation, particularly in terms of fidelity and consistency of facial expressions.", "section": "4.3 Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2410.22901/extracted/5965254/show/lp1.jpg", "caption": "(d) FollowyourEmoji", "description": "This figure shows results from the FollowYourEmoji method. It is part of a qualitative comparison of several methods for self-reenactment performance. The image displays five frames sampled from a video generated by FollowYourEmoji, showcasing its ability to generate talking video.  The first frame serves as a reference image and is outlined in red dashed lines. The comparison allows assessment of the visual quality and accuracy of facial expressions and head poses compared to the ground truth.", "section": "4.3 Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2410.22901/extracted/5965254/show/lp2.jpg", "caption": "(e) Ours", "description": "This figure shows a video frame generated by the proposed \"HelloMeme\" method, demonstrating the quality of facial reenactment and the ability to generate realistic meme videos.  It is part of a comparison with other state-of-the-art methods (a-d) to illustrate the superior performance of the proposed method in handling exaggerated facial expressions and generating smooth, continuous video frames.", "section": "4.3 Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2410.22901/extracted/5965254/show/lp3.jpg", "caption": "Figure 4: Examples of self-reenactment performance comparisons, with five frames sampled from each video for illustration. The first row represents the ground truth, with the initial frame serving as the reference image (outlined in red dashed lines).", "description": "Figure 4 presents a qualitative comparison of self-reenactment performance across five different methods.  Each method is shown with five frames from a generated video sequence. The first row displays the ground truth video frames, clearly indicating the initial frame used as a reference image via a red dashed outline. This visualization directly allows for comparison between the ground truth and the outputs of each method, highlighting differences in facial expression and head pose accuracy.  The figure directly supports the claims made in the paper regarding performance.", "section": "4.3 Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2410.22901/extracted/5965254/show/ap1.jpg", "caption": "Figure 5: SD_EXP vs. SK_EXP", "description": "This figure compares the results of two experiments: SD_EXP and SK_EXP.  SD_EXP uses the standard cross-attention mechanism in the Stable Diffusion 1.5 model, while SK_EXP replaces it with the Spatial Knitting Attention (SKA) mechanism.  The comparison demonstrates the impact of SKA on image generation, particularly in terms of visual quality and adherence to various conditions or prompts.  The results show image samples generated under different conditions (text-to-image and image-to-image) for each method, highlighting the effectiveness of SKA in enhancing image generation.", "section": "A. Experiments on Spatial Knitting Attentions"}, {"figure_path": "https://arxiv.org/html/2410.22901/extracted/5965254/show/ap2.jpg", "caption": "Figure 6: ControlNet vs. ControlNetSK", "description": "This figure compares the results of using ControlNet and ControlNetSK for image generation. ControlNet is a pre-existing method, while ControlNetSK incorporates Spatial Knitting Attention.  Both methods were tested under the same conditions. The figure visually demonstrates the outputs for different tasks (text-to-image and image-to-image) using both methods. The Ground Truth images are also provided for reference. This allows for a direct visual comparison of the image quality and fidelity generated by each method.", "section": "A. Experiments on Spatial Knitting Attentions"}, {"figure_path": "https://arxiv.org/html/2410.22901/extracted/5965254/show/ap3.jpg", "caption": "Figure 7: IPAdapter vs. IPAdapterSK", "description": "This figure compares the performance of IPAdapter and IPAdapterSK, two methods for integrating face features into diffusion models. The top row shows examples where only text was used as input to the model, and the second row shows examples where both text and images were used as input. IPAdapterSK uses Spatial Knitting Attention, which improved the model's ability to generate high-quality images, even when given limited information.  The 'Mix' column shows a combination of both approaches.", "section": "A.2. Application of SKAttentions"}]