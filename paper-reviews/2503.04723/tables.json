[{"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.1\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S3.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.2.1\">Input Length</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S3.T1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.3.1\">Output Length</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.2.2.1\">LongAlpaca-12k</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.1.2.2.2\">5,945</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.1.2.2.3\">218</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.3.3.1\">LongAlign-10k</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.3.3.2\">12,134</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.3.3.3\">169</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.4.4.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.4.4.1.1\">\\hdashline</span>Suri</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.4.4.2\">347</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.4.4.3\">4,371</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.1.5.5.1\">LongWriter-6k</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T1.1.5.5.2\">262</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T1.1.5.5.3\">5,333</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 1: Comparison of Average Input and Output Lengths (words) for Long-Context SFT Datasets.", "description": "This table presents a comparison of the average input and output lengths, measured in words, across four different long-context supervised fine-tuning (SFT) datasets.  It highlights the disparity between the lengths of input prompts and the corresponding generated outputs in these datasets, which are frequently used in training long-context large language models (LLMs). This disparity is relevant to understanding the current challenges in training models capable of producing long-form text outputs.", "section": "3. Current State of Long-Output LLMs"}]