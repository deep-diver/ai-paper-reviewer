{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper introduces CLIP, which is the foundational model upon which many downstream tasks and other models discussed in the paper are based, making it highly important."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This paper is crucial because it presents latent diffusion models, which, like CLIP, are used by many downstream tasks discussed in the paper, and CLIP is used as one part of it's structure."}, {"fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2015-01-01", "reason": "Deep residual learning is a foundational concept in image recognition, enabling the training of very deep networks, which is likely a basis for the models in this paper."}, {"fullname_first_author": "Aditya Ramesh", "paper_title": "Zero-shot text-to-image generation", "publication_date": "2021-01-01", "reason": "This paper is an important reference, as it discusses zero-shot text-to-image generation, which is used in the downstream tasks discussed in this paper."}, {"fullname_first_author": "Xiaohua Zhai", "paper_title": "Sigmoid loss for language image pre-training", "publication_date": "2023-01-01", "reason": "This paper is an important reference because it discusses language image pre-training, which is used to train CLIP and SigLIP models, both of which are central models for this paper."}]}