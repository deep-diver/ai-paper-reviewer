{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is highly relevant because it provides a comprehensive overview of a state-of-the-art large language model (LLM), GPT-4, which is directly related to the research on long-context LLMs.  Understanding GPT-4's capabilities and limitations is essential for developing efficient inference methods for long-context LLMs, which is the core focus of the current paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yushi Bai", "paper_title": "LongBench: A bilingual, multitask benchmark for long context understanding", "reason": "This paper introduces a benchmark specifically designed for evaluating the performance of long-context LLMs.  The benchmark's inclusion in the study adds significant weight to the empirical results.  LongBench provides a robust and comprehensive evaluation framework, which directly relates to the performance of the proposed SHADOWKV system.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Suyu Ge", "paper_title": "Model tells you what to discard: Adaptive KV cache compression for LLMs", "reason": "This paper presents a method for compressing the KV cache in LLMs, which is directly related to the core problem addressed in the current paper.  The techniques used in this paper may influence or provide an alternative approach that could help to improve the performance of the system being developed.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Jiaao He", "paper_title": "Fastdecode: High-throughput GPU-efficient LLM serving using heterogeneous pipelines", "reason": "This paper explores methods for improving the throughput of LLM inference using heterogeneous pipelines, which complements the goal of the current paper. The approaches discussed in this paper could be used for comparison or as inspiration for improving performance further.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Coleman Hooper", "paper_title": "KVQuant: Towards 10 million context length LLM inference with KV cache quantization", "reason": "This paper focuses on quantization techniques for KV cache compression in LLMs. Quantization is a well-known method for reducing memory usage and increasing speed, so understanding the relevant techniques is crucial in this research.  This paper provides valuable insights into the latest advancements in quantization that could inform the design and optimization of the current model.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Jordan Juravsky", "paper_title": "Hydragen: High-throughput LLM inference with shared prefixes", "reason": "This paper directly addresses the problem of high-throughput LLM inference, which is the main focus of the current research. Studying this work helps to compare and contrast different approaches for efficient inference, possibly providing insights for improvements.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Woosuk Kwon", "paper_title": "Efficient memory management for large language model serving with pagedattention", "reason": "This paper is highly relevant because it focuses on efficient memory management for large language models (LLMs).  Memory management is crucial for long-context LLMs because the KV cache size scales with sequence length, leading to significant memory usage. Efficient memory management is one of the key challenges addressed in this work.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yaniv Leviathan", "paper_title": "Fast inference from transformers via speculative decoding", "reason": "This paper explores speculative decoding as a method for accelerating transformer-based model inference.  Speculative decoding is a technique that may help reduce the latency of decoding long sequences, improving performance.  The comparison and contrast between different approaches may provide inspiration for improvements.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yucheng Li", "paper_title": "Compressing context to enhance inference efficiency of large language models", "reason": "This paper is relevant to the core problem addressed in the current paper.  The research focus is on compressing the context to enhance inference efficiency of large language models, directly aiming at improving performance.  Understanding this paper's methods could enhance our own.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Hao Liu", "paper_title": "World model on million-length video and language with ringattention", "reason": "This paper is highly relevant because it tackles the challenge of handling extremely long contexts (millions of tokens) in LLMs.  The techniques for handling very long contexts are relevant and comparable to the problem this paper solves.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zichang Liu", "paper_title": "Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression at test time", "reason": "This paper proposes a method for compressing the KV cache, which is directly related to the core problem addressed in the current research.  The approach and techniques discussed in this paper may help influence or provide an alternative approach for better results.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zirui Liu", "paper_title": "Kivi: A tuning-free asymmetric 2bit quantization for KV cache", "reason": "This paper investigates quantization techniques for KV cache compression, a method well known to decrease memory usage and improve speed.  The proposed quantization technique is important in this research and understanding it will allow for better optimization.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Adam Paszke", "paper_title": "PyTorch: An imperative style, high-performance deep learning library", "reason": "This paper is highly relevant because PyTorch is the deep learning framework used in this paper.  Understanding PyTorch is fundamental to understanding how the proposed methods were implemented and can allow for reproducibility and comparison of results.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Jack W Rae", "paper_title": "Compressive transformers for long-range sequence modelling", "reason": "This paper focuses on compressive transformers for long-range sequence modeling, which is closely related to the work being done in the current paper.  Studying the methods used may provide inspiration or alternative approaches that could lead to improved results.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Ying Sheng", "paper_title": "Flexgen: High-throughput generative inference of large language models with a single GPU", "reason": "This paper is highly relevant as it directly addresses the challenge of high-throughput LLM inference, the core focus of the current paper.  A comparison and contrast of methodologies may show how to improve our proposed system further.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Prajwal Singhania", "paper_title": "Loki: Low-rank keys for efficient sparse attention", "reason": "This paper presents a method for efficient sparse attention using low-rank keys, which is directly relevant to the techniques used in the current research.  The comparison and contrast of methodologies may enhance the overall effectiveness of our system.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zezheng Song", "paper_title": "FMint: Bridging human designed and data pretrained models for differential equation foundation model", "reason": "This paper is related to the foundational models for the research done in this paper.  The paper's significance is its ability to provide a foundational model for further development and comparison of results, especially regarding model performance.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Jianlin Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "reason": "This paper introduces Roformer, which utilizes rotary position embeddings.  Rotary Position Embeddings are crucial to the current paper's methods and understanding them is fundamental to the reproducibility and comparison of results with related work.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Hanshi Sun", "paper_title": "Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding", "reason": "This paper presents a method for accelerating long sequence generation using speculative decoding. Speculative decoding is a technique that may help to reduce the latency of decoding long sequences.  Studying this work may provide inspiration or alternative approaches for performance improvement.", "section_number": 1}]}