{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is foundational as it presents the GPT-4 model, a state-of-the-art large language model that is at the forefront of the field.  The technical report offers detailed insights into its architecture, training process, and capabilities which are relevant to the understanding and advancement of efficient long-context LLM inference. Its scale and performance benchmarks serve as a point of comparison for evaluating other models.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yushi Bai", "paper_title": "LongBench: A bilingual, multitask benchmark for long context understanding", "reason": "LongBench serves as a crucial benchmark for evaluating the performance of various models in handling long contexts, which makes this a pivotal reference when exploring improvements to high-throughput long-context LLM inference.  It provides a broad range of tasks for a comprehensive evaluation.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Suyu Ge", "paper_title": "Model tells you what to discard: Adaptive KV cache compression for LLMs", "reason": "This paper directly addresses the problem of KV cache management in LLMs, a central focus of the target paper. By proposing an adaptive compression technique, it offers insights into efficient memory management that are relevant and applicable to the proposed SHADOWKV system. The adaptive approach is particularly important for addressing the tradeoffs between memory and accuracy.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Coleman Hooper", "paper_title": "Kvquant: Towards 10 million context length llm inference with kv cache quantization", "reason": "This paper explores the use of quantization to improve the efficiency of KV cache operations. As an orthogonal approach to memory management, it provides additional techniques for improving efficiency.  The focus on extremely long contexts is also directly relevant to the challenge of handling long sequences efficiently.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jiaao He", "paper_title": "Fastdecode: High-throughput GPU-efficient LLM serving using heterogeneous pipelines", "reason": "Fastdecode is a relevant reference as it explores techniques for optimizing the efficiency of LLM serving using a heterogeneous pipeline, offering different aspects to improve efficiency and throughput that complement the key approach in the target paper. This work showcases an alternative approach to efficient LLM inference.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Chi-Chih Chang", "paper_title": "Palu: Compressing KV-cache with low-rank projection", "reason": "This paper directly addresses the challenges of large KV caches and proposes a solution using low-rank projection, a related but distinct approach to the SHADOWKV method.  It highlights the potential benefits and challenges of low-rank methods and allows for comparison with the methods presented in the target paper.  The use of low-rank methods is a core component of SHADOWKV.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yaniv Leviathan", "paper_title": "Fast inference from transformers via speculative decoding", "reason": "This paper examines speculative decoding, a technique for improving the speed of transformer-based models. It introduces methods for accelerating inference that are orthogonal to the central memory-saving approach in the target paper, providing additional avenues to consider when optimizing inference performance. Understanding speculative decoding is important for evaluating the trade-offs between speed and accuracy.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Luka Ribar", "paper_title": "SparQ attention: Bandwidth-efficient llm inference", "reason": "SparQ attention is a relevant and comparable method to those discussed in the target paper.  By focusing on efficient sparse attention, it tackles similar challenges related to decoding speed. Comparing SparQ attention to the accurate KV selection methods in SHADOWKV enables a nuanced evaluation of the efficiency trade-offs in various sparse attention techniques.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Prajwal Singhania", "paper_title": "Loki: Low-rank keys for efficient sparse attention", "reason": "This paper focuses on improving the efficiency of sparse attention methods through low-rank key representation, an approach similar to one core aspect of SHADOWKV.  Comparing the performance and techniques of Loki with SHADOWKV allows for a direct assessment of the effectiveness of different low-rank approaches.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jiaming Tang", "paper_title": "Quest: Query-aware sparsity for efficient long-context LLM inference", "reason": "This paper introduces Quest, a dynamic sparse attention method for long-context LLMs, directly addressing the problem of efficiency in handling long sequences.  A comparison to Quest is crucial for evaluating the performance and efficiency gains of SHADOWKV and its proposed accurate KV selection method, which is a similar approach but with different mechanisms for selecting important KV pairs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Hanshi Sun", "paper_title": "Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding", "reason": "Triforce tackles the efficiency of long sequence generation from a different perspective, highlighting the benefit of speculative decoding, which is an approach orthogonal to the core methods proposed in SHADOWKV. Speculative decoding is a complementary approach that can be used in conjunction with other optimization techniques.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Wonbeom Lee", "paper_title": "InfiniGen: Efficient generative inference of large language models with dynamic KV cache management", "reason": "This paper tackles the issue of dynamic KV cache management in LLMs, a directly related problem addressed by SHADOWKV.  It provides insights into alternative methods of managing and utilizing the KV cache, offering a complementary perspective on efficient long-context LLM inference.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Cheng-Ping Hsieh", "paper_title": "Ruler: What's the real context size of your long-context language models?", "reason": "RULER is an essential benchmark for evaluating the performance of long-context LLMs.  It helps to rigorously test the claims regarding efficiency and accuracy by providing a wide range of tasks and complexities in its evaluation.  The comprehensive nature of RULER ensures that claims of improved performance are thoroughly investigated and validated.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Yushi Bai", "paper_title": "LongBench: A bilingual, multitask benchmark for long context understanding", "reason": "LongBench is a benchmark specifically designed for evaluating long-context LLMs, allowing for a comprehensive evaluation of SHADOWKV's performance across a variety of tasks and model sizes. Its inclusion ensures a robust evaluation against existing methods.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Greg Kamradt", "paper_title": "Needle in a haystack - pressure testing LLMs", "reason": "Needle in a Haystack provides a unique evaluation approach by focusing on information retrieval within long contexts, a key capability of long-context LLMs. Its structure is relevant for assessing the performance of SHADOWKV and its ability to effectively handle extremely long contexts.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Yichi Zhang", "paper_title": "Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling", "reason": "This paper proposes a compression technique that uses a pyramidal structure for reducing the size of the KV cache, offering a different approach to memory management in LLMs.  Understanding the strengths and limitations of PyramidKV is crucial for comparing it with the SHADOWKV approach.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Zhenyu Zhang", "paper_title": "H2o: Heavy-hitter oracle for efficient generative inference of large language models", "reason": "This paper proposes an efficient LLM inference method, H2O, that focuses on improving efficiency through careful management of the KV cache.  Comparison with H2O helps to assess the relative advantages and improvements offered by SHADOWKV, particularly in terms of memory reduction and accuracy.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Huiqiang Jiang", "paper_title": "Minference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention", "reason": "Minference offers a significant improvement in pre-filling for long-context models. The integration of SHADOWKV with Minference showcases its compatibility and potential for further improvements in efficiency.  The combination enhances the overall performance by addressing the limitations of both pre-filling and inference phases.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Jordan Juravsky", "paper_title": "Hydragen: High-throughput llm inference with shared prefixes", "reason": "This paper focuses on improving the throughput of LLM inference by utilizing shared prefixes, a technique that can complement the memory optimization techniques employed in SHADOWKV.  The use of shared prefixes is a supplementary approach that can be integrated with other optimization techniques for further efficiency gains.", "section_number": 5}]}