{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is foundational because it presents a comprehensive technical report on a state-of-the-art large language model (LLM), GPT-4. Its detailed analysis of the model's architecture, training process, and capabilities provides valuable insights for understanding the advancements and challenges in the field, making it crucial for the development of efficient inference methods for long-context LLMs. The insights from this paper are particularly relevant to understanding the complexities of handling long contexts and the key-value caches that are central to this paper's focus.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Leo Gao", "paper_title": "The pile: An 800gb dataset of diverse text for language modeling", "reason": "This paper is critical because it introduces The Pile, a massive and diverse text dataset used to train many large language models. Its significance in the context of this paper is undeniable as the scale and diversity of the data significantly impact the performance of LLMs, particularly those aiming to handle long contexts.  The capabilities of long-context LLMs are directly influenced by the quality and size of the training data, making the dataset introduced in this paper instrumental to the field's progress.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yushi Bai", "paper_title": "Longbench: A bilingual, multitask benchmark for long context understanding", "reason": "This paper is key because it presents LongBench, a comprehensive and multilingual benchmark specifically designed for evaluating the long-context capabilities of LLMs.  The benchmark's design and tasks directly address the issues explored in this paper, providing a standardized evaluation framework that is crucial for assessing the effectiveness of proposed techniques for high-throughput long-context LLM inference. Its multi-lingual aspect further strengthens its importance in demonstrating the generalizability of the proposed method.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Suyu Ge", "paper_title": "Model tells you what to discard: Adaptive KV cache compression for LLMs", "reason": "This paper directly addresses the key challenge of managing the KV cache in long-context LLMs by proposing an adaptive compression technique.  The adaptive nature of the method, driven by the model's own understanding, is a crucial aspect that distinguishes it from other methods, addressing the limitations of static approaches. This relates directly to the paper's goal of efficiently managing the KV cache for long-context LLMs.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Coleman Hooper", "paper_title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization", "reason": "This paper is important because it pushes the boundaries of context length in LLMs by focusing on KV cache quantization.  Quantization is a key technique to reduce memory footprint and improve efficiency, which is directly related to the core challenge of this paper. By achieving significant improvements in context length, this paper establishes a benchmark for future research in this direction.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Woosuk Kwon", "paper_title": "Efficient memory management for large language model serving with pagedattention", "reason": "This paper is significant because it explores efficient memory management for large language model serving, which is directly relevant to the problem of handling the large memory footprint associated with long-context LLMs. The paper proposes a novel approach to manage the memory effectively by leveraging a paged attention mechanism, thus contributing to the overall efficiency of the system.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Yaniv Leviathan", "paper_title": "Fast inference from transformers via speculative decoding", "reason": "This paper is important for its focus on speculative decoding as a technique for accelerating inference in transformer models.  Speculative decoding is a method for reducing latency during inference, which is a crucial aspect of efficient long-context LLM inference, and hence directly relates to the key challenges highlighted in this paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yucheng Li", "paper_title": "Compressing context to enhance inference efficiency of large language models", "reason": "This paper is significant as it directly addresses the challenge of enhancing the inference efficiency of large language models by focusing on compressing the context. This is highly relevant to the main focus of the paper, which aims to improve the efficiency of serving long-context LLMs by managing the memory footprint of their KV caches.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Jiaao He", "paper_title": "Fastdecode: High-throughput GPU-efficient LLM serving using heterogeneous pipelines", "reason": "This paper directly tackles the problem of achieving high throughput in LLM serving, particularly on GPUs, which is directly relevant to the goals of this paper. The method presented in this paper uses heterogeneous pipelines to achieve high throughput, which represents a significant advancement in efficient LLM inference and improves the overall efficiency of LLM inference systems.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Harry Dong", "paper_title": "Get more with less: Synthesizing recurrence with KV cache compression for efficient LLM inference", "reason": "This paper is closely related to the central theme of this paper, focusing on techniques to improve the efficiency of LLM inference by compressing the KV cache.  The methods explored for compression are highly relevant and insightful, and the focus on efficiency in inference is directly aligned with the motivation of the paper.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Cheng-Ping Hsieh", "paper_title": "Ruler: What's the real context size of your long-context language models?", "reason": "This paper introduces RULER, a benchmark specifically designed to evaluate the capabilities of long-context LLMs, directly addressing the challenges associated with long context processing. The benchmark's design and results are particularly important to the current paper, providing a standardized evaluation framework for assessing the proposed techniques' performance.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Chi-Chih Chang", "paper_title": "Palu: Compressing KV-cache with low-rank projection", "reason": "This paper is highly relevant to the main focus of this paper as it deals with compressing the KV cache using low-rank projections.  The technique of low-rank projection is key to improving efficiency and reducing memory footprint for long-context LLMs, and this paper provides valuable insights and techniques.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Guangxuan Xiao", "paper_title": "Smoothquant: Accurate and efficient post-training quantization for large language models", "reason": "This paper focuses on quantization techniques for LLMs, specifically targeting improvements in accuracy and efficiency during post-training quantization. Quantization plays a significant role in optimizing memory usage and inference speed in LLMs, which is directly related to the central problem of this paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zirui Liu", "paper_title": "KIVI: A tuning-free asymmetric 2bit quantization for KV cache", "reason": "This paper presents KIVI, a novel quantization technique for the key-value cache that's directly related to the challenges addressed in this paper. By focusing on reducing memory footprint through bit-width reduction, KIVI contributes to the ongoing research in efficient LLM inference.  The \"tuning-free\" aspect suggests robustness and practicality.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jordan Juravsky", "paper_title": "Hydragen: High-throughput LLM inference with shared prefixes", "reason": "This paper tackles the problem of improving throughput in LLM inference, especially relevant given the focus of this paper.  It introduces a novel approach using shared prefixes to significantly enhance inference speed, which contributes to the broader goal of efficient long-context LLM inference.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Ying Sheng", "paper_title": "Flexgen: High-throughput generative inference of large language models with a single GPU", "reason": "This paper is significant for its focus on achieving high-throughput generative inference using a single GPU.  The methods and techniques presented are highly relevant to the challenges of efficiently serving long-context LLMs. The high throughput achieved is a key factor that directly supports the goals of this paper.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Greg Kamradt", "paper_title": "Needle in a haystack - pressure testing LLMs", "reason": "This paper is highly relevant because it introduces the Needle In A Haystack dataset, which is used as one of the benchmarks in this paper.  This benchmark is specifically designed to test the ability of LLMs to retrieve information across various context windows and is crucial for evaluating the performance of the proposed method.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yushi Bai", "paper_title": "LongBench: A bilingual, multitask benchmark for long context understanding", "reason": "LongBench is a crucial benchmark for evaluating the performance of long-context LLMs, which is directly relevant to this paper.  The use of LongBench helps establish the effectiveness and efficiency of SHADOWKV in different scenarios and languages, demonstrating its broader applicability.  The paper's use of diverse tasks is a major strength.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Cheng-Ping Hsieh", "paper_title": "Ruler: What's the real context size of your long-context language models?", "reason": "RULER is used as a key benchmark to evaluate the performance of SHADOWKV, providing an objective measure of its effectiveness.  RULER\u2019s focus on long-context tasks directly aligns with this paper's goal, demonstrating that SHADOWKV can significantly improve efficiency on tasks that are computationally demanding due to long contexts.", "section_number": 5}]}