[{"figure_path": "2410.21465/charts/charts_2_0.png", "caption": "Figure 1: (a) For a sample from PG-19 [12, 40] fed into Llama-3.1-8B, the pre-RoPE keys are the most low-rank, as indicated by the sharpest decay in singular values. (b) Average similarities, defined in Section 3.1, between rank-256 truncated SVD projections of pre-RoPE keys from PG-19 sequences using Llama-3.1-8B. Similarity is measured between a length 16K \"Context\" and either a 16K+2K continuation on \"Context\" (\"Extended context\") or a new length 16K sequence (\"Inter-context\"). Pre-RoPE keys within sequences exhibit similar low-rank subspaces, while those between sequences show different patterns. (c) The relative overhead of singular value decomposition (SVD) decreases as sequence length scales for the pre-filling stage.", "description": "Figure 1 shows the low-rank properties of pre-RoPE keys, the similarity between key subspaces within and between sequences, and the decreasing overhead of SVD with increasing sequence length.", "section": "3 Observations"}, {"figure_path": "2410.21465/charts/charts_4_0.png", "caption": "Figure 3: (a) Accuracy on the needle retrieval task across various ranks shows that the pre-RoPE key cache can be compressed by over 6 times without a drop in accuracy. (b) The number of notable outlier chunks is small, taking only 0.2-0.3%. (c) The KV cache has a high hit rate, reducing computations and data movements by over 60% for each decoding step.", "description": "Figure 3 shows the low rank properties of pre-RoPE keys, the small number of outlier chunks, and high hit rate of KV cache in SHADOWKV.", "section": "Observations"}, {"figure_path": "2410.21465/charts/charts_9_0.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart visualizes the performance of different LLMs on the Needle In A Haystack benchmark with and without SHADOWKV, demonstrating the ability to process information at various positions across different context windows.", "section": "5 Empirical Evaluation"}, {"figure_path": "2410.21465/charts/charts_9_1.png", "caption": "Figure 7: Multi-turn NIAH.", "description": "The chart displays the accuracy of different methods (Full Attention, SnapKV, StreamingLLM, and ShadowKV) across multiple conversation turns in a multi-turn Needle In A Haystack (Multi-NIAH) task.", "section": "5.2 Efficiency Evaluation"}, {"figure_path": "2410.21465/charts/charts_10_0.png", "caption": "Figure 8: Comparison results between the models with full cache, our SHADOWKV, and Quest.", "description": "The chart compares the accuracy of SHADOWKV and Quest against a full cache model across various tasks and sparse KV cache budgets.", "section": "5.3 Ablation Results"}, {"figure_path": "2410.21465/charts/charts_11_0.png", "caption": "Figure 9: (a) Impact of chunk size on batch size and accuracy. (b) Minimal effect of chunk size on hit rate. (c) Accuracy trends across different ranks with Llama-3-8B-1M on different tasks.", "description": "Figure 9 shows the impact of chunk size and rank on batch size, accuracy, and hit rate for different tasks using Llama-3-8B-1M.", "section": "5.3 Ablation Results"}, {"figure_path": "2410.21465/charts/charts_17_0.png", "caption": "Figure 6: Needle In A Haystack.", "description": "The chart displays the ability of SHADOWKV to process information at different positions across various context windows, ranging from 16K to 1M tokens.", "section": "5.1 Accuracy Evaluation"}, {"figure_path": "2410.21465/charts/charts_17_1.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart visualizes the performance of several large language models on the Needle In A Haystack benchmark, showing their ability to retrieve information at different positions within various context window sizes.", "section": "5 Empirical Evaluation"}, {"figure_path": "2410.21465/charts/charts_17_2.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart visualizes the performance of several large language models on the Needle In A Haystack benchmark with and without the SHADOWKV method, showing the ability to process information at different positions across various context windows.", "section": "5 Empirical Evaluation"}, {"figure_path": "2410.21465/charts/charts_17_3.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart visualizes the performance of different LLMs on the Needle In A Haystack benchmark with and without SHADOWKV, showing the ability to process information at various positions and context window lengths.", "section": "A.3 Needle In A Haystack"}, {"figure_path": "2410.21465/charts/charts_17_4.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart visualizes the performance of several long-context LLMs on the Needle In A Haystack benchmark with and without SHADOWKV, showing the ability to process information at different positions across various context windows.", "section": "5 Empirical Evaluation"}, {"figure_path": "2410.21465/charts/charts_17_5.png", "caption": "Figure 6: Needle In A Haystack.", "description": "The heatmap visualizes the ability of SHADOWKV to process information at different positions across various context windows, ranging from 16K to 1M tokens.", "section": "5.1 Accuracy Evaluation"}, {"figure_path": "2410.21465/charts/charts_17_6.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart visualizes the performance of different LLMs on the Needle In A Haystack benchmark with and without SHADOWKV, showing the accuracy of information retrieval across various context lengths and needle depths.", "section": "5 Empirical Evaluation"}, {"figure_path": "2410.21465/charts/charts_17_7.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The heatmap shows the accuracy of different LLMs on the Needle In A Haystack dataset with and without the application of SHADOWKV across various context lengths.", "section": "5.1 Accuracy Evaluation"}, {"figure_path": "2410.21465/charts/charts_17_8.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart displays the accuracy of different LLMs in retrieving information at various positions across different context windows in the Needle In A Haystack dataset, with and without SHADOWKV.", "section": "5.1 Accuracy Evaluation"}, {"figure_path": "2410.21465/charts/charts_17_9.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart visualizes the accuracy of different LLMs with and without SHADOWKV on the Needle In A Haystack dataset across various context window lengths.", "section": "5.1 Accuracy Evaluation"}]