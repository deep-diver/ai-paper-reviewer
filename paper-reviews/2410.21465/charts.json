[{"figure_path": "2410.21465/charts/charts_2_0.png", "caption": "Figure 1: (a) For a sample from PG-19 [12, 40] fed into Llama-3.1-8B, the pre-RoPE keys are the most\nlow-rank, as indicated by the sharpest decay in singular values. (b) Average similarities, defined in Section 3.1,\nbetween rank-256 truncated SVD projections of pre-RoPE keys from PG-19 sequences using Llama-3.1-8B.\nSimilarity is measured between a length 16K \"Context\" and either a 16K+2K continuation on \"Context\"\n(\"Extended context\") or a new length 16K sequence (\"Inter-context\"). Pre-RoPE keys within sequences exhibit\nsimilar low-rank subspaces, while those between sequences show different patterns. (c) The relative overhead\nof singular value decomposition (SVD) decreases as sequence length scales for the pre-filling stage.", "description": "The chart displays the low-rank properties of pre-RoPE keys, subspace similarity between sequences, and the relative overhead of SVD for different sequence lengths.", "section": "3 Observations"}, {"figure_path": "2410.21465/charts/charts_4_0.png", "caption": "Figure 3: (a) Accuracy on the needle retrieval task across various ranks shows that the pre-RoPE key cache can be compressed by over 6 times without a drop in accuracy. (b) The number of notable outlier chunks is small, taking only 0.2-0.3%. (c) The KV cache has a high hit rate, reducing computations and data movements by over 60% for each decoding step.", "description": "The chart displays the low-rank properties of the pre-RoPE keys, the small number of outlier chunks, and the high hit rate of the KV cache, supporting the design choices of SHADOWKV.", "section": "3 Observations"}, {"figure_path": "2410.21465/charts/charts_9_0.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart visualizes the performance of different LLMs on the Needle In A Haystack benchmark with and without the SHADOWKV optimization.", "section": "5.1 Accuracy Evaluation"}, {"figure_path": "2410.21465/charts/charts_9_1.png", "caption": "Figure 7: Multi-turn NIAH.", "description": "The chart displays the accuracy of different methods (Full Attention, SnapKV, StreamingLLM, and ShadowKV) across multiple conversation turns in a multi-turn Needle In A Haystack (Multi-NIAH) task.", "section": "5.2 Efficiency Evaluation"}, {"figure_path": "2410.21465/charts/charts_10_0.png", "caption": "Figure 8: Comparison results between the models with full cache, our SHADOWKV, and Quest.", "description": "The chart compares the accuracy of SHADOWKV and Quest models against a model with full cache across various tasks and sparse KV cache budgets.", "section": "5.3 Ablation Results"}, {"figure_path": "2410.21465/charts/charts_11_0.png", "caption": "Figure 9: (a) Impact of chunk size on batch size and accuracy. (b) Minimal effect of chunk size on hit rate. (c) Accuracy trends across different ranks with Llama-3-8B-1M on different tasks.", "description": "Figure 9 shows the impact of chunk size and rank on batch size, accuracy, and chunk hit rate for Llama-3-8B-1M across different tasks.", "section": "5.3 Ablation Results"}, {"figure_path": "2410.21465/charts/charts_17_0.png", "caption": "Figure 6: Needle In A Haystack.", "description": "The chart visualizes the ability of SHADOWKV to process information at different positions across various context windows, ranging from 16K to 1M tokens.", "section": "5.1 Accuracy Evaluation"}, {"figure_path": "2410.21465/charts/charts_17_1.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart visualizes the performance of different LLMs on the Needle In A Haystack benchmark with and without the proposed SHADOWKV method, showing the accuracy of information retrieval across various context windows and needle depths.", "section": "5.1 Accuracy Evaluation"}, {"figure_path": "2410.21465/charts/charts_17_2.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart visualizes the performance of different models with and without SHADOWKV on the Needle In A Haystack benchmark, showing the impact of SHADOWKV on information retrieval capabilities across various context lengths and depths.", "section": "A.3 Needle In A Haystack"}, {"figure_path": "2410.21465/charts/charts_17_3.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart displays the performance of different large language models on the Needle In A Haystack benchmark, with and without the SHADOWKV optimization, showing the ability to process information at various positions within different context window sizes.", "section": "5.1 Accuracy Evaluation"}, {"figure_path": "2410.21465/charts/charts_17_4.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart displays the accuracy of several large language models (LLMs) in retrieving information from different positions within various context window lengths, with and without the use of SHADOWKV.", "section": "5.1 Accuracy Evaluation"}, {"figure_path": "2410.21465/charts/charts_17_5.png", "caption": "Figure 6: Needle In A Haystack.", "description": "The heatmap visualizes the accuracy of SHADOWKV in retrieving information at various positions across different context window lengths.", "section": "5.1 Accuracy Evaluation"}, {"figure_path": "2410.21465/charts/charts_17_6.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart visualizes the performance of different LLMs (GLM-4-9B-1M, Llama-3.1-8B-Instruct, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K) on the Needle In A Haystack benchmark, comparing the models with and without SHADOWKV.", "section": "A.3 Needle In A Haystack"}, {"figure_path": "2410.21465/charts/charts_17_7.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart visualizes the performance of different LLMs on the Needle In A Haystack benchmark with and without the SHADOWKV optimization.", "section": "5 Empirical Evaluation"}, {"figure_path": "2410.21465/charts/charts_17_8.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart visualizes the performance of different LLMs on the Needle In A Haystack benchmark with and without SHADOWKV, showing the ability of SHADOWKV to maintain performance across various context lengths.", "section": "5 Empirical Evaluation"}, {"figure_path": "2410.21465/charts/charts_17_9.png", "caption": "Figure 10: Needle In A Haystack [24] results using GLM-4-9B-1M [14], Llama-3.1-8B-Instruct [33], Yi-9B-200K [3], Phi-3-Mini-128K [1], and Qwen2-7B-128K [59].", "description": "The chart visualizes the performance of different LLMs on the Needle In A Haystack benchmark with and without the SHADOWKV optimization, showing the accuracy of retrieving information at different positions across various context windows.", "section": "5.1 Accuracy Evaluation"}]