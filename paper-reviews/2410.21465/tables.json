[{"figure_path": "2410.21465/tables/table_7_0.html", "caption": "Table 1: Performance of different models and different methods on RULER [20] evaluated at length of 128K. SHADOWKV outperforms other methods with a 1.56% sparse budget.", "description": "Table 1 presents the performance comparison of various methods (Loki, Quest, and SHADOWKV) on the RULER benchmark for Llama-3.8B-1M, GLM-4-9B-1M, Llama-3.1-8B, and Yi-9B-200K models, showcasing SHADOWKV's superior accuracy with a minimal sparse budget.", "section": "5.1 Accuracy Evaluation"}, {"figure_path": "2410.21465/tables/table_9_0.html", "caption": "Table 1: Performance of different models and different methods on RULER [20] evaluated at length of 128K. SHADOWKV outperforms other methods with a 1.56% sparse budget.", "description": "Table 1 presents the performance comparison of different models and methods on the RULER benchmark with a context length of 128K, highlighting SHADOWKV's superior performance with a 1.56% sparse budget.", "section": "5.1 Accuracy Evaluation"}, {"figure_path": "2410.21465/tables/table_10_0.html", "caption": "Table 4: Generation throughput (tokens/s) on an A100. The gray text in brackets denotes batch size.", "description": "Table 4 presents the generation throughput (tokens per second) achieved by both full attention and SHADOWKV on an A100 GPU for various models and context lengths.", "section": "5.2 Efficiency Evaluation"}, {"figure_path": "2410.21465/tables/table_16_0.html", "caption": "Table 5: Accuracy of different methods with different models on InfiniteBench [65].", "description": "Table 5 presents the accuracy results of different LLMs (Llama-3-8B-1M, GLM-4-9B-1M, Llama-3.1-8B, and Yi-9B-200K) and their SHADOWKV counterparts across various tasks within the InfiniteBench benchmark.", "section": "A.4 InfiniteBench"}]