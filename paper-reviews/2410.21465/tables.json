[{"figure_path": "2410.21465/tables/table_7_0.html", "caption": "Table 1: Performance of different models and different methods on RULER [20] evaluated at length of 128K. SHADOWKV outperforms other methods with a 1.56% sparse budget.", "description": "Table 1 presents the performance comparison of various methods (Loki, Quest, and SHADOWKV) on the RULER benchmark with different language models, evaluated at a context length of 128K, highlighting SHADOWKV's superior performance with a minimal sparse budget.", "section": "5.1 Accuracy Evaluation"}, {"figure_path": "2410.21465/tables/table_9_0.html", "caption": "Table 1: Performance of different models and different methods on RULER [20] evaluated at length of 128K. SHADOWKV outperforms other methods with a 1.56% sparse budget.", "description": "Table 1 presents the performance comparison of various methods (Loki, Quest, and SHADOWKV) across different models (Llama-3-8B-1M, GLM-4-9B-1M, and Llama-3.1-8B) on the RULER benchmark, showing the accuracy under different sparsity budget.", "section": "5.1 Accuracy Evaluation"}, {"figure_path": "2410.21465/tables/table_10_0.html", "caption": "Table 4: Generation throughput (tokens/s) on an A100. The gray text in brackets denotes batch size.", "description": "Table 4 presents the generation throughput in tokens per second on an A100 GPU for various models and context lengths, comparing full attention with SHADOWKV and showing the throughput gains achieved by SHADOWKV.", "section": "5.2 Efficiency Evaluation"}, {"figure_path": "2410.21465/tables/table_16_0.html", "caption": "Table 5: Accuracy of different methods with different models on InfiniteBench [65].", "description": "Table 5 presents the accuracy of different methods (including SHADOWKV) using different models on the InfiniteBench benchmark.", "section": "A.4 InfiniteBench"}]