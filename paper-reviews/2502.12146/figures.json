[{"figure_path": "https://arxiv.org/html/2502.12146/x1.png", "caption": "Figure 1: Comparison of Three Diffusion-Based Methods for Reward-Driven Optimization: (i) Diffusion Reinforcement Learning, (ii) Diffusion Sampling Trajectory Optimization, and (iii) Diffusion Sharpening.", "description": "Figure 1 illustrates three distinct approaches to optimizing diffusion models using reward signals.  (i) Diffusion Reinforcement Learning focuses on optimizing a single timestep using reinforcement learning techniques, ignoring the trajectory leading up to that step. (ii) Diffusion Sampling Trajectory Optimization extends optimization to the entire sampling trajectory, but this method involves high computational costs. (iii) Diffusion Sharpening, the proposed method, optimizes the sampling trajectory in a more efficient manner using a path integral framework that leverages reward feedback during training to select optimal trajectories and amortizes inference costs.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.12146/x2.png", "caption": "Figure 2: Overview of Our Diffusion Sharpening Framework: (i) Training, (ii) Inference, and (iii) Reward Model Selection", "description": "This figure illustrates the Diffusion Sharpening framework, showcasing its training, inference, and reward model selection processes.  Panel (i) details the training phase, emphasizing trajectory-level optimization and the use of sharpening loss to refine denoising trajectories. Panel (ii) depicts the inference stage, demonstrating how the sharpened denoising trajectory leads to improved generative output quality. Finally, Panel (iii) highlights the flexibility of the framework by demonstrating the capability to integrate diverse reward models such as CLIP scores, compositional metrics from MLLMs, and human preference models, offering a highly adaptable and configurable fine-tuning method.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.12146/x3.png", "caption": "Figure 3: Qualitative results comparing Diffusion Sharpening methods using different reward models. The images show the generated results with CLIP Score, Compositional Reward, MLLM, and Human Preferences as reward models, showcasing the effectiveness of SFT Diffusion Sharpening and RLHF Diffusion Sharpening in diffusion finetuning.", "description": "Figure 3 presents a qualitative comparison of image generation results obtained using different diffusion sharpening methods and reward models.  The figure is divided into four sections, each corresponding to a specific reward model: CLIP Score, Compositional Reward, MLLM, and Human Preferences. Within each section, multiple images generated by three different methods are displayed: a baseline model and the two proposed Diffusion Sharpening approaches (SFT and RLHF). This allows for a visual comparison of the impact of each method on the generated images under different reward structures. The purpose is to demonstrate the effectiveness of the SFT and RLHF Diffusion Sharpening methods in improving the quality and alignment of generated images according to diverse evaluation criteria.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.12146/x4.png", "caption": "Figure 4: SDXL Finetuning Loss across Difference Datasets. Here \u201dDiffusion-Sharpening\u201d represents SFT Diffusion-Sharpening specifically.", "description": "This figure displays the training loss curves for fine-tuning the SDXL model across three different datasets: JourneyDB, Text-to-Image-2M, and Pokemon.  Each curve represents the loss for a different fine-tuning method, comparing the standard baseline method against the SFT (Supervised Fine-Tuning) Diffusion-Sharpening approach.  The x-axis shows the number of training steps, and the y-axis shows the training loss. The plots illustrate how the loss changes over the course of training for each dataset and method, demonstrating the relative convergence speeds and final loss values.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.12146/x5.png", "caption": "Figure 5: Inference Performance of Diffusion Sharpening.", "description": "This figure demonstrates the inference efficiency of the proposed Diffusion Sharpening method compared to other sampling trajectory optimization methods.  It shows how inference time (measured in number of function evaluations, or NFEs) impacts the CLIP score, a measure of image-text alignment quality.  Diffusion Sharpening achieves superior performance with significantly fewer NFEs compared to the alternatives like Demon and Inference Scaling, highlighting its computational efficiency.", "section": "4.3. Model Efficiency"}, {"figure_path": "https://arxiv.org/html/2502.12146/x6.png", "caption": "Figure 6: Diffusion Sharpening Fine-tuning Reward Curve.", "description": "This figure shows the reward curves during the fine-tuning process for both SFT and RLHF Diffusion Sharpening methods.  The x-axis represents the number of training steps, and the y-axis shows the average reward. The shaded area around the curves represents the standard deviation across multiple sampled trajectories at each step.  The plot visually demonstrates the convergence of both methods towards higher rewards as training progresses, indicating that the models are learning to generate more optimal trajectories.  The curves also show that the RLHF method reaches higher rewards faster than the SFT method.", "section": "4.4. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2502.12146/x7.png", "caption": "Figure 7: User Study about Comparision with Other Methods", "description": "This figure presents the results of a user study comparing the performance of Diffusion-Sharpening with other methods.  Users were shown pairs of images generated by different models and asked to select their preferred image.  The results are displayed as a bar chart showing the percentage of times each model was chosen as preferred.  This allows for a direct comparison of the visual quality and user preference between Diffusion-Sharpening and other fine-tuning or optimization methods.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.12146/x8.png", "caption": "Figure 8: The detailed prompt for evaluation with the MMLLM Grader.", "description": "This figure details the instructions given to a large language model (LLM) to evaluate images generated by a text-to-image model. The LLM is instructed to provide a score (0-10) and justification for five aspects of each image: accuracy to prompt, creativity and originality, visual quality and realism, consistency and cohesion, and emotional or thematic resonance.  The LLM should also provide a final weighted overall score for the image.", "section": "C. MLLM Grader Design"}, {"figure_path": "https://arxiv.org/html/2502.12146/x9.png", "caption": "Figure 9: More Qualitative Results for SFT Diffusion-Sharpening.", "description": "This figure displays a diverse set of images generated using the SFT Diffusion-Sharpening method. Each image showcases the method's ability to generate high-quality and detailed outputs across a variety of styles, themes, and subject matters.  The images demonstrate a balance between realism and creativity, and suggest the model's skill in achieving high levels of compositional coherence and artistic proficiency. The diverse subjects further demonstrate SFT Diffusion-Sharpening's capabilities for general image generation, rather than specialization in a specific domain or aesthetic.", "section": "4. Experiments"}]