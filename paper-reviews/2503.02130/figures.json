[{"figure_path": "https://arxiv.org/html/2503.02130/x3.png", "caption": "Figure 1: Default architecture of FoX. (left) A single FoX block. (right) A single FoX (Pro) layer. All RMSNorms on the right are applied independently to each head. \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 is the sigmoid function. \u2297tensor-product\\otimes\u2297 is element-wise multiplication. ShiftLinear implements the computation in Equation\u00a014.", "description": "Figure 1 illustrates the architecture of the Forgetting Transformer (FoX) model. The left panel shows a single FoX block, a fundamental building block of the model, consisting of a multi-head self-attention mechanism, followed by a feed-forward network.  The right panel presents a single FoX (Pro) layer, an enhanced layer incorporating additional architectural components often found in recurrent sequence models for improved performance. These components include output gates for regulating the output of each layer and QK-norm (query-key normalization) for better attention score normalization. Both diagrams show how operations such as RMSNorm (Root Mean Square Layer Normalization), sigmoid activation (\u03c3), and element-wise multiplication are utilized within each block and layer. The ShiftLinear operation, as detailed in Equation 14 of the paper, processes the key inputs to add a recency bias, making recent information more influential in the attention mechanism. All RMSNorm operations in the (Pro) layer are applied independently to each head.", "section": "Architecture design"}, {"figure_path": "https://arxiv.org/html/2503.02130/x33.png", "caption": "Figure 2: (left) Per-token loss L\u2062(i)\ud835\udc3f\ud835\udc56L(i)italic_L ( italic_i ) at different token position i\ud835\udc56iitalic_i. (right) Validation perplexity P\u2062(l)\ud835\udc43\ud835\udc59P(l)italic_P ( italic_l ) over different validation context length l\ud835\udc59litalic_l. The vertical dashed line indicates the training context length. The per-token loss is smoothed using a moving average sliding window of 101101101101 tokens.", "description": "Figure 2 presents a comparison of the Forgetting Transformer (FoX) and the standard Transformer, and also several recurrent models (Mamba-2, HGRN2, DeltaNet), on long-context language modeling.  The left panel shows the per-token loss (L(i)) plotted against the token position (i) within a validation sequence.  A lower loss indicates better performance.  The right panel shows validation perplexity (P(l)) which is the average loss over a given context length (l). Lower perplexity means better performance. The dashed vertical line in both panels denotes the length of the context seen during training. The per-token loss curves are smoothed using a moving average to reduce noise.", "section": "Long-Context Language Modeling"}, {"figure_path": "https://arxiv.org/html/2503.02130/x43.png", "caption": "Figure 3: Visualization of the forget gate weight matrix \ud835\udc6d\ud835\udc6d{\\bm{F}}bold_italic_F and the attention score matrix \ud835\udc68\ud835\udc68{\\bm{A}}bold_italic_A from two heads in different layers. Since \ud835\udc68\ud835\udc68{\\bm{A}}bold_italic_A is very sparse, we only show entries with scores larger than 0.50.50.50.5. These results use FoX (Pro). More examples can be found in Appendix\u00a0F.10.", "description": "This figure visualizes the forget gate weight matrix (F) and the attention score matrix (A) from two different layers of the Forgetting Transformer (FoX) model.  The heatmaps show how the forget gate mechanism modulates attention weights. Because the attention score matrix (A) is sparse, only the entries with scores above 0.5 are displayed.  This visualization helps to understand how the model's attention is influenced by the forget gate, allowing it to selectively focus on relevant parts of the input sequence and effectively 'forget' less relevant information.", "section": "Visualization of forget gate values and attention map"}, {"figure_path": "https://arxiv.org/html/2503.02130/x44.png", "caption": "Figure 4: Needle-in-the-haystack analysis for different models. We show results for the easy mode on the left and the standard mode on the right. The results are scored on a scale of 1111 to 10101010 by GPT-4o-2024-08-06. The vertical dashed line indicates the training context length.", "description": "This figure displays the results of a needle-in-the-haystack test, which evaluates a language model's ability to retrieve specific information from a long sequence of text.  The test is performed using two different modes: an 'easy mode' and a 'standard mode'.  In the easy mode, both the question and the answer are explicitly present in the context.  In the standard mode, only the answer is present.  The model's success rate in each mode is presented as a heatmap, where different colors represent different accuracy scores.  Scores range from 1 to 10 and were determined by GPT-4.  The vertical dashed line in each heatmap represents the length of the training context used to train the model.  By comparing performance across the two modes and different context lengths, we can gain insight into the model's long-context retrieval capabilities.", "section": "4.3 NEEDLE IN THE HAYSTACK"}, {"figure_path": "https://arxiv.org/html/2503.02130/x45.png", "caption": "Figure 5: FoX (Pro) easy mode needle-in-the-haystack results and per-token loss for different numbers of training tokens and learning rates. The vertical dashed line indicates the training context length. More results can be found in Appendix\u00a0F.7.", "description": "This figure displays the results of the \"needle in the haystack\" experiment, a test of long-context retrieval capabilities, using the Forgetting Transformer (FoX) model.  It shows the accuracy of the model at retrieving information from a long document. The \"easy mode\" of the experiment includes both the question and answer in the \"needle.\"  The figure also presents the per-token loss, which measures how well the model uses context at various distances from the predicted token.  Different lines correspond to different numbers of training tokens and learning rates, allowing for the comparison of training data quantity and rate on the performance and loss. The dashed line indicates where the training context ends.", "section": "4.3 NEEDLE IN THE HAYSTACK"}, {"figure_path": "https://arxiv.org/html/2503.02130/x58.png", "caption": "Figure 6: Per-token loss given different model sizes, numbers of training tokens, and training context lengths. At each token index i\ud835\udc56iitalic_i, we report the averaged loss over a window of 101101101101 centered at i\ud835\udc56iitalic_i. We only show results within the training context length to reduce visual clutter. See Appendix\u00a0F.6 for additional results, including length extrapolation and 125M-parameter model results.", "description": "Figure 6 illustrates the impact of model size, training data size, and training context length on the per-token loss of language models.  The plots show the average loss calculated across a 101-token sliding window for each token position. Results are shown only for token positions within the training context length to improve clarity, but Appendix F.6 provides additional results, including an analysis of length extrapolation and results for a 125M parameter model.", "section": "4 EMPIRICAL STUDY"}, {"figure_path": "https://arxiv.org/html/2503.02130/x59.png", "caption": "Figure 7: \nData-dependent forget gate (data-dep) vs. data-independent (data-indep) and fixed forget gate.\n(left and middle-left) Comparison using the LLaMA architecture. (middle-right and right) Comparison using the Pro architecture. We use 360360360360M-parameter models trained on roughly 7.57.57.57.5B tokens on LongCrawl64. All per-token loss curves are smoothed with a moving average sliding window of 1001100110011001 tokens. The vertical dashed line indicates the training context length.", "description": "Figure 7 compares the performance of different forget gate mechanisms in the Forgetting Transformer model.  It contrasts data-dependent, data-independent, and fixed forget gates across two model architectures: the LLaMA and Pro architectures.  The results show the per-token loss for each configuration, calculated using a 360M parameter model trained on 7.5B tokens of the LongCrawl64 dataset. Smoothing is applied to the loss curves using a moving average over 1001 tokens. The training context length is marked by a dashed vertical line.", "section": "4.5 ANALYSES"}]