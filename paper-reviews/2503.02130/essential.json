{"importance": "This paper introduces a novel attention mechanism, potentially influencing future architectures. The Forgetting Transformer's superior performance and ability to incorporate forget gates and hardware-aware implementation offer valuable insights for the community. It addresses the limitations of transformers, opening new research.", "summary": "Transformers get forgetful! This paper introduces the Forgetting Transformer (FoX), incorporating a forget gate into the attention mechanism for improved sequence modeling.", "takeaways": ["Forgetting gates can be naturally incorporated into Transformers by down-weighting unnormalized attention scores in a data-dependent way.", "The Forgetting Transformer (FoX) outperforms the standard Transformer on long-context language modeling and length extrapolation tasks.", "FoX retains Transformer's long-context capabilities and can be implemented in a hardware-aware way with modifications to the FlashAttention algorithm."], "tldr": "Recurrent sequence models use forget gates to manage information, while Transformers lack an explicit forgetting mechanism, potentially limiting their ability to handle long sequences effectively. Current models also suffer in long-context tasks. Addressing these issues is crucial for advancing sequence modeling capabilities. This paper explores integrating a forget gate into the Transformer architecture to improve its ability to process long-range dependencies. \n\nThe paper introduces the **Forgetting Transformer (FoX)**, which incorporates a forget gate into the softmax attention mechanism by down-weighting unnormalized attention scores. FoX outperforms the standard Transformer on long-context language modeling and length extrapolation tasks. It's also compatible with FlashAttention. FoX retains retrieval abilities and achieves near-perfect accuracy in the needle-in-the-haystack test. The Pro block improves both FoX and Transformer.", "affiliation": "Mila & Universit\u00e9 de Montr\u00e9al", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.02130/podcast.wav"}