[{"figure_path": "https://arxiv.org/html/2502.12996/extracted/6214305/figures/std-diloco.png", "caption": "Figure 1: Data flow and operations in standard DiLoCo. Here, 4 workers execute in parallel and alternate sequentially computation (the outer and inner optimization steps) and communication (averaging outer gradients across workers).", "description": "Figure 1 illustrates the data flow and operations within the standard DiLoCo algorithm.  DiLoCo divides the training process into inner and outer optimization phases. In the inner phase, four workers (shown in the diagram) concurrently perform multiple optimization steps using their local data. This phase involves only local computation. Then, in the outer phase, these workers communicate their updates (outer gradients) and collectively average them. This averaged outer gradient is then applied to update the global model parameters, thus completing one iteration. The process repeats, alternating between parallel inner optimization and synchronized outer gradient averaging.", "section": "2. Algorithms"}, {"figure_path": "https://arxiv.org/html/2502.12996/extracted/6214305/figures/delayed-diloco.png", "caption": "Figure 2: Data flow and operations in DiLoCo with delayed outer gradients. Here, 4 workers execute optimization steps in parallel with each other, as well as with the communication required for averaging outer gradients. This is accomplished by delaying the application of the averaged outer gradient in the outer optimizer.", "description": "Figure 2 illustrates the process of DiLoCo with delayed outer gradients.  Multiple workers perform inner optimization steps concurrently.  The communication for averaging the outer gradients happens in parallel with the next inner optimization phase. The application of the averaged outer gradient in the outer optimizer is delayed until after the next inner phase completes. This overlapping of communication and computation reduces idle time.", "section": "Algorithms"}, {"figure_path": "https://arxiv.org/html/2502.12996/extracted/6214305/figures/bandwidth_1b.png", "caption": "(a) 1B parameters model.", "description": "The figure shows the compute utilization simulated across a range of bandwidth for a model with 1 billion parameters. Compute utilization is defined as the percentage of time spent on computation versus communication.  The plot illustrates how different methods (Data-Parallel, Streaming DiLoCo with various overlapping strategies) perform across different bandwidths, revealing the impact of bandwidth limitations on compute efficiency. It highlights the improvements achieved using the proposed eager updates method in reducing the communication overhead and maximizing compute utilization, particularly in low-bandwidth scenarios.", "section": "3.1. Compute utilization simulation"}, {"figure_path": "https://arxiv.org/html/2502.12996/extracted/6214305/figures/bandwidth_10b.png", "caption": "(b) 10B parameters model", "description": "This figure displays the compute utilization simulated across various bandwidths for a 10 billion parameter model. Compute utilization is defined as the percentage of time spent on computation, with the remaining time dedicated to communication.  The graph compares different distributed training methods, illustrating how the compute utilization changes as bandwidth varies. This helps visualize the efficiency of each method in balancing computational work with communication overhead.", "section": "3.1. Compute utilization simulation"}, {"figure_path": "https://arxiv.org/html/2502.12996/extracted/6214305/figures/bandwidth_100b.png", "caption": "(c) 100B parameters model", "description": "This figure shows the compute utilization simulated across a range of bandwidth for a model with 100 billion parameters. Compute utilization is defined as the percentage of time spent on computation versus communication.  The figure compares different distributed training methods: Data-Parallel, Streaming DiLoCo (with various overlap configurations for inner and outer optimization steps), and Streaming DiLoCo with 1-inner-step and 1-outer-step overlapped forward and backward passes.  It demonstrates the effect of bandwidth on compute utilization for each method, showing that the proposed methods significantly improve compute utilization, especially at lower bandwidths.", "section": "3.1 Compute utilization simulation"}, {"figure_path": "https://arxiv.org/html/2502.12996/extracted/6214305/figures/scaling_loss.png", "caption": "Figure 3: Compute Utilization simulated across a range of bandwidth. A compute utilization of 0.8 means 80% of the time is spent in computation, and 20% in communication. Our best method reaches a compute utilization of 95% for models 1B, 10B, and 100B with a bandwidth roughly constant between 1 and 5 Gbit/s. Data-Parallel on the other hand requires 100, 200, and 300Gbit/s.", "description": "This figure illustrates the compute utilization achieved by various distributed training methods across a range of network bandwidths.  Compute utilization represents the percentage of time spent on actual computation versus communication.  The results show that the proposed 'eager updates' method significantly improves compute utilization, achieving nearly 95% utilization for models with 1B, 10B, and 100B parameters, even with relatively low bandwidth (1-5 Gbit/s). In contrast, the standard data-parallel method requires much higher bandwidth (100-300 Gbit/s) to achieve comparable utilization.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.12996/extracted/6214305/figures/scaling_hellaswag.png", "caption": "(a) Evaluation loss on C4", "description": "The figure shows the evaluation loss on the C4 dataset for various models, comparing the performance of different distributed training methods. The x-axis represents the total training FLOPs, and the y-axis represents the evaluation loss. This provides insights into how different training methods scale with model size and computational resources.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.12996/extracted/6214305/figures/stale_vs_eager_h.png", "caption": "(b) HellaSwag accuracy", "description": "The plot shows how the HellaSwag accuracy changes as the total training FLOPS increases for different distributed training methods.  The x-axis represents the total number of floating point operations (FLOPs) performed during training, while the y-axis shows the HellaSwag accuracy achieved. Each line corresponds to a different training method: Data-Parallel, Streaming DiLoCo with 1-inner-step overlap (H=30 and H=100), and Streaming DiLoCo with 1-outer-eager-overlap (H=30 and H=100). The figure illustrates how the accuracy improves as the total training FLOPS increases, but at different rates for each training method, providing insights into their comparative performance and efficiency at various scales.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.12996/extracted/6214305/figures/cu_overlap.png", "caption": "Figure 4: Scaling models from 35M (1.49e17 flops) to 1B parameters (1.9e20 flops) on C4.", "description": "This figure displays scaling results for language models trained on the C4 dataset, demonstrating model performance across different scales.  The x-axis represents the total number of training FLOPs (floating-point operations), ranging from 1.49e17 for the 35M parameter model to 1.9e20 for the 1B parameter model. The y-axis shows two key performance metrics: evaluation loss on the C4 dataset (Figure 4a) and HellaSwag accuracy (Figure 4b).  Each line represents a different training method. The purpose is to visualize how model performance (loss and accuracy) changes with model size, comparing various distributed training methods.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.12996/extracted/6214305/figures/loss_change_compression.png", "caption": "Figure 5: Comparison of overlapping communication over an outer step, using the na\u00efve delayed version (Algorithm\u00a02) and the eager version (Algorithm\u00a03) when varying the number of inner steps H\ud835\udc3bHitalic_H.", "description": "This figure compares two approaches for overlapping communication with computation in DiLoCo, a distributed optimization method.  The x-axis represents the number of inner optimization steps (H) performed before a synchronization step. The y-axis shows the evaluation loss on a benchmark dataset. The blue line shows the performance of the 'eager updates' method, which starts the next inner optimization step immediately after sending the outer gradient, using a local outer gradient estimate as a proxy. The orange line shows the performance of the 'na\u00efve delayed outer gradients' method, which only starts the next inner step after the synchronization step is complete. The results show that the eager update strategy maintains performance even with a large H, while the naive delayed version suffers significantly increased loss as H grows.", "section": "3. Experiments"}]