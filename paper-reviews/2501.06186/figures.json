[{"figure_path": "https://arxiv.org/html/2501.06186/x4.png", "caption": "Figure 1: \nComparison of the reasoning abilities of our model (LlamaV-o1) with closed-source Gemini-1.5-Flash and Claude-3.5-Sonnet on an example in pattern recognition task from our proposed VRC-Bench. While Claude-3.5-Sonnet concludes \"none of the options,\" its reasoning steps lack full alignment with the observed logic (highlighted in red). Gemini-1.5-Flash demonstrates weaker reasoning with less logical coherence details (highlighted in red). Our LlamaV-o1 provides better and more systematic reasoning, identifying that option D follows the established pattern, thereby showcasing its logical reasoning capability. Additional results are presented in Fig. 5.", "description": "Figure 1 demonstrates a comparative analysis of three large language models (LLMs) \u2013 LlamaV-01 (the authors' model), Gemini-1.5-Flash, and Claude-3.5-Sonnet \u2013 on a pattern recognition task from the Visual Reasoning Chain (VRC) benchmark.  The task involves identifying which image follows a pattern established by preceding images. The figure highlights the step-by-step reasoning process of each model.  Claude-3.5-Sonnet's reasoning is shown to be flawed, as it incorrectly concludes that none of the options match, failing to fully align with the established pattern. Gemini-1.5-Flash demonstrates weaker reasoning abilities, lacking the logical coherence displayed by LlamaV-01.  LlamaV-01 exhibits superior, more systematic reasoning, correctly identifying option D as the correct answer and providing a logically sound justification for its selection.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.06186/x5.png", "caption": "Figure 2: \nThe proposed VRC-Bench examples show the diverse and challenging reasoning tasks our benchmark encompasses, spanning a wide range of modalities and contexts. Each example emphasizes step-by-step reasoning, starting from task comprehension and progressing to logical inference and answer generation. The tasks include mathematical reasoning using geometric principles, scientific classification based on molecular structures, visual interpretation of charts and diagrams, artistic identification from historical paintings, and medical diagnosis from tissue images.\nFor instance, one example demonstrates the calculation of an angle in a geometric diagram by leveraging linear pairs and perpendicular relationships. Another highlights scientific reasoning by identifying ethane as a compound based on its molecular composition. Visual perception tasks challenge the model to analyze pie charts for global energy reserves or recognize reflected shapes. Artistic and cultural tasks require identifying paintings and sports based on visual and contextual cues. Finally, tasks in medical imaging and advertisement recognition test the model\u2019s ability to classify tissue types or extract product names through careful observation.", "description": "Figure 2 presents diverse visual reasoning tasks from the VRC-Bench benchmark.  It showcases examples across various modalities and contexts, highlighting the step-by-step reasoning process involved.  These examples include: geometric problem solving using linear pairs and perpendicular lines; scientific reasoning based on molecular structures (e.g., identifying ethane as a compound); visual interpretation of charts and diagrams (e.g., analyzing pie charts); artistic identification based on visual style and context; medical image diagnosis; and extracting information from advertisements. The figure demonstrates the benchmark's breadth in evaluating complex visual reasoning abilities.", "section": "Step-by-Step Visual Reasoning Benchmark: VRC-Bench"}, {"figure_path": "https://arxiv.org/html/2501.06186/x6.png", "caption": "Figure 3: \nThe figure illustrates our comprehensive benchmark structure and comparative performance of LMMs on the proposed ReasoningChain-Bench. (Left) The dataset spans multiple domains, including carefully selected samples for mathematical and logical reasoning (e.g., MathVista [38] with 231 samples and LogicVista with 158 samples), scientific reasoning (e.g., Science-QA [40] with 83 samples), and visual perception (e.g., Blink-IQ-Test [15] with 35 samples). Additionally, it includes specialized areas such as medical imaging (e.g., MMMU-Medical [72] with 29 samples), cultural and social understanding (e.g., ALM-Bench [57] with 104 samples), and document understanding through OCR (e.g., Doc-VQA [46] with 61 samples). By integrating tasks like chart and diagram comprehension (e.g., Chart-VQA [44] with 107 samples), our dataset not only covers a broad spectrum of real-world applications but also expand LMM\u2019s ability to reason, perceive, and interpret complex multimodal information.\n(Right) The bar chart compares various SoTA reasoning models on the VRC-Bench, highlighting both final answer accuracy and step-by-step reasoning scores. The models evaluated for complex reasoning tasks include GPT-4o, Gemini-2.0-Flash, Claude-3.5-Sonnet, and Llava-CoT. Our benchmark evaluates models not only on their ability to generate accurate final answers but also on the coherence and logical flow of their reasoning steps. Our approach, LlamaV-o1, outperforms GPT-4o-mini, Gemini-1.5-Flash and Llava-CoT in the VRC-Bench, achieving superior results in final answer accuracy across complex multimodal reasoning tasks.", "description": "Figure 3 is a dual figure showing (left) the comprehensive benchmark structure of the VRC-Bench dataset and (right) a bar chart comparing the performance of various state-of-the-art large multimodal models (LLMs) on the benchmark. The left panel illustrates the diverse range of domains covered by the VRC-Bench, encompassing mathematical and logical reasoning, scientific reasoning, visual perception, medical imaging, cultural and social understanding, and document understanding tasks.  Each domain includes numerous samples with multiple reasoning steps, testing the LLMs' ability to reason across modalities. The right panel displays the performance of several LLMs, including GPT-40, Gemini models, Claude-3.5-Sonnet, and Llava-CoT on both final answer accuracy and step-by-step reasoning scores. The figure highlights that LlamaV-01 outperforms the other models.", "section": "3 Step-by-Step Visual Reasoning Benchmark: VRC-Bench"}, {"figure_path": "https://arxiv.org/html/2501.06186/x7.png", "caption": "Figure 4: \nThe comprehensive comparison of category-wise and overall performance scores achieved by various models on diverse reasoning tasks. The evaluation spans multiple domains, including Math & Logic Reasoning, Scientific Reasoning, Complex Visual Perception, Chart & Diagram Understanding, Medical Imaging, Social & Cultural Context, Visual Reasoning, and OCR & Document Understanding. The models assessed include GPT-4o, Claude-3.5-Sonnet, Gemini variants, LLAVA-CoT, and our proposed model.\nOur model demonstrates consistently superior performance in critical categories such as Math & Logic Reasoning, Chart & Diagram Understanding, and Medical Imaging, achieving a balanced improvement across both step-by-step reasoning (Step Scores) and final answer accuracy (Final Answer Scores). Compared to LLAVA-CoT, our approach excels in maintaining high accuracy across tasks while showcasing robustness and interpretability in multi-step reasoning challenges.", "description": "Figure 4 presents a detailed comparison of the performance of different large language models (LLMs) on various visual reasoning tasks.  The models compared are GPT-40, Claude-3.5-Sonnet, Gemini (various versions), LLaVA-CoT, and the authors' LlamaV-01 model. The tasks cover eight diverse categories: Math & Logic Reasoning, Scientific Reasoning, Complex Visual Perception, Chart & Diagram Understanding, Medical Imaging, Social & Cultural Context, Visual Reasoning, and OCR & Document Understanding. The figure uses bar charts to show the performance of each model in each category, broken down into both the final answer accuracy and the step-by-step reasoning score.  LlamaV-01 consistently outperforms other models in several key categories and demonstrates high accuracy and robustness across all categories, particularly when compared to the LLaVA-CoT model.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.06186/x8.png", "caption": "Figure 5: \nQualitative comparison between Llava-CoT and the proposed LlamaV-o1 on examples from the VRC-Bench. First row: the example shows visual reasoning capabilities on an example chart. Here, Llava-CoT makes mistakes (highlighted in red) for both the intermediate steps and the final answer. In Comparison, our LlamaV-o1 provides an accurate description of the steps as well as the final answer. Second row: While both Llava-CoT and our LlamaV-o1 provide accurate step descriptions on an example real-world VQA, Llava-CoT fails to infer the final answer. Last row: Llava-CoT fails to accurately answer for the counting task, while also missing the intermediate counting steps. In contrast, our LlamaV-o1 model performs better in intermediate reasoning steps while also providing the accurate final answer.", "description": "Figure 5 presents a qualitative comparison of LlamaV-01 and Llava-CoT's performance on the VRC-Bench.  Three example tasks from different categories of the benchmark are shown.  The first row demonstrates a visual reasoning task using a chart. Llava-CoT's reasoning steps and final answer are incorrect, while LlamaV-01 provides accurate steps and the correct answer. The second row shows a real-world visual question answering (VQA) task where both models offer accurate reasoning steps; however, only LlamaV-01 arrives at the correct final answer. The last row displays a counting task. Llava-CoT both misses intermediate steps and fails to give the correct answer, whereas LlamaV-01's reasoning steps are complete and lead to the correct answer.", "section": "5 Experiments"}]