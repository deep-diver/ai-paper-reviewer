{"references": [{"fullname_first_author": "Duyu Guo", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning", "publication_date": "2025-01-01", "reason": "This paper is important because the current paper directly adapts the GRPO optimization strategy and multi-stage training insights from DeepSeek-R1 to their visual maze navigation task."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-01-01", "reason": "This paper is important as the current work builds upon the concept of CoT reasoning, aiming to induce a similar step-by-step thought process in LLMs for maze navigation."}, {"fullname_first_author": "Xiaohu Jiang", "paper_title": "Supervised fine-tuning in turn improves visual foundation models", "publication_date": "2024-01-01", "reason": "This paper highlights the effectiveness of SFT in enhancing visual foundation models, demonstrating its utility in visual tasks."}, {"fullname_first_author": "Jiajun Mao", "paper_title": "Neural-symbolic visual reasoning: A survey", "publication_date": "2023-01-01", "reason": "This is an important survey paper in neural-symbolic visual reasoning, exploring combining neural networks with symbolic AI for visual tasks."}, {"fullname_first_author": "Jason Wei", "paper_title": "Finetuned language models are zero-shot learners", "publication_date": "2022-01-01", "reason": "This paper is important because it represents a commonly used technique for adapting pre-trained LLMs to specific downstream tasks."}]}