{"references": [{"fullname_first_author": "E. da Silva Maldonado", "paper_title": "Using natural language processing to automatically detect self-admitted technical debt", "publication_date": "2017-11-01", "reason": "This paper is seminal in introducing a widely used dataset for SATD detection and developing models for SATD recognition."}, {"fullname_first_author": "J. Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-18", "reason": "BERT is a foundational model in NLP, enabling advancements in SATD detection by providing a robust language representation."}, {"fullname_first_author": "Y. Liu", "paper_title": "Roberta: A robustly optimized bert pretraining approach", "publication_date": "2019-07-17", "reason": "RoBERTa is a significant improvement over BERT, showing superior performance for SATD detection tasks."}, {"fullname_first_author": "Z. Lan", "paper_title": "Albert: A lite bert for self-supervised learning of language representations", "publication_date": "2019-09-01", "reason": "ALBERT offers a more efficient approach than BERT, vital for handling the large datasets involved in technical debt research."}, {"fullname_first_author": "Z. Feng", "paper_title": "Codebert: A pre-trained model for programming and natural languages", "publication_date": "2020-11-16", "reason": "CodeBERT is a specialized model that directly addresses the need for understanding code context crucial to detecting technical debt in code."}]}