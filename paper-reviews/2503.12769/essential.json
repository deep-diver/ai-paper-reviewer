{"importance": "This paper introduces a novel task and benchmark that addresses the growing need for models that can understand and respond to visual instructions in streaming videos, mirroring real-time human interactions. By addressing the gap in current models that primarily focus on offline video understanding, this research opens up new possibilities for creating more interactive and responsive AI systems in various applications.", "summary": "ViSpeak: Enables visual instruction feedback in streaming videos, enhancing human-AI interaction.", "takeaways": ["Introduces Visual Instruction Feedback, a new task for streaming video understanding, requiring models to actively respond to visual cues.", "Provides ViSpeak-Bench and ViSpeak-Instruct, benchmark and training dataset, to facilitate research in this area.", "Proposes the ViSpeak model, a SOTA streaming video LMM with GPT-4o-level performance"], "tldr": "Current Large Multi-modal Models (LMMs) are primarily focused on offline video understanding. Streaming video poses challenges to recent models due to its time-sensitive and interactive characteristics. The paper aims to extend streaming video understanding and proposes Visual Instruction Feedback, in which models understand visual content and extract instructions. This greatly enhances user-agent interactions.\n\nThe study defines key subtasks and collects ViSpeak-Instruct dataset for training and ViSpeak-Bench for evaluation. The paper proposes the ViSpeak model, a SOTA streaming video understanding LMM with GPT-4o-level performance. After fine-tuning, ViSpeak gains basic visual instruction feedback ability, serving as a solid baseline.", "affiliation": "School of Computer Science and Engineering, Sun Yat-sen University, China", "categories": {"main_category": "Multimodal Learning", "sub_category": "Human-AI Interaction"}, "podcast_path": "2503.12769/podcast.wav"}