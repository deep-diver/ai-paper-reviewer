{"references": [{"fullname_first_author": "Chaoyou Fu", "paper_title": "Vita: Towards open-source interactive omni multimodal llm", "publication_date": "2024-08-05", "reason": "This paper presents VITA, which is used as the base for ViSpeak and has high performance in omni-modal data processing."}, {"fullname_first_author": "Junming Lin", "paper_title": "Streamingbench: Assessing the gap for mllms to achieve streaming video understanding", "publication_date": "2024-11-03", "reason": "This paper introduces StreamingBench, a benchmark used to evaluate the performance of ViSpeak in streaming video understanding."}, {"fullname_first_author": "Yifei Li", "paper_title": "Ovo-bench: How far is your video-Ilms from real-world online video understanding?", "publication_date": "2025-NA-NA", "reason": "This paper introduces OVO-Bench, another benchmark used to evaluate ViSpeak's performance in streaming video understanding."}, {"fullname_first_author": "Yuanhan Zhang", "paper_title": "Llava-next: A strong zero-shot video understanding model", "publication_date": "2024-NA-NA", "reason": "This paper introduces LLava-Next which provides context for video understanding approaches and sets standards."}, {"fullname_first_author": "Joanna Materzynska", "paper_title": "The jester dataset: A large-scale video dataset of human gestures", "publication_date": "2019-NA-NA", "reason": "This paper introduces the Jester Dataset and provides an open source to evaluate performance of recognizing a large number of human gestures."}]}