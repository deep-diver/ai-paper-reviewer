[{"heading_title": "Visual Feedback", "details": {"summary": "Visual feedback in AI, especially for streaming videos, introduces a novel human-agent interaction. It requires agents to be aware of visual cues, like gestures, and react accordingly. **The core idea is to provide real-time assistance and responses based on visual input**, enhancing user experience. The task includes subtasks like wake-up via body language, anomaly warnings, gesture understanding, visual referencing, interruption handling, humor reactions, and ending conversations. Such a system uses visual input for conversational AI. **This interaction improves human-agent relationships** by allowing for in-time, effective exchanges and feedback. **Understanding visual cues greatly improves overall experience**."}}, {"heading_title": "Streaming LMMs", "details": {"summary": "**Streaming Large Multimodal Models (LMMs)** present unique challenges compared to offline models, primarily due to their **time-sensitive, omni-modal and interactive nature**. Unlike offline processing where the entire video context is available, streaming LMMs must make decisions based on continually arriving data. This necessitates prompt responses to questions, and effective integration of streaming audio. The interactive characteristic further distinguishes streaming LMMs, encompassing **non-awakening interaction, interruption, and proactive output**. This interactive aspect is often overlooked, requiring models to respond to user actions at any time, handle interruptions smoothly, and express their mind proactively. These factors make streaming LMMs a complex but important area for enhancing human-agent interactions in real-time visual settings."}}, {"heading_title": "ViSpeak Model", "details": {"summary": "The ViSpeak model is designed as an **omni-modal LMM** (Large Multi-modal Model) to tackle the Visual Instruction Feedback task. It uses an image encoder, an audio encoder, and a large language model to process visual and audio cues. A key innovation is the **two-stream chat template**, which allows the model to process user inputs and generate outputs concurrently, facilitating real-time interaction. To handle visual interruptions, the model can output a \"\u2193 Stop!\" token. The model also has an **informative head** to proactively generate visual outputs. This head helps predict when to speak, addressing a crucial aspect of streaming video processing. A weighted sum of the two streams is used to combine user input and system output, with a linear layer predicting weights. Finally, the streaming inputs from users are segmented into multiple fragments and organized in chronological order."}}, {"heading_title": "Three-Stage Tune", "details": {"summary": "**Three-stage tuning is vital for refining large models.** Initially, aligning the model's template ensures it adapts to specific input formats while retaining core understanding. Subsequent tuning focuses on enhancing task-specific abilities, like question answering, using relevant datasets. Finally, fine-tuning on a targeted dataset optimizes performance for the desired task. This layered approach allows for both broad adaptation and specialized expertise, improving model efficiency."}}, {"heading_title": "ViSpeak-Bench", "details": {"summary": "The paper introduces **ViSpeak-Bench**, a benchmark to evaluate models on Visual Instruction Feedback in streaming videos. It seems designed to test how well models understand and respond to visual cues, gestures, and actions without explicit text instructions. **The benchmark includes diverse subtasks** which address important aspects of human-agent interaction. This task aims to facilitate in-time interaction and assistance effectively. By restricting the feedback in the conversational scenarios, **ViSpeak-Bench becomes a valuable resource** for developing more intuitive and responsive video understanding systems, specifically concerning the integration of visual cues into interactive dialogues. **The benchmark appears to be comprehensive**, which requires the model to respond actively to visual contents. As a result, it can enhance human-agent interactions and advance research in this domain."}}]