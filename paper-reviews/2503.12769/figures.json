[{"figure_path": "https://arxiv.org/html/2503.12769/x1.png", "caption": "Figure 1: Examples of some actions in Visual Instruction Feedback task, which are Visual Wake-Up, Visual Reference, Visual Interruption, and Visual Termination in order. The content in parentheses is displayed by body language instead of text or speech.", "description": "This figure showcases examples of human-computer interaction using body language as instructions in the Visual Instruction Feedback task.  It illustrates four key subtasks: Visual Wake-Up (initiating interaction), Visual Reference (indicating an object), Visual Interruption (stopping the agent), and Visual Termination (ending the interaction).  The image displays screen captures of a video stream at different time points, each showing a distinct body language cue.  The text in parentheses indicates the action performed via body language instead of verbal communication.", "section": "3. Visual Instruction Feedback Task"}, {"figure_path": "https://arxiv.org/html/2503.12769/x2.png", "caption": "Figure 2: ViSpeak is an omni-modality LMM with multiple encoders and a LLM. To support streaming video analysis, ViSpeak takes two input streams as inputs, one for user inputs and one for self-generated outputs. Two streams will be combined into a single one before sending to LLM. An informative head is trained for visual proactive output.", "description": "The ViSpeak model architecture is shown.  It's a large multimodal model (LMM) designed for streaming video understanding.  It uses separate encoders for audio, video, and text inputs. Notably, it accepts two input streams: one for user inputs (e.g., questions, commands) and another for the model's own previously generated outputs. These two streams are combined before being fed into a large language model (LLM). A key feature is the inclusion of an 'informative head', which enables the model to proactively generate visual outputs, such as initiating conversation based on observed actions in the video stream.", "section": "4. The ViSpeak Model"}, {"figure_path": "https://arxiv.org/html/2503.12769/extracted/6284660/images/personnel_information.jpg", "caption": "Figure 3: Statistics on participants who recorded videos. The participants comprised nearly equal numbers of males and females, with ages ranging from 10 to 70 years.", "description": "The figure shows a breakdown of the demographics of the participants involved in recording videos for the ViSpeak-Bench and ViSpeak-Instruct datasets.  The data is presented as two charts: a pie chart displaying the gender distribution (approximately equal numbers of males and females), and a bar chart illustrating the age distribution, ranging from 10 to 70 years old.", "section": "7. More Details for ViSpeak-Bench and ViSpeak-Instruct"}, {"figure_path": "https://arxiv.org/html/2503.12769/x3.png", "caption": "Figure 4: Examples of Visual Wake-Up in ViSpeak-Bench and the corresponding output by ViSpeak.", "description": "This figure shows a sequence of images from the ViSpeak-Bench dataset that depicts a person performing a visual wake-up gesture.  The person is making a sequence of hand gestures, and the figure displays timestamps for each gesture.  Below the images, there are annotations showing the corresponding text prompt and the ViSpeak model\u2019s response to that visual input. The response demonstrates the model's ability to recognize the visual wake-up cue and start a conversation accordingly.  This example illustrates the type of data used for the Visual Instruction Feedback task.", "section": "3. Visual Instruction Feedback Task"}, {"figure_path": "https://arxiv.org/html/2503.12769/x4.png", "caption": "Figure 5: Examples of Visual Termination in ViSpeak-Bench and the corresponding output by ViSpeak. The first round conversation is used as context.", "description": "This figure showcases an example from the ViSpeak-Bench dataset illustrating the \"Visual Termination\" subtask.  It displays a video sequence where a user signals the end of a conversation using a nonverbal cue (body language).  The annotation details the conversation's timing and the user's action.  It also shows the corresponding output generated by the ViSpeak model, demonstrating its ability to understand the visual signal and appropriately terminate the interaction. The first round of conversation is included as context for the model's response.", "section": "3.2. Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2503.12769/x5.png", "caption": "Figure 6: Examples of Visual Interruption in ViSpeak-Bench and the corresponding output by ViSpeak. The first round conversation is used as context.", "description": "This figure showcases examples from the ViSpeak-Bench dataset where a visual interruption occurs.  The top panel displays a sequence of video frames depicting the interruption, showing the user visually signaling a halt in the conversation. The bottom panel presents the original conversation prompt from the dataset, the model's initial response before interruption, the user's visual interruption signal (denoted as '(Stop)'), and finally, the model's concise response to the interruption.  This illustrates the model's ability to recognize and respond appropriately to this non-verbal form of user feedback in real-time.", "section": "3. Visual Instruction Feedback Task"}, {"figure_path": "https://arxiv.org/html/2503.12769/x6.png", "caption": "Figure 7: Examples of Gesture Understanding in ViSpeak-Bench and the corresponding output by ViSpeak. The first round conversation is used as context.", "description": "This figure showcases examples from the ViSpeak-Bench dataset demonstrating the Gesture Understanding subtask.  It visually presents a sequence of frames from a video showing a person making a gesture (a finger heart).  Accompanying the video frames is a transcript of the human-agent conversation.  The first part of the conversation serves as context for the gesture.  The figure then shows the model's (ViSpeak) response to the gesture, highlighting the model's ability to understand and interpret the visual cue within the conversational context.", "section": "3. Visual Instruction Feedback Task"}, {"figure_path": "https://arxiv.org/html/2503.12769/x7.png", "caption": "Figure 8: Examples of Anomaly Warning in ViSpeak-Bench and the corresponding output by ViSpeak.", "description": "This figure showcases two examples from the ViSpeak-Bench dataset demonstrating the 'Anomaly Warning' subtask. Each example shows a short video clip depicting an anomalous event (a person falling on a trampoline). The figure displays the corresponding ground truth caption of the video, the response generated by the GPT-4 model, and finally the response generated by the ViSpeak model.  The comparison highlights the different abilities of GPT-4 and ViSpeak in handling anomaly detection and generating appropriate warnings.", "section": "3.2 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2503.12769/x8.png", "caption": "Figure 9: Examples of Humor Reaction in ViSpeak-Bench and the corresponding output by ViSpeak.", "description": "Figure 9 shows two examples of the Humor Reaction subtask from the ViSpeak-Bench dataset. Each example displays a short video clip with a humorous scene, followed by the ground truth annotation and ViSpeak's generated response.  The annotations provide context and describe the humorous aspects of the video. ViSpeak's response attempts to capture the humor in a conversational and engaging way, demonstrating its ability to understand and generate appropriate responses to humorous visual content.", "section": "3.2 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2503.12769/x9.png", "caption": "Figure 10: Examples of Visual Reference in ViSpeak-Bench and the corresponding output by ViSpeak.", "description": "This figure showcases examples from the ViSpeak-Bench dataset, specifically focusing on the Visual Reference subtask. It presents several short video clips where a user points to an object, and the corresponding response from the ViSpeak model.  The goal of the subtask is to test the model's capability to understand visual references within videos. The figure highlights the accuracy of ViSpeak in correctly identifying the referenced object based on the user's visual cue, demonstrating its ability to process visual information and respond accordingly within the context of the video.", "section": "3. Visual Instruction Feedback Task"}, {"figure_path": "https://arxiv.org/html/2503.12769/x10.png", "caption": "Figure 11: Examples of our self-annotated data for gesture understanding.", "description": "This figure showcases examples from the self-annotated dataset used for gesture understanding.  It displays video segments with annotations showing both the gesture itself and the intended meaning or context of that gesture within a conversation. The annotations help train the model to accurately interpret and respond appropriately to various gestures.  These examples illustrate the diversity of gestures and their accompanying explanations present in the dataset.", "section": "3.2. Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2503.12769/x11.png", "caption": "Figure 12: Examples of failure cases. The \u2018Time Mistake\u2019 denotes the model responds at an improper time. The \u2018Content Mistake\u2019 denotes the model fails to understand the visual content in the video. The \u2018Context Mistake\u2019 means the model is unaware of the context of the conversation.", "description": "Figure 12 showcases three instances where the ViSpeak model's responses were inaccurate.  The first example, 'Time Mistake,' illustrates a situation where the model generates a response at an inappropriate moment in the video, unrelated to the on-screen activity. The second example, 'Content Mistake,' demonstrates a failure to correctly understand the visual content of the video.  The model's response misinterprets what is happening in the video.  The third example, 'Context Mistake,' highlights the model's inability to grasp the context of the conversation, leading to an irrelevant and inaccurate response.", "section": "7.9. Failure Case and Analysis"}]