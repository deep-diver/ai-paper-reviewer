[{"figure_path": "https://arxiv.org/html/2412.01820/x1.png", "caption": "Figure 1: Overview.\nWe present the largest soccer dataset to date, termed SoccerReplay-1988, and the first vision-language foundation model for soccer, MatchVision, capable of various tasks such as event classification and commentary generation.", "description": "Figure 1 provides a high-level overview of the paper's main contributions: the SoccerReplay-1988 dataset and the MatchVision model.  SoccerReplay-1988 is highlighted as the largest publicly available soccer video dataset, containing annotations for a variety of tasks.  MatchVision is presented as a novel vision-language model specifically designed for soccer video understanding.  The figure visually represents the model's capabilities in performing tasks such as event classification and commentary generation, showcasing its multi-modal nature and comprehensive approach to soccer video analysis.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.01820/x2.png", "caption": "Figure 2: \nAutomated Data Curation Pipeline.\nThe collected soccer video data are automatically processed for temporal alignment, event summarization, and anonymization by our curation pipeline.", "description": "This figure illustrates the automated pipeline used to process raw soccer video data.  The pipeline performs three main functions: temporal alignment (synchronizing video frames with commentary timestamps), event summarization (generating concise event labels from commentary text), and anonymization (replacing names and other identifying information with placeholders to protect privacy). The result is a cleaned, structured dataset suitable for training and evaluation of soccer video understanding models. This is a crucial step for the SoccerReplay-1988 dataset creation.", "section": "3. SoccerReplay-1988 Dataset"}, {"figure_path": "https://arxiv.org/html/2412.01820/x3.png", "caption": "Figure 3: Overview of MatchVision:\n(a) The model architecture and its spatiotemporal feature extraction process;\n(b) Details of visual encoder pretraining, such as supervised training and video-language contrastive learning;\n(c) Implementation details of specific heads for various downstream tasks, including commentary generation, foul recognition, and event classification.", "description": "Figure 3 provides a comprehensive overview of the MatchVision model, illustrating its architecture and training process. Part (a) details the model architecture, focusing on the spatiotemporal feature extraction. Part (b) explains the visual encoder pretraining methods, including supervised classification and video-language contrastive learning. Lastly, part (c) illustrates the implementation of specific heads for downstream tasks such as commentary generation, foul recognition, and event classification.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2412.01820/x4.png", "caption": "Figure 4: \nQualitative Results for Event Classification and Commentary Generation.\nHere, \u201cw/o SR\u201d and \u201cw/ SR\u201d indicate models trained without and with the SoccerReplay-1988 dataset, respectively.\nIncorporating the SoccerReplay-1988 dataset improves event classification accuracy.\nFor commentary generation, this enriched training data enables the commentary generation head to demonstrate notable advantages in several aspects: (a) more detailed descriptions, (b) greater linguistic variety, (c) higher accuracy in event depiction, (d) better adherence to updated rules, and (e) improved specificity in scenario response.", "description": "Figure 4 presents a qualitative comparison of event classification and commentary generation results using models trained with and without the SoccerReplay-1988 dataset.  The 'w/o SR' (without SoccerReplay-1988) and 'w/ SR' (with SoccerReplay-1988) columns show that adding the SoccerReplay-1988 dataset significantly improves the accuracy of event classification.  For commentary generation, the enhanced dataset leads to several improvements: more detailed descriptions of events, greater linguistic variety in the generated text, higher accuracy in representing events, better adherence to updated soccer rules, and more precise responses specific to game situations.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.01820/x5.png", "caption": "Figure 5: Comprehensive Visualizations of SoccerReplay-1988 Dataset.", "description": "Figure 5 provides a comprehensive visual overview of the SoccerReplay-1988 dataset. Subfigure (a) compares the size of SoccerReplay-1988 with other existing soccer video datasets. Subfigure (b) shows the distribution of 24 event classes in SoccerReplay-1988. Subfigures (c), (d), (e), and (f) present detailed analyses of the commentary data, including the frequency distribution of commentaries (c), a word cloud illustrating frequent words (d), the distribution of temporal occurrences of commentaries (e), and the distribution of commentary word counts (f).", "section": "3. SoccerReplay-1988 Dataset"}, {"figure_path": "https://arxiv.org/html/2412.01820/x6.png", "caption": "Figure 6: Training Loss Curves of Visual Encoders Pretraining.", "description": "This figure shows the training loss curves for visual encoder pretraining.  Two different pretraining strategies are compared: supervised training and video-language contrastive learning. The curves show how the loss changes over the course of training for both SigLIP (a baseline model) and MatchVision (the proposed model).  The x-axis represents the number of training iterations, and the y-axis represents the loss value. This allows for a visual comparison of the training efficiency and performance of the two different pretraining approaches.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.01820/x7.png", "caption": "Figure 7: \nMore Qualitative Results of Commentary Generation.", "description": "Figure 7 presents qualitative examples of commentary generated by the MatchVision model for various on-field events.  For each event type (corner, clearance, substitution, foul with no card), the figure shows a comparison between the ground truth commentary and the commentary generated by the model. This allows for a visual assessment of the model's ability to generate accurate and contextually appropriate descriptions of soccer game events.", "section": "5.4. Qualitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2412.01820/x8.png", "caption": "Figure 8: \nMore Qualitative Results of Commentary Generation.", "description": "This figure showcases qualitative examples of commentary generated by the model for various events in soccer matches. It visually demonstrates the model's ability to generate detailed and contextually relevant commentary, reflecting nuances and specific details of different game situations.  The comparisons between ground truth (GT) commentary and model-generated (Ours) commentary highlight the model's strength in capturing the key aspects of the event and producing human-quality, descriptive text.", "section": "5.4 Qualitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2412.01820/x9.png", "caption": "Figure 9: \nMore Qualitative Results of Commentary Generation.", "description": "Figure 9 presents qualitative examples of commentary generated by the model for various events in a soccer match.  Each row shows a specific event (e.g., End of Half Game, Show Added Time, etc.), along with the ground truth commentary, and the model's generated commentary.  The figure visually demonstrates the model's ability to generate detailed and contextually relevant commentaries for diverse soccer events, reflecting different aspects of gameplay.", "section": "5.4 Qualitative Comparisons"}]