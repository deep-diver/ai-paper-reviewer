[{"content": "| Visual Encoder | Foul Class | Severity |\n|---|---|---|\n| **Backbone** | **Train** | **Agg.** | **Acc.@1** | **Acc.@2** | **Acc.@1** |\n| ResNet [20] | \u2713 | Mean | 0.31 | 0.56 | 0.34 |\n|  |  | Max | 0.32 | 0.60 | 0.32 |\n| R(2+1)D [46] | \u2713 | Mean | 0.31 | 0.55 | 0.34 |\n|  |  | Max | 0.32 | 0.56 | 0.39 |\n| MViT [12] | \u2713 | Mean | 0.40 | 0.65 | 0.38 |\n|  |  | Max | 0.47 | 0.69 | 0.43 |\n| **MatchVision** | \u2717 | Mean | 0.44 | 0.53 | 0.58 |\n|  |  | Max | 0.35 | 0.70 | 0.46 |", "caption": "Table 1: \nStatistics of Soccer Datasets.\nOur SoccerReplay-1988 significantly surpasses existing datasets in both scale and diversity.\nHere, # Anno. and # Com. refer to the number of event annotations and textual commentaries, respectively.", "description": "Table 1 provides a comparison of several soccer video datasets, highlighting the size and diversity of the SoccerReplay-1988 dataset.  It shows the number of games, the duration of each game, the number of event annotations (# Anno.), and the number of textual commentaries (# Com.) for each dataset.  The table demonstrates that SoccerReplay-1988 is significantly larger and more comprehensive than previous datasets, offering a richer resource for soccer video understanding research.", "section": "3. SoccerReplay-1988 Dataset"}, {"content": "| Pretrain |  |  | Classification(%) |  |  |\n|---|---|---|---|---|---|---|\n| Sup. | Contra. | SR | Acc.@1 | Acc.@3 | Acc.@5 |\n| \u2713 | \u2717 | \u2717 | 62.67 | 83.00 | 89.81 |\n| \u2713 | \u2717 | \u2713 | **68.03** | **86.90** | **92.38** |\n| \u2717 | \u2713 | \u2717 | 46.97 | 75.53 | 85.85 |\n| \u2717 | \u2713 | \u2713 | 57.41 | 83.13 | 91.00 |\n| \u2713 | \u2713 | \u2717 | 56.86 | 80.30 | 88.09 |\n| \u2713 | \u2713 | \u2713 | **63.59** | **85.21** | **91.63** |", "caption": "Table 2: \nQuantitative Results on Event Classification and Commentary Generation.\nHere, SN, MT, and SR represent curated SoccerNet-v2\u00a0[9], MatchTime\u00a0[40], and SoccerReplay-1988, respectively.\nMoreover, B, M, R-L, and C refer to BLEU, METEOR, ROUGE-L, and CIDEr metrics, respectively.\nWithin each unit, we denote the best performance in RED and the second-best performance in BLUE.", "description": "This table presents a quantitative comparison of different visual encoders' performance on event classification and commentary generation tasks.  The results are evaluated across three datasets: SoccerNet-v2 (SN), MatchTime (MT), and SoccerReplay-1988 (SR).  For event classification, the accuracy at top-1, top-3, and top-5 predictions is reported.  Commentary generation performance is assessed using four standard metrics: BLEU (B), METEOR (M), ROUGE-L (R-L), and CIDEr (C). The best performing model for each metric in each dataset is highlighted in red, with the second-best in blue, facilitating easy comparison of the different models and their performance across the datasets.  The table allows researchers to compare the effectiveness of different visual encoders across various datasets and metrics, offering valuable insights for understanding model strengths and weaknesses in soccer video analysis.", "section": "5. Experiments"}, {"content": "| League | # Match |\n|---|---| \n| Italy Serie-a | 367 |\n| England Premier League | 552 |\n| UEFA Champions League | 469 |\n| France Ligue 1 | 123 |\n| Spain LaLiga | 235 |\n| Germany Bundesliga | 242 |", "caption": "Table 3: \nQuantitative Results on Multi-view Fouls Recognition.\nOur frozen MatchVision encoder can achieve comparable performance with other jointly finetuned visual encoders.", "description": "Table 3 presents a quantitative comparison of the performance of different visual encoders on a multi-view foul recognition task.  The key finding is that MatchVision, even when its parameters are frozen (not further trained), achieves comparable accuracy to other models that are jointly fine-tuned (meaning their parameters are adjusted during the specific task training). This highlights MatchVision's strong performance as a general-purpose visual feature extractor for soccer video analysis.", "section": "5. Experiments"}, {"content": "| Season | # Match |\n|---|---| \n| 2017-2018 | 172 |\n| 2018-2019 | 325 |\n| 2019-2020 | 300 |\n| 2020-2021 | 323 |\n| 2021-2022 | 330 |\n| 2022-2023 | 416 |\n| 2023-2024 | 122 |", "caption": "Table 4: Ablation Studies on Event Classification.\nWe explore the impact of various training settings of our MatchVision encoder on the SoccerReplay-test benchmark.\nHere, Sup., Contra., and SR refer to supervised classification,\nvisual-language contrastive learning, and the SoccerReplay-1988 dataset, respectively.", "description": "This table presents ablation study results for event classification using the MatchVision model on the SoccerReplay-test benchmark. It investigates how different training strategies (supervised classification, visual-language contrastive learning) and the inclusion of the SoccerReplay-1988 dataset affect the model's performance.  The results are presented as classification accuracy at different levels (Acc.@1, Acc.@3, Acc.@5).", "section": "5. Experiments"}, {"content": "| Original Label | Processed Label | Reference |\n|---|---|---|\n| Penalty | Penalty | Scored penalties are categorized as \u201cPenalty.\u201d |\n| Penalty Missed | Penalty Missed | Missed penalties are categorized as \u201cPenalty Missed.\u201d |\n| Kick-off | Start of Game (Half) | Matches the start of a half after goals. |\n| Shots off target | Shot Off Target | No change. |\n| Throw-in | Throw In | No change. |\n| Ball out of play | Ball Out of Play | No change. |\n| Foul | Foul (No Card) | Refers to fouls without cards for only. |\n| Yellow card | Yellow Card | No change. |\n| Yellow \u2192 red card | Second Yellow Card | No change. |\n| Red card | Red Card | No change. |\n| Direct free-kick | Free Kick | Both direct and indirect free kicks are grouped. |\n| Indirect free-kick |  |  |\n| Substitution | Substitution | No change. |\n| Goal | Goal | No change. |\n| Clearance | Clearance | No change. |\n| Offside | Off-Side | No change. |\n| Corner | Corner | No change. |", "caption": "Table 5: \nAblation Studies of Commentary Generation.\nWe investigate the impact of different training strategies and datasets on MatchVision using the SoccerReplay-test benchmark.\n\u2018V\u2019 and \u2018L\u2019 denote the visual encoder and the LLM decoder, respectively.", "description": "This table presents ablation study results focusing on commentary generation.  It shows the impact of different training approaches (using either only the visual encoder, only the language model, or both) and the inclusion of the SoccerReplay-1988 dataset on the performance of the MatchVision model. The metrics used are BLEU, METEOR, ROUGE-L, and CIDEr, which are standard evaluation metrics for evaluating the quality of generated text.  The 'V' and 'L' columns indicate whether the visual encoder and language model were trained, respectively.", "section": "5. Experiments"}, {"content": "| Dataset | Train | Valid | Test | Total |\n|---|---|---|---|---|\n| SoccerNet-v2 [9] | 300 | 100 | 100 | 500 |\n| MatchTime [40] | 373 | 49 | 49 | 471 |\n| **SoccerReplay-1988** | **1488** | **250** | **250** | **1988** |", "caption": "Table 6: League-wise Match Statistics.", "description": "This table shows the number of soccer matches included in the SoccerReplay-1988 dataset for each of the six major European soccer leagues: England Premier League, Spain LaLiga, Germany Bundesliga, Italy Serie A, France Ligue 1, and UEFA Champions League.", "section": "3. SoccerReplay-1988 Dataset"}]