[{"figure_path": "2410.18451/tables/table_5_0.html", "caption": "Table 1 | Statistics of the Skywork Reward Preference 80K dataset for reward modeling.", "description": "Table 1 presents the statistics of the Skywork Reward Preference 80K dataset, including the number of pairs, average number of turns, average number of tokens in prompts and responses, completion methods, and annotators.", "section": "3.1. Dataset Mixture"}, {"figure_path": "2410.18451/tables/table_6_0.html", "caption": "Table 1 | Statistics of the Skywork Reward Preference 80K dataset for reward modeling.", "description": "This table presents statistics for the Skywork Reward Preference 80K dataset used for reward modeling, including the number of pairs, average number of tokens in prompts and responses, and task completion annotator information.", "section": "3.1. Dataset Mixture"}, {"figure_path": "2410.18451/tables/table_11_0.html", "caption": "Table 2 | Performance comparison of different reward models on RewardBench. The first block of the table includes the top reward models on the RewardBench leaderboard. The superscript in this block indicates that the results have not been officially verified. The second block of the table corresponds to Llama-3.1-8B and Gemma-2-27B (both instruct version) trained on Preference 700K and Preference 378K data, respectively. The final block of the table showcases the performance of our Skywork-Reward model series, which are trained on the Skywork Reward Preference 80K dataset. Notably, Skywork-Reward-Gemma-2-27B achieves state-of-the-art performance, outperforming several competitive models on RewardBench. The highest performance in each column is masked as bold.", "description": "Table 2 presents a performance comparison of various reward models on the RewardBench benchmark, highlighting the superior performance of the Skywork-Reward models trained on a curated dataset.", "section": "4.2. Experimental Results"}, {"figure_path": "2410.18451/tables/table_12_0.html", "caption": "Table 3 | Ablation studies of loss functions that optimize the margin between chosen and rejected responses on Gemma-2-27B.", "description": "Table 3 presents a comparison of different loss functions used in reward model training, showing the Bradley-Terry loss as the best performing one.", "section": "4.3. Potential Prompt Contamination"}, {"figure_path": "2410.18451/tables/table_13_0.html", "caption": "Table 2 | Performance comparison of different reward models on RewardBench. The first block of the table includes the top reward models on the RewardBench leaderboard. The superscript in this block indicates that the results have not been officially verified. The second block of the table corresponds to Llama-3.1-8B and Gemma-2-27B (both instruct version) trained on Preference 700K and Preference 378K data, respectively. The final block of the table showcases the performance of our Skywork-Reward model series, which are trained on the Skywork Reward Preference 80K dataset. Notably, Skywork-Reward-Gemma-2-27B achieves state-of-the-art performance, outperforming several competitive models on RewardBench. The highest performance in each column is masked as bold.", "description": "Table 2 presents a performance comparison of various reward models on the RewardBench benchmark, highlighting the superior performance of the Skywork-Reward models.", "section": "4.2. Experimental Results"}, {"figure_path": "2410.18451/tables/table_13_1.html", "caption": "Table 2 | Performance comparison of different reward models on RewardBench. The first block of the table includes the top reward models on the RewardBench leaderboard. The superscript in this block indicates that the results have not been officially verified. The second block of the table corresponds to Llama-3.1-8B and Gemma-2-27B (both instruct version) trained on Preference 700K and Preference 378K data, respectively. The final block of the table showcases the performance of our Skywork-Reward model series, which are trained on the Skywork Reward Preference 80K dataset. Notably, Skywork-Reward-Gemma-2-27B achieves state-of-the-art performance, outperforming several competitive models on RewardBench. The highest performance in each column is masked as bold.", "description": "Table 2 presents a performance comparison of various reward models on RewardBench, highlighting the superior performance of the Skywork-Reward models trained on a smaller, curated dataset.", "section": "4.2. Experimental Results"}]