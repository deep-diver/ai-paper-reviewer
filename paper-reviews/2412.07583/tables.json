[{"content": "|---|---| \n| ![Refer to caption](https://arxiv.org/html/2412.07583/girl-4898696_1280.png) | ![Refer to caption](https://arxiv.org/html/2412.07583/rocket.png) | \n| ![Refer to caption](https://arxiv.org/html/2412.07583/eagle.png) | ![Refer to caption](https://arxiv.org/html/2412.07583/dog-7396912_1280.png) | \n| ![Refer to caption](https://arxiv.org/html/2412.07583/powerstation.png) | ![Refer to caption](https://arxiv.org/html/2412.07583/woman-4549327_1280.png) | ", "caption": "Table 1: \nComparison with recent models. FLOPs and latency are provided for a single function evaluation with batch size of 1. For rows marked with asterisk\u2217 FVD measurements were taken from\u00a0Zhang et\u00a0al. [71], while performance metrics are based on our measurements for UNet used by SVD.\nFor consistency with these results, FVD for SVD and our MobileVD model was measured on UCF-101 dataset at 7 frames per second.", "description": "This table compares the performance of the MobileVD model with several state-of-the-art video generation models.  It shows the floating point operations (FLOPs) and latency (time taken for processing) required for generating a single video frame.  Importantly, it notes that the Fr\u00e9chet Video Distance (FVD) values for some models are taken from an external source (Zhang et al., 2024), while others are measured using the same UCF-101 dataset and conditions for consistent comparison. The FVD, a metric of video quality, and FLOPs, and latency are reported for both high- and low-resolution settings.", "section": "4. Experiments"}, {"content": "| Model | NFE | FVD \u2193 | TFLOPs \u2193 | GPU Latency (ms) \u2193 | Phone Latency (ms) \u2193 |\n|---|---|---|---|---|---| \n| *Resolution 1024x576* |  |  |  |  |  |\n| SVD | 50 | 149 | 45.43 | 376 | OOM |\n| AnimateLCM* | 8 | 281 | 45.43 | 376 | OOM |\n| UFOGen* | 1 | 1917 | 45.43 | 376 | OOM |\n| LADD* | 1 | 1894 | 45.43 | 376 | OOM |\n| SF-V* | 1 | 181 | 45.43 | 376 | OOM |\n| *Resolution 512x256* |  |  |  |  |  |\n| SVD | 50 | 476 | 8.60 | 82 | OOM |\n| MobileVD (ours) | 1 | 171 | 4.34 | 45 | 1780 |", "caption": "Table 2: \nEffect of our optimizations.\nWe successfully deployed the image-to-video model to a mobile device without significantly sacrificing the visual quality.\nFLOPs and latency are provided for a single function evaluation with batch size of 1.\nWe call the model in the bottom row Mobile Video Diffusion, or MobileVD.", "description": "This table shows the impact of various optimizations on the performance and efficiency of a Stable Video Diffusion model adapted for mobile devices.  The optimizations include lowering resolution, using optimized cross-attention, adversarial finetuning, temporal multiscaling, temporal block pruning, and channel funneling. The table shows the number of function evaluations (NFE), Fr\u00e9chet Video Distance (FVD) values at both 25 frames per second (FPS) and 7 FPS, TeraFLOPS (TFLOPS), and latency in milliseconds (ms) on both a GPU and a mobile phone.  The bottom row represents the final, optimized Mobile Video Diffusion (MobileVD) model.", "section": "3. Mobile Video Diffusion"}, {"content": "| Model | NFE | FVD \u2193 (25 FPS) | FVD \u2193 (7 FPS) | TFLOPs \u2193 | Latency (ms) \u2193 (GPU) | Latency (ms) \u2193 (Phone) | \n|---|---|---|---|---|---|---|\n| SVD (resolution 1024x576) | 50 | 140 | 149 | 45.43 | 376 | OOM |\n| SVD (resolution 512x256) | 50 | 366 | 476 | 8.60 | 82 | OOM |\n| + low-resolution finetuning | 50 | 194 | 196 | 8.60 | 82 | OOM |\n| + optimized cross-attention | 50 | 194 | 196 | 8.24 | 76 | 3630 |\n| + adversarial finetuning | 1 | 133 | 168 | 8.24 | 76 | 3630 |\n| + temporal multiscaling | 1 | 139 | 156 | 5.42 | 59 | 2590 |\n| + temporal block pruning | 1 | 127 | 150 | 4.64 | 47 | 2100 |\n| + channel funneling | 1 | 149 | 171 | 4.34 | 45 | 1780 |", "caption": "Table 3: \nEffect of additional multiscaling layers in UNet.\nWe observe that both temporal and spatial multiscaling has good impact on mobile latency without compromising much on FVD, while combining the two increases FVD by a noticeable amount.", "description": "This table presents ablation study results on the impact of using multi-scale representations within the UNet architecture on mobile video diffusion model performance.  It shows the effects of adding either spatial or temporal downsampling layers in the UNet, or both.  The results are measured by FID (Fr\u00e9chet Inception Distance) and TFLOPs, reflecting the trade-off between image quality and computational efficiency.  The key takeaway is that while either spatial or temporal multi-scaling improves performance on mobile devices, combining both methods leads to a noticeable drop in image quality.", "section": "3.2 Mobile-friendly UNet"}, {"content": "| Spatial multiscaling | Temporal multiscaling | FVD \u2193 | TFLOPs \u2193 | Latency (ms) \u2193 | Latency (ms) \u2193 |\n|---|---|---|---|---|---| \n| \u00d7 | \u00d7 | 133 | 8.24 | 76 | 3630 |\n| \u00d7 | \u2713 | 138 | 5.42 | 59 | 2590 |\n| \u2713 | \u00d7 | 145 | 4.35 | 51 | 2280 |\n| \u2713 | \u2713 | 163 | 3.39 | 48 | \u2014 |", "caption": "Table 4: \nEffect of funnel initialization and fun-factor.\nInitialization funnels with CSI is crucial to getting good FVD as He initialization\u00a0[20] obtains roughly 200 FVD units more. Additionally, we see that reducing the fun-factor beyond 0.5 starts to affect the performance.", "description": "This table investigates the impact of two hyperparameters\u2014channel funnel initialization methods (Coupled Singular Initialization (CSI) vs. He initialization) and the funnel factor\u2014on the Fr\u00e9chet Video Distance (FVD) metric.  The results demonstrate that using CSI is significantly better than He initialization for achieving lower FVD scores.  Furthermore, decreasing the funnel factor below 0.5 leads to a decline in model performance.", "section": "3.3 Channel funnels"}, {"content": "| Initialization Method | Fun-factor | FVD \u2193 |\n|---|---|---|\n| Coupled singular init. (CSI) | 0.25 | 155 |\n| Coupled singular init. (CSI) | 0.50 | 132 |\n| Coupled singular init. (CSI) | 0.75 | 145 |\n| Coupled singular init. (CSI) | 1.00 | 133 |\n| He init. [20] | 0.50 | 332 |", "caption": "Table 5: \nComparison of model width reduction methods.\nWe compare the proposed channel funneling (in grey) with finetuned low-rank approximation of individual attention layers with truncated singular decomposition. We additionally compare to Funnels applied to convolutions instead of attention. The reduction rate (referred to as fun-factor in case of funnels) is highlighted with r\ud835\udc5fritalic_r.", "description": "This table compares different methods for reducing the width of neural network layers, aiming to improve model efficiency. It contrasts channel funneling (the proposed method), finetuned low-rank approximation of attention layers using truncated singular value decomposition, and channel funneling applied to convolutional layers instead of attention layers.  The 'fun-factor', representing the reduction rate, is indicated for channel funneling.", "section": "4.3 Ablations"}, {"content": "| Width reduction method | r | FVD \u2193 | TFLOPs \u2193 | Latency (ms) \u2193 |\n|---|---|---|---|---|\n| Original UNet | - | 133 | 8.6 | 3630 |\n| + Funnels | 0.5 | 132 | 8.0 | 2870 |\n| + Funnels (merge before finetune) | 0.5 | 138 | 8.0 | 2870 |\n| + Funnels (convolutions) | 0.5 | 139 | 7.2 | 3400 |\n| + Truncated singular decomposition | 0.5 | 142 | 8.6 | 3482 |\n| + Truncated singular decomposition | 0.25 | 130 | 8.0 | 3345 |", "caption": "Table 6: \nImpact of temporal blocks pruning.\nOur pruning outperforms the L1\u2212limit-fromsubscript\ud835\udc3f1L_{1}-italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - regularization which does not have explicit control over the number of removed blocks. We use the checkpoint, optimized up to the temporal block pruning stage, as the starting point.", "description": "This table compares the performance of different temporal block pruning methods.  Specifically, it shows how a learnable pruning technique outperforms L1 regularization, which lacks explicit control over the number of pruned blocks. The experiment starts with a model optimized up to the temporal block pruning stage.  The results demonstrate the effectiveness of the proposed method in reducing computational cost while maintaining model performance.", "section": "3.4. Temporal block pruning"}, {"content": "| Blocks pruned (%) | FVD \u2193 | TFLOPs \u2193 | Latency GPU (ms) \u2193 |\n|---|---|---|---| \n| <em style=\"font-style:italic;\">Our method</em> |  |  |  |\n| 90 | 201 | 4.06 | 42 |\n| 80 | 245 | 4.35 | 44 |\n| 70 | 127 | 4.64 | 47 |\n| <em style=\"font-style:italic;\">L<sub>1</sub> regularization</em> |  |  |  |\n| 70 | 207 | 4.67 | 48 |\n| 53 | 165 | 5.17 | 52 |", "caption": "Table 7: \nComparison with recent models. The set of optimizations proposed in our paper, can also be applied to high-resolution generation. FLOPs and latency are provided for a single function evaluation with batch size of 1. For rows marked with asterisk\u2217 FVD measurements were taken from\u00a0Zhang et\u00a0al. [71], while performance metrics are based on our measurements for UNet used by SVD.\nFor consistency with these results, FVD for SVD and our models was measured on UCF-101 dataset at 7 frames per second.", "description": "This table compares the performance of the proposed MobileVD model with several state-of-the-art video generation models.  It includes the number of function evaluations (NFE), Fr\u00e9chet Video Distance (FVD), TeraFLOPs (TFLOPS), and latency (in milliseconds) on both GPU and mobile phone.  The comparison is done for both high-resolution (1024x576) and low-resolution (512x256) settings.  Note that FVD values marked with an asterisk (*) were obtained from an external source (Zhang et al., 2024), while the other performance metrics are based on the authors' measurements of the UNet used in Stable Video Diffusion (SVD). To ensure consistency, the FVD for SVD and MobileVD was measured on the UCF-101 dataset at 7 frames per second.", "section": "4. Experiments"}, {"content": "| Model | NFE | FVD \u2193 | TFLOPs \u2193 | Latency (ms) \u2193 GPU | Latency (ms) \u2193 Phone |\n|---|---|---|---|---|---| \n| *Resolution 1024x576* |  |  |  |  |  |\n| SVD | 50 | 149 | 45.43 | 376 | OOM |\n| AnimateLCM* | 8 | 281 | 45.43 | 376 | OOM |\n| UFOGen* | 1 | 1917 | 45.43 | 376 | OOM |\n| LADD* | 1 | 1894 | 45.43 | 376 | OOM |\n| SF-V* | 1 | 181 | 45.43 | 376 | OOM |\n| MobileVD-HD (ours) | 1 | 184 | 23.63 | 227 | OOM |\n| *Resolution 512x256* |  |  |  |  |  |\n| SVD | 50 | 476 | 8.60 | 82 | OOM |\n| MobileVD (ours) | 1 | 171 | 4.34 | 45 | 1780 |", "caption": "Table 8: \nEffect of our optimizations.\nWe successfully deployed the image-to-video model to a mobile device without significantly sacrificing the visual quality.\nFLOPs and latency are provided for a single function evaluation with batch size of 1.\nWe call the model in the bottom row Mobile Video Diffusion, or MobileVD.\nThe model trained with the same hyperparameters but intended for high-resolution generations is referred to as MobileVD-HD.", "description": "This table presents a quantitative analysis of the impact of various optimization techniques on the performance of a mobile video diffusion model.  It compares the original Stable Video Diffusion (SVD) model with several optimized versions, showcasing the trade-offs between computational cost (measured in FLOPs and latency on both GPU and mobile phone), and visual quality (evaluated using the Fr\u00e9chet Video Distance (FVD) and JEDi metrics). The optimizations include: low-resolution finetuning,  optimized cross-attention, adversarial finetuning, temporal multi-scaling, temporal block pruning, and channel funneling. The table highlights the significant reduction in computational cost achieved by the MobileVD model (the final optimized version) while maintaining relatively good visual quality, demonstrating its suitability for mobile deployment. A high-resolution variant, MobileVD-HD, is also included, showcasing the scalability of the optimizations.", "section": "4. Experiments"}, {"content": "| Model | NFE | FVD \u2193 (25 FPS) | FVD \u2193 (7 FPS) |  | JEDi \u2193 (25 FPS) | JEDi \u2193 (7 FPS) | TFLOPs \u2193 | Latency (ms) \u2193 (GPU) | Latency (ms) \u2193 (Phone) |\n|---|---|---|---|---|---|---|---|---|---| \n| <span style=\"font-size:144%;\">Resolution</span> <math alttext=\"\\mathit{1024\\times 576}\" class=\"ltx_Math\" display=\"inline\" id=\"A0.T8.5.5.1.1.m1.1\"><semantics id=\"A0.T8.5.5.1.1.m1.1a\"><mrow id=\"A0.T8.5.5.1.1.m1.1.1\" xref=\"A0.T8.5.5.1.1.m1.1.1.cmml\"><mn class=\"ltx_mathvariant_italic\" id=\"A0.T8.5.5.1.1.m1.1.1.2\" mathbackground=\"#E6E6E6\" mathvariant=\"italic\" xref=\"A0.T8.5.5.1.1.m1.1.1.2.cmml\">1024</mn><mo id=\"A0.T8.5.5.1.1.m1.1.1.1\" lspace=\"0.222em\" mathbackground=\"#E6E6E6\" rspace=\"0.222em\" xref=\"A0.T8.5.5.1.1.m1.1.1.1.cmml\">\u00d7</mo><mn class=\"ltx_mathvariant_italic\" id=\"A0.T8.5.5.1.1.m1.1.1.3\" mathbackground=\"#E6E6E6\" mathvariant=\"italic\" xref=\"A0.T8.5.5.1.1.m1.1.1.3.cmml\">576</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A0.T8.5.5.1.1.m1.1b\"><apply id=\"A0.T8.5.5.1.1.m1.1.1.cmml\" xref=\"A0.T8.5.5.1.1.m1.1.1\"><times id=\"A0.T8.5.5.1.1.m1.1.1.1.cmml\" xref=\"A0.T8.5.5.1.1.m1.1.1.1\"></times><cn id=\"A0.T8.5.5.1.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"A0.T8.5.5.1.1.m1.1.1.2\">1024</cn><cn id=\"A0.T8.5.5.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"A0.T8.5.5.1.1.m1.1.1.3\">576</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A0.T8.5.5.1.1.m1.1c\">\\mathit{1024\\times 576}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A0.T8.5.5.1.1.m1.1d\">italic_1024 \u00d7 italic_576</annotation></semantics></math> |  |  |  |  |  |  |  | \n| SVD | 50 | 140 | 149 |  | 0.61 | 0.59 | 45.43 | 376 | OOM |\n| MobileVD-HD | 1 | 126 | 184 |  | 0.96 | 1.75 | 23.63 | 227 | OOM |\n| <span style=\"font-size:144%;\">Resolution</span> <math alttext=\"\\mathit{512\\times 256}\" class=\"ltx_Math\" display=\"inline\" id=\"A0.T8.7.7.1.1.m1.1\"><semantics id=\"A0.T8.7.7.1.1.m1.1a\"><mrow id=\"A0.T8.7.7.1.1.m1.1.1\" xref=\"A0.T8.7.7.1.1.m1.1.1.cmml\"><mn class=\"ltx_mathvariant_italic\" id=\"A0.T8.7.7.1.1.m1.1.1.2\" mathbackground=\"#E6E6E6\" mathvariant=\"italic\" xref=\"A0.T8.7.7.1.1.m1.1.1.2.cmml\">512</mn><mo id=\"A0.T8.7.7.1.1.m1.1.1.1\" lspace=\"0.222em\" mathbackground=\"#E6E6E6\" rspace=\"0.222em\" xref=\"A0.T8.7.7.1.1.m1.1.1.1.cmml\">\u00d7</mo><mn class=\"ltx_mathvariant_italic\" id=\"A0.T8.7.7.1.1.m1.1.1.3\" mathbackground=\"#E6E6E6\" mathvariant=\"italic\" xref=\"A0.T8.7.7.1.1.m1.1.1.3.cmml\">256</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A0.T8.7.7.1.1.m1.1b\"><apply id=\"A0.T8.7.7.1.1.m1.1.1.cmml\" xref=\"A0.T8.7.7.1.1.m1.1.1\"><times id=\"A0.T8.7.7.1.1.m1.1.1.1.cmml\" xref=\"A0.T8.7.7.1.1.m1.1.1.1\"></times><cn id=\"A0.T8.7.7.1.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"A0.T8.7.7.1.1.m1.1.1.2\">512</cn><cn id=\"A0.T8.7.7.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"A0.T8.7.7.1.1.m1.1.1.3\">256</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A0.T8.7.7.1.1.m1.1c\">\\mathit{512\\times 256}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A0.T8.7.7.1.1.m1.1d\">italic_512 \u00d7 italic_256</annotation></semantics></math> |  |  |  |  |  |  |  | \n| SVD | 50 | 366 | 476 |  | 1.05 | 1.14 | 8.60 | 82 | OOM |\n| + low-resolution finetuning | 50 | 194 | 196 |  | 0.71 | 0.65 | 8.60 | 82 | OOM |\n| + optimized cross-attention | 50 | 194 | 196 |  | 0.71 | 0.65 | 8.24 | 76 | 3630 |\n| + adversarial finetuning | 1 | 133 | 168 |  | 0.66 | 0.71 | 8.24 | 76 | 3630 |\n| + temporal multiscaling | 1 | 139 | 156 |  | 0.83 | 0.81 | 5.42 | 59 | 2590 |\n| + temporal block pruning | 1 | 127 | 150 |  | 0.97 | 1.32 | 4.64 | 47 | 2100 |\n| + channel funneling | 1 | 149 | 171 |  | 1.07 | 1.21 | 4.34 | 45 | 1780 |", "caption": "Table 9: \nImpact of latent decoder.\nWhile being significantly faster on device, decoder from TAESD has little to no impact on visual quality as measured by FVD and JEDi.", "description": "This table compares the performance of two different decoders used to convert latent codes generated by the MobileVD model into actual RGB video frames.  It shows that the TAESD decoder is significantly faster for on-device processing compared to the original decoder from SVD, while maintaining virtually identical visual quality as measured by the FVD and JEDi metrics.  This indicates that the TAESD decoder is a suitable and more efficient alternative for mobile applications.", "section": "4.3 Ablations"}]