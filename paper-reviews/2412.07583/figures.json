[{"figure_path": "https://arxiv.org/html/2412.07583/x1.png", "caption": "Figure 1: \nQuality-efficiency trade-off.\nOur MobileVD accelerate SVD by 523\u00d7523\\times523 \u00d7 (in FLOPs) with a slight decrease in the generation qualities (in FVD) reaching to a better quality vs. efficiency trade-off than alternatives.", "description": "The figure illustrates the trade-off between computational efficiency and generated video quality for different video diffusion models.  MobileVD, a novel mobile-optimized model, achieves a 523x speedup over Stable Video Diffusion (SVD) while incurring only a slight reduction in quality as measured by the Fr\u00e9chet Video Distance (FVD). This improvement positions MobileVD favorably against competing models in terms of its quality-efficiency balance.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.07583/x2.png", "caption": "Figure 2: \nEffect of optimized cross-attention for a mobile device.\nWe show the number of cycles of the top-4 operations on mobile hardware for an input resolution of 128\u00d7128128128128\\times 128128 \u00d7 128. Note that removing the no-op similarity map computation in cross-attention layers reduces cycles on softmax operations by roughly 80808080%.", "description": "This figure demonstrates the impact of optimizing cross-attention mechanisms on mobile device performance.  It displays the number of CPU cycles required for the four most computationally expensive operations (MatMul, Softmax, Gelu, and Conv2D) within the UNet architecture, using an input resolution of 128x128 pixels.  A key finding highlighted is that removing unnecessary computations related to the similarity map in cross-attention layers leads to an approximately 80% reduction in the number of cycles needed for the Softmax operation, significantly improving efficiency on mobile hardware.", "section": "3.2 Mobile-friendly UNet"}, {"figure_path": "https://arxiv.org/html/2412.07583/x3.png", "caption": "Figure 3: \nChannel funnels.\nWe show an example of channel funnels applied to a couple of layers within the model.\nAt training time, funnels serve as adaptors reducing model width.\nAt inference, they are merged with corresponding weight matrices without loss of quality.", "description": "This figure illustrates the concept of \"channel funnels,\" a technique used to optimize the model's efficiency.  Channel funnels act as adaptors during training, reducing the model's width (number of channels) and thus the number of parameters. This speeds up training without a significant impact on accuracy.  Crucially, at inference time, the funnels are merged with the weight matrices, so there is no loss in accuracy or quality of the final output. The diagram visually shows the process, highlighting how the model's width is reduced during training and restored during inference.", "section": "3. Mobile Video Diffusion"}, {"figure_path": "https://arxiv.org/html/2412.07583/x4.png", "caption": "(a) \nTemporal blocks in the original architecture of SVD.", "description": "This figure shows the architecture of temporal blocks in Stable Video Diffusion (SVD).  Panel (a) illustrates how the original SVD model incorporates temporal blocks to handle the temporal dynamics of videos. Each temporal block receives inputs from the spatial blocks, processes them through temporal layers, and then integrates the result with the spatial block output via a weighted sum, controlled by the parameter 'a', allowing for variable emphasis on spatial or temporal features.", "section": "3.4. Temporal block pruning"}, {"figure_path": "https://arxiv.org/html/2412.07583/x5.png", "caption": "(b) \nA zero-one gate multiplier is sampled to each temporal block during training.", "description": "During training, a random binary gate (0 or 1) is applied to each temporal block. This gate determines whether the temporal block is active (gate=1) or inactive (gate=0) during that specific training iteration.  This technique is used to perform a learnable pruning of temporal blocks where the network learns which temporal blocks are more important for generating high-quality videos. At inference time, only the most important temporal blocks (those with the highest learned importance scores) are used, resulting in a smaller and more efficient model.", "section": "3.4. Temporal block pruning"}]