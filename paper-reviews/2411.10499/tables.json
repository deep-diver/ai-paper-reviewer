[{"content": "| Methods | DressCode Paired SSIM \u2191 | DressCode Paired LPIPS \u2193 | DressCode Paired FID \u2193 | DressCode Paired KID \u2193 | DressCode Unpaired FID \u2193 | DressCode Unpaired KID \u2193 | VITON-HD Paired SSIM \u2191 | VITON-HD Paired LPIPS \u2193 | VITON-HD Paired FID \u2193 | VITON-HD Paired KID \u2193 | VITON-HD Unpaired FID \u2193 | VITON-HD Unpaired KID \u2193 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| LaDI-VTON (2023) | 0.9025 | 0.0719 | 4.8636 | 1.5580 | 6.8421 | 2.3345 | 0.8763 | 0.0911 | 6.6044 | 1.0672 | 9.4095 | 1.6866 |\n| StableVTON (2024) | - | - | - | - | - | - | 0.8665 | 0.0835 | 6.8581 | 1.2553 | 9.5868 | 1.4508 |\n| IDM-VTON (2024) | 0.9228 | 0.0478 | 3.8001 | 1.2012 | 5.6159 | 1.5536 | 0.8806 | 0.0789 | 6.3381 | 1.3224 | 9.6114 | 1.6387 |\n| OOTDiffusion (2024) | 0.8975 | 0.0725 | 3.9497 | 0.7198 | 6.7019 | 1.8630 | 0.8513 | 0.0964 | 6.5186 | 0.8961 | 9.6733 | 1.2061 |\n| CatVTON (2024) | 0.9011 | 0.0705 | 3.2755 | 0.6696 | 5.4220 | 1.5490 | 0.8694 | 0.0970 | 6.1394 | 0.9639 | 9.1434 | 1.2666 |\n| FitDiT (Ours) | **0.9259** | **0.0431** | **2.6383** | **0.4990** | **4.7324** | **0.9011** | **0.8985** | **0.0661** | **4.7309** | **0.1895** | **8.2042** | **0.3421** |", "caption": "Table 1: Quantitative results on VITON-HD and DressCode datasets. We compare the metrics under both paired (model\u2019s clothing is the same as the given cloth image) and unpaired settings (model\u2019s clothing differs) with other methods.", "description": "This table presents a quantitative comparison of different virtual try-on methods on the VITON-HD and DressCode datasets.  The evaluation metrics used are SSIM (Structural Similarity Index), LPIPS (Learned Perceptual Image Patch Similarity), FID (Fr\u00e9chet Inception Distance), and KID (Kernel Inception Distance).  Results are shown for both \"paired\" settings (where the generated clothing matches the input garment) and \"unpaired\" settings (where the generated clothing differs from the input). This allows for a comprehensive assessment of the models' ability to generate realistic and accurate virtual try-ons under various conditions.", "section": "4. Experiments"}, {"content": "| Methods | Paired SSIM \u2191 | Paired LPIPS \u2193 | Paired FID \u2193 | Paired KID \u2193 | Unpaired FID \u2193 | Unpaired KID \u2193 |\n|---|---|---|---|---|---|---|\n| LaDI-VTON (2023) | 0.8431 | 0.1432 | 26.4509 | 1.024 | 39.4821 | 3.0239 |\n| IDM-VTON (2024) | 0.8529 | 0.1399 | 24.9510 | 0.7931 | 35.8422 | 1.1313 |\n| OOTDiffusion (2024) | 0.8397 | 0.1485 | 26.2757 | 1.1137 | 40.7213 | 4.3277 |\n| CatVTON (2024) | 0.8457 | 0.1494 | 27.7435 | 1.7160 | 38.7899 | 3.4777 |\n| FitDiT (Ours) | **0.8636** | **0.1130** | **20.7543** | **0.1602** | **33.4937** | **0.7434** |", "caption": "Table 2: Quantitative results on CVDD.", "description": "Quantitative results on the Complex Virtual Dressing Dataset (CVDD) evaluating virtual try-on performance.  Metrics include SSIM (structural similarity index), LPIPS (Learned Perceptual Image Patch Similarity), FID (Fr\u00e9chet Inception Distance), and KID (Kernel Inception Distance) for both paired (ground truth garment matches generated image) and unpaired (ground truth garment differs from generated image) settings.", "section": "4. Experiments"}, {"content": "| Method | SSIM \u2191 | LPIPS \u2193 | FID \u2193 | KID \u2193 |\n|---|---|---|---|---|\n| - w/o Frequency loss | 0.8593 | 0.1239 | 22.6325 | 0.2960 |\n| - w/o garment priors evolution | 0.8578 | 0.1269 | 23.1786 | 0.5214 |\n| Full FitDiT | **0.8636** | **0.1130** | **20.7543** | **0.1602** |", "caption": "Table 3: Ablation study results on CVDD.", "description": "This table presents the results of ablation studies conducted on the Complex Virtual Dressing Dataset (CVDD).  It shows the impact of removing key components of the FitDiT model, specifically the frequency loss and the garment prior evolution process, on the model's performance.  The results are evaluated using SSIM (structural similarity index), LPIPS (Learned Perceptual Image Patch Similarity), FID (Fr\u00e9chet Inception Distance), and KID (Kernel Inception Distance).  By comparing the performance of the full model with these ablated versions, researchers can determine the contribution of each component to the model's overall accuracy and effectiveness in generating high-fidelity virtual try-on images.", "section": "4.5 Ablation Study"}, {"content": "| Method | StableVITON | OOTDiffusion | IDM | CatVTON | FitDiT |\n|---|---|---|---|---|---| \n| Inference time (s) | 6.23 | 8.51 | 9.99 | 7.87 | **4.57** |\n| GPU memory (M) | 10,978 | 8,962 | 19,504 | **8,384** | 19,550 |", "caption": "Table 4: Computational analysis of different methods.", "description": "This table presents a computational analysis comparing different virtual try-on methods.  It shows the inference time (in seconds) and GPU memory usage (in MB) for each method: Stable VITON, OOTDiffusion, IDM, CatVTON, and FitDiT. This comparison highlights the efficiency and resource requirements of each approach, offering insights into their practical applicability and scalability.", "section": "4.6. Computational Analysis"}]