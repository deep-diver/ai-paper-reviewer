{"references": [{" publication_date": "2009", "fullname_first_author": "Brenna D Argall", "paper_title": "A survey of robot learning from demonstration", "reason": "This paper provides a comprehensive overview of robot learning from demonstration (RLfD), a foundational technique for training robots using human demonstrations.  Its broad coverage of RLfD methods makes it essential for understanding the landscape of robot learning techniques, including the imitation learning approach used in the current study's experimental setup. The paper also delves into the challenges and future directions of RLfD, contributing context for current research.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "reason": "This paper introduces Vision Transformer (ViT), a novel architecture for image recognition that significantly impacts computer vision and related fields like robotics.  Its influence on robotic visual representation is notable, as several methods in this study utilize ViT or related transformer-based architectures for pre-training and visual feature extraction, demonstrating its importance as a benchmark in the field of robotic vision.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Christoph Feichtenhofer", "paper_title": "Masked autoencoders as spatiotemporal learners", "reason": "This paper introduces Masked Autoencoders (MAE), a self-supervised learning method that significantly impacts the field of computer vision and robotics. MAE is used in several methods mentioned in this paper for pre-training visual representations, highlighting its importance as a benchmark in pre-training of visual representations for robotic manipulation.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Raghav Goyal", "paper_title": "The \"something something\" video database for learning and evaluating visual common sense", "reason": "This paper introduces the Something-Something dataset, a large-scale dataset of human videos used to pre-train visual representations in several prior works on robotic manipulation.  Understanding this dataset is key to comprehending the baseline methods in this study, which commonly utilize data from this dataset for pre-training and evaluating representation learning techniques.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Kristen Grauman", "paper_title": "Ego4d: Around the world in 3,000 hours of egocentric video", "reason": "This paper introduces the Ego4D dataset, a large-scale egocentric video dataset that is also employed in several baseline methods for pre-training visual representations.  The significance of this dataset in robotic representation learning is underscored by the extensive use of its data in related studies.  Understanding this dataset is crucial for comprehending the context and implications of the comparative analysis in the current study.", "section_number": 1}, {" publication_date": "2015", "fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "reason": "This is a seminal paper that introduces the ResNet architecture, widely used in computer vision and robotics. Several methods used in the study leverage ResNet or its variants for visual feature extraction and representation learning.  Its significant impact on image recognition and its influence on the baseline methods make it highly relevant and important.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "reason": "This paper introduces Masked Autoencoders (MAE), a self-supervised learning technique that has significantly influenced the pre-training of visual representations.  The importance of this paper stems from its direct influence on the baseline methods in this study, which often utilize MAE for pre-training, highlighting its significance in the comparative analysis.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "This paper introduces BERT, a significant advancement in natural language processing that influences related fields like robotics.  The use of large language models is increasingly prominent in robotics for understanding task instructions and human demonstrations.  The principles and techniques of BERT are indirectly relevant to current research in robotics.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Michael Laskin", "paper_title": "CURL: Contrastive unsupervised representations for reinforcement learning", "reason": "This paper introduces CURL, a method for contrastive unsupervised representation learning, which is relevant to the study's approach of leveraging contrastive learning for training robotic representations.  Understanding CURL's methodology is essential for comprehending the techniques employed in training the MCR model, contributing to the significance of this citation.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Suraj Nair", "paper_title": "R3m: A universal visual representation for robot manipulation", "reason": "This paper introduces R3M, a significant baseline method that is directly compared against the proposed method (MCR) in this study.  A deep understanding of R3M's methodology and its performance characteristics is crucial for evaluating and interpreting the results of the comparative analysis.  R3M's influence on the field of robotic representation learning makes it a highly important citation.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Alexander Khazatsky", "paper_title": "DROID: A large-scale in-the-wild robot manipulation dataset", "reason": "This paper introduces the DROID dataset, a key component of the proposed MCR method. DROID provides large-scale robot manipulation data, crucial for pre-training the MCR model.  Understanding DROID's characteristics and the methodology for data collection are key for fully comprehending the advantages of using large-scale robot datasets in robotic representation learning, particularly for improving manipulation centricity.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Sudeep Dasari", "paper_title": "An unbiased look at datasets for visuo-motor pre-training", "reason": "This paper presents a comparative analysis of various datasets used in pre-training for visuo-motor control, with a focus on the benefits and drawbacks of different data types.  This analysis is relevant to the current study's discussion of the advantages of robot-specific datasets over human datasets for improving manipulation centricity, particularly in addressing domain shift and data bias.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Open X-Embodiment Collaboration", "paper_title": "Open X-Embodiment: Robotic learning datasets and RT-X models", "reason": "This paper introduces the Open X-Embodiment dataset, one of the largest and most diverse robotic datasets available. Its significance lies in its contribution to the broader discussion of using large-scale robot datasets for pre-training, contrasting it with human-based datasets.  Understanding the scale and diversity of this dataset helps to contextualize the importance of using appropriately large and diverse datasets in robotic representation learning.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Ajay Mandlekar", "paper_title": "What matters in learning from offline human demonstrations for robot manipulation", "reason": "This paper is crucial because it discusses the challenges of using offline human demonstrations in robot learning and motivates the use of robot-specific datasets for overcoming the embodiment gap between human and robot actions. This is directly relevant to the current study's rationale for using the DROID dataset and emphasizing the improvement in performance by leveraging robot-specific dynamics data.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "Ramprasaath R Selvaraju", "paper_title": "Grad-cam: Visual explanations from deep networks via gradient-based localization", "reason": "This paper introduces Grad-CAM, a visualization technique used to understand the attention regions of deep learning models.  Grad-CAM is a central component in this study's methodology for quantifying manipulation centricity, where Grad-CAM heatmaps are used to assess how well a representation focuses on manipulation-relevant regions. Understanding Grad-CAM is essential for interpreting the results.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Nikhila Ravi", "paper_title": "Sam 2: Segment anything in images and videos", "reason": "This paper introduces SAM 2, a powerful segmentation model used to generate ground truth segmentation masks for quantifying manipulation centricity.  SAM 2 provides a robust and efficient method for segmenting images, which is crucial for accurately measuring the alignment between the representation's attention regions and the manipulation-relevant regions in the image.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Younggyo Seo", "paper_title": "Masked world models for visual control", "reason": "This paper is important as it introduces a method for visual control using masked world models, relevant to the current study's use of simulation environments for evaluation.  The study's methodology uses multiple simulation environments, and understanding this method helps in evaluating the generalizability and reliability of results obtained across those diverse simulation environments.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Tianhe Yu", "paper_title": "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning", "reason": "This paper introduces the MetaWorld benchmark, a key dataset used in this study's experimental setup.  MetaWorld provides a diverse set of robotic manipulation tasks, enabling a thorough evaluation of the proposed representation learning method. The selection of MetaWorld as one of the evaluation environments demonstrates the importance of using a standard and recognized benchmark in evaluating robotic representation learning.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Chen Bao", "paper_title": "Dexart: Benchmarking generalizable dexterous manipulation with articulated objects", "reason": "This paper introduces the DexArt benchmark, which is a key dataset used in this study's experimental setup.  The DexArt dataset is specifically designed for evaluating dexterous manipulation, and its inclusion in this study emphasizes the comprehensive nature of the evaluation across different manipulation tasks.  DexArt helps to confirm the generalizability of the results by testing on tasks requiring finer motor skills.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tony Z Zhao", "paper_title": "Learning fine-grained bimanual manipulation with low-cost hardware", "reason": "This paper highlights the increasing importance of bimanual manipulation in robotics and provides insights into learning more effective representations for these types of actions. The use of this technique is implicitly relevant to this study, as the use of bimanual manipulation would likely benefit from improved representation learning techniques, such as those explored here.  It also demonstrates the practical applicability of the research findings.", "section_number": 5}]}