{"references": [{" publication_date": "2024", "fullname_first_author": "Open X-Embodiment Collaboration", "paper_title": "Open X-Embodiment: Robotic learning datasets and RT-X models", "reason": "This paper introduces the Open X-Embodiment dataset, a large-scale dataset that provides extensive data from diverse robot embodiments and tasks.  The scale and diversity of this dataset are highly relevant to the current work, as it directly addresses the limitations of small, homogenous datasets and the need for large-scale data to train effective robotic representations.  The dataset\u2019s impact is crucial for the proposed method's improved performance and generalization capabilities in robotic manipulation.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Dasari", "paper_title": "An unbiased look at datasets for visuo-motor pre-training", "reason": "This paper provides a critical evaluation of existing datasets for visuo-motor pre-training, highlighting the limitations and biases in commonly used datasets. The critical analysis of existing datasets is highly relevant to the current paper, as it directly addresses the limitations of using human-centric datasets for robotic learning, motivating the use of the more suitable DROID dataset.  The identification of biases and limitations in previous data is crucial for justifying the choice of DROID and supporting the findings related to the efficacy of robot-specific data.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Khazatsky", "paper_title": "DROID: A large-scale in-the-wild robot manipulation dataset", "reason": "This paper introduces the DROID dataset, a large-scale dataset specifically designed for robotic manipulation learning. The DROID dataset is the core data source for the current work, providing the necessary large-scale, robot-centric data for training manipulation-centric representations.  The rich data including robot proprioceptive states and actions directly contribute to the improved performance and manipulation centricity observed in the proposed method.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Nair", "paper_title": "R3M: A universal visual representation for robot manipulation", "reason": "This paper introduces R3M, a baseline method for learning robotic representations using pre-trained visual encoders. R3M serves as a crucial comparison point for the current work, providing a benchmark against which the proposed method's performance and manipulation centricity are evaluated.  Understanding and comparing against this existing method enables a strong quantitative assessment of the improvement offered by the proposed method.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Xiao", "paper_title": "MVP: pre-trains a Vision Transformer (ViT)", "reason": "This paper introduces MVP, a baseline method for learning robotic representations that also utilizes Masked Autoencoders. MVP's reliance on masked autoencoders and comparison with the proposed method highlight the differences in approach and the advantages of using robot-centric data over human video data for robotic learning.  The comparison with MVP underscores the superiority of the proposed method in achieving better performance.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Srirama", "paper_title": "HRP: fine-tunes a pre-trained ViT to predict human affordance labels", "reason": "This paper presents HRP, another baseline method, that uses human videos for learning robotic representations.  Comparing HRP with the proposed method highlights the benefits of using robot datasets over human datasets and demonstrates the advantages of using robot-specific data and dynamics information for training effective representations.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Majumdar", "paper_title": "VC-1: is trained similarly to MVP but also additionally incorporates navigation and the ImageNet", "reason": "This paper presents VC-1, a baseline method incorporating both human video data and ImageNet data for training.  Comparing VC-1 with the proposed method demonstrates the efficacy of robot-centric data and specifically addresses the impact of using human videos versus robot datasets for robotic representation learning. The comparative analysis between using both human data and ImageNet data versus only robot-specific data strengthens the rationale of the current work.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "reason": "This paper introduces the Vision Transformer (ViT) architecture, which is a foundational component in many of the baseline methods used in the paper, including MVP and VC-1.  Understanding the ViT architecture is critical for evaluating and comparing the proposed approach and its contribution to robotic representation learning.  The paper is significant as it laid the groundwork for many architectures compared against in the paper.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "He", "paper_title": "Masked autoencoders are scalable vision learners", "reason": "This paper introduces Masked Autoencoders (MAE), a crucial technique utilized in several baseline methods for robotic representation learning. The significance of this paper lies in understanding the core methodology of several baseline methods that are later compared with the proposed approach.  MAE directly impacts the training process and representation learning capabilities of multiple methods being studied.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Selvaraju", "paper_title": "Grad-cam: Visual explanations from deep networks via gradient-based localization", "reason": "This paper introduces Grad-CAM, a visualization technique used to analyze and quantify manipulation centricity. Grad-CAM is a key component in the evaluation methodology, providing a quantitative measure of the representation's ability to focus on manipulation-relevant regions.  The methodology used to measure manipulation centricity is a crucial part of the paper\u2019s results and conclusions.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Ravi", "paper_title": "SAM 2: Segment anything in images and videos", "reason": "This paper introduces SAM 2, a powerful video segmentation model used to generate ground truth segmentation masks for calculating manipulation centricity. SAM 2 is a critical component in the evaluation methodology, providing accurate ground truth data for comparing and quantifying the manipulation centricity of different representations.  The accuracy and reliability of the ground truth masks are crucial for the validity of results.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Zheng", "paper_title": "TACO: optimizes mutual information between current state-action pairs and future states", "reason": "This paper explores the optimization of mutual information between state-action pairs and future states, which provides an alternative approach to learning temporal dynamics.  The comparison between TACO and the proposed method highlights the advantages of using the specific training objectives in MCR for improving manipulation centricity and downstream performance.  The alternative is an important comparative method.", "section_number": 6}, {" publication_date": "2019", "fullname_first_author": "Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "This paper introduces BERT, a pre-trained language model that influences many Natural Language Processing (NLP) approaches, which are indirectly related to the current work through their influence on other baseline approaches that use language models to enhance robotic representation learning.  The influence of BERT is indirect but nevertheless contributes to the background of many comparison methods used in the paper.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Yu", "paper_title": "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning", "reason": "This paper introduces MetaWorld, one of the simulation environments used in the current work for evaluating robotic representations. MetaWorld is a crucial component of the experimental setup, providing diverse and challenging tasks for testing the generalization capabilities of the different representations being compared.  It ensures a robust evaluation of the different representations.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Mandlekar", "paper_title": "What matters in learning from offline human demonstrations for robot manipulation", "reason": "This paper introduces Robomimic, one of the simulation environments used in the current work for evaluating robotic representations. Robomimic serves as a crucial testing ground for the proposed method and is vital to evaluating its performance across diverse robotic manipulation tasks and is necessary to fully understand the context of this experiment.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Nasiriany", "paper_title": "Robocasa: Large-scale simulation of everyday tasks for generalist robots", "reason": "This paper introduces RoboCasa, another simulation environment used for evaluating robotic representations in the current work. RoboCasa, similar to MetaWorld and Robomimic, contributes to the comprehensive and diverse evaluation of the proposed method and is crucial to demonstrating generalizability.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Bao", "paper_title": "Dexart: Benchmarking generalizable dexterous manipulation with articulated objects", "reason": "This paper introduces DexArt, a simulation environment specifically designed for dexterous manipulation tasks.  The inclusion of DexArt expands the scope of the evaluation by testing the representation's ability to handle complex manipulation scenarios requiring dexterous skills, ensuring a thorough assessment of its generalization capabilities.  This demonstrates the impact on more complex manipulation tasks.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Argall", "paper_title": "A survey of robot learning from demonstration", "reason": "This survey paper provides a comprehensive overview of robot learning from demonstration (RLfD), which forms the foundation of the imitation learning (IL) approach used in the current work.  Understanding RLfD is essential for contextualizing the current work and its contributions to the field of robotic manipulation.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Burns", "paper_title": "What makes pre-trained visual representations successful for robust manipulation?", "reason": "This paper investigates the factors that contribute to the success of pre-trained visual representations for robotic manipulation.  Understanding these factors is highly relevant to the current work, as it helps contextualize the importance of manipulation centricity and the effectiveness of the proposed method in improving both representation quality and downstream task performance.  It addresses the same key problems.", "section_number": 3}]}