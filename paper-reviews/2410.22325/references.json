{"references": [{" publication_date": "2022", "fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "reason": "This paper introduces Masked Autoencoders (MAE), a self-supervised learning method that significantly improves the efficiency and scalability of training vision transformers.  MAE's success in computer vision has inspired many robotic vision applications, influencing the approach used in the target paper to leverage self-supervised learning techniques to efficiently train robotic representations.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Suraj Nair", "paper_title": "R3M: A universal visual representation for robot manipulation", "reason": "This work presents a pre-trained visual representation model, R3M, which is evaluated and compared to the proposed model, MCR, as a baseline.  The importance of this citation stems from its influence on the proposed method.  R3M uses time contrastive learning and a video-language alignment approach; MCR takes inspiration from this but then extends these approaches to leverage robotic dynamic information.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "reason": "This paper introduces Vision Transformers (ViT), a novel architecture that achieves state-of-the-art results in image recognition tasks.  The target paper leverages this architecture to extract visual representations that better capture manipulation centricity.  ViT is an important baseline for many computer vision tasks, especially in the robotic domain.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Alexander Khazatsky", "paper_title": "Droid: A large-scale in-the-wild robot manipulation dataset", "reason": "This paper introduces DROID, a large-scale robot manipulation dataset used for training the proposed Manipulation Centric Representation (MCR) model. DROID's significance is key because it provides the training data necessary for improving the performance and manipulation centricity of robotic representations. The availability of this dataset is directly related to the success of the MCR method. ", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Tianhe Yu", "paper_title": "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning", "reason": "This paper introduces the MetaWorld benchmark, a collection of diverse robotic manipulation tasks used for evaluating the performance of the proposed MCR model. MetaWorld is important because it serves as one of the primary evaluation environments to rigorously assess the proposed method against baseline methods, showing the impact of the improved manipulation centricity.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Ajay Mandlekar", "paper_title": "What matters in learning from offline human demonstrations for robot manipulation", "reason": "This paper investigates the challenges and key factors in learning robotic manipulation skills from offline human demonstrations. The work establishes that capturing manipulation-relevant regions in the dataset improves downstream performance. This directly influenced the design and analysis of the proposed MCR method.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Michael Laskin", "paper_title": "CURL: Contrastive unsupervised representations for reinforcement learning", "reason": "This paper introduces CURL, a contrastive unsupervised learning method for reinforcement learning. CURL's contrastive learning approach inspired the dynamics alignment loss in MCR, demonstrating the importance of contrastive learning in robotics.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Chen Bao", "paper_title": "DexArt: Benchmarking generalizable dexterous manipulation with articulated objects", "reason": "This paper introduces DexArt, a benchmark for dexterous manipulation tasks used in the experimental setup. The significance of this dataset lies in evaluating the generalization of the robotic representations learned by the MCR method to a wide range of manipulation tasks and end-effectors.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Younggyo Seo", "paper_title": "Masked world models for visual control", "reason": "This paper investigates masked world models for visual control in robotics.  It is a key comparison model to demonstrate the impact of the new proposed metric and large-scale robot datasets on the task performance. The key significance of this reference is that it provides a state-of-the-art baseline method for evaluating robotic visual representation methods.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Sudeep Dasari", "paper_title": "An unbiased look at datasets for visuo-motor pre-training", "reason": "This work offers a critical evaluation of the choice of datasets used for pre-training visual representations in robotics.  It directly impacts the choice of the DROID dataset in the target paper and adds to the discussion of the benefits and drawbacks of using human vs robot datasets for this purpose.  The selection of datasets is directly impactful on the result of MCR, making this citation critical.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Alexander Khazatsky", "paper_title": "Droid: A large-scale in-the-wild robot manipulation dataset", "reason": "This paper introduces DROID, a significant and large-scale dataset used for training the proposed method (MCR).  DROID's importance is evident in its direct impact on MCR's performance, providing critical data to improve the manipulation centricity and downstream success rate of robotic representations.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Kristen Grauman", "paper_title": "Ego4d: Around the world in 3,000 hours of egocentric video", "reason": "This paper introduces Ego4d, a large-scale egocentric video dataset. Although not directly used for training the MCR model, it serves as a point of comparison, highlighting the challenges and limitations of using human data for learning robotic visual representations, and motivating the choice of robot data.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Raghav Goyal", "paper_title": "The something something video database for learning and evaluating visual common sense", "reason": "This paper introduces the Something-Something dataset, which is often used in pre-training robotic visual representations.  This citation highlights the limitations of human video datasets by comparison to robot datasets, establishing the motivation for the use of DROID in the MCR method. Thus, it is impactful on the argumentation of the target paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Nicklas Hansen", "paper_title": "On pre-training for visuo-motor control: Revisiting a learning-from-scratch baseline", "reason": "This work revisits the efficacy of learning from scratch for robotic visual representation learning.  The work forms a direct comparison to MCR, with the results showing the significant advantage of using pre-trained models, further substantiating the importance of MCR's approach to manipulation-centric representation learning.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Soroush Nasiriany", "paper_title": "Robocasa: Large-scale simulation of everyday tasks for generalist robots", "reason": "This paper introduces RoboCasa, a large-scale robot manipulation simulation platform. RoboCasa is an important evaluation environment for the proposed method.  The use of this diverse simulation environment adds robustness and ensures generalizability of the learned manipulation-centric representations.  It's important to the evaluation in the target paper.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Christoph Feichtenhofer", "paper_title": "Masked autoencoders as spatiotemporal learners", "reason": "This paper proposes a method for training spatiotemporal learners using masked autoencoders, which has influenced the development of the temporal contrastive loss in MCR. Its importance lies in illustrating the advancements made in contrastive learning for temporal information that the MCR model also leverages.", "section_number": 4}, {" publication_date": "2015", "fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "reason": "This paper introduces the ResNet architecture, a fundamental building block for many computer vision tasks.  ResNet serves as the encoder architecture in the MCR model, providing an effective and widely-used backbone for extracting visual features.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Aaron van den Oord", "paper_title": "Representation learning with contrastive predictive coding", "reason": "This paper introduces the InfoNCE loss, which is used as a key component in the dynamics alignment loss objective of the MCR model. It plays a pivotal role in enhancing the manipulation centricity of the learned representations.  Its importance lies in the core of MCR's ability to learn good representations.", "section_number": 4}, {" publication_date": "2009", "fullname_first_author": "Brenna D Argall", "paper_title": "A survey of robot learning from demonstration", "reason": "This paper provides a comprehensive survey of robot learning from demonstration (LfD), which is relevant because imitation learning is the downstream task for evaluating the quality of robotic visual representations.   It adds to the foundation of the methodology in the target paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tony Z Zhao", "paper_title": "Learning fine-grained bimanual manipulation with low-cost hardware", "reason": "This work provides insights into the challenges of learning complex bimanual manipulation skills using low-cost hardware, which is particularly relevant for real-world robotic applications.  It relates to the real-world experiments in the target paper and highlights the difficulty and importance of the task.", "section_number": 5}]}