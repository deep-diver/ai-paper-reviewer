[{"content": "| Type | Method | Total Res. | Frame |\n|---|---|---|---| \n| Front View | GAIA-1<sup>\u2217</sup>[<sup>15</sup>] | 288\u00d7512\u00d71 | 26 |\n|  | DriveDreamer [<sup>35</sup>] | 128\u00d7192\u00d71 | 32 |\n|  | Vista<sup>\u2217</sup>[<sup>12</sup>] | 576\u00d71024\u00d71 | 25 |\n| Multi-view | MagicDrive [<sup>11</sup>] | 224\u00d7400\u00d76 | 60 |\n|  | Drive-WM [<sup>38</sup>] | 192\u00d7384\u00d76 | 8 |\n|  | Panacea [<sup>39</sup>] | 256\u00d7512\u00d76 | 8 |\n|  | DriveDreamer2 [<sup>46</sup>] | 256\u00d7448\u00d76 | 8 |\n|  | Delphi [<sup>27</sup>] | 512\u00d7512\u00d76 | 10 |\n|  | DiVE [<sup>18</sup>] | 480p\u00d76 | 16 |\n|  | MagicDriveDiT | **848\u00d71600\u00d76** | <sup>\u2020</sup>**129** |\n|  |  | **424\u00d7800\u00d76** | **241** |", "caption": "Table 1: Comparison of Resolution and Frame Count. We only consider a single inference, since rollout notably degrades quality. \u2217Only support text & image(s) conditions. \u2020See note in Appendix.", "description": "This table compares the resolution and frame count of various methods for video generation in autonomous driving.  It highlights the limitations of existing approaches in achieving both high resolution and long video sequences, demonstrating the superior capabilities of MagicDriveDiT.  The asterisk (*) indicates methods that only support text and image condition inputs, and the dagger (\u2020) refers to additional notes within the Appendix of the paper.  Rollout inference is not considered because it significantly reduces the quality of the generated videos.", "section": "2. Related Work"}, {"content": "| Method | FVD\u2193 | mAP\u2191 | mIoU\u2191 |\n|---|---|---|---|\n| MagicDrive [11] (16f) | 218.12 | 11.86 | 18.34 |\n| MagicDrive [11] (60f) | 217.94 | 11.49 | 18.27 |\n| MagicDrive3D [10] | 210.40 | 12.05 | 18.27 |\n| *MagicDriveDiT* | **94.84** | **18.17** | **20.40** |", "caption": "Table 2: Comparison with Baselines for Controllable Video Generation.\nVideos are generated according to conditions from the nuScenes validation set. Only first 16 frames are kept for evaluation, as in [29].\n\u2191\u2191\\uparrow\u2191/\u2193\u2193\\downarrow\u2193 indicates that a higher/lower value is better.", "description": "This table compares the performance of MagicDriveDiT with several baseline methods on controllable video generation.  The videos were generated using conditions from the nuScenes validation set, and only the first 16 frames of each generated video were used for evaluation, consistent with the methodology in paper [29]. The metrics used for comparison include FVD (lower is better), indicating video quality, mAP (higher is better), measuring the accuracy of object detection, and mIoU (higher is better), representing the accuracy of road map segmentation. The arrows indicate whether a higher or lower value is preferred for each metric.", "section": "5. Experiments"}, {"content": "| Method | FID \u2193 | Road mIoU \u2191 | Vehicle mIoU \u2191 | mAP \u2191 |\n|---|---|---|---|---|\n| BEVControl [41] | 24.85 | 60.80 | 26.80 | N/A |\n| MagicDrive [11] (Img) | **16.20** | **61.05** | 27.01 | 12.30 |\n| *MagicDriveDiT* | 20.91 | 59.79 | **32.73** | **17.65** |", "caption": "Table 3: \nComparison with Baselines for Controllable Image Generation.\nAll the annotations & camera views from the nuScenes validation set are used for evaluation. \u2191\u2191\\uparrow\u2191/\u2193\u2193\\downarrow\u2193 indicates that a higher/lower value is better.", "description": "Table 3 presents a comparison of the performance of MagicDriveDiT against several baselines on controllable image generation.  The evaluation uses all annotations and camera views from the nuScenes validation set.  Metrics used include FID (lower is better, indicating higher image quality), Road mIoU, Vehicle mIoU (both higher is better, representing better segmentation accuracy), and mAP (higher is better, demonstrating superior object detection performance).  This table highlights MagicDriveDiT's performance relative to established methods in generating realistic and controllable street-view images.", "section": "5. Experiments"}, {"content": "| FID |\n|---|---| \n| \u2193 |", "caption": "Table 4: \nSpeed for Each Training Stage of MagicDriveDiT, measured on NVIDA A800 GPUs.\nOver a 4-day period (for example), Stage 1 training yields nearly 60 times more iterations than Stage 3, and Stage 2 offers about 7 times more.\n\u2217This value is calculated by multiplication with sequence parallel (SP) size (in practice, we use SP size of 4 for the stage 3, with 66.24s/it).", "description": "This table presents the training speed for each stage of the MagicDriveDiT model, using NVIDIA A800 GPUs.  The training process is divided into three stages with varying resolutions and video lengths.  The table shows the seconds per iteration and the total number of iterations achieved over a four-day period for each stage.  Stage 1, which uses low-resolution images, completes significantly more iterations than Stages 2 and 3, which use higher-resolution videos of increasing length.  The asterisk (*) indicates that the time per iteration for Stage 3 is calculated using a sequence parallel (SP) size of 4, resulting in a longer time per iteration compared to Stages 1 and 2. The data highlights the efficiency of the progressive training strategy.", "section": "4.3 Progressive Bootstrap Training"}, {"content": "| Road | mIoU\u2191 |\n|---|---|", "caption": "Table 5: \nComparison between Different Training Configurations. To test adaptation ability for higher resolution and longer videos, all the models load a pre-trained weight for short videos (9\u00d7\\times\u00d7424\u00d7\\times\u00d7800) and are trained with the same GPU hours.", "description": "This table compares the performance of models trained with different configurations to assess their ability to adapt to higher resolutions and longer video sequences.  All models began with pre-trained weights from a short video dataset (9 frames of 424x800 resolution). The training time (GPU hours) was kept constant across all configurations. The table shows how varying the training data (resolution and length of videos) affects the final model's performance as measured by FVD (lower is better), mAP, and mIoU (higher is better). This helps to determine which training approach leads to the best generalization for generating high-resolution, long videos.", "section": "5. Experiments"}, {"content": "| Vehicle | mIoU\u2191 |\n|---|---|", "caption": "Table 6: \nGeneration Quality for Videos Longer than Training.\nWe randomly sample 10 sequence from the nuScenes validation set and report FVD (the lower the better). n\u00d7n\\timesitalic_n \u00d7: n\ud835\udc5bnitalic_n times of maximum training frame number, i.e., 129 frames for 424\u00d7\\times\u00d7800 and 33 for 848\u00d7\\times\u00d71600. /: exceed the maximum frame of dataset.", "description": "This table presents the results of evaluating the quality of videos generated by the model, specifically focusing on videos that are longer than those used during the model's training phase. The quality is measured using the FVD metric (lower is better). Ten video sequences were randomly selected from the nuScenes validation set for this evaluation. The table indicates how many times the number of frames in the generated videos exceed the maximum number of frames seen during training (indicated by 'n'). The symbol '/' denotes cases where the generated videos' frame counts surpassed those in the training data.", "section": "5. Experiments"}, {"content": "| mAP |\n|---|---| \n| \u2191 |", "caption": "Table I: Configuration for Variable Length and Resolution Training.\nThe mixing configuration aligns with our progressive bootstrap training with 3 stages, from low-resolution images to high-resolution long videos.", "description": "This table details the training configurations for MagicDriveDiT across three stages.  The progressive bootstrap training approach starts with low-resolution images, then moves to low-resolution short videos, and finally high-resolution long videos. Each stage uses a mix of video lengths and resolutions to improve model generalization and performance.  The table shows the resolution, number of frames, sequence parallelism setting, and number of training steps for each stage.", "section": "5. Experiments"}, {"content": "| Stages | Sec./Iter. | Iter. for 4 days |\n|---|---|---|\n| stage 1 | 4.32 | 80k |\n| stage 2 | 39.84 | 8.7k |\n| stage 3 | *264.96 | 1.3k |", "caption": "Table II: VAE Comparison for Street Views.\nCogVAE\u00a0[43] and Open-Sora\u00a0[47] (1.2) are 3D VAEs; SD VAE\u00a0[31] is 2D VAE, which is also widely adopted by previous street view generation (e.g., [11]).\nResults are PSNRs calculated through videos from the nuScenes validation set.\nMagicDriveDiT adopts CogVAE.", "description": "This table compares the performance of three different Variational Autoencoders (VAEs) for reconstructing street view videos from the nuScenes dataset.  CogVAE and Open-Sora are 3D VAEs, while SD VAE is a 2D VAE commonly used in previous street view generation methods.  The comparison is based on the Peak Signal-to-Noise Ratio (PSNR) calculated from the reconstructed videos.  The results show the PSNR values for different resolutions (224x400, 424x800, 848x1600) and video lengths (17 frames, 33/34 frames). The MagicDriveDiT model utilizes the CogVAE.", "section": "5. Experiments"}]