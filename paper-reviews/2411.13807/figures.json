[{"figure_path": "https://arxiv.org/html/2411.13807/x1.png", "caption": "Figure 1: MagicDriveDiT generates high-resolution and long videos with multi-view and control supports, significantly exceeding the limitation of previous works\u00a0[35, 15, 11, 12].", "description": "Figure 1 showcases the superior capabilities of MagicDriveDiT in generating high-resolution, extended-length videos compared to existing methods.  It highlights MagicDriveDiT's ability to produce videos with multiple viewpoints and offers precise control over the generated content.  The resolution and frame count significantly surpass those achieved by previous techniques such as DriveDreamer, GAIA-1, Vista, and MagicDrive, demonstrating a substantial improvement in video synthesis for autonomous driving applications.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.13807/x2.png", "caption": "Figure 2: Different from spatial latent\u00a0[11, 39, 38, 27, 18], spatial-temporal latent (ours) requires spatial-temporal condition injection for geometry controls (we omit the text condition here).", "description": "Figure 2 illustrates the core difference between existing video generation methods and the proposed MagicDriveDiT approach.  Existing methods typically utilize spatial latents to control video generation, meaning that control signals are only associated with spatial locations within each frame. This limits their ability to precisely manipulate the temporal aspects of video synthesis. In contrast, MagicDriveDiT employs spatial-temporal latents. This requires injecting spatial and temporal condition signals, not just spatial ones. This allows for more precise and nuanced control over the video's spatiotemporal dynamics, enabling more realistic and coherent video generation, especially crucial for long video generation where temporal consistency is paramount.", "section": "3. Preliminary"}, {"figure_path": "https://arxiv.org/html/2411.13807/x3.png", "caption": "Figure 3: Architecture Overview of MagicDriveDiT. To incorporate different conditions for video generation, MagicDriveDiT adopts a two-branch architecture as in [44] with basic STDiT3 blocks from [6, 47]. We also propose the MVDiT block for multi-view consistency and Spatial-Temporal (Box/Traj.) Encoder to inject condition into the Spatial-Temporal (SP) Latent.", "description": "Figure 3 provides a detailed architecture overview of the MagicDriveDiT model, focusing on how it handles various conditions for video generation.  It uses a two-branch architecture (similar to [44]) with STDiT3 blocks ([6, 47]) as the foundation.  A key innovation is the introduction of a Multi-View DiT (MVDiT) block for maintaining consistency across multiple views. The model also features a Spatial-Temporal (Box/Traj.) Encoder to effectively integrate control signals (like bounding boxes and trajectories) directly into the spatial-temporal latents, thus enabling precise control over the generated video content.", "section": "4. Methods"}, {"figure_path": "https://arxiv.org/html/2411.13807/x4.png", "caption": "Figure 4: Spatial-Temporal Encoder for Maps (a) and Boxes (b). Our spatial encoding module follows [11], and temporal encoding integrates the downsampling strategy in our 3D VAE\u00a0[43], resulting in temporally aligned embedding between the control signals and video latents.", "description": "Figure 4 illustrates the architecture of the spatial-temporal encoders for maps and 3D bounding boxes used in MagicDriveDiT.  The spatial encoding module, based on the design in reference [11], processes spatial information from these control signals. The temporal encoding component leverages the downsampling strategy employed by the 3D Variational Autoencoder (VAE) from reference [43]. This integration ensures temporal alignment between the control signals and the video latents, enhancing the model's ability to control the generation of high-resolution, long videos.", "section": "4.2. Design for High-Resolution Long Video"}, {"figure_path": "https://arxiv.org/html/2411.13807/x5.png", "caption": "Figure 5: Progressive Bootstrap Training in MagicDriveDiT. For high-resolution long video generation, we train the model to progressively scale up from both resolution and frame count.", "description": "This figure illustrates the progressive training strategy used in MagicDriveDiT to generate high-resolution and long videos.  The model is initially trained on low-resolution images, then progressively trained on low-resolution videos of increasing length, and finally on high-resolution videos of increasing length. This stepwise approach allows the model to gradually learn to generate more complex and realistic videos. The training process starts with low-resolution images (224x400) and a few frames and progresses through increasing video lengths (9,17,33,65 frames) and higher resolutions (424x800 and 848x1600).", "section": "4.3. Progressive Bootstrap Training"}, {"figure_path": "https://arxiv.org/html/2411.13807/x6.png", "caption": "Figure 6: Qualitative Comparison between MagicDriveDiT and MagicDrive\u00a0[11].\nFrames are extracted from the generated videos.\nTo conserve space, we only present the 3 (out of 6) perspectives that include the front view.\nTwo crops from the generated views are magnified and shown on the right.\nBy generating 4\u00d7\\times\u00d7 resolution of MagicDrive,\nthe synthesized street view of our model contains finer details.", "description": "Figure 6 presents a qualitative comparison of street scene videos generated by MagicDriveDiT and MagicDrive [11]. It showcases the improved visual quality and resolution achieved by MagicDriveDiT. The figure displays frames extracted from generated videos, focusing on three out of six perspectives that include the front view for brevity.  To highlight the increased detail, two crops from the generated views are magnified and presented alongside the original frames.  MagicDriveDiT demonstrates its superior performance by generating videos with 4 times the resolution of MagicDrive, resulting in significantly finer details and a more realistic street scene representation.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.13807/x7.png", "caption": "Figure 7: \nReconstruction Visualization from Different VAEs.\nCogVAE\u00a0[43] maintains most details compared with others and exhibits better performance for high-resolution contents.", "description": "Figure 7 presents a comparison of reconstruction quality from three different variational autoencoders (VAEs): CogVAE [43], SD-VAE, and OpenSora VAE.  Each VAE was used to reconstruct high-resolution images from the nuScenes dataset.  The figure visually demonstrates that CogVAE [43] achieves superior reconstruction quality, preserving significantly more detail than the other VAEs tested, particularly in high-resolution scenarios.", "section": "5.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.13807/x8.png", "caption": "Figure 8: MagicDriveDiT generates high-resolution (e.g., 424\u00d7\\times\u00d7800) street-view videos for 241 frames (i.e., the full length of nuScenes videos, approximately 20 seconds at 12 FPS) with multiple controls (i.e., road map, object boxes, ego trajectory, and text). Notably, the 241-frame length at 424\u00d7\\times\u00d7800 are unseen during training, demonstrating our method\u2019s generalization capability to video length. We annotate the ego-vehicle trajectory and selected objects to aid localization, with same-color boxes denoting the same object. Due to space constraints, the \u201crainy\u201d example includes only two frames; additional examples can be found in the Appendix\u00a0H.", "description": "Figure 8 showcases MagicDriveDiT's ability to generate high-resolution (424x800) and long (241 frames, ~20 seconds at 12 FPS) street-view videos.  The impressive aspect is that the model generalizes to this video length, despite not seeing such long sequences during training.  The figure demonstrates this by displaying videos under both sunny and rainy conditions.  The video includes multiple controls (road map, object boxes, ego trajectory, text), which are used by the model to generate the realistic street scenes.  For better visualization, the ego-vehicle trajectory and selected objects are annotated with same-colored boxes representing the same object across frames. Due to space limitations, only two frames are shown for the rainy condition.  More examples are provided in Appendix H.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.13807/x9.png", "caption": "Figure 9: Validation Loss through Training with Different SP Encodings.\n4\u00d74\\times4 \u00d7down (our methods in MagicDriveDiT) can help the model converge, performing the best among all the encodings.", "description": "This figure displays the validation loss curves during training for different spatial-temporal encoding methods used in MagicDriveDiT.  The x-axis represents training steps and the y-axis shows the validation loss (log scale). Three lines represent different encoding methods: one is a baseline method using a global reduction of the temporal dimension, another is a baseline method using interpolation for temporal alignment, and the final line represents the authors' proposed method using 4x downsampling with a spatial-temporal encoder.  The results demonstrate that the proposed 4x downsampling method significantly improves model convergence, resulting in lower validation loss compared to the two baseline methods.", "section": "5.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.13807/x10.png", "caption": "Figure 10: \nVisual Effect of Spatial-Temporal Encoding for Boxes.\nFor videos with spatial-temporal latent encoding, a simple global reduce baseline can cause artifacts and trailing effects in object trajectories across viewpoints (we highlight them with red boxes). Our spatial-temporal encoding effectively resolves this, maintaining object clarity and accurate motion trajectories.", "description": "This figure demonstrates the impact of spatial-temporal encoding on handling object trajectories in multi-view videos.  The top row shows video frames generated without proper spatial-temporal alignment. A simple \"global reduce\" method is used, leading to artifacts and trailing effects in object trajectories across different viewpoints (highlighted in red). The bottom row shows frames from a video generated using the authors' spatial-temporal encoding technique. This approach effectively resolves the issues observed in the top row, maintaining the clarity and accuracy of object trajectories across viewpoints.", "section": "4. Methods"}, {"figure_path": "https://arxiv.org/html/2411.13807/x11.png", "caption": "Figure I: Diagram for Sequence Parallel.\nLeft: We split the spatial dimension before the first block and gather them after the last block.\nRight: For each attention module, we use all-to-all communication, changing the splitting dimension to attention heads.\nB: batch; T: temporal dimension; S: spatial dimension; D: latent dimension; HD: number of heads; CH: per-head dimension; SP: sequence parallel size.", "description": "This figure illustrates the sequence parallel training strategy used to handle long video sequences. The left panel shows how the spatial dimension of the input is split across multiple GPUs before the first block of the network and gathered back together after the last block.  The right panel details the all-to-all communication used within each attention module to efficiently manage the data across the GPUs.  The annotations define the variables: B (batch size), T (temporal dimension), S (spatial dimension), D (latent dimension), HD (number of heads), CH (per-head dimension), and SP (sequence parallel size).", "section": "A. Sequence Parallel Training"}, {"figure_path": "https://arxiv.org/html/2411.13807/x12.png", "caption": "(a) Generation from Vista. It takes the first frame as input and generates the following (only support the front view).", "description": "This figure compares the video generation quality between Vista and MagicDriveDiT. Vista uses a rollout method, taking the last frame of each inference as the input for the next inference. This method is limited in the length of generated videos and the quality decreases as the length increases.  In contrast, MagicDriveDiT generates high-quality long videos without the rollout method, showing its ability to maintain quality in longer sequences.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.13807/x13.png", "caption": "(b) Generation from MagicDriveDiT. We take conditions as inputs and generate the full video (only show the first 9s for comparison).", "description": "This figure shows a comparison of video generation between the Vista method and the MagicDriveDiT method.  The Vista method uses a rollout approach, where the last frames from one prediction are used as input to the next, creating a sequence.  This is shown in subfigure (a) only shows the first 9 seconds of the video for brevity. In contrast, subfigure (b) shows the first 9 seconds of a video created by the MagicDriveDiT method, which generates the full video directly from the input conditions.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.13807/x14.png", "caption": "Figure II: Comparison between Rollout for Long Videos (Vista\u00a0[12]) and Single Inference (our MagicDriveDiT). Although rollout can handle long videos, the quality is significantly degraded. In contrast, our extrapolation maintains high quality in long video generation.", "description": "Figure II demonstrates a comparison of long video generation methods. Vista [12], employing a rollout approach (where predictions from previous frames are used as input for the next), produces videos of considerable length.  However, the image quality degrades significantly due to accumulated prediction errors. In contrast, MagicDriveDiT, using a single inference, generates high-quality videos that maintain quality even at longer durations by extrapolating from shorter video sequences learned during training.  This showcases the superior performance of MagicDriveDiT in producing long videos while preserving the quality of the generated content.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.13807/x15.png", "caption": "Figure III: We show some frames from the generated 6\u00d7848\u00d71600\u00d7241684816002416\\times 848\\times 1600\\times 2416 \u00d7 848 \u00d7 1600 \u00d7 241 videos with the same scene configuration (i.e., boxes, maps, cameras, and ego trajectory) but under different weather conditions. Conditions are from the nuScenes validation set.", "description": "Figure III presents a visual comparison of video frames generated using the MagicDriveDiT model under varying weather conditions.  The model is given the same scene configuration (3D bounding boxes, bird's-eye view map, camera positions and ego-vehicle trajectory).  However, the weather condition (sunny or rainy) is changed as a control input. Each row shows frames from a 241-frame long video at 848x1600 resolution, captured at different time points (+0s, +2.7s, +5.3s, and so on). This demonstrates the model's capacity to generate realistic videos with consistent scene elements while modifying specified attributes (weather) in a controlled manner.", "section": "5. Experiments"}]