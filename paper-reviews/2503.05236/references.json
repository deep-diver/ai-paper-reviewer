{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-01", "reason": "This is likely important because the paper uses GPT-4V to provide a stronger reward signal for their method through direct preference optimization."}, {"fullname_first_author": "Runtao Liu", "paper_title": "VideoDPO: Omni-preference alignment for video diffusion generation", "publication_date": "2024-12-01", "reason": "VideoDPO is important because the authors implement their unified system to perform against its video generation reward system, OmniScore, as well as sharing several implementation details."}, {"fullname_first_author": "Yibin Wang", "paper_title": "LIFT: Leveraging human feedback for text-to-video model alignment", "publication_date": "2024-12-01", "reason": "LIFT is cited throughout this work as a relevant reward model in the video domain, specifically with a Human Rating Annotation dataset, LiFT-HRA, is specifically used in the creation of its reward model."}, {"fullname_first_author": "Tianyi Xiong", "paper_title": "LLaVA-Critic: Learning to evaluate multimodal models", "publication_date": "2024-10-01", "reason": "LLaVA-Critic is mentioned as a reward model baseline, and is used as the first iteration to enhance to provide a fair comparison as well as for its use in creating a critic to understand images."}, {"fullname_first_author": "Bram Wallace", "paper_title": "Diffusion model alignment using direct preference optimization", "publication_date": "2024-01-01", "reason": "This work is important because the proposed reward model uses direct preference optimization (DPO) to optimize with the objective described in this paper, and so its details of implementation are relevant."}]}