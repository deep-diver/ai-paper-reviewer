[{"figure_path": "https://arxiv.org/html/2411.14405/extracted/6016353/assets/strawberry_2.jpg", "caption": "Figure 1: A classic \u2018strawberry\u2019 question reasoned by our Marco-o1 model: \u201cHow many \u2018r\u2019s are in \u2018strawberry\u2019.\u201d", "description": "This figure demonstrates the reasoning process of the Marco-01 model on a simple word problem: counting the occurrences of the letter 'r' in the word 'strawberry'.  The figure shows a four-step reasoning chain that starts with the question and ends with the correct answer. Each step involves a step-by-step analysis of the word to count and verify the number of 'r's.  This example showcases the model's ability to break down a problem into smaller, manageable steps before arriving at a solution. ", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.14405/extracted/6016353/assets/intro_2.jpg", "caption": "Figure 2: The overview of Marco-o1.", "description": "This figure illustrates the architecture of Marco-01, a reasoning model that enhances its capabilities by integrating LLMs with Monte Carlo Tree Search (MCTS).  It shows how the model uses supervised fine-tuning on datasets including the filtered Open-01 CoT dataset, synthetic Marco-01 CoT data, and the Marco Instruction dataset. The MCTS component is highlighted, showing how it explores multiple reasoning paths using confidence scores derived from LLM outputs.  The reasoning action strategy, employing both step-level and mini-step-level actions, is also depicted, along with the calculation of confidence scores to guide the search towards more effective and reliable reasoning chains.", "section": "3. Solution Space Expansion via MCTS"}, {"figure_path": "https://arxiv.org/html/2411.14405/extracted/6016353/assets/results.jpg", "caption": "Figure 3: The main results of Marco-o1.", "description": "Figure 3 presents a bar chart comparison of the performance of different models on the MGSM benchmark dataset (English and Chinese). The models compared include Qwen2-7B-Instruct, Marco-01-CoT, Marco-01-MCTS (step), Marco-01-MCTS (mini-step of 64 tokens), and Marco-01-MCTS (mini-step of 32 tokens). The chart visually demonstrates the accuracy improvements achieved by incorporating various techniques such as Chain-of-Thought (CoT) fine-tuning and Monte Carlo Tree Search (MCTS) into the base model, Qwen2-7B-Instruct.  The results highlight the effectiveness of the proposed methods in enhancing the reasoning capabilities of the Marco-01 model.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.14405/extracted/6016353/assets/cot-step.jpg", "caption": "Figure 4: MCTS Expands the Solution Space for Correct Answers. Comparison between Marco-o1-CoT (left) and Marco-o1-MCTS (step) (right) on the MGSM dataset. While Marco-o1-CoT failed to provide the correct answer, integrating MCTS with step-level actions allowed the model to explore a broader solution space, increasing the likelihood of arriving at the correct solution.", "description": "Figure 4 presents a comparative analysis of two model variations: Marco-01-CoT (without MCTS) and Marco-01-MCTS (with step-level MCTS integration), both applied to the MGSM dataset.  The left side shows the reasoning process of Marco-01-CoT, which fails to reach the correct solution. The right side showcases Marco-01-MCTS successfully finding the correct answer. By using MCTS with step-level actions, the model effectively explores a much wider range of potential solution paths, significantly enhancing its chances of arriving at the correct solution, illustrating the benefit of MCTS in expanding the search space for complex problem-solving.", "section": "3. Solution Space Expansion via MCTS"}, {"figure_path": "https://arxiv.org/html/2411.14405/extracted/6016353/assets/step-ministep32.jpg", "caption": "Figure 5: Finer Granularity with mini-steps Enhances Problem-Solving. Comparison between Marco-o1-MCTS (step) (left) and Marco-o1-MCTS (mini-step of 32 tokens) (right) on the MGSM dataset. The step-level action strategy did not yield the correct answer, but by using a finer-grained mini-step of 32 tokens, the model successfully navigated the solution space to find the correct answer, demonstrating the effectiveness of increased action granularity.", "description": "Figure 5 presents a detailed comparison of two approaches within the Marco-01-MCTS model, using different action granularities to solve problems in the MGSM dataset.  The left side shows the results using a step-level action strategy (a coarser granularity), where the model failed to arrive at the correct solution.  In contrast, the right side displays the results when employing a finer-grained mini-step strategy of 32 tokens, which successfully led to the correct answer. This visualization effectively demonstrates how increasing the action granularity (using smaller steps) significantly improves the model's ability to navigate the solution space and find accurate solutions.", "section": "4. Reasoning Action Strategy"}, {"figure_path": "https://arxiv.org/html/2411.14405/extracted/6016353/assets/ministep64-step.jpg", "caption": "Figure 6: Optimal Action Granularity Depends on Problem Complexity. Comparison between Marco-o1-MCTS (mini-step of 64 tokens) (left) and Marco-o1-MCTS (step) (right) on the MGSM dataset. The model with a mini-step of 64 tokens failed to find the correct answer, whereas using step-level actions enabled the model to correctly solve the problem. This highlights that we cannot draw definitive conclusions about which action strategy is superior. We believe that as the reward becomes more accurate, the larger solution space provided by MCTS will demonstrate greater potential.", "description": "Figure 6 demonstrates how the optimal granularity of actions within the Monte Carlo Tree Search (MCTS) algorithm depends on the complexity of the problem.  The figure presents a comparison between two versions of the Marco-01 model using MCTS. One version uses mini-steps of 64 tokens as actions, while the other uses steps as actions.  The results on the MGSM dataset show that for this particular problem, the step-level actions led to the correct answer, while the mini-step approach failed.  This highlights that there's no universally superior action granularity;  the best choice depends on the problem's complexity.  The authors suggest that more accurate reward signals within MCTS would likely lead to better performance with the finer-grained mini-step approach, due to the larger solution space that it explores.", "section": "4. Reasoning Action Strategy"}, {"figure_path": "https://arxiv.org/html/2411.14405/extracted/6016353/assets/translation.jpg", "caption": "Figure 7: Demonstration of translation task using Marco-o1 of a colloquial expression \u201cThis shoe has a comfortable sole and is highly recommended for purchase\u201d.", "description": "Figure 7 showcases Marco-01's translation capabilities by translating the Chinese colloquial expression \"\u8fd9\u4e2a\u978b\u62e5\u6709\u8e29\u5c4e\u611f,\u5f88\u8212\u670d,\u63a8\u8350\u8d2d\u4e70\" which literally translates to \"This shoe has a feeling of stepping on feces, very comfortable, recommended to buy.\"  The model demonstrates its ability to understand nuanced language and cultural context, translating the phrase into the more natural and appropriate English equivalent: \"This shoe has a comfortable sole and is highly recommended for purchase.\" This highlights Marco-01's superior grasp of colloquialisms and its ability to produce accurate and fluent translations.", "section": "6. Case Study on Translation Tasks"}, {"figure_path": "https://arxiv.org/html/2411.14405/extracted/6016353/assets/trans-case-1.jpg", "caption": "Figure 8: Translation comparison of a colloquial expression \u201cIt\u2019s so beautiful that it\u2019s captivating, the upper part has a distinctly Korean style, the soft and fluffy material is perfectly thick, and it\u2019s complemented by a base layer, creating a unique and everyday-wear outfit\u201d.", "description": "Figure 8 presents a comparison of how Google Translate and the Marco-01 model translate a colloquial Chinese sentence.  The Chinese sentence describes clothing, highlighting its beauty, Korean style, soft texture, appropriate thickness, and layering.  The comparison shows Marco-01's more nuanced translation, capturing the original text's descriptive style and cultural references better than Google Translate's more literal approach.", "section": "6. Case Study on Translation Tasks"}, {"figure_path": "https://arxiv.org/html/2411.14405/extracted/6016353/assets/trans-case-2.jpg", "caption": "Figure 9: Translation comparison of a colloquial expression \u201cIt\u2019s so beautiful! And it\u2019s so cheap, super straight and doesn\u2019t curl. Buy it, buy it!\u201d.", "description": "Figure 9 shows a comparison of how Google Translate and the Marco-01 model translate the Chinese colloquial expression  \"\u592a\u592a\u592a\u592a\u597d\u770b\u4e86!\u800c\u4e14\u4ef7\u683c\u8fd9\u4e48\u4fbf\u5b9c,\u8d85\u7ea7\u677f\u6b63\u4e0d\u5377\u8fb9,\u90fd\u4e70\u5b83,\u4e70\u5b83.\"  The Marco-01 model's translation, \"It's so beautiful! And it's so cheap, super straight and doesn't curl. Buy it, buy it!\" more accurately captures the enthusiastic and colloquial tone of the original Chinese.", "section": "6. Case Study on Translation Tasks"}]