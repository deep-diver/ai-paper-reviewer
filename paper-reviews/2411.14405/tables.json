[{"content": "| Dataset | Number of Samples |\n|---|---| \n| Open-O1 CoT Dataset (Filtered) [Team, 2024] | 45,125 |\n| Marco-o1 CoT Dataset (Synthetic) | 10,000 |\n| Marco Instruction Dataset | 5,141 |\n| **Total** | **60,266** |", "caption": "Table 1: Overview of Marco Reasoning Datasets", "description": "This table presents a summary of the datasets used to train and enhance the Marco-01 reasoning model.  It details the name of each dataset, its source (including citations where applicable), and the number of samples contained within. The datasets are categorized to highlight the various aspects of reasoning they focus on, such as Chain-of-Thought (CoT) reasoning, instructions, and synthetically generated data.", "section": "2. Marco Reasoning Datasets"}, {"content": "| Model | MGSM-En (Acc.) | MGSM-Zh (Acc.) |\n|---|---|---|\n| Qwen2-7B-Instruct | 84.23% | 76.80% |\n| Marco-o1-CoT | 85.60% | 71.20% |\n| Marco-o1-MCTS (step) | 90.40% | 80.00% |\n| Marco-o1-MCTS (mini-step of 64 tokens) | 88.40% | 80.40% |\n| Marco-o1-MCTS (mini-step of 32 tokens) | 87.60% | 82.40% |", "caption": "Table 2: Experimental results on MGSM datasets", "description": "This table presents the experimental results obtained from evaluating different model configurations on the MGSM (Massive General Reasoning benchmark) datasets.  The models evaluated include the baseline Qwen2-7B-Instruct, Marco-01-CoT (fine-tuned with Chain-of-Thought data), and three variations of Marco-01-MCTS (integrating Monte Carlo Tree Search with different action granularities: step-level, 64-token mini-steps, and 32-token mini-steps). Results are shown in terms of accuracy for both English and Chinese subsets of the MGSM dataset.  The table highlights the impact of CoT fine-tuning and MCTS with various action strategies on model performance in general reasoning tasks.", "section": "5. Experiments"}]