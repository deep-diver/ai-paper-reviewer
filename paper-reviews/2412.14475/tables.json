[{"content": "Task|Zero-shot|Zero-shot|Zero-shot|Zero-shot|Zero-shot|Zero-shot|Zero-shot|Zero-shot|Fine-Tune|Fine-Tune\n---|---|---|---|---|---|---|---|---|---|---\n|CLIP|OpenCLIP|SigLIP|BLIP2|MagicLens|E5-V|UniIR|MMRet|VLM2Vec|MMRet|\nClassification (10 tasks)|55.8|63.5|45.4|10.3|48.0|9.6|53.7|49.1|65.6|58.8\nImageNet-1K|55.8|63.5|45.4|10.3|48.0|9.6|53.7|49.1|65.6|58.8\nN24News|34.7|38.6|13.9|36.0|33.7|23.4|33.9|45.8|79.5|71.3\nHatefulMemes|51.1|51.7|47.2|49.6|49.0|49.7|51.0|51.0|67.1|53.7\nVOC2007|50.7|52.4|64.3|52.1|51.6|49.9|62.7|74.6|88.6|85.0\nSUN397|43.4|68.8|39.6|34.5|57.0|33.1|61.7|60.1|72.7|70.0\nPlace365|28.5|37.8|20.0|21.5|31.5|8.6|38.0|35.3|42.6|43.0\nImageNet-A|25.5|14.2|42.6|3.2|8.0|2.0|12.9|31.6|19.3|36.1\nImageNet-R|75.6|83.0|75.0|39.7|70.9|30.8|61.6|66.2|70.2|71.6\nObjectNet|43.4|51.4|40.3|20.6|31.6|7.5|37.1|49.2|29.5|55.8\nCountry-211|19.2|16.8|14.2|2.5|6.2|3.1|8.8|9.3|13.0|14.7\nAll Classification|42.8|47.8|40.3|27.0|38.8|21.8|42.1|47.2|54.8|56.0\nVQA (10 tasks)|7.5|11.5|2.4|8.7|12.7|8.9|24.5|28.0|63.2|73.3\nOK-VQA|7.5|11.5|2.4|8.7|12.7|8.9|24.5|28.0|63.2|73.3\nA-OKVQA|3.8|3.3|1.5|3.2|2.9|5.9|10.6|11.6|50.2|56.7\nDocVQA|4.0|5.3|4.2|2.6|3.0|1.7|5.6|12.6|78.4|78.5\nInfographicsVQA|4.6|4.6|2.7|2.0|5.9|2.3|5.0|10.6|40.8|39.3\nChartQA|1.4|1.5|3.0|0.5|0.9|2.4|1.8|2.4|59.0|41.7\nVisual7W|4.0|2.6|1.2|1.3|2.5|5.8|12.3|9.0|47.7|49.5\nScienceQA|9.4|10.2|7.9|6.8|5.2|3.6|11.6|23.3|43.4|45.2\nVizWiz|8.2|6.6|2.3|4.0|1.7|2.6|19.2|25.9|39.2|51.7\nGQA|41.3|52.5|57.5|9.7|43.5|7.8|49.3|41.3|60.7|59.0\nTextVQA|7.0|10.9|1.0|3.3|4.6|3.2|10.6|18.9|66.1|79.0\nAll VQA|9.1|10.9|8.4|4.2|8.3|4.9|15.0|18.4|54.9|57.4\nRetrieval (12 tasks)|30.7|25.4|21.5|18.0|24.8|9.2|37.6|62.6|73.3|83.0\nVisDial|30.7|25.4|21.5|18.0|24.8|9.2|37.6|62.6|73.3|83.0\nCIRR|12.6|15.4|15.1|9.8|39.1|6.1|53.2|65.7|47.8|61.4\nVisualNews_t2i|78.9|74.0|51.0|48.1|50.7|13.5|63.6|45.7|67.2|74.2\nVisualNews_i2t|79.6|78.0|52.4|13.5|21.1|8.1|68.8|53.4|70.7|78.1\nMSCOCO_t2i|59.5|63.6|58.3|53.7|54.1|20.7|72.0|68.7|70.6|78.6\nMSCOCO_i2t|57.7|62.1|55.0|20.3|40.0|14.0|74.1|56.7|66.5|72.4\nNIGHTS|60.4|66.1|62.9|56.5|58.1|4.2|69.7|59.4|66.1|68.3\nWebQA|67.5|62.1|58.1|55.4|43.0|17.7|86.3|76.3|88.1|90.2\nFashionIQ|11.4|13.8|20.1|9.3|11.2|2.8|39.3|31.5|12.9|54.9\nWiki-SS-NQ|55.0|44.6|55.1|28.7|18.7|8.6|11.3|25.4|56.6|24.9\nOVEN|41.1|45.0|56.0|39.5|1.6|5.9|66.6|73.0|47.3|87.5\nEDIS|81.0|77.5|23.6|54.4|62.6|26.8|78.2|59.9|79.9|65.6\nAll Retrieval|53.0|52.3|31.6|33.9|35.4|11.5|60.1|56.5|62.3|69.9\nVisual Grounding (4 tasks)|33.8|34.5|46.4|28.9|22.1|10.8|46.6|42.7|67.3|76.8\nMSCOCO|33.8|34.5|46.4|28.9|22.1|10.8|46.6|42.7|67.3|76.8\nRefCOCO|56.9|54.2|70.8|47.4|22.8|11.9|67.8|69.3|84.7|89.8\nRefCOCO-matching|61.3|68.3|50.8|59.5|35.6|38.9|62.9|63.2|79.2|90.6\nVisual7W-pointing|55.1|56.3|70.1|52.0|23.4|14.3|71.3|73.5|86.8|77.0\nAll Visual Grounding|51.8|53.3|59.5|47.0|26.0|19.0|62.2|62.2|79.5|83.6\nFinal Score (36 tasks)|37.8|39.7|34.8|25.2|27.8|13.3|42.8|44.0|60.1|64.1\nAll|37.8|39.7|34.8|25.2|27.8|13.3|42.8|44.0|60.1|64.1\nAll IND|37.1|39.3|32.3|25.3|31.0|14.9|44.7|43.5|66.5|59.1\nAll OOD|38.7|40.2|38.0|25.1|23.7|11.5|40.4|44.3|52.0|68.0", "caption": "Table 1: Zero-shot retrieval performance on various CIR benchmarks. \u2217 denotes the previous best performance for each benchmark prior to MMRet. \u2020 indicates methods with multiple components (e.g., GPT-3.5, Qwen1.5-32B); we report # parameters of components with known sizes. The CoCa-based MagicLens\u2021 models are proprietary. Results in bold and underline denote the best and second-best performances for each model scale, respectively. Our MMRet model achieves state-of-the-art results across different model sizes and benchmarks, surpassing the previous SOTA by 8.1% on the main benchmark CIRCO, significantly advancing zero-shot CIR methods.", "description": "This table presents a comparison of zero-shot performance across four popular Composed Image Retrieval (CIR) benchmarks: CIRCO, CIRR, FashionIQ, and GeneCIS.  The results show the mean Average Precision at 5 (mAP@5) and Recall at 1, 10, and 1 (R@1, R@10, R@1) for several different methods, including models based on CLIP, CoCa, and LLaVA architectures.  The number of parameters in each model is also provided.  Models denoted with a \u2020 symbol use multiple components; for these, only the parameters of the known components are given.  Methods marked with \u2021 used proprietary components (CoCa-based MagicLens).  The table highlights the MMRet models' state-of-the-art zero-shot performance across different model sizes, outperforming previous top performers significantly, notably by 8.1% on the CIRCO benchmark.", "section": "4.1 Zero-shot Performance on CIR tasks"}]