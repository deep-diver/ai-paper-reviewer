{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational vision-language model that significantly advanced the field of multimodal retrieval and is frequently used as a base for many other models."}, {"fullname_first_author": "Alberto Baldrati", "paper_title": "Zero-shot composed image retrieval with textual inversion", "publication_date": "2023-10-01", "reason": "This paper introduces a strong zero-shot composed image retrieval baseline that is used for comparisons in the paper."}, {"fullname_first_author": "Junjie Zhou", "paper_title": "VISTA: Visualized text embedding for universal multi-modal retrieval", "publication_date": "2024-06-01", "reason": "This paper introduces a novel approach for visual text embeddings that is compared in this work."}, {"fullname_first_author": "Jianlv Chen", "paper_title": "Pretrain like you inference: Masked tuning improves zero-shot composed image retrieval", "publication_date": "2023-11-01", "reason": "This paper provides a strong competitor using masked tuning for zero-shot composed image retrieval."}, {"fullname_first_author": "Ziyan Jiang", "paper_title": "VLM2Vec: Training vision-language models for massive multimodal embedding tasks", "publication_date": "2024-10-01", "reason": "This paper introduces a strong competitor that uses instruction tuning for multimodal embedding tasks."}]}