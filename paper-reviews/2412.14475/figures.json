[{"figure_path": "https://arxiv.org/html/2412.14475/x1.png", "caption": "Figure 1: Construction pipeline of multimodal triplets: (a) mining of image pairs, (b) generation of open-ended instructions. Multiple similarity models are used to introduce diversified correlations for the image pairs.", "description": "Figure 1 illustrates the process of creating multimodal triplets for training a universal multimodal retriever.  Panel (a) shows how image pairs are mined from a large-scale image corpus using multiple similarity models (CLIP vision-encoder, DINO vision-encoder, and CLIP text-encoder) to ensure diverse correlations between images. These models identify various relationships between image pairs, including semantic similarity, visual pattern similarity, and caption similarity.  Panel (b) demonstrates how open-ended instructions are generated for each image pair using a Multimodal Large Language Model (MLLM) and a Large Language Model (LLM). The MLLM generates a detailed description of the relationship between the images, and the LLM then refines this description into multiple open-ended instructions. These instructions provide diverse ways to describe the relationship between the image pairs and improve the model's ability to generalize.", "section": "Methodology"}, {"figure_path": "https://arxiv.org/html/2412.14475/x2.png", "caption": "Figure 2: Performance scaling of MMRet-base on the MegaPairs as data size increases. The dashed lines indicate the performance of MagicLens-B (CLIP) trained on their dataset of 36.7M data pairs.", "description": "This figure demonstrates the performance scaling of the MMRet-base model as the size of the MegaPairs training dataset increases.  The x-axis represents the number of training data pairs used, while the y-axis shows the model's performance across four different benchmarks (CIRCO, CIRR, FashionIQ, and GeneCIS).  The solid lines depict the performance of MMRet-base trained on various subsets of MegaPairs, showcasing improved performance with more data.  For comparison, dashed lines show the performance of the MagicLens-B (CLIP) model, trained on a much larger dataset (36.7M pairs). This comparison highlights the effectiveness of MegaPairs, as MMRet-base trained on a small subset of MegaPairs outperforms MagicLens-B, which was trained on a significantly larger dataset.  The figure visually illustrates that MegaPairs, despite being a much smaller dataset, leads to superior zero-shot performance.", "section": "4.1 Zero-shot Performance on CIR tasks"}, {"figure_path": "https://arxiv.org/html/2412.14475/x3.png", "caption": "Figure 3: The specific prompts for MLLM. The value of WORD_NUM ranges from 60 to 100 in our practical data generation to enhance the diversity of the generated description.", "description": "This figure details the prompts used for the Multimodal Large Language Model (MLLM) during the data synthesis process.  The MLLM receives a pair of images and is tasked with generating a detailed description highlighting commonalities and differences between them.  The prompt structure is designed to encourage diverse and nuanced descriptions by allowing for flexibility in word count (WORD_NUM ranging from 60 to 100 words).  This variability helps create a richer and more varied instruction dataset.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2412.14475/x4.png", "caption": "Figure 4: The specific prompts for LLM. The figure showcases two demonstrations, while in our practical data generation process, five demonstrations are randomly selected from a pool of 50 and fed into the LLM.", "description": "Figure 4 details the prompts used for the large language model (LLM) in the MegaPairs data synthesis pipeline.  The caption highlights that while only two examples are shown, in practice, five demonstrations were randomly selected from a pool of 50 and used to prompt the LLM. This ensured diversity and quality in the generated instructions, which described the relationships between pairs of images. The instructions are crucial for creating the final dataset used to train the multimodal retrieval models.", "section": "3.1 MegaPairs Construction"}, {"figure_path": "https://arxiv.org/html/2412.14475/x5.png", "caption": "Figure 5: The visualized examples of MegaPairs. Each row represents a single example, with the query item highlighted in a blue rectangle and the target items enclosed within a dashed box.", "description": "Figure 5 presents several examples from the MegaPairs dataset. Each row showcases a single example: a query (an image paired with its alt-text description, highlighted by a blue rectangle) and its associated target images (enclosed in dashed boxes).  The target images demonstrate diversity, including those visually similar to the query and those semantically related but visually distinct. This visual representation illustrates the varied relationships captured within the MegaPairs dataset, highlighting its capacity to encompass both visual and semantic similarity.", "section": "3.1 MegaPairs Construction"}, {"figure_path": "https://arxiv.org/html/2412.14475/x6.png", "caption": "Figure 6: Top-5 retrieved images of MMRet and MagicLens on zero-shot CIR tasks, both using the CLIP-L backbone. Queries are shown with a blue background, and the most correct retrieved images are marked with green outlines.", "description": "Figure 6 presents a comparative analysis of the top 5 image retrieval results for both MMRet and MagicLens models.  Both models used the CLIP-L backbone for a zero-shot cross-modal image retrieval (CIR) task.  Each row showcases a different query, with the query text displayed on a blue background. The retrieved images are presented, with the most relevant images (considered correct by human evaluation) highlighted by green outlines. This visual comparison allows for a direct assessment of the retrieval accuracy and the relative strengths of the two models in handling various query scenarios.", "section": "4.1 Zero-shot Performance on CIR tasks"}]