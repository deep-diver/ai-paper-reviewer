[{"figure_path": "2410.13726/figures/figures_4_0.png", "caption": "Figure 1: The pipeline of DAWN. First, we train the Latent Flow Generator (LFG) in (a) to extract the motion representation from the video. Then the Pose and Blink generation Network (PBNet) in (b) is utilized to generate the head pose and blink sequences of the avatar. Subsequently, the Audio-to-Video Flow Diffusion Model (A2V-FDM) in (c) generates the talking head video from the source image conditioned by the audio and pose/blink sequences provided by the PBNet.", "description": "The figure illustrates the overall architecture of DAWN, detailing the three main components: Latent Flow Generator (LFG), Pose and Blink generation Network (PBNet), and Audio-to-Video Flow Diffusion Model (A2V-FDM).", "section": "3 METHOD"}, {"figure_path": "2410.13726/figures/figures_4_1.png", "caption": "Figure 1: The pipeline of DAWN. First, we train the Latent Flow Generator (LFG) in (a) to extract the motion representation from the video. Then the Pose and Blink generation Network (PBNet) in (b) is utilized to generate the head pose and blink sequences of the avatar. Subsequently, the Audio-to-Video Flow Diffusion Model (A2V-FDM) in (c) generates the talking head video from the source image conditioned by the audio and pose/blink sequences provided by the PBNet.", "description": "The figure illustrates the pipeline of the DAWN framework for talking head video generation, showing the Latent Flow Generator, Pose and Blink generation Network, and Audio-to-Video Flow Diffusion Model.", "section": "3 METHOD"}, {"figure_path": "2410.13726/figures/figures_8_0.png", "caption": "Figure 2: Qualitative comparison with several state-of-the-art methods methods on HDTF (Zhang et al., 2021) and CREMA (Cao et al., 2014) datasets. Our method produces higher-quality results in video quality, lip-sync consistency, identity preservation, and head motions.", "description": "Figure 2 shows a qualitative comparison of DAWN with several state-of-the-art methods on two datasets, highlighting DAWN's superior video quality, lip synchronization, identity preservation, and head motion.", "section": "4.2 OVERALL COMPARISON"}, {"figure_path": "2410.13726/figures/figures_10_0.png", "caption": "Figure 3: Visualization of cross-identity reenactment. We extract the audio, head pose, and blink signals from the video in the first row, and use them to drive the source image, generating the talking head video in the second row.", "description": "Figure 3 shows the cross-identity reenactment results of DAWN, demonstrating its ability to generate talking head videos using audio, pose, and blink signals from a source video.", "section": "3 METHOD"}, {"figure_path": "2410.13726/figures/figures_15_0.png", "caption": "Figure 4: The qualitative study on higher resolution (256 \u00d7 256) and different portrait styles.", "description": "Figure 4 shows qualitative results of DAWN on higher resolution images (256x256) and various portrait styles, demonstrating its generalization ability.", "section": "A.2.1 EXPERIMENT ON HIGHER RESOLUTION AND DIFFERENT PORTRAIT STYLES"}, {"figure_path": "2410.13726/figures/figures_17_0.png", "caption": "Figure 3: Visualization of cross-identity reenactment. We extract the audio, head pose, and blink signals from the video in the first row, and use them to drive the source image, generating the talking head video in the second row.", "description": "Figure 3 shows the results of cross-identity reenactment where audio, head pose, and blink signals from one video are used to generate a talking head video from a different source image.", "section": "3 METHOD"}]