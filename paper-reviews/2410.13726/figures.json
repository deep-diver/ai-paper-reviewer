[{"figure_path": "2410.13726/figures/figures_4_0.png", "caption": "Figure 1: The pipeline of DAWN. First, we train the Latent Flow Generator (LFG) in (a) to extract the motion representation from the video. Then the Pose and Blink generation Network (PBNet) in (b) is utilized to generate the head pose and blink sequences of the avatar. Subsequently, the Audio-to-Video Flow Diffusion Model (A2V-FDM) in (c) generates the talking head video from the source image conditioned by the audio and pose/blink sequences provided by the PBNet.", "description": "Figure 1 illustrates the pipeline of the DAWN framework, showing how it uses a latent flow generator, a pose and blink generation network, and an audio-to-video flow diffusion model to generate talking head videos.", "section": "3 METHOD"}, {"figure_path": "2410.13726/figures/figures_4_1.png", "caption": "Figure 1: The pipeline of DAWN. First, we train the Latent Flow Generator (LFG) in (a) to extract the motion representation from the video. Then the Pose and Blink generation Network (PBNet) in (b) is utilized to generate the head pose and blink sequences of the avatar. Subsequently, the Audio-to-Video Flow Diffusion Model (A2V-FDM) in (c) generates the talking head video from the source image conditioned by the audio and pose/blink sequences provided by the PBNet.", "description": "The figure illustrates the overall architecture of DAWN, showing the three main components: Latent Flow Generator (LFG), Pose and Blink generation Network (PBNet), and Audio-to-Video Flow Diffusion Model (A2V-FDM), and their interactions.", "section": "3 METHOD"}, {"figure_path": "2410.13726/figures/figures_8_0.png", "caption": "Figure 2: Qualitative comparison with several state-of-the-art methods on HDTF (Zhang et al., 2021) and CREMA (Cao et al., 2014) datasets. Our method produces higher-quality results in video quality, lip-sync consistency, identity preservation, and head motions.", "description": "Figure 2 shows a qualitative comparison of DAWN with other state-of-the-art talking head generation methods on two datasets, highlighting DAWN's superior video quality, lip synchronization, identity preservation, and head motion.", "section": "4.2 OVERALL COMPARISON"}, {"figure_path": "2410.13726/figures/figures_10_0.png", "caption": "Figure 3: Visualization of cross-identity reenactment. We extract the audio, head pose, and blink signals from the video in the first row, and use them to drive the source image, generating the talking head video in the second row.", "description": "Figure 3 shows the results of cross-identity reenactment, where audio, head pose, and blink signals from one video are used to generate a talking head video from a different source image.", "section": "3 METHOD"}, {"figure_path": "2410.13726/figures/figures_15_0.png", "caption": "Figure 4: The qualitative study on higher resolution (256 \u00d7 256) and different portrait styles.", "description": "Figure 4 presents qualitative results demonstrating the model's ability to generate high-resolution talking head videos across various portrait styles, including photos, paintings, anime, and sketches.", "section": "A.2.1 EXPERIMENT ON HIGHER RESOLUTION AND DIFFERENT PORTRAIT STYLES"}, {"figure_path": "2410.13726/figures/figures_17_0.png", "caption": "Figure 3: Visualization of cross-identity reenactment. We extract the audio, head pose, and blink signals from the video in the first row, and use them to drive the source image, generating the talking head video in the second row.", "description": "The figure visualizes the results of cross-identity reenactment, showing how the model can generate talking head videos using audio, pose, and blink signals from one video and apply them to a different source image.", "section": "3 METHOD"}]