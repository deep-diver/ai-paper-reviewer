{"importance": "This paper is important because it addresses the limitations of existing human referring models by introducing a new benchmark and a novel MLLM. It opens new avenues for research in human-centric CV, encouraging the development of more **robust and generalizable models**.", "summary": "Introducing HumanRef, a new dataset & RexSeek, a multimodal LLM, to improve human-centric referring tasks by addressing limitations of existing methods.", "takeaways": ["Identified 5 key aspects and 3 critical characteristics for referring to any person.", "Introduced HumanRef, a new dataset for human-centric referring expressions.", "Proposed RexSeek, a detection-oriented multimodal LLM, demonstrating strong referring capabilities."], "tldr": "Current computer vision models struggle with real-world usability in referring to specific people in images because existing benchmarks focus on one-to-one referring and neglect multiple individuals. Existing models don't capture attributes, spatial relations, interactions, and reasoning effectively.\n\nTo address these issues, the paper introduces **HumanRef**, a new dataset designed for human referring tasks, and **RexSeek**, a novel model designed for this task.  The new dataset includes 103,028 referring statements for multiple instances, and model integrates a multimodal large language model (MLLM) with an object detection framework.", "affiliation": "International Digital Economy Academy (IDEA)", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.08507/podcast.wav"}