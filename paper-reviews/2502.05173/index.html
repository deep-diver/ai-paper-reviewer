<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>VideoRoPE: What Makes for Good Video Rotary Position Embedding? &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="VideoRoPE: What Makes for Good Video Rotary Position Embedding? &#183; HF Daily Paper Reviews by AI"><meta name=description content="VideoRoPE enhances video processing in Transformer models by introducing a novel 3D rotary position embedding that preserves spatio-temporal relationships, resulting in superior performance across var..."><meta name=keywords content="Computer Vision,Video Understanding,üè¢ Fudan University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05173/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05173/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="VideoRoPE: What Makes for Good Video Rotary Position Embedding?"><meta property="og:description" content="VideoRoPE enhances video processing in Transformer models by introducing a novel 3D rotary position embedding that preserves spatio-temporal relationships, resulting in superior performance across var‚Ä¶"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-02-07T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-07T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Video Understanding"><meta property="article:tag" content="üè¢ Fudan University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05173/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05173/cover.png"><meta name=twitter:title content="VideoRoPE: What Makes for Good Video Rotary Position Embedding?"><meta name=twitter:description content="VideoRoPE enhances video processing in Transformer models by introducing a novel 3D rotary position embedding that preserves spatio-temporal relationships, resulting in superior performance across var‚Ä¶"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"VideoRoPE: What Makes for Good Video Rotary Position Embedding?","headline":"VideoRoPE: What Makes for Good Video Rotary Position Embedding?","abstract":"VideoRoPE enhances video processing in Transformer models by introducing a novel 3D rotary position embedding that preserves spatio-temporal relationships, resulting in superior performance across var\u0026hellip;","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2502.05173\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-02-07T00:00:00\u002b00:00","datePublished":"2025-02-07T00:00:00\u002b00:00","dateModified":"2025-02-07T00:00:00\u002b00:00","keywords":["Computer Vision","Video Understanding","üè¢ Fudan University"],"mainEntityOfPage":"true","wordCount":"3961"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-02-19/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-02-19</p></a><a href=/ai-paper-reviewer/2025-02-20/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-02-20</p></a><a href=/ai-paper-reviewer/2025-02-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-02-21</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-19/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-02-19</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-20/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-02-20</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-02-21</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2502.05173/cover_hu18332006766325361552.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2502.05173/>VideoRoPE: What Makes for Good Video Rotary Position Embedding?</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">VideoRoPE: What Makes for Good Video Rotary Position Embedding?</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-02-07T00:00:00+00:00>7 February 2025</time><span class="px-2 text-primary-500">&#183;</span><span>3961 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">19 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2502.05173/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2502.05173/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/video-understanding/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Video Understanding
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-fudan-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Fudan University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#ropes-video-limits>RoPE&rsquo;s Video Limits</a></li><li><a href=#videorope-design>VideoRoPE Design</a></li><li><a href=#v-niah-d-challenge>V-NIAH-D Challenge</a></li><li><a href=#empirical-results>Empirical Results</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#ropes-video-limits>RoPE&rsquo;s Video Limits</a></li><li><a href=#videorope-design>VideoRoPE Design</a></li><li><a href=#v-niah-d-challenge>V-NIAH-D Challenge</a></li><li><a href=#empirical-results>Empirical Results</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2502.05173</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Xilin Wei et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-02-10</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2502.05173 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2502.05173 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2502.05173/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Rotary Position Embeddings (RoPE) are widely used in Transformer models for handling long sequences, but extending them to video data, which has a complex spatiotemporal structure, is challenging. Existing methods often flatten video data into a 1D sequence, losing crucial spatial and temporal information, or use suboptimal frequency allocations, leading to poor performance, especially when dealing with distractors.</p><p>This paper introduces VideoRoPE, a new 3D RoPE specifically designed for video. <strong>VideoRoPE uses a 3D structure, a diagonal layout to maintain spatial symmetry, adjustable temporal spacing, and low-frequency temporal allocation to reduce periodic oscillations</strong>. The authors introduce a new challenging benchmark (V-NIAH-D), which demonstrates the superiority of VideoRoPE over existing methods. <strong>VideoRoPE achieves state-of-the-art results across diverse video tasks</strong> such as long video retrieval, video understanding, and video hallucination.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-ca405054be805df532fa1bf7fc974e97></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-ca405054be805df532fa1bf7fc974e97",{strings:[" VideoRoPE, a novel 3D rotary position embedding, outperforms existing methods on video understanding, retrieval, and hallucination tasks. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-e3f881c5063c6bf876117029f1741742></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-e3f881c5063c6bf876117029f1741742",{strings:[" The paper identifies four key characteristics for effective video RoPE: 3D structure, frequency allocation, spatial symmetry, and temporal index scaling. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-769a17fe29edd0b940b1be34c30c9a40></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-769a17fe29edd0b940b1be34c30c9a40",{strings:[" VideoRoPE addresses the limitations of previous RoPE variants by mitigating periodic oscillations and enhancing robustness to distractors. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is important because it addresses a critical challenge in adapting rotary position embeddings (RoPE) for video data, which is crucial for the development of advanced video large language models. It proposes a novel method, VideoRoPE, that significantly outperforms existing methods on various benchmarks. This work provides a deeper understanding of the key properties required for effective video RoPE, and opens up new avenues for research into improved positional encoding techniques for other multi-modal applications.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.05173/x1.png alt></figure></p><blockquote><p>üîº This radar chart displays the performance of VideoRoPE and other RoPE variants across multiple video benchmarks: Long Video Bench, MLVU, VideoMME, V-NIAH, and VideoHalluciner. Each benchmark assesses different aspects of video understanding capabilities. VideoRoPE demonstrates superior performance compared to other methods, showcasing its effectiveness in various video tasks. The chart visually highlights VideoRoPE&rsquo;s consistent improvements across these diverse benchmarks.</p><details><summary>read the caption</summary>Figure 1: VideoRoPE outperforms RoPE variants on benchmarks.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S1.T1.2><tr class=ltx_tr id=S1.T1.2.1><td class="ltx_td ltx_border_tt" id=S1.T1.2.1.1 style=padding-left:2.8pt;padding-right:2.8pt></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S1.T1.2.1.2 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.1.2.1></span><span class=ltx_text id=S1.T1.2.1.2.2 style=font-size:50%> </span><span class=ltx_text id=S1.T1.2.1.2.3 style=font-size:50%><span class="ltx_tabular ltx_align_middle" id=S1.T1.2.1.2.3.1><span class=ltx_tr id=S1.T1.2.1.2.3.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S1.T1.2.1.2.3.1.1.1 style=padding-left:2.8pt;padding-right:2.8pt><span class="ltx_text ltx_font_bold" id=S1.T1.2.1.2.3.1.1.1.1>2D/3D</span></span></span>
<span class=ltx_tr id=S1.T1.2.1.2.3.1.2><span class="ltx_td ltx_nopad_r ltx_align_center" id=S1.T1.2.1.2.3.1.2.1 style=padding-left:2.8pt;padding-right:2.8pt><span class="ltx_text ltx_font_bold" id=S1.T1.2.1.2.3.1.2.1.1>Structure</span></span></span>
</span></span><span class=ltx_text id=S1.T1.2.1.2.4></span><span class=ltx_text id=S1.T1.2.1.2.5 style=font-size:50%></span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S1.T1.2.1.3 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.1.3.1></span><span class=ltx_text id=S1.T1.2.1.3.2 style=font-size:50%> </span><span class=ltx_text id=S1.T1.2.1.3.3 style=font-size:50%><span class="ltx_tabular ltx_align_middle" id=S1.T1.2.1.3.3.1><span class=ltx_tr id=S1.T1.2.1.3.3.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S1.T1.2.1.3.3.1.1.1 style=padding-left:2.8pt;padding-right:2.8pt><span class="ltx_text ltx_font_bold" id=S1.T1.2.1.3.3.1.1.1.1>Frequency</span></span></span>
<span class=ltx_tr id=S1.T1.2.1.3.3.1.2><span class="ltx_td ltx_nopad_r ltx_align_center" id=S1.T1.2.1.3.3.1.2.1 style=padding-left:2.8pt;padding-right:2.8pt><span class="ltx_text ltx_font_bold" id=S1.T1.2.1.3.3.1.2.1.1>Allocation</span></span></span>
</span></span><span class=ltx_text id=S1.T1.2.1.3.4></span><span class=ltx_text id=S1.T1.2.1.3.5 style=font-size:50%></span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S1.T1.2.1.4 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.1.4.1></span><span class=ltx_text id=S1.T1.2.1.4.2 style=font-size:50%> </span><span class=ltx_text id=S1.T1.2.1.4.3 style=font-size:50%><span class="ltx_tabular ltx_align_middle" id=S1.T1.2.1.4.3.1><span class=ltx_tr id=S1.T1.2.1.4.3.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S1.T1.2.1.4.3.1.1.1 style=padding-left:2.8pt;padding-right:2.8pt><span class="ltx_text ltx_font_bold" id=S1.T1.2.1.4.3.1.1.1.1>Spatial</span></span></span>
<span class=ltx_tr id=S1.T1.2.1.4.3.1.2><span class="ltx_td ltx_nopad_r ltx_align_center" id=S1.T1.2.1.4.3.1.2.1 style=padding-left:2.8pt;padding-right:2.8pt><span class="ltx_text ltx_font_bold" id=S1.T1.2.1.4.3.1.2.1.1>Symmetry</span></span></span>
</span></span><span class=ltx_text id=S1.T1.2.1.4.4></span><span class=ltx_text id=S1.T1.2.1.4.5 style=font-size:50%></span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S1.T1.2.1.5 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.1.5.1></span><span class=ltx_text id=S1.T1.2.1.5.2 style=font-size:50%> </span><span class=ltx_text id=S1.T1.2.1.5.3 style=font-size:50%><span class="ltx_tabular ltx_align_middle" id=S1.T1.2.1.5.3.1><span class=ltx_tr id=S1.T1.2.1.5.3.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S1.T1.2.1.5.3.1.1.1 style=padding-left:2.8pt;padding-right:2.8pt><span class="ltx_text ltx_font_bold" id=S1.T1.2.1.5.3.1.1.1.1>Temporal</span></span></span>
<span class=ltx_tr id=S1.T1.2.1.5.3.1.2><span class="ltx_td ltx_nopad_r ltx_align_center" id=S1.T1.2.1.5.3.1.2.1 style=padding-left:2.8pt;padding-right:2.8pt><span class="ltx_text ltx_font_bold" id=S1.T1.2.1.5.3.1.2.1.1>Index Scaling</span></span></span>
</span></span><span class=ltx_text id=S1.T1.2.1.5.4></span><span class=ltx_text id=S1.T1.2.1.5.5 style=font-size:50%></span></td></tr><tr class=ltx_tr id=S1.T1.2.2><td class="ltx_td ltx_align_left ltx_border_t" id=S1.T1.2.2.1 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.2.1.1></span><span class=ltx_text id=S1.T1.2.2.1.2 style=font-size:50%>
<span class="ltx_tabular ltx_align_middle" id=S1.T1.2.2.1.2.1><span class=ltx_tr id=S1.T1.2.2.1.2.1.1><span class="ltx_td ltx_nopad_r ltx_align_left" id=S1.T1.2.2.1.2.1.1.1 style=padding-left:2.8pt;padding-right:2.8pt>Vanilla RoPE <cite class="ltx_cite ltx_citemacro_citep">(Su et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib52 title>2024</a>)</cite></span></span>
</span></span><span class=ltx_text id=S1.T1.2.2.1.3></span><span class=ltx_text id=S1.T1.2.2.1.4 style=font-size:50%></span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S1.T1.2.2.2 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.2.2.1 style=font-size:50%;color:#ff8080>‚úó</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S1.T1.2.2.3 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.2.3.1 style=font-size:50%;color:#ff8080>‚úó</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S1.T1.2.2.4 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.2.4.1 style=font-size:50%;color:#ff8080>‚úó</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S1.T1.2.2.5 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.2.5.1 style=font-size:50%;color:#ff8080>‚úó</span></td></tr><tr class=ltx_tr id=S1.T1.2.3><td class="ltx_td ltx_align_left" id=S1.T1.2.3.1 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.3.1.1></span><span class=ltx_text id=S1.T1.2.3.1.2 style=font-size:50%>
<span class="ltx_tabular ltx_align_middle" id=S1.T1.2.3.1.2.1><span class=ltx_tr id=S1.T1.2.3.1.2.1.1><span class="ltx_td ltx_nopad_r ltx_align_left" id=S1.T1.2.3.1.2.1.1.1 style=padding-left:2.8pt;padding-right:2.8pt>TAD-RoPE <cite class="ltx_cite ltx_citemacro_citep">(Gao et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib16 title>2024</a>)</cite></span></span>
</span></span><span class=ltx_text id=S1.T1.2.3.1.3></span><span class=ltx_text id=S1.T1.2.3.1.4 style=font-size:50%></span></td><td class="ltx_td ltx_align_center" id=S1.T1.2.3.2 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.3.2.1 style=font-size:50%;color:#ff8080>‚úó</span></td><td class="ltx_td ltx_align_center" id=S1.T1.2.3.3 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.3.3.1 style=font-size:50%;color:#ff8080>‚úó</span></td><td class="ltx_td ltx_align_center" id=S1.T1.2.3.4 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.3.4.1 style=font-size:50%;color:#ff8080>‚úó</span></td><td class="ltx_td ltx_align_center" id=S1.T1.2.3.5 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.3.5.1 style=font-size:50%;color:#098842>‚úì</span></td></tr><tr class=ltx_tr id=S1.T1.2.4><td class="ltx_td ltx_align_left" id=S1.T1.2.4.1 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.4.1.1></span><span class=ltx_text id=S1.T1.2.4.1.2 style=font-size:50%>
<span class="ltx_tabular ltx_align_middle" id=S1.T1.2.4.1.2.1><span class=ltx_tr id=S1.T1.2.4.1.2.1.1><span class="ltx_td ltx_nopad_r ltx_align_left" id=S1.T1.2.4.1.2.1.1.1 style=padding-left:2.8pt;padding-right:2.8pt>RoPE-Tie <cite class="ltx_cite ltx_citemacro_citep">(Su, <a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib50 title>2024a</a>)</cite></span></span>
</span></span><span class=ltx_text id=S1.T1.2.4.1.3></span><span class=ltx_text id=S1.T1.2.4.1.4 style=font-size:50%></span></td><td class="ltx_td ltx_align_center" id=S1.T1.2.4.2 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.4.2.1 style=font-size:50%;color:#098842>‚úì</span></td><td class="ltx_td ltx_align_center" id=S1.T1.2.4.3 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.4.3.1 style=font-size:50%;color:#ff8080>‚úó</span></td><td class="ltx_td ltx_align_center" id=S1.T1.2.4.4 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.4.4.1 style=font-size:50%;color:#098842>‚úì</span></td><td class="ltx_td ltx_align_center" id=S1.T1.2.4.5 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.4.5.1 style=font-size:50%;color:#ff8080>‚úó</span></td></tr><tr class=ltx_tr id=S1.T1.2.5><td class="ltx_td ltx_align_left" id=S1.T1.2.5.1 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.5.1.1></span><span class=ltx_text id=S1.T1.2.5.1.2 style=font-size:50%>
<span class="ltx_tabular ltx_align_middle" id=S1.T1.2.5.1.2.1><span class=ltx_tr id=S1.T1.2.5.1.2.1.1><span class="ltx_td ltx_nopad_r ltx_align_left" id=S1.T1.2.5.1.2.1.1.1 style=padding-left:2.8pt;padding-right:2.8pt>M-RoPE <cite class="ltx_cite ltx_citemacro_citep">(Wang et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib60 title>2024b</a>)</cite></span></span>
</span></span><span class=ltx_text id=S1.T1.2.5.1.3></span><span class=ltx_text id=S1.T1.2.5.1.4 style=font-size:50%></span></td><td class="ltx_td ltx_align_center" id=S1.T1.2.5.2 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.5.2.1 style=font-size:50%;color:#098842>‚úì</span></td><td class="ltx_td ltx_align_center" id=S1.T1.2.5.3 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.5.3.1 style=font-size:50%;color:#ff8080>‚úó</span></td><td class="ltx_td ltx_align_center" id=S1.T1.2.5.4 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.5.4.1 style=font-size:50%;color:#ff8080>‚úó</span></td><td class="ltx_td ltx_align_center" id=S1.T1.2.5.5 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.5.5.1 style=font-size:50%;color:#ff8080>‚úó</span></td></tr><tr class=ltx_tr id=S1.T1.2.6 style=background-color:#f2f3f5><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S1.T1.2.6.1 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.6.1.1 style=font-size:50%;background-color:#f2f3f5>VideoRoPE (Ours)</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S1.T1.2.6.2 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.6.2.1 style=font-size:50%;color:#098842;background-color:#f2f3f5>‚úì</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S1.T1.2.6.3 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.6.3.1 style=font-size:50%;color:#098842;background-color:#f2f3f5>‚úì</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S1.T1.2.6.4 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.6.4.1 style=font-size:50%;color:#098842;background-color:#f2f3f5>‚úì</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S1.T1.2.6.5 style=padding-left:2.8pt;padding-right:2.8pt><span class=ltx_text id=S1.T1.2.6.5.1 style=font-size:50%;color:#098842;background-color:#f2f3f5>‚úì</span></td></tr></table></table></figure><blockquote><p>üîº This table compares several variants of Rotary Position Embedding (RoPE), a technique used in Transformer models to incorporate positional information, specifically focusing on their effectiveness for Video Large Language Models (Video LLMs). It highlights key differences across four aspects: the 2D/3D structure of the model (how it handles the spatial and temporal dimensions of video data), frequency allocation (how frequencies are assigned to different dimensions), spatial symmetry (whether the model maintains symmetry in its handling of spatial information), and temporal index scaling (how the model handles the different granularities of temporal and spatial information). Each RoPE variant is evaluated according to whether it satisfies these characteristics with a checkmark or an &lsquo;X&rsquo;. This allows for a comparison of different approaches and their suitability for processing video data.</p><details><summary>read the caption</summary>Table 1: Comparison between different RoPE variants for Video Large Language Models (Video LLMs).</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">RoPE&rsquo;s Video Limits<div id=ropes-video-limits class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#ropes-video-limits aria-label=Anchor>#</a></span></h4><p>The heading &ldquo;RoPE&rsquo;s Video Limits&rdquo; suggests an exploration of the challenges in applying Rotary Position Embeddings (RoPE), a successful technique in natural language processing, to the spatiotemporal domain of video data. <strong>The core limitation lies in RoPE&rsquo;s inherent 1D structure</strong>, which struggles to directly represent the 2D or 3D nature of video frames and their temporal evolution. Simply flattening video data into a 1D sequence loses crucial spatial and temporal relationships, hindering effective positional encoding. The paper likely delves into how existing attempts to adapt RoPE to video, such as through direct extensions or 3D adaptations, <strong>fall short of adequately capturing complex spatiotemporal relationships</strong>. This might involve a discussion of frequency allocation strategies, the impact of periodic oscillations, and the challenges of aligning temporal and spatial dimensions. <strong>A key insight likely uncovered is the need for a more sophisticated 3D positional encoding scheme</strong> that respects the unique characteristics of video. The authors probably propose a novel method that addresses these limitations by explicitly modeling spatiotemporal dependencies with improved frequency allocation or a new architectural approach to positional embedding for video.</p><h4 class="relative group">VideoRoPE Design<div id=videorope-design class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#videorope-design aria-label=Anchor>#</a></span></h4><p>The VideoRoPE design is a novel approach to rotary position embedding (RoPE) specifically tailored for video data. It addresses the limitations of previous methods by incorporating <strong>three key components</strong>: <strong>Low-frequency Temporal Allocation (LTA)</strong>, which mitigates periodic oscillations and improves robustness to distractors; a <strong>Diagonal Layout (DL)</strong>, which maintains spatial symmetry and ensures equal contextual influence from surrounding tokens; and <strong>Adjustable Temporal Spacing (ATS)</strong>, allowing for flexible scaling of temporal indices to better align with varying granularities of temporal and spatial information in video. These components are carefully integrated into a <strong>3D structure</strong> to preserve spatio-temporal relationships inherent in video, providing an effective and robust representation of positional information within video-language models. The design&rsquo;s effectiveness is demonstrated through superior performance across various benchmarks, including those involving long video understanding, retrieval, and hallucination tasks, showcasing its adaptability and superior ability to handle long-range dependencies within complex video data.</p><h4 class="relative group">V-NIAH-D Challenge<div id=v-niah-d-challenge class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#v-niah-d-challenge aria-label=Anchor>#</a></span></h4><p>The V-NIAH-D challenge, a novel extension of the V-NIAH benchmark, introduces a crucial test for video understanding models. By adding periodic distractors to the original V-NIAH dataset, <strong>V-NIAH-D highlights the susceptibility of models to misleading visual information</strong>, especially when temporal relationships are not properly handled. This clever augmentation effectively assesses a model&rsquo;s robustness in retrieving relevant information amidst noise, pushing beyond simple long-range visual understanding. <strong>The challenge reveals weaknesses in existing rotary position embedding (RoPE) variants</strong>, demonstrating that inadequate temporal dimension allocation leads to poor performance. Addressing the V-NIAH-D challenge necessitates a more sophisticated approach to spatio-temporal modeling, such as the proposed VideoRoPE architecture, which strategically allocates temporal and spatial dimensions to mitigate the impact of distractors and achieve superior results. <strong>Successfully tackling the V-NIAH-D challenge points to significant advancements in the field of video understanding</strong>, pushing the development of more robust and reliable models capable of filtering noise and prioritizing relevant information in complex, real-world scenarios.</p><h4 class="relative group">Empirical Results<div id=empirical-results class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#empirical-results aria-label=Anchor>#</a></span></h4><p>An Empirical Results section in a research paper would ideally present a comprehensive evaluation of the proposed method, demonstrating its effectiveness and comparing it to existing state-of-the-art techniques. This involves presenting key performance metrics across various datasets and experimental settings. <strong>A strong Empirical Results section will detail the experimental setup, including datasets, evaluation metrics, and baseline models used for comparison.</strong> In-depth analysis of the results should be provided, explaining any trends or unexpected outcomes. The presentation of results should be clear, concise, and visually engaging, often employing tables and figures to facilitate understanding. Ideally, <strong>the authors should also acknowledge limitations and potential biases of their experimental methodology</strong>. A robust Empirical Results section is critical for establishing the credibility and impact of the research findings; it&rsquo;s where the theoretical claims of the paper are put to the test and validated through rigorous empirical evidence.</p><h4 class="relative group">Future Work<div id=future-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-work aria-label=Anchor>#</a></span></h4><p>Future work in video rotary position embedding (RoPE) could explore several promising avenues. <strong>Extending VideoRoPE to handle even longer video sequences</strong> is crucial, potentially involving techniques like hierarchical attention or memory mechanisms to manage the increased computational complexity. <strong>Investigating different frequency allocation strategies</strong> beyond the low-frequency temporal allocation could yield further improvements, particularly in balancing spatial and temporal information processing. <strong>A deeper investigation into the interaction between RoPE and other components of video LLMs</strong>, such as attention mechanisms and normalization layers, is warranted to optimize overall model performance. <strong>Benchmarking VideoRoPE on a wider range of video understanding tasks</strong> is needed to fully assess its generalizability and effectiveness, potentially including tasks beyond those currently considered. Finally, <strong>exploring the application of VideoRoPE to other multi-modal tasks</strong>, such as video question answering and video generation, would broaden its impact and showcase its potential as a fundamental building block in advanced video AI systems.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.05173/x2.png alt></figure></p><blockquote><p>üîº Figure 2 demonstrates the effect of frequency allocation on video retrieval performance. The left side shows two examples, (a) V-NIAH (Visual Needle-in-a-Haystack) and (b) V-NIAH-D (Visual Needle-in-a-Haystack with Distractors). V-NIAH-D is a more challenging variant that introduces visually similar distractor images around the relevant ‚Äòneedle‚Äô image. The right side presents a comparison of retrieval accuracy between M-ROPE and VideoRoPE on both tasks. This comparison highlights VideoRoPE&rsquo;s improved robustness to distractors, suggesting the effectiveness of its frequency allocation strategy.</p><details><summary>read the caption</summary>Figure 2: Left: To demonstrate the importance of frequential allocation, based on VIAH (a) we present a more challenging V-NIAH-D task (b) that similar images are inserted as distractors. Right: Compared to M-RoPE, our VideoRoPE is more robust in retrieval and is less affected by distractors.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.05173/x3.png alt></figure></p><blockquote><p>üîº Figure 3 visualizes the attention weights of different RoPE methods for a video retrieval task. The top row shows the attention weights for M-ROPE, highlighting how its temporal dimension focuses on short-term relationships, resulting in a diagonal pattern. This limitation hinders the model&rsquo;s ability to capture long-range temporal dependencies, which are crucial for accurately identifying the target &rsquo;needle&rsquo; in a long video sequence. The bottom row displays the attention weights for VideoRoPE, demonstrating its improved ability to capture long-range temporal dependencies. The VideoRoPE&rsquo;s temporal dimension effectively focuses on the relevant temporal segments, allowing it to successfully retrieve the &rsquo;needle&rsquo;. The x and y axes represent the spatial coordinates (horizontal and vertical frame indices) in the video, and the color intensity of each cell represents the magnitude of the attention weight.</p><details><summary>read the caption</summary>Figure 3: Attention-based frequential allocation analysis. Middle: M-RoPE‚Äôs temporal dimension (tùë°titalic_t) is limited to local information, resulting in a diagonal layout. Bottom: VideoRoPE effectively retrieves the needle using the temporal dimension. The x and y coordinates represent the video frame number, e.g., 50 for 50 frames. For more details see Appendix E.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.05173/x4.png alt></figure></p><blockquote><p>üîº This figure compares the temporal frequency allocation strategies of M-ROPE and VideoRoPE. M-ROPE allocates higher frequencies (shorter monotonic intervals) to the temporal dimension, while VideoRoPE uses lower frequencies (wider monotonic intervals). This difference is visually represented using graphs that show how quickly the frequency changes across different dimensions. The y-axis shows the index of the dimension, and the x-axis shows the token index (or time). The difference in frequency allocation significantly impacts the robustness of the models to periodic oscillations and distractors, as discussed in the paper.</p><details><summary>read the caption</summary>(a) Temporal Frequency Allocation in M-RoPE</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.05173/x5.png alt></figure></p><blockquote><p>üîº This figure compares the frequency allocation strategies between M-ROPE and VideoRoPE for modeling temporal dependencies. M-ROPE uses higher frequencies for the temporal dimension resulting in pronounced oscillations and a diagonal layout in the attention patterns. In contrast, VideoRoPE allocates lower frequencies to the temporal dimension, resulting in wider, more monotonic intervals, mitigating the periodic oscillations and resulting in improved robustness against distractors in long video retrieval. The x-axis represents the token index, indicating the position within the sequence, while the y-axis represents the dimension index, showing the different frequency bands used for temporal encoding.</p><details><summary>read the caption</summary>(b) Temporal Frequency Allocation in VideoRoPE (ours)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.05173/x6.png alt></figure></p><blockquote><p>üîº Figure 4 illustrates the difference in temporal frequency allocation between M-ROPE and VideoRoPE. M-ROPE allocates higher frequencies (and thus, more pronounced oscillations) to the early dimensions, making it susceptible to interference from periodic distractors. In contrast, VideoRoPE uses lower frequencies for the temporal dimension, resulting in wider, monotonic intervals that are more robust against periodic distractions in the V-NIAH-D task. This is because the higher frequency of the temporal dimension in M-ROPE makes it more sensitive to noise and periodic patterns, whereas the lower frequency in VideoRoPE makes it more resilient to noise.</p><details><summary>read the caption</summary>Figure 4: (a) M-RoPE (Wang et¬†al., 2024b) models temporal dependencies using the first 16 rotary angles, which exhibit higher frequencies and more pronounced oscillations. (b) In contrast, VideoRoPE models temporal dependencies using the last 16 rotary angles, characterized by significantly wider, monotonic intervals. Our frequency allocation effectively mitigates the misleading influence of distractors in V-NIAH-D. For a more detailed analysis, please refer to Appendix F.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.05173/x7.png alt></figure></p><blockquote><p>üîº This figure compares the positional embeddings of adjacent text and visual tokens for three different methods: Vanilla RoPE, M-ROPE, and VideoRoPE. Vanilla RoPE, designed for 1D sequential data, flattens the video frames into a 1D sequence. M-ROPE uses a 3D structure but divides the dimensions into distinct groups for temporal and spatial features. VideoRoPE, in contrast, employs a 3D structure with an interleaved spatial and temporal layout, aiming to better represent the spatio-temporal relationships in video data. The visualization helps show how each method handles the positional encoding, highlighting differences in how they capture spatial and temporal context.</p><details><summary>read the caption</summary>Figure 5: The position embeddings of adjacent text tokens for Vanilla RoPE (top row), the corresponding visual tokens in adjacent frames for M-RoPE (middle row) and our VideoRoPE (bottom row) with interleaved spatial and temporal last design.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.05173/x8.png alt></figure></p><blockquote><p>üîº This 3D visualization illustrates the relative positional encoding scheme of Vanilla RoPE. It shows how the vanilla RoPE, designed for 1D sequential data, handles higher dimensional data by flattening it into a single dimension. The plot lacks the explicit representation of spatiotemporal relationships inherent in 3D data. This visualization helps to illustrate the limitations of applying a 1D positional encoding method to video data, which possesses an inherent 3D structure (temporal, height, width).</p><details><summary>read the caption</summary>(a) 3D visualization for Vanilla RoPE.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.05173/x9.png alt></figure></p><blockquote><p>üîº This 3D visualization shows how M-RoPE (Multi-dimensional Rotary Position Embedding) allocates dimensions for representing temporal, horizontal, and vertical information in video data. It highlights that M-RoPE, while employing a 3D structure, introduces inconsistencies in the index growth pattern for visual tokens across frames. Some indices remain constant, leading to an imbalance in representing spatial and temporal relationships.</p><details><summary>read the caption</summary>(b) 3D visualization for M-RoPE.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.05173/x10.png alt></figure></p><blockquote><p>üîº This 3D visualization showcases VideoRoPE&rsquo;s unique approach to positional encoding in video data. Unlike previous methods that flatten video frames into a 1D sequence or exhibit inconsistencies in index growth across dimensions, VideoRoPE maintains a consistent index growth pattern while simultaneously incorporating spatial modeling. The visualization clearly illustrates the consistent progression of indices across the temporal (t), horizontal (x), and vertical (y) axes, highlighting the balance achieved between spatial and temporal information.</p><details><summary>read the caption</summary>(c) 3D visualization for VideoRoPE.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.05173/x11.png alt></figure></p><blockquote><p>üîº Figure 6 visualizes the positional encoding of three different methods: Vanilla RoPE, M-ROPE, and VideoRoPE. Vanilla RoPE, being a 1D method, lacks spatial information, resulting in a linear progression along the time axis (t). M-ROPE attempts 3D modeling, but introduces inconsistencies in the index growth for visual tokens across frames; some indices remain static, breaking the consistent progression. In contrast, VideoRoPE maintains a consistent index growth across all dimensions (t, x, y) while effectively incorporating spatial relationships within its 3D structure. This balanced approach is visualized as a smooth, consistent progression in the figure.</p><details><summary>read the caption</summary>Figure 6: The 3D visualization for different position embedding. (a) The vanilla 1D RoPE¬†(Su et¬†al., 2024) does not incorporate spatial modeling. (b) M-RoPE¬†(Wang et¬†al., 2024b), while have the 3D structure, introduces a discrepancy in index growth for visual tokens across frames, with some indices remaining constant. (c) In contrast, our VideoRoPE achieves the desired balance, maintaining the consistent index growth pattern of vanilla RoPE while simultaneously incorporating spatial modeling.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.05173/x12.png alt></figure></p><blockquote><p>üîº Figure 7 visualizes the performance of different RoPE methods on the V-NIAH and V-NIAH-D datasets. The plots show how well each method can locate the &rsquo;needle&rsquo; (target frame) within the &lsquo;haystack&rsquo; (video sequence), particularly in the presence of distractors (V-NIAH-D). The color gradient shifting from green to red signifies the performance, ranging from perfect retrieval (green) to complete failure (red). The x-axis shows the length of the haystack, demonstrating the models&rsquo; ability to perform long-range retrieval. The y-axis represents the number of frames examined. The four subplots show the results for Vanilla RoPE, TAD-ROPE, M-ROPE, and VideoRoPE, respectively. The figure highlights how VideoRoPE significantly outperforms the others in accurately locating the target frame even with increasing haystack length and distractors.</p><details><summary>read the caption</summary>Figure 7: Visualization of the retrieval results for V-NIAH and V-NIAH-D. The color gradient from green to red represents the progression of needle retrieval performance, from perfect to zero.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T2.8><tr class=ltx_tr id=S5.T2.8.1><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T2.8.1.1 rowspan=2 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.1.1.1 style=font-size:80%>Method</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=4 id=S5.T2.8.1.2 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.1.2.1 style=font-size:80%>LongVideoBench</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=4 id=S5.T2.8.1.3 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.1.3.1 style=font-size:80%>MLVU</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=4 id=S5.T2.8.1.4 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.1.4.1 style=font-size:80%>Video-MME</span></td></tr><tr class=ltx_tr id=S5.T2.8.2><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.2.1 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.2.1.1 style=font-size:80%>8k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.2.2 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.2.2.1 style=font-size:80%>16k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.2.3 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.2.3.1 style=font-size:80%>32k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.2.4 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.2.4.1 style=font-size:80%>64k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.2.5 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.2.5.1 style=font-size:80%>8k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.2.6 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.2.6.1 style=font-size:80%>16k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.2.7 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.2.7.1 style=font-size:80%>32k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.2.8 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.2.8.1 style=font-size:80%>64k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.2.9 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.2.9.1 style=font-size:80%>8k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.2.10 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.2.10.1 style=font-size:80%>16k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.2.11 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.2.11.1 style=font-size:80%>32k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.2.12 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.2.12.1 style=font-size:80%>64k</span></td></tr><tr class=ltx_tr id=S5.T2.8.3><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.8.3.1 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.3.1.1 style=font-size:80%>Vanilla RoPE </span><cite class="ltx_cite ltx_citemacro_citep"><span class=ltx_text id=S5.T2.8.3.1.2.1 style=font-size:80%>(</span>Su et¬†al.<span class=ltx_text id=S5.T2.8.3.1.3.2.1.1 style=font-size:80%>, </span><a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib52 title>2024</a><span class=ltx_text id=S5.T2.8.3.1.4.3 style=font-size:80%>)</span></cite></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.3.2 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.3.2.1 style=font-size:80%>54.97</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.3.3 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.3.3.1 style=font-size:80%>54.87</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.3.4 style="padding:.4pt 5pt"><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T2.8.3.4.1 style=font-size:80%>54.56</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.3.5 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.3.5.1 style=font-size:80%>54.04</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.3.6 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.3.6.1 style=font-size:80%>63.31</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.3.7 style="padding:.4pt 5pt"><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T2.8.3.7.1 style=font-size:80%>65.79</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.3.8 style="padding:.4pt 5pt"><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T2.8.3.8.1 style=font-size:80%>65.93</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.3.9 style="padding:.4pt 5pt"><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T2.8.3.9.1 style=font-size:80%>62.02</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.3.10 style="padding:.4pt 5pt"><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T2.8.3.10.1 style=font-size:80%>60.67</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.3.11 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.3.11.1 style=font-size:80%>60.00</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.3.12 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.3.12.1 style=font-size:80%>61.33</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.8.3.13 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.3.13.1 style=font-size:80%>58.33</span></td></tr><tr class=ltx_tr id=S5.T2.8.4><td class="ltx_td ltx_align_center" id=S5.T2.8.4.1 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.4.1.1 style=font-size:80%>TAD-RoPE </span><cite class="ltx_cite ltx_citemacro_citep"><span class=ltx_text id=S5.T2.8.4.1.2.1 style=font-size:80%>(</span>Gao et¬†al.<span class=ltx_text id=S5.T2.8.4.1.3.2.1.1 style=font-size:80%>, </span><a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib16 title>2024</a><span class=ltx_text id=S5.T2.8.4.1.4.3 style=font-size:80%>)</span></cite></td><td class="ltx_td ltx_align_left" id=S5.T2.8.4.2 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.4.2.1 style=font-size:80%>54.14</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.4.3 style="padding:.4pt 5pt"><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T2.8.4.3.1 style=font-size:80%>55.08</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.4.4 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.4.4.1 style=font-size:80%>53.94</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.4.5 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.4.5.1 style=font-size:80%>53.42</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.4.6 style="padding:.4pt 5pt"><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T2.8.4.6.1 style=font-size:80%>63.67</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.4.7 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.4.7.1 style=font-size:80%>65.28</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.4.8 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.4.8.1 style=font-size:80%>65.28</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.4.9 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.4.9.1 style=font-size:80%>60.73</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.4.10 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.4.10.1 style=font-size:80%>60.33</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.4.11 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.4.11.1 style=font-size:80%>61.33</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.4.12 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.4.12.1 style=font-size:80%>62.00</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.4.13 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.4.13.1 style=font-size:80%>58.67</span></td></tr><tr class=ltx_tr id=S5.T2.8.5><td class="ltx_td ltx_align_center" id=S5.T2.8.5.1 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.5.1.1 style=font-size:80%>M-RoPE </span><cite class="ltx_cite ltx_citemacro_citep"><span class=ltx_text id=S5.T2.8.5.1.2.1 style=font-size:80%>(</span>Wang et¬†al.<span class=ltx_text id=S5.T2.8.5.1.3.2.1.1 style=font-size:80%>, </span><a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib60 title>2024b</a><span class=ltx_text id=S5.T2.8.5.1.4.3 style=font-size:80%>)</span></cite></td><td class="ltx_td ltx_align_left" id=S5.T2.8.5.2 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.5.2.1 style=font-size:80%>53.42</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.5.3 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.5.3.1 style=font-size:80%>52.80</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.5.4 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.5.4.1 style=font-size:80%>53.11</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.5.5 style="padding:.4pt 5pt"><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T2.8.5.5.1 style=font-size:80%>54.35</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.5.6 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.5.6.1 style=font-size:80%>60.41</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.5.7 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.5.7.1 style=font-size:80%>60.68</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.5.8 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.5.8.1 style=font-size:80%>61.56</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.5.9 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.5.9.1 style=font-size:80%>61.10</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.5.10 style="padding:.4pt 5pt"><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T2.8.5.10.1 style=font-size:80%>60.67</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.5.11 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.5.11.1 style=font-size:80%>59.67</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.5.12 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.5.12.1 style=font-size:80%>61.00</span></td><td class="ltx_td ltx_align_left" id=S5.T2.8.5.13 style="padding:.4pt 5pt"><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T2.8.5.13.1 style=font-size:80%>59.67</span></td></tr><tr class=ltx_tr id=S5.T2.8.6 style=background-color:#f2f3f5><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T2.8.6.1 style="padding:.4pt 5pt"><span class=ltx_text id=S5.T2.8.6.1.1 style=font-size:80%;background-color:#f2f3f5>VideoRoPE (Ours)</span></td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S5.T2.8.6.2 style="padding:.4pt 5pt"><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T2.8.6.2.1 style=font-size:80%;background-color:#f2f3f5>54.46</span></td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S5.T2.8.6.3 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.6.3.1 style=font-size:80%;background-color:#f2f3f5>55.29</span></td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S5.T2.8.6.4 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.6.4.1 style=font-size:80%;background-color:#f2f3f5>57.15</span></td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S5.T2.8.6.5 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.6.5.1 style=font-size:80%;background-color:#f2f3f5>57.26</span></td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S5.T2.8.6.6 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.6.6.1 style=font-size:80%;background-color:#f2f3f5>65.19</span></td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S5.T2.8.6.7 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.6.7.1 style=font-size:80%;background-color:#f2f3f5>66.29</span></td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S5.T2.8.6.8 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.6.8.1 style=font-size:80%;background-color:#f2f3f5>66.02</span></td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S5.T2.8.6.9 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.6.9.1 style=font-size:80%;background-color:#f2f3f5>65.56</span></td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S5.T2.8.6.10 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.6.10.1 style=font-size:80%;background-color:#f2f3f5>61.33</span></td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S5.T2.8.6.11 style="padding:.4pt 5pt"><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T2.8.6.11.1 style=font-size:80%;background-color:#f2f3f5>61.00</span></td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S5.T2.8.6.12 style="padding:.4pt 5pt"><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T2.8.6.12.1 style=font-size:80%;background-color:#f2f3f5>61.67</span></td><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S5.T2.8.6.13 style="padding:.4pt 5pt"><span class="ltx_text ltx_font_bold" id=S5.T2.8.6.13.1 style=font-size:80%;background-color:#f2f3f5>61.33</span></td></tr></table></table></figure><blockquote><p>üîº This table compares the performance of different rotary position embedding (RoPE) methods on three video understanding benchmarks: LongVideoBench, MLVU, and Video-MME. The benchmarks test performance with varying context lengths (8k, 16k, 32k, and 64k tokens), where 8k represents the context length used during training, and the others represent longer contexts unseen during training. The results show that VideoRoPE consistently outperforms other RoPE variants across all three benchmarks and context lengths. Bold values indicate the best performance, while underlined values indicate second-best performance.</p><details><summary>read the caption</summary>Table 2: Comparison of different RoPE methods on LongVidionBench, MLVU, and Video-MME. The benchmarks evaluate performance across three context lengths: 8k, 16k, 32k, and 64k, where 8k represents context within the training range, and others represent context outside the training range. Our VideoRoPE outperforms other RoPE variants across all three benchmarks. The best results are marked in bold, and the second-best results are underlined. For more information on the evaluation, see Appendix B.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T3.8><tr class=ltx_tr id=S5.T3.8.1><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.8.1.1 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.8.1.1.1 style=font-size:80%>Method</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.8.1.2 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.8.1.2.1 style=font-size:80%>V-NIAH Acc.</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.8.1.3 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.8.1.3.1 style=font-size:80%>V-NIAH-D Acc.</span></td></tr><tr class=ltx_tr id=S5.T3.8.2><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.8.2.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.8.2.1.1 style=font-size:80%>Vanilla RoPE¬†</span><cite class="ltx_cite ltx_citemacro_citep"><span class=ltx_text id=S5.T3.8.2.1.2.1 style=font-size:80%>(</span>Su et¬†al.<span class=ltx_text id=S5.T3.8.2.1.3.2.1.1 style=font-size:80%>, </span><a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib52 title>2024</a><span class=ltx_text id=S5.T3.8.2.1.4.3 style=font-size:80%>)</span></cite></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.8.2.2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.8.2.2.1 style=font-size:80%>31.78</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.8.2.3 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.8.2.3.1 style=font-size:80%>30.22</span></td></tr><tr class=ltx_tr id=S5.T3.8.3><td class="ltx_td ltx_align_center" id=S5.T3.8.3.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.8.3.1.1 style=font-size:80%>TAD-RoPE¬†</span><cite class="ltx_cite ltx_citemacro_citep"><span class=ltx_text id=S5.T3.8.3.1.2.1 style=font-size:80%>(</span>Gao et¬†al.<span class=ltx_text id=S5.T3.8.3.1.3.2.1.1 style=font-size:80%>, </span><a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib16 title>2024</a><span class=ltx_text id=S5.T3.8.3.1.4.3 style=font-size:80%>)</span></cite></td><td class="ltx_td ltx_align_center" id=S5.T3.8.3.2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.8.3.2.1 style=font-size:80%>29.33</span></td><td class="ltx_td ltx_align_center" id=S5.T3.8.3.3 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.8.3.3.1 style=font-size:80%>29.56</span></td></tr><tr class=ltx_tr id=S5.T3.8.4><td class="ltx_td ltx_align_center" id=S5.T3.8.4.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.8.4.1.1 style=font-size:80%>M-RoPE¬†</span><cite class="ltx_cite ltx_citemacro_citep"><span class=ltx_text id=S5.T3.8.4.1.2.1 style=font-size:80%>(</span>Wang et¬†al.<span class=ltx_text id=S5.T3.8.4.1.3.2.1.1 style=font-size:80%>, </span><a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib60 title>2024b</a><span class=ltx_text id=S5.T3.8.4.1.4.3 style=font-size:80%>)</span></cite></td><td class="ltx_td ltx_align_center" id=S5.T3.8.4.2 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.8.4.2.1 style=font-size:80%>78.67</span></td><td class="ltx_td ltx_align_center" id=S5.T3.8.4.3 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.8.4.3.1 style=font-size:80%>74.67</span></td></tr><tr class=ltx_tr id=S5.T3.8.5 style=background-color:#f2f3f5><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T3.8.5.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.8.5.1.1 style=font-size:80%;background-color:#f2f3f5>VideoRoPE</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T3.8.5.2 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.8.5.2.1 style=font-size:80%;background-color:#f2f3f5>91.11</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T3.8.5.3 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.8.5.3.1 style=font-size:80%;background-color:#f2f3f5>87.11</span></td></tr></table></table></figure><blockquote><p>üîº This table presents a performance comparison of various Rotary Position Embedding (RoPE) methods on two video retrieval tasks: V-NIAH (Visual Needle-In-A-Haystack) and V-NIAH-D (V-NIAH with Distractors). V-NIAH-D is a more challenging version of V-NIAH that includes distractor images to test the robustness of the RoPE methods. The table shows the average accuracy (&lsquo;Acc.&rsquo;) achieved by each RoPE method across different haystack lengths (number of frames in the video) and frame depths. Higher accuracy indicates better performance in retrieving the target &rsquo;needle&rsquo; image from the haystack, even in the presence of distractors.</p><details><summary>read the caption</summary>Table 3: Performance comparison of different RoPEs on V-NIAH and V-NIAH-D. ‚ÄúAcc.‚Äù refers to the average accuracy across haystack length and frame depth.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T4.7><tr class=ltx_tr id=S5.T4.7.1><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.7.1.1 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T4.7.1.1.1 style=font-size:80%>Method</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.7.1.2 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T4.7.1.2.1 style=font-size:80%>OR</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.7.1.3 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T4.7.1.3.1 style=font-size:80%>T</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.7.1.4 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T4.7.1.4.1 style=font-size:80%>SD</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.7.1.5 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T4.7.1.5.1 style=font-size:80%>F</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.7.1.6 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T4.7.1.6.1 style=font-size:80%>NF</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.7.1.7 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T4.7.1.7.1 style=font-size:80%>Avg.</span></td></tr><tr class=ltx_tr id=S5.T4.7.2><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.7.2.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T4.7.2.1.1 style=font-size:80%>Vanilla RoPE¬†</span><cite class="ltx_cite ltx_citemacro_citep"><span class=ltx_text id=S5.T4.7.2.1.2.1 style=font-size:80%>(</span>Su et¬†al.<span class=ltx_text id=S5.T4.7.2.1.3.2.1.1 style=font-size:80%>, </span><a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib52 title>2024</a><span class=ltx_text id=S5.T4.7.2.1.4.3 style=font-size:80%>)</span></cite></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.7.2.2 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.7.2.2.1 style=font-size:80%>51.5</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.7.2.3 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T4.7.2.3.1 style=font-size:80%>30.0</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.7.2.4 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.7.2.4.1 style=font-size:80%>48.0</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.7.2.5 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T4.7.2.5.1 style=font-size:80%>8.0</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.7.2.6 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T4.7.2.6.1 style=font-size:80%>43.0</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.7.2.7 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T4.7.2.7.1 style=font-size:80%>36.1</span></td></tr><tr class=ltx_tr id=S5.T4.7.3><td class="ltx_td ltx_align_center" id=S5.T4.7.3.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T4.7.3.1.1 style=font-size:80%>TAD-RoPE¬†</span><cite class="ltx_cite ltx_citemacro_citep"><span class=ltx_text id=S5.T4.7.3.1.2.1 style=font-size:80%>(</span>Gao et¬†al.<span class=ltx_text id=S5.T4.7.3.1.3.2.1.1 style=font-size:80%>, </span><a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib16 title>2024</a><span class=ltx_text id=S5.T4.7.3.1.4.3 style=font-size:80%>)</span></cite></td><td class="ltx_td ltx_align_center" id=S5.T4.7.3.2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T4.7.3.2.1 style=font-size:80%>51.0</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.3.3 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.7.3.3.1 style=font-size:80%>37.0</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.3.4 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.7.3.4.1 style=font-size:80%>48.0</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.3.5 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T4.7.3.5.1 style=font-size:80%>11.5</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.3.6 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.7.3.6.1 style=font-size:80%>47.5</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.3.7 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T4.7.3.7.1 style=font-size:80%>39.0</span></td></tr><tr class=ltx_tr id=S5.T4.7.4><td class="ltx_td ltx_align_center" id=S5.T4.7.4.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T4.7.4.1.1 style=font-size:80%>M-RoPE¬†</span><cite class="ltx_cite ltx_citemacro_citep"><span class=ltx_text id=S5.T4.7.4.1.2.1 style=font-size:80%>(</span>Wang et¬†al.<span class=ltx_text id=S5.T4.7.4.1.3.2.1.1 style=font-size:80%>, </span><a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib60 title>2024b</a><span class=ltx_text id=S5.T4.7.4.1.4.3 style=font-size:80%>)</span></cite></td><td class="ltx_td ltx_align_center" id=S5.T4.7.4.2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T4.7.4.2.1 style=font-size:80%>39.0</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.4.3 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T4.7.4.3.1 style=font-size:80%>29.0</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.4.4 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T4.7.4.4.1 style=font-size:80%>43.5</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.4.5 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.7.4.5.1 style=font-size:80%>12.5</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.4.6 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.7.4.6.1 style=font-size:80%>47.5</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.4.7 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T4.7.4.7.1 style=font-size:80%>34.3</span></td></tr><tr class=ltx_tr id=S5.T4.7.5 style=background-color:#f2f3f5><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T4.7.5.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T4.7.5.1.1 style=font-size:80%;background-color:#f2f3f5>VideoRoPE</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T4.7.5.2 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T4.7.5.2.1 style=font-size:80%;background-color:#f2f3f5>57.0</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T4.7.5.3 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T4.7.5.3.1 style=font-size:80%;background-color:#f2f3f5>58.5</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T4.7.5.4 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T4.7.5.4.1 style=font-size:80%;background-color:#f2f3f5>50.5</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T4.7.5.5 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T4.7.5.5.1 style=font-size:80%;background-color:#f2f3f5>15.0</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T4.7.5.6 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T4.7.5.6.1 style=font-size:80%;background-color:#f2f3f5>50.0</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T4.7.5.7 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T4.7.5.7.1 style=font-size:80%;background-color:#f2f3f5>46.2</span></td></tr></table></table></figure><blockquote><p>üîº This table presents a performance comparison of four different Rotary Position Embedding (RoPE) methods on the VideoHallucer benchmark. VideoHallucer is a video hallucination benchmark that evaluates the model&rsquo;s ability to correctly answer questions about video content, including both basic and hallucinated questions. The comparison is done across four different context lengths (8k, 16k, 32k, and 64k tokens) to assess the models&rsquo; performance with varying amounts of contextual information. For each RoPE method, the table displays the maximum performance achieved across the four context lengths. The best performance is indicated in bold, and the second-best is underlined. The table also breaks down the results by five sub-categories of the VideoHallucer benchmark: Object-Relation (OR), Temporal (T), Semantic Detail (SD), Factual (F), and Non-Factual (NF), giving a more granular view of each method&rsquo;s strengths and weaknesses.</p><details><summary>read the caption</summary>Table 4: Performance comparison of different RoPEs on VideoHallucer, evaluated at context lengths of 8k, 16k, 32k, and 64k. The maximum result for each RoPE variant across these context lengths is displayed, with bold for the top result and underlined for the second-highest. ‚ÄòOR‚Äô = Object-Relation, ‚ÄòT‚Äô = Temporal, ‚ÄòSD‚Äô = Semantic Detail, ‚ÄòF‚Äô = Factual, ‚ÄòNF‚Äô = Non-factual.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T5.5><tr class=ltx_tr id=S5.T5.5.1><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T5.5.1.1 rowspan=2 style=padding-left:3pt;padding-right:3pt><span class="ltx_text ltx_font_bold" id=S5.T5.5.1.1.1 style=font-size:70%>Method</span></td><td class="ltx_td ltx_align_center ltx_border_t" colspan=4 id=S5.T5.5.1.2 style=padding-left:3pt;padding-right:3pt><span class="ltx_text ltx_font_bold" id=S5.T5.5.1.2.1 style=font-size:70%>LongVideoBench</span></td><td class="ltx_td ltx_align_center ltx_border_t" colspan=4 id=S5.T5.5.1.3 style=padding-left:3pt;padding-right:3pt><span class="ltx_text ltx_font_bold" id=S5.T5.5.1.3.1 style=font-size:70%>MLVU</span></td></tr><tr class=ltx_tr id=S5.T5.5.2><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.2.1 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.2.1.1 style=font-size:70%>8k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.2.2 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.2.2.1 style=font-size:70%>16k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.2.3 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.2.3.1 style=font-size:70%>32k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.2.4 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.2.4.1 style=font-size:70%>64k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.2.5 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.2.5.1 style=font-size:70%>8k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.2.6 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.2.6.1 style=font-size:70%>16k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.2.7 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.2.7.1 style=font-size:70%>32k</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.2.8 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.2.8.1 style=font-size:70%>64k</span></td></tr><tr class=ltx_tr id=S5.T5.5.3><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T5.5.3.1 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.3.1.1 style=font-size:70%>Baseline</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.3.2 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.3.2.1 style=font-size:70%>53.42</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.3.3 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.3.3.1 style=font-size:70%>52.80</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.3.4 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.3.4.1 style=font-size:70%>53.11</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.3.5 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.3.5.1 style=font-size:70%>54.35</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.3.6 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.3.6.1 style=font-size:70%>60.41</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.3.7 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.3.7.1 style=font-size:70%>60.68</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.3.8 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.3.8.1 style=font-size:70%>61.56</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T5.5.3.9 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.3.9.1 style=font-size:70%>61.10</span></td></tr><tr class=ltx_tr id=S5.T5.5.4><td class="ltx_td ltx_align_center" id=S5.T5.5.4.1 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.4.1.1 style=font-size:70%>+ DL</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.4.2 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.4.2.1 style=font-size:70%>52.17</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.4.3 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.4.3.1 style=font-size:70%>52.07</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.4.4 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.4.4.1 style=font-size:70%>53.31</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.4.5 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.4.5.1 style=font-size:70%>53.63</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.4.6 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.4.6.1 style=font-size:70%>62.06</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.4.7 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.4.7.1 style=font-size:70%>63.03</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.4.8 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.4.8.1 style=font-size:70%>62.52</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.4.9 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.4.9.1 style=font-size:70%>62.75</span></td></tr><tr class=ltx_tr id=S5.T5.5.5><td class="ltx_td ltx_align_center" id=S5.T5.5.5.1 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.5.1.1 style=font-size:70%>+ DL & LTA</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.5.2 style=padding-left:3pt;padding-right:3pt><span class="ltx_text ltx_font_bold" id=S5.T5.5.5.2.1 style=font-size:70%>54.46</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.5.3 style=padding-left:3pt;padding-right:3pt><span class="ltx_text ltx_font_bold" id=S5.T5.5.5.3.1 style=font-size:70%>55.49</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.5.4 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.5.4.1 style=font-size:70%>54.66</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.5.5 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.5.5.1 style=font-size:70%>55.60</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.5.6 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.5.6.1 style=font-size:70%>63.35</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.5.7 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.5.7.1 style=font-size:70%>64.09</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.5.8 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.5.8.1 style=font-size:70%>64.00</span></td><td class="ltx_td ltx_align_left" id=S5.T5.5.5.9 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.5.9.1 style=font-size:70%>63.26</span></td></tr><tr class=ltx_tr id=S5.T5.5.6><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T5.5.6.1 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.6.1.1 style=font-size:70%>+ DL & LTA & ATS</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=S5.T5.5.6.2 style=padding-left:3pt;padding-right:3pt><span class="ltx_text ltx_font_bold" id=S5.T5.5.6.2.1 style=font-size:70%>54.46</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=S5.T5.5.6.3 style=padding-left:3pt;padding-right:3pt><span class=ltx_text id=S5.T5.5.6.3.1 style=font-size:70%>55.29</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=S5.T5.5.6.4 style=padding-left:3pt;padding-right:3pt><span class="ltx_text ltx_font_bold" id=S5.T5.5.6.4.1 style=font-size:70%>57.15</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=S5.T5.5.6.5 style=padding-left:3pt;padding-right:3pt><span class="ltx_text ltx_font_bold" id=S5.T5.5.6.5.1 style=font-size:70%>57.26</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=S5.T5.5.6.6 style=padding-left:3pt;padding-right:3pt><span class="ltx_text ltx_font_bold" id=S5.T5.5.6.6.1 style=font-size:70%>65.19</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=S5.T5.5.6.7 style=padding-left:3pt;padding-right:3pt><span class="ltx_text ltx_font_bold" id=S5.T5.5.6.7.1 style=font-size:70%>66.29</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=S5.T5.5.6.8 style=padding-left:3pt;padding-right:3pt><span class="ltx_text ltx_font_bold" id=S5.T5.5.6.8.1 style=font-size:70%>66.02</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=S5.T5.5.6.9 style=padding-left:3pt;padding-right:3pt><span class="ltx_text ltx_font_bold" id=S5.T5.5.6.9.1 style=font-size:70%>65.56</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the results of an ablation study that investigates the impact of different components of the proposed VideoRoPE model on its performance. It systematically evaluates the contribution of the Diagonal Layout (DL), Low-frequency Temporal Allocation (LTA), and Adjustable Temporal Spacing (ATS) modules. The study assesses performance on two key benchmarks: Long VideoBench and MLVU, using various context lengths (8k, 16k, 32k, and 64k) to understand the impact of context window size.</p><details><summary>read the caption</summary>Table 5: Ablation study about different modules of VideoRoPE.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=A1.T6.3><tr class=ltx_tr id=A1.T6.3.1><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T6.3.1.1 rowspan=2 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=A1.T6.3.1.1.1>Scaling Factor<math alttext="\bm{\delta}" class="ltx_Math" display="inline" id="A1.T6.3.1.1.1.1.m1.1"><semantics id="A1.T6.3.1.1.1.1.m1.1a"><mi id="A1.T6.3.1.1.1.1.m1.1.1" xref="A1.T6.3.1.1.1.1.m1.1.1.cmml">Œ¥</mi><annotation-xml encoding="MathML-Content" id="A1.T6.3.1.1.1.1.m1.1b"><ci id="A1.T6.3.1.1.1.1.m1.1.1.cmml" xref="A1.T6.3.1.1.1.1.m1.1.1">ùõø</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.3.1.1.1.1.m1.1c">\bm{\delta}</annotation><annotation encoding="application/x-llamapun" id="A1.T6.3.1.1.1.1.m1.1d">bold_italic_Œ¥</annotation></semantics></math></span></td><td class="ltx_td ltx_align_center ltx_border_t" colspan=4 id=A1.T6.3.1.2 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=A1.T6.3.1.2.1>LongVideoBench</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T6.3.1.3 rowspan=2 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=A1.T6.3.1.3.1>Average</span></td></tr><tr class=ltx_tr id=A1.T6.3.2><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T6.3.2.1 style=padding-left:5pt;padding-right:5pt>8k</td><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T6.3.2.2 style=padding-left:5pt;padding-right:5pt>16k</td><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T6.3.2.3 style=padding-left:5pt;padding-right:5pt>32k</td><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T6.3.2.4 style=padding-left:5pt;padding-right:5pt>64k</td></tr><tr class=ltx_tr id=A1.T6.3.3><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T6.3.3.1 style=padding-left:5pt;padding-right:5pt>0.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T6.3.3.2 style=padding-left:5pt;padding-right:5pt>51.92</td><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T6.3.3.3 style=padding-left:5pt;padding-right:5pt>53.52</td><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T6.3.3.4 style=padding-left:5pt;padding-right:5pt>52.80</td><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T6.3.3.5 style=padding-left:5pt;padding-right:5pt>52.07</td><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T6.3.3.6 style=padding-left:5pt;padding-right:5pt>52.57</td></tr><tr class=ltx_tr id=A1.T6.3.4><td class="ltx_td ltx_align_center" id=A1.T6.3.4.1 style=padding-left:5pt;padding-right:5pt>1.0</td><td class="ltx_td ltx_align_center" id=A1.T6.3.4.2 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=A1.T6.3.4.2.1>54.46</span></td><td class="ltx_td ltx_align_center" id=A1.T6.3.4.3 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=A1.T6.3.4.3.1>55.49</span></td><td class="ltx_td ltx_align_center" id=A1.T6.3.4.4 style=padding-left:5pt;padding-right:5pt>54.66</td><td class="ltx_td ltx_align_center" id=A1.T6.3.4.5 style=padding-left:5pt;padding-right:5pt>55.60</td><td class="ltx_td ltx_align_center" id=A1.T6.3.4.6 style=padding-left:5pt;padding-right:5pt>55.05</td></tr><tr class=ltx_tr id=A1.T6.3.5><td class="ltx_td ltx_align_center" id=A1.T6.3.5.1 style=padding-left:5pt;padding-right:5pt>1.5</td><td class="ltx_td ltx_align_center" id=A1.T6.3.5.2 style=padding-left:5pt;padding-right:5pt>54.35</td><td class="ltx_td ltx_align_center" id=A1.T6.3.5.3 style=padding-left:5pt;padding-right:5pt>55.00</td><td class="ltx_td ltx_align_center" id=A1.T6.3.5.4 style=padding-left:5pt;padding-right:5pt>55.31</td><td class="ltx_td ltx_align_center" id=A1.T6.3.5.5 style=padding-left:5pt;padding-right:5pt>55.91</td><td class="ltx_td ltx_align_center" id=A1.T6.3.5.6 style=padding-left:5pt;padding-right:5pt>55.14</td></tr><tr class=ltx_tr id=A1.T6.3.6 style=background-color:#f2f3f5><td class="ltx_td ltx_align_center" id=A1.T6.3.6.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=A1.T6.3.6.1.1 style=background-color:#f2f3f5>2.0</span></td><td class="ltx_td ltx_align_center" id=A1.T6.3.6.2 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=A1.T6.3.6.2.1 style=background-color:#f2f3f5>54.46</span></td><td class="ltx_td ltx_align_center" id=A1.T6.3.6.3 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=A1.T6.3.6.3.1 style=background-color:#f2f3f5>55.29</span></td><td class="ltx_td ltx_align_center" id=A1.T6.3.6.4 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=A1.T6.3.6.4.1 style=background-color:#f2f3f5>57.15</span></td><td class="ltx_td ltx_align_center" id=A1.T6.3.6.5 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=A1.T6.3.6.5.1 style=background-color:#f2f3f5>57.26</span></td><td class="ltx_td ltx_align_center" id=A1.T6.3.6.6 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=A1.T6.3.6.6.1 style=background-color:#f2f3f5>56.04</span></td></tr><tr class=ltx_tr id=A1.T6.3.7><td class="ltx_td ltx_align_center" id=A1.T6.3.7.1 style=padding-left:5pt;padding-right:5pt>2.5</td><td class="ltx_td ltx_align_center" id=A1.T6.3.7.2 style=padding-left:5pt;padding-right:5pt>53.42</td><td class="ltx_td ltx_align_center" id=A1.T6.3.7.3 style=padding-left:5pt;padding-right:5pt>53.73</td><td class="ltx_td ltx_align_center" id=A1.T6.3.7.4 style=padding-left:5pt;padding-right:5pt>54.25</td><td class="ltx_td ltx_align_center" id=A1.T6.3.7.5 style=padding-left:5pt;padding-right:5pt>55.08</td><td class="ltx_td ltx_align_center" id=A1.T6.3.7.6 style=padding-left:5pt;padding-right:5pt>54.12</td></tr><tr class=ltx_tr id=A1.T6.3.8><td class="ltx_td ltx_align_center ltx_border_b" id=A1.T6.3.8.1 style=padding-left:5pt;padding-right:5pt>3.0</td><td class="ltx_td ltx_align_center ltx_border_b" id=A1.T6.3.8.2 style=padding-left:5pt;padding-right:5pt>53.63</td><td class="ltx_td ltx_align_center ltx_border_b" id=A1.T6.3.8.3 style=padding-left:5pt;padding-right:5pt>53.63</td><td class="ltx_td ltx_align_center ltx_border_b" id=A1.T6.3.8.4 style=padding-left:5pt;padding-right:5pt>53.63</td><td class="ltx_td ltx_align_center ltx_border_b" id=A1.T6.3.8.5 style=padding-left:5pt;padding-right:5pt>55.18</td><td class="ltx_td ltx_align_center ltx_border_b" id=A1.T6.3.8.6 style=padding-left:5pt;padding-right:5pt>54.01</td></tr></table></table></figure><blockquote><p>üîº This table presents the results of an ablation study investigating the impact of different scaling factors (Œ¥) on the performance of the VideoRoPE model. The study focuses on the LongVideoBench benchmark, evaluating performance across four different context lengths (8k, 16k, 32k, 64k) for each scaling factor tested. The average performance across all context lengths is also reported. The purpose is to determine the optimal scaling factor that best balances the temporal alignment between text and video tokens, thereby maximizing the model&rsquo;s ability to understand long videos.</p><details><summary>read the caption</summary>Table 6: Ablation Study About Different Scaling Factor Œ¥ùõø\deltaitalic_Œ¥.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=A1.T7.17><tr class=ltx_tr id=A1.T7.17.1><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T7.17.1.1 rowspan=2 style="padding:.5pt 5pt"><span class="ltx_text ltx_font_bold" id=A1.T7.17.1.1.1>Method</span></td><td class="ltx_td ltx_align_center ltx_border_t" colspan=4 id=A1.T7.17.1.2 style="padding:.5pt 5pt"><span class="ltx_text ltx_font_bold" id=A1.T7.17.1.2.1>LongVideoBench</span></td><td class="ltx_td ltx_align_center ltx_border_t" colspan=4 id=A1.T7.17.1.3 style="padding:.5pt 5pt"><span class="ltx_text ltx_font_bold" id=A1.T7.17.1.3.1>MLVU</span></td></tr><tr class=ltx_tr id=A1.T7.17.2><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.2.1 style="padding:.5pt 5pt">8k</td><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.2.2 style="padding:.5pt 5pt">16k</td><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.2.3 style="padding:.5pt 5pt">32k</td><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.2.4 style="padding:.5pt 5pt">64k</td><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.2.5 style="padding:.5pt 5pt">8k</td><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.2.6 style="padding:.5pt 5pt">16k</td><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.2.7 style="padding:.5pt 5pt">32k</td><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.2.8 style="padding:.5pt 5pt">64k</td></tr><tr class=ltx_tr id=A1.T7.17.3><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T7.17.3.1 style="padding:.5pt 5pt">VideoRoPE(Sequential)</td><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.3.2 style="padding:.5pt 5pt">53.73</td><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.3.3 style="padding:.5pt 5pt">53.52</td><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.3.4 style="padding:.5pt 5pt">54.97</td><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.3.5 style="padding:.5pt 5pt">54.77</td><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.3.6 style="padding:.5pt 5pt">62.75</td><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.3.7 style="padding:.5pt 5pt">63.31</td><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.3.8 style="padding:.5pt 5pt">62.75</td><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T7.17.3.9 style="padding:.5pt 5pt">63.08</td></tr><tr class=ltx_tr id=A1.T7.17.4 style=background-color:#f2f3f5><td class="ltx_td ltx_align_center ltx_border_b" id=A1.T7.17.4.1 style="padding:.5pt 5pt"><span class=ltx_text id=A1.T7.17.4.1.1 style=background-color:#f2f3f5>VideoRoPE (Interleaved)</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=A1.T7.17.4.2 style="padding:.5pt 5pt"><span class="ltx_text ltx_font_bold" id=A1.T7.17.4.2.1 style=background-color:#f2f3f5>54.46</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=A1.T7.17.4.3 style="padding:.5pt 5pt"><span class="ltx_text ltx_font_bold" id=A1.T7.17.4.3.1 style=background-color:#f2f3f5>55.29</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=A1.T7.17.4.4 style="padding:.5pt 5pt"><span class="ltx_text ltx_font_bold" id=A1.T7.17.4.4.1 style=background-color:#f2f3f5>57.15</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=A1.T7.17.4.5 style="padding:.5pt 5pt"><span class="ltx_text ltx_font_bold" id=A1.T7.17.4.5.1 style=background-color:#f2f3f5>57.26</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=A1.T7.17.4.6 style="padding:.5pt 5pt"><span class="ltx_text ltx_font_bold" id=A1.T7.17.4.6.1 style=background-color:#f2f3f5>65.19</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=A1.T7.17.4.7 style="padding:.5pt 5pt"><span class="ltx_text ltx_font_bold" id=A1.T7.17.4.7.1 style=background-color:#f2f3f5>66.29</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=A1.T7.17.4.8 style="padding:.5pt 5pt"><span class="ltx_text ltx_font_bold" id=A1.T7.17.4.8.1 style=background-color:#f2f3f5>66.02</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=A1.T7.17.4.9 style="padding:.5pt 5pt"><span class="ltx_text ltx_font_bold" id=A1.T7.17.4.9.1 style=background-color:#f2f3f5>65.56</span></td></tr></table></table></figure><blockquote><p>üîº This table presents an ablation study comparing two different strategies for allocating the horizontal (x) and vertical (y) dimensions in the VideoRoPE positional encoding scheme. The &lsquo;Sequential&rsquo; allocation method assigns all x dimensions consecutively followed by all y dimensions, mirroring the approach used in M-ROPE (Wang et al., 2024b). In contrast, the &lsquo;Interleaved&rsquo; method alternates between x and y dimensions, which is similar to the approach used in Agrawal et al. (2024). The table shows the performance of both methods on the Long VideoBench and MLVU benchmarks using 8k, 16k, 32k, and 64k context lengths to evaluate the impact of the different allocation strategies on model performance.</p><details><summary>read the caption</summary>Table 7: Ablation Study on xùë•xitalic_x, yùë¶yitalic_y Allocation. VideoRoPE (Sequential) represents the sequential allocation of xùë•xitalic_x and yùë¶yitalic_y, following the pattern x,x,x,‚Ä¶,y,y,y,‚Ä¶ùë•ùë•ùë•‚Ä¶ùë¶ùë¶ùë¶‚Ä¶x,x,x,\dots,y,y,y,\dotsitalic_x , italic_x , italic_x , ‚Ä¶ , italic_y , italic_y , italic_y , ‚Ä¶ (similar to M-RoPE¬†(Wang et¬†al., 2024b)). VideoRoPE (Interleaved) represents the interleaved allocation, following the pattern x,y,x,y,‚Ä¶ùë•ùë¶ùë•ùë¶‚Ä¶x,y,x,y,\dotsitalic_x , italic_y , italic_x , italic_y , ‚Ä¶ (similar to Agrawal et¬†al. (2024)).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=A1.T8.4><tr class=ltx_tr id=A1.T8.4.1><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T8.4.1.1 rowspan=2 style="padding:.5pt 5pt"><span class="ltx_text ltx_font_bold" id=A1.T8.4.1.1.1>Method</span></td><td class="ltx_td ltx_align_center ltx_border_t" colspan=2 id=A1.T8.4.1.2 style="padding:.5pt 5pt"><span class="ltx_text ltx_font_bold" id=A1.T8.4.1.2.1>LongVideoBench</span></td></tr><tr class=ltx_tr id=A1.T8.4.2><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T8.4.2.1 style="padding:.5pt 5pt">64k</td><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T8.4.2.2 style="padding:.5pt 5pt">128k</td></tr><tr class=ltx_tr id=A1.T8.4.3><td class="ltx_td ltx_align_left ltx_border_t" id=A1.T8.4.3.1 style="padding:.5pt 5pt">Vanilla RoPE¬†<cite class="ltx_cite ltx_citemacro_citep">(Su et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib52 title>2024</a>)</cite></td><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T8.4.3.2 style="padding:.5pt 5pt">54.04</td><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T8.4.3.3 style="padding:.5pt 5pt">48.01</td></tr><tr class=ltx_tr id=A1.T8.4.4><td class="ltx_td ltx_align_left" id=A1.T8.4.4.1 style="padding:.5pt 5pt">TAD-RoPE¬†<cite class="ltx_cite ltx_citemacro_citep">(Gao et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib16 title>2024</a>)</cite></td><td class="ltx_td ltx_align_center" id=A1.T8.4.4.2 style="padding:.5pt 5pt">53.42</td><td class="ltx_td ltx_align_center" id=A1.T8.4.4.3 style="padding:.5pt 5pt">45.77</td></tr><tr class=ltx_tr id=A1.T8.4.5><td class="ltx_td ltx_align_left" id=A1.T8.4.5.1 style="padding:.5pt 5pt">M-RoPE¬†<cite class="ltx_cite ltx_citemacro_citep">(Wang et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.05173v1#bib.bib60 title>2024b</a>)</cite></td><td class="ltx_td ltx_align_center" id=A1.T8.4.5.2 style="padding:.5pt 5pt"><span class="ltx_text ltx_framed ltx_framed_underline" id=A1.T8.4.5.2.1>54.35</span></td><td class="ltx_td ltx_align_center" id=A1.T8.4.5.3 style="padding:.5pt 5pt"><span class="ltx_text ltx_framed ltx_framed_underline" id=A1.T8.4.5.3.1>51.45</span></td></tr><tr class=ltx_tr id=A1.T8.4.6 style=background-color:#f2f3f5><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=A1.T8.4.6.1 style="padding:.5pt 5pt"><span class=ltx_text id=A1.T8.4.6.1.1 style=background-color:#f2f3f5>VideoRoPE</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=A1.T8.4.6.2 style="padding:.5pt 5pt"><span class="ltx_text ltx_font_bold" id=A1.T8.4.6.2.1 style=background-color:#f2f3f5>57.26</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=A1.T8.4.6.3 style="padding:.5pt 5pt"><span class="ltx_text ltx_font_bold" id=A1.T8.4.6.3.1 style=background-color:#f2f3f5>55.64</span></td></tr></table></table></figure><blockquote><p>üîº This table presents a comparison of the performance of four different video rotary position embedding (RoPE) methods‚ÄîVanilla RoPE, TAD-ROPE, M-ROPE, and VideoRoPE‚Äîwhen evaluated on the LongVideoBench benchmark dataset. The comparison is made using two different context lengths: 64k and 128k tokens, allowing for an assessment of how each method scales to extremely long sequences. The results showcase the accuracy (as measured by the LongVideoBench metric) obtained by each method at both context lengths, enabling an analysis of their relative performance and robustness when processing very long video sequences.</p><details><summary>read the caption</summary>Table 8: Comparison of model performance at 64k and 128k context lengths for different methods.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-8e5bbd97f9dc5a44aa34b132d89beebe class=gallery><img src=https://ai-paper-reviewer.com/2502.05173/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.05173/19.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05173/&amp;title=VideoRoPE:%20What%20Makes%20for%20Good%20Video%20Rotary%20Position%20Embedding?" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05173/&amp;text=VideoRoPE:%20What%20Makes%20for%20Good%20Video%20Rotary%20Position%20Embedding?" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05173/&amp;subject=VideoRoPE:%20What%20Makes%20for%20Good%20Video%20Rotary%20Position%20Embedding?" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2502.05173/index.md",oid_likes="likes_paper-reviews/2502.05173/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2502.04403/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Agency Is Frame-Dependent</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-06T00:00:00+00:00>6 February 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2502.05171/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-07T00:00:00+00:00>7 February 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>