{"references": [{"fullname_first_author": "Su, J.", "paper_title": "RoFormer: Enhanced transformer with rotary position embedding", "publication_date": "2024", "reason": "This paper introduces Rotary Position Embedding (RoPE), a core concept extended and analyzed in the current research."}, {"fullname_first_author": "Wang, P.", "paper_title": "Qwen2-VL: Enhancing vision-language model's perception of the world at any resolution", "publication_date": "2024", "reason": "This paper introduces M-ROPE, a significant prior work that VideoRoPE improves upon."}, {"fullname_first_author": "Gao, M.", "paper_title": "TC-LLaVA: Rethinking the transfer from image to video understanding with temporal considerations", "publication_date": "2024", "reason": "This paper proposes TAD-ROPE, another important previous approach for video position embedding that VideoRoPE builds upon."}, {"fullname_first_author": "Zhang, Z.", "paper_title": "A comprehensive benchmark for multi-task long video understanding", "publication_date": "2024", "reason": "This paper introduces V-NIAH, a key benchmark dataset used for evaluating video understanding models, which is extended in this paper."}, {"fullname_first_author": "Zhang, Z.", "paper_title": "Vision Needle-in-a-Haystack", "publication_date": "2024", "reason": "This paper introduces the V-NIAH benchmark dataset, which is further extended and used in this work"}]}