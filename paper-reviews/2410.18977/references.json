{"references": [{" publication_date": "2018", "fullname_first_author": "Hyemin Ahn", "paper_title": "Text2action: Generative adversarial synthesis from language to action", "reason": "This paper is highly relevant as it's one of the earliest works on text-driven human motion generation, making it foundational to the field and providing valuable context for understanding the evolution of techniques and challenges that MotionCLR aims to address.  Understanding its approach is crucial to appreciate the advancements offered by MotionCLR.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Xiao Lin", "paper_title": "Human motion modeling using dvgans", "reason": "This is a seminal paper in using generative adversarial networks (GANs) for human motion modeling.  It's an important baseline to compare the performance and capabilities of the newer attention-based diffusion model, MotionCLR, illustrating progress in model architecture and results.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Kfir Aberman", "paper_title": "Skeleton-aware networks for deep motion retargeting", "reason": "This paper significantly impacts the understanding of motion editing because it provides a context of early techniques in motion editing.  It directly relates to the problem of MotionCLR, illustrating the evolution of techniques from traditional methods to more recent methods based on deep learning and attention mechanisms.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Kfir Aberman", "paper_title": "Unpaired motion style transfer from video to animation", "reason": "This work is highly relevant because it focuses on motion style transfer, a critical problem addressed by MotionCLR. It's essential to understand the previous approaches and limitations in the style transfer area to better appreciate the advantages offered by MotionCLR's attention-based approach.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Chaitanya Ahuja", "paper_title": "Language2pose: Natural language grounded pose forecasting", "reason": "This paper tackles the challenging task of language-based motion generation.  It's relevant as MotionCLR addresses similar challenges, but with a focus on using attention mechanisms for more explainable and controllable editing, moving beyond simple pose prediction.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Nikos Athanasiou", "paper_title": "Sinc: Spatial composition of 3d human motions for simultaneous action generation", "reason": "This work shows sophisticated applications of motion generation and editing, providing additional context for the goals of MotionCLR.  Understanding its approaches, particularly the composition of 3D human motions, is essential for evaluating the strengths and novelty of MotionCLR.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Guy Tevet", "paper_title": "Human motion diffusion model", "reason": "This highly influential work introduces diffusion models to human motion generation, forming the basis for several subsequent works in the area. Comparing MotionCLR's approach to this foundational work highlights the advancement of using attention mechanisms for improved explainability and editability.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Mathis Petrovich", "paper_title": "Temos: Generating diverse human motions from textual descriptions", "reason": "This is one of the most relevant papers because it tackles the same problem as MotionCLR: using text to drive the generation of diverse motions. It's important to analyze the differences in their approaches (transformer-based vs. attention-based diffusion model) to highlight MotionCLR's advantages.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Rishabh Dabral", "paper_title": "Mofusion: A framework for denoising-diffusion-based motion synthesis", "reason": "This paper is significant as it presents a diffusion-based approach to motion synthesis, providing a strong comparison to MotionCLR's methods. Analyzing the differences in model architecture and the resulting performance helps in understanding the contributions of MotionCLR.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Chuan Guo", "paper_title": "Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts", "reason": "This paper's relevance stems from its focus on the reciprocal generation of human motions and texts, a challenging task also addressed by MotionCLR. By contrasting their approaches, we highlight the innovative aspects of using attention mechanisms for improved controllability and explainability in MotionCLR.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Jianrong Zhang", "paper_title": "Motiondiffuse: Text-driven human motion generation with diffusion model", "reason": "This highly relevant work presents a diffusion model for text-driven motion generation that is directly comparable to MotionCLR's approach.  Comparing their performance and capabilities, especially regarding explainability and fine-grained editing control, is essential in evaluating the advantages of MotionCLR.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Mingdeng Cao", "paper_title": "Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing", "reason": "This paper provides valuable insights into the manipulation of attention mechanisms for image editing, providing inspiration for and a comparison point for MotionCLR's attention-based approach to motion editing.  It's relevant for understanding techniques of attention manipulation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Hila Chefer", "paper_title": "Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers", "reason": "This paper is essential because it tackles the problem of attention mechanism explainability, a direct motivation for MotionCLR's development. Understanding the context of existing methods for explainability is crucial in appreciating the contribution of MotionCLR's approach.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This is a landmark paper in the field of deep learning, introducing the transformer architecture which underpins many modern natural language processing (NLP) models. It's crucial because MotionCLR leverages the self and cross-attention mechanisms from this foundation.", "section_number": 3}, {" publication_date": "2015", "fullname_first_author": "Olaf Ronneberger", "paper_title": "U-net: Convolutional networks for biomedical image segmentation", "reason": "This paper introduces the U-Net architecture, which MotionCLR adapts for motion generation.  The U-Net's effectiveness in image segmentation shows its utility for handling sequential data, illustrating a relevant architectural choice in MotionCLR.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Shunlin Lu", "paper_title": "Humantomato: Text-aligned whole-body motion generation", "reason": "This paper is directly comparable to MotionCLR because it uses text to drive the generation of whole-body motions.  Analyzing the differences between their methods clarifies the contribution and advantages of MotionCLR.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Jiaman Li", "paper_title": "Object motion guided human motion synthesis", "reason": "This paper focuses on motion generation guided by object motion, offering a closely related research direction to MotionCLR.  Understanding the impact of additional constraints on human motion generation is crucial to appreciate MotionCLR\u2019s approach.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Weiyu Li", "paper_title": "Motion texture: a two-level statistical model for character motion synthesis", "reason": "This paper uses motion texture for motion synthesis, providing a useful comparison to MotionCLR's approach.  Analyzing the strengths and weaknesses of different methods clarifies the contributions of MotionCLR, particularly in handling diverse actions and achieving natural results.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Chuan Guo", "paper_title": "Momask: Generative masked modeling of 3d human motions", "reason": "This paper is highly relevant as it uses masked modeling to enhance motion generation, offering a similar concept to MotionCLR's masking approach for improving generation.  Comparing these methods clarifies the novelty of MotionCLR.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Nikos Athanasiou", "paper_title": "MotionFix: Text-driven 3d human motion editing", "reason": "This paper directly addresses human motion editing, making it directly comparable to MotionCLR.  It is crucial to understand the state-of-the-art before MotionCLR in terms of text-driven motion editing to appreciate the advancements offered by MotionCLR's attention-based approach.", "section_number": 5}]}