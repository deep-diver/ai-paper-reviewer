[{"figure_path": "https://arxiv.org/html/2412.04424/x1.png", "caption": "Figure 1: Comparison of LLaVA-style MLLMs with our Florence-VL. LLaVA-style models use CLIP, pretrained with contrastive learning, to generate a single high-level image feature. In contrast, Florence-VL leverages Florence-2, pretrained with generative modeling across various vision tasks such as image captioning, OCR, and grounding. This enables Florence-VL to flexibly extract multiple task-specific image features using Florence-2 as the image encoder.", "description": "This figure compares the image encoding methods used in LLaVA-style Multimodal Large Language Models (MLLMs) and the proposed Florence-VL model.  LLaVA-style models rely on CLIP, a contrastive learning model, to produce a single, high-level image representation.  In contrast, Florence-VL utilizes Florence-2, a generative model, trained on diverse visual tasks (image captioning, OCR, grounding).  This allows Florence-VL to extract multiple, task-specific image features tailored to the downstream task, offering greater flexibility and potentially improved performance.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.04424/x2.png", "caption": "Figure 2: An overview of Florence-VL, which extracts visual features of different depths (levels of feature concepts) and breaths (prompts) from Florence-2, combines them using DBFusion, and project the fused features to an LLM\u2019s input space. Florence-VL is fully pretrained on image captioning data and then partially finetuned on instruction-tuning data.", "description": "Figure 2 illustrates the architecture of Florence-VL, a multimodal large language model.  It begins by using Florence-2, a generative vision model, to extract visual features.  Crucially, Florence-2 extracts features at multiple \"depths\" (different levels of abstraction, from low-level details to high-level concepts) and \"breadths\" (using various prompts to capture different aspects of the image, such as detailed captions, OCR text, and object grounding). These diverse visual features are then fused using a novel Depth-Breadth Fusion (DBFusion) mechanism.  The fused features are finally projected into the input space of a large language model (LLM), allowing for effective multimodal understanding.  The entire model is first fully pre-trained on image captioning data before undergoing a partial fine-tuning phase using instruction-tuning data.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.04424/x3.png", "caption": "Figure 3: Visualization of the first three PCA components: we apply PCA to image features generated from Detailed Caption, OCR, and Grounding prompts, excluding the background by setting a threshold on the first PCA component. The image features derived from the Detailed Caption prompt (second column) capture the general context of the image, those from the OCR prompt (third column) focus primarily on text information, and those from the Grounding prompt (fourth column) highlight spatial relationships between objects. Additionally, we visualize the final layer features from OpenAI CLIP (ViT-L/14@336) in the last column, showing that CLIP features often miss certain region-level details, such as text information in many cases.", "description": "This figure visualizes the effectiveness of Florence-2 in capturing various levels of visual information compared to the CLIP model.  PCA was applied to image features generated by Florence-2 using three different prompts: Detailed Caption (focuses on overall scene understanding), OCR (focuses on text extraction), and Grounding (highlights spatial relationships between objects).  The results show that Florence-2, unlike CLIP, effectively captures fine-grained details such as text within the image. The visualization clearly demonstrates that Florence-2's multi-faceted visual representations offer a richer, more nuanced understanding of the image than CLIP's single high-level representation.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.04424/x4.png", "caption": "Figure 4: We plot the alignment loss for different vision encoders, which clearly shows that Florence-2 vision encoder achieves the lowest alignment loss compared to the other vision encoders, demonstrating the best alignment with text embeddings.", "description": "This figure presents a comparison of the alignment loss between various vision encoders and a language model. The alignment loss is a measure of how well the visual representations from the encoder align with the textual representations from the language model.  Lower alignment loss indicates better alignment. The results show that Florence-2 achieves the lowest alignment loss, indicating the strongest alignment between its visual features and text embeddings compared to other vision encoders like Stable Diffusion, DINOv2, SigLIP, and OpenAI CLIP.", "section": "Analysis on Different Vision Encoders"}, {"figure_path": "https://arxiv.org/html/2412.04424/x5.png", "caption": "Figure 5: We plot the alignment loss for various feature combinations, removing one feature at a time from different depths and breadths. The results clearly show that our method achieves the lowest alignment loss compared to others, highlighting the importance of all features from different depths and breadths for optimal alignment.", "description": "Figure 5 shows the results of an ablation study on the impact of different visual features on the alignment loss between vision and text representations in a multimodal large language model.  Four sets of visual features (using different combinations of depth and breadth of features) are compared: the complete set of features, and the sets that remove either the detailed caption features, OCR features, or grounding features. The graph clearly demonstrates that the combination of all features results in the lowest alignment loss, underscoring the importance of combining features from both different depths (levels of detail) and breadths (various tasks) for optimal cross-modal alignment and model performance.", "section": "Analysis on Different Vision Encoders"}]