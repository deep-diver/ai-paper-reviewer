[{"content": "|                     | # Vis tok | MMBench (EN) | POPE | MM-Vet | MME-P | Seed-image | HallusionBench | LLaVA-bench | AI2D | MathVista | MMMU | OCRBench | ChartQA | DocVQA | InfoVQA | Average |\n| :------------------ | :-------- | :----------- | :---- | :------ | :----- | :---------- | :------------- | :---------- | :---- | :-------- | :---- | :-------- | :------ | :----- | :------ | :------- |\n| Token Integration   | 1728      | **66.6**      | 88.7  | 34.1     | 1536.3 | **70.9**     | 45.0           | 63.3        | 56.9  | **28.1**     | **36.4** | 40.8      | 23.0    | 44.6    | **29.5** | 50.3    |\n| Average Pooling    | 576       | 65.7         | 88.8  | 32.3     | **1551.3** | 70.3         | 45.7           | 64.6        | 56.6  | 27.4       | 36.0   | 41.2      | **24.6** | **44.8** | 29.3    | 50.4    |\n| Channel Integration | 576       | 66.1         | **89.4** | **35.2** | 1543.5 | 70.3         | **46.8**      | **65.0**     | **57.2** | 28.0       | 35.6   | **41.4**    | 24.3    | 44.5    | 29.4    | **50.8**   |", "caption": "Table 1: Experiments for different fusion strategies. The vision token count is 1728 for token integration, which leads to longer training and inference times. The channel integration strategy shows better performance and training efficiency compared to the other two fusion methods.", "description": "This table presents a comparison of three different strategies for integrating visual features in a multimodal large language model (MLLM): Token Integration, Average Pooling, and Channel Integration.  Token Integration concatenates all visual features along the token dimension, leading to a larger number of tokens, increased training time, and slower inference. Average Pooling averages all features, potentially resulting in information loss. Channel Integration concatenates features along the channel dimension, providing an efficient balance of information retention and processing speed. The results show that the Channel Integration method achieves the best performance and training efficiency.", "section": "3. Method"}, {"content": "|               | # Vis tok. | VQAv2 | GQA  | MMBench (EN) | MMBench (CN) | VizWiz | POPE | MM-Vet | MME-P | MME-C | Seed-image | HallusionBench | LLaVA-bench | MMStar |\n| :------------ | :---------: | :----: | :----: | :-----------: | :-----------: | :-----: | :----: | :-----: | :----: | :----: | :---------: | :-------------: | :----------: | :-----: |\n| Vila 3B       |     -      |  80.4  |  61.5  |    63.4     |    52.7      |  53.5   | 86.9  |  35.4   | 1442.4 |   -    |    67.9     |       -        |      -       |  40.3  |\n| Phi 3.5-Vision|     -      |    -   |  **63.5** |   **75.5**   |   **64.2**   |  58.2   | 82.2  |  46.5   | 1473.4 | **412.1** |    69.9     |       53.3       |     68.8      |  **49.0** |\n| Florence-VL 3B (ours) |   576    | **82.1** |  61.8  |    71.6     |    60.8      | **59.1** | **88.3** | **51.0** | **1498.7** |  403.9  |   **70.6**   |       58.1       |     **71.1**    |  44.9  |\n| LLaVA next 8B |   2880    |    -   | **65.4** |     -       |     -        |  57.7   | 86.6  |  41.7   | 1595.1 |  379.3  |    72.7     |       47.7       |    **76.8**    |    -   |\n| Vila 8B       |     -      |  80.9  |  61.7  |    72.3     |    66.2      |  58.7   | 84.4  |  38.3   | 1577.0 |    -    |    71.4     |       -        |      -       |    -   |\n| Mini-Gemini-HD 8B|   2880    |    -   |  64.5  |     -       |     -        |    -   |    -   |    -   | **1606.0** |    -    |    73.2     |       -        |      -       |    -   |\n| Cambrain 8B   |   576    |    -   |  64.6  |    75.9     |    67.9      |    -   | 87.4  |  48.0   | 1547.1 |    -    |    74.7     |       48.7       |     71.0      | **50.0** |\n| Florence-VL 8B (ours) |   576    | **84.7** |  64.4  | **76.2** | **69.5** | **59.1** | **89.9** | **56.3** | 1560.0 | **381.1** | **74.9** | **57.3** | 74.2 | **50.0** |", "caption": "(a) Results on general multimodal benchmarks.", "description": "This table presents the performance comparison of different vision-language models on a range of general multimodal benchmarks.  The benchmarks assess various aspects of visual understanding and reasoning capabilities. The models are evaluated based on their accuracy across the benchmarks, providing a comprehensive overview of their strengths and weaknesses.  The table includes metrics that quantify the models' performance on different tasks.", "section": "5. Experiments"}, {"content": "|                     | # Vis tok. | Realworldqa | CV-Bench* | MMVP | AI2D | MathVista | MMMU | SciQA-IMG | TextVQA | OCRBench | ChartQA | DocVQA | InfoVQA |\n| :------------------ | :--------- | :---------- | :-------- | :---- | :---- | :-------- | :---- | :-------- | :------ | :------- | :------ | :----- | :------ |\n| Vila 3B             | -          | 53.3        | 55.2      | -     | -     | 30.6      | 34.1  | 67.9      | 58.1    | -        | -       | -      | -       |\n| Phi 3.5 Vision     | -          | 53.5        | 69.3      | **67.7** | **77.4** | -          | **43.3** | **89.0**  | 61.1    | 59.8     | **72.0** | 75.9   | 40.7    |\n| Florence-VL 3B (ours) | 576        | **60.4**    | **70.2**  | 64.7  | 73.8  | 52.2      | 41.8  | 84.6      | **69.1** | **63.0** | 70.7    | **82.1** | **51.3** |\n| LLaVA next 8B      | 2880       | 59.6        | 63.8      | 38.7  | 71.6  | 37.4      | 40.1  | 73.3      | 65.4    | 55.2     | 69.3    | 78.2   | -       |\n| Vila 8B             | -          | -           | -         | -     | -     | -          | 36.9  | 79.9      | -       | -        | -       | -      | -       |\n| Mini-Gemini-HD 8B   | 2880       | 62.1        | 62.6      | 18.7  | 73.5  | 37.0      | 37.3  | 75.1      | 70.2    | 47.7     | 59.1    | 74.6   | -       |\n| Cambrian 8B         | 576        | **64.2**    | 72.2      | 51.3  | 73.0  | 49.0      | 42.7  | 80.4      | 71.7    | 62.4     | 73.3    | 77.8   | -       |\n| Florence-VL 8B (ours) | 576        | **64.2**    | **73.4**  | **73.3** | **74.2** | **55.5** | **43.7** | **85.9**  | **74.2** | **63.4** | **74.7** | **84.9** | **51.7** |", "caption": "(b) Results on Vision centric, Knowledge based, and OCR & Chart benchmarks.", "description": "Table 2b presents the performance comparison of various Multimodal Large Language Models (MLLMs) across a range of vision-centric, knowledge-based, and OCR & Chart tasks.  It shows the results for different models, including Florence-VL (with both 3B and 8B parameter variants), along with several baselines and other state-of-the-art models.  The table details the performance of each model on multiple benchmarks within each category, offering a comprehensive evaluation of their capabilities in diverse multimodal tasks.  This is particularly useful for assessing the specific strengths and weaknesses of each model in different domains.", "section": "5. Experiments"}, {"content": "|               | LLM       | GQA  | MMBench (EN) | MMBench (CN) | VizWiz | POPE  | MM-Vet | MME-P     | MME-C    | HallusionBench | LLaVA-bench | MMStar |\n| :------------ | :-------- | :--- | :------------ | :------------ | :----- | :---- | :----- | :-------- | :------- | :------------- | :---------- | :------ |\n| LLaVA 1.5 3B | Phi 3.5    | 61.4 | **69.4**      | 60.6         | 38.4   | 86.2  | 35.4   | 1399.5    | 284.6   | 44.5          | **68.0**     | 40.6    |\n| Florence-VL 3B | Phi 3.5    | **62.7** | 68.7         | **61.7**      | **42.6** | **89.9** | 35.4   | **1448.5** | **299.6** | **45.5**     | 64.9       | **40.8** |\n| LLaVA 1.5 7B | Vicuna 1.5 | 62.0 | 64.8         | **57.6**      | 50.0   | 85.9  | 30.6   | 1510.7    | 294.0   | 44.8          | 64.2       | 30.3    |\n| Florence-VL 7B | Vicuna 1.5 | **62.7** | **66.1**     | 55.8         | **54.5** | **89.4** | **35.2** | **1543.5** | **316.4** | **46.8**     | **65.0**     | **36.8** |\n| LLaVA 1.5 8B | Llama 3    | 62.8 | **71.4**      | 65.5         | 49.3   | 84.8  | 34.2   | 1539.4    | 292.5   | 45.7          | **71.0**     | 38.5    |\n| Florence-VL 8B | Llama 3    | **63.8** | 71.1         | **65.8**      | **54.0** | **88.4** | **36.4** | **1584.1** | **346.8** | **46.8**     | 66.2       | **39.1** |", "caption": "Table 2: \nResults on general multimodal benchmarks, Vision centric, Knowledge based, and OCR & Chart benchmarks.", "description": "This table presents a comprehensive evaluation of the Florence-VL model across a diverse range of benchmarks.  It's broken down into four categories: general multimodal benchmarks (assessing overall performance across multiple tasks), vision-centric benchmarks (focus on vision-specific capabilities), knowledge-based benchmarks (testing reasoning and factual understanding), and OCR & Chart benchmarks (evaluating performance on text extraction from images and chart understanding).  For each category, the table shows the performance of Florence-VL alongside various baseline models (different sizes and architectures), allowing for direct comparisons and highlighting the model's strengths and weaknesses in different areas.", "section": "5. Experiments"}, {"content": "|           | LLM        | Realworldqa | MMVP   | AI2D   | MathVista | MMMU   | SciQA-IMG | TextVQA  | OCRBench | ChartQA | DocVQA  | InfoVQA |\n| :--------- | :--------- | :---------- | :----- | :----- | :-------- | :----- | :-------- | :------- | :------ | :------ | :------ | :------ |\n| LLaVA 1.5 3B | Phi 3.5    | 54.4        | 2.0    | 63.3   | 30.6      | **40.7** | **72.0**   | 43.7     | 30.4    | 16.4    | 28.1    | 26.4    |\n| Florence-VL 3B | Phi 3.5    | **58.4**    | **6.0** | **64.9** | 30.6      | 39.6   | 68.7      | **61.6** | **40.3** | **21.8** | **46.1** | **29.6** |\n| LLaVA 1.5 7B | Vicuna 1.5 | 54.8        | 6.0    | 54.8   | 26.7      | 35.3   | **66.8**   | 58.2     | 31.4    | 18.2    | 28.1    | 25.8    |\n| Florence-VL 7B | Vicuna 1.5 | **60.4**    | **12.3** | **57.2** | **28.0**  | **35.6** | 66.5      | **62.8** | **41.4** | **24.3** | **44.5** | **29.4** |\n| LLaVA 1.5 8B | Llama 3    | 55.7        | 7.3    | 60.2   | 29.3      | 39.4   | **76.5**   | 45.4     | 34.6    | 15.4    | 28.6    | 26.4    |\n| Florence-VL 8B | Llama 3    | **59.9**    | **8.3** | **62.4** | **31.8**  | **39.9** | 73.6      | **68.0** | **41.1** | **23.4** | **44.4** | **29.0** |", "caption": "(a) Results on general multimodal benchmarks.", "description": "This table presents the quantitative results of Florence-VL and various baseline models on general multimodal benchmarks.  The metrics assess performance across different tasks involving diverse visual and textual inputs.  It shows a comparison of the performance of Florence-VL models with varying sizes (3B, 7B, 8B parameters) against other state-of-the-art Multimodal Large Language Models (MLLMs). The benchmarks cover image captioning, question answering, visual reasoning, and other multimodal understanding tasks.", "section": "5. Experiments"}, {"content": "| Features used | MMBench (EN) | POPE | MM-Vet | MME-P | Seed-image | HallusionBench | LLaVA-bench | AI2D | MathVista | MMMU | OCRBench | ChartQA | DocVQA | InfoVQA |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| [\\mathbf{V}] | 64.3 | 86.1 | 31.1 | 1510.7 | 66.0 | 44.8 | 64.2 | 54.7 | 26.7 | 35.2 | 31.2 | 18.3 | 27.9 | 25.7 |\n| [\\mathbf{V},\\mathbf{V}_{t_{1}}^{\\prime},\\mathbf{V}_{t_{2}}^{\\prime},\\mathbf{V}_{t_{3}}^{\\prime}] | 66.1 | 89.4 | 35.2 | 1543.5 | 70.3 | 46.8 | 65.0 | 57.2 | 28.0 | 35.6 | 41.4 | 24.3 | 44.5 | 29.4 |", "caption": "(b) Results on Vision centric, Knowledge based, and OCR & Chart benchmarks.", "description": "Table 2b presents a breakdown of the performance of various models on three categories of benchmarks: Vision-centric, Knowledge-based, and OCR & Chart.  Vision-centric tasks focus on visual understanding and perception. Knowledge-based tasks require reasoning and factual knowledge. OCR & Chart tasks involve extracting information from text in images or charts. The table shows the performance (measured as accuracy) of different models\u2014including the Florence-VL models of varying sizes and several baselines\u2014on each benchmark.", "section": "5. Experiments"}, {"content": "|   | GQA | MMBench (EN) | MMBench (CN) | VizWiz | POPE | MM-Vet | MME-P | MME-C | Seed-image | HallusionBench | LLaVA-bench | MMStar |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Florence-VL 7B | 62.7 | 66.1 | 55.8 | 54.5 | 89.4 | 35.2 | 1543.5 | 316.4 | 70.3 | 46.8 | 65.0 | 36.8 |\n| Remove Caption Feature  \ud835\udc15\u2032t1  | 62.2 | 64.9 | 56.1 | 53.5 | 89.3 | 31.8 | 1477.8 | 354.3 | 69.0 | 44.9 | 65.2 | 36.0 |\n| Remove OCR Feature  \ud835\udc15\u2032t2  | 62.0 | 65.6 | 55.4 | 56.0 | 88.8 | 30.2 | 1506.3 | 345.4 | 67.6 | 45.4 | 62.6 | 35.2 |\n| Remove Grounding Feature  \ud835\udc15\u2032t3  | 63.0 | 66.6 | 56.8 | 56.5 | 88.8 | 32.9 | 1494.8 | 338.9 | 70.8 | 44.7 | 65.1 | 36.2 |", "caption": "Table 3: We compare LLaVA 1.5 with our model (Florence-VL 3B/7B/8B) across multiple multimodal benchmarks. The key difference between them lies in the vision encoders used (CLIP for LLaVA vs. Florence-2 for our model), while we maintain the same training data and backbone LLMs for both. The results show that our models significantly outperform LLaVA 1.5 with the same training data.", "description": "This table compares the performance of LLaVA 1.5 and Florence-VL (in 3B, 7B, and 8B parameter versions) across a range of multimodal benchmark datasets.  The key difference between the models is the vision encoder used: LLaVA 1.5 employs CLIP, while Florence-VL utilizes Florence-2.  Importantly, both models were trained using the same training data and underlying large language models (LLMs). The results demonstrate that Florence-VL achieves significantly better performance than LLaVA 1.5, highlighting the advantages of using Florence-2 as the vision encoder.", "section": "5. Experiments"}, {"content": "| | OCRBench | ChartQA | DocVQA | InfoVQA | Average |\n|---|---|---|---|---|---| \n| Florence-VL 7B | **41.4** | **24.3** | **44.5** | **29.4** | **34.9** |\n| OCR | 40.9 | 22.9 | 44.4 | 29.0 | 34.2 |", "caption": "Table 4: The comparison between keeping only the lower-level feature [\ud835\udc15]delimited-[]\ud835\udc15[\\mathbf{V}][ bold_V ] and our method, which includes both lower- and higher-level features, clearly demonstrates that maintaining both types of features achieves better performance.", "description": "This table compares the performance of using only lower-level visual features (from the DaViT vision encoder) against using both lower-level and higher-level visual features (from Florence-2).  The results show that combining features from different levels (depth) significantly improves the model's performance across various benchmarks.", "section": "3.3 Depth-Breadth Fusion"}, {"content": "|           | AI2D | MathVista | MMMU | SciQA-IMG | Average |\n| :-------- | :-------: | :--------: | :------: | :--------: | :------: |\n| Florence-VL 7B | **57.2** | **28.0** | 35.6 | **66.5** | 46.8 |\n| Caption | 56.8 | 27.5 | **36.9** | 65.5 | 46.7 |\n| OCR       | 55.7 | 27.0 | 35.8 | 65.6 | 46.0 |\n| Grounding | 56.7 | 27.9 | **36.9** | 66.4 | **47.0** |", "caption": "Table 5: Ablation study was conducted by removing one high level image feature at a time, demonstrating that all high-level features are essential for maintaining optimal performance.", "description": "This ablation study investigates the impact of individual high-level visual features extracted by Florence-2 on the overall performance of the Florence-VL model. By systematically removing one high-level feature (Detailed Caption, OCR, Grounding) at a time while keeping other features, the table quantifies the effect on various downstream tasks. The results demonstrate the importance of all three high-level visual features in achieving optimal performance, highlighting the complementary nature of different visual representations.", "section": "3. Method"}]