[{"figure_path": "https://arxiv.org/html/2502.11433/x1.png", "caption": "Figure 1: A high-level overview of our LLM-based reinforcement learning setup for financial trading. The environment provides the current state\nstsubscript\ud835\udc60\ud835\udc61s_{t}italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. A prompt containing task details, the action space, and the current state is fed into the LLM, which outputs a trading action\natsubscript\ud835\udc4e\ud835\udc61a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. The action is executed in the environment, yielding a reward\nr\u2062(st,at)\ud835\udc5fsubscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61r(s_{t},a_{t})italic_r ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) and next state\nst+1subscript\ud835\udc60\ud835\udc611s_{t+1}italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT. The log-likelihood\nlog\u03c0\u03b8\u2061(at|lang\u2062(st))subscriptsubscript\ud835\udf0b\ud835\udf03conditionalsubscript\ud835\udc4e\ud835\udc61langsubscript\ud835\udc60\ud835\udc61\\log_{\\pi_{\\theta}}(a_{t}|\\texttt{lang}(s_{t}))roman_log start_POSTSUBSCRIPT italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | lang ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) is then leveraged by a policy gradient method (e.g., PPO), with experience tuples stored in a replay buffer for iterative updates.", "description": "This figure illustrates the FLAG-TRADER framework, which combines a large language model (LLM) with reinforcement learning for financial trading.  The environment provides the current market state (s<sub>t</sub>). This state, along with the trading task details and the allowed actions, is presented to the LLM as a prompt.  The LLM then determines an action (a<sub>t</sub>), which is executed in the trading environment. The environment provides feedback in the form of a reward (r(s<sub>t</sub>, a<sub>t</sub>)) and updates the system's state to s<sub>t+1</sub>. The LLM\u2019s action likelihood is used in a policy gradient algorithm (like PPO) to iteratively improve trading decisions.  Experience from trading (states, actions, rewards) is stored in a replay buffer to enhance the training process.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.11433/x2.png", "caption": "Figure 2: The FLAG-Trader pipeline for financial trading, utilizing an LLM-based actor-critic architecture. The LLM consists of frozen base layers \u03b8frozensubscript\ud835\udf03frozen\\theta_{\\texttt{frozen}}italic_\u03b8 start_POSTSUBSCRIPT frozen end_POSTSUBSCRIPT that retain pre-trained knowledge and trainable top layers \u03b8trainsubscript\ud835\udf03train\\theta_{\\texttt{train}}italic_\u03b8 start_POSTSUBSCRIPT train end_POSTSUBSCRIPT for financial decision-making. Both the Policy_Net and Value_Net share these trainable layers while maintaining separate policy head \u03b8Psubscript\ud835\udf03\ud835\udc43\\theta_{P}italic_\u03b8 start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT and value head \u03b8Vsubscript\ud835\udf03\ud835\udc49\\theta_{V}italic_\u03b8 start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT, which are updated by policy gradient method.", "description": "FLAG-TRADER uses an LLM as its core, splitting it into frozen base layers (retaining pre-trained knowledge) and trainable top layers (adapted for finance).  Both a policy network (for choosing actions) and a value network (for estimating future value) use these trainable layers, but each has its own separate head which is updated during training using a policy gradient method.  This architecture allows for efficient fine-tuning of the LLM for the financial trading task.", "section": "4 FLAG-TRADER"}, {"figure_path": "https://arxiv.org/html/2502.11433/x3.png", "caption": "Figure 3: The format of input prompt. It contains the task description, the legible action set, the current state description, and the output action format.", "description": "This figure shows the structure of the prompt used as input to the LLM in the FLAG-TRADER model.  The prompt is designed to be comprehensive and unambiguous, providing all the necessary information for the model to make informed trading decisions. It includes four key parts: 1) a clear description of the trading task, stating the overall objective; 2) a definition of the permissible actions (buy, sell, or hold); 3) a detailed description of the current market state, including historical prices, account status, cash balance, asset position, and other relevant indicators; and 4) instructions on the expected format of the output (a JSON specifying the action to take). The example provided illustrates the prompt's structure in the context of stock trading.", "section": "4 FLAG-TRADER"}]