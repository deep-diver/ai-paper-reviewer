{"references": [{" publication_date": "2017", "fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "reason": "This paper is foundational to the field of reinforcement learning from human feedback (RLHF), which is central to the work presented. It lays the groundwork for using human preferences to train reward models for aligning LLMs, a core concept in the current research.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is highly influential, demonstrating the success of RLHF in aligning LLMs with human preferences.  Its methodology and results directly inform the current work, which also seeks to improve the efficiency of preference data collection in RLHF.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "reason": "This paper presents a significant advancement in RLHF, demonstrating the ability to train helpful and harmless LLMs using human feedback. It's highly relevant because the current research aims to improve the efficiency of the data collection process used in similar RLHF workflows.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hannah Kirk", "paper_title": "The past, present and better future of feedback learning in large language models for subjective human preferences and values", "reason": "This paper provides a comprehensive overview of feedback learning methods in LLMs, focusing on the challenges of obtaining subjective human preferences.  Its insights regarding the costs and limitations of human annotation directly inform the current work.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Hannah Rose Kirk", "paper_title": "The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models", "reason": "This paper offers valuable insights into the diverse aspects of human feedback and alignment, highlighting challenges such as subjectivity and cultural differences in preferences, directly relevant to the current work's exploration of cost-effective preference collection.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "reason": "This paper introduces the Proximal Policy Optimization (PPO) algorithm, a widely used reinforcement learning method.  Understanding PPO is essential for grasping the context of reinforcement learning from human feedback and how reward models are trained.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Hamish Ivison", "paper_title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning From Preference Feedback", "reason": "This paper provides a critical analysis of DPO and PPO algorithms and their relationship to reward model training, directly relevant to the current work's exploration of alternative annotation strategies and their effect on reward model performance.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Alon Albalak", "paper_title": "A survey on data selection for language models", "reason": "This paper provides a valuable overview of data selection techniques for language models. It's relevant to the present work because data selection is a crucial aspect of efficiently and accurately collecting preferences.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Nathan Lambert", "paper_title": "RewardBench: Evaluating reward models for language modeling", "reason": "This paper introduces RewardBench, a benchmark for evaluating reward models.  Its use as a central evaluation metric in the current research makes it a critical reference.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Hamish Ivison", "paper_title": "Camels in a changing climate: Enhancing LM adaptation with Tulu 2", "reason": "This paper presents the T\u00fclu model, used in the current work for both generating responses and training reward models. Understanding its capabilities and characteristics is therefore critical.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yann Dubois", "paper_title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback", "reason": "This paper introduces AlpacaFarm, a simulation framework for studying reinforcement learning from human feedback, offering a relevant comparative approach to the present work.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Chatbot arena: An open platform for evaluating llms by human preference", "reason": "This paper introduces the Chatbot Arena, a dataset utilized in the current work for evaluating model performance.  Understanding its characteristics and its use in this study is important.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Lianmin Zheng", "paper_title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena", "reason": "This paper presents a further analysis of Chatbot Arena, providing valuable context and insights for evaluating the performance of LLMs, a core aspect of the current research.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Yizhong Wang", "paper_title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources", "reason": "This paper contributes to understanding instruction tuning techniques, which are relevant to creating and evaluating preference datasets, as the current study also uses models trained through instruction tuning.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zhilin Wang", "paper_title": "HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM", "reason": "This paper introduces the Helpsteer dataset, used for evaluation in the current work. Understanding its characteristics is crucial for interpreting the results and comparing the performance of the proposed routing framework.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zhilin Wang", "paper_title": "Helpsteer2-preference: Complementing ratings with preferences", "reason": "This paper provides a related dataset, Helpsteer2-Preferences, offering a valuable comparative dataset with different annotation strategies that enhances the analysis of the routing framework's performance.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "reason": "This paper introduces GSM8K, a benchmark used in the current study's evaluation of reward models.  The dataset's importance and its use in downstream task evaluation make it a significant reference.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Mirac Suzgun", "paper_title": "Challenging big-bench tasks and whether chain-of-thought can solve them", "reason": "This paper introduces BIG-Bench Hard (BBH), another benchmark used in the current study's evaluation.  Understanding BBH and its use in downstream task evaluation is critical to interpreting the results.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Jeffrey Zhou", "paper_title": "Instruction-following evaluation for large language models", "reason": "This paper introduces IFEval, a benchmark for evaluating instruction-following capabilities in LLMs.  Understanding IFEval is crucial for interpreting the downstream task evaluation results in the current work.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "reason": "This paper introduces HumanEval, a benchmark used for evaluating code generation capabilities in LLMs. Its relevance as a downstream task in the current study makes it a critical reference.", "section_number": 4}]}