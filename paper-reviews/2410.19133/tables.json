[{"figure_path": "2410.19133/tables/table_1_0.html", "caption": "Table 3: Comparison of full direct human preferences and synthetic preferences on the best hybrid preference mix given unlimited budget on RewardBench. Reporting the average of three runs.", "description": "The table compares the performance of reward models trained on different mixtures of human and synthetic preferences (best hybrid, full human, and full synthetic) on the RewardBench benchmark.", "section": "4.2 Generalization to unseen preference datasets"}, {"figure_path": "2410.19133/tables/table_5_0.html", "caption": "Table 2: Spearman p of the predicted and actual ranks of 16 held-out candidate datasets, and the RMSE between the predicted performance against actual performance.", "description": "The table shows the Spearman correlation and RMSE between the predicted and actual performance of 16 held-out candidate datasets, evaluating the fit of the performance prediction model.", "section": "DETAILS OF THE PERFORMANCE PREDICTION MODEL"}, {"figure_path": "2410.19133/tables/table_5_1.html", "caption": "Table 1: MULTIPREF dataset statistics.", "description": "The table presents statistics of the MULTIPREF dataset, including the number of unique prompts, models, model pairs, comparisons, and annotations, as well as annotator statistics.", "section": "3 MULTIPREF: A NEW PREFERENCE DATASET"}, {"figure_path": "2410.19133/tables/table_6_0.html", "caption": "Table 2: Spearman p of the predicted and actual ranks of 16 held-out candidate datasets, and the RMSE between the predicted performance against actual performance.", "description": "The table presents the Spearman correlation and RMSE between predicted and actual performance for 16 held-out candidate datasets, indicating the performance prediction model's accuracy.", "section": "4.1 DETAILS OF THE PERFORMANCE PREDICTION MODEL"}, {"figure_path": "2410.19133/tables/table_7_0.html", "caption": "Table 3: Comparison of full direct human preferences and synthetic preferences on the best hybrid preference mix given unlimited budget on RewardBench. Reporting the average of three runs.", "description": "The table compares the performance of reward models trained on full human preferences, full synthetic preferences, and the best hybrid mix of human and synthetic preferences on the RewardBench benchmark.", "section": "4.2 Generalization to Unseen Preference Datasets"}, {"figure_path": "2410.19133/tables/table_8_0.html", "caption": "Table 3: Comparison of full direct human preferences and synthetic preferences on the best hybrid preference mix given unlimited budget on RewardBench. Reporting the average of three runs.", "description": "The table compares the RewardBench performance of models trained on three types of preference data: full direct human preferences, full synthetic preferences, and the best hybrid mix of human and synthetic preferences,  given an unlimited annotation budget.", "section": "4.2 Generalization to Unseen Preference Datasets"}, {"figure_path": "2410.19133/tables/table_9_0.html", "caption": "Table 5: Average gain in MULTIPREF\u2019s performance (as predicted by the quadratic PPM) when routing 100 random preference instances to a human annotator for each tag. Showing top- and bottom-ten tags (See the full list in Appendix Table 12).", "description": "This table shows the average performance gain in MULTIPREF when routing 100 instances with specific tags to human annotators, as predicted by a quadratic performance prediction model.", "section": "5 ANALYSIS: WHEN ARE HUMAN ANNOTATIONS HELPFUL?"}, {"figure_path": "2410.19133/tables/table_17_0.html", "caption": "Table 3: Comparison of full direct human preferences and synthetic preferences on the best hybrid preference mix given unlimited budget on RewardBench. Reporting the average of three runs.", "description": "This table compares the performance of reward models trained on three different preference mixes (full human, full synthetic, and the best hybrid mix) across four datasets, using RewardBench as the evaluation metric.", "section": "4.2 GENERALIZATION TO UNSEEN PREFERENCE DATASETS"}, {"figure_path": "2410.19133/tables/table_18_0.html", "caption": "Table 17: Comparison of DPO-trained models using different human-LLM preference mixes.", "description": "The table compares the performance of different reward models trained using various mixes of human and synthetic preferences on different downstream tasks using direct preference optimization.", "section": "I Direct Preference Optimization Results"}, {"figure_path": "2410.19133/tables/table_19_0.html", "caption": "Table 3: Comparison of full direct human preferences and synthetic preferences on the best hybrid preference mix given unlimited budget on RewardBench. Reporting the average of three runs.", "description": "The table compares the RewardBench performance of models trained on three types of preference data: full direct human preferences, full synthetic preferences, and the best hybrid mix of human and synthetic preferences.", "section": "4.2 GENERALIZATION TO UNSEEN PREFERENCE DATASETS"}, {"figure_path": "2410.19133/tables/table_20_0.html", "caption": "Table 3: Comparison of full direct human preferences and synthetic preferences on the best hybrid preference mix given unlimited budget on RewardBench. Reporting the average of three runs.", "description": "The table compares the RewardBench performance of models trained on 100% human preferences, 100% synthetic preferences, and the best hybrid mix of human and synthetic preferences, showing the superior performance of the hybrid approach.", "section": "4.2 Generalization to Unseen Preference Datasets"}, {"figure_path": "2410.19133/tables/table_21_0.html", "caption": "Table 3: Comparison of full direct human preferences and synthetic preferences on the best hybrid preference mix given unlimited budget on RewardBench. Reporting the average of three runs.", "description": "Table 3 compares the performance of reward models trained on full direct human preferences, full synthetic preferences, and the best hybrid mix of human and synthetic preferences, as evaluated by RewardBench.", "section": "4.2 GENERALIZATION TO UNSEEN PREFERENCE DATASETS"}, {"figure_path": "2410.19133/tables/table_22_0.html", "caption": "Table 3: Comparison of full direct human preferences and synthetic preferences on the best hybrid preference mix given unlimited budget on RewardBench. Reporting the average of three runs.", "description": "The table compares the RewardBench performance of models trained on three types of preference datasets: full direct human, full synthetic, and the best hybrid mix generated by the proposed routing framework.", "section": "4.2 GENERALIZATION TO UNSEEN PREFERENCE DATASETS"}, {"figure_path": "2410.19133/tables/table_23_0.html", "caption": "Table 12: Average gain in MULTIPREF's performance (as predicted by the quadratic regressor) when routing random 100 units to human annotators.", "description": "This table shows the performance gain for each tag when routing 100 random instances to human annotators, comparing against a set with no human annotations.", "section": "F Performance Gain"}, {"figure_path": "2410.19133/tables/table_30_0.html", "caption": "Table 10: RewardBench scores of reward models using different inference-time sampling strategies based on a linear model: top-k and simulated (Sim). Reporting average of three runs.", "description": "The table presents RewardBench scores of reward models trained using different inference-time sampling strategies (top-k and simulated) and varying proportions of human annotations for four different preference datasets.", "section": "E Inference-time Selection Strategies"}, {"figure_path": "2410.19133/tables/table_31_0.html", "caption": "Table 11: RewardBench scores of reward models using different inference-time sampling strategies based on a quadratic model: top-k and simulated (Sim). Reporting average of three runs.", "description": "The table presents RewardBench scores achieved by reward models trained on different preference datasets using various inference-time sampling strategies (top-k and simulated) based on a quadratic performance prediction model.", "section": "4.2 GENERALIZATION TO UNSEEN PREFERENCE DATASETS"}, {"figure_path": "2410.19133/tables/table_31_1.html", "caption": "Table 12: Average gain in MULTIPREF's performance (as predicted by the quadratic regressor) when routing random 100 units to human annotators.", "description": "The table shows the average gain in MULTIPREF's performance for each tag when 100 instances are randomly routed to human annotators, as predicted by the quadratic PPM.", "section": "F Performance Gain"}, {"figure_path": "2410.19133/tables/table_33_0.html", "caption": "Table 13: Finegrained RewardBench results on the Chat category", "description": "The table presents fine-grained RewardBench results, comparing the performance of different preference mixes (full human, full synthetic, and best hybrid) across various datasets on the Chat category.", "section": "4.2 GENERALIZATION TO UNSEEN PREFERENCE DATASETS"}, {"figure_path": "2410.19133/tables/table_33_2.html", "caption": "Table 15: Finegrained RewardBench results on the Safety category", "description": "The table presents fine-grained RewardBench results on the Safety category, comparing the performance of different preference mixes (full direct human preferences, full synthetic preferences, and best hybrid mixes) across four datasets.", "section": "4.2 GENERALIZATION TO UNSEEN PREFERENCE DATASETS"}, {"figure_path": "2410.19133/tables/table_33_3.html", "caption": "Table 16: Finegrained RewardBench results on the Reasoning category", "description": "The table shows the fine-grained RewardBench results on the reasoning category for different preference mixes (MULTIPREF, Helpsteer2, AlpacaFarm, and ChatArena).", "section": "4.3 GENERALIZATION TO OTHER EVALUATION TASKS"}, {"figure_path": "2410.19133/tables/table_34_0.html", "caption": "Table 17: Comparison of DPO-trained models using different human-LLM preference mixes.", "description": "This table compares the downstream task performance of models trained using different human-LLM preference mixes via direct preference optimization (DPO).", "section": "I Direct Preference Optimization Results"}, {"figure_path": "2410.19133/tables/table_34_1.html", "caption": "Table 18: Reward Model Training Hyperparameters", "description": "This table lists the hyperparameters used for training the reward models in the MULTIPREF study.", "section": "J Reward Model Training Details"}, {"figure_path": "2410.19133/tables/table_39_0.html", "caption": "Table 3: Comparison of full direct human preferences and synthetic preferences on the best hybrid preference mix given unlimited budget on RewardBench. Reporting the average of three runs.", "description": "The table compares the performance of reward models trained on full direct human preferences, full synthetic preferences, and the best hybrid preference mix determined by the routing framework, evaluated using RewardBench.", "section": "4.2 Generalization to Unseen Preference Datasets"}, {"figure_path": "2410.19133/tables/table_39_1.html", "caption": "Table 3: Comparison of full direct human preferences and synthetic preferences on the best hybrid preference mix given unlimited budget on RewardBench. Reporting the average of three runs.", "description": "The table compares the RewardBench performance of models trained using only human preferences, only synthetic preferences, and the best hybrid mix of human and synthetic preferences determined by the proposed routing framework.", "section": "4.2 Generalization to Unseen Preference Datasets"}]