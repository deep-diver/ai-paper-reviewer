<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Predicting Emergent Capabilities by Finetuning &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="Predicting Emergent Capabilities by Finetuning &#183; HF Daily Paper Reviews by AI"><meta name=description content="Predicting emergent LLM capabilities is now possible by finetuning smaller models; this approach shifts the emergence point, enabling accurate predictions of future model performance, even with up to ..."><meta name=keywords content="Natural Language Processing,Large Language Models,üè¢ UC Berkeley,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="Predicting Emergent Capabilities by Finetuning"><meta property="og:description" content="Predicting emergent LLM capabilities is now possible by finetuning smaller models; this approach shifts the emergence point, enabling accurate predictions of future model performance, even with up to ‚Ä¶"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-11-25T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-25T00:00:00+00:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="üè¢ UC Berkeley"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/cover.png"><meta name=twitter:title content="Predicting Emergent Capabilities by Finetuning"><meta name=twitter:description content="Predicting emergent LLM capabilities is now possible by finetuning smaller models; this approach shifts the emergence point, enabling accurate predictions of future model performance, even with up to ‚Ä¶"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Predicting Emergent Capabilities by Finetuning","headline":"Predicting Emergent Capabilities by Finetuning","abstract":"Predicting emergent LLM capabilities is now possible by finetuning smaller models; this approach shifts the emergence point, enabling accurate predictions of future model performance, even with up to \u0026hellip;","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2411.16035\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2024","dateCreated":"2024-11-25T00:00:00\u002b00:00","datePublished":"2024-11-25T00:00:00\u002b00:00","dateModified":"2024-11-25T00:00:00\u002b00:00","keywords":["Natural Language Processing","Large Language Models","üè¢ UC Berkeley"],"mainEntityOfPage":"true","wordCount":"6002"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-19/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-19</p></a><a href=/ai-paper-reviewer/2025-03-20/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-20</p></a><a href=/ai-paper-reviewer/2025-03-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-21</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-19/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-19</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-20/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-20</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-21</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2411.16035/cover_hu914379553525517916.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2411.16035/>Predicting Emergent Capabilities by Finetuning</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Predicting Emergent Capabilities by Finetuning</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-11-25T00:00:00+00:00>25 November 2024</time><span class="px-2 text-primary-500">&#183;</span><span>6002 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">29 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2411.16035/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2411.16035/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/natural-language-processing/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Natural Language Processing
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/large-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-uc-berkeley/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ UC Berkeley</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#emergence-prediction>Emergence Prediction</a></li><li><a href=#finetuning-effects>Finetuning Effects</a></li><li><a href=#emergence-laws>Emergence Laws</a></li><li><a href=#real-world-uses>Real-World Uses</a></li><li><a href=#future-directions>Future Directions</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#emergence-prediction>Emergence Prediction</a></li><li><a href=#finetuning-effects>Finetuning Effects</a></li><li><a href=#emergence-laws>Emergence Laws</a></li><li><a href=#real-world-uses>Real-World Uses</a></li><li><a href=#future-directions>Future Directions</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2411.16035</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Charlie Snell et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2024-11-26</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2411.16035 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2411.16035 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/predicting-emergent-capabilities-by target=_self role=button>‚Üó Papers with Code</a></p><audio controls><source src=https://ai-paper-reviewer.com/2411.16035/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Large language models (LLMs) sometimes exhibit surprising new capabilities as they grow larger, a phenomenon known as &rsquo;emergence&rsquo;. This poses a challenge for researchers because it&rsquo;s difficult to predict when and how these capabilities will appear. The paper tackles this challenge. It highlights the difficulty in anticipating emergent capabilities in LLMs due to the unpredictability of downstream capabilities despite predictable pretraining loss. This difficulty impacts model developers, policymakers, and stakeholders who make decisions based on future LLM capabilities.</p><p>The researchers propose a novel approach to predict LLM emergence. They found that finetuning LLMs on a specific task shifts the point at which emergence occurs towards less capable models. They operationalized this insight by finetuning LLMs with varying data amounts and fitting a parametric function to predict emergence. Their method successfully predicted emergence using only smaller-scale LLMs, showing promise in forecasting capabilities of models trained with up to 4x more compute. They also demonstrated practical uses for their method in assessing data quality and anticipating future LLM capabilities.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-843b8df0871432805578ee3a9d19f159></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-843b8df0871432805578ee3a9d19f159",{strings:[" Finetuning smaller LLMs can accurately predict when larger models will exhibit emergent capabilities. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-689d5e2791a54de19ed1caf965cb9451></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-689d5e2791a54de19ed1caf965cb9451",{strings:[" A simple parametric function can model how finetuning data affects the point of emergence in LLMs. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-422c32e2aa1d8b662c598c2bbc047496></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-422c32e2aa1d8b662c598c2bbc047496",{strings:[" This method enables cheaper assessments of pretraining data quality and prediction of complex future LLM capabilities. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial for researchers working with large language models (LLMs). It introduces a novel method for predicting the emergence of capabilities in LLMs, which is a significant challenge in the field. This allows for better resource allocation and safer development of future LLMs by anticipating capabilities and potential risks <strong>before</strong> they emerge, enabling proactive risk management. The proposed method is also valuable for improving data quality assessment during LLM training.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x1.png alt></figure></p><blockquote><p>üîº This figure illustrates the effect of finetuning on the emergence of capabilities in large language models. The left panel shows that for a specific task, finetuning a model with increasing amounts of data shifts the point at which it starts showing non-trivial performance (emergence point) towards models with lower capabilities. The right panel introduces a novel method to predict when few-shot emergence will occur. An &rsquo;emergence law&rsquo; is proposed ‚Äì a parametric function that models how the emergence point shifts as a function of finetuning data. Using this law, the emergence point for few-shot learning can be predicted by extrapolating to the limit of small amounts of finetuning data. This allows researchers to anticipate the capabilities of future, larger models.</p><details><summary>read the caption</summary>Figure 1: We find that task-specific finetuning systematically shifts the point of emergence towards less capable models. Motivated by this finding, we develop an emergence law, which models how the point of emergence shifts as a function of the amount of finetuning data. Using this emergence law, we can then extrapolate a prediction for the point of emergence in the few-shot setting.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Symbol</th><th>Description</th></tr></thead><tbody><tr><td><math alttext="D" class="ltx_Math" display="inline" id="S5.T1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.m1.1a"><mi id="S5.T1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.m1.1d">italic_D</annotation></semantics></math></td><td>Amount of finetuning data</td></tr><tr><td>&lt;math alttext=&ldquo;L(</td><td></td></tr></tbody></table><p>text{M})&rdquo; class=&ldquo;ltx_Math&rdquo; display=&ldquo;inline&rdquo; id=&ldquo;S5.T1.2.2.1.m1.1&rdquo;><semantics id=S5.T1.2.2.1.m1.1a><mrow id=S5.T1.2.2.1.m1.1.2 xref=S5.T1.2.2.1.m1.1.2.cmml><mi id=S5.T1.2.2.1.m1.1.2.2 xref=S5.T1.2.2.1.m1.1.2.2.cmml>L</mi><mo id=S5.T1.2.2.1.m1.1.2.1 xref=S5.T1.2.2.1.m1.1.2.1.cmml>‚Å¢</mo><mrow id=S5.T1.2.2.1.m1.1.2.3.2 xref=S5.T1.2.2.1.m1.1.1a.cmml><mo id=S5.T1.2.2.1.m1.1.2.3.2.1 stretchy=false xref=S5.T1.2.2.1.m1.1.1a.cmml>(</mo><mtext id=S5.T1.2.2.1.m1.1.1 xref=S5.T1.2.2.1.m1.1.1.cmml>M</mtext><mo id=S5.T1.2.2.1.m1.1.2.3.2.2 stretchy=false xref=S5.T1.2.2.1.m1.1.1a.cmml>)</mo></mrow></mrow><annotation-xml encoding=MathML-Content id=S5.T1.2.2.1.m1.1b><apply id=S5.T1.2.2.1.m1.1.2.cmml xref=S5.T1.2.2.1.m1.1.2><times id=S5.T1.2.2.1.m1.1.2.1.cmml xref=S5.T1.2.2.1.m1.1.2.1></times><ci id=S5.T1.2.2.1.m1.1.2.2.cmml xref=S5.T1.2.2.1.m1.1.2.2>ùêø</ci><ci id=S5.T1.2.2.1.m1.1.1a.cmml xref=S5.T1.2.2.1.m1.1.2.3.2><mtext id=S5.T1.2.2.1.m1.1.1.cmml xref=S5.T1.2.2.1.m1.1.1>M</mtext></ci></apply></annotation-xml><annotation encoding=application/x-tex id=S5.T1.2.2.1.m1.1c>L(</p><p>text{M})</annotation><annotation encoding=application/x-llamapun id=S5.T1.2.2.1.m1.1d>italic_L ( M )</annotation></semantics></math> | Pretraining loss of model <span class="ltx_text ltx_align_left ltx_markedasmath" id=S5.T1.3.3.2.1.1.1>M</span> |
| <span class="ltx_text ltx_markedasmath" id=S5.T1.4.4.1.1>Perf</span> | Downstream performance |
| <math alttext="A,B,E" class="ltx_Math" display="inline" id="S5.T1.5.5.1.m1.3"><semantics id="S5.T1.5.5.1.m1.3a"><mrow id="S5.T1.5.5.1.m1.3.4.2" xref="S5.T1.5.5.1.m1.3.4.1.cmml"><mi id="S5.T1.5.5.1.m1.1.1" xref="S5.T1.5.5.1.m1.1.1.cmml">A</mi><mo id="S5.T1.5.5.1.m1.3.4.2.1" xref="S5.T1.5.5.1.m1.3.4.1.cmml">,</mo><mi id="S5.T1.5.5.1.m1.2.2" xref="S5.T1.5.5.1.m1.2.2.cmml">B</mi><mo id="S5.T1.5.5.1.m1.3.4.2.2" xref="S5.T1.5.5.1.m1.3.4.1.cmml">,</mo><mi id="S5.T1.5.5.1.m1.3.3" xref="S5.T1.5.5.1.m1.3.3.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.1.m1.3b"><list id="S5.T1.5.5.1.m1.3.4.1.cmml" xref="S5.T1.5.5.1.m1.3.4.2"><ci id="S5.T1.5.5.1.m1.1.1.cmml" xref="S5.T1.5.5.1.m1.1.1">ùê¥</ci><ci id="S5.T1.5.5.1.m1.2.2.cmml" xref="S5.T1.5.5.1.m1.2.2">ùêµ</ci><ci id="S5.T1.5.5.1.m1.3.3.cmml" xref="S5.T1.5.5.1.m1.3.3">ùê∏</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.1.m1.3c">A,B,E</annotation><annotation encoding="application/x-llamapun" id="S5.T1.5.5.1.m1.3d">italic_A , italic_B , italic_E</annotation></semantics></math> | ReLU parameters |
| &lt;math alttext=&ldquo;E_{</p><p>theta}(D)&rdquo; class=&ldquo;ltx_Math&rdquo; display=&ldquo;inline&rdquo; id=&ldquo;S5.T1.6.6.1.m1.1&rdquo;><semantics id=S5.T1.6.6.1.m1.1a><mrow id=S5.T1.6.6.1.m1.1.2 xref=S5.T1.6.6.1.m1.1.2.cmml><msub id=S5.T1.6.6.1.m1.1.2.2 xref=S5.T1.6.6.1.m1.1.2.2.cmml><mi id=S5.T1.6.6.1.m1.1.2.2.2 xref=S5.T1.6.6.1.m1.1.2.2.2.cmml>E</mi><mi id=S5.T1.6.6.1.m1.1.2.2.3 xref=S5.T1.6.6.1.m1.1.2.2.3.cmml>Œ∏</mi></msub><mo id=S5.T1.6.6.1.m1.1.2.1 xref=S5.T1.6.6.1.m1.1.2.1.cmml>‚Å¢</mo><mrow id=S5.T1.6.6.1.m1.1.2.3.2 xref=S5.T1.6.6.1.m1.1.2.cmml><mo id=S5.T1.6.6.1.m1.1.2.3.2.1 stretchy=false xref=S5.T1.6.6.1.m1.1.2.cmml>(</mo><mi id=S5.T1.6.6.1.m1.1.1 xref=S5.T1.6.6.1.m1.1.1.cmml>D</mi><mo id=S5.T1.6.6.1.m1.1.2.3.2.2 stretchy=false xref=S5.T1.6.6.1.m1.1.2.cmml>)</mo></mrow></mrow><annotation-xml encoding=MathML-Content id=S5.T1.6.6.1.m1.1b><apply id=S5.T1.6.6.1.m1.1.2.cmml xref=S5.T1.6.6.1.m1.1.2><times id=S5.T1.6.6.1.m1.1.2.1.cmml xref=S5.T1.6.6.1.m1.1.2.1></times><apply id=S5.T1.6.6.1.m1.1.2.2.cmml xref=S5.T1.6.6.1.m1.1.2.2><csymbol cd=ambiguous id=S5.T1.6.6.1.m1.1.2.2.1.cmml xref=S5.T1.6.6.1.m1.1.2.2>subscript</csymbol><ci id=S5.T1.6.6.1.m1.1.2.2.2.cmml xref=S5.T1.6.6.1.m1.1.2.2.2>ùê∏</ci><ci id=S5.T1.6.6.1.m1.1.2.2.3.cmml xref=S5.T1.6.6.1.m1.1.2.2.3>ùúÉ</ci></apply><ci id=S5.T1.6.6.1.m1.1.1.cmml xref=S5.T1.6.6.1.m1.1.1>ùê∑</ci></apply></annotation-xml><annotation encoding=application/x-tex id=S5.T1.6.6.1.m1.1c>E_{</p><p>theta}(D)</annotation><annotation encoding=application/x-llamapun id=S5.T1.6.6.1.m1.1d>italic_E start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_D )</annotation></semantics></math> | Emergence law; models emergence shift |
| &lt;math alttext=&ldquo;k,</p><p>alpha,C&rdquo; class=&ldquo;ltx_Math&rdquo; display=&ldquo;inline&rdquo; id=&ldquo;S5.T1.7.7.1.m1.3&rdquo;><semantics id=S5.T1.7.7.1.m1.3a><mrow id=S5.T1.7.7.1.m1.3.4.2 xref=S5.T1.7.7.1.m1.3.4.1.cmml><mi id=S5.T1.7.7.1.m1.1.1 xref=S5.T1.7.7.1.m1.1.1.cmml>k</mi><mo id=S5.T1.7.7.1.m1.3.4.2.1 xref=S5.T1.7.7.1.m1.3.4.1.cmml>,</mo><mi id=S5.T1.7.7.1.m1.2.2 xref=S5.T1.7.7.1.m1.2.2.cmml>Œ±</mi><mo id=S5.T1.7.7.1.m1.3.4.2.2 xref=S5.T1.7.7.1.m1.3.4.1.cmml>,</mo><mi id=S5.T1.7.7.1.m1.3.3 xref=S5.T1.7.7.1.m1.3.3.cmml>C</mi></mrow><annotation-xml encoding=MathML-Content id=S5.T1.7.7.1.m1.3b><list id=S5.T1.7.7.1.m1.3.4.1.cmml xref=S5.T1.7.7.1.m1.3.4.2><ci id=S5.T1.7.7.1.m1.1.1.cmml xref=S5.T1.7.7.1.m1.1.1>ùëò</ci><ci id=S5.T1.7.7.1.m1.2.2.cmml xref=S5.T1.7.7.1.m1.2.2>ùõº</ci><ci id=S5.T1.7.7.1.m1.3.3.cmml xref=S5.T1.7.7.1.m1.3.3>ùê∂</ci></list></annotation-xml><annotation encoding=application/x-tex id=S5.T1.7.7.1.m1.3c>k,</p><p>alpha,C</annotation><annotation encoding=application/x-llamapun id=S5.T1.7.7.1.m1.3d>italic_k , italic_Œ± , italic_C</annotation></semantics></math> | Emergence law parameters |
| <math alttext="D_{0}" class="ltx_Math" display="inline" id="S5.T1.8.8.1.m1.1"><semantics id="S5.T1.8.8.1.m1.1a"><msub id="S5.T1.8.8.1.m1.1.1" xref="S5.T1.8.8.1.m1.1.1.cmml"><mi id="S5.T1.8.8.1.m1.1.1.2" xref="S5.T1.8.8.1.m1.1.1.2.cmml">D</mi><mn id="S5.T1.8.8.1.m1.1.1.3" xref="S5.T1.8.8.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.1.m1.1b"><apply id="S5.T1.8.8.1.m1.1.1.cmml" xref="S5.T1.8.8.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T1.8.8.1.m1.1.1.1.cmml" xref="S5.T1.8.8.1.m1.1.1">subscript</csymbol><ci id="S5.T1.8.8.1.m1.1.1.2.cmml" xref="S5.T1.8.8.1.m1.1.1.2">ùê∑</ci><cn id="S5.T1.8.8.1.m1.1.1.3.cmml" type="integer" xref="S5.T1.8.8.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.1.m1.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.8.8.1.m1.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> | Low data extrapolation limit |
| <math alttext="\Delta" class="ltx_Math" display="inline" id="S5.T1.9.9.1.m1.1"><semantics id="S5.T1.9.9.1.m1.1a"><mi id="S5.T1.9.9.1.m1.1.1" mathvariant="normal" xref="S5.T1.9.9.1.m1.1.1.cmml">Œî</mi><annotation-xml encoding="MathML-Content" id="S5.T1.9.9.1.m1.1b"><ci id="S5.T1.9.9.1.m1.1.1.cmml" xref="S5.T1.9.9.1.m1.1.1">Œî</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.9.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S5.T1.9.9.1.m1.1d">roman_Œî</annotation></semantics></math> | Optional parameter; shifts ReLU base |</p></table></figure><blockquote><p>üîº This table lists the symbols used in Section 5 of the paper and their corresponding descriptions. It provides a key for understanding the mathematical notation and variables used in the emergence law model presented in that section. The symbols include those representing the amount of finetuning data, the model&rsquo;s pretraining loss, downstream performance, and the parameters used in the emergence law itself.</p><details><summary>read the caption</summary>Table 1: Symbols used in Section¬†5.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Emergence Prediction<div id=emergence-prediction class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#emergence-prediction aria-label=Anchor>#</a></span></h4><p>The concept of &lsquo;Emergence Prediction&rsquo; in the context of large language models (LLMs) is a significant advancement. The core idea revolves around <strong>predicting when a model will exhibit emergent capabilities</strong> on a given task, even before it reaches the necessary scale for such capabilities to manifest. This is a crucial step because emergent capabilities, while desirable, are also unpredictable and potentially dangerous. The approach detailed utilizes finetuning as a crucial tool. <strong>Finetuning smaller LLMs on a specific task systematically shifts the point of emergence</strong> to lower-capacity models, revealing insights into the data and training dynamics needed for emergence. By fitting a parametric function to these results, researchers develop ‚Äòemergence laws‚Äô‚Äîessentially predictive models‚Äîto anticipate emergence in larger models. This process avoids the expensive process of training multiple, large-scale models to identify emergent capabilities, offering significant cost and time savings while mitigating potential risks. The validity of this approach is demonstrated via successful predictions on multiple standard NLP benchmarks, where emergence has already been observed, proving the efficacy of predicting emergence using this novel method. <strong>The ability to predict emergent capabilities in advance allows researchers and developers to make more informed decisions</strong> regarding model development, resource allocation, and safety protocols.</p><h4 class="relative group">Finetuning Effects<div id=finetuning-effects class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#finetuning-effects aria-label=Anchor>#</a></span></h4><p>Finetuning&rsquo;s impact on large language models (LLMs) is a significant area of study. The core concept is that <strong>fine-tuning systematically shifts the point of emergence</strong>, making the capabilities of less powerful models more predictable. By training smaller LLMs on a task and observing how their performance changes with varying amounts of data, researchers can fit a parametric function (an &ldquo;emergence law&rdquo;). This law extrapolates to the low-data limit, effectively predicting when more substantial models will exhibit emergent capabilities on the same task. The study shows that <strong>finetuning data amount directly influences the emergence point</strong>, shifting it towards weaker models with more data. This insight is crucial for predicting downstream capabilities in larger, more costly models which makes it a valuable tool for LLM development and resource allocation. Further research should investigate the underlying mechanisms driving this shift and explore the generalizability of emergence laws across different model architectures and tasks.</p><h4 class="relative group">Emergence Laws<div id=emergence-laws class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#emergence-laws aria-label=Anchor>#</a></span></h4><p>The concept of &ldquo;Emergence Laws&rdquo; in the context of large language models (LLMs) is a significant contribution because it proposes a <strong>systematic way to predict the emergence of capabilities</strong> in future models. Instead of relying on unpredictable jumps in performance, this framework uses finetuning experiments on smaller LLMs to extrapolate how larger models will behave, effectively constructing a predictive model (the &ldquo;emergence law&rdquo;). This is a powerful tool because <strong>it shifts the prediction problem from the realm of highly unpredictable emergent behavior to a more manageable task of fitting a parametric function</strong>, paving the way for earlier and more accurate assessments of future LLM capabilities. The core idea is that finetuning, by shifting the point of emergence, reveals critical data on the underlying scaling behavior; thus, the law allows for predictions even with limited data, provided this data is carefully selected. The application of this approach opens possibilities for <strong>informed decision-making in LLM development</strong>; however, the validity and generalization of such laws require further investigation and validation.</p><h4 class="relative group">Real-World Uses<div id=real-world-uses class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#real-world-uses aria-label=Anchor>#</a></span></h4><p>The &lsquo;Real-World Uses&rsquo; section of this research paper explores the practical applications of the proposed emergence prediction methodology. <strong>Two key applications are highlighted</strong>: 1) evaluating the quality of pretraining data more efficiently and 2) predicting the emergence of complex capabilities in future, more powerful LLMs. The first application offers a cost-effective method for assessing data quality, avoiding the need for extensive large-scale model training. <strong>This is crucial for resource management and iterative development processes.</strong> The second application focuses on anticipating the capabilities of future models, potentially identifying dangerous or unforeseen emergent behaviors, <strong>critical for safety and responsible development.</strong> Both applications highlight the significant impact this work could have on the broader LLM development landscape by improving efficiency and enabling proactive risk mitigation strategies.</p><h4 class="relative group">Future Directions<div id=future-directions class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-directions aria-label=Anchor>#</a></span></h4><p>The paper&rsquo;s exploration of emergence prediction in large language models (LLMs) reveals a promising yet nascent field. <strong>Future directions should prioritize refining data collection strategies to improve prediction accuracy</strong>. This could involve leveraging active learning techniques to focus on the most informative data points. <strong>Addressing the mechanistic understanding of why finetuning shifts the point of emergence</strong> is crucial. Is it accelerating an underlying phase transition or merely surfacing latent capabilities? Further investigation is needed to test the <strong>generalizability of these findings to different LLM architectures and training paradigms</strong>. The current approach may not directly translate to models with significant architectural differences or those trained using alternative methods like distillation. Finally, <strong>extending the predictive capabilities to more complex, safety-relevant capabilities</strong> present in frontier LLMs remains a critical challenge. This will necessitate developing more sophisticated methods beyond simple emergence prediction, perhaps integrating interpretability techniques to gain deeper insights into the inner workings of these advanced models.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x2.png alt></figure></p><blockquote><p>üîº This figure demonstrates the core idea of the paper: predicting the emergence of capabilities in large language models (LLMs) by finetuning smaller, pre-emergence models. The left panel shows how finetuning shifts the point at which a capability emerges (the &rsquo;emergence point&rsquo;) to lower-performing models. The more data used for finetuning, the greater this shift. By fitting a mathematical function (&rsquo;emergence law&rsquo;) to this relationship, the authors can extrapolate to predict when emergence will occur in larger, unseen models, even using very limited data. The right panel shows the success rate of this method, demonstrating that they can accurately predict the emergence point up to 4 times the computational resources of the models used to build the prediction.</p><details><summary>read the caption</summary>Figure 2: Left: we predict emergence in the few-shot setting by leveraging information about how ‚Äúpre-emergence‚Äù models behave after finetuning. Our key finding is that finetuning effectively shifts the point of emergence from stronger to weaker models. Moreover, by varying the amount of finetuning data, the emergence point is shifted accordingly. We can use this finding to predict when few-shot emergence will occur by fitting a parametric function to the results (i.e., emergence law) and then taking a limit. Right: using this approach, we can predict emergence up to 4x the FLOPs in advance.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x3.png alt></figure></p><blockquote><p>üîº This figure shows how finetuning affects the emergence of capabilities in LLMs. The left panel plots the few-shot and finetuned performance of intermediate-sized LLMs (3B, 7B, and 13B parameters) on two tasks (MMLU and GSM8K). It demonstrates that finetuning systematically shifts the point at which a capability emerges towards less powerful models. Importantly, the degree of this shift is consistent across different model sizes at the same pretraining loss. The right panel shows how the amount of data used for finetuning further influences this shift, with more data leading to an even earlier emergence on less capable models.</p><details><summary>read the caption</summary>Figure 3: Left: the finetuned and few-shot performance of intermediate LLM checkpoints. We plot downstream accuracy against pretraining loss for all 3B, 7B, and 13B intermediate OpenLLaMA V1 checkpoints on MMLU and GSM8K. We see that the point of emergence is systematically shifted towards weaker LLMs after finetuning. Additionally, the magnitude of the shift is consistent across all model sizes at the same pretraining loss. Right: varying the amount of finetuning data. We finetune the 3B intermediate checkpoints on subsets of the full finetuning data. We see that as we increase the amount of finetuning data, the point of emergence shifts further towards less capable LLMs.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x4.png alt></figure></p><blockquote><p>üîº Figure 4 presents the maximum likelihood estimations of emergence points from the emergence law model. The grey line represents the extrapolated prediction of the emergence point for each task (MMLU, GSM8K, CSQA, CoLA) in a few-shot setting. The multi-colored lines show the model&rsquo;s fit to the data using different amounts of finetuning data. Each colored line represents the shift in the emergence point as the amount of finetuning data changes. The figure highlights the model&rsquo;s accurate predictions of the emergence points, typically within a margin of 0.1 nats, often much less. The full ReLU function is displayed for visual clarity, though the primary focus is on the precise point of emergence (the &rsquo;elbow&rsquo; of the ReLU). A subset of the data is shown for clarity; Appendix A.11 contains all data used in the model fitting.</p><details><summary>read the caption</summary>Figure 4: Our MLE emergence law predictions on each task. The grey line is our extrapolated prediction and the multi-color lines are the fit. While our focus is on predicting the specific point of emergence (e.g., the ReLU elbow), we plot the full ReLU for visual clarity. We see that across all tasks, we are able to successfully predict the point of emergence within 0.1 nats and in many cases much less than that. For visual clarity, we plot a subset of the data used for fitting (see Appendix¬†A.11 for all).</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x5.png alt></figure></p><blockquote><p>üîº This figure displays the cumulative distribution function (CDF) plots illustrating the emergence posterior for two NLP tasks: GSM8K and MMLU. The x-axis represents the point of emergence (in terms of pretraining loss), and the y-axis shows the cumulative probability. The peak of the CDF&rsquo;s slope indicates the most likely point of emergence, which is close to the actual observed point. The moderately long tail suggests uncertainty in precisely pinpointing the emergence point, implying the model&rsquo;s confidence in prediction isn&rsquo;t absolute but is concentrated near the actual emergence point.</p><details><summary>read the caption</summary>Figure 5: The cumulative distribution function (CDF) of our emergence posterior on GSM8K and MMLU (see Appendix¬†A.11 for all tasks). The CDF‚Äôs slope peaks at the mode of the distribution. We see that the distribution spikes near the true emergence point, followed by a moderately long tail.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/extracted/6021902/figures/emergence_cdf_v4.png alt></figure></p><blockquote><p>üîº This figure presents ablation studies on the emergence prediction method. The left panel compares three different functional forms for the emergence law: a log power law, a power law, and a log power law without the delta parameter. The results show that removing the log term significantly worsens the prediction accuracy, while the delta parameter has minimal impact. The right panel investigates the effect of varying the low-data extrapolation limit (D0) on prediction accuracy. The results indicate that within a reasonable range (less than 10 times the number of few-shot examples), the choice of D0 has minimal effect on accuracy.</p><details><summary>read the caption</summary>Figure 6: Ablations. The bar height represents the MLE prediction error (lower is better). The error bar represents the 5th and 95th percentile errors obtained from MCMC posterior sampling, and the circle is the median. Left: comparing emergence law functional forms. ‚ÄúLog Power Law‚Äù and ‚ÄúPower Law‚Äù refer to different functional forms for EŒ∏‚Å¢(D)subscriptùê∏ùúÉùê∑E_{\theta}(D)italic_E start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_D ). ‚ÄúNo Few-shot‚Äù is the ‚ÄúLog Power Law‚Äù without the ŒîŒî\Deltaroman_Œî parameter. We see that removing the log worsens predictions, and the ŒîŒî\Deltaroman_Œî has a minimal effect on accuracy. Right: varying the low data extrapolation limit D0subscriptùê∑0D_{0}italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. NùëÅNitalic_N is the number of few-shot examples. We see that within a reasonable range (e.g., <10‚Å¢Nabsent10ùëÅ<10N< 10 italic_N) the value of D0subscriptùê∑0D_{0}italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT has minimal impact on accuracy.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x6.png alt></figure></p><blockquote><p>üîº Figure 7 investigates how far in advance the proposed emergence prediction method can accurately predict the emergence point in terms of FLOPs (floating point operations). By holding out checkpoints from the model training process, the study assesses the predictive capability for various benchmarks. Each blue bar in the figure indicates the FLOPs required for training the most capable model involved in the emergence prediction. The blue circle marks the median of the MCMC (Markov Chain Monte Carlo) posterior distribution of predicted emergence points. Error bars illustrate the 5th to 95th percentiles of this distribution. Predictions with errors exceeding 0.1 nats are deemed unsuccessful and marked with red crosses. The analysis reveals that for MMLU and GSM8K, accurate predictions can be made using models trained with significantly fewer FLOPs (4-5x and 4x, respectively) than those needed for the earliest post-emergence models. However, for CoLA and CommonsenseQA, the predictive ability is more limited, with predictions accurate only up to approximately 2x the required FLOPs.</p><details><summary>read the caption</summary>Figure 7: How far in advance can we predict emergence? We hold out checkpoints to see how far in advance, in pretraining FLOPS, we can successfully predict emergence. The y position of each blue bar corresponds to the FLOPS needed to train the most capable model used for fitting. The blue circle represents the median of the MCMC posterior, and the error bar represents the 5th to 95th percentiles. If the MLE prediction error is >0.1absent0.1>0.1> 0.1 nats, we consider that prediction unsuccessful and denote it with a red-cross222In some cases the failed predictions would be well off the plot and we want to keep the axis bounds constrained for presentation clarity. We include the full results in Appendix¬†A.6.. On MMLU we can predict emergence using models trained with ‚àº1022similar-toabsentsuperscript1022\sim 10^{22}‚àº 10 start_POSTSUPERSCRIPT 22 end_POSTSUPERSCRIPT FLOPS, but no fewer. The earliest post-emergence model on MMLU was trained with ‚àº5‚àó1022similar-toabsent5superscript1022\sim 5*10^{22}‚àº 5 ‚àó 10 start_POSTSUPERSCRIPT 22 end_POSTSUPERSCRIPT FLOPS, hence we can predict 4-5x the FLOPS in advance on MMLU. On GSM8K we also predict 4x the FLOPS in advance333We count the earliest successful prediction for this calculation. However, GSM8K has a failed prediction between two successes, likely due to noise. In Appendix¬†A.6, we see that this failed prediction is just outside the success threshold, with much of the error bar falling well within 0.1 nats.. However, on CoLA and CommonsenseQA we only predict 2x the FLOPS in advance.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x7.png alt></figure></p><blockquote><p>üîº Figure 8 presents a comparison of the emergence behavior of OpenLLaMA V1 and V2 language models on two benchmark tasks: MMLU and CommonsenseQA. Emergence refers to the point at which a model&rsquo;s performance on a task suddenly surpasses random chance. The x-axis represents the C4 validation loss, a measure of the model&rsquo;s performance during pretraining. The y-axis shows the model&rsquo;s accuracy on the respective benchmark task. The plot demonstrates that for both MMLU and CommonsenseQA, the OpenLLaMA V2 models achieve emergence at a lower C4 validation loss than the V1 models. This suggests that the pretraining data used for OpenLLaMA V2 is of higher quality, leading to the faster emergence of capabilities in the model. The difference in emergence point highlights the impact of pretraining data quality on model performance.</p><details><summary>read the caption</summary>Figure 8: Comparing OpenLLaMA V1 and V2 emergence. On both MMLU and CommonsenseQA, the V2 models emerge first, suggesting that the V2 pretraining data is likely higher quality.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x8.png alt></figure></p><blockquote><p>üîº Figure 9 presents a comparison of emergence predictions for two versions of the OpenLLaMA language model (V1 and V2) when evaluated on the MMLU benchmark. The left panel shows maximum likelihood estimates (MLE) of the emergence point (the model scale at which capabilities suddenly improve) for each model version. The right panel displays cumulative distribution functions (CDFs) of the emergence point, illustrating the probability distribution for where emergence might occur. A key observation is that the model trained on the V2 data is predicted to demonstrate emergence at a smaller model scale than V1, aligning with the expectation based on the improved pretraining data quality indicated in the paper. Appendix A.11 provides additional visualizations.</p><details><summary>read the caption</summary>Figure 9: Comparing emergence predictions for OpenLLaMA V1 and V2 on MMLU. We plot the MLE predictions (left) and the CDFs (right) for both series. While our focus is on predicting the specific point of emergence (e.g., the ReLU elbow), we plot the full ReLU for visual clarity. The V2 models are correctly predicted to emerge before V1, providing initial evidence that our approach can be used to evaluate data quality. See Appendix¬†A.11 for plots with all the data used for fitting.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/extracted/6021902/figures/comparing_v1_v2_mle_cdf_v5.png alt></figure></p><blockquote><p>üîº Figure 10 presents the findings on predicting the emergence of capabilities in LLaMA 2 models on the APPS benchmark. The left panel displays the maximum likelihood estimation (MLE) of the emergence point, represented as a function of pretraining loss. The right panel converts this loss-based prediction into a parameter count, leveraging the LLaMA 2 scaling law. The green point indicates the MLE prediction, with the error bar showing the 5th to 95th percentiles derived from Markov Chain Monte Carlo (MCMC) posterior sampling. This analysis predicts that emergence is most likely to occur around 325 billion parameters, but with substantial uncertainty ranging from 250B to 500B parameters.</p><details><summary>read the caption</summary>Figure 10: Predicting emergence on APPS with LLaMA 2. On the left, we plot our MLE prediction. On the right, we convert this loss-based prediction into parameter count under the LLaMA 2 scaling law. The green point represents the MLE prediction, and the error bar represents the 5th to 95th percentiles under the MCMC posterior. We predict that emergence would most likely occur at ‚àº325similar-toabsent325\sim 325‚àº 325B parameters with a wide error bar from ‚àº250similar-toabsent250\sim 250‚àº 250B to ‚àº500similar-toabsent500\sim 500‚àº 500B parameters. For visual clarity, the left plot includes a subset of the full data used for fitting (see Appendix¬†A.11 for all).</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x9.png alt></figure></p><blockquote><p>üîº This figure shows the results of two experiments to determine whether prefix tuning or the number of shots used affects the point at which capabilities emerge in language models. The left panel shows that while prefix tuning improves performance after emergence, it does not shift the point at which capabilities emerge. The right panel shows that using zero-shot prompting instead of five-shot prompting also does not shift the emergence point. This suggests that techniques like prefix tuning and altering the number of shots are not effective at changing when capabilities emerge.</p><details><summary>read the caption</summary>Figure 11: One the left we compare full fine-tuning against continuous prefix tuning on MMLU. We find that prefix tuning provides effectively no shift to the point of emergence, despite improving the performance of post-emergence models. On the right we compare 0-shot verses 5-shot prompting on MMLU. We see that using fewer shots has no meaningful effect on the point of emergence. Together these results suggest that the ability for prompt tuning to shift the point of emergence is very limited.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x10.png alt></figure></p><blockquote><p>üîº This figure compares the impact of different LoRA ranks (1, 2, 4, 64) and full finetuning on the emergence of capabilities in a language model. It shows how varying the rank affects the point at which the model begins to perform non-randomly (emergence point) on the MMLU benchmark. The key finding is that even low-rank LoRA finetuning (rank 1) significantly shifts this emergence point, almost as much as full finetuning. This demonstrates the effectiveness of LoRA in rapidly achieving substantial performance gains, even in the context of emergent capabilities.</p><details><summary>read the caption</summary>Figure 12: Comparing LoRA finetuning, with rank 1, 2, 4, and 64 against full finetuning on MMLU. We see that LoRA finetuning even with rank 1 shifts the point of emergence to a comparable degree to that of full finetuning.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x11.png alt></figure></p><blockquote><p>üîº This figure demonstrates that the phenomenon of emergence in large language models is not solely an artifact of using discontinuous metrics. It shows that even when evaluating model performance using a continuous metric (LLM log-probability), rather than a discrete metric (accuracy), the characteristic &rsquo;emergent&rsquo; behavior‚Äîa sudden improvement beyond random chance‚Äîis still observed. Two standard benchmarks, MMLU (5-shot) and CommonsenseQA (6-shot), are used for this demonstration.</p><details><summary>read the caption</summary>Figure 13: On a standard 5-shot MMLU and 6-shot CommonsenseQA (CSQA) evaluation, we observe emergence using both the standard correct answer accuracy evaluation and a continuous LLM log-probability metric.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x12.png alt></figure></p><blockquote><p>üîº Figure 14 presents a detailed analysis of how model size affects both the emergence point and post-emergence performance scaling in large language models (LLMs). The emergence point, where performance significantly improves beyond random chance, and the scaling of performance after emergence are plotted against the pretraining loss for four standard NLP benchmarks (MMLU, GSM8K, CommonsenseQA, and CoLA). Data is shown for three model sizes (3B, 7B, and 13B parameters) using both few-shot and finetuned settings. The key finding is the consistent relationship between pretraining loss and performance across all model sizes and settings, indicating that pretraining loss is a reliable indicator of LLM capabilities.</p><details><summary>read the caption</summary>Figure 14: We plot few-shot and full data finetuning performance as a function of pretraining loss using all 3B, 7B, and 13B model checkpoints for all tasks. We see that both the point of emergence and the downstream performance scaling thereafter, as a function of pretraining loss, is consistent across model size in both the few-shot and finetuned setting.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x13.png alt></figure></p><blockquote><p>üîº Figure 15 shows the scaling law fit for the LLaMA 2 series of models. The x-axis represents the parameter count (in billions), and the y-axis shows the pretraining loss. The plot displays the data points for the different LLaMA 2 models (7B, 13B, 34B, and 70B parameters) along with the fitted scaling law curve. The equation of the fitted curve is also provided, showing the relationship between the parameter count (N) and pretraining loss (L(N)). The figure demonstrates that the pretraining loss of LLaMA 2 models closely follows the predicted scaling law, indicating a good fit.</p><details><summary>read the caption</summary>Figure 15: We plot our scaling law fit for the LLaMA 2 series of models. We also include the learned values for our final fit on the plot. In this case NùëÅNitalic_N corresponds to parameter count in billions. We see that the LLaMA 2 models are well modeled by our scaling law.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x14.png alt></figure></p><blockquote><p>üîº Figure 16 shows the maximum likelihood predictions of the emergence point for four NLP tasks (GSM8K, MMLU, CSQA, CoLA) using the proposed emergence law. The figure displays the model&rsquo;s fit across different amounts of finetuning data (shown in multiple colors) for each task. A gray line represents the model&rsquo;s extrapolated prediction for the emergence point in the few-shot setting. The results demonstrate the accuracy of the emergence law in predicting the point where a model exhibits a sudden improvement beyond random chance performance, typically within a margin of 0.1 nats and often significantly smaller.</p><details><summary>read the caption</summary>Figure 16: We plot the maximum likelihood predictions from our emergence law on each task. These plots include results from every finetuning run used for fitting the emergence law. The grey line represents our extrapolated prediction and the multi-color lines correspond to the fit produced by the emergence law for the various data levels. We see that across all tasks we are able to successfully predict the point of emergence within 0.1 nats and in many cases much less than that.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x15.png alt></figure></p><blockquote><p>üîº This figure compares the emergence predictions for OpenLLaMA V1 and V2 language models on the MMLU benchmark. The x-axis represents the C4 validation loss, a measure of the model&rsquo;s performance on a general language understanding task. The y-axis shows the accuracy on the MMLU benchmark. Multiple colored lines show the performance of models fine-tuned with varying amounts of data, demonstrating how the point of emergence shifts depending on the amount of training data. A grey line represents an extrapolated prediction of the emergence point using a fitted emergence law. The results show that the emergence point for both V1 and V2 models can be predicted with high accuracy (within 0.1 nats). This suggests that the methodology can effectively anticipate when a model will demonstrate emergent capabilities.</p><details><summary>read the caption</summary>Figure 17: We plot the maximum likelihood predictions from our emergence law with OpenLLaMA V1 (left) and OpenLLaMA V2 (right) on MMLU. We plot C4 Validation loss on the x-axis. These plots include results from every finetuning run used for fitting the emergence law. The grey line represents our extrapolated prediction and the multi-color lines correspond to the fit produced by the emergence law for the various data levels. We see that in both cases we are able to successfully predict the point of emergence within 0.1 nats.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x16.png alt></figure></p><blockquote><p>üîº Figure 18 displays the results of applying an emergence prediction model to the APPS benchmark using LLaMA 2. The left panel shows the maximum likelihood estimate of emergence (grey line) alongside model fits using different amounts of training data (colored lines). The right panel shows the cumulative distribution function (CDF) of the emergence point&rsquo;s posterior distribution, indicating confidence and uncertainty in the prediction. The model predicts that emergence will likely occur at a pretraining loss roughly 0.15 nats beyond the LLaMA 2 70B parameter model.</p><details><summary>read the caption</summary>Figure 18: We plot the MLE prediction (left) and MCMC CDF (right) for our emergence law fit using LLaMA 2 on APPS. The left plot includes results from every finetuning run used for fitting the emergence law. The grey line represents our extrapolated prediction and the multi-color lines correspond to the fit produced by the emergence law for the various data levels. We see that our emergence law predicts emergence roughly 0.15 nats beyond the LLaMA 2 70B model.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.16035/x17.png alt></figure></p><blockquote><p>üîº This figure displays the cumulative distribution function (CDF) of the model&rsquo;s predicted emergence point for four different NLP tasks (GSM8K, MMLU, CSQA, and CoLA). The x-axis represents the model&rsquo;s pretraining loss, while the y-axis shows the cumulative probability of emergence. Each CDF curve shows a sharp increase near the actual emergence point (marked by a star), indicating high confidence in the prediction near the actual emergence point. However, the CDFs also have a moderately long tail extending beyond the emergence point, demonstrating some uncertainty in precisely predicting the point at which the capabilities emerge, especially when the model displays low performance before exhibiting emergent behavior. The stars represent the actual few-shot performance data, illustrating the pre-emergence phase&rsquo;s near-random accuracy and subsequent sharp improvement after the emergence threshold.</p><details><summary>read the caption</summary>Figure 19: We plot the cumulative distribution function of our estimated posterior distribution over the point of emergence on each task. The stars correspond to few-shot performance on the task and represent the true emergence curve. The point at which the slope of the CDF peaks represents the mode of the distribution. We see across all tasks that the distribution spikes near the true point of emergence and is followed by a moderately long tail.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Ablation Setting</th><th>GSM8K</th><th>MMLU</th><th>CSQA</th><th>CoLA</th></tr></thead><tbody><tr><td>Full Data</td><td>0.022 [0.004, 0.170]</td><td>0.041 [0.011, 0.055]</td><td>0.003 [0.001, 0.045]</td><td>0.064 [0.030, 0.121]</td></tr><tr><td>-1 Smallest Subset</td><td>0.014 [0.002, 0.051]</td><td>0.022 [0.010, 0.031]</td><td>0.051 [0.037, 0.084]</td><td>0.071 [0.024, 0.097]</td></tr><tr><td>-2 Smallest Subset</td><td>0.047 [0.018, 0.063]</td><td>0.025 [0.003, 0.032]</td><td>0.087 [0.063, 0.129]</td><td>0.634 [0.577, 0.711]</td></tr><tr><td>-3 Smallest Subset</td><td>0.005 [0.002, 0.050]</td><td>0.001 [0.002, 0.025]</td><td>0.988 [0.913, 1.099]</td><td>1.513 [1.291, 1.698]</td></tr><tr><td>-1 Largest Subset</td><td>0.022 [0.002, 0.032]</td><td>0.034 [0.024, 0.034]</td><td>0.045 [0.003, 0.096]</td><td>0.036 [0.007, 0.090]</td></tr><tr><td>-2 Largest Subset</td><td>0.005 [0.002, 0.054]</td><td>1.017 [1.482, 1.985]</td><td>0.057 [0.056, 0.059]</td><td>0.004 [0.002, 0.058]</td></tr><tr><td>-3 Largest Subset</td><td>0.016 [0.002, 0.083]</td><td>0.332 [1.371, 4.200]</td><td>0.089 [0.077, 0.098]</td><td>0.044 [0.019, 0.143]</td></tr><tr><td>Only Subset Sample 1</td><td>0.006 [0.001, 0.031]</td><td>0.244 [0.223, 0.489]</td><td>1.557 [1.616, 2.573]</td><td>0.179 [0.125, 0.269]</td></tr><tr><td>Only Subset Sample 2</td><td>0.026 [0.002, 0.067]</td><td>0.073 [0.059, 0.077]</td><td>0.035 [0.004, 0.102]</td><td>0.034 [0.022, 0.047]</td></tr><tr><td>Last 6 Checkpoints</td><td>0.010 [0.003, 0.113]</td><td>0.041 [0.007, 0.049]</td><td>0.075 [0.063, 0.301]</td><td>0.118 [0.087, 0.167]</td></tr><tr><td>Last 5 Checkpoints</td><td>0.047 [0.048, 0.176]</td><td>0.032 [0.020, 0.038]</td><td>1.165 [1.070, 1.728]</td><td>0.130 [0.099, 0.170]</td></tr><tr><td>Last 4 Checkpoints</td><td>0.080 [0.072, 4.925]</td><td>0.030 [0.001, 0.042]</td><td>1.734 [1.555, 2.308]</td><td>0.076 [0.052, 0.111]</td></tr><tr><td>Last 3 Checkpoints</td><td>0.159 [0.124, 0.703]</td><td>0.013 [0.002, 0.059]</td><td>0.986 [0.802, 1.249]</td><td>0.039 [0.004, 0.077]</td></tr><tr><td>Last 6 Checkpoints, Every Other Even</td><td>0.019 [0.003, 0.162]</td><td>0.070 [0.059, 0.075]</td><td>1.663 [1.667, 1.781]</td><td>0.033 [0.007, 0.050]</td></tr><tr><td>Last 6 Checkpoints, Every Other Odd</td><td>0.044 [0.041, 0.126]</td><td>0.040 [0.024, 0.045]</td><td>0.037 [0.031, 0.191]</td><td>0.224 [0.190, 0.287]</td></tr><tr><td>-1 Last Checkpoints</td><td>0.069 [0.003, 0.149]</td><td>0.043 [0.023, 0.055]</td><td>0.985 [0.858, 1.800]</td><td>0.026 [0.004, 0.079]</td></tr><tr><td>-2 Last Checkpoints</td><td>0.087 [0.003, 0.176]</td><td>0.076 [0.046, 0.089]</td><td>0.102 [0.098, 0.104]</td><td>0.165 [0.098, 0.242]</td></tr><tr><td>-3 Last Checkpoints</td><td>0.110 [0.010, 0.468]</td><td>0.664 [0.500, 0.959]</td><td>0.616 [0.510, 0.822]</td><td>2.217 [2.031, 2.407]</td></tr><tr><td>-4 Last Checkpoints</td><td>0.044 [0.005, 0.089]</td><td>2.308 [2.347, 57.068]</td><td>0.581 [0.546, 1.169]</td><td>1.039 [0.905, 1.298]</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of ablations conducted on the emergence prediction method. The main goal is to assess the impact of using different subsets of finetuning data and varying numbers of model checkpoints when fitting the emergence law. The table shows the absolute error between the predicted and actual emergence points, along with 5th and 95th percentile errors from MCMC sampling. Errors larger than 0.1 nats are marked in red (indicating failure). The top row shows the baseline using all data, the middle rows show results with various amounts of held-out finetuning data to analyze data selection&rsquo;s impact, and the bottom rows vary the number of model checkpoints used for fitting to determine the minimum number needed for accurate predictions. More details on the ablations are in Appendix A.6.</p><details><summary>read the caption</summary>Table 2: Ablating the effect of holding out different finetuning subsets and model checkpoints when fitting the emergence law. We present the absolute error between the maximum likelihood predicted point of emergence and the ground-truth. In brackets we include the 5th and 95th percentile of prediction errors produced by our MCMC posterior sampling. We consider fits where the maximum likelihood prediction is greater than 0.1 nats from the ground-truth to be failures and highlight these cases in red; otherwise we highlight in green. In the top row we present results for the fit obtained using all finetuning data amounts and model checkpoints. In the middle rows (e.g., ‚Äú-1 Smallest Subset‚Äù to ‚ÄúOnly Subset Sample 2‚Äù), we present ablations in which we hold out various finetuning data subsets, so as to understand the effect of our data subset selection methodology on our predictions. Finally, in the bottom rows, we present ablations in which we hold out various model checkpoints, so as to understand how many checkpoints are needed to obtain good predictions (e.g., ‚ÄúLast 6 Checkpoints‚Äù to ‚Äú-4 Last Checkpoint‚Äù). We describe each ablation in more detail in Appendix¬†A.6.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Task</th><th>Method</th><th>5%</th><th>10%</th><th>25%</th><th>50%</th><th>75%</th><th>90%</th><th>95%</th><th>MLE</th><th>GT</th></tr></thead><tbody><tr><td>GSM8K</td><td>MCMC</td><td>1.813</td><td>1.852</td><td>1.900</td><td>1.937</td><td>1.970</td><td>1.992</td><td>2.003</td><td>2.006</td><td>1.984</td></tr><tr><td></td><td>Bootstrap</td><td>1.978</td><td>1.984</td><td>1.995</td><td>2.007</td><td>2.021</td><td>2.031</td><td>2.036</td><td></td><td></td></tr><tr><td>MMLU</td><td>MCMC</td><td>1.825</td><td>1.828</td><td>1.837</td><td>1.847</td><td>1.858</td><td>1.866</td><td>1.869</td><td>1.855</td><td>1.814</td></tr><tr><td></td><td>Bootstrap</td><td>1.818</td><td>1.825</td><td>1.836</td><td>1.848</td><td>1.859</td><td>1.867</td><td>1.871</td><td></td><td></td></tr><tr><td>CSQA</td><td>MCMC</td><td>1.781</td><td>1.810</td><td>1.821</td><td>1.829</td><td>1.835</td><td>1.840</td><td>1.843</td><td>1.830</td><td>1.827</td></tr><tr><td></td><td>Bootstrap</td><td>1.723</td><td>1.736</td><td>1.815</td><td>1.835</td><td>1.846</td><td>1.857</td><td>1.863</td><td></td><td></td></tr><tr><td>CoLA</td><td>MCMC</td><td>1.712</td><td>1.724</td><td>1.742</td><td>1.761</td><td>1.779</td><td>1.795</td><td>1.804</td><td>1.769</td><td>1.833</td></tr><tr><td></td><td>Bootstrap</td><td>1.738</td><td>1.746</td><td>1.758</td><td>1.770</td><td>1.782</td><td>1.791</td><td>1.798</td><td></td><td></td></tr><tr><td>MMLU C4 V1</td><td>MCMC</td><td>2.207</td><td>2.221</td><td>2.241</td><td>2.246</td><td>2.255</td><td>2.259</td><td>2.261</td><td>2.254</td><td>2.226</td></tr><tr><td></td><td>Bootstrap</td><td>2.183</td><td>2.200</td><td>2.216</td><td>2.228</td><td>2.238</td><td>2.246</td><td>2.250</td><td></td><td></td></tr><tr><td>MMLU C4 V2</td><td>MCMC</td><td>2.264</td><td>2.275</td><td>2.289</td><td>2.306</td><td>2.310</td><td>2.316</td><td>2.320</td><td>2.311</td><td>2.318</td></tr><tr><td></td><td>Bootstrap</td><td>2.249</td><td>2.257</td><td>2.272</td><td>2.284</td><td>2.296</td><td>2.305</td><td>2.311</td><td></td><td></td></tr><tr><td>APPS</td><td>MCMC</td><td>1.324</td><td>1.332</td><td>1.344</td><td>1.357</td><td>1.370</td><td>1.380</td><td>1.386</td><td>1.361</td><td>‚Äî</td></tr><tr><td></td><td>Bootstrap</td><td>1.285</td><td>1.304</td><td>1.330</td><td>1.352</td><td>1.370</td><td>1.385</td><td>1.393</td><td></td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares two methods for estimating the uncertainty of emergence predictions: Markov Chain Monte Carlo (MCMC) and bootstrapping. For each task (MMLU, GSM8K, CSQA, CoLA, and APPS), it shows the 5th, 10th, 25th, 50th, 75th, 90th, and 95th percentiles of the emergence point&rsquo;s pretraining loss, as determined by each method. The maximum likelihood estimate (MLE) and the ground truth are also provided for comparison. The results demonstrate that both methods generally produce similar uncertainty estimates. The table is organized into three sections: the first shows results for the tasks in Section 6.2, the second shows results for the data quality experiments in Section 7.1, and the third presents the results for the APPS experiment in Section 7.2. The &lsquo;MMLU C4 V1&rsquo; and &lsquo;MMLU C4 V2&rsquo; rows refer to the OpenLLaMA model versions used.</p><details><summary>read the caption</summary>Table 3: Comparing emergence prediction uncertainty estimates obtained via MCMC and bootstrapping. On each task, we present seven a range of percentiles for the point of emergence in terms of pretraining loss for each distribution. We also present the maximum likelihood prediction (‚ÄúMLE‚Äù), and the ground-truth (‚ÄúGT‚Äù) point of emergence. We see that both methods generally produce similar distributions. In the top section we present the uncertainties for each task used in Section¬†6.2. In the middle we include uncertainties for the data quality experiments in Section¬†7.1. ‚ÄúMMLU C4 V1‚Äù refers to the OpenLLaMA V1 fit and ‚ÄúMMLU C4 V2‚Äù refers to the V2 fit. At the bottom, we include uncertainties for the APPS experiment in Section¬†7.2.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-bf71e014a663ba19c56f2df1794c1fe4 class=gallery><img src=https://ai-paper-reviewer.com/2411.16035/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.16035/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/&amp;title=Predicting%20Emergent%20Capabilities%20by%20Finetuning" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/&amp;text=Predicting%20Emergent%20Capabilities%20by%20Finetuning" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/&amp;subject=Predicting%20Emergent%20Capabilities%20by%20Finetuning" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2411.16035/index.md",oid_likes="likes_paper-reviews/2411.16035/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2411.16173/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-11-25T00:00:00+00:00>25 November 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2411.16819/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Pathways on the Image Manifold: Image Editing via Video Generation</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-11-25T00:00:00+00:00>25 November 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>