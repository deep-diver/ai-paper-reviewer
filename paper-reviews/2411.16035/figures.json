[{"figure_path": "https://arxiv.org/html/2411.16035/x1.png", "caption": "Figure 1: We find that task-specific finetuning systematically shifts the point of emergence towards less capable models. Motivated by this finding, we develop an emergence law, which models how the point of emergence shifts as a function of the amount of finetuning data. Using this emergence law, we can then extrapolate a prediction for the point of emergence in the few-shot setting.", "description": "This figure illustrates the effect of finetuning on the emergence of capabilities in large language models. The left panel shows that for a specific task, finetuning a model with increasing amounts of data shifts the point at which it starts showing non-trivial performance (emergence point) towards models with lower capabilities.  The right panel introduces a novel method to predict when few-shot emergence will occur.  An \"emergence law\" is proposed \u2013 a parametric function that models how the emergence point shifts as a function of finetuning data. Using this law, the emergence point for few-shot learning can be predicted by extrapolating to the limit of small amounts of finetuning data. This allows researchers to anticipate the capabilities of future, larger models.", "section": "4 How does Finetuneing Interact with Emergence?"}, {"figure_path": "https://arxiv.org/html/2411.16035/x2.png", "caption": "Figure 2: Left: we predict emergence in the few-shot setting by leveraging information about how \u201cpre-emergence\u201d models behave after finetuning. Our key finding is that finetuning effectively shifts the point of emergence from stronger to weaker models. Moreover, by varying the amount of finetuning data, the emergence point is shifted accordingly. We can use this finding to predict when few-shot emergence will occur by fitting a parametric function to the results (i.e., emergence law) and then taking a limit. Right: using this approach, we can predict emergence up to 4x the FLOPs in advance.", "description": "This figure demonstrates the core idea of the paper: predicting the emergence of capabilities in large language models (LLMs) by finetuning smaller, pre-emergence models.  The left panel shows how finetuning shifts the point at which a capability emerges (the \"emergence point\") to lower-performing models. The more data used for finetuning, the greater this shift. By fitting a mathematical function (\"emergence law\") to this relationship, the authors can extrapolate to predict when emergence will occur in larger, unseen models, even using very limited data. The right panel shows the success rate of this method, demonstrating that they can accurately predict the emergence point up to 4 times the computational resources of the models used to build the prediction.", "section": "4 How does Finetuning Interact with Emergence?"}, {"figure_path": "https://arxiv.org/html/2411.16035/x3.png", "caption": "Figure 3: Left: the finetuned and few-shot performance of intermediate LLM checkpoints. We plot downstream accuracy against pretraining loss for all 3B, 7B, and 13B intermediate OpenLLaMA V1 checkpoints on MMLU and GSM8K. We see that the point of emergence is systematically shifted towards weaker LLMs after finetuning. Additionally, the magnitude of the shift is consistent across all model sizes at the same pretraining loss. Right: varying the amount of finetuning data. We finetune the 3B intermediate checkpoints on subsets of the full finetuning data. We see that as we increase the amount of finetuning data, the point of emergence shifts further towards less capable LLMs.", "description": "This figure shows how finetuning affects the emergence of capabilities in LLMs.  The left panel plots the few-shot and finetuned performance of intermediate-sized LLMs (3B, 7B, and 13B parameters) on two tasks (MMLU and GSM8K). It demonstrates that finetuning systematically shifts the point at which a capability emerges towards less powerful models.  Importantly, the degree of this shift is consistent across different model sizes at the same pretraining loss. The right panel shows how the amount of data used for finetuning further influences this shift, with more data leading to an even earlier emergence on less capable models.", "section": "4 How does Finetuneing Interact with Emergence?"}, {"figure_path": "https://arxiv.org/html/2411.16035/x4.png", "caption": "Figure 4: Our MLE emergence law predictions on each task. The grey line is our extrapolated prediction and the multi-color lines are the fit. While our focus is on predicting the specific point of emergence (e.g., the ReLU elbow), we plot the full ReLU for visual clarity. We see that across all tasks, we are able to successfully predict the point of emergence within 0.1 nats and in many cases much less than that. For visual clarity, we plot a subset of the data used for fitting (see Appendix\u00a0A.11 for all).", "description": "Figure 4 presents the maximum likelihood estimations of emergence points from the emergence law model.  The grey line represents the extrapolated prediction of the emergence point for each task (MMLU, GSM8K, CSQA, CoLA) in a few-shot setting. The multi-colored lines show the model's fit to the data using different amounts of finetuning data.  Each colored line represents the shift in the emergence point as the amount of finetuning data changes.  The figure highlights the model's accurate predictions of the emergence points, typically within a margin of 0.1 nats, often much less. The full ReLU function is displayed for visual clarity, though the primary focus is on the precise point of emergence (the 'elbow' of the ReLU).  A subset of the data is shown for clarity; Appendix A.11 contains all data used in the model fitting.", "section": "5 Scaling Laws for Emergence Prediction"}, {"figure_path": "https://arxiv.org/html/2411.16035/x5.png", "caption": "Figure 5: The cumulative distribution function (CDF) of our emergence posterior on GSM8K and MMLU (see Appendix\u00a0A.11 for all tasks). The CDF\u2019s slope peaks at the mode of the distribution. We see that the distribution spikes near the true emergence point, followed by a moderately long tail.", "description": "This figure displays the cumulative distribution function (CDF) plots illustrating the emergence posterior for two NLP tasks: GSM8K and MMLU. The x-axis represents the point of emergence (in terms of pretraining loss), and the y-axis shows the cumulative probability.  The peak of the CDF's slope indicates the most likely point of emergence, which is close to the actual observed point. The moderately long tail suggests uncertainty in precisely pinpointing the emergence point, implying the model's confidence in prediction isn't absolute but is concentrated near the actual emergence point.", "section": "Evaluating the Emergence Law"}, {"figure_path": "https://arxiv.org/html/2411.16035/extracted/6021902/figures/emergence_cdf_v4.png", "caption": "Figure 6: Ablations. The bar height represents the MLE prediction error (lower is better). The error bar represents the 5th and 95th percentile errors obtained from MCMC posterior sampling, and the circle is the median. Left: comparing emergence law functional forms. \u201cLog Power Law\u201d and \u201cPower Law\u201d refer to different functional forms for E\u03b8\u2062(D)subscript\ud835\udc38\ud835\udf03\ud835\udc37E_{\\theta}(D)italic_E start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_D ). \u201cNo Few-shot\u201d is the \u201cLog Power Law\u201d without the \u0394\u0394\\Deltaroman_\u0394 parameter. We see that removing the log worsens predictions, and the \u0394\u0394\\Deltaroman_\u0394 has a minimal effect on accuracy. Right: varying the low data extrapolation limit D0subscript\ud835\udc370D_{0}italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. N\ud835\udc41Nitalic_N is the number of few-shot examples. We see that within a reasonable range (e.g., <10\u2062Nabsent10\ud835\udc41<10N< 10 italic_N) the value of D0subscript\ud835\udc370D_{0}italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT has minimal impact on accuracy.", "description": "This figure presents ablation studies on the emergence prediction method. The left panel compares three different functional forms for the emergence law: a log power law, a power law, and a log power law without the delta parameter. The results show that removing the log term significantly worsens the prediction accuracy, while the delta parameter has minimal impact. The right panel investigates the effect of varying the low-data extrapolation limit (D0) on prediction accuracy.  The results indicate that within a reasonable range (less than 10 times the number of few-shot examples), the choice of D0 has minimal effect on accuracy.", "section": "5 Scaling Laws for Emergence Prediction"}, {"figure_path": "https://arxiv.org/html/2411.16035/x6.png", "caption": "Figure 7: How far in advance can we predict emergence? We hold out checkpoints to see how far in advance, in pretraining FLOPS, we can successfully predict emergence. The y position of each blue bar corresponds to the FLOPS needed to train the most capable model used for fitting. The blue circle represents the median of the MCMC posterior, and the error bar represents the 5th to 95th percentiles. If the MLE prediction error is >0.1absent0.1>0.1> 0.1 nats, we consider that prediction unsuccessful and denote it with a red-cross222In some cases the failed predictions would be well off the plot and we want to keep the axis bounds constrained for presentation clarity. We include the full results in Appendix\u00a0A.6.. On MMLU we can predict emergence using models trained with \u223c1022similar-toabsentsuperscript1022\\sim 10^{22}\u223c 10 start_POSTSUPERSCRIPT 22 end_POSTSUPERSCRIPT FLOPS, but no fewer. The earliest post-emergence model on MMLU was trained with \u223c5\u22171022similar-toabsent5superscript1022\\sim 5*10^{22}\u223c 5 \u2217 10 start_POSTSUPERSCRIPT 22 end_POSTSUPERSCRIPT FLOPS, hence we can predict 4-5x the FLOPS in advance on MMLU. On GSM8K we also predict 4x the FLOPS in advance333We count the earliest successful prediction for this calculation. However, GSM8K has a failed prediction between two successes, likely due to noise. In Appendix\u00a0A.6, we see that this failed prediction is just outside the success threshold, with much of the error bar falling well within 0.1 nats.. However, on CoLA and CommonsenseQA we only predict 2x the FLOPS in advance.", "description": "Figure 7 investigates how far in advance the proposed emergence prediction method can accurately predict the emergence point in terms of FLOPs (floating point operations). By holding out checkpoints from the model training process, the study assesses the predictive capability for various benchmarks. Each blue bar in the figure indicates the FLOPs required for training the most capable model involved in the emergence prediction. The blue circle marks the median of the MCMC (Markov Chain Monte Carlo) posterior distribution of predicted emergence points. Error bars illustrate the 5th to 95th percentiles of this distribution. Predictions with errors exceeding 0.1 nats are deemed unsuccessful and marked with red crosses. The analysis reveals that for MMLU and GSM8K, accurate predictions can be made using models trained with significantly fewer FLOPs (4-5x and 4x, respectively) than those needed for the earliest post-emergence models. However, for CoLA and CommonsenseQA, the predictive ability is more limited, with predictions accurate only up to approximately 2x the required FLOPs.", "section": "6 Evaluating the Emergence Law"}, {"figure_path": "https://arxiv.org/html/2411.16035/x7.png", "caption": "Figure 8: Comparing OpenLLaMA V1 and V2 emergence. On both MMLU and CommonsenseQA, the V2 models emerge first, suggesting that the V2 pretraining data is likely higher quality.", "description": "Figure 8 presents a comparison of the emergence behavior of OpenLLaMA V1 and V2 language models on two benchmark tasks: MMLU and CommonsenseQA.  Emergence refers to the point at which a model's performance on a task suddenly surpasses random chance. The x-axis represents the C4 validation loss, a measure of the model's performance during pretraining. The y-axis shows the model's accuracy on the respective benchmark task.  The plot demonstrates that for both MMLU and CommonsenseQA, the OpenLLaMA V2 models achieve emergence at a lower C4 validation loss than the V1 models. This suggests that the pretraining data used for OpenLLaMA V2 is of higher quality, leading to the faster emergence of capabilities in the model. The difference in emergence point highlights the impact of pretraining data quality on model performance.", "section": "7 A Case Study of Real World Uses for Emergence Prediction"}, {"figure_path": "https://arxiv.org/html/2411.16035/x8.png", "caption": "Figure 9: Comparing emergence predictions for OpenLLaMA V1 and V2 on MMLU. We plot the MLE predictions (left) and the CDFs (right) for both series. While our focus is on predicting the specific point of emergence (e.g., the ReLU elbow), we plot the full ReLU for visual clarity. The V2 models are correctly predicted to emerge before V1, providing initial evidence that our approach can be used to evaluate data quality. See Appendix\u00a0A.11 for plots with all the data used for fitting.", "description": "Figure 9 presents a comparison of emergence predictions for two versions of the OpenLLaMA language model (V1 and V2) when evaluated on the MMLU benchmark.  The left panel shows maximum likelihood estimates (MLE) of the emergence point (the model scale at which capabilities suddenly improve) for each model version.  The right panel displays cumulative distribution functions (CDFs) of the emergence point, illustrating the probability distribution for where emergence might occur.  A key observation is that the model trained on the V2 data is predicted to demonstrate emergence at a smaller model scale than V1, aligning with the expectation based on the improved pretraining data quality indicated in the paper. Appendix A.11 provides additional visualizations.", "section": "7 A Case Study of Real World Uses for Emergence Prediction"}, {"figure_path": "https://arxiv.org/html/2411.16035/extracted/6021902/figures/comparing_v1_v2_mle_cdf_v5.png", "caption": "Figure 10: Predicting emergence on APPS with LLaMA 2. On the left, we plot our MLE prediction. On the right, we convert this loss-based prediction into parameter count under the LLaMA 2 scaling law. The green point represents the MLE prediction, and the error bar represents the 5th to 95th percentiles under the MCMC posterior. We predict that emergence would most likely occur at \u223c325similar-toabsent325\\sim 325\u223c 325B parameters with a wide error bar from \u223c250similar-toabsent250\\sim 250\u223c 250B to \u223c500similar-toabsent500\\sim 500\u223c 500B parameters. For visual clarity, the left plot includes a subset of the full data used for fitting (see Appendix\u00a0A.11 for all).", "description": "Figure 10 presents the findings on predicting the emergence of capabilities in LLaMA 2 models on the APPS benchmark. The left panel displays the maximum likelihood estimation (MLE) of the emergence point, represented as a function of pretraining loss. The right panel converts this loss-based prediction into a parameter count, leveraging the LLaMA 2 scaling law.  The green point indicates the MLE prediction, with the error bar showing the 5th to 95th percentiles derived from Markov Chain Monte Carlo (MCMC) posterior sampling. This analysis predicts that emergence is most likely to occur around 325 billion parameters, but with substantial uncertainty ranging from 250B to 500B parameters.", "section": "7 A Case Study of Real World Uses for Emergence Prediction"}, {"figure_path": "https://arxiv.org/html/2411.16035/x9.png", "caption": "Figure 11: One the left we compare full fine-tuning against continuous prefix tuning on MMLU. We find that prefix tuning provides effectively no shift to the point of emergence, despite improving the performance of post-emergence models. On the right we compare 0-shot verses 5-shot prompting on MMLU. We see that using fewer shots has no meaningful effect on the point of emergence. Together these results suggest that the ability for prompt tuning to shift the point of emergence is very limited.", "description": "This figure shows the results of two experiments to determine whether prefix tuning or the number of shots used affects the point at which capabilities emerge in language models. The left panel shows that while prefix tuning improves performance after emergence, it does not shift the point at which capabilities emerge.  The right panel shows that using zero-shot prompting instead of five-shot prompting also does not shift the emergence point. This suggests that techniques like prefix tuning and altering the number of shots are not effective at changing when capabilities emerge.", "section": "4 How does Finetuneing Interact with Emergence?"}, {"figure_path": "https://arxiv.org/html/2411.16035/x10.png", "caption": "Figure 12: Comparing LoRA finetuning, with rank 1, 2, 4, and 64 against full finetuning on MMLU. We see that LoRA finetuning even with rank 1 shifts the point of emergence to a comparable degree to that of full finetuning.", "description": "This figure compares the impact of different LoRA ranks (1, 2, 4, 64) and full finetuning on the emergence of capabilities in a language model.  It shows how varying the rank affects the point at which the model begins to perform non-randomly (emergence point) on the MMLU benchmark.  The key finding is that even low-rank LoRA finetuning (rank 1) significantly shifts this emergence point, almost as much as full finetuning.  This demonstrates the effectiveness of LoRA in rapidly achieving substantial performance gains, even in the context of emergent capabilities.", "section": "4 How does Finetuneing Interact with Emergence?"}, {"figure_path": "https://arxiv.org/html/2411.16035/x11.png", "caption": "Figure 13: On a standard 5-shot MMLU and 6-shot CommonsenseQA (CSQA) evaluation, we observe emergence using both the standard correct answer accuracy evaluation and a continuous LLM log-probability metric.", "description": "This figure demonstrates that the phenomenon of emergence in large language models is not solely an artifact of using discontinuous metrics.  It shows that even when evaluating model performance using a continuous metric (LLM log-probability), rather than a discrete metric (accuracy), the characteristic 'emergent' behavior\u2014a sudden improvement beyond random chance\u2014is still observed. Two standard benchmarks, MMLU (5-shot) and CommonsenseQA (6-shot), are used for this demonstration.", "section": "Emergence is a Mirage?"}, {"figure_path": "https://arxiv.org/html/2411.16035/x12.png", "caption": "Figure 14: We plot few-shot and full data finetuning performance as a function of pretraining loss using all 3B, 7B, and 13B model checkpoints for all tasks. We see that both the point of emergence and the downstream performance scaling thereafter, as a function of pretraining loss, is consistent across model size in both the few-shot and finetuned setting.", "description": "Figure 14 presents a detailed analysis of how model size affects both the emergence point and post-emergence performance scaling in large language models (LLMs). The emergence point, where performance significantly improves beyond random chance, and the scaling of performance after emergence are plotted against the pretraining loss for four standard NLP benchmarks (MMLU, GSM8K, CommonsenseQA, and CoLA).  Data is shown for three model sizes (3B, 7B, and 13B parameters) using both few-shot and finetuned settings.  The key finding is the consistent relationship between pretraining loss and performance across all model sizes and settings, indicating that pretraining loss is a reliable indicator of LLM capabilities.", "section": "4 How does Finetuneing Interact with Emergence?"}, {"figure_path": "https://arxiv.org/html/2411.16035/x13.png", "caption": "Figure 15: We plot our scaling law fit for the LLaMA 2 series of models. We also include the learned values for our final fit on the plot. In this case N\ud835\udc41Nitalic_N corresponds to parameter count in billions. We see that the LLaMA 2 models are well modeled by our scaling law.", "description": "Figure 15 shows the scaling law fit for the LLaMA 2 series of models.  The x-axis represents the parameter count (in billions), and the y-axis shows the pretraining loss.  The plot displays the data points for the different LLaMA 2 models (7B, 13B, 34B, and 70B parameters) along with the fitted scaling law curve. The equation of the fitted curve is also provided, showing the relationship between the parameter count (N) and pretraining loss (L(N)). The figure demonstrates that the pretraining loss of LLaMA 2 models closely follows the predicted scaling law, indicating a good fit.", "section": "5 Scaling Laws for Emergence Prediction"}, {"figure_path": "https://arxiv.org/html/2411.16035/x14.png", "caption": "Figure 16: We plot the maximum likelihood predictions from our emergence law on each task. These plots include results from every finetuning run used for fitting the emergence law. The grey line represents our extrapolated prediction and the multi-color lines correspond to the fit produced by the emergence law for the various data levels. We see that across all tasks we are able to successfully predict the point of emergence within 0.1 nats and in many cases much less than that.", "description": "Figure 16 shows the maximum likelihood predictions of the emergence point for four NLP tasks (GSM8K, MMLU, CSQA, CoLA) using the proposed emergence law.  The figure displays the model's fit across different amounts of finetuning data (shown in multiple colors) for each task. A gray line represents the model's extrapolated prediction for the emergence point in the few-shot setting.  The results demonstrate the accuracy of the emergence law in predicting the point where a model exhibits a sudden improvement beyond random chance performance, typically within a margin of 0.1 nats and often significantly smaller.", "section": "5 Scaling Laws for Emergence Prediction"}, {"figure_path": "https://arxiv.org/html/2411.16035/x15.png", "caption": "Figure 17: We plot the maximum likelihood predictions from our emergence law with OpenLLaMA V1 (left) and OpenLLaMA V2 (right) on MMLU. We plot C4 Validation loss on the x-axis. These plots include results from every finetuning run used for fitting the emergence law. The grey line represents our extrapolated prediction and the multi-color lines correspond to the fit produced by the emergence law for the various data levels. We see that in both cases we are able to successfully predict the point of emergence within 0.1 nats.", "description": "This figure compares the emergence predictions for OpenLLaMA V1 and V2 language models on the MMLU benchmark.  The x-axis represents the C4 validation loss, a measure of the model's performance on a general language understanding task. The y-axis shows the accuracy on the MMLU benchmark.  Multiple colored lines show the performance of models fine-tuned with varying amounts of data, demonstrating how the point of emergence shifts depending on the amount of training data.  A grey line represents an extrapolated prediction of the emergence point using a fitted emergence law.  The results show that the emergence point for both V1 and V2 models can be predicted with high accuracy (within 0.1 nats). This suggests that the methodology can effectively anticipate when a model will demonstrate emergent capabilities.", "section": "7.1 Cheaply Evaluating Pretraining Data Quality"}, {"figure_path": "https://arxiv.org/html/2411.16035/x16.png", "caption": "Figure 18: We plot the MLE prediction (left) and MCMC CDF (right) for our emergence law fit using LLaMA 2 on APPS. The left plot includes results from every finetuning run used for fitting the emergence law. The grey line represents our extrapolated prediction and the multi-color lines correspond to the fit produced by the emergence law for the various data levels. We see that our emergence law predicts emergence roughly 0.15 nats beyond the LLaMA 2 70B model.", "description": "Figure 18 displays the results of applying an emergence prediction model to the APPS benchmark using LLaMA 2. The left panel shows the maximum likelihood estimate of emergence (grey line) alongside model fits using different amounts of training data (colored lines). The right panel shows the cumulative distribution function (CDF) of the emergence point's posterior distribution, indicating confidence and uncertainty in the prediction. The model predicts that emergence will likely occur at a pretraining loss roughly 0.15 nats beyond the LLaMA 2 70B parameter model.", "section": "7 A Case Study of Real World Uses for Emergence Prediction"}, {"figure_path": "https://arxiv.org/html/2411.16035/x17.png", "caption": "Figure 19: We plot the cumulative distribution function of our estimated posterior distribution over the point of emergence on each task. The stars correspond to few-shot performance on the task and represent the true emergence curve. The point at which the slope of the CDF peaks represents the mode of the distribution. We see across all tasks that the distribution spikes near the true point of emergence and is followed by a moderately long tail.", "description": "This figure displays the cumulative distribution function (CDF) of the model's predicted emergence point for four different NLP tasks (GSM8K, MMLU, CSQA, and CoLA).  The x-axis represents the model's pretraining loss, while the y-axis shows the cumulative probability of emergence.  Each CDF curve shows a sharp increase near the actual emergence point (marked by a star), indicating high confidence in the prediction near the actual emergence point.  However, the CDFs also have a moderately long tail extending beyond the emergence point, demonstrating some uncertainty in precisely predicting the point at which the capabilities emerge, especially when the model displays low performance before exhibiting emergent behavior.  The stars represent the actual few-shot performance data, illustrating the pre-emergence phase's near-random accuracy and subsequent sharp improvement after the emergence threshold.", "section": "6 Evaluating the Emergence Law"}]