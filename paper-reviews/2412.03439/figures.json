[{"figure_path": "https://arxiv.org/html/2412.03439/extracted/6045344/images/diffusion_feature_noise.png", "caption": "Figure 1: Our proposed CleanDIFT feature extraction method yields noise-free, timestep-independent, general-purpose features that significantly outperform standard diffusion features. CleanDIFT operates on clean images, while extracting diffusion features with existing approaches requires adding noise to an image before passing it through the model. Adding noise reduces the information present in the image and requires tuning a timestep per downstream task.", "description": "This figure illustrates the core concept of CleanDIFT, a novel feature extraction method for diffusion models.  CleanDIFT's key innovation is extracting features directly from *clean* images, unlike existing methods which require adding noise to the image as a preprocessing step.  The figure visually compares the standard diffusion feature extraction process (left side) with CleanDIFT (right side). The standard approach involves adding noise to an image, resulting in noisy features whose quality depends heavily on the chosen noise level ('timestep').  This requires tuning the timestep parameter for optimal performance on each downstream task.  CleanDIFT, in contrast, operates on clean images, producing noise-free, timestep-independent features that consistently outperform standard diffusion features across various tasks.  The improvement is attributed to avoiding the information loss caused by the addition of noise in conventional methods. The illustration shows the impact of noise on the image, the feature extraction process, and the need for timestep tuning in traditional methods.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.03439/x2.png", "caption": "Figure 2: Deterioration of Diffusion Features. As current methods need to pass noisy images to the model to obtain useful features, they significantly reduce the information available. We alleviate this problem by obtaining useful features without noise, improving the performance of downstream tasks.", "description": "This figure compares the reconstruction of an image using a diffusion model with and without added noise. Panel (a) shows a reconstruction without noise, where the model is directly given a clean image. Panel (b) shows reconstruction with added noise (t=261), a typical approach for existing diffusion feature extraction methods, where the model attempts to recover the clean image from a noisy input. By comparing the two panels, it highlights that adding noise greatly reduces the information available for feature extraction, leading to lower quality features. This is because the model is trained to remove noise, making its internal representation primarily centered on noise patterns rather than semantic content. The proposed method avoids this issue by extracting features directly from clean images.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.03439/x3.png", "caption": "Figure 3: Fraction of variance of diffusion features explained by 1) encoding the clean image at t=0\ud835\udc610t=0italic_t = 0 (no additive noise), and 2) encoding just the added noise \u03f5bold-italic-\u03f5\\boldsymbol{\\epsilon}bold_italic_\u03f5 at t=999\ud835\udc61999t=999italic_t = 999. Even at relatively low timesteps such as t=261\ud835\udc61261t=261italic_t = 261 as used by DIFT\u00a0[54], a substantial part of the features directly depends only on the added noise.", "description": "This figure shows the fraction of variance in diffusion features explained by the clean image and the added noise at different timesteps.  The x-axis represents the timestep (t), ranging from 0 (clean image) to 999 (pure noise).  The y-axis represents the fraction of variance explained.  Two lines are plotted: one shows the variance explained by the clean image at t=0, and the other shows the variance explained by the added noise at t=999.  The figure demonstrates that even at relatively low timesteps (like t=261, used by the DIFT method), a significant portion of the features' variance is directly attributable to the added noise, highlighting a limitation of using noisy images for feature extraction.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.03439/x4.png", "caption": "Figure 4: Our training setup. We train our model to predict features from a clean input image, while the frozen diffusion model is fed the noisy image. The projection heads project our model\u2019s features onto the noisy diffusion model features, given the noising timestep t\ud835\udc61titalic_t. For downstream tasks, we discard the projection heads and directly use our model\u2019s internal representations as features.", "description": "The figure illustrates the training process of CleanDIFT.  A clean image is input to a trainable feature extraction model (CleanDIFT), while a noisy version of the same image (with noise level determined by timestep 't') is fed into a frozen diffusion model. Projection heads are used during training to align the features extracted by CleanDIFT with the features from the noisy diffusion model at each timestep.  Crucially, for downstream applications, these projection heads are discarded, and only the noise-free features directly from the CleanDIFT model are utilized.", "section": "3.2 CleanDIFT: Noise-Free Diffusion Features"}, {"figure_path": "https://arxiv.org/html/2412.03439/x5.png", "caption": "Figure 5: Following\u00a0[54], we evaluate semantic correspondence matching accuracy for different noise levels. Our feature extractor outperforms the standard noisy diffusion features across all timesteps t\ud835\udc61titalic_t. We additionally demonstrate that simply providing the diffusion model with a clean image and a non-zero timestep does not result in improved performance.", "description": "This figure compares the performance of semantic correspondence matching using standard noisy diffusion features and CleanDIFT features at various noise levels. The x-axis represents the timestep (t), while the y-axis represents the percentage of correctly matched keypoints (PCK). The results show that CleanDIFT features consistently outperform standard noisy diffusion features across all timesteps.  Furthermore, it demonstrates that merely providing a clean image to a diffusion model without the addition of noise (i.e., a non-zero timestep) does not improve performance, highlighting the importance of CleanDIFT's noise-free approach.", "section": "4.2. Unsupervised Semantic Correspondence"}, {"figure_path": "https://arxiv.org/html/2412.03439/x6.png", "caption": "Figure 6: Semantic correspondence results using DIFT\u00a0[54] features with the standard SD 2.1 (t=261\ud835\udc61261t=261italic_t = 261) and our CleanDIFT features. Our clean features show significantly less incorrect matches than the base diffusion model.", "description": "This figure compares semantic correspondence results obtained using standard diffusion features (DIFT [54] with SD 2.1 at timestep t=261) against results from CleanDIFT features.  The images visually demonstrate that CleanDIFT features produce significantly fewer incorrect matches in semantic correspondence tasks compared to the standard approach.  This showcases the improved accuracy and reliability of the proposed CleanDIFT feature extraction method.", "section": "4.2. Unsupervised Semantic Correspondence"}, {"figure_path": "https://arxiv.org/html/2412.03439/x7.png", "caption": "Figure 7: Qualitative results for depth estimation using a linear probe on diffusion features on NYUv2 [34]. Our CleanDIFT features enable substantially better depth estimation than standard diffusion features. Note how the CleanDIFT features are far less noisy when compared to the standard diffusion features.", "description": "This figure presents a qualitative comparison of depth estimation results obtained using standard diffusion features and CleanDIFT features on the NYUv2 dataset.  The images show that CleanDIFT features produce depth maps with significantly less noise and finer details compared to those generated using standard diffusion features, leading to a substantial improvement in the accuracy and quality of depth estimation. The visual difference highlights the effectiveness of CleanDIFT in extracting cleaner and more informative features from the input images.", "section": "4.3. Depth Estimation"}, {"figure_path": "https://arxiv.org/html/2412.03439/x8.png", "caption": "Figure 8: Performance on semantic segmentation using linear probes. Our clean features outperform the noisy diffusion features for the best noising timestep t\ud835\udc61titalic_t. Semantic segmentation performance of a standard diffusion model heavily depends on the used noising timestep. Unlike for semantic correspondence matching, the optimal t\ud835\udc61titalic_t value appears to be around t=100\ud835\udc61100t=100italic_t = 100.", "description": "Figure 8 shows the performance comparison of semantic segmentation using linear probes trained on CleanDIFT features and standard noisy diffusion features.  The x-axis represents the noise timestep (t), and the y-axis represents the mean Intersection over Union (mIOU) score, a common metric for evaluating semantic segmentation accuracy. The graph reveals that CleanDIFT features consistently achieve higher mIOU scores across various timesteps. Notably, while noisy diffusion features exhibit a strong dependence on the choice of timestep (with the optimal performance around t=100), CleanDIFT features demonstrate more consistent performance, suggesting their robustness and reduced sensitivity to the noise level. This highlights the benefit of using noise-free features for semantic segmentation.", "section": "4.4. Semantic Segmentation"}, {"figure_path": "https://arxiv.org/html/2412.03439/x9.png", "caption": "Figure 9: Qualitative results for semantic segmentation from diffusion features on Pascal VOC [15]. Standard SD features use t=100\ud835\udc61100t=100italic_t = 100 as the timestep, which we found to perform best quantitatively (c.f.\u00a0Figure\u00a08). Note how the CleanDIFT segmentation maps are far less noisy compared to those of the standard diffusion features.", "description": "Figure 9 presents a qualitative comparison of semantic segmentation results obtained using standard Stable Diffusion (SD) features and CleanDIFT features on the Pascal VOC dataset.  The standard SD features utilize a timestep (t) of 100, determined in Figure 8 to yield the best quantitative performance. The comparison highlights the significant reduction in noise present in the segmentation maps generated by CleanDIFT features compared to those produced by standard SD features. This visual demonstration underscores CleanDIFT's effectiveness in generating cleaner and less noisy feature representations.", "section": "4.4. Semantic Segmentation"}, {"figure_path": "https://arxiv.org/html/2412.03439/x10.png", "caption": "Figure 10: Classification performance on ImageNet1k\u00a0[10], using kNN classifier with k=10\ud835\udc5810k=10italic_k = 10 and cosine similarity as the distance metric. We sweep over different timesteps and feature maps. We find that the feature map with the lowest spatial resolution (feature map #0) yields the highest classification accuracy.", "description": "This figure displays the classification accuracy achieved using a k-Nearest Neighbors (kNN) classifier with k=10 on the ImageNet1k dataset.  The results show the impact of different diffusion timesteps and feature maps from the diffusion model on the classification performance. Notably, using feature map #0 (the feature map with the lowest spatial resolution) consistently yields the highest classification accuracy across various timesteps.", "section": "4.5. Classification"}]