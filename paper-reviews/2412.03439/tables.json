[{"content": "| Input | Ours | SD 2.1<sub>t=299</sub> | Ground Truth |\n|---|---|---|---| \n| ![Input](https://arxiv.org/html/2412.03439/extracted/6045344/images/depth_input.png) | ![Ours](https://arxiv.org/html/2412.03439/extracted/6045344/images/depth_student.png) | ![SD 2.1<sub>t=299</sub>](https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/2/t299.png) | ![Ground Truth](https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/1/gt.png) |\n| ![Input](https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/2/input.png) | ![Ours](https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/2/s.png) | ![SD 2.1<sub>t=299</sub>](https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/2/t299.png) | ![Ground Truth](https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/2/gt.png) |\n| ![Input](https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/3/input.png) | ![Ours](https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/3/s.png) | ![SD 2.1<sub>t=299</sub>](https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/3/t299.png) | ![Ground Truth](https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/3/gt.png) |", "caption": "Table 1: Zero-shot unsupervised semantic correspondence matching performance comparison on SPair71k [31]. Our improved features consistently lead to substantial improvements in matching performance. We report PCK on the test split of SPair71k, aggregated per point. Numbers are reproduced, for a discussion and comparison to reported numbers view\u00a0Tab.\u00a05.", "description": "This table presents a comparison of zero-shot unsupervised semantic correspondence matching performance using different feature extraction methods on the SPair-71k dataset.  The key metric is Percentage of Correct Keypoints (PCK), averaged across all keypoints in the test set. The table shows that the proposed CleanDIFT features significantly outperform existing methods, demonstrating substantial improvements in matching accuracy.  For a detailed discussion on the reproduction of numbers and a comparison with those reported in the original papers, refer to Table 5.", "section": "4. Experiments"}, {"content": "| Input | Ours | SD 2.1 |\n|---|---|---|\n| ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_8_2.png) | ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_8_2_student.png) | ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_8_2_teacher.png) |\n| ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_9_1.png) | ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_9_1_student.png) | ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_9_1_teacher.png) |\n| ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_66_2.png) | ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_66_2_student.png) | ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_66_2_teacher.png) |", "caption": "Table 2: Monocular Depth Estimation. Following [37], we evaluate metric depth prediction on NYUv2\u00a0[34] using a linear probe. Our clean features outperform the noisy features by a significant margin. Probes trained on the noisy features can be reused for the clean features, but incur a smaller performance gain.", "description": "This table presents a comparison of monocular depth estimation results on the NYUv2 dataset.  It compares the performance of a linear probe trained on standard noisy diffusion features against a linear probe trained on the proposed CleanDIFT noise-free features. The results show that CleanDIFT features significantly improve depth estimation accuracy.  Furthermore, it demonstrates that while a probe trained on noisy features can be applied to CleanDIFT features, the performance gain is smaller than when using a probe specifically trained on CleanDIFT features.", "section": "4.3. Depth Estimation"}, {"content": "| Input | Ours | SD 2.1 |\n|---|---|---|\n| ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_0_0.png) | ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_0_0_student.png) | ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_0_0_teacher.png) |\n| ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_41_3.png) | ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_41_3_student.png) | ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_41_3_teacher.png) |\n| ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_67_2.png) | ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_67_2_student.png) | ![Refer to caption](https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_67_2_teacher.png) |", "caption": "Table 3: Ablation Study Results. We evaluate the feature extraction models\u2019 performance for zero-shot semantic correspondence matching on a subset of the SPair71k test split. PCK is aggregated per point.", "description": "This table presents the ablation study results for evaluating the impact of different training objectives and the use of projection heads on the performance of the feature extraction model. The model's performance is measured using the Percentage of Correct Keypoints (PCK) metric, which is averaged across all keypoints in a subset of the SPair71k test set for zero-shot semantic correspondence matching.", "section": "4.6 Ablation Studies"}, {"content": "| Objective | Projection Heads | PCK@\u03b1 (\u2191) \u03b1img=0.1 | PCK@\u03b1 (\u2191) \u03b1bbox=0.1 |\n|---|---|---|---|\n| Cosine Sim. | \u2713 | **67.61** | **60.28** |\n|  | \u2717 | **67.37** | **60.22** |\n| L2 | \u2713 | 66.71 | 59.30 |\n|  | \u2717 | 66.65 | 59.34 |\n| L1 | \u2713 | 66.18 | 58.79 |\n|  | \u2717 | 66.07 | 59.01 |\n| SD 2.1 | - | 63.41 | 55.92 |", "caption": "Table 4: Reproduced results for zero-shot unsupervised semantic correspondence matching, evaluated on SPair71k\u00a0[31]. The three categories for which we observe the largest overall gains are marked in blue. We report PCK@\u03b1=0.1\ud835\udefc0.1\\alpha=0.1italic_\u03b1 = 0.1 with an error margin relative to bounding box sizes on the test split of SPair71k, aggregated per point and per category. We compare our reproductions against the papers\u2019 reported numbers in\u00a0Tab.\u00a05", "description": "This table presents a detailed breakdown of the performance of three different methods (DIFT, A Tale of Two Features, and Telling Left from Right) for zero-shot unsupervised semantic correspondence matching on the SPair71k dataset.  The performance is measured using the Percentage of Correct Keypoints (PCK) metric at \u03b1=0.1, considering both the image size and bounding box size as error margins. Results are shown for each category within the dataset, with the three categories exhibiting the largest performance improvements highlighted. The table also includes comparisons with the original results reported in the referenced papers.", "section": "4.2. Unsupervised Semantic Correspondence"}, {"content": "| Method | Our Features | Aero | Bike | Bird | Boat | Bottle | Bus | Car | Cat | Chair | Cow | Dog | Horse | Motor | Person | Plant | Sheep | Train | TV | All |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| DIFT [54] | \u2717 | 63.41 | 55.10 | 80.40 | 34.55 | 46.15 | 52.26 | 48.02 | 75.86 | 39.46 | 75.57 | 55.00 | 61.71 | 53.32 | 46.53 | 56.36 | 57.68 | 71.30 | 63.63 | 59.57 |\n|  | \u2713 | 63.72 | 55.90 | 80.50 | 35.40 | 49.36 | 53.46 | 48.08 | 75.78 | 43.10 | 76.20 | 55.69 | 61.01 | 54.17 | 49.14 | 62.56 | 58.37 | 74.63 | 71.54 | 61.43\u00b9\u2078 |\n| A Tale of Two Features [58] | \u2717 | 71.26 | 62.23 | 87.01 | 37.24 | 53.78 | 54.32 | 51.20 | 78.61 | 46.50 | 78.93 | 64.43 | 69.47 | 62.23 | 69.27 | 59.28 | 68.03 | 65.40 | 53.81 | 63.73 |\n|  | \u2713 | 71.12 | 62.70 | 87.42 | 38.33 | 54.78 | 54.67 | 51.20 | 78.52 | 47.86 | 79.38 | 64.88 | 69.18 | 62.61 | 69.72 | 62.82 | 68.87 | 67.51 | 59.04 | 64.81\u00b9\u2078 |\n| Telling Left from Right [59] | \u2717 | 78.14 | 66.37 | 89.60 | 43.74 | 53.29 | 66.61 | 59.94 | 82.66 | 51.75 | 82.79 | 68.95 | 74.91 | 65.84 | 71.67 | 57.71 | 72.24 | 83.46 | 49.66 | 68.64 |\n|  | \u2713 | 77.17 | 65.65 | 89.58 | 44.24 | 54.27 | 67.24 | 60.63 | 82.33 | 56.57 | 82.53 | 68.37 | 75.91 | 65.99 | 71.37 | 62.29 | 70.42 | 84.58 | 59.84 | 69.99\u00b9\u2078 |", "caption": "Table 5: Reproduced vs reported numbers for zero-shot semantic correspondences, evaluated on SPair71k\u00a0[31]. A Tale of Two Features\u00a0[58] and Telling Left from Right\u00a0[59] report higher PCK values than our reproduction because they utilize a conditioning mechanism on CLIP image embeddings from\u00a0[56] that was fine-tuned for panoptic segmentation. As this task is related to semantic correspondence matching, we do not consider using this conditioning mechanism fair in comparison to other zero-shot approaches for semantic correspondences. Therefore, we exclude it from our reproductions.", "description": "This table compares the reproduced results of zero-shot semantic correspondence matching performance on the SPair71k dataset [31] against the values reported in three different papers ([54], [58], [59]). The discrepancies in performance are attributed to the fact that the papers [58] and [59] utilize a CLIP image embedding conditioning mechanism [56] which was fine-tuned for the related task of panoptic segmentation.  Since this additional conditioning mechanism is not a zero-shot approach, the authors of the current paper excluded it from their reproduction for a fair comparison.", "section": "4.2. Unsupervised Semantic Correspondence"}]