[{"content": "| Attacker | Success Rate | LPIPS | SSIM | FID |\n|---|---|---|---|---| \n| **DenseNet121** | | | | | \n| FGSM | 0.88 | 0.40 | 0.81 | 123.51 | \n| PGD | 0.57 | 0.29 | 0.45 | 378.62 | \n| Diff-PGD | **1.0** | 0.30 | **0.87** | 111.03 | \n| P2P (Ours) | 0.98 | **0.13** | 0.85 | **45.84** | \n| **ResNet34** | | | | | \n| FGSM | 0.96 | 0.41 | 0.81 | 131.62 | \n| PGD | 0.55 | 0.25 | 0.37 | 332.01 | \n| Diff-PGD | **1.0** | 0.31 | **0.84** | 117.49 | \n| P2P (Ours) | 0.97 | **0.12** | 0.81 | **43.03** | \n| **SqueezeNet1.1** | | | | | \n| FGSM | 0.49 | 0.16 | 0.40 | 118.03 | \n| PGD | 0.33 | 0.20 | 0.30 | 250.38 | \n| Diff-PGD | 0.74 | 0.14 | 0.56 | 79.51 | \n| P2P (Ours) | **0.96** | **0.09** | **0.63** | **47.64** |", "caption": "Table 1: Evaluation of adversarial attacks on different attack models for BUSI dataset [2] with 3 classifiers. LPIPS, SSIM, and FID are reported on successful attack examples.", "description": "This table evaluates the performance of different adversarial attacks (FGSM, PGD, Diff-PGD, and the proposed P2P) against three classifiers (DenseNet121, ResNet34, and SqueezeNet1.1) on the BUSI dataset. The metrics used to evaluate the attack performance are Success Rate, LPIPS, SSIM, and FID.  The table shows these metrics specifically for the successfully attacked examples, not on the whole dataset.", "section": "4. Experiments"}, {"content": "| Attacker | Success Rate | LPIPS | SSIM | FID |\n|---|---|---|---|---| \n| **DenseNet121** | | | | | \n| FGSM | 0.93 | 0.40 | 0.77 | 112.11 |\n| PGD | 0.43 | 0.19 | 0.56 | 213.65 |\n| Diff-PGD | **1.0** | 0.29 | **0.82** | 90.5 |\n| P2P (Ours) | 0.94 | **0.12** | 0.78 | **38.00** |\n| **ResNet34** | | | | |\n| FGSM | 0.81 | 0.35 | 0.66 | 133.17 |\n| PGD | 0.31 | 0.12 | 0.24 | 158.24 |\n| Diff-PGD | **1.0** | 0.29 | **0.78** | 100.2 |\n| P2P (Ours) | 0.93 | **0.11** | 0.72 | **44.09** |\n| **SqueezeNet1.1** | | | | |\n| FGSM | 0.69 | 0.16 | 0.77 | 120.14 |\n| PGD | 0.43 | 0.26 | 0.40 | 292.99 |\n| Diff-PGD | **0.75** | 0.12 | 0.47 | 89.47 |\n| P2P (Ours) | 0.74 | **0.08** | **0.49** | **58.60** |", "caption": "Table 2:  Evaluation of adversarial attacks on different attack models for BUS-BRA dataset [18] with 3 classifiers. LPIPS, SSIM, and FID are reported on successful attack examples.", "description": "This table presents a comparative evaluation of four different adversarial attack methods (FGSM, PGD, Diff-PGD, and the proposed P2P) on three distinct classifier models (DenseNet121, ResNet34, and SqueezeNet1.1). These attacks were applied to breast ultrasound images from the BUS-BRA dataset to assess their effectiveness in misclassifying images while maintaining perceptual similarity. The table reports the success rate of each attack, along with quality metrics including LPIPS (Learned Perceptual Image Patch Similarity), SSIM (Structural Similarity Index Measure), and FID (Fr\u00e9chet Inception Distance), which measure the perceptual difference, structural similarity, and distribution similarity, respectively, between the attacked and original images.  The metrics are calculated only on successfully attacked examples.  Lower LPIPS and FID, along with higher SSIM, generally indicate better perceptual quality of the adversarial example, meaning it appears more like a real medical ultrasound image.", "section": "4. Experiments"}, {"content": "| Attacker | Success Rate | LPIPS | SSIM | FID |\n|---|---|---|---|---| \n| **DenseNet121** | | | | | \n| FGSM | 0.97 | 0.37 | 0.77 | 103.07 | \n| PGD | 0.17 | 0.07 | 0.11 | 147.84 | \n| Diff-PGD | **1.0** | 0.27 | **0.80** | 81.31 | \n| P2P (Ours) | 0.86 | **0.12** | 0.62 | **27.18** | \n| **ResNet34** | | | | | \n| FGSM | 0.98 | 0.41 | 0.75 | 103.68 | \n| PGD | 0.23 | 0.10 | 0.19 | 135.95 | \n| Diff-PGD | **1.0** | 0.31 | **0.77** | 80.89 | \n| P2P (Ours) | 0.97 | **0.15** | 0.74 | **20.3** | \n| **SqueezeNet1.1** | | | | | \n| FGSM | 0.59 | 0.20 | 0.42 | 72.51 | \n| PGD | 0.16 | 0.14 | 0.19 | 292.21 | \n| Diff-PGD | **0.77** | 0.16 | **0.54** | 32.47 | \n| P2P (Ours) | 0.76 | **0.10** | 0.52 | **23.50** |", "caption": "Table 3: Evaluation of adversarial attacks on different attack models for UDIAT dataset [62] with 3 classifiers. LPIPS, SSIM, and FID are reported on successful attack examples.", "description": "Table 3 shows the success rate and image quality metrics (LPIPS, SSIM, and FID) of four different adversarial attack methods (FGSM, PGD, Diff-PGD, and P2P) on a breast ultrasound dataset named UDIAT. The table includes results for three different classifiers: DenseNet121, ResNet34, and SqueezeNet. Lower LPIPS and FID scores and higher SSIM scores mean better image quality and less noticeable alterations caused by the attack.", "section": "4. Experiments"}, {"content": "| a) FGSM | b) PGD |\n|---|---| \n| ![FGSM](https://arxiv.org/html/2412.09910/extracted/6063652/figures/TSNE/FGSM_1.png) | ![PGD](https://arxiv.org/html/2412.09910/extracted/6063652/figures/TSNE/PGD_1.png) |\n| c) Diff-PGD | d) P2P (Ours) |\n| ![Diff-PGD](https://arxiv.org/html/2412.09910/extracted/6063652/figures/TSNE/Diff-PGD_1.png) | ![P2P (Ours)](https://arxiv.org/html/2412.09910/extracted/6063652/figures/TSNE/P2P_1.png) |", "caption": "Table 4: Comparison of the ablation study on different components of the P2P pipeline. The baseline configuration uses T=50 with MSE in the loss function. In each row, only one component is modified from the baseline. \u2019Time\u2019 indicates the duration of the generation process for the attack per image.", "description": "This table presents the ablation study results for the Prompt2Perturb (P2P) method, comparing different configurations against a baseline. The baseline configuration uses 50 timesteps (T=50) and includes Mean Squared Error (MSE) as part of the loss function.  Each row in the table modifies one component of the baseline, either changing the loss function by removing the MSE term or adjusting the number of diffusion steps (T). The 'Time' column denotes the computational time required to generate an adversarial attack for a single image.", "section": "4.4. Ablation Study"}]