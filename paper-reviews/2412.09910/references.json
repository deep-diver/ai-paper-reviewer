{"references": [{"fullname_first_author": "Ian J. Goodfellow", "paper_title": "Explaining and Harnessing Adversarial Examples", "publication_date": "2014-01-01", "reason": "This foundational work introduced the concept of adversarial examples and significantly impacted the field of adversarial machine learning."}, {"fullname_first_author": "Nicholas Carlini", "paper_title": "Towards Evaluating the Robustness of Neural Networks", "publication_date": "2017-01-01", "reason": "This work proposed three powerful white-box adversarial attacks (L0, L2, and Linf) that are widely used as benchmarks in adversarial robustness research."}, {"fullname_first_author": "Aleksander Madry", "paper_title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "publication_date": "2017-01-01", "reason": "This seminal paper introduced the projected gradient descent (PGD) attack, a fundamental technique in adversarial training and robustness evaluation."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning Transferable Visual Models From Natural Language Supervision", "publication_date": "2021-01-01", "reason": "This paper introduced CLIP, a powerful vision-language model that enables text-guided image manipulation and understanding, which is crucial for prompt-based adversarial attacks."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-Resolution Image Synthesis with Latent Diffusion Models", "publication_date": "2022-01-01", "reason": "This paper presented Latent Diffusion Models (LDMs), a key component of many state-of-the-art text-to-image generation models, making it highly relevant to diffusion-based adversarial attacks."}]}