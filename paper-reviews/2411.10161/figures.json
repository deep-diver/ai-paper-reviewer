[{"figure_path": "https://arxiv.org/html/2411.10161/extracted/6001720/imgs/idea.png", "caption": "Figure 1: (A) Illustrations of the typical Vision-based and VLM-based IQA. Both of them are designed to analyze the quality of overall image. (B) Our Seagull has the capability in fine-grained quality assessment for specified ROI. The mask-based ROI is extracted by SAM [30]. Best viewed in color.", "description": "Figure 1 illustrates the difference between traditional vision-based and vision-language model (VLM)-based image quality assessment (IQA) methods, and introduces the proposed SEAGULL model.  Panel (A) shows that vision-based and VLM-based methods assess the overall image quality, lacking fine-grained analysis.  Panel (B) demonstrates SEAGULL's ability to perform fine-grained quality assessment for specified Regions of Interest (ROIs).  ROIs are identified using segmentation masks generated by the Segment Anything Model (SAM), allowing for precise, localized quality analysis.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.10161/extracted/6001720/imgs/pipeline.png", "caption": "Figure 2: The automatic pipeline for generating the Seagull-100w dataset.", "description": "This figure illustrates the automated process of creating the SEAGULL-100w dataset. It begins with collecting distorted images through an Image Signal Processor (ISP).  These images are then processed using a mask-based ROI collection method.  Finally, labels are generated for the ROIs, providing quality scores, importance scores, and distortion analysis for each ROI. This entire pipeline is automated to generate a large-scale dataset for training a vision-language model for image quality assessment.", "section": "3. The Proposed Datasets"}, {"figure_path": "https://arxiv.org/html/2411.10161/extracted/6001720/imgs/framework.png", "caption": "Figure 3: Overview of the Seagull (left) and the Mask-based Feature Extractor (right). Best viewed in color.", "description": "This figure provides a detailed illustration of the SEAGULL architecture, focusing on two key components: the overall network architecture (left panel) and the Mask-based Feature Extractor (MFE) (right panel). The left panel shows the flow of image and text inputs through the image encoder, mask-based feature extractor, and large language model to produce final quality assessments.  The right panel illustrates the MFE in detail, showing how global and local view tokens are extracted from the input image and mask, combined, and fed into the LLM.  Color is important for differentiating various aspects of the network and data flow in this diagram.", "section": "4. The Proposed SEAGULL Network"}, {"figure_path": "https://arxiv.org/html/2411.10161/extracted/6001720/imgs/visualization.png", "caption": "Figure 4: ROI quality analysis results from Human, VLMs and Seagull. Best viewed in color.", "description": "Figure 4 presents a comparative analysis of Region of Interest (ROI) quality assessment results.  It contrasts the assessments provided by humans, various Vision-Language Models (VLMs), and the proposed SEAGULL model.  The figure visually demonstrates the differences in how these methods perceive and describe the quality of the ROIs, including details about blur, exposure, and color distortions, and their importance to the overall image quality. This comparison highlights the strengths and weaknesses of each approach in fine-grained quality assessment of image regions.", "section": "5. Visualization"}]