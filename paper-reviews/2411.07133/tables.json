[{"content": "| Model Family | Release Date | Model ID | Size |\n|---|---|---|---| \n| Qwen2<br>Yang et al. (2024) | Jun, 2024 | Qwen2-1.5B-Instruct | 1.5B |\n|  |  | Qwen2-7B-Instruct | 7B |\n|  |  | Qwen2-72B-Instruct | 72B |\n| Qwen2.5<br>Team (2024) | Sept, 2024 | Qwen2.5-3B-Instruct | 3B |\n|  |  | Qwen2.5-7B-Instruct | 7B |\n|  |  | Qwen2.5-14B-Instruct | 14B |\n|  |  | Qwen2.5-32B-Instruct | 32B |\n|  |  | Qwen2.5-72B-Instruct | 72B |\n| Llama 3<br>(Meta, 2024c) | Apr, 2024 | Llama-3-8B-Instruct | 8B |\n|  |  | Llama-3-70B-Instruct | 70B |\n| Llama 3.1<br>(Meta, 2024c) | Jul, 2024 | Llama-3.1-8B-Instruct | 8B |\n|  |  | Llama-3.1-70B-Instruct | 70B |\n|  |  | Llama-3.1-405B-Instruct | 405B |\n| Gemma 2<br>Team et al. (2024) | Jun, 2024 | Gemma-2-2b-it | 2B |\n|  |  | Gemma-2-9b-it | 9B |\n|  |  | Gemma-2-27b-it | 27B |\n| Phi-3<br>Abdin et al. (2024) | Jun, 2024 | Phi-3-mini-128k-instruct | 3.8B |\n|  |  | Phi-3-small-128k-instruct | 7B |\n|  |  | Phi-3-medium-128k-instruct | 14B |\n| GPT-4<br>Achiam et al. (2023) | Since<br>Mar, 2023 | GPT-4 & GPT-4 Turbo | - |", "caption": "Table 1: Overview of 20 response generators used in our study.", "description": "This table lists the twenty different large language models (LLMs) used to generate responses for synthetic instruction datasets.  For each LLM, it specifies the model family it belongs to, the model's size (in parameters), and its release date.  These LLMs serve as response generators, and the resulting responses are paired with instructions to create the instruction-following datasets used in training various base models.", "section": "3.2 Experimental Setup"}, {"content": "| Response | AlpacaEval 2 | AlpacaEval 2 | Arena-Hard | AP |\n|---|---|---|---|---|\n| **Generator Model** | LC (%) | WR (%) | WR (%) | (%) |\n| Gemma-2-9b-it | 16.09 | 13.70 | 13.7 | 14.90 |\n| Gemma-2-27b-it | 13.93 | 13.31 | 12.4 | 13.17 |\n| Llama-3-70b-Instruct | 10.55 | 10.68 | 6.7 | 8.62 |\n| Llama-3.1-70b-Instruct | 9.52 | 10.10 | 8.3 | 8.91 |\n| Qwen2.5-7B-Instruct | 13.50 | 14.33 | 10.6 | 12.05 |\n| Qwen2.5-72B-Instruct | **19.20** | **21.01** | **13.1** | **16.15** |\n| GPT-4 | 6.63 | 5.70 | 4.8 | 5.72 |", "caption": "Table 2: This table compares the performance of GPT-4 and other state-of-the-art open source LLMs as the response generator. All models are supervised-fine-tuned on the Llama-3.1-Minitron-4B base model.", "description": "This table presents a comparison of the performance of various Large Language Models (LLMs) when used as response generators in instruction tuning.  Specifically, it focuses on GPT-4 (a closed-source model) and several state-of-the-art open-source LLMs. The performance is evaluated by fine-tuning a Llama-3.1-Minitron-4B base model using instruction datasets generated by each of these LLMs as response generators.  The table shows the AlpacaEval 2 LC (Length-Controlled Win Rate), AlpacaEval 2 WR (Win Rate), Arena-Hard WR, and the average performance (AP) across these metrics for each LLM, allowing for a direct comparison of their effectiveness in this role.", "section": "3 Which Models are the most effective teachers for instruction tuning?"}, {"content": "| Base Model | Method | AlpacaEval 2 LC (%) | AlpacaEval 2 WR (%) | Arena-Hard WR (%) | AP (%) |\n|---|---|---|---|---|---|---|\n| Llama-3.1-Minitron-4B | Best-of-N | **15.94** | **15.14** | **11.9** | **13.92** |\n|  | Worst-of-N | 13.02 | 12.66 | 11.0 | 12.01 |\n|  | Sampling | 15.71 | 14.81 | 11.8 | 13.755 |\n|  | Greedy | 16.13 | 14.51 | 11.0 | 13.565 |\n| Qwen2.5-3B-Instruct | Best-of-N | **13.83** | **13.57** | **21.0** | **17.415** |\n|  | Worst-of-N | 12.37 | 12.54 | 17.9 | 15.135 |\n|  | Sampling | 13.43 | 13.29 | 20.1 | 16.765 |\n|  | Greedy | 13.78 | 13.57 | 19.4 | 16.59 |", "caption": "Table 3: This table investigates the impact of reject sampling on model performance.", "description": "This table presents the results of an experiment evaluating the effect of reject sampling on the performance of instruction-tuned language models.  Reject sampling is a technique used to improve the quality of generated responses by discarding samples below a certain quality threshold. The table shows the average performance across various evaluation metrics for models trained using both reject sampling and greedy sampling (without rejection).  The models were fine-tuned on a synthetic dataset using a specific response generator, Gemma-2-9b-it, for different base models. Performance is measured across the AlpacaEval 2 benchmark (using Length Controlled Win Rate and Win Rate) and the Arena-Hard benchmark (using Win Rate). The metrics show how reject sampling impacts the instruction-following capabilities of models trained on synthetic datasets generated under different sampling approaches. ", "section": "3.3 Empirical Evaluation"}, {"content": "| Base Models | Reward |  |  | Difficulty |  |  |  | Response Length | CAR |\n|---|---|---|---|---|---|---|---|---|---|---|\n| **Base Models** | **Reward** |  |  | **Difficulty** |  |  |  | **Response Length** | **CAR** |\n|  | \u211b\u2133\u2081 | \u211b\u2133\u2082 | \u211b\u2133\u2083 | IFD-GPT2 | IFD-Self | PPL-GPT2 | PPL-Self |  |  |\n| **Qwen2-1.5B** | 0.5526 | 0.7895 | 0.8754 | 0.7088 | 0.7719 | 0.1473 | 0.5596 | 0.5404 | **0.8842** |\n| **Gemma 2-2B** | 0.5526 | 0.7982 | 0.8842 | 0.8281 | 0.8930 | 0.1614 | 0.4351 | 0.6298 | **0.9000** |\n| **Qwen2.5-3B** | 0.4526 | 0.7351 | 0.7456 | 0.7386 | 0.8088 | 0.0456 | -0.0614 | 0.6088 | **0.8105** |\n| **Llama 3.2-3B** | 0.6088 | 0.8105 | **0.9088** | 0.7632 | 0.8579 | 0.0456 | 0.6018 | 0.5877 | 0.9053 |\n| **Llama-3.1-Minitron-4B** | 0.6632 | 0.8860 | 0.9386 | 0.7491 | 0.8555 | 0.1579 | 0.6263 | 0.5807 | **0.9439** |\n| **Average** | 0.5660 | 0.8039 | 0.8705 | 0.7575 | 0.8374 | 0.1116 | 0.4323 | 0.5895 | **0.8888** |", "caption": "Table 4: Spearman\u2019s rank correlation coefficient (\u03c1\ud835\udf0c\\rhoitalic_\u03c1) for different measurement metrics. Here \u211b\u2062\u21331\u211bsubscript\u21331\\mathcal{RM}_{1}caligraphic_R caligraphic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, \u211b\u2062\u21332\u211bsubscript\u21332\\mathcal{RM}_{2}caligraphic_R caligraphic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u211b\u2062\u21333\u211bsubscript\u21333\\mathcal{RM}_{3}caligraphic_R caligraphic_M start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT are reward models ArmoRM-Llama3-8B-v0.1, Skywork-Reward-Llama-3.1-8B, and Skywork-Reward-Gemma-2-27B respectively. We observe that our proposed CAR shows the highest correlation between the effectiveness of the response generator and the instruction-following capabilities of fine-tuned base models.", "description": "Table 4 presents Spearman's rank correlation coefficients (\u03c1) to compare different metrics for evaluating response generators.  The metrics include three reward models (ArmoRM-Llama3-8B-v0.1, Skywork-Reward-Llama-3.1-8B, and Skywork-Reward-Gemma-2-27B), instruction-following difficulty metrics (IFD-GPT2, IFD-Self, PPL-GPT2, PPL-Self), and response length.  The table shows the correlation between each metric's ranking of response generators and the actual average performance (AP) achieved after fine-tuning five different base models using instruction datasets generated by those response generators.  The key finding is that the proposed Compatibility-Adjusted Reward (CAR) metric exhibits the strongest correlation, indicating its superior ability to predict the effectiveness of a response generator based on its compatibility with the base model.", "section": "4.5 Experimental Results"}, {"content": "| Hyper-parameter | Value |\n|---|---| \n| Learning Rate | 2e-05 |\n| Number of Epochs | 2 |\n| Number of Devices | 4 |\n| Per-device Batch Size | 1 |\n| Gradient Accumulation Steps | 8 |\n| Effective Batch Size | 32 |\n| Optimizer | Adamw |\n| Learning Rate Scheduler | cosine |\n| Warmup Steps | 100 |\n| Max Sequence Length | 4096 |", "caption": "Table 5: This table shows the hyper-parameters for supervised fine-tuning.", "description": "This table details the hyperparameters used in the supervised fine-tuning process of the language models.  It includes the learning rate, number of epochs, batch size, optimizer, learning rate scheduler, and other parameters relevant to the training process.", "section": "3.2 Experimental Setup"}, {"content": "| Base Model | Metric | Phi-3 Mini | Phi-3 Small | Phi-3 Medium | Phi-3 2B | Gemma 2 2B | Gemma 2 9B | Gemma 2 27B | Llama 3 8B | Llama 3 70B | Llama 3.1 405B | Llama 3.1 1.5B | Llama 3.1 7B | Llama 3.1 72B | Qwen2 3B | Qwen2 7B | Qwen2 14B | Qwen2 32B | Qwen2 72B | Qwen2.5 3B | Qwen2.5 7B | Qwen2.5 14B | Qwen2.5 32B | Qwen2.5 72B |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Qwen2-1.5B | AE 2 WR | 3.65 | 3.64 | 2.80 | 5.34 | 6.13 | 5.49 | 3.39 | 3.74 | 2.76 | 3.49 | 3.09 | 2.83 | 4.09 | 3.35 | 5.60 | 6.84 | 5.13 | 5.65 | 7.03 |\n|  | AE 2 LC | 2.85 | 2.98 | 2.18 | 4.16 | 5.60 | 4.99 | 2.64 | 3.10 | 2.10 | 2.74 | 2.36 | 2.68 | 3.47 | 2.82 | 4.50 | 5.66 | 4.38 | 4.96 | 5.83 |\n|  | AH | 1.8 | 1.8 | 1.2 | 4.4 | 5.2 | 4.5 | 1.9 | 2.6 | 2.2 | 2.8 | 2.4 | 1.0 | 3.3 | 1.8 | 2.6 | 4.3 | 4.4 | 3.7 | 4.8 |\n| Gemma 2-2B | AE 2 WR | 6.60 | 6.54 | 4.54 | 16.88 | 11.83 | 12.09 | 7.09 | 8.49 | 7.20 | 9.45 | 8.92 | 2.14 | 7.11 | 6.07 | 7.91 | 12.00 | 8.07 | 9.19 | 16.68 |\n|  | AE 2 LC | 5.90 | 5.89 | 3.99 | 12.93 | 12.51 | 13.09 | 5.70 | 7.13 | 5.63 | 7.32 | 7.11 | 1.91 | 6.45 | 5.46 | 6.84 | 10.94 | 7.53 | 8.77 | 13.85 |\n|  | AH | 3.3 | 4.1 | 2.6 | 12.9 | 9.3 | 9.9 | 5.2 | 5.6 | 4.9 | 5.8 | 5.8 | 0.9 | 5.7 | 3.4 | 6.5 | 7.1 | 8.4 | 6.9 | 9.6 |\n| Qwen2.5-3B | AE 2 WR | 8.19 | 7.79 | 5.97 | 10.52 | 13.57 | 10.01 | 8.07 | 10.17 | 7.91 | 9.68 | 9.12 | 2.98 | 8.54 | 6.86 | 16.22 | 12.76 | 10.32 | 11.71 | 18.42 |\n|  | AE 2 LC | 7.22 | 7.29 | 5.49 | 9.58 | 13.78 | 10.18 | 7.85 | 9.37 | 7.22 | 8.94 | 8.59 | 2.54 | 7.98 | 6.59 | 14.79 | 11.89 | 10.28 | 11.65 | 16.41 |\n|  | AH | 10.5 | 11.0 | 8.3 | 11.8 | 19.4 | 19.6 | 9.7 | 11.4 | 10.9 | 13.8 | 12.7 | 2.1 | 14.4 | 10.6 | 24.8 | 20.4 | 17.9 | 19.9 | 21.2 |\n| Llama-3.2-3B | AE 2 WR | 4.88 | 3.54 | 3.05 | 8.89 | 11.45 | 10.58 | 4.67 | 5.45 | 4.26 | 6.68 | 6.44 | 1.72 | 6.23 | 5.13 | 6.09 | 7.72 | 6.82 | 7.10 | 12.12 |\n|  | AE 2 LC | 4.11 | 2.95 | 2.37 | 7.49 | 10.60 | 9.79 | 3.79 | 4.52 | 3.17 | 5.19 | 5.17 | 1.28 | 5.41 | 4.49 | 5.11 | 6.63 | 5.92 | 6.32 | 9.99 |\n|  | AH | 3.3 | 4.1 | 2.6 | 9.0 | 10.9 | 8.5 | 5.1 | 6.5 | 3.6 | 5.7 | 5.3 | 0.6 | 5.6 | 4.0 | 7.2 | 9.8 | 9.5 | 8.9 | 10.8 |\n| Llama-3.1-Minitron-4B | AE 2 WR | 6.35 | 7.11 | 4.83 | 11.80 | 14.50 | 11.90 | 6.11 | 9.87 | 8.24 | 9.61 | 10.03 | 2.30 | 7.84 | 8.45 | 10.27 | 12.05 | 11.30 | 11.65 | 19.58 |\n|  | AE 2 LC | 5.74 | 6.61 | 4.31 | 10.37 | 16.13 | 12.34 | 4.80 | 8.93 | 6.96 | 8.52 | 9.23 | 2.03 | 7.31 | 8.11 | 9.17 | 11.12 | 10.89 | 11.13 | 17.77 |\n|  | AH | 3.9 | 4.5 | 3.6 | 10.7 | 11.0 | 11.9 | 4.7 | 6.0 | 6.0 | 5.6 | 6.2 | 0.9 | 6.4 | 5.1 | 8.3 | 9.2 | 11.1 | 10.2 | 12.2 |", "caption": "Table 6: This table details benchmark scores of AE2 and AH when tuning different base models with diverse response generators.", "description": "Table 6 presents a detailed breakdown of the performance of various base language models after being fine-tuned using instruction datasets generated by a diverse set of response generators.  The performance is evaluated using two benchmark metrics: AlpacaEval 2 (AE2) and Arena-Hard (AH).  AE2 and AH each provide a win rate (WR) and, in the case of AE2, a length-controlled win rate (LC) score for each model and response generator combination. This allows for a comprehensive comparison of different model and generator pairings across the two benchmarks.", "section": "3.3 Empirical Evaluation"}]