[{"figure_path": "https://arxiv.org/html/2411.07133/x1.png", "caption": "Figure 1: This figure demonstrates the process of instruction tuning and the scope of this paper.", "description": "This figure illustrates the instruction tuning process.  Instruction tuning adapts a pre-trained large language model (LLM) to better follow user instructions. It involves creating an instruction dataset (pairs of instructions and corresponding responses) and fine-tuning the LLM on this dataset. The figure highlights that this paper focuses on the generation of high-quality responses using various response generators (different LLMs). These responses are paired with instructions to build the instruction dataset.  The resulting fine-tuned model's ability to follow instructions is then evaluated.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2411.07133/x2.png", "caption": "Figure 2: Average performance of five base models fine-tuned on various response generators across six model families. We use different colors to distinguish between model families, with darker bars indicating larger response generators within each family.", "description": "This figure displays the average performance results for five base language models that have been fine-tuned using instruction datasets generated by 20 different response generators. The response generators represent seven distinct model families. The x-axis categorizes the response generators by their family and size, while the y-axis represents the average performance score. The color-coding helps differentiate the various model families, with darker shades indicating larger models within each family.  The figure visually demonstrates the effect that different response generators have on the performance of the fine-tuned base models.", "section": "3.3 Empirical Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.07133/x3.png", "caption": "Figure 3: This figure demonstrates the impact of different sampling hyper-parameters when generating responses. We use Gemma-2-9b-it as the response generator. All models are supervised-fine-tuned on the Llama-3.1-Minitron-4B base model.", "description": "This figure displays the results of an experiment investigating how different sampling methods affect the quality of responses generated by a large language model (LLM).  The experiment uses Gemma-2-9b-it as the response generator, which creates responses to a set of instructions. These responses are then used to fine-tune a smaller base model, Llama-3.1-Minitron-4B, via supervised fine-tuning. The figure shows the average performance of the fine-tuned model across various temperature and top-p settings, which are hyperparameters controlling the randomness of the LLM's output. Higher temperature and top-p values generally lead to more diverse and creative, but potentially less coherent, responses. The experiment aims to determine the optimal sampling strategy for generating high-quality training data for instruction tuning.", "section": "3.3 Empirical Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.07133/x4.png", "caption": "Figure 4: This figures demonstrates the response quality measured by three reward models.", "description": "Figure 4 presents the average reward scores obtained from three different reward models (ArmoRM-Llama3-8B-v0.1, Skywork-Reward-Llama-3.1-8B, and Skywork-Reward-Gemma-2-27B) for responses generated by various LLMs. The x-axis displays the response generators, categorized by model family and size, while the y-axis shows the average reward. This visualization helps in assessing the quality of responses produced by different LLMs when used as response generators in instruction tuning. The figure highlights the varying performance of different models as response generators in terms of the quality of their generated responses as evaluated by human preferences via reward models.", "section": "4.3 Baseline Methods Fails to Measure the Effectiveness of Response Generators"}, {"figure_path": "https://arxiv.org/html/2411.07133/x5.png", "caption": "Figure 5: Task categories of the Magpie-100K instruction set used in our study.", "description": "This pie chart visualizes the distribution of instruction types within the Magpie-100K dataset, a subset of 100,000 high-quality instructions used in the study.  The dataset is categorized into several task types, illustrating the variety of instructions included.  This breakdown helps to understand the diversity of the data used to train and evaluate the instruction-tuned language models.", "section": "3.2 Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2411.07133/x6.png", "caption": "Figure 6: Average Output Length of synthetic datasets generated using different response generators (measured in Tokens).", "description": "Figure 6 presents a bar chart illustrating the average length, measured in tokens, of responses generated by various Large Language Models (LLMs) used as response generators in the creation of synthetic instruction datasets.  The x-axis categorizes the different LLMs, while the y-axis represents the average response length.  The chart allows for a comparison of the output lengths produced by different models, highlighting variations in response brevity and verbosity.", "section": "3.2 Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2411.07133/x7.png", "caption": "Figure 7: PPL-GPT2 and IFD-GPT2.", "description": "This figure shows the average response perplexity (PPL) and instruction following difficulty (IFD), both calculated using GPT-2, across different response generators.  The x-axis represents the various response generators used, categorized by model family. The y-axis shows the average PPL and IFD scores. This visualization helps to understand how the quality and difficulty of responses generated by different models vary. Lower PPL indicates higher response quality, while lower IFD suggests less difficulty in following the instructions.", "section": "4.3 Baseline Methods Fails to Measure the Effectiveness of Response Generators"}, {"figure_path": "https://arxiv.org/html/2411.07133/x8.png", "caption": "Figure 8: PPL-Self of five base models.", "description": "This figure displays the perplexity scores (PPL-Self) calculated using each of the five base language models.  The perplexity measures how well each base model predicts the responses generated by different response generators across six model families (Phi-3, Gemma 2, Llama 3, Llama 3.1, Qwen2, and Qwen2.5). The x-axis represents the various response generators within the model families, while the y-axis shows the perplexity values. Lower perplexity indicates better prediction of the generated responses by the corresponding base model.", "section": "3.3 Empirical Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.07133/x9.png", "caption": "Figure 9: IFD-Self of five base models.", "description": "This figure displays the Instruction Following Difficulty (IFD) scores, calculated using each base model itself (IFD-Self), for five different base language models.  Each base model was evaluated using instruction-response pairs generated by twenty different response generators spanning across seven model families: Qwen2, Qwen2.5, Llama 3, Llama 3.1, Gemma 2, Phi-3, and GPT-4.  The x-axis represents the different response generators, grouped by model family, and ordered by increasing size. The y-axis represents the IFD-Self score.  Lower IFD-Self scores indicate that the responses generated by the model were easier for the corresponding base model to process, suggesting better compatibility. The purpose of the figure is to show the compatibility between response generators and different base models, in the context of instruction tuning, thereby helping to explain the Larger Models' Paradox.", "section": "3.3 Empirical Evaluation"}]