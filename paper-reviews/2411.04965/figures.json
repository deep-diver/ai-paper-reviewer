[{"figure_path": "https://arxiv.org/html/2411.04965/x1.png", "caption": "Figure 1: The overview of BitNet a4.8 with both weight and activation quantization. All the parameters are ternery (i.e., 1.58-bit as in BitNet b1.58\u00a0[12]). We use a hybrid quantization\nand sparsification strategy to deal with outlier activations in certain Transformer sub-layers.", "description": "BitNet a4.8 uses a hybrid approach to quantization, combining both weight and activation quantization.  The weights are ternary (1.58 bits), as in its predecessor BitNet b1.58. However, activations are handled differently.  Inputs to the attention and feed-forward network (FFN) layers utilize 4-bit quantization. To manage potential errors caused by outlier values in certain Transformer sub-layers, intermediate states undergo a sparsification process followed by 8-bit quantization. This combination aims to balance quantization efficiency with accuracy.", "section": "2 BitNet a4.8"}, {"figure_path": "https://arxiv.org/html/2411.04965/x2.png", "caption": "Figure 2: The distribution of the inputs to each projection. The visualization is conducted with a 7B BitNet b1.58 model on a subset of the valid set of C4. For the layers that exhibit Gaussian-like distributions, we employ 4-bit activation quantization. For the layers which distributions are sharp, we adopt Q-Sparse\u00a0[18] to perform sparsification on the activations.", "description": "Figure 2 presents the distribution patterns of inputs for different projections within a 7B parameter BitNet b1.58 model, using a subset of the C4 validation dataset.  The visualizations reveal that some layers exhibit Gaussian-like distributions, leading to the application of 4-bit activation quantization. Other layers, however, show sharp distributions, prompting the use of Q-Sparse (a technique described in reference [18] of the paper) for sparsification of the activations. This figure highlights the model's adaptive quantization strategy based on the input distribution characteristics.", "section": "2 BitNet a4.8"}, {"figure_path": "https://arxiv.org/html/2411.04965/x3.png", "caption": "Figure 3: The distribution of the inputs to the output projection of attention with different quantization and sparsification. The visualization is conducted with a 7B BitNet b1.58 model on a subset of the valid set of C4.", "description": "This figure compares the distribution of inputs to the output projection of the attention mechanism in a 7B parameter BitNet b1.58 model under different quantization and sparsification strategies.  It visualizes the impact of these techniques on the distribution of activation values.  The three subplots show how using INT8 (8-bit integer) quantization, INT4 (4-bit integer) quantization, and a combined INT8 quantization with TopK 50% sparsification affects the activation value distribution. This helps illustrate the effectiveness of the hybrid quantization and sparsification approach in managing outliers and maintaining performance. The data is from a subset of the valid set of C4.", "section": "2 BitNet a4.8"}, {"figure_path": "https://arxiv.org/html/2411.04965/x4.png", "caption": "Figure 4: Ablation study on the hybrid quantization and sparsification.", "description": "This figure presents an ablation study comparing the performance of different quantization strategies on a language model.  It shows the training loss curves for a model trained with full 4-bit integer (INT4) quantization, full 4-bit floating-point (FP4) quantization, and the hybrid quantization and sparsification approach (A4.8) proposed in the paper. The graph visualizes how the training loss changes over the number of training tokens used, allowing comparison of the different approaches.", "section": "3.2 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.04965/x5.png", "caption": "Figure 5: Ablation study on different quantization or activation function for the inputs to down projection of FFN.", "description": "This ablation study investigates the impact of various quantization methods (INT4, FP4, INT8) and activation functions (Swish, ReLU2) on the inputs to the feed-forward network's (FFN) down projection.  The figure displays the training loss curves for different configurations, allowing for a comparison of their performance in terms of training perplexity.  It helps determine the optimal combination of quantization and activation function for this specific layer of the model.", "section": "3.2 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.04965/x6.png", "caption": "Figure 6: Ablations on 4-bit quantizers for the inputs to attention and FFN.", "description": "This figure presents an ablation study comparing different 4-bit quantization methods for the inputs of the attention and feed-forward network (FFN) layers.  It shows the training loss curves for various quantization techniques, including floating-point quantization with different exponent and mantissa bit configurations (E1M2 and E2M1 formats using MinMax quantizer), and integer quantization using absmax and absmean quantizers. The goal is to determine which 4-bit quantization approach yields the best training performance for these critical layers in the model.", "section": "3.2 Ablation Study"}]