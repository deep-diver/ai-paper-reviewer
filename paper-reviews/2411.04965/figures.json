[{"figure_path": "https://arxiv.org/html/2411.04965/x1.png", "caption": "Figure 1: The overview of BitNet a4.8 with both weight and activation quantization. All the parameters are ternery (i.e., 1.58-bit as in BitNet b1.58\u00a0[12]). We use a hybrid quantization\nand sparsification strategy to deal with outlier activations in certain Transformer sub-layers.", "description": "BitNet a4.8 employs a hybrid approach to quantization, using 1.58-bit weights (ternary: -1, 0, 1) and a mixed strategy for activations.  For inputs to the attention and feed-forward network layers, it uses 4-bit quantization. However, to mitigate the effects of outlier activations, intermediate states within certain Transformer sub-layers are sparsified (using TopK) before being quantized to 8 bits. This hybrid method balances the benefits of low-bit quantization with a strategy to handle data points that might otherwise cause significant quantization errors. The figure visually depicts this data flow through the network layers.", "section": "2 BitNet a4.8"}, {"figure_path": "https://arxiv.org/html/2411.04965/x2.png", "caption": "Figure 2: The distribution of the inputs to each projection. The visualization is conducted with a 7B BitNet b1.58 model on a subset of the valid set of C4. For the layers that exhibit Gaussian-like distributions, we employ 4-bit activation quantization. For the layers which distributions are sharp, we adopt Q-Sparse\u00a0[18] to perform sparsification on the activations.", "description": "Figure 2 visualizes the distribution of input values for different projection layers within a 7B parameter BitNet b1.58 model.  The distributions are shown for a subset of the validation set (C4).  The key takeaway is the differing approaches used based on the shape of the distribution:  layers with Gaussian-like distributions use 4-bit activation quantization, whereas layers exhibiting sharp, non-Gaussian distributions employ the Q-Sparse technique for sparsification to mitigate the impact of outliers.", "section": "2 BitNet a4.8"}, {"figure_path": "https://arxiv.org/html/2411.04965/x3.png", "caption": "Figure 3: The distribution of the inputs to the output projection of attention with different quantization and sparsification. The visualization is conducted with a 7B BitNet b1.58 model on a subset of the valid set of C4.", "description": "This figure visualizes the impact of different quantization and sparsification techniques on the distribution of inputs to the output projection layer of the attention mechanism within a 7B parameter BitNet b1.58 model.  The distributions are shown for three different scenarios: 8-bit integer quantization, 4-bit integer quantization, and a hybrid approach combining 8-bit integer quantization with 50% top-K sparsification. Each distribution plot represents a layer's input distribution from the attention mechanism, illustrating how different quantization methods affect activation sparsity and data characteristics.  The analysis is performed on a subset of the C4 validation dataset.  The figure highlights the effectiveness of the hybrid quantization/sparsification strategy in managing outlier values.", "section": "2 BitNet a4.8"}, {"figure_path": "https://arxiv.org/html/2411.04965/x4.png", "caption": "Figure 4: Ablation study on the hybrid quantization and sparsification.", "description": "This figure presents an ablation study comparing the training performance of different quantization methods for a large language model.  The study contrasts a fully 4-bit integer (INT4) quantization approach with the hybrid quantization and sparsification strategy proposed in the paper (A4.8).  The x-axis represents the number of training tokens, and the y-axis shows the perplexity, a metric indicating the model's performance. The results demonstrate that the hybrid approach significantly outperforms the fully INT4 quantization method.  A comparison with a full floating point (FP4) quantization is also included. The figure visually shows that the hybrid method is more stable and effective during training.", "section": "3.2 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.04965/x5.png", "caption": "Figure 5: Ablation study on different quantization or activation function for the inputs to down projection of FFN.", "description": "This ablation study investigates the impact of different quantization methods (INT4, FP4, INT8) and activation functions (Swish, ReLU) on the down-projection layer of the feed-forward network (FFN).  The results show how the choice of quantization and activation function affect the training loss of a 1.3B parameter language model.  The experiment aims to determine the optimal combination for minimizing training loss and maximizing model efficiency.", "section": "3.2 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.04965/x6.png", "caption": "Figure 6: Ablations on 4-bit quantizers for the inputs to attention and FFN.", "description": "This figure presents an ablation study comparing different 4-bit quantization methods for the inputs to the attention and feed-forward network (FFN) layers.  It shows the training loss curves for various quantization techniques, including floating-point (FP4) with E1M2 and E2M1 formats using the MinMax quantizer, and integer quantization (INT4) using the absmax and absmean quantizers.  The goal is to determine which quantization method provides the best training performance and efficiency.", "section": "3.2 Ablation Study"}]