[{"content": "| Models | Size | PPL\u2193 | ARCc\u2191 | ARCe\u2191 | HS\u2191 | PQ\u2191 | WGe\u2191 | Avg\u2191 |\n|---|---|---|---|---|---|---|---|---|\n| LLaMA LLM | 700M | 11.44 | 27.13 | 43.27 | 44.70 | 68.12 | 53.99 | 47.44 |\n| BitNet b1.58 | 700M | 12.32 | 25.00 | 42.68 | 42.08 | 66.97 | 54.14 | 46.17 |\n| **BitNet a4.8** (FP4) | 700M | 12.40 | 25.17 | 42.68 | 42.36 | 66.27 | 52.96 | 45.89 |\n| **BitNet a4.8** | 700M | 12.40 | 25.17 | 41.58 | 42.44 | 66.38 | 53.04 | 45.72 |\n| LLaMA LLM | 1.3B | 10.82 | 27.90 | 45.16 | 47.65 | 69.91 | 53.35 | 48.79 |\n| BitNet b1.58 | 1.3B | 11.27 | 27.65 | 45.33 | 46.86 | 68.39 | 54.06 | 48.46 |\n| **BitNet a4.8** (FP4) | 1.3B | 11.38 | 28.50 | 44.36 | 47.03 | 68.61 | 54.06 | 48.51 |\n| **BitNet a4.8** | 1.3B | 11.35 | 28.50 | 44.15 | 46.98 | 68.34 | 54.14 | 48.42 |\n| LLaMA LLM | 3B | 9.61 | 29.95 | 48.11 | 55.25 | 71.76 | 57.46 | 52.51 |\n| BitNet b1.58 | 3B | 9.97 | 29.27 | 49.41 | 54.42 | 70.89 | 57.54 | 52.30 |\n| **BitNet a4.8** (FP4) | 3B | 9.99 | 29.10 | 49.24 | 54.60 | 71.38 | 56.12 | 52.08 |\n| **BitNet a4.8** | 3B | 9.97 | 28.33 | 49.58 | 54.62 | 71.16 | 54.38 | 51.61 |\n| LLaMA LLM | 7B | 9.20 | 33.36 | 51.22 | 58.33 | 73.34 | 58.41 | 54.93 |\n| BitNet b1.58 | 7B | 9.24 | 32.00 | 50.88 | 59.79 | 72.96 | 59.83 | 55.09 |\n| **BitNet a4.8** (FP4) | 7B | 9.42 | 31.57 | 51.22 | 58.20 | 72.47 | 59.59 | 54.61 |\n| **BitNet a4.8** | 7B | 9.37 | 31.66 | 50.88 | 58.78 | 73.01 | 59.35 | 54.74 |", "caption": "Table 1: Perplexity and results of BitNet a4.8, BitNet b1.58 and LLaMA LLM on the end tasks. The standard variance of error for average scores is 1.06%.", "description": "This table presents a comparison of the performance of three different language models: BitNet a4.8, BitNet b1.58, and LLaMA, across various sizes (700M, 1.3B, 3B, and 7B parameters).  The models are evaluated on several downstream tasks, measuring perplexity (PPL) and accuracy on ARC-Easy (ARCe), ARC-Challenge (ARCc), HellaSwag (HS), PIQA (PQ), Winogrande (WGe).  The average score across all tasks is also shown for each model and size.  The standard deviation of the average scores is 1.06%. This allows for a direct comparison of the effectiveness of the different models in terms of their performance and size.", "section": "3.1 Main Results"}, {"content": "| Models | Activated | QKV | Out | Up | Gate | Down | Overall |\n|---|---|---|---|---|---|---|---| \n| **LLaMA LLM** | 679M | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n| BitNet b1.58 | 638M | 1.2 | 5.9 | 1.2 | 1.2 | 21.8 | 6.2 |\n| **BitNet a4.8** | 390M | 12.1 | 50.0 | 66.2 | 12.1 | 80.9 | 42.5 |\n| **LLaMA LLM** | 1.2B | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n| BitNet b1.58 | 1.1B | 1.3 | 5.8 | 1.2 | 1.2 | 22.8 | 6.4 |\n| **BitNet a4.8** | 0.7B | 12.0 | 50.0 | 65.9 | 12.1 | 81.8 | 42.7 |\n| **LLaMA LLM** | 3.2B | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n| BitNet b1.58 | 3.0B | 1.4 | 7.1 | 1.3 | 1.3 | 30.0 | 8.2 |\n| **BitNet a4.8** | 1.8B | 12.1 | 50.0 | 70.7 | 12.1 | 85.6 | 44.7 |\n| **LLaMA LLM** | 6.5B | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n| BitNet b1.58 | 6.0B | 1.7 | 11.2 | 1.4 | 1.4 | 24.2 | 7.3 |\n| **BitNet a4.8** | 3.4B | 12.1 | 50.0 | 71.4 | 12.0 | 84.2 | 44.5 |", "caption": "Table 2: Detailed sparsity of BitNet a4.8, BitNet b1.58 and LLaMA LLM on the valid set of C4.", "description": "This table presents a detailed breakdown of the sparsity (percentage of non-activated parameters) across different components of the BitNet a4.8, BitNet b1.58, and LLaMA LLMs.  The components analyzed include query, key, value (QKV) projections, output projections, up and gate projections, down projections within the attention and feed-forward networks, and the overall model.  Sparsity is calculated using the valid set of C4 dataset.  The table helps to illustrate the significant reduction in active parameters achieved by BitNet a4.8 compared to BitNet b1.58 and LLaMA LLMs.", "section": "2 BitNet a4.8"}, {"content": "| Models | Size | ARCc\u2191 | ARCe\u2191 | HS\u2191 | PQ\u2191 | WGe\u2191 | Avg\u2191 |\n|---|---|---|---|---|---|---|---| \n| BitNet a4.8 | 3B | 28.33 | 49.58 | 54.62 | 71.16 | 54.38 | 51.61 |\n| w/ 4-bit KV |  | 28.24 | 48.86 | 54.41 | 71.87 | 55.49 | 51.77 |\n| w/ 4-bit QKV |  | 27.30 | 48.91 | 54.32 | 71.98 | 56.75 | 51.85 |\n| w/ 4-bit Q, 3-bit KV |  | 28.84 | 48.91 | 53.87 | 70.95 | 56.35 | 51.78 |\n| BitNet a4.8 | 7B | 31.66 | 50.88 | 58.78 | 73.01 | 59.35 | 54.74 |\n| w/ 4-bit KV |  | 31.40 | 50.93 | 58.68 | 73.12 | 60.85 | 55.00 |\n| w/ 4-bit QKV |  | 30.63 | 51.30 | 58.45 | 72.52 | 59.83 | 54.55 |\n| w/ 4-bit Q, 3-bit KV |  | 31.14 | 50.93 | 58.07 | 72.96 | 59.04 | 54.43 |", "caption": "Table 3: Detailed results of BitNet a4.8 with QKV states varying bit-widths on the end tasks. We reported the zero-shot accuracy of all models.", "description": "This table presents a detailed comparison of BitNet a4.8's performance on various downstream tasks under different quantization configurations of the Query, Key, and Value (QKV) states within the attention mechanism.  Specifically, it shows the impact of using 4-bit quantization for the QKV states, and the combined effects of 4-bit QKV and 3-bit Key/Value (KV) quantization. The zero-shot accuracy results for these different configurations are provided for several model sizes, providing insights into the trade-offs between quantization levels and performance.", "section": "3.2 Ablation Study"}, {"content": "| Quantization | Sparsification | PPL\u2193 | ARCc\u2191 | ARCe\u2191 | HS\u2191 | PQ\u2191 | WGe\u2191 | Avg\u2191 |\n|---|---|---|---|---|---|---|---|---|\n| INT8 | - | 9.95 | 28.33 | 48.53 | 54.90 | 72.31 | 56.51 | 52.11 |\n| INT8 | TopK 50% | 9.97 | 28.33 | 49.58 | 54.62 | 71.16 | 54.38 | 51.61 |", "caption": "Table 4: Ablations on the TopK sparsification for the inputs to the output projection of attention.", "description": "This table presents an ablation study on the impact of TopK sparsification applied to the input of the output projection within the attention mechanism. It compares the performance (PPL, ARCc, ARCe, HS, PQ, WGe, Avg) of a model with and without TopK sparsification, shedding light on the effectiveness of this technique for improving efficiency.", "section": "3.2 Ablation Study"}, {"content": "| Models | HS | PQ | WGe | OBQA | Lambada | MMLU | ARCc | ARCe | Avg |\n|---|---|---|---|---|---|---|---|---|---| \n| BitNet b1.58 2B | 68.66 | 77.09 | 62.58 | 41.40 | 63.36 | 50.29 | 47.61 | 70.74 | 60.22 |\n| **BitNet a4.8 2B** | 68.21 | 76.55 | 64.40 | 40.60 | 63.75 | 50.30 | 46.59 | 70.00 | 60.05 |", "caption": "Table 5: Results of BitNet a4.8 and BitNet b1.58 with 2B parameters and 2T training tokens.", "description": "This table presents a comparison of the performance of BitNet a4.8 and BitNet b1.58, both trained with 2 billion parameters and 2 trillion training tokens.  It shows the results on several downstream tasks, offering insights into the relative performance of these models, which use different activation quantization strategies, under a larger training regime than previously explored.", "section": "3.3 More Training Tokens"}, {"content": "| Size | Hidden Size | GLU Size | #Heads | #Layers | Batch Size | # Tokens | Seq Length |\n|---|---|---|---|---|---|---|---| \n| 700M | 1536 | 4096 | 24 | 24 | 1M | 100B | 2048 |\n| 1.3B | 2048 | 5460 | 32 | 24 | 1M | 100B | 2048 |\n| 3B | 3200 | 8640 | 32 | 26 | 1M | 100B | 2048 |\n| 7B | 4096 | 11008 | 32 | 32 | 1M | 100B | 2048 |", "caption": "Table 6: Model configurations for both BitNet a4.8, BitNet b1.58 and LLaMA LLM.", "description": "This table details the architectural hyperparameters used for training three different large language models: BitNet a4.8, BitNet b1.58, and LLaMA.  For each model, it lists the model size, hidden layer size, GLU (Gated Linear Unit) size, number of attention heads, number of layers, batch size used during training, total number of training tokens, and the sequence length used.", "section": "2 BitNet a4.8"}, {"content": "| Model | Size | Learning Rate | Weight Decay | Warm-up | Adam \u03b2 | \n|---|---|---|---|---|---| \n| BitNet a4.8 | 700M | 1.5e-3 \u2192 1e-3 | 0.1 \u2192 0 | 375 | (0.9, 0.95) | \n|  | 1.3B | 1.2e-3 \u2192 8e-4 | 0.1 \u2192 0 | 375 | (0.9, 0.95) | \n|  | 3B | 1.2e-3 \u2192 6.4e-4 | 0.1 \u2192 0 | 375 | (0.9, 0.95) | \n|  | 7B | 1e-3 \u2192 6e-4 | 0.1 \u2192 0 | 375 | (0.9, 0.95) | \n| LLaMA LLM | 700M | 2.5e-4 | 0.1 | 375 | (0.9, 0.95) | \n|  | 1.3B | 2.0e-4 | 0.1 | 375 | (0.9, 0.95) | \n|  | 3B | 2.0e-4 | 0.1 | 375 | (0.9, 0.95) | \n|  | 7B | 1.5e-4 | 0.1 | 375 | (0.9, 0.95) | ", "caption": "Table 7: Hyper-parameters for both BitNet a4.8 and LLaMA LLM training.", "description": "This table details the hyperparameters used during the training process for both BitNet a4.8 and the LLaMA Large Language Model (LLM).  It includes the model size, learning rate, weight decay, warm-up period, and Adam optimizer parameters (beta1 and beta2).  Different values are shown for different sizes of the models, reflecting adjustments needed for optimal training across various model scales.", "section": "3 Experiments"}]