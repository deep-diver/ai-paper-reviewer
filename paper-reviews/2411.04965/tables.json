[{"content": "| Models | Size | PPL\u2193 | ARCc\u2191 | ARCe\u2191 | HS\u2191 | PQ\u2191 | WGe\u2191 | Avg\u2191 |\n|---|---|---|---|---|---|---|---|---|\n| LLaMA LLM | 700M | 11.44 | 27.13 | 43.27 | 44.70 | 68.12 | 53.99 | 47.44 |\n| BitNet b1.58 |  | 12.32 | 25.00 | 42.68 | 42.08 | 66.97 | 54.14 | 46.17 |\n| **BitNet a4.8** (FP4) |  | 12.40 | 25.17 | 42.68 | 42.36 | 66.27 | 52.96 | 45.89 |\n| **BitNet a4.8** |  | 12.40 | 25.17 | 41.58 | 42.44 | 66.38 | 53.04 | 45.72 |\n| LLaMA LLM | 1.3B | 10.82 | 27.90 | 45.16 | 47.65 | 69.91 | 53.35 | 48.79 |\n| BitNet b1.58 |  | 11.27 | 27.65 | 45.33 | 46.86 | 68.39 | 54.06 | 48.46 |\n| **BitNet a4.8** (FP4) |  | 11.38 | 28.50 | 44.36 | 47.03 | 68.61 | 54.06 | 48.51 |\n| **BitNet a4.8** |  | 11.35 | 28.50 | 44.15 | 46.98 | 68.34 | 54.14 | 48.42 |\n| LLaMA LLM | 3B | 9.61 | 29.95 | 48.11 | 55.25 | 71.76 | 57.46 | 52.51 |\n| BitNet b1.58 |  | 9.97 | 29.27 | 49.41 | 54.42 | 70.89 | 57.54 | 52.30 |\n| **BitNet a4.8** (FP4) |  | 9.99 | 29.10 | 49.24 | 54.60 | 71.38 | 56.12 | 52.08 |\n| **BitNet a4.8** |  | 9.97 | 28.33 | 49.58 | 54.62 | 71.16 | 54.38 | 51.61 |\n| LLaMA LLM | 7B | 9.20 | 33.36 | 51.22 | 58.33 | 73.34 | 58.41 | 54.93 |\n| BitNet b1.58 |  | 9.24 | 32.00 | 50.88 | 59.79 | 72.96 | 59.83 | 55.09 |\n| **BitNet a4.8** (FP4) |  | 9.42 | 31.57 | 51.22 | 58.20 | 72.47 | 59.59 | 54.61 |\n| **BitNet a4.8** |  | 9.37 | 31.66 | 50.88 | 58.78 | 73.01 | 59.35 | 54.74 |", "caption": "Table 1: Perplexity and results of BitNet a4.8, BitNet b1.58 and LLaMA LLM on the end tasks. The standard variance of error for average scores is 1.06%.", "description": "This table presents a comparison of the performance of three different language models: BitNet a4.8, BitNet b1.58, and LLaMA, across various sizes (700M, 1.3B, 3B, and 7B parameters).  For each model size, it shows the perplexity (PPL) score on a held-out dataset and zero-shot accuracy scores on several downstream tasks: ARC-Challenge (ARCc), ARC-Easy (ARCe), HellaSwag (HS), PIQA (PQ), Winogrande (WGe).  The average of these downstream task scores is also provided. The table highlights the comparable performance of BitNet a4.8 to BitNet b1.58, despite BitNet a4.8 using 4-bit activations, suggesting improved efficiency.", "section": "3.1 Main Results"}, {"content": "| Models | Activated | QKV | Out | Up | Gate | Down | Overall |\n|---|---|---|---|---|---|---|---| \n| **LLaMA LLM** | 679M | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n| BitNet b1.58 | 638M | 1.2 | 5.9 | 1.2 | 1.2 | 21.8 | 6.2 |\n| **BitNet a4.8** | 390M | 12.1 | 50.0 | 66.2 | 12.1 | 80.9 | 42.5 |\n| **LLaMA LLM** | 1.2B | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n| BitNet b1.58 | 1.1B | 1.3 | 5.8 | 1.2 | 1.2 | 22.8 | 6.4 |\n| **BitNet a4.8** | 0.7B | 12.0 | 50.0 | 65.9 | 12.1 | 81.8 | 42.7 |\n| **LLaMA LLM** | 3.2B | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n| BitNet b1.58 | 3.0B | 1.4 | 7.1 | 1.3 | 1.3 | 30.0 | 8.2 |\n| **BitNet a4.8** | 1.8B | 12.1 | 50.0 | 70.7 | 12.1 | 85.6 | 44.7 |\n| **LLaMA LLM** | 6.5B | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n| BitNet b1.58 | 6.0B | 1.7 | 11.2 | 1.4 | 1.4 | 24.2 | 7.3 |\n| **BitNet a4.8** | 3.4B | 12.1 | 50.0 | 71.4 | 12.0 | 84.2 | 44.5 |", "caption": "Table 2: Detailed sparsity of BitNet a4.8, BitNet b1.58 and LLaMA LLM on the valid set of C4.", "description": "This table presents a detailed breakdown of the sparsity (percentage of inactive parameters) observed in different components of three large language models (LLMs): BitNet a4.8, BitNet b1.58, and LLaMA LLM.  Sparsity is calculated for various model sizes (700M, 1.3B, 3B, and 7B parameters). The components analyzed include the Query, Key, Value (QKV) projections in the self-attention mechanism, the output projection of the attention, the feed-forward network's (FFN's) up and gate projections, the FFN's down projection, and an overall sparsity measure that aggregates all components. The data reflects the sparsity levels observed on the validation set of the C4 dataset.", "section": "2 BitNet a4.8"}, {"content": "| Models | Size | ARCc\u2191 | ARCe\u2191 | HS\u2191 | PQ\u2191 | WGe\u2191 | Avg\u2191 |\n|---|---|---|---|---|---|---|---| \n| BitNet a4.8 | 3B | 28.33 | 49.58 | 54.62 | 71.16 | 54.38 | 51.61 |\n| w/ 4-bit KV |  | 28.24 | 48.86 | 54.41 | 71.87 | 55.49 | 51.77 |\n| w/ 4-bit QKV |  | 27.30 | 48.91 | 54.32 | 71.98 | 56.75 | 51.85 |\n| w/ 4-bit Q, 3-bit KV |  | 28.84 | 48.91 | 53.87 | 70.95 | 56.35 | 51.78 |\n| BitNet a4.8 | 7B | 31.66 | 50.88 | 58.78 | 73.01 | 59.35 | 54.74 |\n| w/ 4-bit KV |  | 31.40 | 50.93 | 58.68 | 73.12 | 60.85 | 55.00 |\n| w/ 4-bit QKV |  | 30.63 | 51.30 | 58.45 | 72.52 | 59.83 | 54.55 |\n| w/ 4-bit Q, 3-bit KV |  | 31.14 | 50.93 | 58.07 | 72.96 | 59.04 | 54.43 |", "caption": "Table 3: Detailed results of BitNet a4.8 with QKV states varying bit-widths on the end tasks. We reported the zero-shot accuracy of all models.", "description": "This table presents a detailed comparison of BitNet a4.8's performance on various downstream tasks using different bit-widths for the Query, Key, and Value (QKV) states within the attention mechanism.  It shows zero-shot accuracy results for the model, varying the precision of the QKV states (4-bit or 3-bit) to analyze the effect on overall model performance. This helps to understand the trade-off between model accuracy and efficiency by reducing the precision of the QKV states.", "section": "3 Experiments"}, {"content": "| Quantization | Sparsification | PPL\u2193 | ARCc\u2191 | ARCe\u2191 | HS\u2191 | PQ\u2191 | WGe\u2191 | Avg\u2191 |\n|---|---|---|---|---|---|---|---|---|\n| INT8 | - | 9.95 | 28.33 | 48.53 | 54.90 | 72.31 | 56.51 | 52.11 |\n| INT8 | TopK 50% | 9.97 | 28.33 | 49.58 | 54.62 | 71.16 | 54.38 | 51.61 |", "caption": "Table 4: Ablations on the TopK sparsification for the inputs to the output projection of attention.", "description": "This table presents ablation study results focusing on the impact of TopK sparsification applied to the input features before the output projection within the attention mechanism.  It compares the performance (PPL, ARCc, ARCe, HS, PQ, WGe, Avg) of models with and without TopK sparsification, providing insights into the effectiveness of this sparsification technique in improving efficiency without sacrificing accuracy.", "section": "3.2 Ablation Study"}, {"content": "| Models | HS | PQ | WGe | OBQA | Lambada | MMLU | ARCc | ARCe | Avg |\n|---|---|---|---|---|---|---|---|---|---|\n| BitNet b1.58 2B | 68.66 | 77.09 | 62.58 | 41.40 | 63.36 | 50.29 | 47.61 | 70.74 | 60.22 |\n| **BitNet a4.8 2B** | 68.21 | 76.55 | 64.40 | 40.60 | 63.75 | 50.30 | 46.59 | 70.00 | 60.05 |", "caption": "Table 5: Results of BitNet a4.8 and BitNet b1.58 with 2B parameters and 2T training tokens.", "description": "This table presents a comparison of the performance of BitNet a4.8 and BitNet b1.58, both with 2 billion parameters, trained using 2 trillion tokens.  It showcases their performance across multiple downstream tasks, including HellaSwag (HS), PIQA (PQ), Winogrande (WGe), OBQA, Lambada, MMLU, ARC-Challenge (ARCC), and ARC-Easy (ARCe), offering a comprehensive evaluation of their capabilities with a large training dataset.", "section": "3.3 More Training Tokens"}, {"content": "| Size | Hidden Size | GLU Size | #Heads | #Layers | Batch Size | # Tokens | Seq Length |\n|---|---|---|---|---|---|---|---| \n| 700M | 1536 | 4096 | 24 | 24 | 1M | 100B | 2048 |\n| 1.3B | 2048 | 5460 | 32 | 24 | 1M | 100B | 2048 |\n| 3B | 3200 | 8640 | 32 | 26 | 1M | 100B | 2048 |\n| 7B | 4096 | 11008 | 32 | 32 | 1M | 100B | 2048 |", "caption": "Table 6: Model configurations for both BitNet a4.8, BitNet b1.58 and LLaMA LLM.", "description": "This table details the architectural hyperparameters for three large language models: BitNet a4.8, BitNet b1.58, and LLaMA.  For each model, it lists the model size, hidden layer size, GLU (Gated Linear Unit) size, number of attention heads, number of layers, batch size used during training, the number of training tokens, and the sequence length.", "section": "2 BitNet a4.8"}, {"content": "| Model | Size | Learning Rate | Weight Decay | Warm-up | Adam \u03b2 | \n|---|---|---|---|---|---| \n| BitNet a4.8 | 700M | 1.5e-3\u21921e-3 | 0.1\u21920 | 375 | (0.9, 0.95) | \n|  | 1.3B | 1.2e-3\u21928e-4 | 0.1\u21920 | 375 | (0.9, 0.95) | \n|  | 3B | 1.2e-3\u21926.4e-4 | 0.1\u21920 | 375 | (0.9, 0.95) | \n|  | 7B | 1e-3\u21926e-4 | 0.1\u21920 | 375 | (0.9, 0.95) | \n| LLaMA LLM | 700M | 2.5e-4 | 0.1 | 375 | (0.9, 0.95) | \n|  | 1.3B | 2.0e-4 | 0.1 | 375 | (0.9, 0.95) | \n|  | 3B | 2.0e-4 | 0.1 | 375 | (0.9, 0.95) | \n|  | 7B | 1.5e-4 | 0.1 | 375 | (0.9, 0.95) | ", "caption": "Table 7: Hyper-parameters for both BitNet a4.8 and LLaMA LLM training.", "description": "This table details the hyperparameters used during the training process for both BitNet a4.8 and the LLaMA Large Language Model (LLM).  It includes the model size, learning rate (with its decay schedule), weight decay, warmup steps, and Adam optimization parameters (beta1 and beta2).  These hyperparameters are crucial for configuring the training process to optimize performance and stability for the respective models.", "section": "3 Experiments"}]