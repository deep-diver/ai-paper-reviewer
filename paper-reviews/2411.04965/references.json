{"references": [{"fullname_first_author": "Shuming Ma", "paper_title": "The era of 1-bit LLMs: All large language models are in 1.58 bits", "publication_date": "2024-02-17", "reason": "This paper introduces BitNet b1.58, a foundational 1-bit LLM model that BitNet a4.8 builds upon, making it a crucial reference for understanding the context and advancements of this work."}, {"fullname_first_author": "Hongyu Wang", "paper_title": "BitNet: Scaling 1-bit transformers for large language models", "publication_date": "2023-10-11", "reason": "This paper details the architecture and training methodology of BitNet, the predecessor of BitNet a4.8, providing essential background information and context."}, {"fullname_first_author": "Hongyu Wang", "paper_title": "Q-sparse: All large language models can be fully sparsely-activated", "publication_date": "2024-07-06", "reason": "This paper introduces the Q-Sparse technique, a key component of BitNet a4.8's hybrid quantization and sparsification strategy, which is directly used in the model's design."}, {"fullname_first_author": "Yoshua Bengio", "paper_title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "publication_date": "2013-08-13", "reason": "This paper introduces the Straight-Through Estimator (STE) gradient approximation technique, which is crucial for training low-bit neural networks and employed by BitNet a4.8."}, {"fullname_first_author": "Tim Dettmers", "paper_title": "LLM.int8(): 8-bit matrix multiplication for transformers at scale", "publication_date": "2022-XX-XX", "reason": "This paper explores 8-bit matrix multiplication for transformers, providing relevant background on low-bit numerical computation techniques used in BitNet a4.8."}]}