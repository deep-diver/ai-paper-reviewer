{"references": [{"fullname_first_author": "Shuming Ma", "paper_title": "The era of 1-bit LLMs: All large language models are in 1.58 bits", "publication_date": "2024-02-17", "reason": "This paper introduces BitNet b1.58, a foundational 1-bit LLM model that BitNet a4.8 builds upon and improves."}, {"fullname_first_author": "Hongyu Wang", "paper_title": "BitNet: Scaling 1-bit transformers for large language models", "publication_date": "2023-10-11", "reason": "This paper introduces the BitNet architecture, which BitNet a4.8 extends to utilize 4-bit activations."}, {"fullname_first_author": "Hongyu Wang", "paper_title": "Q-sparse: All large language models can be fully sparsely-activated", "publication_date": "2024-07-06", "reason": "This paper introduces the Q-Sparse sparsification strategy that BitNet a4.8 incorporates to handle outlier activations."}, {"fullname_first_author": "Yoshua Bengio", "paper_title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "publication_date": "2013-08-13", "reason": "This paper introduces the straight-through estimator (STE) gradient approximation technique used in BitNet a4.8 training."}, {"fullname_first_author": "Tim Dettmers", "paper_title": "LLM.int8(): 8-bit matrix multiplication for transformers at scale", "publication_date": "2022-XX-XX", "reason": "This paper explores low-bit matrix multiplication techniques, providing relevant background for the low-bit activation approach in BitNet a4.8."}]}