[{"figure_path": "2410.18572/charts/charts_2_0.png", "caption": "Figure 1: Model Performance Comparison. a) Perplexity across different context lengths. Lower perplexity indicates better performance. b) Latency comparison of models at various generation lengths. Taipan exhibits significantly lower latency and superior scaling compared to other strong baselines for longer sequences.", "description": "Figure 1 shows the perplexity and latency of Taipan compared to other models across different context lengths and generation lengths, demonstrating Taipan's superior performance and scaling.", "section": "4.4 LONG-CONTEXT EXTRAPOLATION"}, {"figure_path": "2410.18572/charts/charts_2_1.png", "caption": "Figure 1: Model Performance Comparison. a) Perplexity across different context lengths. Lower perplexity indicates better performance. b) Latency comparison of models at various generation lengths. Taipan exhibits significantly lower latency and superior scaling compared to other strong baselines for longer sequences.", "description": "The chart compares the perplexity and latency of four different language models (Transformer, Jamba, Mamba, and Taipan) across varying context and generation lengths.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18572/charts/charts_5_0.png", "caption": "Figure 3: Attention mechanisms in Taipan's Selective Attention Layers. White areas indicate no attention. (a) Full Causal Attention (b) Sliding Window Attention (w = 4) (c) Selective Attention (C = 0.3, w = 5)", "description": "The chart compares three different attention mechanisms: full causal attention, sliding window attention, and Taipan's selective attention, showing their attention weight distributions.", "section": "3.1 SELECTIVE ATTENTION LAYERS"}, {"figure_path": "2410.18572/charts/charts_9_0.png", "caption": "Figure 5: Effect of Attention Budget Capacity C on Taipan's Performance", "description": "The chart displays Taipan's performance on SWDE and Hellaswag tasks at various attention budget capacities (C), showing optimal performance at C=0.15.", "section": "5.1 EFFECT OF ATTENTION BUDGET CAPACITY"}, {"figure_path": "2410.18572/charts/charts_10_0.png", "caption": "Figure 6: Perplexity comparison of Taipan variants with and without Positional Embeddings across different context lengths. Lower perplexity indicates better performance.", "description": "The chart compares the perplexity of Taipan models with and without positional embeddings across various sequence lengths.", "section": "5.2 IMPACT OF POSITIONAL EMBEDDINGS"}]