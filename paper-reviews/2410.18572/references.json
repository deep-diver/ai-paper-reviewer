{"references": [{" publication_date": "2017", "fullname_first_author": "Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduced the Transformer architecture, a cornerstone of modern NLP.  Its impact is foundational as it revolutionized many NLP tasks and is directly relevant to Taipan, which builds upon and improves upon Transformer limitations.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Katharopoulos", "paper_title": "Transformers are RNNs: Fast autoregressive transformers with linear attention", "reason": "This work is crucial for understanding the trade-offs between efficiency and accuracy in attention mechanisms.  Its exploration of linear attention is directly relevant to Taipan's design, which aims to balance efficiency and expressiveness.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Brown", "paper_title": "Language models are few-shot learners", "reason": "This influential paper demonstrates the capabilities of large language models and the potential of few-shot learning. It provides a critical background for understanding the context of Taipan's development and the significance of its long-context capabilities.", "section_number": 1}, {" publication_date": "2021a", "fullname_first_author": "Gu", "paper_title": "Efficiently modeling long sequences with structured state spaces", "reason": "This paper introduced the S4 architecture, a key precursor to Taipan.  It establishes the foundation of structured state-space models (SSMs) and their potential in language modeling, forming the basis of Taipan's efficiency.", "section_number": 2}, {" publication_date": "2021b", "fullname_first_author": "Gu", "paper_title": "Combining recurrent, convolutional, and continuous-time models with linear state space layers", "reason": "This paper is a key contribution to the evolution of SSMs and explores the hybrid approaches to long-sequence modeling.  Its combination of different model types is highly relevant to Taipan's own hybrid design.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Gupta", "paper_title": "Diagonal state spaces are as effective as structured state spaces", "reason": "This research further advances the development of SSMs, demonstrating the effectiveness of simpler structures while maintaining performance.  This insight directly influences Taipan's design choices regarding efficiency.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "reason": "This is the most directly related work to Taipan.  Mamba provides the foundational SSM architecture that Taipan builds upon, adding selective attention for improved performance in long-range dependency tasks.  Understanding Mamba is critical to understanding Taipan.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "Llama serves as an important baseline model in Taipan's experiments, showcasing its capabilities and limitations in long-context modeling, which Taipan directly addresses and aims to improve upon.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Bulatov", "paper_title": "Scaling transformer to 1m tokens and beyond with rmt", "reason": "This paper tackles the challenge of long-context processing, which is the central focus of Taipan.  Its approach and results provide a benchmark for Taipan's achievement in handling extremely long sequences.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Dao", "paper_title": "Transformers are ssms: Generalized models and efficient algorithms through structured state space duality", "reason": "This recent work significantly advances the understanding of SSMs, providing a theoretical framework that directly informs Taipan's architecture and design choices.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Arora", "paper_title": "Simple linear attention language models balance the recall-throughput tradeoff", "reason": "This paper presents an alternative approach to attention mechanisms, directly relevant to Taipan's focus on efficient long-context modeling. The insights on balancing recall and efficiency are directly applicable to Taipan's design.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Lieber", "paper_title": "Jamba: A hybrid transformer-mamba language model", "reason": "Jamba serves as a key comparative model in Taipan's experiments, allowing for a direct comparison of its hybrid approach with Taipan's innovative design, highlighting the advantages and novel contributions of Taipan.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Waleffe", "paper_title": "An empirical study of mamba-based language models", "reason": "This paper provides further insights into the capabilities and limitations of Mamba models, offering a crucial perspective on the advancements presented by Taipan.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Wen", "paper_title": "Rnns are not transformers (yet): The key bottleneck on in-context retrieval", "reason": "This work directly addresses the challenges in long-context retrieval, one of Taipan's key focus areas.  Its findings directly support the need for Taipan's novel design and the importance of handling long-range dependencies efficiently.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Jang", "paper_title": "Categorical reparameterization with gumbel-softmax", "reason": "This work provides the theoretical foundation for the Gumbel-Softmax trick used in Taipan's selective attention layers, enabling differentiability and enabling end-to-end training of the model, which is crucial for Taipan's design and successful implementation.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Shazeer", "paper_title": "Glu variants improve transformer", "reason": "This work details the SwiGLU activation function, used in Taipan's architecture to improve efficiency and performance.  SwiGLU provides the theoretical underpinnings for a component of Taipan's design.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Beltagy", "paper_title": "Longformer: The long-document transformer", "reason": "This paper introduces the sliding window attention mechanism, an important aspect of Taipan's efficiency-enhancing strategy.  It shows the effectiveness of limiting attention scope for long sequences and inspires a similar approach in Taipan's architecture.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Merrill", "paper_title": "The illusion of state in state-space models", "reason": "This research provides crucial insights into the assumptions and limitations of State Space Models (SSMs), which are directly relevant to Taipan's design and highlight the reasons why a hybrid approach might be beneficial.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Poli", "paper_title": "Hyena hierarchy: Towards larger convolutional language models", "reason": "This recent work showcases another approach to long-context modeling that serves as a relevant comparison for Taipan.  It provides context and perspective on the challenges involved in scaling to handle extremely long sequences.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Glorioso", "paper_title": "Zamba: A compact 7b ssm hybrid model", "reason": "This paper offers another example of hybrid models blending SSMs and other techniques, providing a useful point of comparison for Taipan's design and performance. The results help to contextualize Taipan's achievements in the hybrid approach.", "section_number": 6}]}