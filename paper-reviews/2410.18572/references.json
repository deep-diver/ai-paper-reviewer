{"references": [{" publication_date": "2017", "fullname_first_author": "A Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduced the Transformer architecture, a groundbreaking model that revolutionized NLP.  Its self-attention mechanism, although computationally expensive for long sequences, established a new standard for language modeling and remains highly influential in the field.  Understanding its strengths and limitations is crucial to developing more efficient long-context models.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper demonstrated the remarkable few-shot learning capabilities of large language models (LLMs), showcasing their ability to perform well on various tasks with limited labeled data. This finding profoundly impacted the NLP landscape and influenced the development of more efficient long-context models that can leverage these capabilities without the computational burden of traditional fine-tuning.", "section_number": 1}, {" publication_date": "2021a", "fullname_first_author": "Albert Gu", "paper_title": "Efficiently modeling long sequences with structured state spaces", "reason": "This foundational paper introduced the structured State Space Model (SSM), S4, which offers significant advantages over traditional recurrent neural networks for modeling long sequences.  This work laid the groundwork for subsequent improvements in SSMs, eventually leading to architectures like Mamba, which are central to Taipan's design. ", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Simran Arora", "paper_title": "Simple linear attention language models balance the recall-throughput tradeoff", "reason": "This paper highlights the limitations of attention-based models for long sequences. It proposes Linear Attention as a compromise between computational efficiency and performance, addressing the challenge of scaling attention mechanisms to long contexts.  The insights presented are relevant to Taipan\u2019s design and its approach to addressing the computational complexity.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Eric Jang", "paper_title": "Categorical reparameterization with gumbel-softmax", "reason": "This paper introduced the Gumbel-Softmax trick, a crucial technique for making discrete variables differentiable.  Taipan leverages this method to make the token selection process differentiable during training, enabling end-to-end training of the entire model.  This is a key technical contribution that enables the efficient training of Taipan.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Iz Beltagy", "paper_title": "Longformer: The long-document transformer", "reason": "This paper addressed the long-sequence problem by introducing the Longformer architecture. Although not directly adopted in Taipan, the Longformer's efficient attention mechanisms, which handle long-range dependencies, are relevant to Taipan's selective attention approach. Understanding Longformer\u2019s strategies for handling long sequences is important for the context of efficient long-context models.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Angelos Katharopoulos", "paper_title": "Transformers are RNNs: Fast autoregressive transformers with linear attention", "reason": "This paper explored linear attention mechanisms, providing a crucial theoretical foundation for efficient attention.  It highlights the trade-offs between computational efficiency and the ability to capture complex relationships between tokens. Linear attention is fundamental to understanding the computational complexity issues addressed by Taipan and informs the design of its selective attention layers.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Albert Gu", "paper_title": "Transformers are ssms: Generalized models and efficient algorithms through structured state space duality", "reason": "This work established a theoretical connection between Transformers and SSMs, showing their underlying mathematical duality.  This unification provides a crucial theoretical underpinning for understanding the strengths and limitations of SSMs and their ability to handle long-range dependencies, as well as for developing new efficient hybrid models like Taipan.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tri Dao", "paper_title": "Transformers are ssms: Generalized models and efficient algorithms through structured state space duality", "reason": "This paper provides an in-depth mathematical analysis and shows the duality between transformers and state space models (SSMs). This duality forms the theoretical foundation upon which Taipan is built, providing the mathematical framework for combining the efficiency of SSMs with the expressiveness of attention mechanisms. ", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "reason": "This paper introduces Mamba, a state-of-the-art SSM that serves as a crucial building block for Taipan.  Mamba\u2019s efficient architecture and its ability to handle long sequences are essential to Taipan's design and are directly used to build up the foundation of the model. Understanding Mamba\u2019s strengths and limitations is key to grasping Taipan\u2019s innovation. ", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Opher Lieber", "paper_title": "Jamba: A hybrid transformer-mamba language model", "reason": "Jamba is a hybrid model, similarly combining Transformers and SSMs; thus, its performance is directly comparable with Taipan\u2019s, providing a strong baseline for evaluation.  Studying Jamba's design and performance offers crucial insights into the effectiveness of hybrid approaches and how Taipan improves upon them.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "LLaMA is a strong baseline model used in Taipan\u2019s experiments, allowing for a fair comparison of Taipan\u2019s performance against a well-established and widely used architecture. The selection of LLaMA as a baseline enhances the credibility and significance of Taipan's experimental results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Simran Arora", "paper_title": "Simple linear attention language models balance the recall-throughput tradeoff", "reason": "This paper offers valuable insights into the trade-offs between computational efficiency and performance in language models, directly relevant to Taipan\u2019s design considerations.  The discussion of linear attention and its limitations provides a crucial context for Taipan\u2019s selective attention approach, highlighting how Taipan balances efficiency and expressiveness.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Pranav Rajpurkar", "paper_title": "Know what you don't know: Unanswerable questions for squad", "reason": "SQUAD is a widely used benchmark dataset for question-answering tasks. Its inclusion in Taipan\u2019s experiments provides a robust and well-established evaluation metric for assessing Taipan's performance in in-context retrieval and question answering.  The use of SQUAD strengthens the paper's claims and provides a direct comparison with existing state-of-the-art models.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Roger Waleffe", "paper_title": "An empirical study of mamba-based language models", "reason": "This paper provides additional empirical evidence on the performance of Mamba-based models and their limitations, adding support to the arguments made in the Taipan paper about the need for improvements over pure SSMs and justifying the design choices made in Taipan. The inclusion of this study strengthens the overall argument and adds validity to Taipan\u2019s approach.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yonatan Bisk", "paper_title": "Piqa: Reasoning about physical commonsense in natural language", "reason": "PIQA serves as a benchmark dataset to evaluate the common-sense reasoning capabilities of Taipan.  The inclusion of PIQA in the experimental evaluation helps assess whether the model's improved efficiency in handling long sequences also translates to superior performance in tasks requiring higher-level reasoning abilities, strengthening the argument and providing more comprehensive evaluation of Taipan.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "reason": "This paper introduces Mamba, a state-of-the-art SSM that is directly relevant to Taipan's design.  Mamba's efficient architecture and its ability to handle long sequences inform Taipan's design choices, while comparisons with Mamba's performance help demonstrate Taipan's improvement. ", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Tri Dao", "paper_title": "Transformers are ssms: Generalized models and efficient algorithms through structured state space duality", "reason": "This paper provides the mathematical framework for understanding the relationship between Transformers and SSMs and is therefore fundamental to understanding the theoretical basis of Taipan\u2019s design and hybrid approach. By establishing the theoretical connection, this work strengthens the arguments for the potential and validity of Taipan's approach.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Aydar Bulatov", "paper_title": "Scaling transformer to 1m tokens and beyond with rmt", "reason": "This paper describes a model capable of processing sequences of 1 million tokens, which is a significant achievement in long-context language modeling. Although not directly related to Taipan's design, this work provides context to the field and helps emphasize the significance of Taipan's ability to handle long sequences efficiently.", "section_number": 6}]}