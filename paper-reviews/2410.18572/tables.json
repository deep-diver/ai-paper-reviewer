[{"figure_path": "2410.18572/tables/table_7_0.html", "caption": "Table 1: Zero shot results of Taipan against baseline models.", "description": "Table 1 presents the zero-shot performance of Taipan and three baseline models (Transformer++, Mamba-2, and Jamba) across various common-sense reasoning and question answering tasks, demonstrating Taipan's superior performance across different model sizes.", "section": "4.2 LANGUAGE MODELING PERFORMANCE"}, {"figure_path": "2410.18572/tables/table_8_0.html", "caption": "Table 1: Zero shot results of Taipan against baseline models.", "description": "Table 1 presents the zero-shot results for models of three sizes: 190M, 450M, and 1.3B parameters, evaluated using the Im-evaluation-harness framework across multiple common-sense reasoning and question-answering tasks.", "section": "4.2 LANGUAGE MODELING PERFORMANCE"}]