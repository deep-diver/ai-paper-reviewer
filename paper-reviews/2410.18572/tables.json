[{"figure_path": "2410.18572/tables/table_7_0.md", "caption": "Table 1: Zero shot results of Taipan against baseline models.", "description": "Table 1 presents the zero-shot performance results across eight different tasks (Winograd Schema Challenge, PIQA, HellaSwag, ARC-Easy, ARC-Challenge, OpenBookQA, TruthfulQA, BoolQ)  for four different language models: Transformer++, Mamba, Jamba, and Taipan.  The results are shown for three different parameter sizes of each model: 190M, 450M, and 1.3B parameters.  Each cell in the table represents the average accuracy score achieved by the model on a given task. The table allows for a comparison of Taipan against other strong baseline models under zero-shot settings, demonstrating the relative performance of the different architectures across diverse language understanding tasks.", "section": "4.2 LANGUAGE MODELING PERFORMANCE"}, {"figure_path": "2410.18572/tables/table_8_0.md", "caption": "Table 1: Zero shot results of Taipan against baseline models.", "description": "Table 1 presents the zero-shot performance results across various common-sense reasoning and question-answering tasks for Taipan and three baseline models (Transformer++, Mamba-2, and Jamba) at three different parameter scales (190M, 450M, and 1.3B).  The tasks evaluated include Winograd Schema Challenge (Wino.), PIQA, HellaSwag (Hella.), ARC-Easy (ARCe), ARC-Challenge (ARCC), OpenBookQA (OB), TruthfulQA (Truth.), RACE, and BoolQ.  The table shows that Taipan consistently outperforms the baselines across most tasks and parameter sizes, particularly for the larger model.  Performance is measured as average accuracy across the listed tasks.", "section": "4.2 LANGUAGE MODELING PERFORMANCE"}]