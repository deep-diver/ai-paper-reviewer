[{"content": "| Style Alignment | Method | Model | win | tie | lose |\n|---|---|---|---|---|---|\n|  | Style-Aligned [14] | SDXL | 61.0 % | 7.1% | 31.9% |\n|  | RB-Mod [44] | Cascade | 55.6 % | 12.6% | 31.8% |\n|  | IP-Adapter [61] | FLUX-dev | 59.2 % | 8.0% | 32.8% |\n|  | DCO [30] | FLUX-dev | 56.0 % | 10.2% | 33.8% |\n|  | SD3 sampler [8] | FLUX-dev | 56.0 % | 9.2% | 34.8% |\n| Text Alignment | Method | Model | win | tie | lose |\n|---|---|---|---|---|---|\n|  | Style-Aligned [14] | SDXL | 60.7% | 7.5% | 31.8% |\n|  | RB-Mod [44] | Cascade | 54.3% | 6.3% | 39.4% |\n|  | IP-Adapter [61] | FLUX-dev | 56.0% | 4.6% | 39.4% |\n|  | DCO [30] | FLUX-dev | 53.2% | 10.0% | 36.8% |\n|  | SD3 sampler [8] | FLUX-dev | 56.5% | 14.0% | 29.5% |", "caption": "Table 1: Human evaluation. User preference results comparing style and text alignments between our method and the baselines.", "description": "This table presents the results of a human evaluation comparing the performance of the proposed Style-Friendly SNR sampler with several baseline methods in terms of style and text alignment in image generation.  Participants were shown reference images, target prompts, and images generated by different methods, and asked to choose the image that best matched either the style of the reference or the target text. The table shows the percentage of times each method was preferred for style and text, and the percentage of ties.", "section": "4. Experiments"}, {"content": "| Method | Model | DINO \u2191 | CLIP-I \u2191 | CLIP-T \u2191 |\n|---|---|---|---|---|\n| Style-Aligned [14] | SDXL | 0.410 | 0.675 | 0.340 |\n| RB-Mod [44] | Cascade | 0.317 | 0.647 | 0.363 |\n| DCO [30] | SD3.5 | 0.399 | 0.661 | 0.355 |\n| SD3 sampler [8] | SD3.5 | 0.424 | 0.670 | 0.350 |\n| **Style-friendly** | SD3.5 | 0.489 | 0.698 | 0.349 |\n| IP-Adapter [61] | FLUX-dev | 0.361 | 0.656 | 0.354 |\n| DCO [30] | FLUX-dev | 0.373 | 0.643 | 0.353 |\n| SD3 sampler [8] | FLUX-dev | 0.373 | 0.645 | 0.350 |\n| **Style-friendly** | FLUX-dev | 0.461 | 0.686 | 0.344 |", "caption": "Table 2: Quantitative comparison. Style alignment (DINO and CLIP-I) and text alignment (CLIP-T) with 18 styles from [51]. Our style-friendly exhibits superior style-alignment scores.", "description": "This table presents a quantitative comparison of different methods for style-driven image generation, using 18 different styles from a reference dataset.  The metrics used are DINO and CLIP-I for style alignment (how well the generated image matches the style of the reference image), and CLIP-T for text alignment (how well the generated image matches the text description).  The results show that the 'Style-friendly' method achieves superior style alignment scores compared to other baselines.", "section": "4. Experiments"}, {"content": "| Method | Model | DINO \u2191 | CLIP-I \u2191 | CLIP-T \u2191 |\n|---|---|---|---|---|\n| SD3 Sampler [8] | FLUX-dev | 0.373 | 0.645 | 0.350 |\n| w/ rank 128 | FLUX-dev | 0.426 | 0.668 | 0.345 |\n| **Style-friendly** | FLUX-dev | 0.461 | 0.686 | 0.344 |", "caption": "Table S1: Comparison to increasing LoRA rank.", "description": "This table compares the performance of the Style-friendly SNR sampler against a baseline method where only the LoRA rank is increased. It shows the DINO and CLIP image similarity scores, and CLIP text similarity scores for both methods. The results demonstrate that Style-friendly SNR sampler outperforms the baseline even when the baseline method uses a higher LoRA rank, indicating that the proposed method is more effective than simply increasing model capacity.", "section": "A.2. Quantitative Results"}, {"content": "| Method | DINO | CLIP-I | CLIP-T |\n|---|---|---|---|\n| Style-friendly | 0.489 | 0.698 | 0.349 |\n| w/o Text attn | 0.462 | 0.693 | 0.349 |", "caption": "Table S2: Ablation study on trainable parameters.", "description": "This table presents the results of an ablation study investigating the impact of different trainable parameters on the performance of the Style-friendly SNR sampler.  It compares the performance of the model when fine-tuning only the image transformer blocks versus fine-tuning both image and text transformer blocks of the MM-DiT architecture. The metrics used for comparison include DINO and CLIP image similarity scores (CLIP-I) as well as CLIP text-image similarity scores (CLIP-T) to evaluate style and text alignment, respectively.", "section": "A.3. Additional Observations"}, {"content": "| Method | Model | DINO \u2191 | CLIP-I \u2191 | CLIP-T \u2191 |\n|---|---|---|---|---|\n| SD3 Sampler [8] | SD3.5 | 0.424 | 0.670 | 0.350 |\n| w/ offset 0.1 | SD3.5 | 0.452 | 0.678 | 0.353 |\n| **Style-friendly** | SD3.5 | 0.489 | 0.698 | 0.349 |\n| w/ offset 0.01 | SD3.5 | 0.476 | 0.697 | 0.350 |\n| SD3 Sampler [8] | FLUX-dev | 0.373 | 0.645 | 0.350 |\n| w/ offset 0.1 | FLUX-dev | 0.451 | 0.679 | 0.349 |\n| **Style-friendly** | FLUX-dev | 0.461 | 0.686 | 0.344 |\n| w/ offset 0.01 | FLUX-dev | 0.500 | 0.704 | 0.341 |", "caption": "Table S3: Incorporating offset noise. Offset noise improves SD3 sampler but still does not reach the performance of our Style-friendly SNR sampler; combining our Style-friendly approach with Offset Noise at a smaller scale (0.01) slightly enhances the style alignment of FLUX-dev.", "description": "Table S3 presents a quantitative comparison of different methods for style-driven image generation.  It shows the results of using the SD3 sampler, with and without added offset noise, and the Style-friendly SNR sampler, both on SD3.5 and FLUX-dev models. The metrics used are DINO and CLIP image similarity scores (CLIP-I) and CLIP text-image similarity scores (CLIP-T), measuring style and text alignment. The table demonstrates that adding offset noise improves results with the SD3 sampler, but it still doesn't outperform the Style-friendly SNR sampler. Moreover, when the Style-friendly approach is combined with a small amount of offset noise, it leads to a slight improvement in style alignment, particularly with the FLUX-dev model.", "section": "A. Additional Results"}]