[{"figure_path": "2410.13198/tables/table_3_0.html", "caption": "Table 1: Performance comparison of GEC across three different ASR benchmarks from three different domains. We evaluate and compare across two scenarios: (i) Matched Scenario: In this case, the hypotheses-transcription pairs for training our GEC model are derived from the Train split of the Test dataset (and not from the dataset the ASR model is trained on) (ii) Mismatched Scenario: In this case, the hypotheses-transcription pairs are derived from the same dataset the ASR model is trained on. We show that (a) For domain shifts, i.e., in cases where both the hypotheses and the ASR training dataset are from a domain different from the test, GEC leads to little to no improvement, and (b) For in-domain scenarios where only the hypotheses are derived from the same domain as the test, employing an ASR model trained on a different domain to derive the hypothesis boosts performance.", "description": "Table 1 compares the performance of Generative Error Correction (GEC) models across three different ASR benchmarks and two scenarios (matched and mismatched) to highlight the impact of domain shifts and hypothesis sources on GEC effectiveness.", "section": "3 Preliminary"}, {"figure_path": "2410.13198/tables/table_4_0.html", "caption": "Table 1: Performance comparison of GEC across three different ASR benchmarks from three different domains. We evaluate and compare across two scenarios: (i) Matched Scenario: In this case, the hypotheses-transcription pairs for training our GEC model are derived from the Train split of the Test dataset (and not from the dataset the ASR model is trained on) (ii) Mismatched Scenario: In this case, the hypotheses-transcription pairs are derived from the same dataset the ASR model is trained on. We show that (a) For domain shifts, i.e., in cases where both the hypotheses and the ASR training dataset are from a domain different from the test, GEC leads to little to no improvement, and (b) For in-domain scenarios where only the hypotheses are derived from the same domain as the test, employing an ASR model trained on a different domain to derive the hypothesis boosts performance.", "description": "Table 1 compares the performance of generative error correction (GEC) models across three different ASR benchmarks and two scenarios (matched and mismatched) to show the impact of domain shifts on GEC performance.", "section": "3 Preliminary"}, {"figure_path": "2410.13198/tables/table_8_0.html", "caption": "Table 3: Performance comparison (WER) of DARAG with other methods on various in-domain and out-of-domain settings (the Test is OOD w.r.t. the Train). We assume all 5 datasets are from different domains. We also report the absolute improvements w.r.t. to the ASR-only Baseline. DARAG outperforms other methods by 8%-30% in in-domain and 10%-33% in OOD settings.", "description": "Table 3 compares the performance of DARAG against various baseline and ablation models across five datasets, reporting word error rates (WER) for both in-domain and out-of-domain settings and showing the improvements achieved by DARAG.", "section": "6 Results and Analysis"}, {"figure_path": "2410.13198/tables/table_8_1.html", "caption": "Table 3: Performance comparison (WER) of DARAG with other methods on various in-domain and out-of-domain settings (the Test is OOD w.r.t. the Train). We assume all 5 datasets are from different domains. We also report the absolute improvements w.r.t. to the ASR-only Baseline. DARAG outperforms other methods by 8%\u201330% in in-domain and 10%\u201333% in OOD settings.", "description": "This table compares the word error rate (WER) achieved by DARAG and other methods across various in-domain and out-of-domain settings on five benchmark datasets.", "section": "6 Results and Analysis"}, {"figure_path": "2410.13198/tables/table_12_0.html", "caption": "Table 3: Performance comparison (WER) of DARAG with other methods on various in-domain and out-of-domain settings (the Test is OOD w.r.t. the Train). We assume all 5 datasets are from different domains. We also report the absolute improvements w.r.t. to the ASR-only Baseline. DARAG outperforms other methods by 8%-30% in in-domain and 10%-33% in OOD settings.", "description": "Table 3 compares the word error rates (WER) of DARAG and several baseline methods across various in-domain and out-of-domain settings on five benchmark ASR datasets, highlighting DARAG's superior performance.", "section": "6 Results and Analysis"}, {"figure_path": "2410.13198/tables/table_12_1.html", "caption": "Table 7: Performance comparison of DARAG with and without voice cloning. Performance drops sharply without voice cloning, especially in OOD scenrios, thereby confirming the importance of the voice cloning for generating augmentations.", "description": "Table 7 compares the performance of DARAG in both ID and OOD scenarios, with and without voice cloning, showing that voice cloning is crucial for generating augmentations.", "section": "6.3 Real Data Outperforms Synthetic"}, {"figure_path": "2410.13198/tables/table_12_2.html", "caption": "Table 8: Performance comparison of DARAG across different settings. OOD Adapt. refers to the dataset for which synthetic data was generated and augmented to the original hypotheses for GEC training. ", "description": "Table 8 shows the performance comparison of DARAG across different settings, demonstrating that even with added synthetic training data, DARAG maintains in-domain performance, and improvements in a specific domain only occur when augmentations match the domain's characteristics.", "section": "6 Results and Analysis"}, {"figure_path": "2410.13198/tables/table_12_3.html", "caption": "Table 4: Performance comparison of DARAG with other methods on the NE transcription. For ID, we employ the train set of the dataset as the test. For OOD, we employ LS for Vox and Vox for LS. w/ ID NE refers to DARAG, where the NE datastore is from the ID train set. w/ synth NE refers to additional synthetic NEs we add to the NE datastore.", "description": "Table 4 compares the performance of DARAG and other methods on named entity (NE) transcription in both in-domain and out-of-domain settings, showing improvements with the use of synthetic data and a retrieval-augmented correction approach.", "section": "3.3 How Well do they fair on Named Entities?"}, {"figure_path": "2410.13198/tables/table_12_4.html", "caption": "Table 10: Performance comparison of DARAG on two OOD settings (with LS as training set) with various values of nsmall. Larger values can lead to improved performance.", "description": "Table 10 shows the performance of DARAG on two out-of-domain settings using different values of the parameter nsmall, demonstrating that larger values lead to improved performance.", "section": "F.2 Effect of nsmall in OOD settings"}, {"figure_path": "2410.13198/tables/table_13_0.html", "caption": "Table 11: Performance comparison of DARAG on two OOD settings (with LS as training set) across different scaling factors of nsyn relative to n. More synthetic samples can lead to improved performance, but plateaus beyond a certain point.", "description": "Table 11 presents the performance of DARAG on two out-of-domain settings with different scaling factors of synthetic data relative to the original training set size.", "section": "F Hyper-parameter Tuning"}, {"figure_path": "2410.13198/tables/table_14_0.html", "caption": "Table 3: Performance comparison (WER) of DARAG with other methods on various in-domain and out-of-domain settings (the Test is OOD w.r.t. the Train). We assume all 5 datasets are from different domains. We also report the absolute improvements w.r.t. to the ASR-only Baseline. DARAG outperforms other methods by 8%-30% in in-domain and 10%-33% in OOD settings.", "description": "This table compares the performance of DARAG against several baseline methods across various in-domain and out-of-domain settings, showing the word error rate (WER) and highlighting the significant improvements achieved by DARAG.", "section": "6 Results and Analysis"}, {"figure_path": "2410.13198/tables/table_15_0.html", "caption": "Table 13: Examples of incorrect ASR transcriptions and their corresponding corrections by DARAG.", "description": "Table 13 qualitatively compares DARAG with traditional GEC, demonstrating DARAG's superior ability to accurately correct errors, particularly named entities, in both in-domain and out-of-domain settings.", "section": "Results and Analysis"}]