{"references": [{" publication_date": "2022", "fullname_first_author": "Kevin Meng", "paper_title": "Locating and editing factual associations in gpt", "reason": "This paper introduces ROME, a foundational knowledge editing technique.  Its method of locating and modifying factual associations within transformer models is highly influential and forms the basis for many subsequent methods discussed in the paper.  Understanding ROME is crucial for grasping the core concepts of knowledge editing and evaluating the improvements and differences offered by subsequent techniques like MEMIT.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Zexuan Zhong", "paper_title": "Mquake: Assessing knowledge editing in language models via multi-hop questions", "reason": "This study provides a comprehensive evaluation framework for assessing knowledge editing in LLMs. The focus on multi-hop questions and the proposed evaluation metrics are essential aspects of the work which offer an important contribution to the field of knowledge editing and is used in the paper's own evaluation.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Roi Cohen", "paper_title": "Evaluating the ripple effects of knowledge editing in language models", "reason": "This paper presents a rigorous evaluation protocol for assessing the performance of knowledge editing methods. It systematically investigates the impact of different edits on various downstream tasks and proposes metrics to quantify generalization and stability.  Understanding this framework enhances the interpretation of the presented work's evaluation results.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Eric Mitchell", "paper_title": "Memory-based model editing at scale", "reason": "This work introduces GRACE, a parameter-efficient technique for knowledge editing that preserves the model's original weights. This method is compared against others in the paper, and understanding it helps to analyze the trade-offs between different approaches to knowledge editing.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Zhoubo Li", "paper_title": "Unveiling the pitfalls of knowledge editing for large language models", "reason": "This paper provides valuable insights into the limitations of existing knowledge editing methods. By highlighting potential pitfalls and challenges, it allows us to contextualize the contributions of the current paper in terms of addressing limitations and providing a more accurate evaluation of these techniques.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Edward J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "reason": "LoRA, a parameter-efficient fine-tuning method, is a crucial technique in knowledge editing.  Its inclusion is critical as the paper directly compares the performance of this method with others, contributing to a better understanding of the comparative advantages and limitations of each editing approach.  Understanding LoRA is essential for assessing the efficiency and efficacy claims of HalluEditBench.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Zexuan Zheng", "paper_title": "Mquake: Assessing knowledge editing in language models via multi-hop questions", "reason": "This paper focuses on multi-hop reasoning and provides a detailed methodology for evaluating knowledge editing in such scenarios. It complements the present study's focus on real-world hallucinations and contributes towards a more thorough evaluation of knowledge editing techniques that go beyond simple question-answer pairs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Baolong Bi", "paper_title": "Struedit: Structured outputs enable the fast and accurate knowledge editing for large language models", "reason": "This paper presents a structured approach to knowledge editing, offering a different perspective compared to other methods discussed in the paper. Including this work allows for a comparative analysis of different approaches to knowledge editing. Understanding the strengths and weaknesses of structured editing is vital for a comprehensive review of knowledge editing techniques.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuchen Cai", "paper_title": "Editing knowledge representation of language model via rephrased prefix prompts", "reason": "This research focuses on editing knowledge representation using rephrased prompts, providing a different perspective on knowledge editing than the methods primarily evaluated in the paper. The inclusion of this work allows for a broader examination of editing strategies and helps illustrate the diverse range of existing techniques.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Canyu Chen", "paper_title": "Combating misinformation in the age of LLMs: Opportunities and challenges", "reason": "This paper provides a broad overview of the challenges and opportunities in combating misinformation in the context of LLMs.  This broader context is important for understanding the significance of accurate knowledge editing benchmarks, such as HalluEditBench, in addressing issues of hallucination and misinformation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Qizhou Chen", "paper_title": "Lifelong knowledge editing for LLMs with retrieval-augmented continuous prompt learning", "reason": "This research explores lifelong knowledge editing, which is crucial for addressing the dynamic nature of real-world knowledge. Including this work provides a complete understanding of the field of knowledge editing, comparing the proposed method with other methods designed for correcting specific problems with a focus on addressing the ongoing challenge of updating LLM knowledge over time.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuheng Chen", "paper_title": "Journey to the center of the knowledge neurons: Discoveries of language-independent knowledge neurons and degenerate knowledge neurons", "reason": "This research explores the neural mechanisms underlying knowledge representation in LLMs, providing a fundamental understanding of how LLMs store and access information. This helps interpret the results of different knowledge editing techniques and the potential impact on the overall performance and efficiency of LLMs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Keyuan Cheng", "paper_title": "Multi-hop question answering under temporal knowledge editing", "reason": "This paper addresses the challenges of knowledge editing in multi-hop question-answering scenarios. Understanding this is important for assessing the robustness and generalizability of different knowledge editing techniques in handling complex questions and reasoning tasks.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Roi Cohen", "paper_title": "Evaluating the ripple effects of knowledge editing in language models", "reason": "This study offers a comprehensive evaluation framework for assessing knowledge editing methods, which is used in the paper's evaluation. This is vital in understanding the impact of the proposed HalluEditBench benchmark on the field. The paper's evaluation criteria are directly inspired by Cohen's evaluation framework.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Govind Gangadhar", "paper_title": "Model editing by pure fine-tuning", "reason": "This paper proposes a fine-tuning approach to knowledge editing, which offers a different perspective than other methods discussed in the current paper.  Understanding this approach and its performance is key for evaluating the relative merits of various knowledge editing techniques.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Huaizhi Ge", "paper_title": "How well can knowledge edit methods edit perplexing knowledge?", "reason": "This paper explores the limitations of current knowledge editing techniques, especially when dealing with complex and ambiguous knowledge.  Its insights are highly relevant to the current work, which attempts to address the challenges of correcting real-world hallucinations in LLMs.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Hengrui Gu", "paper_title": "Pokemqa: Programmable knowledge editing for multi-hop question answering", "reason": "This research presents a programmable approach to knowledge editing, offering a different perspective to the techniques discussed in the current paper.  This paper's contribution is that it focuses on multi-hop question answering, an important aspect of LLM evaluation and helps understand different strategies that can be used for knowledge editing.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Tom Hartvigsen", "paper_title": "Aging with grace: Lifelong model editing with discrete key-value adaptors", "reason": "This paper presents GRACE, a method of knowledge editing that is significantly different from other methods discussed in the current paper. The fact that this study employs a different technique makes its contribution relevant to evaluating the various strategies for knowledge editing.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Jason Hoelscher-Obermaier", "paper_title": "Detecting edit failures in large language models: An improved specificity benchmark", "reason": "This paper presents a method for detecting failures in knowledge editing, providing a critical perspective for evaluating the effectiveness of these techniques.  It is relevant because it focuses on the accuracy and reliability of knowledge editing, helping to understand how effective various editing methods are in practice.  Such insights are essential for a holistic evaluation of knowledge editing techniques.", "section_number": 3}]}