{"references": [{" publication_date": "2022", "fullname_first_author": "Kevin Meng", "paper_title": "Locating and editing factual associations in gpt", "reason": "This paper is foundational to the field, introducing ROME, a method highly relevant to the present research.  Its innovative approach of identifying and directly modifying specific parameters within the LLM for factual correction is a key focus of the current work. The findings of Meng et al. directly inform the current paper's analysis and methodology, making this a crucial foundational reference for evaluating knowledge editing techniques.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Kevin Meng", "paper_title": "Mass-editing memory in a transformer", "reason": "This paper builds upon the foundational work of Meng et al. (2022) and introduces MEMIT, a more scalable technique for correcting factual errors in LLMs. MEMIT extends the core concept of targeting specific model parameters, as used in ROME, to address a larger volume of edits more efficiently, directly addressing the scalability concerns relevant to the proposed benchmark.  The findings and methodology of this paper are essential to understanding the landscape of knowledge editing techniques and comparing their relative efficiency and efficacy in the current work.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Edward J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "reason": "LoRA, introduced in this paper, is a highly efficient parameter-efficient fine-tuning method that is directly relevant to the current study's exploration of knowledge editing techniques.  LoRA's approach of training low-rank matrices to modify model behavior without altering the original weights is particularly relevant to the paper's analysis of the trade-offs between efficacy, computational cost, and the preservation of existing knowledge. The paper's contribution to efficient model adaptation makes it a critical reference for the current work's evaluation of knowledge editing techniques.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Zexuan Zheng", "paper_title": "Mquake: Assessing knowledge editing in language models via multi-hop questions", "reason": "This paper introduces a novel approach to evaluating knowledge editing techniques by focusing on multi-hop questions, making it highly relevant to the current research.  The emphasis on multi-hop reasoning directly aligns with the present paper's evaluation of knowledge editing's capabilities in complex reasoning tasks. The findings from this paper inform the current paper's consideration of portability and the challenges of applying edited knowledge in scenarios beyond simple question-answering.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Roi Cohen", "paper_title": "Evaluating the ripple effects of knowledge editing in language models", "reason": "This paper offers a rigorous and comprehensive evaluation framework for knowledge editing, which is highly relevant to the current research.  Its focus on the side effects and ripple effects of knowledge editing is important to understanding the limitations of these techniques. The paper directly informs the methodology and analysis of the current work, particularly in assessing the robustness and generalization capabilities of different knowledge editing techniques.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Zhoubo Li", "paper_title": "Unveiling the pitfalls of knowledge editing for large language models", "reason": "This paper provides a critical analysis of existing knowledge editing techniques, highlighting potential pitfalls and limitations.  This is directly relevant to the current work's evaluation methodology, as the paper informs the design of the proposed benchmark to address the limitations and biases of current evaluation methods and datasets. This critical assessment of the field informs the design and evaluation process of the proposed HalluEditBench.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Zexuan Zheng", "paper_title": "In-Context Knowledge Editing (IKE)", "reason": "This paper introduces the In-Context Editing (ICE) method, which is a key technique evaluated in the current study.  ICE's reliance on in-context learning rather than direct parameter modification offers a distinct approach to knowledge editing.  Understanding ICE's capabilities and limitations is crucial to the comprehensive analysis provided in this research.  The study also directly compares ICE against other methods, highlighting ICE's advantages and disadvantages in comparison to parameter-modifying techniques.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tom Hartvigsen", "paper_title": "Aging with grace: Lifelong model editing with discrete key-value adaptors", "reason": "This paper introduces the GRACE method, another key technique evaluated in the current study.  GRACE's unique approach of using a discrete codebook for managing and updating knowledge contrasts with other parameter-modifying techniques.  Evaluating GRACE's performance is crucial to understanding the strengths and limitations of different knowledge editing approaches.  The paper's methodology directly informs the current work's analysis of various knowledge editing techniques.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Baolong Bi", "paper_title": "Struedit: Structured outputs enable the fast and accurate knowledge editing for large language models", "reason": "This paper presents a structured approach to knowledge editing in LLMs, which is relevant to the current research's investigation into the effectiveness of different knowledge editing techniques.  The paper's focus on structured outputs and its emphasis on efficiency and accuracy directly inform the current study's benchmark design. The methodology presented by this paper contributes to the current work's comprehensive evaluation framework for knowledge editing.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuchen Cai", "paper_title": "Editing knowledge representation of language model via rephrased prefix prompts", "reason": "This paper explores the use of rephrased prompts to edit the knowledge representation in LLMs, which offers a complementary technique to the parameter-based approaches. The methodology employed in this paper helps inform the current study's methodology in investigating the capabilities of rephrasing and prompt engineering in knowledge editing. This method provides an alternative approach that the proposed benchmark considers.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuchen Cai", "paper_title": "Locating and mitigating gender bias in large language models", "reason": "This paper addresses bias mitigation in LLMs, which is a crucial aspect related to the broader topic of knowledge editing. Addressing biases and inaccuracies in LLMs are closely related problems, and the techniques and insights from this paper help inform the current study's comprehensive evaluation of knowledge editing techniques. This study also shows the importance of handling various kinds of knowledge editing problems.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Canyu Chen", "paper_title": "Combating misinformation in the age of LLMs: Opportunities and challenges", "reason": "This paper offers a comprehensive overview of misinformation challenges within the context of LLMs. Misinformation and hallucinations are closely related problems, and addressing inaccuracies in factual knowledge is a shared goal for both fields. The understanding of the challenges in misinformation research offers valuable insights into the difficulties in addressing similar challenges in knowledge editing. The paper also presents the opportunities for research.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Canyu Chen", "paper_title": "Can editing LLMs inject harm?", "reason": "This paper directly investigates the potential harms and risks associated with knowledge editing in LLMs. The paper highlights the safety aspects of knowledge editing, which is directly relevant to the current study's comprehensive analysis of various knowledge editing techniques and their limitations.  This research directly informs the study by emphasizing the significance of safety considerations in the development and application of knowledge editing techniques.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Qizhou Chen", "paper_title": "Lifelong knowledge editing for LLMs with retrieval-augmented continuous prompt learning", "reason": "This paper proposes a lifelong learning approach to knowledge editing, which is a significant advancement in the field and highly relevant to this research.  The methodology of this paper expands the scope of knowledge editing and is relevant to addressing the limitations of existing approaches. The findings from this paper also inform the methodology and design of the proposed benchmark.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuheng Chen", "paper_title": "Journey to the center of the knowledge neurons: Discoveries of language-independent knowledge neurons and degenerate knowledge neurons", "reason": "This paper explores the neural mechanisms underlying knowledge representation in LLMs, offering valuable insights into the underlying workings of knowledge editing techniques.  By understanding how knowledge is represented and processed in LLMs, the current study can better assess the effectiveness and limitations of different knowledge editing methods. The paper also expands the scope of the knowledge editing research.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuheng Chen", "paper_title": "Knowledge localization: Mission not accomplished? enter query localization!", "reason": "This paper offers a critical perspective on knowledge localization and its application in knowledge editing, offering valuable insights for this research. The paper directly informs the current study's analysis of the effectiveness and limitations of different knowledge editing methods, and its focus on query localization provides a complementary perspective on addressing knowledge gaps in LLMs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Keyuan Cheng", "paper_title": "Leveraging logical rules in knowledge editing: A cherry on the top", "reason": "This paper presents a novel approach to knowledge editing using logical rules, offering a complementary perspective to the data-driven approaches. This approach provides an alternative methodology that the proposed benchmark considers. The use of logical rules in knowledge editing offers a different approach that the proposed benchmark must consider.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Keyuan Cheng", "paper_title": "Multi-hop question answering under temporal knowledge editing", "reason": "This paper investigates the challenges of applying knowledge editing in multi-hop question answering, a crucial aspect directly related to the current research.  The paper addresses the limitations of existing knowledge editing techniques in handling complex reasoning tasks.  The findings and insights from this paper directly inform the design and evaluation methodology of the proposed HalluEditBench.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Jingcheng Deng", "paper_title": "Unke: Unstructured knowledge editing in large language models", "reason": "This paper focuses on unstructured knowledge editing, providing a contrasting approach to the structured methods.  This diverse approach enhances the comprehensiveness of the current work, allowing for a more holistic evaluation of knowledge editing. The paper also considers the challenges in handling unstructured knowledge which is a common problem in real-world data.", "section_number": 2}]}