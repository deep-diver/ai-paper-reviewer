[{"figure_path": "2410.16251/charts/charts_5_0.png", "caption": "Efficacy Scores of Knowledge Editing Methods. The \"overall\" refers to the Efficacy Score (%) on the whole HalluEditBench embracing 9 domains for different methods. The Efficacy Score on each domain is also reported. Efficacy scores (%) are measured by the accuracy on Efficacy Evaluation Question-answer Pairs, where the pre-edit scores of each LLM are ensured 0.", "description": "This chart presents the efficacy scores of seven different knowledge editing methods across nine domains and an overall score for three large language models (LLMs): Llama2-7B, Llama3-8B, and Mistral-v0.3-7B.  Each bar represents the efficacy score (percentage) of a specific knowledge editing method in a given domain for a particular LLM. The chart is subdivided into three sub-charts, one for each LLM, allowing for a comparison of performance across models.  The \"overall\" column provides a summary of the average performance across all domains for each method and LLM. The scores show the effectiveness of each method in correcting hallucinations within each domain and model, with higher scores indicating better performance.", "section": "3.1 FACET 1: EFFICACY"}, {"figure_path": "2410.16251/charts/charts_6_0.png", "caption": "Figure 4: Generalization Scores of Knowledge Editing Methods. Generalization Scores (%) are measured by accuracy on five types of Generalization Evaluation Questions including Rephrased Questions (\"rephrase\"), Yes-or-No Questions with Yes or No as answers (\"yes\" or \"no\"), Multi-Choice Questions (\u201cmc\u201d), Reversed Questions (\u201creversed\u201d). The \u201caverage\u201d refers to averaged scores over five question types. The figure only shows the overall Generalization Scores for each type on the whole HalluEditBench. Generalization Scores for each domain are given in Appendix D.1.", "description": "This chart presents the Generalization Scores of various knowledge editing methods across three different Large Language Models (LLMs): Llama2-7B, Llama3-8B, and Mistral-v0.3-7B.  The scores are broken down by five types of generalization evaluation questions: rephrased, yes/no, multiple choice, reversed, and an average across all five question types.  Each LLM has a separate subplot, visually comparing the pre-edit performance with seven different post-edit knowledge editing methods (FT-L, FT-M, MEMIT, ROME, LoRA, ICE, GRACE)  for each question type. The chart displays the percentage accuracy achieved by each method, illustrating the ability of the models to generalize knowledge after undergoing editing.", "section": "3.2 FACET 2: GENERALIZATION"}, {"figure_path": "2410.16251/charts/charts_7_0.png", "caption": "Figure 5: Portability Scores of Knowledge Editing Methods. Portability Scores (%) are measured by the accuracy on Portability Evaluation Questions, which are Efficacy Evaluation Questions with N hops (N = 1 ~ 6). The Portability Evaluation Questions are the same as Efficacy Evaluation Questions when N is 1. The results for more domains are given in Appendix D.2. The \u201coverall\u201d refers to the Portability Score (%) on the whole HalluEditBench embracing 9 domains.", "description": "This chart presents the Portability Scores of knowledge editing methods across three different LLMs (Llama2-7B, Llama3-8B, Mistral-v0.3-7B) and for two specific domains (\"human\" and \"places\") along with an overall score. The portability is evaluated using questions with varying hop distances (N=1 to 6), where higher hop distances indicate more complex reasoning.  The chart displays the pre-edit and post-edit Portability Scores (%), with post-edit scores representing the performance after applying seven different knowledge editing techniques (FT-L, FT-M, MEMIT, ROME, LoRA, ICE, and GRACE). The x-axis shows the hop distance, and the y-axis shows the Portability Score (%).  Each LLM and domain combination is represented by a separate subplot, facilitating comparison of the different techniques.", "section": "3.3 FACET 3: PORTABILITY"}, {"figure_path": "2410.16251/charts/charts_8_0.png", "caption": "Figure 3: Efficacy Scores of Knowledge Editing Methods. The \"overall\" refers to the Efficacy Score (%) on the whole HalluEditBench embracing 9 domains for different methods. The Efficacy Score on each domain is also reported. Efficacy scores (%) are measured by the accuracy on Efficacy Evaluation Question-answer Pairs, where the pre-edit scores of each LLM are ensured 0.", "description": "This bar chart displays the Efficacy Scores of seven knowledge editing methods across nine domains and an overall average for three different Large Language Models (LLMs): Llama2-7B, Llama3-8B, and Mistral-v0.3-7B.  Each LLM has a separate sub-chart showing the efficacy scores for each domain (art, business, entertainment, event, geography, health, human, places, technology) as well as a final \"overall\" score representing the average across all domains. The height of each bar represents the percentage accuracy of the method in correcting hallucinations within that specific domain for each LLM.  The pre-edit scores for each LLM are set to 0 for comparison.", "section": "3.1 FACET 1: EFFICACY"}, {"figure_path": "2410.16251/charts/charts_9_0.png", "caption": "Figure 17: Robustness Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Robustness Scores are calculated by the accuracy on Robustness Evaluation Questions with M turns (M = 1 ~ 10). We regard Efficacy Scores as the Robustness Scores when M is 0. The domains include \"geography\", \"health\", and \"technology\".", "description": "This chart displays the Robustness Scores of seven knowledge editing methods (FT-L, FT-M, MEMIT, ROME, LoRA, ICE, GRACE) across three different LLMs (Llama2-7B, Llama3-8B, Mistral-v0.3-7B) and three domains (geography, health, technology).  The x-axis represents the number of turns (M) in the Robustness Evaluation Questions (0-10), with 0 representing the Efficacy Score. The y-axis shows the Robustness Score (%), indicating the percentage of times the LLMs provided a \"yes\" response to the robustness evaluation questions.  Each line represents a specific knowledge editing method, showing the change in robustness scores across different LLMs and domains over 10 turns. This allows for comparison of the resilience of each method's knowledge editing to external manipulations in prompts across different LLMs and domains.", "section": "3.5 FACET 5: ROBUSTNESS"}, {"figure_path": "2410.16251/charts/charts_22_0.png", "caption": "Figure 4: Generalization Scores of Knowledge Editing Methods. Generalization Scores (%) are measured by accuracy on five types of Generalization Evaluation Questions including Rephrased Questions (\"rephrase\"), Yes-or-No Questions with Yes or No as answers (\"yes\" or \"no\"), Multi-Choice Questions (\u201cmc\u201d), Reversed Questions (\u201creversed\u201d). The \u201caverage\u201d refers to averaged scores over five question types. The figure only shows the overall Generalization Scores for each type on the whole HalluEditBench. Generalization Scores for each domain are given in Appendix D.1.", "description": "This chart presents the Generalization Scores of various knowledge editing methods across three different LLMs (Llama2-7B, Llama3-8B, and Mistral-v0.3-7B). The scores are calculated based on the accuracy of the LLMs' responses to five types of generalization evaluation questions: rephrased, yes/no, multiple choice, and reversed questions.  For each LLM, the chart displays the performance for two domains, \"places\" and \"human\". The pre-edit scores represent the baseline performance before any knowledge editing, while the post-edit scores show the performance after applying each of the seven knowledge editing methods. The chart allows for a comparison of the effectiveness of different methods in generalizing knowledge across various question types and LLMs.", "section": "3.2 FACET 2: GENERALIZATION"}, {"figure_path": "2410.16251/charts/charts_23_0.png", "caption": "Figure 4: Generalization Scores of Knowledge Editing Methods. Generalization Scores (%) are measured by accuracy on five types of Generalization Evaluation Questions including Rephrased Questions (\"rephrase\"), Yes-or-No Questions with Yes or No as answers (\"yes\" or \"no\"), Multi-Choice Questions (\u201cmc\u201d), Reversed Questions (\u201creversed\u201d). The \u201caverage\u201d refers to averaged scores over five question types. The figure only shows the overall Generalization Scores for each type on the whole HalluEditBench. Generalization Scores for each domain are given in Appendix D.1.", "description": "This chart presents the generalization scores of various knowledge editing methods across three different LLMs (Llama2-7B, Llama3-8B, and Mistral-v0.3-7B).  The scores are calculated based on the accuracy of the LLMs' responses to five types of generalization evaluation questions: rephrased questions, yes/no questions, multiple-choice questions, reversed questions, and an average across all five question types. The chart displays the pre-edit and post-edit scores for each method, allowing for a direct comparison of the effectiveness of the different knowledge editing techniques in enhancing the generalizability of LLMs' responses.", "section": "3.2 FACET 2: GENERALIZATION"}, {"figure_path": "2410.16251/charts/charts_23_1.png", "caption": "Figure 4: Generalization Scores of Knowledge Editing Methods. Generalization Scores (%) are measured by accuracy on five types of Generalization Evaluation Questions including Rephrased Questions (\"rephrase\"), Yes-or-No Questions with Yes or No as answers (\"yes\" or \"no\"), Multi-Choice Questions (\u201cmc\u201d), Reversed Questions (\u201creversed\u201d). The \u201caverage\u201d refers to averaged scores over five question types. The figure only shows the overall Generalization Scores for each type on the whole HalluEditBench. Generalization Scores for each domain are given in Appendix D.1.", "description": "The bar chart displays the Generalization Scores of several knowledge editing methods across five different question types for three large language models (LLMs): Llama2-7B, Llama3-8B, and Mistral-v0.3-7B. Each bar represents the average Generalization Score for a particular LLM and question type. The question types are Rephrased, Yes-No (Yes), Yes-No (No), Multi-choice, and Reversed. The pre-edit scores (before applying editing methods) are shown as a baseline for comparison. The chart shows the overall Generalization scores across five types of questions and the scores for each domain is provided in the Appendix.", "section": "3.2 FACET 2: GENERALIZATION"}, {"figure_path": "2410.16251/charts/charts_23_2.png", "caption": "Figure 12: Generalization Scores of Knowledge Editing Methods on 3 LLMs and 2 Domains.", "description": "This chart displays the Generalization Scores for various knowledge editing methods across three different Large Language Models (LLMs) and two domains. The scores are broken down by five types of generalization evaluation questions: rephrased, yes/no, multiple choice, reversed, and an average across all five types.  The chart allows for a comparison of each method's performance on different LLMs (Llama2-7B, Llama3-8B, and Mistral-v0.3-7B) and across two specified domains within each LLM. Pre-edit scores are also included as a baseline for comparison.", "section": "3.2 FACET 2: GENERALIZATION"}, {"figure_path": "2410.16251/charts/charts_23_3.png", "caption": "Figure 4: Generalization Scores of Knowledge Editing Methods. Generalization Scores (%) are measured by accuracy on five types of Generalization Evaluation Questions including Rephrased Questions (\"rephrase\"), Yes-or-No Questions with Yes or No as answers (\"yes\" or \"no\"), Multi-Choice Questions (\u201cmc\u201d), Reversed Questions (\u201creversed\u201d). The \u201caverage\u201d refers to averaged scores over five question types. The figure only shows the overall Generalization Scores for each type on the whole HalluEditBench. Generalization Scores for each domain are given in Appendix D.1.", "description": "This bar chart presents the Generalization Scores of various knowledge editing methods across different question types for three LLMs (Llama2-7B, Llama3-8B, and Mistral-v0.3-7B).  The chart displays the pre-edit and post-edit performance for each method (FT-L, FT-M, MEMIT, ROME, LoRA, ICE, GRACE) across five question types: Rephrased, Yes/No, Multi-choice, Reversed, and an average across these five.  Each question type is represented by a set of bars, and the scores are shown as percentages, indicating the accuracy of each LLM after being subjected to the given knowledge editing methods. The chart illustrates the performance of each technique in generating correct responses for diverse but conceptually related questions, thus reflecting the generalizability of the edits.", "section": "3.2 FACET 2: GENERALIZATION"}, {"figure_path": "2410.16251/charts/charts_23_4.png", "caption": "Figure 3: Efficacy Scores of Knowledge Editing Methods. The \"overall\" refers to the Efficacy Score (%) on the whole HalluEditBench embracing 9 domains for different methods. The Efficacy Score on each domain is also reported. Efficacy scores (%) are measured by the accuracy on Efficacy Evaluation Question-answer Pairs, where the pre-edit scores of each LLM are ensured 0.", "description": "This bar chart displays the efficacy scores of seven knowledge editing methods across nine domains and an overall average for three different large language models (LLMs): Llama2-7B, Llama3-8B, and Mistral-v0.3-7B.  Each bar represents the efficacy score (percentage) of a specific knowledge editing method in a given domain, with separate charts for each LLM.  The domains are art, business, entertainment, event, geography, health, human, places, and technology.  The overall efficacy scores are presented in the final columns of each chart. The pre-edit scores (performance before applying knowledge editing) are set to 0 for all LLMs and domains.", "section": "3 RESULTS AND ANALYSIS"}, {"figure_path": "2410.16251/charts/charts_24_0.png", "caption": "Figure 10: Generalization Scores of Knowledge Editing Methods on 3 LLMs and 2 Domains.", "description": "This chart presents the Generalization Scores of several knowledge editing methods across three different Large Language Models (LLMs): Llama2-7B, Llama3-8B, and Mistral-v0.3-7B.  The scores are broken down by five types of Generalization Evaluation Questions (Rephrased, Yes/No, Multi-Choice, Reversed, and an average across all types) and two domains: \"entertainment\" and \"event\". Each bar represents a specific LLM and editing method, showcasing the accuracy achieved on the corresponding question type and domain.  The pre-edit scores are included for comparison, offering a baseline for assessing the effectiveness of each knowledge editing technique in terms of generalization.", "section": "3.2 FACET 2: GENERALIZATION"}, {"figure_path": "2410.16251/charts/charts_25_0.png", "caption": "Figure 8: Generalization Scores of Knowledge Editing Methods on 3 LLMs and 2 Domains.", "description": "This chart displays Generalization Scores for knowledge editing methods across three different LLMs (Llama2-7B, Llama3-8B, Mistral-v0.3-7B) and two domains (places, human).  The scores are broken down by five types of generalization evaluation questions: rephrased, yes/no, multi-choice, reversed, and an average of all five.  Each bar represents a single LLM and question type, with different colors representing different knowledge editing methods (Pre-edit, FT-L, FT-M, MEMIT, ROME, LORA, ICE, GRACE). The chart allows for a comparison of the generalization performance of each method across the different LLMs and question types within the specified domains.", "section": "3.2 FACET 2: GENERALIZATION"}, {"figure_path": "2410.16251/charts/charts_26_0.png", "caption": "Figure 12: Generalization Scores of Knowledge Editing Methods on 3 LLMs and 2 Domains.", "description": "This chart displays the Generalization Scores of several knowledge editing methods across three different LLMs (Llama2-7B, Llama3-8B, and Mistral-v0.3-7B) for the \"technology\" domain.  The scores are broken down by five question types: rephrased, yes/no (yes and no separately), multiple-choice, and reversed questions.  Each bar represents the accuracy of a specific method on a given question type.  The \"average\" column shows the average generalization score across all question types for each method and LLM. The chart allows for comparison of the generalization performance of different methods on different LLMs for a specified domain.", "section": "3.2 FACET 2: GENERALIZATION"}, {"figure_path": "2410.16251/charts/charts_27_0.png", "caption": "Figure 13: Portability Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Portability Scores (%) are measured by the accuracy on Portability Evaluation Questions, which are Efficacy Evaluation Questions when with N hops. The Portability Evaluation Questions are the same as Efficacy Evaluation Questions when N is 1. The domains include \u201cbusiness\u201d, \u201centertainment", "description": "This chart presents the Portability Scores for three different LLMs (Llama2-7B, Llama3-8B, and Mistral-v0.3-7B) across three domains (business, entertainment, and event). The Portability Score, measured as a percentage, reflects the accuracy of the models' responses to Portability Evaluation Questions, which are essentially Efficacy Evaluation Questions with multiple hops (N=1 to 6).  Each LLM's performance is displayed as a line graph across the six hop distances, comparing its performance before editing (Pre-edit) and after applying seven different knowledge editing methods (FT-L, FT-M, MEMIT, ROME, LoRA, ICE, and GRACE). This allows for a comparison of the effectiveness of each editing method in maintaining the accuracy of the LLM's responses as the complexity of the reasoning task increases.", "section": "3.3 FACET 3: PORTABILITY"}, {"figure_path": "2410.16251/charts/charts_28_0.png", "caption": "Figure 13: Portability Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Portability Scores (%) are measured by the accuracy on Portability Evaluation Questions, which are Efficacy Evaluation Questions when with N hops. The Portability Evaluation Questions are the same as Efficacy Evaluation Questions when N is 1. The domains include \u201cbusiness\u201d, \u201centertainment\u201d, and \u201cevent\u201d.", "description": "This chart displays the Portability Scores of seven knowledge editing methods (Pre-edit, FT-L, FT-M, MEMIT, ROME, LoRA, ICE, GRACE) across three different LLMs (Llama2-7B, Llama3-8B, Mistral-v0.3-7B) and three domains (business, entertainment, event).  The x-axis represents the hop distance (N=1 to 6) of the Portability Evaluation Questions, where N=1 is equivalent to the Efficacy Evaluation Questions. The y-axis indicates the Portability Score (%), representing the accuracy of the LLMs in answering these multi-hop questions after the application of each editing method. The chart visualizes the performance of each editing method in maintaining consistent accuracy across different hop distances within each LLM and domain, highlighting their ability to reason through multiple steps of inference based on the edited knowledge.", "section": "3.3 FACET 3: PORTABILITY"}, {"figure_path": "2410.16251/charts/charts_29_0.png", "caption": "Figure 15: Portability Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Portability Scores (%) are measured by the accuracy on Portability Evaluation Questions, which are Efficacy Evaluation Questions when with N hops. The Portability Evaluation Questions are the same as Efficacy Evaluation Questions when N is 1. The domain is \"art\".", "description": "This chart displays the portability scores for different knowledge editing methods across six hop distances (1-6) for the Llama2-7B model applied to the \"art\" domain.  The pre-edit scores (before any editing) begin at 0% for one hop but gradually increase with the increase in hop distance.   Post-edit scores (after applying various editing techniques) show varying levels of performance across hop distances and compared to the pre-edit scores.  Some methods initially maintain higher scores that then decrease as hop distance increases while other methods show lower overall scores compared to pre-edit scores throughout.", "section": "3.3 FACET 3: PORTABILITY"}, {"figure_path": "2410.16251/charts/charts_29_1.png", "caption": "Figure 5: Portability Scores of Knowledge Editing Methods. Portability Scores (%) are measured by the accuracy on Portability Evaluation Questions, which are Efficacy Evaluation Questions with N hops (N = 1 ~ 6). The Portability Evaluation Questions are the same as Efficacy Evaluation Questions when N is 1. The results for more domains are given in Appendix D.2. The \u201coverall\u201d refers to the Portability Score (%) on the whole HalluEditBench embracing 9 domains.", "description": "The chart displays the Portability Scores of various knowledge editing methods across different hop distances (1-6) for Llama3-8B on the 'art' domain.  The x-axis represents the hop distance, while the y-axis shows the Portability Score (%). Each colored line corresponds to a specific knowledge editing method, showing its performance at each hop distance. The pre-edit score is represented by a dashed grey line, illustrating the baseline performance before knowledge editing is applied. The chart visually compares the portability performance of the different methods, revealing how well they maintain accuracy across various reasoning steps.", "section": "3.3 FACET 3: PORTABILITY"}, {"figure_path": "2410.16251/charts/charts_29_2.png", "caption": "Figure 15: Portability Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Portability Scores (%) are measured by the accuracy on Portability Evaluation Questions, which are Efficacy Evaluation Questions when with N hops. The Portability Evaluation Questions are the same as Efficacy Evaluation Questions when N is 1. The domain is \u201cart\u201d.", "description": "This chart displays the Portability Scores for various knowledge editing methods across three different LLMs (Llama2-7B, Llama3-8B, and Mistral-v0.3-7B) and a single domain ('art'). The Portability Scores are calculated based on the accuracy of answering Portability Evaluation Questions, which are multi-hop questions derived from Efficacy Evaluation Questions.  The X-axis represents the \u201cHop Distance\u201d of the questions (ranging from 1 to 6), indicating the number of hops or reasoning steps required to arrive at the answer. The Y-axis represents the Portability Score (percentage). Each line in the chart corresponds to a specific knowledge editing method, showing how its performance changes as the hop distance (complexity) of the question increases. The chart helps visualize the impact of question complexity on each method's ability to maintain accuracy after editing.", "section": "3.3 FACET 3: PORTABILITY"}, {"figure_path": "2410.16251/charts/charts_30_0.png", "caption": "Figure 17: Robustness Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Robustness Scores are calculated by the accuracy on Robustness Evaluation Questions with M turns (M = 1 ~ 10). We regard Efficacy Scores as the Robustness Scores when M is 0. The domains include \"geography\", \"health\", and \"technology\".", "description": "This chart displays the Robustness Scores of seven knowledge editing methods (FT-L, FT-M, MEMIT, ROME, LoRA, ICE, GRACE) across three large language models (LLMs: Llama2-7B, Llama3-8B, Mistral-v0.3-7B) and three domains (geography, health, technology).  The x-axis represents the number of turns (M) in the Robustness Evaluation Questions (0-10), while the y-axis indicates the Robustness Score (0-100%).  Each bar group represents a specific LLM and domain combination, showing the robustness of the edited knowledge against distractions for each method and turn.  The scores at M=0 correspond to the Efficacy Scores for each method.", "section": "3.5 FACET 5: ROBUSTNESS"}, {"figure_path": "2410.16251/charts/charts_31_0.png", "caption": "Figure 17: Robustness Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Robustness Scores are calculated by the accuracy on Robustness Evaluation Questions with M turns (M = 1 ~ 10). We regard Efficacy Scores as the Robustness Scores when M is 0. The domains include \"geography\", \"health\", and \"technology\".", "description": "This chart displays the Robustness Scores of seven knowledge editing methods (FT-L, FT-M, MEMIT, ROME, LoRA, ICE, GRACE) across three different large language models (LLMs: Llama2-7B, Llama3-8B, Mistral-v0.3-7B) and three domains (geography, health, technology).  The x-axis represents the number of turns (M) in the robustness evaluation questions (0-10), with 0 representing the Efficacy Score. The y-axis shows the Robustness Score (%), indicating the accuracy of the LLMs' responses to the robustness evaluation questions after knowledge editing. Each LLM has a separate set of three subplots corresponding to the three domains.  The chart illustrates how the robustness of the edited knowledge changes across different models, domains, and editing techniques as more challenging prompts (more turns) are introduced.", "section": "3.5 FACET 5: ROBUSTNESS"}, {"figure_path": "2410.16251/charts/charts_32_0.png", "caption": "Figure 18: Robustness Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Robustness Scores are calculated by the accuracy on Robustness Evaluation Questions with M turns (M = 1 ~ 10). We regard Efficacy Scores as the Robustness Scores when M is 0. The domain is \"art\".", "description": "The chart displays the Robustness Scores of different knowledge editing methods across 11 turns (0-10) for Llama2-7B on the \"art\" domain.  The robustness score is calculated by the accuracy on Robustness Evaluation Questions. Turn 0 represents the Efficacy Score.  The chart shows the performance of seven methods: Pre-edit, FT-L, FT-M, MEMIT, ROME, LoRA, ICE, and GRACE.  Each method's performance is represented by a line graph across the 11 turns, allowing for visual comparison of their robustness.", "section": "3.5 FACET 5: ROBUSTNESS"}, {"figure_path": "2410.16251/charts/charts_32_1.png", "caption": "Figure 18: Robustness Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Robustness Scores are calculated by the accuracy on Robustness Evaluation Questions with M turns (M = 1 ~ 10). We regard Efficacy Scores as the Robustness Scores when M is 0. The domain is \"art\".", "description": "The chart displays the Robustness Scores of seven knowledge editing methods (Pre-edit, FT-L, FT-M, MEMIT, ROME, LoRA, ICE, GRACE) across three Large Language Models (LLMs) in the \"art\" domain.  The x-axis represents the number of turns (0-10) in the Robustness Evaluation Questions, with 0 representing the Efficacy Score. The y-axis shows the Robustness Score (percentage).  Each line represents a different knowledge editing method, illustrating how their performance changes as the number of turns in the evaluation increases, showing the robustness of the method against distractive prompts.  The chart visually compares the performance of different methods over multiple turns, revealing their resistance to external manipulation in the context of correcting hallucinations.", "section": "3.5 FACET 5: ROBUSTNESS"}, {"figure_path": "2410.16251/charts/charts_32_2.png", "caption": "Figure 17: Robustness Scores of Knowledge Editing Methods on 3 LLMs and 3 Domains. Robustness Scores are calculated by the accuracy on Robustness Evaluation Questions with M turns (M = 1 ~ 10). We regard Efficacy Scores as the Robustness Scores when M is 0. The domains include \"geography\", \"health\", and \"technology\".", "description": "This chart displays the robustness scores of seven different knowledge editing methods across three large language models (LLMs) and three domains (geography, health, and technology).  The robustness scores, measured as percentages, represent the methods' resistance to disruption after being presented with a series of distracting prompts. The x-axis shows the number of turns, or distracting prompts, presented (0-10), with 0 indicating the initial efficacy score.  The y-axis represents the robustness score (0-100%).  Each line on the chart represents a different knowledge editing method, allowing for visual comparison of their resilience to these distractions across the LLMs and domains.", "section": "3.5 FACET 5: ROBUSTNESS"}]