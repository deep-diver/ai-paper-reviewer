[{"figure_path": "https://arxiv.org/html/2502.05179/extracted/6185701/figure/teaserv3.png", "caption": "Figure 1:  Comparison between FlashVideo and other text-to-video generation paradigms.\n(a) Single Stage DiT suffers from an explosive increase in computation cost when generating at large resolutions, rising from 30s to 2150s (circle in (d)) when increasing the resolution from 270p to 1080p. (b) Though the vanilla cascade can reduce the model size in the high resolution, its second stage still samples from Gaussian noise and only uses the first-stage results as a condition. This approach cannot effectively reduce the number of function evaluations at high resolution and still costs 571.5s (square in (d)) to generate a 1080p video. (c) In contrast, FlashVideo not only decreases the model size in the second stage but also starts sampling from the first-stage results, requiring only 4 function evaluations at high resolution while integrating a wealth of visually pleasant details, which can generate 1080P video with only 102.3s (triangle in (d)). Details on obtaining these statistics are provided in our Supplementary Materials.", "description": "Figure 1 compares three different text-to-video generation approaches: a single-stage diffusion model, a vanilla cascade model, and the proposed FlashVideo model.  The single-stage model shows a dramatic increase in computation time (from 30 seconds to 2150 seconds) when increasing the output resolution from 270p to 1080p. The vanilla cascade reduces model size at high resolution but still requires a significant amount of time (571.5 seconds for 1080p), as it samples from Gaussian noise and only uses the first-stage results as a condition. In contrast, FlashVideo achieves substantial computational efficiency by using a two-stage process.  The first stage generates a low-resolution video using a large model and many function evaluations to ensure high fidelity, and then the second stage leverages flow matching to generate a high-resolution video from the low-resolution output using a smaller model and only 4 function evaluations. This process results in a significant reduction in computation time (102.3 seconds for 1080p), while still maintaining high visual quality.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.05179/extracted/6185701/figure/methodv3.png", "caption": "Figure 2:  The overall pipeline of FlashVideo. FlashVideo adopts a cascade paradigm comprised of a 5-billion-parameter DiT at the low resolution (i.e., Stage \\Romannum1) and a 2-billion-parameter DiT at a higher resolution (i.e., Stage \\Romannum2). The 3D RoPE is employed at both stages to model the global and relative spatiotemporal distances efficiently. We construct training data pairs for Stage \\Romannum1 by randomly sampling Gaussian noise and low-resolution video latent. For Stage \\Romannum2, we apply both pixel and latent degradation to high-quality videos to obtain low-quality latent values. These are then paired with high-quality latents to serve as training data. During inference, we retain a sufficient N\u2062F\u2062E=50\ud835\udc41\ud835\udc39\ud835\udc3850NFE=50italic_N italic_F italic_E = 50 at a low resolution of 270p for Stage \\Romannum1. The generated videos retains high fidelity and seamless motion, albeit with detail loss. These videos are then upscaled to a higher resolution of 1080p and processed by latent degradation. With only 4 steps, our Stage \\Romannum2 regenerates accurate structures and rich high-frequency details.", "description": "FlashVideo uses a two-stage approach. Stage I uses a large 5-billion parameter model at low resolution (270p) with many function evaluations (NFEs) to prioritize accuracy and high fidelity.  Stage II employs a smaller 2-billion parameter model at high resolution (1080p) to refine details using a flow matching technique that requires only a few NFEs.  Both stages utilize 3D RoPE for efficient spatiotemporal modeling.  The training data for Stage I consists of random Gaussian noise and low-resolution video latents, while Stage II's training data is created by degrading high-quality videos to obtain low-quality latents that are then paired with their high-quality counterparts. During inference, Stage I generates a low-resolution video (with 50 NFEs), which is then upscaled and further processed by Stage II (with 4 NFEs) to produce the final high-resolution video.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.05179/x1.png", "caption": "Figure 3: Visual showcase of D\u2062E\u2062Gp\u2062i\u2062x\u2062e\u2062l\ud835\udc37\ud835\udc38subscript\ud835\udc3a\ud835\udc5d\ud835\udc56\ud835\udc65\ud835\udc52\ud835\udc59DEG_{pixel}italic_D italic_E italic_G start_POSTSUBSCRIPT italic_p italic_i italic_x italic_e italic_l end_POSTSUBSCRIPT and D\u2062E\u2062Gl\u2062a\u2062t\u2062e\u2062n\u2062t\ud835\udc37\ud835\udc38subscript\ud835\udc3a\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61DEG_{latent}italic_D italic_E italic_G start_POSTSUBSCRIPT italic_l italic_a italic_t italic_e italic_n italic_t end_POSTSUBSCRIPT impact on quality enhancement. From left to right, the first is the i\u2062n\u2062p\u2062u\u2062t\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61inputitalic_i italic_n italic_p italic_u italic_t, generated by the first-stage model. The term D\u2062E\u2062Gp\u2062i\u2062x\u2062e\u2062l\ud835\udc37\ud835\udc38subscript\ud835\udc3a\ud835\udc5d\ud835\udc56\ud835\udc65\ud835\udc52\ud835\udc59DEG_{pixel}italic_D italic_E italic_G start_POSTSUBSCRIPT italic_p italic_i italic_x italic_e italic_l end_POSTSUBSCRIPT stands for the improved result yielded from the model trained only with pixel-space degradation, which adds high-frequency details to the i\u2062n\u2062p\u2062u\u2062t\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61inputitalic_i italic_n italic_p italic_u italic_t. Further, D\u2062E\u2062Gp\u2062i\u2062x\u2062e\u2062l\ud835\udc37\ud835\udc38subscript\ud835\udc3a\ud835\udc5d\ud835\udc56\ud835\udc65\ud835\udc52\ud835\udc59DEG_{pixel}italic_D italic_E italic_G start_POSTSUBSCRIPT italic_p italic_i italic_x italic_e italic_l end_POSTSUBSCRIPT & D\u2062E\u2062Gl\u2062a\u2062t\u2062e\u2062n\u2062t\ud835\udc37\ud835\udc38subscript\ud835\udc3a\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61DEG_{latent}italic_D italic_E italic_G start_POSTSUBSCRIPT italic_l italic_a italic_t italic_e italic_n italic_t end_POSTSUBSCRIPT refers to the enhanced result with model trained under both types of degradation, which further improves small structures, such as generating branches for small trees. The improvement is significantly apparent when compared to pixel degradation only.", "description": "This figure demonstrates the impact of different data augmentation techniques on the quality of video generation.  The input video is generated by the first stage of the FlashVideo model and is relatively low resolution.  The image shows three versions of the same video: \n\n1. **Input:** The original output of the first-stage model, showing the basic scene and motion but lacking fine details.\n2. **DEGpixel:** The result after applying only pixel-space degradation to the input video. This enhances the high-frequency details, resulting in a sharper video.\n3. **DEGpixel & DEGlatent:** The result after applying both pixel-space and latent-space degradation techniques to the input. This further improves the fine details (e.g., small tree branches), demonstrating the added benefit of the latent degradation approach.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.05179/x2.png", "caption": "Figure 4: Generated videos of FlashVideo. The results in the top and bottom rows are from Stage \\Romannum1 and Stage \\Romannum2, respectively. Stage \\Romannum1 generates videos with natural motion and high prompt fidelity, as evident from the visual elements (bold in prompts). However, they lack detailed structures for small objects and high-frequency textures (see the red box). In Stage \\Romannum2, details are significantly enriched (see the green box), while content remains highly consistent with the original. Visualization results are compressed. More uncompressed cases can be found on our project page.", "description": "This figure showcases the two-stage video generation process of the FlashVideo model.  The top row displays the output from Stage 1, which prioritizes accurate representation of the text prompt (shown in bold). While the videos exhibit natural motion and good overall fidelity, they lack fine details in smaller objects and high-frequency textures (highlighted by a red box). The bottom row shows the output from Stage 2, which refines the Stage 1 output. This stage significantly enhances the visual details (highlighted by a green box) while ensuring high consistency with the initial, prompt-focused output. Note that these are compressed visualizations; higher-resolution versions are available on the project's GitHub page.", "section": "Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2502.05179/x3.png", "caption": "Figure 5: Quality improvements in Stage \\Romannum2 . We mark regions with artifacts and lacking detail in the first-stage videos using red boxes, while improvements from the second stage are highlighted in green. Zoom in for a better view. Our Stage \\Romannum2 significantly elevates visual quality across diverse content\u2014enhancing oil painting\u2013style sunflowers in (a), refining wrinkles and hair in (b), enriching texture structures of animals and plants in (c) and (d), and mitigating facial and object artifacts in (e).", "description": "Figure 5 presents a visual comparison showcasing the improvements achieved by Stage II of the FlashVideo model.  The left column displays the outputs of Stage I, highlighting areas with artifacts and missing details using red boxes. The right column shows the corresponding results after processing with Stage II, where enhanced regions are marked with green boxes. Specifically, the figure demonstrates improved visual quality through several examples: (a) enhanced detail and clarity in oil-painted sunflowers, (b) refined wrinkles and hair, (c) and (d) richer textures in animals and plants, and (e) correction of facial and object artifacts.  The comparison visually emphasizes the enhancement in fine details and overall visual quality resulting from the two-stage process.", "section": "Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2502.05179/x4.png", "caption": "Figure 6: Visual comparison with various video enhancement methods. We present our results alongside enhanced versions, derived from the first-stage outputs, of four video enhancement methods.", "description": "Figure 6 shows a comparison of FlashVideo's high-resolution video generation results with those of four other state-of-the-art video enhancement methods.  The comparison uses the low-resolution output from FlashVideo's first stage as input to each of the other methods to highlight the relative performance gains of FlashVideo's two-stage approach.  The figure visually demonstrates the enhanced detail and overall visual quality achieved by FlashVideo compared to the other methods.", "section": "4.5 Comparison with Video Enhancement Methods"}, {"figure_path": "https://arxiv.org/html/2502.05179/x5.png", "caption": "Figure 7: Comparison of long-range detail consistency in large-motion videos. We select a first-stage generated video with significant motion and sample three key frames. The girl in this video undergoes substantial scale variation from distant to close-up views. VEhancer\u00a0He et\u00a0al. [2024], with spatial-temporal module and time slicing, fails to preserve identity and detail consistency. In contrast, FlashVideo leverages 3D full attention to maintain consistent facial identity and texture details.", "description": "This figure compares the long-range detail consistency of two different video generation models in videos containing significant motion and substantial scale variation. Three key frames are extracted from a video generated by the first stage of the FlashVideo model, showcasing a girl whose size changes considerably from distant to close-up views.  The comparison highlights that VEnhancer, using a spatial-temporal module and time slicing, struggles to maintain consistent facial identity and detail over these changes. In contrast, the FlashVideo model, which leverages 3D full attention, successfully preserves facial identity and texture details throughout.", "section": "4.5 Comparison with Video Enhancement Methods"}, {"figure_path": "https://arxiv.org/html/2502.05179/x6.png", "caption": "Figure 8: Results of resolution extrapolation using absolute sinusoidal and RoPE position embeddings. Both settings perform well at the training resolution. However, while RoPE preserves detail enhancement at higher resolutions, absolute position embedding introduces noticeable artifacts beyond the training range.", "description": "This figure compares the performance of two different positional encoding methods, absolute sinusoidal positional embedding and Rotary Position Embedding (RoPE), in handling resolution extrapolation in a diffusion model for video generation.  Both methods perform well within the training resolution.  However, when the model is asked to generate videos at resolutions beyond those it was trained on, absolute sinusoidal positional embedding produces noticeable artifacts, while RoPE maintains the quality of detail enhancement. This demonstrates RoPE's robustness in handling unseen resolutions.", "section": "Ablation"}, {"figure_path": "https://arxiv.org/html/2502.05179/extracted/6185701/figure/cfg_step.png", "caption": "Table 8: Results of FlashVideo under different numbers of function evaluations (NFEs). The recommended range is highlighted in gray.", "description": "This table presents the results of the FlashVideo model when varying the number of function evaluations (NFEs).  The NFEs represent the number of iterative denoising steps in the diffusion model.  The table shows how different numbers of NFEs impact the quality of the generated videos, measured using various metrics such as MUSIQ, MANIQA, CLIPIQA, and NIQE. The gray highlighted range indicates the recommended number of NFEs for optimal video quality and computational efficiency.  The results show a balance between improving video quality and keeping the computation cost reasonable, since very high NFEs lead to high quality but are computationally expensive.", "section": "4.4 Quantitative Results"}]