[{"content": "| Model | $n_{\\text{layers}}$ | $d_{\\text{model}}$ | $r_{\\text{ffn}}$ | $n_{\\text{heads}}$ | $n_{\\text{kv_heads}}$ |\n|---|---|---|---|---|---| \n| LLaMA-3.2-3B | 28 | 3,072 | 2.7 | 24 | 8 |\n| Phi-3-mini-4k-instruct | 32 | 3,072 | 2.7 | 32 | 32 |\n| MiniCPM-2B | 40 | 2,304 | 2.5 | 36 | 36 |\n| MiniCPM3-4B | 62 | 2,560 | 2.5 | 40 | 40 |\n| Qwen2.5-1.5B | 28 | 1,536 | 5.8 | 12 | 2 |\n| MobileLLM-1B | 54 | 1,280 | 2.8 | 20 | 5 |\n| YuLan-Mini | 56 | 1,920 | 2.5 | 30 | 6 |", "caption": "Table 1: Hyperparameter settings of diffrent models. rffnsubscript\ud835\udc5fffnr_{\\text{ffn}}italic_r start_POSTSUBSCRIPT ffn end_POSTSUBSCRIPT is the ratio of the feed-forward network\u2019s hidden size to the model\u2019s hidden size. The definition of the symbols is available at Table\u00a08", "description": "This table details the hyperparameter settings for various language models, highlighting the differences in their architectures.  It shows the number of layers (Nlayers), the model's hidden dimension (dmodel), the feed-forward network hidden size relative to the model's hidden size (rffn), the number of attention heads (Nheads), and the number of key-value heads (Nkv_heads). These hyperparameters are crucial in determining the model's capacity, training efficiency, and overall performance.  A full definition of these symbols can be found in Table 8 of the paper.", "section": "2.1 Model Architecture"}, {"content": "| Tokenizer | Vocabulary Size | Web | Chinese | Math | Code |\n|---|---|---|---|---|---| \n| Gemma2-2B | 256,000 | 4.928 | 3.808 | 2.865 | 3.309 |\n| Qwen2.5 | 151,936 | 4.935 | 3.956 | 2.890 | 3.881 |\n| LLaMA-3.1 | 128,000 | 4.994 | 3.263 | 3.326 | 3.911 |\n| MiniCPM-2.4B | 122,753 | 4.753 | 4.273 | 2.739 | 3.052 |\n| Phi-3.5-mini | 100,352 | 4.311 | 1.914 | 2.654 | 3.110 |\n| MiniCPM-1.2B | 73,440 | 4.631 | 4.042 | 2.696 | 3.017 |\n| YuLan-Mini | 99,000 | 4.687 | 4.147 | 2.716 | 3.033 |\n| YuLan-Mini + Dropout | 99,000 | 4.687 | 4.146 | 2.715 | 3.031 |", "caption": "Table 2: Compression rate of different tokenizers. Higher values indicate more effective compression.", "description": "This table presents a comparison of the compression rates achieved by different tokenizers.  The compression rate is calculated based on the size of the vocabulary (in terms of the number of unique tokens) and the amount of text compressed in different domains: Web, Chinese, Math, and Code. A higher compression rate indicates that the tokenizer is able to represent the same amount of text using a smaller vocabulary, leading to increased efficiency and reduced storage requirements.", "section": "2 Overall Pre-Training Configuration"}, {"content": "Method|SI|MiniCPM|CerebrasGPT|YuLan-Mini\n---|---|---|---|---\nScale Embedding Output|1|12|10|10\nScale MHA equation|1/\u221ad<sub>head</sub>|1/\u221ad<sub>head</sub>|1/d<sub>head</sub>|1/\u221ad<sub>head</sub>\nScale Residual Connection|1|1.4/\u221an<sub>layers</sub>|1|1.4/\u221an<sub>layers</sub>\nQKV Weights LR|\u03b7<sub>base</sub>|\u03b7<sub>base</sub>/m<sub>width</sub>|\u03b7<sub>base</sub>/m<sub>width</sub>|\u03b7<sub>base</sub>/m<sub>width</sub>\nQKV \u03c3 Init|\u03c3<sub>base</sub><sup>2</sup>|\u03c3<sub>base</sub><sup>2</sup>/m<sub>width</sub>|\u03c3<sub>base</sub><sup>2</sup>/m<sub>width</sub>|\u03c3<sub>base</sub><sup>2</sup>/m<sub>width</sub>\nO Weights LR|\u03b7<sub>base</sub>|\u03b7<sub>base</sub>/m<sub>width</sub>|\u03b7<sub>base</sub>/m<sub>width</sub>|\u03b7<sub>base</sub>/m<sub>width</sub>\nO \u03c3 Init|\u03c3<sub>base</sub><sup>2</sup>/(2n<sub>layers</sub>)|\u03c3<sub>base</sub><sup>2</sup>/m<sub>width</sub>|\u03c3<sub>base</sub><sup>2</sup>/m<sub>width</sub>|\u03c3<sub>base</sub><sup>2</sup>/(2m<sub>width</sub>\u22c5n<sub>layers</sub>)\nFFN1 Weights LR|\u03b7<sub>base</sub>|\u03b7<sub>base</sub>/m<sub>width</sub>|\u03b7<sub>base</sub>/m<sub>width</sub>|\u03b7<sub>base</sub>/m<sub>width</sub>\nFFN1 \u03c3 Init|\u03c3<sub>base</sub><sup>2</sup>|\u03c3<sub>base</sub><sup>2</sup>/m<sub>width</sub>|\u03c3<sub>base</sub><sup>2</sup>/m<sub>width</sub>|\u03c3<sub>base</sub><sup>2</sup>/m<sub>width</sub>\nFFN2 Weights LR|\u03b7<sub>base</sub>|\u03b7<sub>base</sub>/m<sub>width</sub>|\u03b7<sub>base</sub>/m<sub>width</sub>|\u03b7<sub>base</sub>/m<sub>width</sub>\nFFN2 \u03c3 Init|\u03c3<sub>base</sub><sup>2</sup>/(2n<sub>layers</sub>)|\u03c3<sub>base</sub><sup>2</sup>/m<sub>width</sub>|\u03c3<sub>base</sub><sup>2</sup>/m<sub>width</sub>|\u03c3<sub>base</sub><sup>2</sup>/(2m<sub>width</sub>\u22c5n<sub>layers</sub>)\nScale Output logits|1|1/m<sub>width</sub>|1/m<sub>width</sub>|1", "caption": "Table 3: Comparison of the used hyperparameter settings for training stability, where the detailed explanation for the variables are in Table\u00a08.\nWe include SI\u00a0(Takase et\u00a0al., 2023) for comparison, MiniCPM\u00a0(Hu et\u00a0al., 2024), CerebrasGPT\u00a0(Dey et\u00a0al., 2023a). The definition of the symbols is available at Table\u00a08 .", "description": "Table 3 compares the hyperparameter settings used for training stability across four different language models: YuLan-Mini, SI, MiniCPM, and CerebrasGPT.  It details various scaling factors, learning rates, and initialization methods applied to different model components (embeddings, attention, feedforward network) to improve training stability.  The table highlights the choices made in each model and helps to understand why the different models may have different training stability properties. Refer to Table 8 for the meaning of the symbols used.", "section": "3.2 Training Instability Mitigation Methods"}, {"content": "| Type | Source | Volume |\n|---|---|---|\n| Web Pages | FineWeb-Edu, DCLM, Chinese-FineWeb-Edu | 559.76B |\n| Math (Pretrain) | AutoMathText, Proof-Pile-2, OpenWebMath Pro | 85.00B |\n| Code (Pretrain) | the-stack-v2, StarCoder | 202.44B |\n| General Knowledge | arXiv, StackExchange, English News | 121.87B |\n| Books | CBook, Gutenberg, LoC-PD-Books | 52.13B |\n| Encyclopedia | Wikipedia, Baidu-Baike | 14.80B |\n| Open-Source Instruction | SlimOrca, OpenMathInstruct-1, JiuZhang3.0 | 11.64B |\n| Synthetic Pretrain Data (Ours) | Synthetic document (seed: AutoMathText, LeetCode) | 8.76B |\n| Synthetic Instruction (Ours) | Reasoning (seed: MetaMathQA, DeepMind Math, \u2026) | 23.52B |\n| Total | - | 1,080B |", "caption": "Table 4: Statistical information of the entire pre-training corpus for YuLan-Mini. The data during the annealing process is detailed in Table\u00a05. For model reproducibility, all curated datasets are placed in Appendix\u00a0D, and the remaining synthetic data we generated is open-sourced.", "description": "This table details the composition of the 1.08 trillion tokens used to pre-train the YuLan-Mini language model.  It breaks down the data by type (web pages, math, code, general knowledge, books, etc.), source (specific datasets used), and volume (in billions of tokens). The data used in the annealing phase of training is further detailed in Table 5. To ensure reproducibility, the curated datasets are available in Appendix D, and the synthetically generated data is open-sourced.", "section": "2 Overall Pre-Training Configuration"}, {"content": "| Domain | Type | Dataset | Volume |\n|---|---|---|---| \n| Mix | Pretrain | FineWeb-Edu, CBook, arXiv | 64.65B |\n| Math | (1) CoT | Deepmind-Math, MathInstruct | 3.07B |\n|  | (2) Long CoT | Numina, AMPS, Platypus | 0.61B |\n|  | (3) Formal math | Lean-GitHub, Lean-WorkBook, DeepSeek-Prover-V1 | 0.10B |\n|  | (4) Curated | Tulu v3, MathInstruct | 1.42B |\n| Code | (1) CoT | OSS-Instruct (seed: the-Stack-v2), OpenCoder-LLM | 6.66B |\n|  | (2) Curated | LeetCode, XCoder-80K | 2.39B |\n| Science | (1) Long CoT | Camel-ai | 0.04B |\n|  | (2) Curated | EvolKit-20k, Celestia, Supernova | 1.06B |\n| Total | - | - | 80B |", "caption": "Table 5: Detailed information of the training data in the annealing stage.", "description": "This table details the composition of the training dataset used during the annealing phase of the YuLan-Mini language model pre-training. It breaks down the data by domain (Mix, Math, Code, Science) and further categorizes it by type (pretrain, CoT, long CoT, formal, curated), showing the source dataset and volume (in billions of tokens) for each category. This provides a granular view of the high-quality data used to optimize YuLan-Mini's capabilities in the final stage of training.", "section": "2 Overall Pre-Training Configuration"}, {"content": "| Models | Model | Size | # Train | Tokens | Context | Length | MATH | GSM | Human | Eval | MBPP | RACE | Middle | RACE | High | RULER |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| MiniCPM | 2.6B | 1.06T | 4K | 15.00 | 53.83 | 50.00<sup>\u2217</sup> | 47.31 | 56.61 | 44.27 | N/A |\n| Qwen-2 | 1.5B | 7T | 128K | 22.60 | 46.90<sup>\u2217</sup> | 34.80<sup>\u2217</sup> | 46.90<sup>\u2217</sup> | 55.77 | 43.69 | 60.16 |\n| Qwen2.5 | 0.5B | 18T | 128K | 23.60 | 41.60<sup>\u2217</sup> | 30.50<sup>\u2217</sup> | 39.30<sup>\u2217</sup> | 52.36 | 40.31 | 49.23 |\n| Qwen2.5 | 1.5B | 18T | 128K | 45.40 | 68.50<sup>\u2217</sup> | 37.20<sup>\u2217</sup> | 60.20<sup>\u2217</sup> | 58.77 | 44.33 | 68.26 |\n| Gemma2 | 2.6B | 2T | 8K | 18.30<sup>\u2217</sup> | 30.30<sup>\u2217</sup> | 19.50<sup>\u2217</sup> | 42.10<sup>\u2217</sup> | - | - | N/A |\n| StableLM2 | 1.7B | 2T | 4K | - | 20.62 | 8.50 | 17.50 | 56.33 | 45.06 | N/A |\n| SmolLM2 | 1.7B | 11T | 8K | 11.80 | - | 23.35 | 45.00 | 55.77 | 43.06 | N/A |\n| Llama3.2 | 3.2B | 9T | 128K | 7.40 | - | 29.30 | 49.70 | 55.29 | 43.34 | 77.06 |\n| YuLan-Mini | 2.4B | 1.04T | 4K | 32.60 | 66.65 | 61.60 | 66.70 | 55.71 | 43.58 | N/A |\n| YuLan-Mini | 2.4B | 1.08T | 28K | 37.80 | 68.46 | 64.00 | 65.90 | 57.18 | 44.57 | 51.48 |", "caption": "Table 6: Performance on math, code, and long context benchmarks. Results marked with * are cited from their official paper or report. The best and second best results are bold and underlined, respectively.", "description": "This table presents a comparative analysis of YuLan-Mini's performance against other baseline language models across various benchmarks categorized into mathematical reasoning, code generation, and long-context understanding.  The benchmarks used are designed to evaluate different aspects of language model capabilities, including mathematical problem-solving, code generation, and understanding long sequences of text. For each benchmark, the table shows the model's performance, indicating its relative strength across different types of tasks.  Results marked with an asterisk (*) were obtained from the cited models' official publications or reports. The best and second-best performance scores for each benchmark are highlighted in bold and underlined, respectively, to facilitate easy identification of top-performing models.", "section": "6.1 Experimental Setup"}, {"content": "| Model | Size |\n|---|---|", "caption": "Table 7: Performance on commonsense reasoning benchmarks. Results marked with * are cited from their official paper or report.", "description": "This table presents a quantitative comparison of YuLan-Mini's performance against other base models on several established commonsense reasoning benchmarks.  The benchmarks assess the model's abilities in understanding and utilizing common sense knowledge, evaluating performance across various aspects of logical and language comprehension.  Each benchmark's results, presented as a percentage score, reflects the model's success rate or accuracy in completing the tasks. Results marked with an asterisk (*) indicate scores reported in other papers or technical reports, not directly obtained in this study. The comparison allows for an evaluation of YuLan-Mini's strengths and weaknesses in comparison to other models and highlights its performance relative to others of similar scale.", "section": "6 Evaluation"}, {"content": "| # Train |\n|---|---| \n| Tokens |", "caption": "Table 8: Definition of the variables for computing the hyperparameters.", "description": "This table lists the meaning of all variables used in Section 3 of the paper for calculating the hyperparameters of the YuLan-Mini model.  Understanding these variables is crucial for reproducing the model's training configuration and results. The table provides a clear reference for the symbols used throughout the paper.", "section": "2 Overall Pre-Training Configuration"}, {"content": "| Context | Length |\n|---|---|", "caption": "Table 9: Small proxy models used to explore the training dynamics.", "description": "This table presents the configurations of smaller proxy models utilized in experiments to analyze training dynamics and instability.  These smaller models, with varying numbers of parameters and layers, helped to explore different aspects of training behavior, such as the impact of learning rate and the effects of various initialization strategies, without the high computational cost of conducting experiments on the full-scale model.", "section": "3.1.1 Preliminary Experiment on Indicators"}, {"content": "| MATH | 500 |\n|---|---|", "caption": "Table 10: Comprehensive list of all open-source datasets used. For datasets that are only available via links, we also offer additional guidance on our project website\u00a0https://github.com/RUC-GSAI/YuLan-Mini.", "description": "This table lists all open-source datasets used in the YuLan-Mini language model's pre-training.  It's categorized by domain (General, Code, Math) and provides the specific dataset names. For datasets only accessible via links, the table refers readers to the project's GitHub repository for further information.", "section": "4 Data Pipeline"}, {"content": "| GSM | 8K |\n|---|---|", "caption": "Table 12: Detailed data composition by training curriculum phases.", "description": "This table details the composition of the training data used in each of the 27 curriculum phases of the YuLan-Mini language model pre-training.  It breaks down the proportion of different data sources (English web pages and general content, Chinese data, code data, and mathematical data) used in each phase.  The phases are categorized into three stages: warm-up, stable training, and annealing.  The table shows how the data mix changes throughout these stages to optimize training efficiency and model performance.  Specific datasets contributing to each data category are listed within each phase.", "section": "4 Data Pipeline"}]