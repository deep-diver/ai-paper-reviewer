[{"figure_path": "https://arxiv.org/html/2502.10550/x1.png", "caption": "Figure 1: Systematic classification of problems with memory in RL reveals distinct memory utilization patterns and enables objective evaluation of memory mechanisms across different agents.", "description": "This figure illustrates a systematic classification framework for memory-intensive reinforcement learning (RL) tasks.  It highlights how different types of memory utilization (e.g., object memory, spatial memory, sequential memory, and memory capacity) lead to distinct problem characteristics. By categorizing problems in this way, the framework facilitates a more objective evaluation of various memory mechanisms across diverse RL agents.  The figure contrasts the limitations of evaluating memory solely on a single task set versus the advantages of a more comprehensive benchmark that incorporates diverse task types and memory demands.  This allows researchers to better understand and compare the strengths and weaknesses of different memory-enhanced agents.", "section": "4 CLASSIFICATION OF MEMORY-INTENSIVE TASKS"}, {"figure_path": "https://arxiv.org/html/2502.10550/x2.png", "caption": "Figure 2: Illustration of demonstrative memory-intensive tasks execution from the proposed MIKASA-Robo benchmark. The ShellGameTouch-v0 task requires the agent to memorize the ball\u2019s location under mugs and touch the correct one. In RememberColor9-v0, the agent must memorize a cube\u2019s color and later select the matching one. In RotateLenientPos-v0, the agent must rotate a peg while keeping track of its previous rotations.", "description": "Figure 2 showcases examples of tasks from the MIKASA-Robo benchmark, highlighting the memory requirements.  The ShellGameTouch-v0 task tests object permanence; the agent must remember a ball's location under cups and then touch the correct one.  The RememberColor9-v0 task assesses object recognition and short-term memory; the agent must remember the color of a cube and later select the same color. Lastly, the RotateLenientPos-v0 task evaluates spatial-temporal reasoning and working memory; the agent must rotate a peg a certain angle while tracking how much it's been rotated already. These diverse scenarios illustrate the various ways the benchmark assesses memory in robotic manipulation.", "section": "6 MIKASA-ROBO"}, {"figure_path": "https://arxiv.org/html/2502.10550/x3.png", "caption": "Figure 3: MIKASA bridges the gap between human-like memory complexity and RL agents requirements. While agents tasks don\u2019t require the full spectrum of human memory capabilities, they can\u2019t be reduced to simple spatio-temporal dependencies. MIKASA provides a balanced framework that captures essential memory aspects for agents tasks while maintaining practical simplicity.", "description": "This figure illustrates MIKASA's position within the spectrum of memory complexity.  Human memory is incredibly complex and nuanced, encompassing numerous aspects beyond the scope of current reinforcement learning (RL) agents.  Conversely,  simplistic RL tasks that only require remembering basic spatio-temporal information are insufficient for evaluating advanced memory capabilities. MIKASA successfully occupies a middle ground, offering a set of tasks that are challenging enough to probe various memory aspects in RL agents yet remain practical to implement and evaluate, bridging the gap between the complexity of human memory and the limitations of current RL agent capabilities.", "section": "4. CLASSIFICATION OF MEMORY-INTENSIVE TASKS"}, {"figure_path": "https://arxiv.org/html/2502.10550/x4.png", "caption": "Figure 4: Performance of PPO-MLP trained in state mode, i.e., in MDP mode without the need for memory. These results suggest that the proposed tasks are inherently solvable with a success rate of 100%percent\\%%.", "description": "Figure 4 presents the results of training a PPO agent with a Multilayer Perceptron (MLP) network in a fully observable environment (state mode).  This means the agent is provided with the complete state information at each time step, effectively eliminating the need for memory.  The plot shows that despite the variation in complexity among different tasks within the benchmark, all tasks achieve 100% success rate. This demonstrates the inherent solvability of the tasks in a fully observable setting and validates that the observed difficulties and performance differences in partially observable modes (using image or joint data, for instance) are directly due to the challenges in memory processing and information retrieval rather than issues in the task design itself.", "section": "6 MIKASA-ROBO"}, {"figure_path": "https://arxiv.org/html/2502.10550/x5.png", "caption": "Figure 5: PPO with MLP and LSTM backbones trained in RGB+joints mode on the RememberColor-v0 environment with dense rewards. Both architectures fail to solve medium and high complexity tasks.", "description": "This figure showcases the performance comparison of two different neural network architectures, MLP and LSTM, trained using Proximal Policy Optimization (PPO) algorithm. Both models were trained using RGB and joint data input, with dense reward settings on the RememberColor-v0 task. The results clearly demonstrate that neither architecture successfully solves the medium or high complexity variants of this task, which requires the agent to memorize and recall color information. This highlights the challenges posed by memory-intensive tasks even for advanced architectures like LSTMs.", "section": "6 MIKASA-ROBO"}, {"figure_path": "https://arxiv.org/html/2502.10550/x6.png", "caption": "Figure 6: Demonstration of PPO-MLP performance on MIKASA-Robo benchmark when trained with oracle-level state information. In this learning mode, MDP problem formulation is considered, i.e. memory is not required for successful problem solving. At the same time, the obtained results show that it is possible to solve these problems and obtain 100% Success Rate.", "description": "Figure 6 displays the success rates achieved by a PPO-MLP agent (without memory) across various tasks within the MIKASA-Robo benchmark.  The key condition is that the agent is trained with complete state information (oracle-level), effectively transforming the tasks into Markov Decision Processes (MDPs).  The results demonstrate that even without memory capabilities, the PPO-MLP agent can solve all presented tasks with 100% success rate, confirming the tasks' inherent solvability and validating the benchmark's design.", "section": "6 MIKASA-ROBO"}, {"figure_path": "https://arxiv.org/html/2502.10550/x7.png", "caption": "Figure 7: Demonstration of PPO-MLP performance on MIKASA-Robo benchmark when trained with oracle-level state information. Results are shown for memory capacity (SeqOfColors[3,5,7]-v0, BunchOfColors[3,5,7]-v0) and sequential memory (ChainOfColors[3,5,7]-v0).", "description": "Figure 7 displays the performance of a Proximal Policy Optimization algorithm with a Multilayer Perceptron (PPO-MLP) network on a subset of tasks from the MIKASA-Robo benchmark.  The key aspect is that the agent is trained with complete state information (oracle-level), meaning it doesn't need memory to solve the problems. The figure shows the learning curves for tasks assessing memory capacity (SeqOfColors and BunchOfColors variants) and sequential memory (ChainOfColors variants), demonstrating that even without the need for memory, the PPO-MLP agent can solve these tasks.  This provides a baseline for comparison to memory-enhanced agents in later experiments.", "section": "6 MIKASA-ROBO"}, {"figure_path": "https://arxiv.org/html/2502.10550/x8.png", "caption": "Figure 8: Performance evaluation of PPO-MLP and PPO-LSTM on the MIKASA-Robo benchmark using the \u201cRGB+joints\u201d training mode with dense reward function, where the agent only receives images from the camera (from above and from the gripper) and information about the state of the joints (position and velocity). The results demonstrate that numerous tasks pose significant challenges even for PPO-LSTM agents with memory, establishing these environments as effective benchmarks for evaluating advanced memory-enhanced architectures.", "description": "This figure showcases the performance comparison of two reinforcement learning (RL) agents, PPO-MLP (a memory-less agent) and PPO-LSTM (an agent with memory), on the MIKASA-Robo benchmark.  The agents were trained under the  \u201cRGB+joints\u201d training mode, which means they only received visual input (images from cameras above and on the gripper) and joint state information (position and velocity).  A dense reward system was used.  The results reveal significant performance differences between the agents across the various tasks.  While the PPO-LSTM (memory-enhanced) agent outperforms PPO-MLP in simpler tasks, the performance of both agents degrades as the tasks become more complex, highlighting the challenges posed by memory-intensive tasks. The overall results support the effectiveness of the MIKASA-Robo benchmark for evaluating advanced memory-enhanced architectures.", "section": "6.3 Performance of Classic Baselines on MIKASA-Robo Benchmark"}, {"figure_path": "https://arxiv.org/html/2502.10550/x9.png", "caption": "Figure 9: Performance evaluation of PPO-MLP and PPO-LSTM on the MIKASA-Robo benchmark using the \u201cRGB+joints\u201d with sparse reward function training mode, where the agent only receives images from the camera (from above and from the gripper) and information about the state of the joints (position and velocity). This training mode with sparse reward function causes even more difficulty for the agent to learn, making this mode even more challenging for memory-enhanced agents.", "description": "Figure 9 displays the performance comparison of two different reinforcement learning agents (PPO-MLP and PPO-LSTM) trained on the MIKASA-Robo benchmark under specific conditions.  The agents were trained using the \u201cRGB+joints\u201d mode, receiving only visual data from cameras (above and gripper views) and joint states (position and velocity).  Crucially, the training utilized a sparse reward system, meaning that the agents only received rewards upon successful task completion, instead of receiving rewards at every step. This created a much more difficult training scenario. The results demonstrate the significant challenge posed by the combination of partial observability (limited RGB and joint information) and sparse rewards, highlighting the difficulty of these memory-intensive tasks.  Even the PPO-LSTM agent, which incorporates an LSTM (a recurrent neural network suitable for sequential tasks), struggled significantly, suggesting that the challenging task conditions strongly impact even advanced memory-enhanced RL agents.", "section": "6.3 Performance of Classic Baselines on MIKASA-Robo Benchmark"}, {"figure_path": "https://arxiv.org/html/2502.10550/x10.png", "caption": "Figure 10: ShellGameTouch-v0: The robot observes a ball in front of it. next, this ball is covered by a mug and then the robot has to touch the mug with the ball underneath.", "description": "The ShellGameTouch-v0 task is a robotic manipulation task where the robot initially observes a ball placed under one of three cups.  The cups are then moved to obscure the ball's location.  The robot's objective is to identify and touch the cup concealing the ball, demonstrating object permanence and spatial reasoning skills.", "section": "E MIKASA-ROBO DETAILED TASKS DESCRIPTION"}, {"figure_path": "https://arxiv.org/html/2502.10550/x11.png", "caption": "Figure 11: RememberColor9-v0: The robot observes a colored cube in front of it, then this cube disappears and an empty table is shown. Then 9 cubes appear on the table, and the agent must touch a cube of the same color as the one it observed at the beginning of the episode.", "description": "The RememberColor9-v0 task evaluates an agent's ability to remember and identify objects based on their visual properties. This capability is essential for real-world robotics applications where agents must recall and match object characteristics across time intervals. The environment presents a sequence of colored cubes on a table. The task proceeds in three phases: 1. Observation Phase (steps 0-4): A cube of a specific color is displayed, and the agent must memorize its color. 2. Delay Phase (steps 5-9): The cube disappears, leaving an empty table. 3. Selection Phase (steps 10+): Multiple cubes of different colors appear (3, 5, or 9 depending on the selected difficulty level), and the agent must identify and touch the cube matching the original color.", "section": "E MIKASA-ROBO DETAILED TASKS DESCRIPTION"}, {"figure_path": "https://arxiv.org/html/2502.10550/x12.png", "caption": "Figure 12: RememberShape9-v0: The robot observes an object with specific shape in front of it, then the object disappears and an empty table appears. Then 9 objects of different shapes appear on the table, and the agent must touch an object of the same shape as the one it observed at the beginning of the episode.", "description": "The task RememberShape-v0 evaluates the agent's ability to remember and identify objects based on their geometric properties.  The environment first shows a single object (cube, sphere, cylinder, etc.). Then, after a delay, 3, 5 or 9 objects of various shapes appear, and the agent must select the object with the same shape as the one shown initially.  This tests the agent's object permanence and shape recognition memory.  The figure shows a sequence of images illustrating the task's progression.", "section": "E MIKASA-ROBO DETAILED TASKS DESCRIPTION"}, {"figure_path": "https://arxiv.org/html/2502.10550/x13.png", "caption": "Figure 13: RememberShapeAndColor5x3-v0: An object of a certain shape and color appears in front of the agent. Then the object disappears and the agent sees an empty table. Then objects of 5 different shapes and 3 different colors appear on the table and the agent has to touch what it observed at the beginning of the episode.", "description": "The RememberShapeAndColor5x3-v0 task presents a sequence of three phases. Initially, the robot observes an object with a specific shape and color.  This object is then removed, leaving an empty table. Finally, the robot is presented with a set of fifteen objects (five shapes and three colors). The robot must successfully identify and touch the object whose shape and color matched the one observed in the first phase, demonstrating the robot's ability to remember visual characteristics (shape and color) across time.", "section": "E MIKASA-ROBO DETAILED TASKS DESCRIPTION"}, {"figure_path": "https://arxiv.org/html/2502.10550/x14.png", "caption": "Figure 14: InterceptMedium-v0: A ball rolls on the table in front of the agent with a random initial velocity, and the agent\u2019s task is to intercept this ball and direct it at the target zone.", "description": "The environment includes a ball rolling across a table and a target zone.  The agent must observe the ball's initial position and velocity, predict its trajectory, and then guide the ball into the target zone by interacting with it (e.g., pushing or hitting). The task has three difficulty levels based on the ball's velocity.", "section": "6.1 MIKASA-ROBO BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2502.10550/x15.png", "caption": "Figure 15: InterceptGrabMedium-v0: A ball rolls on the table in front of the agent with a random initial velocity, and the agent\u2019s task is to intercept this ball with a gripper and lift it up.", "description": "The figure shows a screenshot sequence of a robot attempting the InterceptGrabMedium-v0 task.  A ball is launched across a table at a random initial velocity. The robot must predict the ball's trajectory, precisely time its movement using its gripper to catch the ball, and then successfully lift the ball. The screenshot sequence illustrates the challenge of requiring both precise prediction of the ball's motion and dexterous manipulation skills.", "section": "6. MIKASA-ROBO"}, {"figure_path": "https://arxiv.org/html/2502.10550/x16.png", "caption": "Figure 16: RotateLenientPos-v0: A randomly oriented peg is placed in front of the agent. The agent\u2019s task is to rotate this peg by a certain angle (the center of the peg can be shifted).", "description": "The figure shows a robotic manipulation task called RotateLenientPos-v0.  A randomly oriented peg is presented to the robot arm. The goal is for the robot to rotate the peg by a specific angle. Importantly, the robot is allowed to slightly shift the peg's position while performing the rotation, making the task less precise than a strict rotation around a fixed point. This allows for evaluation of an agent's ability to accomplish a task while accounting for minor positional deviations.", "section": "E MIKASA-ROBO DETAILED TASKS DESCRIPTION"}, {"figure_path": "https://arxiv.org/html/2502.10550/x17.png", "caption": "Figure 17: RotateStrictPos-v0: A randomly oriented peg is placed in front of the agent. The agent\u2019s task is to rotate this peg by a certain angle (it is not allowed to move the center of the peg)", "description": "The figure shows a robotic manipulation task where an agent needs to rotate a peg to a specific angle without moving its center.  The task involves precise motor control and demonstrates a memory challenge because the agent needs to keep track of how much it has already rotated the peg.", "section": "E MIKASA-ROBO DETAILED TASKS DESCRIPTION"}, {"figure_path": "https://arxiv.org/html/2502.10550/x18.png", "caption": "Figure 18: TakeItBack-v0: The agent observes a green cube in front of him. The agent\u2019s task is to move the green cube to the red target, and as soon as it lights up violet, return the cube to its original position (the agent does not observes the original position of the cube).", "description": "The task starts with a green cube on a table. The agent must move this cube to a red target zone. Once the cube reaches the target, the target zone changes color to violet, indicating the agent must now return the cube to its original starting position.  The challenge is that the initial position of the cube is not visible to the agent after it has been moved to the red target zone; the agent must remember it from the first phase of the task.", "section": "E MIKASA-ROBO DETAILED TASKS DESCRIPTION"}, {"figure_path": "https://arxiv.org/html/2502.10550/x19.png", "caption": "Figure 19: SeqOfColors7-v0: In front of the agent, 7 cubes of different colors appear sequentially. After the last cube is shown, the agent observes an empty table. Then 9 cubes of different colors appear on the table and the agent has to touch the cubes that were shown at the beginning of the episode in any order.", "description": "The task presented in Figure 19, SeqOfColors7-v0, evaluates the agent's sequential memory. Initially, seven uniquely colored cubes are sequentially displayed to the agent, each for five time steps.  A delay period follows, showing an empty table. Finally, nine cubes (including those from the initial sequence) are displayed, and the agent must touch any of the seven cubes from the initial sequence. The order in which the agent touches the cubes does not matter; only the selection of the correct seven cubes is assessed.  This design tests the ability to remember a sequence of items and retrieve them from a larger set, emphasizing the unordered retrieval aspect of sequential memory.", "section": "E MIKASA-ROBO DETAILED TASKS DESCRIPTION"}, {"figure_path": "https://arxiv.org/html/2502.10550/x20.png", "caption": "Figure 20: BunchOfColors7-v0: 7 cubes of different colors appear simultaneously in front of the agent. After the agent observes an empty table. Then, 9 cubes of different colors appear on the table and the agent has to touch the cubes that were shown at the beginning of the episode in any order.", "description": "This figure illustrates the BunchOfColors7-v0 task, part of the MIKASA-Robo benchmark.  The task begins with seven cubes of different colors presented simultaneously to the agent.  After a brief pause (empty table), nine cubes of varying colors appear. The agent's objective is to touch only the seven cubes that were shown initially, in any order.  This tests the agent's ability to remember and recall a set of items presented at once, demonstrating memory capacity.", "section": "E MIKASA-ROBO DETAILED TASKS DESCRIPTION"}, {"figure_path": "https://arxiv.org/html/2502.10550/x21.png", "caption": "Figure 21: ChainOfColors7-v0: In front of the agent, 7 cubes of different colors appear sequentially. After the last cube is shown, the agent sees an empty table. Then 9 cubes of different colors appear on the table and the agent must unmistakably touch the cubes that were shown at the beginning of the episode, in the same strict order.", "description": "The figure demonstrates the ChainOfColors7-v0 task from the MIKASA-Robo benchmark.  First, seven uniquely colored cubes are shown to the agent one at a time. Then, the cubes disappear, and nine new cubes of various colors are displayed.  The agent's task is to touch the initial seven cubes in the exact order they were initially presented. Success requires perfect sequential memory of the colors. This tests the agent's capacity for sequential memory and precise action sequencing.", "section": "E MIKASA-Robo Detailed Tasks Description"}]