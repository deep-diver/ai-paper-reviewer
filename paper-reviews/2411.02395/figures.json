[{"figure_path": "https://arxiv.org/html/2411.02395/x1.png", "caption": "Figure 1: Overview of our method. Given user-defined or LLM-generated regional prompt-mask pairs, we can effectively achieve fine-grained compositional text-to-image generation.", "description": "This figure illustrates the architecture of the proposed Regional Prompting FLUX method for fine-grained compositional text-to-image generation.  It contrasts a naive approach with the authors' method. The naive attempt shows a single global prompt being used to generate the entire image.  The Regional Prompting FLUX method, however, breaks down the user-defined or LLM-generated prompt into multiple regional prompts, each paired with a corresponding mask specifying the area of the image it affects. These regional prompts and masks allow for finer control over the composition of the generated image, enabling the creation of complex scenes with distinct regions possessing different characteristics.  The process involves enriching the prompt using LLM to extract key features and concepts, then using a FLUX diffusion transformer to generate the image through a process that combines global and regional prompts.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2411.02395/x2.png", "caption": "Figure 2: Main results. Simplified regional prompts are colored according to the layout mask. In practice, we input more detailed regional prompt about each region.", "description": "Figure 2 showcases the results of the proposed method on several example images.  Each image is generated using regional prompts, meaning different parts of the image are controlled by different text descriptions.  The simplified regional prompts shown in the figure are color-coded according to their corresponding regions in the image layout. However, the authors note that the actual regional prompts used during generation are more detailed than what is shown in the figure.  Each example demonstrates how fine-grained control is possible, generating different parts of a single image based on various detailed descriptions.  The examples shown include varied scenes and styles from surreal landscapes to more realistic depictions.", "section": "Main results"}, {"figure_path": "https://arxiv.org/html/2411.02395/x3.png", "caption": "Figure 3: Illustration of our Region-Aware Attention Manipulation module. The unified self-attention in FLUX can be broken down into four parts: cross-attention from image to text, cross-attention from text to image, and self-attention between image. After calculating the attention manipulation mask, we merge them to get the overall attention mask that is later fed into the attention calculation process.", "description": "Figure 3 details the Region-Aware Attention Manipulation module, a key component of the proposed method.  The figure illustrates how the unified self-attention mechanism within the FLUX model is decomposed into four distinct attention processes: cross-attention from image features to text embeddings, cross-attention from text embeddings to image features, and two self-attention processes (one for image features and one for text embeddings).  These individual attention mechanisms are each modified using specific masks. Finally, these individual attention masks are combined to create a unified attention mask which is then used to modulate the standard attention process, thereby achieving fine-grained control over how different regions of the image interact with their corresponding textual descriptions. This approach enables the model to effectively generate images that accurately reflect the spatial and semantic relationships specified in complex prompts.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2411.02395/x4.png", "caption": "Figure 4: Results with LoRAs and ControlNet. Colored prompts and masks are provided for the regional control for each example. The control image (pose & depth-map) for controlnet is attached within the left image. Zoom in to see in detail.", "description": "Figure 4 showcases the results of applying the proposed regional prompting method in conjunction with LoRAs (Low-Rank Adaptation) and ControlNet.  Each example demonstrates the effects of regional prompting on the generated images.  Colored prompts and masks highlight how different image regions correspond to specific textual descriptions. The left-most image in each set includes an inset showing the pose and depth map used as ControlNet input.  The caption encourages closer examination of the images for details.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2411.02395/x5.png", "caption": "Figure 5: Ablation results with base ratio \u03b2\ud835\udefd\\betaitalic_\u03b2, control steps T\ud835\udc47Titalic_T and control blocks B\ud835\udc35Bitalic_B.", "description": "This ablation study investigates the impact of three key hyperparameters on the performance of the regional prompting method: the base ratio (\u03b2), the number of control steps (T), and the number of control blocks (B).  Each hyperparameter is varied systematically across several settings while keeping the others constant. The results showcase how different values of \u03b2, T, and B affect the balance between maintaining regional distinctions and ensuring global image coherence. The figure visually demonstrates the impact of these hyperparameters on the final generated image, highlighting trade-offs between precise regional control and overall image quality.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2411.02395/x6.png", "caption": "Figure 6: Inference speed and gpu memory consumption comparison with standard FLUX.1-dev, FLUX equipped with RPG-based regional control, and our method.", "description": "Figure 6 presents a comparison of inference speed and GPU memory consumption among three different methods for image generation: the standard FLUX.1-dev model, FLUX.1-dev enhanced with RPG-based regional control, and the proposed method.  The x-axis shows the number of masks (regions) used in the image generation, while the y-axis represents inference time in seconds.  The bars also indicate the GPU memory used during inference. This comparison demonstrates the efficiency gains of the proposed method over other approaches, particularly as the number of regions increases.  The graph provides insights into the computational resource requirements of each approach for generating images with varying levels of compositional complexity.", "section": "Experiments"}]