{"references": [{" publication_date": "2015", "fullname_first_author": "Antol, S.", "paper_title": "VQA: Visual question answering", "reason": "This paper introduced the VQA task and dataset, which is foundational to the field of vision-language models and directly relevant to the evaluation and benchmarking of VLMs.  The paper's dataset and task definition have greatly influenced subsequent work, including the development of NaturalBench, which builds upon and addresses some of VQA's limitations.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Goyal, Y.", "paper_title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering", "reason": "This paper highlighted the limitations of early VQA datasets, where models could often succeed without truly understanding the image, by exploiting biases in the questions or relying on prior knowledge.  This critique directly informs NaturalBench's design, which is specifically crafted to mitigate these \"blind\" solution strategies by using a vision-centric approach.", "section_number": 1}, {" publication_date": "2014", "fullname_first_author": "Lin, T.-Y.", "paper_title": "Microsoft COCO: Common objects in context", "reason": "The COCO dataset is a large-scale image dataset that provides a rich source of image-text pairs for building VQA benchmarks.  NaturalBench leverages this dataset to collect natural adversarial examples, thereby enhancing the realism and diversity of its samples compared to previous benchmarks that relied on more controlled and curated image-text data.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Radford, A.", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "CLIP is a highly influential vision-language model used for pretraining and image-text embedding. NaturalBench uses CLIP to identify image-text pairs that are challenging for VLMs, demonstrating a reliance on vision-language models beyond human-level performance. This is crucial to its methodology and thus a key reference.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Li, J.", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "reason": "BLIP-2 is another influential vision-language model that is used in NaturalBench's methodology to identify challenging image-text pairs.  Comparing the performance of various VLMs on these challenging pairs is central to the benchmark's evaluation process, thereby highlighting the importance of this reference.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "OpenAI", "paper_title": "GPT-4 technical report", "reason": "GPT-4 is a leading large language model, and its performance is evaluated on NaturalBench.  The comparison between GPT-4's performance and human performance on NaturalBench is pivotal for demonstrating the benchmark's ability to effectively evaluate even state-of-the-art models.  GPT-4's use here is important to the main argument.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Liu, H.", "paper_title": "LLaVA-1.5", "reason": "LLaVA-1.5 is a strong open-source VLM used for benchmarking.  Its performance on NaturalBench is used as a point of comparison against other models, and its inclusion highlights the benchmark's relevance in the context of current VLM development.  Its inclusion demonstrates the breadth of the evaluation.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Fu, C.", "paper_title": "MME: A comprehensive evaluation benchmark for multimodal large language models", "reason": "MME is a popular VQA benchmark that NaturalBench contrasts with.  The comparison highlights MME's susceptibility to \"blind\" solutions, which is a major motivation behind NaturalBench's design and creation. This is important in setting context.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Chen, L.", "paper_title": "Are we on the right way for evaluating large vision-language models?", "reason": "This paper offers critical insights into the biases and limitations of existing VQA benchmarks, similar to NaturalBench's arguments.  Its analysis of benchmarks' flaws in preventing \"blind\" solutions and the importance of visual reasoning strongly relates to the core argument of NaturalBench.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Kiela, D.", "paper_title": "Dynabench: Rethinking benchmarking in NLP", "reason": "This paper advocates for dynamic evaluation in NLP, a concept that is relevant to NaturalBench's design.  NaturalBench's adaptability to new datasets and its emphasis on continuous evaluation aligns with the principles of dynamic evaluation outlined in this paper.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Hendrycks, D.", "paper_title": "Natural adversarial examples", "reason": "This work introduces the concept of adversarial examples in the context of image classification and directly relates to the concept of \"natural adversarial samples\" used in NaturalBench.  This paper inspires the core concept of the benchmark, making this a highly important reference.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Nie, Y.", "paper_title": "Adversarial NLI: A new benchmark for natural language understanding", "reason": "This paper discusses the development of adversarial examples for natural language inference (NLI), a related task that shares similarities with VQA in terms of benchmark design and challenges in preventing \"blind\" solutions.  The techniques and insights from this work are relevant for creating more robust VQA benchmarks like NaturalBench.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Lu, P.", "paper_title": "MathVista: Evaluating mathematical reasoning of foundation models in visual contexts", "reason": "MathVista is another example of a multimodal benchmark.  The discussion of complex reasoning problems in MathVista and how they challenge VLMs highlights the need for a benchmark focused on visio-linguistic compositional reasoning, such as NaturalBench.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Li, B.", "paper_title": "Seed-Bench: Benchmarking multimodal LLMs with generative comprehension", "reason": "This paper presents Seed-Bench, another VQA benchmark that is compared against NaturalBench.  The comparison highlights the similarities and differences between Seed-Bench and NaturalBench in terms of their focus on visio-linguistic skills and their approaches to prevent \"blind\" solutions.  This is key to understanding the benchmark's unique contribution.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Chen, L.", "paper_title": "Are we on the right way for evaluating large vision-language models?", "reason": "This work provides a critical evaluation of existing VQA benchmarks, highlighting the limitations that NaturalBench aims to address. The discussion of \"blind\" solutions and biased datasets directly informs the design and rationale behind NaturalBench's creation.  It's important to the core argument.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Liu, Y.", "paper_title": "MMBench: Is your multi-modal model an all-around player?", "reason": "MMBench is another multimodal benchmark that NaturalBench contrasts with. The comparison highlights MMBench's susceptibility to \"blind\" solutions, a key motivation for NaturalBench's design. This comparison is key in demonstrating the novel contribution.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Dong, X.", "paper_title": "InternLM-XC2-4KHD: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd", "reason": "InternLM-XC2 is a strong VLM used for benchmarking.  Its performance on NaturalBench is a key result in showcasing the benchmark's effectiveness in evaluating state-of-the-art VLMs, which is central to the paper's purpose.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Hudson, D. A.", "paper_title": "GQA: A new dataset for real-world visual reasoning and compositional question answering", "reason": "GQA is a well-known VQA dataset that is frequently referenced in VQA research.  Comparing NaturalBench with GQA highlights the differences in their approaches to creating a challenging benchmark.  GQA's dataset and methods are relevant to the comparison.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Thrush, T.", "paper_title": "Winoground: Probing vision and language models for visio-linguistic compositionality", "reason": "Winoground is a benchmark for evaluating compositionality in vision-language models. NaturalBench builds on this work by introducing a more comprehensive skill taxonomy for a more granular evaluation of compositional reasoning capabilities.  This direct comparison is important in showing the novelty of the work.", "section_number": 5}]}