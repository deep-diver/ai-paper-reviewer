{"references": [{" publication_date": "2015", "fullname_first_author": "Antol, S.", "paper_title": "VQA: Visual question answering", "reason": "This paper introduced the VQA task, a foundational problem in vision-language research.  Its dataset and task definition have significantly influenced the development of vision-language models and benchmarks, making it a cornerstone of the field. The paper's impact is evident in the numerous subsequent works that build upon its concepts and datasets, shaping the direction of vision-language research.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Goyal, Y.", "paper_title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering", "reason": "This work directly addresses the issue of 'blind' solutions in VQA, where models rely on biases in the questions rather than visual information. The authors' efforts to improve the image understanding aspect of VQA are relevant to the current paper's focus on creating a benchmark that is less susceptible to such biases.  This paper's contributions to improving the robustness and quality of VQA benchmarks are directly relevant to the current research.", "section_number": 1}, {" publication_date": "2014", "fullname_first_author": "Goodfellow, I. J.", "paper_title": "Explaining and harnessing adversarial examples", "reason": "This seminal work introduced the concept of adversarial examples, which are crucial for understanding the robustness of machine learning models. The current paper's focus on 'natural adversarial samples' is directly related to the adversarial example concept. Understanding the limitations of models in the presence of adversarial examples is essential for developing robust and reliable vision-language models. The concept of adversarial examples is directly relevant to the proposed natural adversarial samples.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Hudson, D. A.", "paper_title": "GQA: A new dataset for real-world visual reasoning and compositional question answering", "reason": "GQA is a widely-used benchmark in VQA that pushes the boundaries of visual reasoning. Its focus on complex reasoning tasks and its scale make it an important benchmark for assessing the capabilities of vision-language models. The comparison of NaturalBench with existing benchmarks including GQA is critical in highlighting the novelty and strengths of NaturalBench.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Kiela, D.", "paper_title": "Dynabench: Rethinking benchmarking in NLP", "reason": "This paper advocates for dynamic evaluation in NLP, a concept directly applicable to the field of vision-language models.  NaturalBench's adaptability to dynamic evaluation, as described in Section 6, aligns with the ideas presented in this paper. The focus on continuous benchmark improvement and adaptation to avoid data leakage is relevant to the current research.  The methodology to improve the benchmark by adapting to evolving data is crucial.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "Plummer, B. A.", "paper_title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models", "reason": "The Flickr30K dataset is used extensively in vision-language research.  As a large-scale image-text dataset, it provides the basis for many vision-language model training and evaluation. Its availability and usage in numerous studies make it a key component of the vision-language research community.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Radford, A.", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "CLIP is a highly influential model in vision-language, and its use in the creation of NaturalBench is highlighted.  CLIP's ability to link images and text has made it a vital tool in various vision-language tasks.  The paper's methodology for creating NaturalBench relies heavily on CLIP's capabilities, highlighting the importance of this work.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "OpenAI", "paper_title": "GPT-4 technical report", "reason": "GPT-4 is a state-of-the-art large language model used in the generation of questions in NaturalBench. Its performance is evaluated on NaturalBench, making it a central model in the experimental results.  The use of a leading language model such as GPT-4 showcases the benchmark's capacity to challenge even the most advanced models.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Thrush, T.", "paper_title": "Winoground: Probing vision and language models for visio-linguistic compositionality", "reason": "Winoground is a benchmark dataset focused on assessing compositional reasoning abilities in vision-language models. The metrics and evaluation strategies in Winoground directly inform the NaturalBench-Retrieval benchmark, highlighting its relevance to the current work.  The current paper extends Winoground\u2019s approach to the challenging vision-language tasks.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Li, J.", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "reason": "BLIP-2 is a prominent vision-language model used in NaturalBench creation. The use of BLIP-2 alongside CLIP highlights the benchmark's reliance on multiple pre-trained models to identify challenging vision-language samples. The usage of this model in the benchmark design process is crucial.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Lu, P.", "paper_title": "Learn to explain: Multimodal reasoning via thought chains for science question answering", "reason": "ScienceQA is a prominent VQA benchmark that assesses complex reasoning capabilities.  It's used for comparison with NaturalBench in demonstrating the challenges of existing benchmarks and the unique strengths of NaturalBench in evaluating visio-linguistic skills. The comparison of NaturalBench with existing benchmarks is central to the argument.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Fu, C.", "paper_title": "MME: A comprehensive evaluation benchmark for multimodal large language models", "reason": "MME is a recent benchmark for evaluating multimodal large language models, often used for comparison with other benchmarks. The comparison with MME is relevant for positioning NaturalBench within the landscape of existing benchmarks. The paper shows that NaturalBench is more robust than MME.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Liu, Y.", "paper_title": "MMBench: Is your multi-modal model an all-around player?", "reason": "MMBench is another prominent VQA benchmark often used for comparison and contrasting with other benchmarks.  The comparison with MMBench helps to establish the relative strengths and weaknesses of NaturalBench in evaluating vision-language models. The paper highlights that NaturalBench is less prone to blind solutions than MMBench.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Li, J.", "paper_title": "InstructBLIP: Towards general-purpose vision-language models with instruction tuning", "reason": "InstructBLIP is a significant vision-language model whose performance on NaturalBench is reported and evaluated.  The inclusion of InstructBLIP\u2019s results highlights the benchmark's ability to evaluate a wide range of models, both open-source and closed-source, and its applicability to instruction-tuned models. The paper is one of the several models that are used to evaluate the benchmark.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Liu, H.", "paper_title": "LLaVA-1.5", "reason": "LLaVA is a leading vision-language model whose performance on NaturalBench is reported. The results demonstrate the benchmark's ability to evaluate advanced models and highlight areas for improvement.  Its inclusion helps to establish NaturalBench\u2019s effectiveness in assessing state-of-the-art models. The paper showcases the performance of a leading model.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Chen, L.", "paper_title": "Are we on the right way for evaluating large vision-language models?", "reason": "This paper directly addresses the issue of biases and limitations in existing VQA benchmarks, providing crucial context for the development of NaturalBench.  The authors' critique of existing benchmarks' susceptibility to bias motivates the creation of NaturalBench, which aims to mitigate those issues. This paper highlights the problems of existing benchmarks and motivates the need for NaturalBench.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Hsieh, C.-Y.", "paper_title": "Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality", "reason": "This paper directly addresses issues of compositionality in vision-language benchmarks and proposes ways to improve them. NaturalBench addresses these concerns by incorporating diverse and carefully defined skill tags, aligning with the efforts to improve benchmark design in terms of compositionality.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Lin, Z.", "paper_title": "Revisiting the role of language priors in vision-language models", "reason": "This paper examines the role of language priors in vision-language models, a crucial aspect in understanding the challenges posed by NaturalBench. By investigating the influence of language biases, this research contributes to the understanding of why existing benchmarks may not fully capture the capabilities of VLMs, making it relevant to the development and justification of NaturalBench.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Onoe, Y.", "paper_title": "Docci: Descriptions of connected and contrasting images", "reason": "DOCCI is a dataset incorporated into NaturalBench to enhance its dynamic evaluation capabilities.  The inclusion of DOCCI demonstrates the adaptability of the NaturalBench creation process and its capacity to integrate new data sources, a key aspect of dynamic evaluation.  The use of this dataset extends the reach and usefulness of the benchmark.", "section_number": 6}, {" publication_date": "2022", "fullname_first_author": "Thapliyal, A. V.", "paper_title": "Crossmodal-3600: A massively multilingual multimodal evaluation dataset", "reason": "XM3600 is another dataset incorporated into NaturalBench for dynamic evaluation, particularly focusing on multilingual capabilities. The inclusion of XM3600 demonstrates the adaptability of the benchmark creation process and its capacity to encompass diverse languages. The addition of XM3600 significantly enhances the diversity and global applicability of the benchmark.", "section_number": 6}]}