[{"figure_path": "2410.14669/tables/table_10_0.html", "caption": "Table 2: Debiased performance on NaturalBench. Many models underperform on NaturalBench due to biases towards certain answers like \u201cYes\u201d and \u201cB\u201d. To illustrate this, we compute a debiased Q-Acc by adjusting the prediction threshold (as described in Section 5) to ensure the model predict different answers for the two images of the same question. Similarly, debiased I-Acc ensures different predicted answers for the two questions of the same image. For debiased G-Acc, we tune the threshold so that the model predicts one answer for two (out of four) image-question pairs, and a different answer for the other two pairs. The substantial performance gains of these metrics suggest that proper debiasing can greatly improve performance. Our Appendix evaluates existing debiasing techniques that do not require prior knowledge of image-question pairings.", "description": "Table 2 shows that debiasing significantly improves the performance of vision-language models on NaturalBench by adjusting the prediction threshold to avoid repetitive answers across images or questions.", "section": "5 Why is NaturalBench Challenging?"}, {"figure_path": "2410.14669/tables/table_10_1.html", "caption": "Table 1: Performance on NaturalBench. We report the performance of 53 leading VLMs on NaturalBench. All models significantly lag behind human performance, with the performance gap (in G-Acc) between humans and models highlighted in red. The latest models, such as BLIP-3 (XGen-MM), Cambrian-1, LLaVA-OneVision, Llama3.2-Vision, Molmo, and Qwen2-VL lag significantly behind humans by 55% to 70%. Even the best closed-source GPT-4o is still 52% behind humans.", "description": "Table 1 presents the performance of 53 vision-language models on the NaturalBench benchmark, showing significant performance gaps between the models and human performance.", "section": "4 Experimental Results"}, {"figure_path": "2410.14669/tables/table_19_0.html", "caption": "Table 5: Performance on NaturalBench-Chinese and NaturalBench-Hindi. We report G-Acc for each dataset, evaluating only models with claimed multilingual capabilities. For both datasets, we also provide G-Acc after translating the original Chinese or Hindi questions into English. This simple translation often boosts performance, except for top models like InternVL-Chat-V1.2-Plus and GPT-40, which seem extensively trained in Chinese. NaturalBench-Hindi remains particularly challenging for open-source models.", "description": "Table 5 presents the group accuracy (G-Acc) of various vision-language models on the Chinese and Hindi subsets of NaturalBench, both before and after translation to English, highlighting the challenges posed by multilingual VQA tasks.", "section": "Expanding NaturalBench"}, {"figure_path": "2410.14669/tables/table_19_1.html", "caption": "Table 6: Ablation on different collection methods. We report G-Acc on datasets generated by different collection methods from Flickr30K. Our adversarial procedure results in a much more challenging dataset. Note that Flickr-Adversarial is the combination of Flickr-YN and Flickr-MCQ.", "description": "This table shows the performance (G-Acc) of various vision-language models on different subsets of the Flickr30K dataset, comparing the results from adversarially-generated samples versus randomly-matched samples, highlighting the effectiveness of the proposed method.", "section": "4 Experimental Results"}, {"figure_path": "2410.14669/tables/table_20_0.html", "caption": "Table 1: Performance on NaturalBench. We report the performance of 53 leading VLMs on NaturalBench. All models significantly lag behind human performance, with the performance gap (in G-Acc) between humans and models highlighted in red. The latest models, such as BLIP-3 (XGen-MM), Cambrian-1, LLaVA-OneVision, Llama3.2-Vision, Molmo, and Qwen2-VL lag significantly behind humans by 55% to 70%. Even the best closed-source GPT-40 is still 52% behind humans.", "description": "Table 1 presents the group accuracy (G-Acc) of 53 vision-language models on the NaturalBench benchmark, highlighting the significant performance gap between the models and human performance.", "section": "4 Experimental Results"}, {"figure_path": "2410.14669/tables/table_22_0.html", "caption": "Table 1: Performance on NaturalBench. We report the performance of 53 leading VLMs on NaturalBench. All models significantly lag behind human performance, with the performance gap (in G-Acc) between humans and models highlighted in red. The latest models, such as BLIP-3 (XGen-MM), Cambrian-1, LLaVA-OneVision, Llama3.2-Vision, Molmo, and Qwen2-VL lag significantly behind humans by 55% to 70%. Even the best closed-source GPT-40 is still 52% behind humans.", "description": "Table 1 presents the performance of 53 vision-language models on the NaturalBench benchmark, highlighting a significant performance gap between the models and human performance.", "section": "4 Experimental Results"}, {"figure_path": "2410.14669/tables/table_23_0.html", "caption": "Table 1: Performance on NaturalBench. We report the performance of 53 leading VLMs on NaturalBench. All models significantly lag behind human performance, with the performance gap (in G-Acc) between humans and models highlighted in red. The latest models, such as BLIP-3 (XGen-MM), Cambrian-1, LLaVA-OneVision, Llama3.2-Vision, Molmo, and Qwen2-VL lag significantly behind humans by 55% to 70%. Even the best closed-source GPT-40 is still 52% behind humans.", "description": "Table 1 presents the group accuracy (G-Acc) of 53 vision-language models on the NaturalBench benchmark, highlighting the significant performance gap between these models and human performance.", "section": "4 Experimental Results"}, {"figure_path": "2410.14669/tables/table_23_1.html", "caption": "Table 10: Model performance on Relation and Reasoning. We report Q-Acc on each tag.", "description": "Table 10 presents the model's question accuracy (Q-Acc) for each relation and reasoning skill tag in the NaturalBench benchmark.", "section": "C Skill Analysis"}]