[{"figure_path": "2410.14669/tables/table_10_0.html", "caption": "Table 2: Debiased performance on NaturalBench. Many models underperform on NaturalBench due to biases towards certain answers like \u201cYes\u201d and \u201cB\u201d. To illustrate this, we compute a debiased Q-Acc by adjusting the prediction threshold (as described in Section 5) to ensure the model predict different answers for the two images of the same question. Similarly, debiased I-Acc ensures different predicted answers for the two questions of the same image. For debiased G-Acc, we tune the threshold so that the model predicts one answer for two (out of four) image-question pairs, and a different answer for the other two pairs. The substantial performance gains of these metrics suggest that proper debiasing can greatly improve performance. Our Appendix evaluates existing debiasing techniques that do not require prior knowledge of image-question pairings.", "description": "Table 2 presents the original and debiased performance of several models on NaturalBench, highlighting the significant performance gains achieved through debiasing techniques.", "section": "5 Why is NaturalBench Challenging?"}, {"figure_path": "2410.14669/tables/table_10_1.html", "caption": "Table 1: Performance on NaturalBench. We report the performance of 53 leading VLMs on NaturalBench. All models significantly lag behind human performance, with the performance gap (in G-Acc) between humans and models highlighted in red. The latest models, such as BLIP-3 (XGen-MM), Cambrian-1, LLaVA-OneVision, Llama3.2-Vision, Molmo, and Qwen2-VL lag significantly behind humans by 55% to 70%. Even the best closed-source GPT-40 is still 52% behind humans.", "description": "Table 1 presents the performance of 53 vision-language models on the NaturalBench benchmark, highlighting the significant gap between model and human performance.", "section": "4 Experimental Results"}, {"figure_path": "2410.14669/tables/table_19_0.html", "caption": "Table 5: Performance on NaturalBench-Chinese and NaturalBench-Hindi. We report G-Acc for each dataset, evaluating only models with claimed multilingual capabilities. For both datasets, we also provide G-Acc after translating the original Chinese or Hindi questions into English. This simple translation often boosts performance, except for top models like InternVL-Chat-V1.2-Plus and GPT-40, which seem extensively trained in Chinese. NaturalBench-Hindi remains particularly challenging for open-source models.", "description": "Table 5 presents the performance of various vision-language models on the NaturalBench-Chinese and NaturalBench-Hindi subsets, evaluating their performance with and without English translation of the questions and answers.", "section": "Expanding NaturalBench"}, {"figure_path": "2410.14669/tables/table_19_1.html", "caption": "Table 6: Ablation on different collection methods. We report G-Acc on datasets generated by different collection methods from Flickr30K. Our adversarial procedure results in a much more challenging dataset. Note that Flickr-Adversarial is the combination of Flickr-YN and Flickr-MCQ.", "description": "This table shows the Group Accuracy (G-Acc) performance of various vision-language models on different subsets of the Flickr30K dataset, comparing the results obtained using the adversarial method proposed in the paper versus a random sampling method, highlighting the effectiveness of the adversarial approach in creating a more challenging benchmark.", "section": "4 Experimental Results"}, {"figure_path": "2410.14669/tables/table_20_0.html", "caption": "Table 1: Performance on NaturalBench. We report the performance of 53 leading VLMs on NaturalBench. All models significantly lag behind human performance, with the performance gap (in G-Acc) between humans and models highlighted in red. The latest models, such as BLIP-3 (XGen-MM), Cambrian-1, LLaVA-OneVision, Llama3.2-Vision, Molmo, and Qwen2-VL lag significantly behind humans by 55% to 70%. Even the best closed-source GPT-4o is still 52% behind humans.", "description": "Table 1 presents the group accuracy (G-Acc) performance of 53 vision-language models on the NaturalBench benchmark, highlighting the significant performance gap between these models and human performance.", "section": "Experimental Results"}, {"figure_path": "2410.14669/tables/table_22_0.html", "caption": "Table 1: Performance on NaturalBench. We report the performance of 53 leading VLMs on NaturalBench. All models significantly lag behind human performance, with the performance gap (in G-Acc) between humans and models highlighted in red. The latest models, such as BLIP-3 (XGen-MM), Cambrian-1, LLaVA-OneVision, Llama3.2-Vision, Molmo, and Qwen2-VL lag significantly behind humans by 55% to 70%. Even the best closed-source GPT-4o is still 52% behind humans.", "description": "Table 1 presents the performance of 53 vision-language models on the NaturalBench benchmark, highlighting the significant gap between model and human performance.", "section": "Experimental Results"}, {"figure_path": "2410.14669/tables/table_23_1.html", "caption": "Table 10: Model performance on Relation and Reasoning. We report Q-Acc on each tag.", "description": "Table 10 presents the model's question accuracy (Q-Acc) on different relation and reasoning skills within the NaturalBench benchmark.", "section": "C Skill Analysis"}]