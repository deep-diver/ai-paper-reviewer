[{"figure_path": "2410.14669/figures/figures_2_0.png", "caption": "Figure 1: NaturalBench examples consist of two questions and two images with alternating answers to prevent \"blind\" models from scoring well (e.g., those that predict the same answer regardless of the image or question, as discussed in Section 3). We compare the ground-truth answer for each (image, question) pair with predictions from leading VLMs including GPT-40 (gpt-40-2024-08-06), Qwen2-VL (72B), Llama3.2-Vision (90B), and Molmo (72B) (see Section 4). Even the best models like GPT-40 lags far behind human performance (which is above 90%). Figure 2 shows the pipeline for collecting these natural adversarial examples.", "description": "Figure 1 shows examples from the NaturalBench dataset, illustrating how even state-of-the-art vision-language models struggle with simple questions about natural images, highlighting the challenge posed by natural adversarial samples.", "section": "Collecting natural adversarial samples"}, {"figure_path": "2410.14669/figures/figures_3_0.png", "caption": "Figure 2: Collecting NaturalBench. We use a semi-automated procedure to collect NaturalBench from natural image-text corpora like Flickr30K [63]. First, we identify confounding pairs of image-text samples that fail discriminative VLMs like CLIP [65] and BLIP-2 [39], e.g., they wrongly match an image with another image's caption. Next, we prompt ChatGPT to design questions that yield different answers for each image, providing the original captions in the prompt. Section 3 details this procedure. We hire human annotators to filter out incorrect VQA samples, such as \u201cIs the motorcyclist wearing a red and white uniform?", "description": "Figure 2 illustrates a semi-automated pipeline for collecting NaturalBench, using off-the-shelf models to identify confounding image-text pairs and ChatGPT to generate corresponding questions.", "section": "Collecting natural adversarial samples"}, {"figure_path": "2410.14669/figures/figures_5_0.png", "caption": "Figure 1: NaturalBench examples consist of two questions and two images with alternating answers to prevent \"blind\" models from scoring well (e.g., those that predict the same answer regardless of the image or question, as discussed in Section 3). We compare the ground-truth answer for each (image, question) pair with predictions from leading VLMs including GPT-40 (gpt-40-2024-08-06), Qwen2-VL (72B), Llama3.2-Vision (90B), and Molmo (72B) (see Section 4). Even the best models like GPT-40 lags far behind human performance (which is above 90%). Figure 2 shows the pipeline for collecting these natural adversarial examples.", "description": "The figure displays example questions from the NaturalBench dataset, highlighting how even state-of-the-art vision-language models struggle with questions that humans find easy to answer, demonstrating the challenges posed by natural adversarial examples.", "section": "1 Introduction"}, {"figure_path": "2410.14669/figures/figures_9_0.png", "caption": "Figure 1: NaturalBench examples consist of two questions and two images with alternating answers to prevent \"blind\" models from scoring well (e.g., those that predict the same answer regardless of the image or question, as discussed in Section 3). We compare the ground-truth answer for each (image, question) pair with predictions from leading VLMs including GPT-40 (gpt-40-2024-08-06), Qwen2-VL (72B), Llama3.2-Vision (90B), and Molmo (72B) (see Section 4). Even the best models like GPT-40 lags far behind human performance (which is above 90%). Figure 2 shows the pipeline for collecting these natural adversarial examples.", "description": "Figure 1 shows examples from the NaturalBench dataset, demonstrating how even state-of-the-art vision-language models struggle with simple questions about natural images, highlighting the challenge of \"blind\" solutions that don't utilize visual information.", "section": "Collecting natural adversarial samples"}, {"figure_path": "2410.14669/figures/figures_17_0.png", "caption": "Figure 1: NaturalBench examples consist of two questions and two images with alternating answers to prevent \"blind\" models from scoring well (e.g., those that predict the same answer regardless of the image or question, as discussed in Section 3). We compare the ground-truth answer for each (image, question) pair with predictions from leading VLMs including GPT-40 (gpt-40-2024-08-06), Qwen2-VL (72B), Llama3.2-Vision (90B), and Molmo (72B) (see Section 4). Even the best models like GPT-40 lags far behind human performance (which is above 90%). Figure 2 shows the pipeline for collecting these natural adversarial examples.", "description": "Figure 1 showcases examples from NaturalBench, a new benchmark, comparing human performance with several state-of-the-art vision-language models' performance on natural adversarial samples.", "section": "1 Introduction"}, {"figure_path": "2410.14669/figures/figures_21_0.png", "caption": "Figure 1: NaturalBench examples consist of two questions and two images with alternating answers to prevent \"blind\" models from scoring well (e.g., those that predict the same answer regardless of the image or question, as discussed in Section 3). We compare the ground-truth answer for each (image, question) pair with predictions from leading VLMs including GPT-40 (gpt-40-2024-08-06), Qwen2-VL (72B), Llama3.2-Vision (90B), and Molmo (72B) (see Section 4). Even the best models like GPT-40 lags far behind human performance (which is above 90%). Figure 2 shows the pipeline for collecting these natural adversarial examples.", "description": "Figure 1 shows examples from NaturalBench, a new benchmark for evaluating vision-language models, comparing human answers with the predictions of several state-of-the-art models on pairs of questions and images with alternating answers.", "section": "Collecting natural adversarial samples"}]