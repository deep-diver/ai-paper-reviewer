[{"figure_path": "2410.14669/figures/figures_2_0.png", "caption": "Figure 1: NaturalBench examples consist of two questions and two images with alternating answers to prevent \"blind\" models from scoring well (e.g., those that predict the same answer regardless of the image or question, as discussed in Section 3). We compare the ground-truth answer for each (image, question) pair with predictions from leading VLMs including GPT-40 (gpt-40-2024-08-06), Qwen2-VL (72B), Llama3.2-Vision (90B), and Molmo (72B) (see Section 4). Even the best models like GPT-40 lags far behind human performance (which is above 90%). Figure 2 shows the pipeline for collecting these natural adversarial examples.", "description": "Figure 1 shows examples from NaturalBench, a new benchmark for evaluating vision-language models, highlighting the difficulty of the task even for state-of-the-art models compared to human performance.", "section": "Collecting natural adversarial samples"}, {"figure_path": "2410.14669/figures/figures_3_0.png", "caption": "Figure 1: NaturalBench examples consist of two questions and two images with alternating answers to prevent \"blind\" models from scoring well (e.g., those that predict the same answer regardless of the image or question, as discussed in Section 3). We compare the ground-truth answer for each (image, question) pair with predictions from leading VLMs including GPT-40 (gpt-40-2024-08-06), Qwen2-VL (72B), Llama3.2-Vision (90B), and Molmo (72B) (see Section 4). Even the best models like GPT-40 lags far behind human performance (which is above 90%). Figure 2 shows the pipeline for collecting these natural adversarial examples.", "description": "Figure 1 shows examples from NaturalBench, demonstrating how even state-of-the-art vision-language models struggle with simple questions about natural images, highlighting the challenge posed by the benchmark.", "section": "1 Introduction"}, {"figure_path": "2410.14669/figures/figures_5_0.png", "caption": "Figure 1: NaturalBench examples consist of two questions and two images with alternating answers to prevent \"blind\" models from scoring well (e.g., those that predict the same answer regardless of the image or question, as discussed in Section 3). We compare the ground-truth answer for each (image, question) pair with predictions from leading VLMs including GPT-40 (gpt-40-2024-08-06), Qwen2-VL (72B), Llama3.2-Vision (90B), and Molmo (72B) (see Section 4). Even the best models like GPT-40 lags far behind human performance (which is above 90%). Figure 2 shows the pipeline for collecting these natural adversarial examples.", "description": "Figure 1 shows examples from the NaturalBench dataset, demonstrating how state-of-the-art vision-language models struggle with simple questions about natural images, even when humans find them easy to answer.", "section": "Collecting natural adversarial samples"}, {"figure_path": "2410.14669/figures/figures_9_0.png", "caption": "Figure 1: NaturalBench examples consist of two questions and two images with alternating answers to prevent \"blind\" models from scoring well (e.g., those that predict the same answer regardless of the image or question, as discussed in Section 3). We compare the ground-truth answer for each (image, question) pair with predictions from leading VLMs including GPT-40 (gpt-40-2024-08-06), Qwen2-VL (72B), Llama3.2-Vision (90B), and Molmo (72B) (see Section 4). Even the best models like GPT-40 lags far behind human performance (which is above 90%). Figure 2 shows the pipeline for collecting these natural adversarial examples.", "description": "Figure 1 shows examples from the NaturalBench dataset, demonstrating how even state-of-the-art vision-language models struggle with simple questions about natural images, highlighting the need for a more robust benchmark.", "section": "Collecting natural adversarial samples"}, {"figure_path": "2410.14669/figures/figures_17_0.png", "caption": "Figure 1: NaturalBench examples consist of two questions and two images with alternating answers to prevent \"blind\" models from scoring well (e.g., those that predict the same answer regardless of the image or question, as discussed in Section 3). We compare the ground-truth answer for each (image, question) pair with predictions from leading VLMs including GPT-40 (gpt-40-2024-08-06), Qwen2-VL (72B), Llama3.2-Vision (90B), and Molmo (72B) (see Section 4). Even the best models like GPT-40 lags far behind human performance (which is above 90%). Figure 2 shows the pipeline for collecting these natural adversarial examples.", "description": "Figure 1 shows examples from the NaturalBench benchmark, demonstrating how state-of-the-art vision-language models struggle with simple questions about natural images, even when humans find them easy to answer.", "section": "Collecting natural adversarial samples"}, {"figure_path": "2410.14669/figures/figures_21_0.png", "caption": "Figure 1: NaturalBench examples consist of two questions and two images with alternating answers to prevent \"blind\" models from scoring well (e.g., those that predict the same answer regardless of the image or question, as discussed in Section 3). We compare the ground-truth answer for each (image, question) pair with predictions from leading VLMs including GPT-40 (gpt-40-2024-08-06), Qwen2-VL (72B), Llama3.2-Vision (90B), and Molmo (72B) (see Section 4). Even the best models like GPT-40 lags far behind human performance (which is above 90%). Figure 2 shows the pipeline for collecting these natural adversarial examples.", "description": "Figure 1 shows examples from the NaturalBench dataset, illustrating how even state-of-the-art vision-language models struggle with simple questions about natural images, while humans easily answer them.", "section": "Collecting natural adversarial samples"}]