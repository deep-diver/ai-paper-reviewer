[{"figure_path": "2410.14669/charts/charts_6_0.png", "caption": "Figure 4: Performance of GPT-3.5 vs. LLaVA-1.5 on previous VQA benchmarks. We split each benchmark into equal-sized training and test sets, and report zero-shot (in blue) and finetuned (in green) results. Previous benchmarks show strong language biases, allowing blind GPT-3.5 to exploit spurious answer patterns (see Section 4) by finetuning on QA data without images. As a result, blind GPT-3.5 greatly surpasses random chance (see the red dotted line) and sometimes even matches the performance of LLaVA-1.5-7B finetuned using images. In contrast, Figure 5 shows that NaturalBench can effectively prevent blind solutions from exceeding chance.", "description": "The chart compares the performance of GPT-3.5 and LLaVA-1.5 on several existing VQA benchmarks, highlighting the vulnerability of these benchmarks to \"blind\" solutions that exploit language biases.", "section": "4 Experimental Results"}, {"figure_path": "2410.14669/charts/charts_7_0.png", "caption": "Figure 5: Performance of GPT-3.5, LLaVA-1.5, and GPT-40 on NaturalBench. We also split NaturalBench (the English subset) into equal-sized training and test sets, and report zero-shot (in blue) and finetuned (in green) results. We report group accuracy (G-Acc) (introduced in Section 4), which awards a point when all four (image, question) pairs are answered correctly. We highlight key results: (1) Blind GPT-3.5 fails to surpass random chance performance (red dotted line), regardless of finetuning. (2) LLaVA-1.5 improves by 9% by finetuning on NaturalBench's training images. (3) Even GPT-4o gains 10% G-Acc through vision finetuning on NaturalBench; however, it falls far behind human performance (purple dotted line). These findings confirm that NaturalBench is a more vision-centric benchmark, and a potentially useful dataset for improving already advanced VLMs.", "description": "The chart compares the performance of three vision-language models (GPT-3.5, LLaVA-1.5, and GPT-4-0) on the NaturalBench benchmark, showing that even the best model significantly lags behind human performance.", "section": "Experimental Results"}]