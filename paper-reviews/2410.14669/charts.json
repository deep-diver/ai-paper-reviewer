[{"figure_path": "2410.14669/charts/charts_6_0.png", "caption": "Figure 4: Performance of GPT-3.5 vs. LLaVA-1.5 on previous VQA benchmarks. We split each benchmark into equal-sized training and test sets, and report zero-shot (in blue) and finetuned (in green) results. Previous benchmarks show strong language biases, allowing blind GPT-3.5 to exploit spurious answer patterns (see Section 4) by finetuning on QA data without images. As a result, blind GPT-3.5 greatly surpasses random chance (see the red dotted line) and sometimes even matches the performance of LLaVA-1.5-7B finetuned using images. In contrast, Figure 5 shows that NaturalBench can effectively prevent blind solutions from exceeding chance.", "description": "The chart compares the performance of GPT-3.5 and LLaVA-1.5 models on several existing VQA benchmarks, highlighting the susceptibility of these benchmarks to language biases that allow blind models to achieve high accuracy without image understanding.", "section": "4 Experimental Results"}, {"figure_path": "2410.14669/charts/charts_7_0.png", "caption": "Figure 5: Performance of GPT-3.5, LLaVA-1.5, and GPT-40 on NaturalBench. We also split NaturalBench (the English subset) into equal-sized training and test sets, and report zero-shot (in blue) and finetuned (in green) results. We report group accuracy (G-Acc) (introduced in Section 4), which awards a point when all four (image, question) pairs are answered correctly. We highlight key results: (1) Blind GPT-3.5 fails to surpass random chance performance (red dotted line), regardless of finetuning. (2) LLaVA-1.5 improves by 9% by finetuning on NaturalBench's training images. (3) Even GPT-4o gains 10% G-Acc through vision finetuning on NaturalBench; however, it falls far behind human performance (purple dotted line). These findings confirm that NaturalBench is a more vision-centric benchmark, and a potentially useful dataset for improving already advanced VLMs.", "description": "Figure 5 shows the group accuracy (G-Acc) of three models (GPT-3.5, LLaVA-1.5, and GPT-4-0) on NaturalBench, highlighting that only GPT-4-0 shows improvement after fine-tuning, but still lags behind human performance.", "section": "Experimental Results"}]