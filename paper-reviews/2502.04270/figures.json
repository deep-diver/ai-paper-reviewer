[{"figure_path": "https://arxiv.org/html/2502.04270/x1.png", "caption": "Figure 1: Overview of our approach. (a) We consider a full RLHF training setup, where a language model (LM) policy is iteratively refined through active data collection. Our goal is to develop an optimal response sampling method for preference labeling. (b) We introduce PILAF, which generates responses by interpolating between the current policy and a reference policy, balancing exploration and exploitation. (c) Our theoretical analysis shows that T-PILAF aligns the parameter gradient with the steepest direction for maximizing human values and achieves more favorable convergence in regions of high sensitivity.", "description": "This figure provides a high-level overview of the PILAF approach. Panel (a) illustrates a standard RLHF training loop, where the language model's policy is iteratively improved by collecting human preference data via pairwise comparisons.  The goal of the paper is to improve the sampling method used to obtain these preference comparisons. Panel (b) introduces the PILAF method, showing how it generates response pairs for comparison by interpolating between a reference policy (a pre-trained or well-understood model) and the current policy. This interpolation balances exploration (trying diverse responses) and exploitation (focusing on responses likely to be highly preferred). Panel (c) summarizes the theoretical analysis showing that PILAF's sampling strategy makes the collected data more informative and leads to faster convergence towards aligning the language model with human preferences.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.04270/x2.png", "caption": "Figure 2: Reward-KL curve for Iterative DPO. All training runs start from the same model obtained at the end of the first iteration via Vanilla Sampling. Each dot represents an evaluation performed every 50 training steps.", "description": "This figure shows the reward and KL divergence during iterative Direct Preference Optimization (DPO).  All training runs begin from the same initial model, trained using vanilla sampling in the first iteration. Each point on the graph represents a model's performance after 50 training steps. The x-axis represents the KL divergence from a reference model, which measures how different the current policy is from a baseline. The y-axis represents the reward achieved by the model on a test set. The plot illustrates the learning trajectories of different sampling methods, including PILAF and various baseline methods, showcasing PILAF's superior performance in reaching higher reward values while maintaining a relatively low KL divergence.", "section": "6.1 Iterative DPO"}, {"figure_path": "https://arxiv.org/html/2502.04270/x3.png", "caption": "Figure 3: Reward-KL curve for Online DPO. Each dot represents an evaluation performed every 50 training steps.", "description": "This figure shows the reward and KL divergence (relative to the reference model) during online DPO training.  Each point represents the model's performance after 50 gradient steps.  The plot illustrates how the reward and KL change as the policy is optimized, showcasing the learning progress and the model's closeness to the reference model. Different sampling methods can lead to different reward-KL tradeoffs.", "section": "6.2 Online DPO"}, {"figure_path": "https://arxiv.org/html/2502.04270/x4.png", "caption": "Figure 4: Online DPO with an overfitted initial policy. Each dot represents an evaluation performed every 50 training steps. Color saturation indicates the training step, with darker colors representing later steps.", "description": "This figure displays the results of an online direct preference optimization (DPO) experiment where the initial policy model is overfitted.  The x-axis shows the KL divergence between the current policy and a reference policy, illustrating the difference between the two policies. The y-axis represents the average reward obtained by the policy. Each point on the graph corresponds to an evaluation of the policy after 50 gradient steps of training. The color intensity of the points gradually darkens to visualize the training progression, with darker shades representing later steps in the training process.  This experiment aims to show the robustness of the PILAF method against an initially overfitted model, comparing its performance to standard Vanilla sampling.", "section": "6.2 Online DPO"}, {"figure_path": "https://arxiv.org/html/2502.04270/x5.png", "caption": "Figure 5: Online DPO with an overfitted initial policy. Full results of the Figure\u00a04. Each dot represents an evaluation performed every 50 training steps. Color saturation indicates the training step, with darker colors representing later steps.", "description": "This figure displays the results of an online direct preference optimization (DPO) experiment where the initial policy model is overfitted.  The x-axis represents the KL divergence between the learned policy and a reference policy, while the y-axis shows the reward achieved by the learned policy.  Each point in the plot corresponds to an evaluation performed after every 50 training steps.  The color intensity of the points increases with the training step, providing a visual representation of the model's learning progression over time. This visualization allows comparison of the performance of different sampling methods when starting from a suboptimal initial policy.", "section": "6.2 Online DPO"}]