{"references": [{"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-31", "reason": "This paper introduces Reinforcement Learning from Human Feedback (RLHF), a core technique in aligning LLMs with human values, which is the central focus of the current research."}, {"fullname_first_author": "Achiam, J.", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This report provides a comprehensive technical overview of GPT-4, a prominent LLM that exemplifies the applications and challenges of aligning LLMs with human values."}, {"fullname_first_author": "Rafailov, R.", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-12-31", "reason": "This paper introduces Direct Preference Optimization (DPO), an efficient method for aligning LLMs, which is directly compared to the proposed approach in the study."}, {"fullname_first_author": "Xiong, W.", "paper_title": "Iterative preference learning from human feedback: Bridging theory and practice for RLHF under KL-constraint", "publication_date": "2024-12-31", "reason": "This paper presents iterative DPO, a method that addresses the challenge of distributional shift during RLHF and is used as a baseline method for comparison."}, {"fullname_first_author": "Guo, S.", "paper_title": "Direct language model alignment from online feedback", "publication_date": "2024-02-04", "reason": "This paper introduces online DPO, which addresses the challenge of iterative data curation during RLHF and serves as a key baseline for the proposed online setting."}]}