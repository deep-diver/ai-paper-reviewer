[{"figure_path": "https://arxiv.org/html/2412.07724/extracted/6059815/figures/ROC_Granite-Guardian-3.0-2B.png", "caption": "Figure 1: Annotation guidelines", "description": "This figure details the guidelines used by annotators to label text samples as either 'safe' or 'unsafe'. It outlines criteria for evaluating both prompts and responses based on categories such as bias, jailbreaking, violence, profanity, sexual content, unethical behavior, AI refusal, and other miscellaneous unsafe content.  Specific examples are provided within each category to clarify the guidelines.", "section": "3 Datasets"}, {"figure_path": "https://arxiv.org/html/2412.07724/extracted/6059815/figures/ROC_Granite-Guardian-3.0-8B.png", "caption": "Figure 2: Prompt for benign prompt generation", "description": "This figure shows the prompt used to generate benign prompts for the synthetic data used to train the Granite Guardian model.  The prompt instructs a language model to create a list of requests that might initially seem harmful but are actually harmless.  The goal is to help the language model learn to distinguish between genuinely harmful requests and those that are benign but might appear otherwise.  It specifies the number of requests needed, a description of the type of safety, and examples to help guide the generation.  The purpose is to provide contrastive examples to those that are harmful, aiding in the training of a more robust and less prone to false-positive model.", "section": "3.2 Synthetic data"}, {"figure_path": "https://arxiv.org/html/2412.07724/extracted/6059815/figures/BarChart_f1_Score_Macro.png", "caption": "Figure 3: Prompt for typical harmful prompt generation", "description": "This figure shows the prompt used to generate synthetic data for typically harmful prompts. The prompt instructs an LLM to generate diverse requests that a language model should refuse, even if they might initially seem harmless.  The prompt requests that each request be on a new line and prefixed with \u201cRequest:\u201d.  Specific parameters such as subcategory, fine-grained subcategory and description of the request can be filled to obtain different types of harmful requests. This methodology allows for the generation of a large dataset of harmful prompts that can be used to train a safety model.", "section": "3.2 Synthetic data"}, {"figure_path": "https://arxiv.org/html/2412.07724/extracted/6059815/figures/BarChart_f1_Score.png", "caption": "Figure 4: Prompt for adversarial harmful prompt generation", "description": "This figure shows a prompt designed to generate adversarial harmful prompts.  The prompt instructs an AI to act as a red-teaming assistant, tasked with revising simple harmful prompts into more sophisticated and harder-to-detect adversarial versions. This involves using multiple revision strategies to make the harmful intent more subtle and less likely to be flagged by safety mechanisms. The prompt provides a simple harmful request as input, along with a set of revision strategies, each defined with its description and examples. The AI is expected to produce a revised adversarial prompt that is logically coherent, maintains the same harmful intent, but is harder to detect using common safety measures.", "section": "3.2 Synthetic data"}, {"figure_path": "https://arxiv.org/html/2412.07724/extracted/6059815/figures/BarChart_recall.png", "caption": "Figure 5: Example revision strategy for adversarial prompt transformation", "description": "This figure presents an example of a revision strategy used to transform a simple harmful prompt into a more sophisticated adversarial prompt.  The goal is to create prompts that can bypass the safety mechanisms of language models and elicit harmful responses.  The example shows how a malicious prompt can be broken down into smaller, seemingly innocuous parts (payloads) that would not individually trigger a safety alert but can be reassembled by a language model to produce harmful content. This technique highlights the importance of considering more complex, adversarial prompt construction during safety evaluations of language models. The figure is crucial for illustrating the types of adversarial techniques that language models are vulnerable to, and how these need to be accounted for in safe deployment strategies.", "section": "3.2 Synthetic data"}, {"figure_path": "https://arxiv.org/html/2412.07724/extracted/6059815/figures/BarChart_fpr.png", "caption": "Figure 6: Prompt for RAG synthetic data generation", "description": "This figure shows a prompt designed for generating synthetic data to evaluate RAG (Retrieval Augmented Generation) models' performance on hallucination-related risks.  The prompt instructs the model to generate several different types of answers to a given question and short answer, each designed to test a specific aspect of RAG quality.  These include correct answers, answers that are irrelevant to the question, incorrect answers taken from the document, and questions that themselves are not relevant to the provided context. This process helps create a comprehensive and diverse dataset for training and evaluating RAG models' ability to produce accurate and contextually relevant responses.", "section": "3 Datasets"}]