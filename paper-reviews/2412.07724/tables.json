[{"content": "| Risk | Prompt | Response |\n|---|---|---|\n| `harm` | \u2713 | \u2713 |\n| `social-bias` | \u2713 | \u2713 |\n| `profanity` | \u2713 | \u2713 |\n| `sexual content` | \u2713 | \u2713 |\n| `unethical behavior` | \u2713 | \u2713 |\n| `violence` | \u2713 | \u2713 |\n| `jailbreaking` | \u2713 |  |\n| `context relevance` (RAG) | \u2713 |  |\n| `groundedness` (RAG) |  | \u2713 |\n| `answer relevance` (RAG) |  | \u2713 |", "caption": "Table 1: Risks detected by Granite Guardian and the corresponding content sections (Prompt or Response) where they may occur.", "description": "This table lists the various risks that the Granite Guardian model is designed to detect.  For each risk, it indicates whether the risk is typically found in the prompt (user input) or the response (model output), or both. This helps clarify where in the LLM interaction the model should focus its attention for risk detection.", "section": "2.1 Types of risks addressed"}, {"content": "| Birth Year | Age | Gender | Education Level | Ethnicity | Region |\n|---|---|---|---|---|---| \n| - | - | Male | Bachelor | African American | Florida |\n| 1989 | 35 | Male | Bachelor | White | Nevada |\n| - | - | Female | Associate\u2019s Degree | African American | Pennsylvania |\n| 1992 | 32 | Male | Bachelor | African American | Florida |\n| 1978 | 46 | Male | Bachelor | White | Colorado |\n| 1999 | 25 | Male | High School Diploma | LATAM or Hispanic | Florida |\n| - | - | Male | Bachelor | White | Texas |\n| 1988 | 36 | Female | Bachelor | White | Florida |\n| 1985 | 39 | Female | Bachelor | Native American | Colorado / Utah |\n| - | - | Female | Bachelor | White | Arkansas |\n| - | - | Female | Master of Science | White | Texas |\n| 2000 | 24 | Female | Bachelor | White | Florida |\n| 1987 | 37 | Male | Associate\u2019s Degree | White | Florida |\n| 1995 | 29 | Female | Master of Epidemiology | African American | Louisiana |\n| 1993 | 31 | Female | Master of Public Health | LATAM or Hispanic | Texas |\n| 1969 | 55 | Female | Bachelor | LATAM or Hispanic | Florida |\n| 1993 | 31 | Female | Bachelor | White | Florida |\n| 1985 | 39 | Female | Master of Music | White | California |", "caption": "Table 2: Annotator Demographics", "description": "This table presents the demographic information of the annotators who participated in the data labeling process for the Granite Guardian project.  It shows the annotators' birth year, age, gender, education level, ethnicity, and region. This data provides valuable context on the diversity of the individuals involved in creating the dataset and can help understand any potential biases in the dataset.", "section": "3 Datasets"}, {"content": "| Category | Prompt | Response |\n|---|---|---|\n| Bias | 0.873 | 0.870 |\n| Jailbreaking | 0.725 | 0.670 |\n| Violence | 0.863 | 0.863 |\n| Profanity | 0.817 | 0.842 |\n| Sexual Content | 0.890 | 0.822 |\n| Unethical Behavior | 0.894 | 0.883 |\n| AI Refusal | - | 0.689 |\n| Other | 0.892 | 0.811 |", "caption": "Table 3: Inter-annotator agreement for prompt/response labels", "description": "This table displays the level of agreement between multiple human annotators who independently labeled prompts and responses for various risk categories.  It quantifies the inter-annotator reliability for each risk category (Bias, Jailbreaking, Violence, Profanity, Sexual Content, Unethical Behavior, AI Refusal, and Other) by providing the inter-annotator agreement score (presumably Cohen's Kappa or a similar metric) separately for prompts and responses.  Higher scores indicate stronger agreement among annotators, suggesting higher quality and consistency in the labeling process.", "section": "3.1 Human annotations"}, {"content": "| Dataset | [Ref.] | # sample | Benign | Harmful | type |\n|---|---|---|---|---|---| \n| AegisSafetyTest | Ghosh et al. (2024) | 359 | 126 | 233 | prompt |\n| HarmBench Prompt | Mazeika et al. (2024) | 239 | \u2717 | 239 | prompt |\n| ToxicChat | Lin et al. (2023) | 2,853 | 2,491 | 362 | prompt |\n| OpenAI Mod. | Markov et al. (2023) | 1,680 | 1,158 | 522 | prompt |\n| SimpleSafetyTests | Vidgen et al. (2023) | 100 | \u2717 | 100 | prompt |\n| BeaverTails | Ji et al. (2023) | 3,021* | 1,288 | 1,733 | response |\n| SafeRLHF | Dai et al. (2024) | 2,000* | 1,000 | 1,000 | response |\n| XSTEST-RH | Han et al. (2024) | 446 | 368 | 78 | response |\n| XSTEST-RR | Han et al. (2024) | 449 | 178\u2020 | 271\u2021 | response |\n| XSTEST-RR(h) | Han et al. (2024) | 200 | 97\u2020 | 103\u2021 | response |", "caption": "Table 4: Details of the public benchmarks used for evaluation. \u2217 indicates sub-sampling from the original set, \u2020refers to refusal responses flagged as benign, and \u2021refers to compliance responses flagged as harmful.", "description": "This table presents the details of eight public benchmark datasets used to evaluate the performance of the Granite Guardian model in detecting harmful content in both prompts and responses.  It lists each dataset's name, the source reference, the total number of samples, the number of samples classified as benign, the number of samples classified as harmful, and the type of data (prompt or response). Note that some datasets have undergone sub-sampling (*), refusal responses flagged as benign (\u2020), and compliance responses flagged as harmful(\u2021). This information is crucial in understanding the evaluation methodology and the nature of the data used to assess the model's performance.", "section": "5.3 Benchmarks"}, {"content": "| Dataset | [Ref.] | # sample | # Consistent | # Inconsistent | Task type |\n|---|---|---|---|---|---| \n| FRANK | Pagnoni et al. (2021) | 671 | 223 | 448 | *Summarization* |\n| SummEval Prompt | Fabbri et al. (2021) | 1,600 | 1,306 | 294 | *Summarization* |\n| MNBM | Maynez et al. (2020) | 2,500 | 255 | 2,245 | *Summarization* |\n| QAGS-CNN/DM | Wang et al. (2020) | 235 | 113 | 122 | *Summarization* |\n| QAGS-XSUM | Wang et al. (2020) | 239 | 116 | 123 | *Summarization* |\n| BEGIN | Dziri et al. (2021) | 836 | 282 | 554 | *Dialogue* |\n| Q<sup>2</sup> | Honovich et al. (2021) | 1,088 | 623 | 460 | *Dialogue* |\n| DialFact | Gupta et al. (2021) | 8,689 | 3,345 | 5,344 | *Dialogue* |\n| PAWS | Zhang et al. (2019) | 8,000 | 3,536 | 4,464 | *Paraphrasing* |", "caption": "Table 5: Details of the TRUE benchmarks used for RAG evaluation.", "description": "This table lists the datasets used to evaluate the performance of RAG (Retrieval Augmented Generation) models on the task of groundedness. It details the name of each dataset, the reference where it is described, the total number of samples, and the breakdown of those samples into consistent and inconsistent examples.  Each dataset focuses on different NLP tasks like summarization, dialogue, and paraphrasing, providing a comprehensive evaluation of RAG across various scenarios.", "section": "3.3 RAG hallucination risk data"}, {"content": "| model | AUC | AUPRC | F1 | Recall | Precision |\n|---|---|---|---|---|---| \n| Llama-Guard-7B | 0.824 | 0.803 | 0.659 | 0.533 | 0.861 |\n| Llama-Guard-2-8B | 0.841 | 0.822 | 0.723 | 0.627 | 0.852 |\n| Llama-Guard-3-1B | 0.796 | 0.775 | 0.656 | 0.575 | 0.765 |\n| Llama-Guard-3-8B | 0.826 | 0.819 | 0.710 | 0.607 | 0.857 |\n| ShieldGemma-2B | 0.748 | 0.704 | 0.421 | 0.277 | 0.883 |\n| ShieldGemma-9B | 0.753 | 0.707 | 0.404 | 0.262 | 0.886 |\n| ShieldGemma-27B | 0.772 | 0.718 | 0.438 | 0.295 | 0.849 |\n| Granite-Guardian-3.0-2B | 0.782 | 0.746 | 0.674 | 0.747 | 0.614 |\n| Granite-Guardian-3.0-8B | 0.871 | 0.846 | 0.758 | 0.735 | 0.781 |", "caption": "Table 6: Results on aggregated datasets for harmful content detection comparing Granite Guardian (using the umbrella harm risk definition) with Llama Guard and ShieldGemma model families. Baselines are suitably adapted for direct comparison (see section 6.1 for details). Numbers in bold represent the best performance within a column, while underlined numbers indicate the second-best.", "description": "This table presents a comparison of the performance of Granite Guardian's harm risk detection model against Llama Guard and ShieldGemma models.  The comparison uses aggregated datasets for harmful content detection, focusing on the umbrella harm risk definition.  The table shows several evaluation metrics (AUC, AUPRC, F1, Recall, and Precision).  The baselines (Llama Guard and ShieldGemma) were adapted to ensure a fair comparison.  The best and second-best performances for each metric are highlighted in bold and underlined, respectively. This allows readers to easily identify the relative strengths of each model in detecting harmful content.", "section": "6.1 Harm risk benchmarks"}, {"content": "| model | AegisSafetyTest | ToxicChat | OpenAI Mod. | BeaverTails | SafeRLHF | XSTEST_RH | XSTEST_RR | XSTEST_RR(h) | F1/AUC |\n|---|---|---|---|---|---|---|---|---|---| \n| Llama-Guard-7B | 0.743/0.852 | 0.596/0.955 | 0.755/0.917 | 0.663/0.787 | 0.607/0.716 | 0.803/0.925 | 0.358/0.589 | 0.704/0.816 | 0.659/0.824 |\n| Llama-Guard-2-8B | 0.718/0.782 | 0.472/0.876 | 0.758/0.903 | 0.718/0.819 | 0.743/0.822 | 0.908/0.994 | 0.428/0.824 | 0.805/0.941 | 0.723/0.841 |\n| Llama-Guard-3-1B | 0.681/0.780 | 0.453/0.810 | 0.686/0.858 | 0.632/0.820 | 0.662/0.790 | 0.846/0.976 | 0.420/0.866 | 0.802/0.959 | 0.656/0.796 |\n| Llama-Guard-3-8B | 0.717/0.816 | 0.542/0.865 | 0.792/0.922 | 0.677/0.831 | 0.705/0.803 | 0.904/0.975 | 0.405/0.558 | 0.798/0.891 | 0.710/0.826 |\n| ShieldGemma-2B | 0.471/0.803 | 0.181/0.811 | 0.245/0.709 | 0.484/0.747 | 0.348/0.657 | 0.792/0.867 | 0.371/0.570 | 0.708/0.735 | 0.421/0.748 |\n| ShieldGemma-9B | 0.458/0.826 | 0.181/0.851 | 0.234/0.721 | 0.459/0.741 | 0.329/0.646 | 0.809/0.880 | 0.356/0.584 | 0.708/0.753 | 0.404/0.753 |\n| ShieldGemma-27B | 0.437/0.860 | 0.177/0.880 | 0.227/0.724 | 0.513/0.757 | 0.386/0.649 | 0.792/0.893 | 0.395/0.546 | 0.744/0.748 | 0.438/0.772 |\n| Granite-Guardian-3.0-2B | 0.842/0.844 | 0.368/0.865 | 0.603/0.836 | 0.757/0.873 | 0.771/0.834 | 0.817/0.974 | 0.382/0.832 | 0.744/0.903 | 0.674/0.782 |\n| Granite-Guardian-3.0-8B | 0.874/0.924 | 0.649/0.940 | 0.745/0.918 | 0.776/0.895 | 0.780/0.846 | 0.849/0.979 | 0.401/0.786 | 0.781/0.919 | 0.758/0.871 |", "caption": "Table 7: F1/AUC results across different datasets, categorised across prompt harmfulness and response harmfulness. Baselines are suitably adapted for direct comparison (see section 6.1 for details).\nNumbers in bold represent the best performance within a column, while underlined numbers indicate the second-best.", "description": "This table presents the performance of various models, including Granite Guardian, in detecting harmful content in both prompts and responses.  The models are evaluated across multiple datasets, categorized by whether the harm is present in the prompt or the response.  The F1 score and Area Under the ROC Curve (AUC) are reported as key performance metrics. The baselines are modified to facilitate fair comparisons with Granite Guardian.  Numbers in bold represent the best performance in each column, and underlined numbers indicate the second-best performance.", "section": "6 Results"}, {"content": "| Model | MNBN | BEGIN | QX | QC | SumE | DialF | PAWS | Q2 | Frank | AVG. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| ANLI-T5-11B | 0.779 | 0.826 | 0.838 | 0.821 | 0.805 | 0.777 | 0.864 | 0.727 | 0.894 | 0.815 |\n| WeCheck-0.4B | **0.830** | **0.864** | 0.814 | 0.826 | 0.798 | 0.900 | **0.896** | 0.840 | 0.881 | 0.850 |\n| Llama-3.1-Bespoke-MiniCheck-7B | 0.817 | 0.806 | **0.907** | 0.882 | **0.851** | 0.931 | 0.870 | 0.870 | **0.924** | **0.873** |\n| Granite-Guardian-3.0-2B | 0.712 | 0.710 | 0.768 | 0.753 | 0.779 | 0.892 | 0.825 | 0.874 | 0.885 | 0.800 |\n| Granite-Guardian-3.0-8B | 0.719 | 0.781 | 0.836 | **0.890** | 0.822 | **0.946** | 0.880 | **0.913** | 0.898 | 0.854 |", "caption": "Table 8: AUC results on the TRUE dataset for groundedness.\nNumbers in bold represent the best performance within a column, while underlined numbers indicate the second-best.", "description": "This table presents the Area Under the Curve (AUC) scores for different models on the TRUE benchmark dataset, specifically focusing on the groundedness aspect of RAG (Retrieval Augmented Generation).  The TRUE dataset encompasses various natural language processing (NLP) tasks designed to assess the factual consistency of generated text. The models' AUC scores are compared across multiple sub-datasets within TRUE, allowing for a comprehensive evaluation of their performance in accurately identifying grounded vs. hallucinated statements.  The bold numbers in each column highlight the top-performing model for that specific task, while underlined numbers denote the second-best performance. This enables easy comparison of the models' strengths and weaknesses across a diverse range of NLP tasks.", "section": "5.3 Benchmarks"}, {"content": "| Risk name | Definition |\n|---|---| \n| `harm` | Content considered universally harmful. This is our general category, which should encompass a variety of risks including those not specifically addressed by the following categories. |\n| `social-bias` | Systemic prejudice against groups based on shared identity or characteristics, often stemming from stereotypes or cultural influences. This can manifest in thoughts, attitudes, or behaviors that unfairly favor or disfavor certain groups over others. |\n| `profanity` | Use of language considered offensive or socially unacceptable in a given context. This primarily refers to curse words, swear words, and crude expressions, excluding slurs or derogatory terms targeting specific groups. |\n| `sexual content` | Material explicitly related to sexual activities, anatomy, or desires. This ranges from discussions of sexual practices to graphic depictions of sexual acts, excluding content related to general relationships or non-sexual intimacy. |\n| `unethical behavior` | Actions that violate moral or professional standards, focusing on exploitation for personal gain or disregard for others\u2019 well-being. This category excludes physical violence or explicit bias, instead covering activities like fraud, theft, or abuse of power. |\n| `violence` | Promoting or describing physical harm to individuals or groups. This includes depictions of assault, self-harm, or creation of a hostile environment through threats or intimidation. |\n| `jailbreaking` | Deliberate circumvention of AI systems\u2019 built-in safeguards or ethical guidelines. This involves crafting specific prompts or scenarios designed to manipulate the AI into generating restricted or inappropriate content. |\n| `context relevance` | This occurs when the retrieved or provided context fails to contain information pertinent to answering the user\u2019s question or addressing their needs. Irrelevant context may be on a different topic, from an unrelated domain, or contain information that doesn\u2019t help in formulating an appropriate response to the user. |\n| `groundedness` | This risk arises in a Retrieval-Augmented Generation (RAG) system when the LLM response includes claims, facts, or details that are not supported by or are directly contradicted by the given context. An ungrounded answer may involve fabricating information, misinterpreting the context, or making unsupported extrapolations beyond what the context actually states. |\n| `answer relevance` | This occurs when the LLM response fails to address or properly respond to the user\u2019s input. This includes providing off-topic information, misinterpreting the query, or omitting crucial details requested by the User. An irrelevant answer may contain factually correct information but still fail to meet the User\u2019s specific needs or answer their intended question. |", "caption": "Table 9: Risk Definitions", "description": "This table provides a comprehensive list of risk categories and their detailed definitions as used in the Granite Guardian model.  It covers a wide range of risks, categorized into general harm, social biases, profanity, sexual content, unethical behavior, violence, jailbreaking, and RAG (Retrieval-Augmented Generation)-specific risks such as context relevance, groundedness, and answer relevance. Each risk category includes a clear and concise definition to facilitate a thorough understanding of the model's capabilities and limitations in risk detection.", "section": "2.1 Types of risks addressed"}, {"content": "| Risk Type | Secondary | Primary |\n|---|---|---|\n| Harm++ (Prompt) | - | `user` |\n| Harm++ (Response) | `user` | `assistant` |\n| Jailbreak (Prompt) | - | `user` |\n| RAG - Context Relevance | `user` | `context` |\n| RAG - Groundedness | `context` | `assistant` |\n| RAG - Answer Relevance | `user` | `assistant` |", "caption": "Table 10: Designated roles in the safety instruction template for different risk categories. Harm++ refers to all harmful content risks (Section\u00a02.1.1). The \u201cPrimary\u201d column indicates the tag that determines the safety agent\u2019s focus, while the \u201cSecondary\u201d column, in conjunction with the \u201cPrimary\u201d tag, specifies the content to be included in the safety instruction template, as detailed in Section\u00a04.1.", "description": "This table details how the safety instruction template is used for different risk categories.  The \"Risk Type\" column lists the various risk types, such as harmful content (Harm++), jailbreaking, and different aspects of Retrieval Augmented Generation (RAG) quality (context relevance, groundedness, and answer relevance). The \"Secondary\" and \"Primary\" columns indicate which parts of the prompt and response text (user or assistant) the safety agent focuses on when assessing the risk. This is crucial to understanding how the model processes the information for different kinds of risks and enables flexibility in adapting the model to various safety concerns.", "section": "4 Model design and development"}, {"content": "| model | AUC | TPr | AUC@0.1 | TPr@0.1 | AUC@0.01 | TPr@0.01 | AUC@0.001 | TPr@0.001 |\n|---|---|---|---|---|---|---|---|---|\n| `Llama-Guard-7B` | 0.824 | 0.533 | 0.454 | 0.617 | 0.148 | 0.224 | 0.037 | 0.068 |\n| `Llama-Guard-2-8B` | 0.841 | 0.627 | 0.506 | 0.660 | 0.137 | 0.239 | 0.014 | 0.032 |\n| `Llama-Guard-3-1B` | 0.796 | 0.575 | 0.414 | 0.546 | 0.152 | 0.247 | 0.030 | 0.054 |\n| `Llama-Guard-3-8B` | 0.826 | 0.607 | 0.521 | 0.648 | 0.174 | 0.320 | 0.016 | 0.033 |\n| `ShieldGemma-2B` | 0.748 | 0.277 | 0.308 | 0.400 | 0.112 | 0.179 | 0.021 | 0.035 |\n| `ShieldGemma-9B` | 0.753 | 0.262 | 0.307 | 0.403 | 0.129 | 0.193 | 0.020 | 0.052 |\n| `ShieldGemma-27B` | 0.772 | 0.295 | 0.305 | 0.399 | 0.133 | 0.191 | 0.016 | 0.049 |\n| `Granite-Guardian-3.0-2B` | 0.782 | 0.747 | 0.355 | 0.504 | 0.102 | 0.185 | 0.012 | 0.021 |\n| `Granite-Guardian-3.0-8B` | 0.871 | 0.735 | 0.515 | 0.676 | 0.170 | 0.290 | 0.041 | 0.072 |", "caption": "Table 11: AUC and TPr results on specific FPr thresholds (i.e., with FPr equal to 0.1, 0.01, 0.001). Numbers in bold represent the best performance within a column, while underlined numbers indicate the second-best.", "description": "This table presents the Area Under the Curve (AUC) and True Positive Rate (TPR) at various False Positive Rate (FPR) thresholds (0.1, 0.01, and 0.001) for several models.  The Area Under the Curve (AUC) provides an overall measure of the model's ability to distinguish between classes.  The True Positive Rate (TPR), at a fixed FPR, measures the model's effectiveness at correctly identifying true positives, while controlling the rate of false positives.  This is crucial for applications with stringent requirements for minimizing false positives.  The models compared include Granite Guardian (two versions: 2B and 8B), Llama Guard models (multiple versions), and ShieldGemma models (multiple versions). The table highlights the best performing model for each metric at each threshold using bold font and the second-best performing model with underlined font.", "section": "5.1 Metrics"}]