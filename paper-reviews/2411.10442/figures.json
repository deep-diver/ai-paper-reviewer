[{"figure_path": "https://arxiv.org/html/2411.10442/x1.png", "caption": "Figure 1: \nOpen-source model performance on MathVista.\nThe X- and Y-axes represent the accuracy evaluated with direct-answer responses and CoT responses, respectively. The bubble size is positively correlated with the number of model parameters.\nThe values in parentheses indicate the performance gap between CoT and direct-answer responses.\nNotably, most open-source models perform worse when answering with CoT.", "description": "This figure displays the performance of various open-source multimodal large language models (MLLMs) on the MathVista benchmark, which assesses multimodal reasoning capabilities.  The x-axis shows accuracy when models provide direct answers, and the y-axis shows accuracy when they use Chain-of-Thought (CoT) reasoning. Each bubble represents a different model, with its size proportional to the number of parameters. The numbers in parentheses next to each model indicate the difference in accuracy between direct answering and CoT reasoning. The plot reveals a concerning trend: most open-source models perform worse with CoT reasoning than with direct answering.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.10442/x2.png", "caption": "Figure 2: \nData examples in MMPR.\nFor instructions with clear ground truths, we propose a correctness-based pipeline, which samples multiple solutions and considers those with correct answers as chosen responses and those with incorrect answers as rejected responses.\nFor instructions without clear ground truths, we propose DropoutNTP to generate rejected responses. Differences between the chosen and rejected responses are emphasized in italicized text.\nRed highlights incorrect responses.", "description": "This figure showcases examples from the MMPR dataset, illustrating the two data generation pipelines used.  The top half shows examples with clear ground truth.  In these cases, the system generates multiple responses to the same prompt.  Responses matching the ground truth are labeled as 'chosen responses,' while those that don't match are 'rejected responses.' The bottom half presents examples where ground truth is unclear.  Here, the 'DropoutNTP' method is used: the model generates a response, truncates it, and then attempts to complete the truncated response without the image context. This generated completion acts as the 'rejected response', while the original, complete response is the 'chosen response'. Differences between chosen and rejected responses are highlighted in italics, and incorrect answers are highlighted in red.", "section": "3. Scalable Multimodal Preference Dataset Generation"}, {"figure_path": "https://arxiv.org/html/2411.10442/x3.png", "caption": "Figure 3: \nResults of models trained with different preference optimization algorithms on M3CoT.\nThe algorithm X extended with the SFT loss is called X+ for brevity. For instance, DPO+ denotes the combination of DPO loss and SFT loss.", "description": "This figure displays the performance comparison of various preference optimization algorithms applied to the M3CoT benchmark.  The algorithms tested include Direct Preference Optimization (DPO), its variants (RSO, IPO, CDPO, RobustDPO, BCO, SPPO, AOT, TR-DPO), and a version of each extended by incorporating the Supervised Fine-Tuning (SFT) loss (indicated by '+'). The graph compares the accuracy results for each method when using direct answers and when using Chain-of-Thought (CoT) reasoning. This helps visualize how effectively each approach enhances the model's ability to generate reasoned responses, highlighting the impact of the SFT loss on CoT reasoning performance.", "section": "5.2.3. Effects of optimization algorithms"}, {"figure_path": "https://arxiv.org/html/2411.10442/x4.png", "caption": "(a)", "description": "This figure shows examples from the MMPR dataset.  Specifically, it presents examples of instructions with and without clear ground truth. For instructions with clear ground truths, a correctness-based pipeline is used which samples multiple solutions, labelling those matching the ground truth as 'chosen responses' and those that don't as 'rejected responses'. For instructions without clear ground truths, a DropoutNTP pipeline is used.  The figure highlights the differences between chosen and rejected responses to illustrate how the dataset was constructed.", "section": "3. Scalable Multimodal Preference Dataset Generation"}, {"figure_path": "https://arxiv.org/html/2411.10442/x5.png", "caption": "(b)", "description": "This figure shows two magnets with their north and south poles labeled.  The north poles of both magnets face each other, and the south poles of both magnets face each other. The caption indicates that this arrangement will cause the magnets to repel.", "section": "3.2. Multimodal Preference Dataset"}, {"figure_path": "https://arxiv.org/html/2411.10442/x6.png", "caption": "(c)", "description": "The figure shows a bar chart visualizing the number of occurrences of different colors in the \"fence\" row of a dataset.  Each color represents a category (legs, index, engine, total), and the bar height indicates the value associated with that color in the \"fence\" row. This illustrates the numerical data distribution within the specified row, revealing the relative frequencies or counts of each category.", "section": "3.2. Multimodal Preference Dataset"}, {"figure_path": "https://arxiv.org/html/2411.10442/x7.png", "caption": "(d)", "description": "The figure shows examples from the MultiModal PReference (MMPR) dataset.  Specifically, it displays examples from the OCR (Optical Character Recognition) category of the dataset. The top shows the question: \"What is the total amount of this receipt?\", along with the chosen and rejected responses generated by a multimodal language model. The chosen response correctly calculates the total amount based on the invoice image, while the rejected response makes an incorrect calculation and includes extraneous information. This illustrates how the dataset differentiates between high-quality, correct reasoning and lower-quality, incorrect reasoning for multimodal tasks.", "section": "3. Scalable Multimodal Preference Dataset Generation"}, {"figure_path": "https://arxiv.org/html/2411.10442/x8.png", "caption": "Figure 4: \nResults of models trained with different data scales or hyer-parameters on M3CoT.\nThe X-axis represents the corresponding data scale or hyper-parameter for this point, while the Y-axis indicates the accuracy on M3CoT.", "description": "This figure displays the performance of models trained on varying amounts of data and different hyperparameters, specifically focusing on the M3CoT benchmark.  The graphs illustrate how changes in training data volume and hyperparameter settings (learning rate, PO coefficient, and SFT coefficient) affect the accuracy of the model.  The x-axis of each graph shows the variable being tested (data scale or hyperparameter), while the y-axis indicates the accuracy achieved on the M3CoT benchmark.  Separate lines represent the results for models making direct answers and those using chain-of-thought (CoT) reasoning.", "section": "8. More Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.10442/x9.png", "caption": "(a)", "description": "The figure shows examples of data samples from the MMPR dataset.  Panel (a) shows an example with clear ground truth, using a correctness-based pipeline.  Multiple response options to the same question are provided, where one response matches the ground truth and serves as the 'chosen response', while the others are incorrect and are labelled 'rejected responses'.  The differences between the chosen and rejected responses are highlighted in italicized text. Incorrect parts of rejected responses are highlighted in red. Panel (b) shows an example without clear ground truth, instead using a DropoutNTP pipeline.  A generated response is truncated, and the model is prompted to complete it without image input. This incomplete response becomes the 'rejected response', while the original, complete response is the 'chosen response'.  Again, differences are highlighted in italicized text, and incorrect parts of the rejected response in red.", "section": "3. Scalable Multimodal Preference Dataset Generation"}, {"figure_path": "https://arxiv.org/html/2411.10442/x10.png", "caption": "(b)", "description": "This figure shows two bar magnets placed with their north poles facing each other.  The caption explains the principles of magnetic attraction and repulsion: like poles repel, and unlike poles attract. The image is used to illustrate that because the north poles of both magnets are facing each other, the magnets will repel each other. This is a visual aid to support the explanation of magnetic forces in a section of the paper that discusses multimodal reasoning.", "section": "3.2 Multimodal Preference Dataset"}, {"figure_path": "https://arxiv.org/html/2411.10442/x11.png", "caption": "(c)", "description": "The figure shows a bar chart visualizing the internet access percentage for five different countries in 2015. Each country is represented by a unique color, enabling easy differentiation between the countries and their internet access statistics. The chart's structure makes it simple to compare the internet access levels across the selected nations.", "section": "3.2. Multimodal Preference Dataset"}, {"figure_path": "https://arxiv.org/html/2411.10442/x12.png", "caption": "(d)", "description": "This figure shows examples from the MultiModal Preference dataset (MMPR). Specifically, it displays samples from the 'Mathematics' domain where the task is to determine the perimeter of triangle ABO given information about quadrilateral ABCD with intersecting diagonals.  The \"Chosen Response\" provides a step-by-step solution that leverages geometric properties and equations. The \"Rejected Response\" demonstrates an alternative approach, highlighting the differences in reasoning processes and correctness in reaching the solution.", "section": "3. Scalable Multimodal Preference Dataset Generation"}, {"figure_path": "https://arxiv.org/html/2411.10442/x13.png", "caption": "(e)", "description": "This figure shows examples from the MMPR dataset. Specifically, it displays examples of instructions with and without clear ground truths. For instructions with clear ground truths, a correctness-based pipeline was used, where responses matching the ground truth are considered chosen responses and those that don't match are rejected responses. For instructions without clear ground truths, the DropoutNTP pipeline was used. The chosen and rejected responses are presented, with differences highlighted to illustrate how the pipeline generates preference pairs for training.  Red highlighting indicates incorrect responses.", "section": "3. Scalable Multimodal Preference Dataset Generation"}, {"figure_path": "https://arxiv.org/html/2411.10442/x14.png", "caption": "(f)", "description": "This figure shows examples from the MultiModal Preference dataset (MMPR).  Specifically, it displays examples of instructions and their corresponding chosen and rejected responses from the OCR (Optical Character Recognition) domain.  The chosen responses provide accurate interpretations of the image content, while the rejected responses contain errors or hallucinations.  This visual aids in understanding the dataset's composition and the differences between preferable and less preferable responses used for model training.", "section": "3. Scalable Multimodal Preference Dataset Generation"}, {"figure_path": "https://arxiv.org/html/2411.10442/x15.png", "caption": "(g)", "description": "This figure shows examples from the MultiModal Preference dataset (MMPR).  Specifically, it displays examples of instructions and responses from the chart domain.  The left side shows a 'chosen response' which is a correct and well-reasoned answer. The right side shows a 'rejected response' which is either incorrect or poorly reasoned.  The responses are paired to illustrate the preference data used to train the model.  The visual is a chart showing data points with different colors and the instruction asks about the color-coding of the chart and the value related to a specific color.", "section": "3. Scalable Multimodal Preference Dataset Generation"}, {"figure_path": "https://arxiv.org/html/2411.10442/x16.png", "caption": "(h)", "description": "This figure visualizes the results of models trained with different data scales on the M3CoT benchmark.  The x-axis represents the size of the training dataset (10K, 40K, 70K, and 100K samples). The y-axis shows the accuracy achieved on the M3CoT benchmark. Two lines are plotted: one for the model's performance using direct answers and the other for its performance using Chain-of-Thought (CoT) reasoning. The graph shows a clear positive correlation between the amount of training data and the model's accuracy, indicating that scaling up the reasoning preference data improves the model's performance.", "section": "8.3. Effects of data scale"}, {"figure_path": "https://arxiv.org/html/2411.10442/x17.png", "caption": "(i)", "description": "The figure displays the performance of various open-source multimodal large language models (MLLMs) on the MathVista benchmark.  The x-axis represents accuracy with direct-answer responses, and the y-axis represents accuracy using Chain-of-Thought (CoT) reasoning. Each bubble represents a different model; the size of the bubble is correlated with the number of parameters in the model.  The plot reveals a common trend among open-source MLLMs:  performance decreases when CoT reasoning is used, indicating a distribution shift problem between training and inference.  The authors' model, InternVL2-8B-MPO, is shown to significantly outperform other models, achieving results comparable to much larger models.", "section": "1. Introduction"}]