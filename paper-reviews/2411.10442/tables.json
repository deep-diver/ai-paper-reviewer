[{"content": "| Task | Dataset |\n|---|---| \n| General VQA | VQAv2 [29], GQA [34], OKVQA [63], IconQA [59] |\n| Science | AI2D [39], ScienceQA [60], M3CoT [16] |\n| Chart | ChartQA [64], DVQA [37], MapQA [13] |\n| Mathematics | GeoQA+ [12], CLEVR-Math [51], Geometry3K [58], GEOS [82], GeomVerse [38], Geo170K [27] |\n| OCR | OCRVQA [68], InfoVQA [66], TextVQA [83], STVQA [8], SROIE [33] |\n| Document | DocVQA [65] |", "caption": "Table 1: \nDatasets used to build our preference dataset.\nWe collect images and instructions from various tasks to ensure the diversity of our dataset.", "description": "This table lists the various datasets used to create the MMPR (MultiModal Preference dataset), a dataset of image-text pairs with associated preference annotations used for training the multimodal language model.  The datasets represent diverse tasks, including general visual question answering, science, charts, mathematics, optical character recognition (OCR), and documents. The inclusion of these diverse sources aims to ensure the MMPR dataset's variety and robustness, facilitating the model's ability to reason across a range of scenarios.", "section": "3. Scalable Multimodal Preference Dataset Generation"}, {"content": "| Dataset | Citation |\n|---|---| \n| GeoQA+ | [12] |\n| CLEVR-Math | [51] |\n| Geometry3K | [58] |\n| GEOS | [82] |\n| GeomVerse | [38] |\n| Geo170K | [27] |", "caption": "Table 2: \nResults on 8 multimodal benchmarks. We report the overall score of MM-Vet and LLaVA-Bench evaluated by GPT-4-Turbo.\nOur InternVL2-8B-MPO demonstrates superior performance compared to InternVL2-8B across multimodal reasoning, VQA, and hallucination evaluation benchmarks. Noteably, our model even achieves reasoning performance comparable to the 10\u00d7\\times\u00d7 larger InternVL2-76B.", "description": "This table presents a comprehensive comparison of various multimodal large language models (MLLMs) across eight benchmark datasets.  The benchmarks assess different aspects of MLLM capabilities, including reasoning, visual question answering (VQA), and hallucination. The table showcases the performance of several open-source MLLMs, including different variants of InternVL2, along with closed-source models like Gemini and GPT-4 for reference.  The key finding is that InternVL2-8B-MPO, the model enhanced by the authors' Mixed Preference Optimization (MPO) technique, significantly outperforms the base InternVL2-8B model, achieving performance on par with much larger models like InternVL2-76B.", "section": "5.1 Main Results"}, {"content": "| Model | Citation |\n|---|---| \n| OCRVQA | [68] |\n| InfoVQA | [66] |\n| TextVQA | [83] |\n| STVQA | [8] |\n| SROIE | [33] |", "caption": "Table 3: Results of models trained with SFT and MPO. The SFT training data consists of the chosen responses from the preference pairs used in MPO training. In the Direct setting, the model is prompted to provide the answer directly, while in the CoT setting, the model is instructed to answer with detailed rationales.", "description": "This table compares the performance of two models: one trained with supervised fine-tuning (SFT) and another trained with the proposed mixed preference optimization (MPO) method.  Both models are evaluated on their ability to answer questions directly (Direct) and using chain-of-thought (CoT) reasoning.  The SFT model uses chosen responses from the MPO training data for its training. The table showcases the accuracy of both methods across multiple benchmark datasets, highlighting the difference in performance between Direct and CoT approaches for both training methods.", "section": "5. Experiments"}, {"content": "| Model Name | M3CoT | MathVista | MathVision | MMVet | LLaVA-Bench | POPE | CRPE | MMHalBench |\n|---|---|---|---|---|---|---|---|---|\n| **Closed-Source Models** |  |  |  |  |  |  |  |  |\n| Gemini-1.5-Pro [78] | - | 63.9 | 19.2 | - | - | - | - | - |\n| GPT-4o [71] | 64.3 | 63.8 | 30.4 | 69.1 | 97.6 | 86.9 | 76.6 | 4.0 |\n| GPT-4o-Mini [71] | 61.9 | 52.4 | 27.3 | 66.9 | 95.4 | 85.1 | 73.1 | 3.6 |\n| **Open-Source Models** |  |  |  |  |  |  |  |  |\n| LLaVA-1.5-13B [52] | 39.5 | 27.6 | 11.1 | 36.3 | 70.7 | 85.9 | 55.6 | 2.4 |\n| Qwen2-VL-7B [96] | 57.8 | 58.2 | 21.1 | 60.6 | 67.7 | 88.1 | 74.4 | 3.4 |\n| MiniCPM-V-2-6-8B [105] | 56.0 | 60.6 | 23.4 | 57.4 | 83.4 | 87.3 | 75.2 | 3.6 |\n| LLaVA-OneVision-7B [44] | 52.3 | 63.2 | 18.4 | 51.4 | 79.9 | 88.4 | 73.7 | 3.1 |\n| **InternVL Models** |  |  |  |  |  |  |  |  |\n| InternVL2-26B [20] | 58.2 | 59.4 | 23.4 | 62.1 | 92.3 | 88.0 | 75.6 | 3.7 |\n| InternVL2-40B [20] | 63.6 | 63.7 | 21.4 | 65.5 | 100.5 | 88.4 | 77.3 | 3.9 |\n| InternVL2-76B [20] | 65.4 | 67.2 | 23.7 | 65.7 | 99.3 | 89.0 | 77.8 | 3.8 |\n| InternVL2-Pro [20] | 65.6 | 66.3 | 18.8 | 69.4 | 99.5 | 88.2 | 77.6 | 3.7 |\n| InternVL2-8B [20] | 59.3 | 58.3 | 20.4 | 54.2 | 73.2 | 86.9 | 75.0 | 3.3 |\n| InternVL2-8B-MPO (ours) | 79.2 | 67.0 | 25.7 | 56.2 | 76.7 | 88.1 | 75.4 | 3.5 |", "caption": "Table 4: \nComparison of DropoutNTP and the divide-and-conquer approach from RLAIF-V. We replace negative samples in RLAIF-V with the responses generated using DropoutNTP.", "description": "This table compares the performance of two methods for generating negative samples in a multimodal preference optimization dataset: DropoutNTP (a novel method proposed in this paper) and the divide-and-conquer approach from RLAIF-V (a previous work).  Specifically, it shows the hallucination rates (in response-level and mention-level) on the Object HalBench benchmark, as well as the overall score and hallucination rates on the MMHalBench benchmark. The goal is to demonstrate the effectiveness of DropoutNTP, which achieves comparable performance to RLAIF-V's more complex method, while being simpler and more efficient.", "section": "5.2.2. Comparison with RLAIF-V"}, {"content": "| Model Name | Setting | M3CoT | MathVista | MMVet | POPE |\n|---|---|---|---|---|---| \n| InternVL2-8B | Direct | 59.3 | 58.3 | 54.2 | 86.9 |\n|  | CoT | 57.0 | 56.8 | 54.7 | 82.9 |\n| InternVL2-8B-SFT | Direct | 63.9 | 62.7 | 54.7 | 86.5 |\n|  | CoT | 67.8 | 64.2 | 53.8 | 84.0 |\n| InternVL2-8B-MPO | Direct | 77.2 | 64.5 | 55.1 | 87.0 |\n|  | CoT | 79.2 | 67.0 | 56.2 | 88.1 |", "caption": "Table 5: \nResults on text-only benchmarks.\nThe model fine-tuned through MPO exhibits superior overall performance on text-only tasks compared to the baseline model and its SFT counterpart, particularly on TheoremQA and IFEval.", "description": "This table presents the results of evaluating the performance of language models on various text-only benchmarks.  The models evaluated include a baseline model, a model fine-tuned with supervised fine-tuning (SFT), and a model fine-tuned using the proposed mixed preference optimization (MPO) method. The benchmarks cover a range of tasks, and the table shows the performance scores for each model on each benchmark.  The MPO model demonstrates significantly improved performance compared to both the baseline model and the SFT model, especially on TheoremQA and IFEval.", "section": "5. Experiments"}, {"content": "| Method | Object HalBench |  | MM HalBench |  |  |\n|---|---|---|---|---|---|---|\n|  | Resp. (\u2193) | Ment. (\u2193) | Score | Hall. (\u2193) |  |  |\n| InternVL2-8B | 18.4 | 8.7 | 3.3 | 40.6 |  |  |\n| RLAIF-V [107] | 7.3 | 3.9 | 3.5 | 32.3 |  |  |\n| DropoutNTP (ours) | 7.6 | 4.1 | 3.6 | 31.3 |  |  |", "caption": "Table 6: \nResults of models trained with different preference optimization algorithms on M3CoT.\n\u0394\u0394\\Deltaroman_\u0394 represents the performance gap between CoT responses and direct-answer responses.", "description": "This table presents the results of experiments evaluating various preference optimization algorithms applied to the M3CoT benchmark.  For each algorithm (DPO, RSO, IPO, CDPO, RobustDPO, BCO, SPPO, AOT, TR-DPO), the table shows the accuracy scores achieved when the model generates responses directly (Direct) and when it uses chain-of-thought (CoT) reasoning.  The final column, \u0394, indicates the performance difference between CoT and direct responses, providing insights into how each optimization method affects the model's reasoning capabilities. A positive \u0394 value suggests CoT improves the model while a negative \u0394 indicates CoT reasoning reduces performance.", "section": "5.2.3 Effects of optimization algorithms"}, {"content": "| Setting | MMLU | Gaokao | TriviaQA | NQ | C3 | Race-h | BBH | GSM8K | Math | TheoremQA | IFEval | HumanEval | MBPP | Average |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Baseline | 73.2 | 75.0 | 62.0 | 28.1 | 94.2 | 90.8 | 72.7 | 75.6 | 39.5 | 15.6 | 52.3 | 69.5 | 58.8 | 62.1 |\n| SFT | 71.8 | 74.4 | 63.7 | 28.2 | 94.3 | 90.6 | 72.1 | 75.5 | 40.1 | 15.8 | 53.6 | 68.3 | 58.0 | 62.0 |\n| MPO | 71.0 | 74.8 | 64.2 | 29.3 | 94.2 | 90.6 | 71.8 | 75.0 | 40.4 | 20.8 | 56.4 | 68.9 | 61.5 | 63.0 |", "caption": "Table 7: \nResults of models trained with different preference optimization algorithms extended with SFT Loss on M3CoT.\nThe algorithm X extended with the SFT Loss is called X+ for brevity.\nFor instance, DPO+ is the combination of DPO and SFT loss.", "description": "This table presents the results of experiments on the M3CoT benchmark, comparing various preference optimization algorithms enhanced with supervised fine-tuning (SFT) loss.  Each algorithm (DPO, RSO, IPO, CDPO, RobustDPO, BCO, SPPO, AOT, TR-DPO, ORPO) was tested in its original form and then with the addition of SFT loss (denoted by '+'). The table shows the performance of each model configuration in terms of accuracy on direct-answer and chain-of-thought (CoT) reasoning tasks, and calculates the difference (\u0394) between the two. The purpose is to evaluate the effectiveness of incorporating SFT loss into different preference optimization approaches for improving the reasoning abilities of multimodal large language models, particularly in CoT scenarios.", "section": "5.2.3. Effects of optimization algorithms"}, {"content": "| Method | Direct | CoT | \u0394 |\n|---|---|---|---| \n| InternVL2-8B | 59.3 | 57.0 | -2.3 |\n| SFT | 65.7 | 68.5 | +2.8 |\n| DPO [76] | 75.8 | 72.7 | -3.1 |\n| RSO [54] | 74.2 | 74.3 | +0.1 |\n| IPO [4] | 72.8 | 73.1 | +0.3 |\n| cDPO [69] | 76.2 | 76.8 | +0.6 |\n| RobustDPO [21] | 75.1 | 74.2 | -0.9 |\n| BCO [36] | 78.1 | 78.4 | +0.3 |\n| SPPO [102] | 66.2 | 67.4 | +1.2 |\n| AOT [67] | 76.7 | 76.0 | -0.7 |\n| TR-DPO [28] | 75.9 | 66.8 | -9.1 |", "caption": "Table 8: Results of models trained with DPO+, BCO+ and MPO using our MMPR.", "description": "This table presents a comparison of the performance of three different multimodal large language models (MLLMs) on eight benchmark datasets. The models are InternVL2-8B (baseline), InternVL2-8B-DPO+, InternVL2-8B-BCO+, and InternVL2-8B-MPO.  InternVL2-8B-DPO+ and InternVL2-8B-BCO+ are fine-tuned using the Direct Preference Optimization (DPO) and Binary Classifier Optimization (BCO) methods, respectively. InternVL2-8B-MPO uses the Mixed Preference Optimization (MPO) method introduced in the paper. The benchmarks cover various multimodal reasoning tasks and hallucination evaluation.  The results show that the MPO model significantly outperforms the baseline and the DPO+ and BCO+ models on most of the benchmarks.", "section": "5. Main Results"}, {"content": "| Method | Direct | CoT | \u0394 |\n|---|---|---|---| \n| ORPO [32] | 66.6 | 73.9 | +7.3 |\n| DPO+ | 76.4 | 78.9 | +2.5 |\n| cDPO+ | 71.6 | 74.2 | +2.7 |\n| RobustDPO+ | 76.5 | 78.0 | +1.5 |\n| BCO+ | 77.4 | 78.4 | +1.0 |\n| AOT+ | 76.3 | 78.0 | +1.7 |\n| MPO | 77.7 | 79.1 | +1.4 |", "caption": "Table 9: Results of DropNTP with different Dropout Ratios.", "description": "This table presents an ablation study on the effect of varying the dropout ratio in the Dropout Next-Token Prediction (DropoutNTP) method used for generating negative samples in the MMPR dataset.  The dropout ratio determines the portion of a positive response that's truncated before the model attempts completion. The table shows the impact of using different dropout ratios (0.25, 0.50, and 0.75) on the hallucination rates measured by two different metrics: response-level and mention-level hallucinations.  The results are presented for the Object HalBench and MMHalBench datasets.", "section": "5.2. Ablation Study"}]