[{"figure_path": "https://arxiv.org/html/2501.06282/x1.png", "caption": "Figure 1: Performance comparison between our MinMo(\u223csimilar-to\\sim\u223c8B parameters) and top-tier speech-text multimodal models, including Moshi(7B)\u00a0(D\u00e9fossez et\u00a0al., 2024), Freeze-Omni(7.5B)\u00a0(Wang et\u00a0al., 2024b), GLM-4-Voice(9B)\u00a0(Zeng et\u00a0al., 2024), SeamlessM4T Large v2(2.3B)\u00a0(Communication et\u00a0al., 2023), NExT-GPT(12.42B)\u00a0(Wu et\u00a0al., 2024), speech-to-text model Qwen2-Audio(\u223csimilar-to\\sim\u223c8B)\u00a0(Chu et\u00a0al., 2024), Whisper-large-v3(1.55B)\u00a0(Radford et\u00a0al., 2023), and others. We demonstrate capabilities of MinMo on automatic speech recognition (ASR), speech-to-text translation (S2TT), spoken question answering (SQA) encompasses both speech-to-text (S2T) and speech-to-speech (S2S), vocal sound classification (VSC), speech emotion recognition (SER), language identification (LID), age recognition and gender detection. ASR is evaluated using 1-WER%, with Fleurs & Common Voice results are averaged over 10 languages (zh, en, ja, ko, yue, de, fr, ru, es, it). S2TT is evaluated using BLEU, with CoVoST2 results averaged over en2zh, en2ja, zh/ja/de/fr/ru/es/it2en translation directions. SQA is eavaluated using Accuracy. SER is evaluated using Weighted Accuracy. MinMo surpasses the previous SOTA models on all these tasks.", "description": "This figure compares the performance of the MinMo model (approximately 8 billion parameters) against other state-of-the-art speech-to-text multimodal models across various tasks.  These tasks include automatic speech recognition (ASR), speech-to-text translation (S2TT), spoken question answering (SQA - both speech-to-text and speech-to-speech), vocal sound classification (VSC), speech emotion recognition (SER), language identification (LID), age recognition, and gender detection.  The evaluation metrics vary depending on the task (WER/CER for ASR, BLEU for S2TT, accuracy for SQA and SER, and weighted accuracy for SER).  The results show that MinMo outperforms the other models on all tasks.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.06282/extracted/6124017/figure/minmo_example_1.png", "caption": "(a) An example showcases MinMo\u2019s capabilities, including speech-to-speech chat, speech-to-text translation, style-controllable speech synthesis, and full duplex interaction.", "description": "The figure showcases an example of MinMo's capabilities in a multi-turn conversation. The example starts with a user initiating a chat with MinMo.  MinMo responds naturally, demonstrates speech-to-speech conversation, seamlessly translates speech to text, generates speech with different styles (as shown in the varying speech bubbles), and handles simultaneous two-way communication (full-duplex interaction).  These features highlight the model's capabilities in voice interaction and the fluidity of its performance. ", "section": "3 MinMo"}, {"figure_path": "https://arxiv.org/html/2501.06282/extracted/6124017/figure/minmo_example_2.png", "caption": "(b) An example showcases MinMo\u2019s capabilities, including speech-to-speech chat, audio event detection, speaker analysis and speech-to-text translation.", "description": "This figure demonstrates MinMo's ability to handle a complex, multi-faceted voice interaction.  It shows a conversation where the user interacts with MinMo in speech, and MinMo responds not only with appropriate conversational text but also with several forms of advanced speech analysis.  The system detects audio events from the input speech, analyzes the speaker's characteristics, and performs accurate speech-to-text transcription, showing its multimodal capabilities.", "section": "3 MinMo"}, {"figure_path": "https://arxiv.org/html/2501.06282/x2.png", "caption": "Figure 2: Examples demonstrating various capabilities of MinMo. More capabilities of MinMo include the tasks shown in Table\u00a02.", "description": "This figure showcases MinMo's capabilities in handling various speech-related tasks, including speech-to-speech chat, speech-to-text translation, style-controllable speech synthesis (controlling aspects like tone and speed), and full duplex interaction (simultaneous two-way communication).  The examples highlight MinMo's ability to understand nuanced user instructions and generate appropriate, natural-sounding responses.", "section": "3 MinMo"}, {"figure_path": "https://arxiv.org/html/2501.06282/extracted/6124017/figure/Speech2Text-Data.png", "caption": "Figure 3: The overall architecture of MinMo. Table\u00a01 provides detailed descriptions of each module in this diagram.", "description": "This figure shows the architecture of the MinMo multimodal large language model.  It details the flow of information processing, starting with voice input (using a voice encoder) that gets projected and combined with text embeddings from the main Large Language Model.  The combined representation is processed by a Voice Token LM and an output projector to generate speech tokens. These speech tokens are then converted into actual waveforms by a token2wav synthesizer.  A full duplex predictor module is integrated to manage simultaneous two-way communication, dynamically deciding whether the system should continue speaking or listen to the user's input.", "section": "3 MinMo"}]