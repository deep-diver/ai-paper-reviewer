[{"figure_path": "https://arxiv.org/html/2501.08187/x1.png", "caption": "Fig. 1: \nOverview of InstructCell.\na, Summary of incorporated single-cell data. InstructCell incorporates 299,155 scRNA-seq samples from human and mouse origins, spanning multiple organs. CPCG denotes Conditional Pseudo-cell Generation, CTA denotes Cell Type Annotation, and DSP denotes Drug Sensitivity Prediction.\nb, Architecture of the multi-modal cell language model. The model processes both text and single-cell data via three primary components: a Q-Former to capture single-cell gene expression knowledge, a pre-trained LM as the backbone, and a cell reconstruction module for generating single-cell gene expression profiles.\nc, Construction of multi-modal single-cell instruction data. Complete instruction-response pairs are formed by combining required and optional attributes from text and single-cell modalities.\nd, Simulation of diverse communication styles. LLMs generate chat templates with varying traits (personality, motivation, and proficiency) to produce instructions that convey task-related information in different communication styles.", "description": "Figure 1 provides a comprehensive overview of INSTRUCTCELL, a multimodal AI copilot for single-cell analysis. Panel (a) summarizes the single-cell RNA sequencing (scRNA-seq) datasets used, highlighting the diverse tissues and species represented. Panel (b) illustrates the architecture of the model, which incorporates a Q-Former module for processing single-cell gene expression data, a pre-trained language model (LM) as the backbone, and a cell reconstruction module for generating gene expression profiles. Panel (c) details the construction of the multi-modal instruction dataset used to train the model, showing how text-based instructions are paired with scRNA-seq profiles. Finally, panel (d) showcases how various communication styles are simulated using LLMs by generating chat templates with differing personality, motivation, and proficiency levels.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2501.08187/x2.png", "caption": "Fig. 2: \nConditional pseudo-cell generation results by InstructCell.\na, UMAP visualizations of real and generated cells. The left plot shows the overlap between real and generated cells. The middle and right plots display real and generated cells, respectively, with distinct colors indicating different cell types.\nb, Dot plots of gene expression patterns derived from real (top) and generated (bottom) cells. Based on the test set from Tabular-Sapiens, we use Welch\u2019s t\ud835\udc61titalic_t-test to identify top three significant genes for each cell type and display them along x-axis. Cell types are arranged along y-axis. The size of each dot indicates the proportion of single cells within the corresponding cell type that express the gene, while the color of the dot represents the mean expression level of the gene within that cell type. The results of the remaining two datasets are available in Fig.12.\nc, Quantitative evaluation of cell generation performance across four datasets. A lower \u25b3\u25b3\\triangle\u25b3sKNN value indicates better structural alignment, a higher pKNN value reflects improved positional correspondence, and a lower MMD value denotes a more accurate approximation of the global data distribution.", "description": "Figure 2 presents a comprehensive evaluation of INSTRUCTCELL's conditional pseudo-cell generation capabilities. (a) shows UMAP visualizations comparing real and generated cells, highlighting the overlap and distinct cell types. (b) uses dot plots to compare gene expression patterns between real and generated cells, focusing on the top three most significant genes for each cell type. This comparison reveals how accurately INSTRUCTCELL replicates real cell patterns. Finally, (c) provides a quantitative evaluation across multiple datasets using AsKNN, pKNN, and MMD scores to assess structural alignment, positional correspondence, and overall data distribution similarity, respectively.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2501.08187/x3.png", "caption": "Fig. 3: \nCell type annotation results by InstructCell.\na, Evaluation of InstructCell\u2019s CTA performance across human heart, liver, pancreas, and mouse skin and pancreas datasets. Performance is quantified using weighted F1, macro F1, and accuracy metrics, with different colors representing different models.\nb, UMAP visualization of three different datasets. The left panel is colored by expert-annotated cell types from the original research, and the right panel is colored by InstructCell prediction results.\nc, Confusion matrices between predicted cell types and actual annotations for the three datasets. Darker shades denote a higher frequency of agreement between the model\u2019s predictions and the actual cell type annotations.", "description": "This figure demonstrates the performance of InstructCell on the cell type annotation (CTA) task. Panel (a) presents a quantitative comparison of InstructCell's performance against existing single-cell foundation models across five different datasets (human liver, pancreas, skin, and mouse pancreas).  The datasets encompass multiple organs and species, and are evaluated based on weighted F1, macro F1, and accuracy metrics. Panel (b) provides UMAP visualizations to compare the expert-annotated cell types against InstructCell's predictions for three of these datasets. Finally, panel (c) presents confusion matrices showing the agreement between InstructCell's predictions and ground truth labels across the three datasets visualized in (b).", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2501.08187/x4.png", "caption": "Fig. 4: \nDrug sensitivity prediction results by InstructCell.\na, Evaluation of InstructCell\u2019s CTA performance across human oral, lung, and mouse bone datasets. Performance is quantified using weighted F1, macro F1, and accuracy metrics, with different colors representing different models.\nb, UMAP visualization of the three datasets, with cells colored by drug sensitivity labels (sensitive, resistant, and holiday) for both expert-annotated results and InstructCell predictions.\nc, Confusion matrices between predicted cell types and actual annotations for the three datasets. Darker shades denote a higher frequency of agreement between the model\u2019s predictions and the actual drug sensitivity annotations.", "description": "Figure 4 presents a comprehensive evaluation of InstructCell's drug sensitivity prediction capabilities. Part (a) compares InstructCell's performance against several other models (scBERT, scGPT, Geneformer, Cell2Sentence) across three datasets (human oral cavity, human lung, and mouse bone marrow) using weighted F1, macro F1, and accuracy metrics.  Different colors represent different models for easier comparison. Part (b) shows UMAP visualizations of these three datasets. The cells are colored to reflect drug sensitivity as determined by both the experts and InstructCell, allowing a visual comparison of the predictions. Finally, part (c) displays confusion matrices that quantify the agreement between predicted and actual drug sensitivity labels for each dataset. Darker colors in these matrices indicate a higher frequency of agreement.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2501.08187/x5.png", "caption": "Fig. 5: \nRobustness of InstructCell.\na, Quantitative comparison of the CPCG task under seen and unseen instruction templates.\nResults are shown for \u25b3\u25b3\\triangle\u25b3sKNN and pKNN metrics at varying numbers of neighbors K\ud835\udc3eKitalic_K, as well as for MMD. Different colors denote whether the instruction templates are seen or unseen.\nb, Average performance of InstructCell under instruct and chat modes across each task. On the left side (classification tasks), the shape of each scatter point indicates whether options are provided or not, while the color distinguishes model versions. Each configuration includes 40 scatter points (20 with options and 20 without). On the right side (generative task), different colors represent different model versions.", "description": "Figure 5 demonstrates the robustness of INSTRUCTCELL across various conditions. Panel (a) compares the performance of the conditional pseudo-cell generation (CPCG) task using seen versus unseen instruction templates.  The performance is evaluated using three metrics: \u25b3sKNN, pKNN, and MMD, calculated with varying numbers of nearest neighbors (K).  Different colors represent whether the instruction templates were seen during training or not. Panel (b) presents the average performance of INSTRUCTCELL in both instruct and chat modes, across all three tasks.  For the classification tasks (cell type annotation and drug sensitivity prediction), point shapes indicate the presence or absence of response options, while point colors distinguish the model versions.  Each condition includes 40 data points (20 with options and 20 without). For the CPCG task, different colors represent different model versions.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2501.08187/x6.png", "caption": "Fig. 6: \nTop 10 significant genes identified by InstructCell for each cell type in two datasets.\na, b Heatmaps of the significant genes extracted from InstructCell by using gradient saliency-based method for (a) He-2020-Liver and (b) Xin-2016 datasets.\nThe color gradient from red to blue represents gene importance, with red indicating higher importance scores and blue indicating lower scores. Red markers in each row indicate that genes among the top 10 key genes identified by the model, are either reported as marker genes for the corresponding cell type in the CellMarker2.0 database or in recent literature.", "description": "Figure 6 presents heatmaps visualizing the top 10 genes most influential in InstructCell's cell type predictions for the He-2020-Liver and Xin-2016 datasets.  Red signifies high importance; blue, low.  Red markers highlight genes already established as markers for those cell types in the CellMarker2.0 database or recent literature, validating InstructCell's findings.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2501.08187/x7.png", "caption": "Fig. 7: \nA closer look of InstructCell.\na, Evaluation of response quality in InstructCell using the LLM-as-a-judge approach. Response quality is assessed based on fluency, grammar, and inclusion of predictive results, with Claude 3.5 Sonnet\u00a0[4] serving as an unbiased evaluator. Text highlighted in purple indicates additional content for the CPCG task compared to the two classification tasks.\nb, Impact of Q-Former on model performance.\nPerformance comparison between the Q-Former and a standard MLP for encoding single-cell data.\nc, Impact of query embedding quantity on model performance.\nPerformance comparison across different numbers of query embeddings.\nd, Comparative performance of multi-task vs. single-task instruction tuning. For single-task instruction tuning, we divide our multi-modal instruction dataset by task type and train separate models for each specific task. We report the average metrics across all datasets for each task using the InstructCell-instruct version.\ne, Comparative performance of without vs. with pre-trained LM weights. We conduct experiments using the InstructCell-chat version to explore the impact of employing or not employing pre-trained weights on model performance.\nf, Impact of varying template ratios on model outputs.\nFour chat version models are trained on all classification datasets using multi-task instruction tuning with varying ratios of templates (0.5%, 5%, 50%, and 100% of the total templates). For the CTA and DSP tasks, 40 unseen instruction templates are selected for evaluation: 20 with multiple-choice options and 20 without. The mean performance and standard deviation for each template are calculated across all datasets for these two tasks. Additionally, we sample 500 unseen instruction templates and use Claude 3.5 Sonnet to score the model\u2019s outputs for expressiveness, while unigram analysis is conducted to assess lexical diversity.", "description": "Figure 7 presents a detailed analysis of the INSTRUCTCELL model's performance and design choices.  Panel A evaluates response quality using an LLM as a judge, focusing on fluency, grammar, and prediction. Panel B compares the performance of using a Q-Former versus a standard MLP for encoding single-cell data. Panel C assesses performance using different numbers of query embeddings in the model. Panel D contrasts multi-task versus single-task instruction tuning. Panel E examines the impact of pre-trained language model weights on performance. Panel F shows the impact of varying instruction template ratios on model performance, also evaluating expressiveness and lexical diversity using an additional LLM.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2501.08187/x8.png", "caption": "Fig. 8: \nThe examples of the prompts used to construct instruction-response templates for CPCG.\na, An example of the prompts for generating personality traits. b, An example of the prompts used to generate motivations for CPCG. c, An example of the prompts used to generate instruction templates for CPCG. d, An example of the prompts used to generate response templates for CPCG. e, An example of the prompts for rewriting instruction templates to enhance diversity.", "description": "Figure 8 illustrates the prompts used to generate the instruction-response template dataset for the Conditional Pseudo-cell Generation (CPCG) task.  Panel (a) shows the prompt used to generate a list of personality traits; panel (b) shows the prompt used to generate a list of motivations for requesting CPCG; panel (c) provides an example prompt for generating CPCG instruction templates; panel (d) shows the prompt used to generate response templates; finally, panel (e) demonstrates how prompts were designed to encourage diverse phrasing in the instruction templates.", "section": "Methods"}, {"figure_path": "https://arxiv.org/html/2501.08187/x10.png", "caption": "Fig. 9: \nDetailed overview and UMAP visualizations of scRNA-seq datasets used in this work.\nA total of 11 datasets are utilized, spanning 2 species and 11 tissue types. Among them, 5 datasets are employed for CTA, 3 for DSP, and 3 for CPCG. The number of samples for each dataset is listed under the column # Samples. The middle three UMAP plots present the single-cell data landscape, showcasing the distributions across different datasets, species, and tissues. The lower UMAP plot shows the label distribution for Ma-2020, GSE110894, and PBMC68K. Covering three distinct tasks, our data include various labels, such as response labels for DSP and cell type labels for other two tasks.", "description": "Figure 9 provides a detailed summary of the single-cell RNA sequencing (scRNA-seq) datasets used in the study.  It shows 11 datasets encompassing two species (human and mouse) and 11 distinct tissue types.  The figure highlights how these datasets are distributed across the three main tasks of the study: Cell Type Annotation (CTA), Drug Sensitivity Prediction (DSP), and Conditional Pseudo-cell Generation (CPCG).  The number of samples within each dataset is clearly indicated.  UMAP plots are shown to visualize the distribution of cells across the datasets. These plots reveal how the cells cluster based on their biological characteristics, including the species and tissue they originate from. Lastly, a separate UMAP plot focuses on three of the datasets (Ma-2020, GSE110894, PBMC68K) to show the distribution of labels (cell type and drug response) within those datasets.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2501.08187/x11.png", "caption": "Fig. 10: \nStatistics of synthetic instruction-response templates.\na, Length of instruction templates for different communication styles. Traits such as personality (orange), motivation (green), proficiency (red), and their combination (purple) are systematically removed to evaluate their impact, compared to the full trait version (blue). Variance for each distribution is shown in parentheses.\nb, Similarity of instruction templates across different communication styles. Template similarity is measured pairwise, with samples exceeding a similarity threshold of 0.75 excluded. Average similarity values for each style are reported in parentheses.\nc, Lexical diversity of instruction templates across communication styles. Lexical diversity is quantified using the unigram ratio, defined as the proportion of unique unigrams to the total number of unigrams in each instruction template.\nd, Length distribution of instruction and response templates across tasks. A scatter plot illustrates the lengths of instructions and responses for distinct tasks, with different colors representing task categories.", "description": "This figure presents a detailed statistical analysis of synthetically generated instruction-response templates used in the INSTRUCTCELL model training.  Panel (a) shows the distribution of instruction template lengths for different communication styles (full traits, and traits systematically removed: personality, motivation, proficiency). The variance for each distribution is provided. Panel (b) illustrates the similarity between instruction templates, with pairs having similarity scores above 0.75 removed to avoid bias. The mean similarity for each style is given. Panel (c) displays lexical diversity calculated as a unigram ratio (unique unigrams/total unigrams). Finally, panel (d) presents a scatter plot visualizing the length distributions of instruction and response templates for the three different tasks: cell type annotation, drug sensitivity prediction, and conditional pseudo-cell generation.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2501.08187/x12.png", "caption": "Fig. 11: \nQualitative examples of InstructCell-chat.\nIllustrative examples for each task are presented, showcasing instructions, model-generated responses from InstructCell-chat, the corresponding ground truth answers, and their sample sources.", "description": "Figure 11 presents several examples demonstrating INSTRUCTCELL-chat's capabilities in handling three different single-cell analysis tasks: cell type annotation (CTA), drug sensitivity prediction (DSP), and conditional pseudo-cell generation (CPCG).  Each example shows the user's natural language instruction, INSTRUCTCELL-chat's response, the ground truth response (a human-generated answer), and the data source (a specific single-cell RNA sequencing dataset) used for that example. This detailed breakdown helps visualize INSTRUCTCELL's ability to interpret complex instructions, generate accurate predictions, and provide helpful responses in a conversational manner. The diverse nature of the instructions and data sources highlights the model's flexibility and adaptability to different scenarios.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2501.08187/x13.png", "caption": "Fig. 12: \nBubble plot for conditional pseudo-cell generation.\nAs a supplement to Fig. 2(b), this plot highlights the top three significant genes for each cell type from the remaining two datasets in our study.\nFor model-generated cells, we display the expression ratios and average expression levels of these significant genes. Redder hues indicate higher average gene expression, while larger circles represent a higher proportion of gene expression within the corresponding cell type.", "description": "Figure 12 provides a detailed visualization of the conditional pseudo-cell generation results.  It expands upon Figure 2(b) by showing the top three most important genes for each cell type across two additional datasets (Mouse Atlas and PBMC68K). For each cell type, the plot displays the average expression level of these key genes (indicated by color intensity) and the proportion of cells expressing each gene (indicated by circle size) for both real and model-generated cells. This allows for a direct comparison of the generated cell profiles to the real profiles, providing a visual assessment of the model's ability to accurately capture the essential biological features.", "section": "Results"}]