<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>CoSTA$st$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="CoSTA$st$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing &#183; HF Daily Paper Reviews by AI"><meta name=description content="COSTA*: A cost-effective agent that smartly navigates AI tools to edit images with high quality and low cost, balancing user preferences!"><meta name=keywords content="Computer Vision,Image Generation,üè¢ University of Maryland,College Park,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10613/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10613/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="CoSTA$st$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing"><meta property="og:description" content="COSTA*: A cost-effective agent that smartly navigates AI tools to edit images with high quality and low cost, balancing user preferences!"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-03-13T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-13T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Image Generation"><meta property="article:tag" content="üè¢ University of Maryland, College Park"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10613/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10613/cover.png"><meta name=twitter:title content="CoSTA$st$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing"><meta name=twitter:description content="COSTA*: A cost-effective agent that smartly navigates AI tools to edit images with high quality and low cost, balancing user preferences!"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"CoSTA$\u0007st$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing","headline":"CoSTA$\u0007st$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing","abstract":"COSTA*: A cost-effective agent that smartly navigates AI tools to edit images with high quality and low cost, balancing user preferences!","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2503.10613\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-03-13T00:00:00\u002b00:00","datePublished":"2025-03-13T00:00:00\u002b00:00","dateModified":"2025-03-13T00:00:00\u002b00:00","keywords":["Computer Vision","Image Generation","üè¢ University of Maryland, College Park"],"mainEntityOfPage":"true","wordCount":"5298"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-20/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-20</p></a><a href=/ai-paper-reviewer/2025-03-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-21</p></a><a href=/ai-paper-reviewer/2025-03-24/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-24</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-20/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-20</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-21</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-24/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-24</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2503.10613/cover_hu10127457056275953896.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2503.10613/>CoSTA$st$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">CoSTA$st$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-03-13T00:00:00+00:00>13 March 2025</time><span class="px-2 text-primary-500">&#183;</span><span>5298 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">25 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2503.10613/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2503.10613/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/image-generation/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Image Generation
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-university-of-maryland-college-park/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ University of Maryland, College Park</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#llma-toolpath>LLM+A* Toolpath</a></li><li><a href=#cost-vs-quality>Cost vs. Quality</a></li><li><a href=#multimodal-edit>Multimodal Edit</a></li><li><a href=#tdg-automaton>TDG Automaton</a></li><li><a href=#human--clip>Human > CLIP</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#llma-toolpath>LLM+A* Toolpath</a></li><li><a href=#cost-vs-quality>Cost vs. Quality</a></li><li><a href=#multimodal-edit>Multimodal Edit</a></li><li><a href=#tdg-automaton>TDG Automaton</a></li><li><a href=#human--clip>Human > CLIP</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2503.10613</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Advait Gupta et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-03-14</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2503.10613 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2503.10613 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2503.10613/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Multi-turn image editing is challenging due to the difficulty of balancing the cost and quality of various AI tools. Existing methods often rely on expensive exploration or lack accurate cost estimations. Current text-to-image models struggle with composite instructions that require multiple adjustments while preserving other parts. Large Language Models are useful for decomposing the task, but finding the toolpath (efficient and successful tool use) is difficult because of the high cost to train and computational costs. Therefore, there is a need for a cost-sensitive agent that can decompose a multi-turn task into subtasks.</p><p>This paper introduces &ldquo;<em><em>COSTA</em>&rdquo;, a cost-sensitive toolpath agent that combines LLMs and A</em> search to find cost-effective tool paths for multi-turn image editing. CoSTA* uses LLMs to create a subtask tree and prunes a graph of AI tools for the given task. It then conducts A* search on the smaller subgraph to find the tool path and balances the total cost and quality to guide the A* search. Each subtask&rsquo;s output is evaluated by a vision-language model, where a failure updates the tool&rsquo;s cost and quality on the subtask. COSTA* automatically switches between modalities across subtasks and outperforms state-of-the-art models in terms of cost and quality.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-220c0ff2a890c6179418078213366365></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-220c0ff2a890c6179418078213366365",{strings:[" CoSTA* leverages LLMs and A* search for efficient and cost-sensitive image editing. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-8908e11e929e7a130266bf871775dc57></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-8908e11e929e7a130266bf871775dc57",{strings:[" The agent balances cost, quality, and user preferences via a cost-sensitive A* search. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-6ce53c66a0b9b671f932600c79364dcc></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-6ce53c66a0b9b671f932600c79364dcc",{strings:[" Experiments on a new benchmark demonstrate state-of-the-art performance in multi-turn image editing. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This research presents a new cost-sensitive image-editing agent that can efficiently combine large multimodal models, offering flexibility to search for the optimal editing path and balance cost and quality. It advances the field of multimodal learning and enables better automation in content creation and image restoration. Further research includes addressing the potential biases from pre-trained models and ensuring ethical and responsible usage.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.10613/x1.png alt></figure></p><blockquote><p>üîº This figure presents a comparison of CoSTA*‚Äôs performance against four other state-of-the-art image editing models/agents, varying the cost-quality trade-off coefficient (Œ±). The x-axis represents the cost (in seconds), and the y-axis represents the quality. Each line represents a different Œ± value for CoSTA*, demonstrating its ability to achieve various cost-quality tradeoffs. The Pareto front highlights the optimal trade-off between cost and quality. CoSTA* consistently outperforms other methods, achieving Pareto optimality and dominating the baselines on both metrics (cost and quality).</p><details><summary>read the caption</summary>Figure 1: CoSTA‚àó with different cost-quality trade-off coefficients Œ±ùõº\alphaitalic_Œ± vs. four recent image-editing models/agents. CoSTA‚àó achieves Pareto optimality and dominates baselines on both metrics.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S3.T1.1.1><thead class=ltx_thead><tr class=ltx_tr id=S3.T1.1.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=S3.T1.1.1.1.1.1>Model</th><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id=S3.T1.1.1.1.1.2>Supported Subtasks</th><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id=S3.T1.1.1.1.1.3>Inputs</th><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id=S3.T1.1.1.1.1.4>Outputs</th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S3.T1.1.1.2.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S3.T1.1.1.2.1.1>YOLO¬†<cite class="ltx_cite ltx_citemacro_citep">(Wang et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib32 title>2022</a>)</cite></th><td class="ltx_td ltx_align_left ltx_border_t" id=S3.T1.1.1.2.1.2>Object Detection</td><td class="ltx_td ltx_align_left ltx_border_t" id=S3.T1.1.1.2.1.3>Input Image</td><td class="ltx_td ltx_align_left ltx_border_t" id=S3.T1.1.1.2.1.4>Bounding Boxes</td></tr><tr class=ltx_tr id=S3.T1.1.1.3.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S3.T1.1.1.3.2.1>SAM¬†<cite class="ltx_cite ltx_citemacro_citep">(Kirillov et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib15 title>2023a</a>)</cite></th><td class="ltx_td ltx_align_left" id=S3.T1.1.1.3.2.2>Segmentation</td><td class="ltx_td ltx_align_left" id=S3.T1.1.1.3.2.3>Bounding Boxes</td><td class="ltx_td ltx_align_left" id=S3.T1.1.1.3.2.4>Segmentation Masks</td></tr><tr class=ltx_tr id=S3.T1.1.1.4.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S3.T1.1.1.4.3.1>DALL-E¬†<cite class="ltx_cite ltx_citemacro_citep">(Ramesh et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib25 title>2021</a>)</cite></th><td class="ltx_td ltx_align_left" id=S3.T1.1.1.4.3.2>Object Replacement</td><td class="ltx_td ltx_align_left" id=S3.T1.1.1.4.3.3>Segmentation Mask</td><td class="ltx_td ltx_align_left" id=S3.T1.1.1.4.3.4>Edited Image</td></tr><tr class=ltx_tr id=S3.T1.1.1.5.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S3.T1.1.1.5.4.1>Stable Diffusion Inpaint</th><td class="ltx_td ltx_align_left" id=S3.T1.1.1.5.4.2>Object Removal, Replacement,</td><td class="ltx_td ltx_align_left" id=S3.T1.1.1.5.4.3>Segmentation Mask</td><td class="ltx_td ltx_align_left" id=S3.T1.1.1.5.4.4>Edited Image</td></tr><tr class=ltx_tr id=S3.T1.1.1.6.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S3.T1.1.1.6.5.1><cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib28 title>2022a</a>)</cite></th><td class="ltx_td ltx_align_left" id=S3.T1.1.1.6.5.2>Recoloration</td><td class=ltx_td id=S3.T1.1.1.6.5.3></td><td class=ltx_td id=S3.T1.1.1.6.5.4></td></tr><tr class=ltx_tr id=S3.T1.1.1.7.6><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S3.T1.1.1.7.6.1>EasyOCR</th><td class="ltx_td ltx_align_left" id=S3.T1.1.1.7.6.2>Text Extraction</td><td class="ltx_td ltx_align_left" id=S3.T1.1.1.7.6.3>Text Bounding Box</td><td class="ltx_td ltx_align_left" id=S3.T1.1.1.7.6.4>Extracted Text</td></tr><tr class=ltx_tr id=S3.T1.1.1.8.7><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S3.T1.1.1.8.7.1><cite class="ltx_cite ltx_citemacro_citep">(Kittinaradorn et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib17 title>2022</a>)</cite></th><td class="ltx_td ltx_border_bb" id=S3.T1.1.1.8.7.2></td><td class="ltx_td ltx_border_bb" id=S3.T1.1.1.8.7.3></td><td class="ltx_td ltx_border_bb" id=S3.T1.1.1.8.7.4></td></tr></tbody></table></table></figure><blockquote><p>üîº This table provides a partial overview of the models used in the CoSTA* system. It lists each model&rsquo;s name, the image-editing subtasks it supports, the inputs it requires, and the outputs it produces. This information is crucial for understanding how the system chooses appropriate tools for each step in the multi-turn image editing process. The table is an excerpt; the full table contains information on all 24 models used in the paper.</p><details><summary>read the caption</summary>Table 1: Model Description Table (excerpt)</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">LLM+A* Toolpath<div id=llma-toolpath class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#llma-toolpath aria-label=Anchor>#</a></span></h4><p><em><em>Integrating LLMs with A</em> search for toolpath optimization presents a compelling approach</em>*. LLMs offer high-level planning but lack precision in tool-specific costs. <em><em>A</em> excels in optimal path finding but struggles with the vast search space of AI tools</em>*. A hybrid approach could leverage LLMs for subtask decomposition, pruning the tool search space, then employ A* on the smaller graph for cost-sensitive path discovery. <strong>This balances global planning with local precision</strong>, addressing the quality-cost trade-off by using LLM insights and A* optimization, leading to better tool selection and efficient solutions, improving results over LLM-only or A*-only search.</p><h4 class="relative group">Cost vs. Quality<div id=cost-vs-quality class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#cost-vs-quality aria-label=Anchor>#</a></span></h4><p><em><em>CoSTA</em> adeptly addresses the trade-off between cost and quality</em>* in image editing. By using an A* search algorithm guided by a cost function that considers both execution time and output quality, it finds a path that balances these factors. The user can control the balance to <strong>optimize the trade-off according to preferences</strong>. With different cost-quality trade-off coefficients, it can achieve Pareto optimality dominating baselines on both metrics, offering more choices of tool use paths to cater to different use-cases based on budgets.</p><h4 class="relative group">Multimodal Edit<div id=multimodal-edit class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multimodal-edit aria-label=Anchor>#</a></span></h4><p><strong>Multimodal editing</strong> represents a significant frontier in image manipulation, moving beyond simple text-guided edits to incorporate various modalities like sketches, audio, or even other images to guide the editing process. The central challenge lies in <strong>effectively fusing information</strong> from disparate sources, each with its unique semantic and structural properties. This requires sophisticated architectures capable of <strong>understanding cross-modal relationships</strong> and resolving potential conflicts or ambiguities arising from inconsistent cues. A robust multimodal editing system needs to <strong>strike a balance between adhering to user intent</strong> expressed through multiple modalities and <strong>maintaining the realism and coherence of the edited image</strong>. Key research directions include developing <strong>novel fusion mechanisms</strong>, exploring techniques for <strong>handling noisy or incomplete modal information</strong>, and creating <strong>intuitive interfaces</strong> that allow users to seamlessly interact with the system using a variety of input methods. Ultimately, successful multimodal editing opens up exciting possibilities for creative expression and content creation.</p><h4 class="relative group">TDG Automaton<div id=tdg-automaton class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tdg-automaton aria-label=Anchor>#</a></span></h4><p><strong>TDG (Tool Dependency Graph) Automaton</strong> could represent an agent that navigates the tool graph to solve image editing tasks. It will explore various tool paths using prior knowledge and benchmarks to enhance efficiency and quality, aiming to find the optimal sequence of tools to achieve the desired result. An <em><em>A</em> search</em>* algorithm would guide the search on the graph.</p><h4 class="relative group">Human > CLIP<div id=human--clip class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#human--clip aria-label=Anchor>#</a></span></h4><p>When assessing the correlation between human evaluations and CLIP scores, several insights emerge. It appears human evaluation is still crucial because CLIP might <strong>not capture nuanced inaccuracies</strong>. This is key, as relying solely on CLIP could lead to overlooking critical details or contextual understanding that a human evaluator would readily identify. The low correlation could be because CLIP might <strong>struggle with complex tasks</strong>, making it unreliable for holistic assessment but useful for individual subtasks. <strong>Humans excel</strong> where CLIP falls short because humans perform a more thorough job especially for nuanced multi-step and multi-modal</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.10613/x2.png alt></figure></p><blockquote><p>üîº Figure 2 showcases a comparison of CoSTA* against several state-of-the-art image editing models and agents. Three complex multi-turn image editing tasks are presented. For each task, the input image and user prompt are displayed alongside the results generated by CoSTA* and four other methods: GenArtist, MagicBrush, InstructPix2Pix, and CLOVA. The comparison highlights the differences in accuracy, visual coherence, and the ability of each method to handle multimodal tasks. CoSTA* demonstrates superior performance across all three tasks, showing a greater ability to correctly interpret and execute the complex, multi-step instructions.</p><details><summary>read the caption</summary>Figure 2: Comparison of CoSTA‚àó with State-of-the-Art image editing models/agents, which include GenArtist (Wang et¬†al., 2024b), MagicBrush (Zhang et¬†al., 2024a), InstructPix2Pix (Brooks et¬†al., 2023), and CLOVA (Gao et¬†al., 2024). The input images and prompts are shown on the left of the figure. The outputs generated by each method illustrate differences in accuracy, visual coherence, and the ability to multimodal tasks. Figure¬†9 shows examples of step-by-step editing using CoSTA‚àówith intermediate subtask outputs presented.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.10613/x3.png alt></figure></p><blockquote><p>üîº This figure compares three different approaches to multi-turn image editing task planning: LLM-only planning, traditional search algorithms (like A*), and the proposed CoSTA* method. LLM-only planning is fast but unreliable due to limitations in its understanding of tool capabilities and costs, leading to suboptimal plans and frequent failures. A* search guarantees optimal solutions but is computationally expensive and impractical for complex tasks with numerous tools. CoSTA* combines the strengths of both by using an LLM to generate a subtask tree, which significantly reduces the search space for A*. Then, A* is applied to this smaller, more manageable subgraph to find a cost-efficient toolpath that balances quality and cost. This hybrid approach allows CoSTA* to achieve efficient and high-quality results.</p><details><summary>read the caption</summary>Figure 3: Comparison of CoSTA‚àó with other planning agents. LLM-only planning is efficient but prone to failure and heuristics. Search algorithms like A‚àó guarantee optimal paths but are computationally expensive. CoSTA‚àó balances cost and quality by first pruning the subtask tree using an LLM, which reduces the graph of tools we conduct fine-grained A‚àó search on.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.10613/x4.png alt></figure></p><blockquote><p>üîº The figure depicts a Tool Dependency Graph (TDG), a directed acyclic graph that visually represents the relationships and dependencies between various AI tools used in multi-turn image editing. Each node in the graph represents a specific AI tool (e.g., an object detector, an image inpainter, a text generator), and a directed edge from node <code>v1</code> to node <code>v2</code> signifies that the output of tool <code>v1</code> serves as a valid input for tool <code>v2</code>. This graph is crucial for the CoSTA* algorithm because it allows for efficient searching of optimal toolpaths (sequences of tools) to accomplish complex image editing tasks described in natural language. The TDG facilitates the selection of appropriate tools for each subtask within a multi-turn image editing workflow, considering the dependencies between tools to ensure a seamless and logical editing process.</p><details><summary>read the caption</summary>Figure 4: Tool Dependency Graph (TDG). A directed graph where nodes represent tools and edges indicate dependencies. An edge (v1,v2)subscriptùë£1subscriptùë£2(v_{1},v_{2})( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) means v1subscriptùë£1v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT‚Äôs output is a legal input of v2subscriptùë£2v_{2}italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. It enables toolpath search for multi-turn image-editing tasks with composite instructions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.10613/x5.png alt></figure></p><blockquote><p>üîº This figure illustrates the three-stage process of the CoSTA* algorithm for multi-turn image editing. Stage 1 shows an LLM generating a subtask tree from the input image and task instructions, breaking down the complex task into smaller, manageable subtasks. Each branch of the tree represents a sequence of tool uses to achieve the overall goal. Stage 2 shows how the LLM&rsquo;s subtask tree translates into a tool subgraph, which is a subset of the complete tool dependency graph. This subgraph only includes the tools and relationships necessary for executing the subtasks identified in Stage 1, making the search space smaller and more efficient. Finally, Stage 3 depicts an A* search algorithm operating on this tool subgraph. The A* search uses a cost function that balances execution time (efficiency) and output quality to find the optimal path (sequence of tool uses) through the subgraph, ultimately leading to the final edited image.</p><details><summary>read the caption</summary>Figure 5: Three stages in CoSTA‚àó: (1) an LLM generates a subtask tree based on the input and task dependencies; (2) the subtask tree spans a tool subgraph that maintains tool dependencies; and (3) A‚àó search finds the best toolpath balancing efficiency and quality.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.10613/x6.png alt></figure></p><blockquote><p>üîº This figure presents a breakdown of the benchmark dataset used to evaluate CoSTA* and other image editing methods. The leftmost panel shows the distribution of tasks involving only image manipulation, categorized by the number of subtasks. The middle panel displays the distribution of tasks involving both image and text manipulation. The rightmost panel compares the performance of various models, including CoSTA*, across these tasks. It highlights CoSTA*&rsquo;s superior performance on complex multi-modal tasks where image and text editing is required, outperforming all baseline methods.</p><details><summary>read the caption</summary>Figure 6: Distribution of image-only (left) and text+image tasks (middle) in our proposed benchmark, and quality comparison of different methods on the benchmark (right). CoSTA‚àó excels in complex multimodal tasks and outperforms all the baselines.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.10613/x7.png alt></figure></p><blockquote><p>üîº Figure 7 demonstrates the impact of incorporating real-time feedback (g(x)) into the A* search algorithm within CoSTA*. The left side shows the results when only the heuristic cost (h(x)) is used to guide path selection, while the right side shows the results when both the heuristic and the actual execution cost (h(x) + g(x)) are considered. The figure visually highlights how the real-time feedback mechanism improves the algorithm&rsquo;s ability to select more efficient paths that also deliver higher quality results. Specifically, it shows how integrating g(x) leads to a path closer to the Pareto-optimal front (where both quality and cost are effectively balanced).</p><details><summary>read the caption</summary>Figure 7: Comparison of a task with h‚Å¢(x)‚Ñéùë•h(x)italic_h ( italic_x ) and h‚Å¢(x)+g‚Å¢(x)‚Ñéùë•ùëîùë•h(x)+g(x)italic_h ( italic_x ) + italic_g ( italic_x ), showing how real-time feedback improves path selection and execution.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.10613/x8.png alt></figure></p><blockquote><p>üîº Figure 8 showcases a qualitative comparison between traditional image editing tools and CoSTA* when handling tasks involving text manipulation within images. The figure demonstrates CoSTA*&rsquo;s superior performance in maintaining both visual quality and textual accuracy. Traditional methods often struggle to accurately edit text within an image while preserving other visual elements, leading to inconsistencies. In contrast, CoSTA*&rsquo;s multimodal approach, integrating multiple specialized models, enables more precise and comprehensive text editing within the image, while preserving other visual elements. This highlights the effectiveness of CoSTA*&rsquo;s approach in preserving both visual and textual fidelity.</p><details><summary>read the caption</summary>Figure 8: Qualitative comparison of image editing tools vs. CoSTA‚àó for text-based tasks, highlighting the advantages of our multimodal support in preserving visual and textual fidelity.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.10613/x9.png alt></figure></p><blockquote><p>üîº This figure showcases the step-by-step process of CoSTA* in handling multi-turn image editing tasks. Each row displays an example, starting with the initial image and prompt. Then, the subtasks identified by CoSTA* are shown, along with the intermediate outputs generated by each subtask&rsquo;s execution. This visualization demonstrates the sequential nature of CoSTA*&rsquo;s operations. Each step refines the output, leading to the final edited image. This step-by-step approach highlights CoSTA*&rsquo;s ability to systematically improve accuracy and consistency, especially in complex tasks involving multiple modalities (text and image).</p><details><summary>read the caption</summary>Figure 9: Step-by-step execution of editing tasks using CoSTA‚àó. Each row illustrates an input image, the corresponding subtask breakdown, and intermediate outputs at different stages of the editing process. This visualization highlights how CoSTA‚àó systematically refines outputs by leveraging specialized models for each subtask, ensuring greater accuracy and consistency in multimodal tasks.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.10613/x10.png alt></figure></p><blockquote><p>üîº This bar chart visualizes the frequency distribution of various subtasks within the benchmark dataset used in the CoSTA* model evaluation. Each bar represents a specific subtask (e.g., object detection, text replacement, image upscaling), and its height indicates the number of instances of that subtask present in the dataset. This figure provides insights into the dataset composition, showing which subtasks are more frequent and which are less frequent. This is useful for understanding the types of image editing tasks the dataset covers and the relative difficulty or complexity of those tasks.</p><details><summary>read the caption</summary>Figure 10: Distribution of the number of instances for each subtask in the dataset.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.10613/x11.png alt></figure></p><blockquote><p>üîº Figure 11 presents a curated selection of image editing tasks from the proposed benchmark dataset. The top half showcases examples involving solely image manipulation, highlighting the range of complexity from simple object edits to more involved scene changes. The bottom half demonstrates tasks requiring both image and text editing. The examples illustrate the multifaceted nature of multi-turn image editing, encompassing modifications such as object removal, addition, recoloring, text manipulation, scene alterations, and more, underscoring the diverse challenges addressed by the CoSTA* method.</p><details><summary>read the caption</summary>Figure 11: An overview of the dataset used for evaluation, showcasing representative input images and prompts across different task categories. The top section presents examples from image-only tasks, while the bottom section includes text+image tasks. These examples illustrate the diversity of tasks in our dataset, highlighting the range of modifications required for both visual and multimodal editing scenarios.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.10613/x12.png alt></figure></p><blockquote><p>üîº This figure illustrates the retry mechanism used in CoSTA*. When a node (representing a tool-subtask pair) fails to meet the quality threshold during the A* search, the retry mechanism is triggered. The diagram shows the adjustments made to the cost and quality of the node, and the re-evaluation of potential paths to completion. The &lsquo;Retry&rsquo; arrow indicates that the process repeats, attempting to fulfill the subtask by adjusting parameters or exploring alternative models. If the subtask still fails to meet the quality threshold after a retry, this node is excluded from further consideration. The figures demonstrate a case where the initial path is blocked by a node failing the quality check. Then, the A* search is re-triggered, this time avoiding the failed node, to explore other possible paths.</p><details><summary>read the caption</summary>Figure 12: Visualization of the Retry Mechanism</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.10613/x13.png alt></figure></p><blockquote><p>üîº This scatter plot visualizes the correlation between CLIP similarity scores and human-evaluated accuracy across 40 multi-turn image editing tasks. Each point represents a single task, plotting its CLIP score against its human-assigned accuracy score. The weak positive correlation (Spearman&rsquo;s œÅ = 0.59, Kendall&rsquo;s œÑ = 0.47) indicates that CLIP scores are not a reliable predictor of human judgment, especially for complex, multi-step tasks. The plot demonstrates that high CLIP scores do not guarantee high human accuracy, highlighting CLIP&rsquo;s limitations in capturing subtle errors or nuances often present in complex editing scenarios.</p><details><summary>read the caption</summary>Figure 13: Scatter plot of CLIP scores vs. human accuracy across 40 tasks. The weak correlation (Spearman‚Äôs œÅ=0.59ùúå0.59\rho=0.59italic_œÅ = 0.59, Kendall‚Äôs œÑ=0.47ùúè0.47\tau=0.47italic_œÑ = 0.47) highlights CLIP‚Äôs limitations in capturing nuanced inaccuracies, particularly in complex, multi-step tasks.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S5.T2.7.1><thead class=ltx_thead><tr class=ltx_tr id=S5.T2.7.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=S5.T2.7.1.1.2><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.1.2.1 style=font-size:90%>Task Type</span></th><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=S5.T2.7.1.1.3><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.1.3.1 style=font-size:90%>Task Category</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T2.7.1.1.1><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.1.1.1 style=font-size:90%>CoSTA<sup class=ltx_sup id=S5.T2.7.1.1.1.1.1><span class="ltx_text ltx_font_medium" id=S5.T2.7.1.1.1.1.1.1>‚àó</span></sup></span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T2.7.1.1.4><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.1.4.1 style=font-size:90%>VisProg</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T2.7.1.1.5><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.1.5.1 style=font-size:90%>CLOVA</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T2.7.1.1.6><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.1.6.1 style=font-size:90%>GenArtist</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T2.7.1.1.7><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.1.7.1 style=font-size:90%>Instruct Pix2Pix</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T2.7.1.1.8><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.1.8.1 style=font-size:90%>MagicBrush</span></th></tr><tr class=ltx_tr id=S5.T2.7.1.2.1><th class="ltx_td ltx_th ltx_th_row" id=S5.T2.7.1.2.1.1></th><th class="ltx_td ltx_th ltx_th_column ltx_th_row" id=S5.T2.7.1.2.1.2></th><th class="ltx_td ltx_th ltx_th_column" id=S5.T2.7.1.2.1.3></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column" id=S5.T2.7.1.2.1.4><cite class="ltx_cite ltx_citemacro_citep"><span class=ltx_text id=S5.T2.7.1.2.1.4.1.1 style=font-size:70%>(</span>Gupta & Kembhavi<span class=ltx_text id=S5.T2.7.1.2.1.4.2.2.1.1 style=font-size:70%>, </span><a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib10 title>2023</a><span class=ltx_text id=S5.T2.7.1.2.1.4.3.3 style=font-size:70%>)</span></cite></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column" id=S5.T2.7.1.2.1.5><cite class="ltx_cite ltx_citemacro_citep"><span class=ltx_text id=S5.T2.7.1.2.1.5.1.1 style=font-size:70%>(</span>Gao et¬†al.<span class=ltx_text id=S5.T2.7.1.2.1.5.2.2.1.1 style=font-size:70%>, </span><a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib8 title>2024</a><span class=ltx_text id=S5.T2.7.1.2.1.5.3.3 style=font-size:70%>)</span></cite></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column" id=S5.T2.7.1.2.1.6><cite class="ltx_cite ltx_citemacro_citep"><span class=ltx_text id=S5.T2.7.1.2.1.6.1.1 style=font-size:70%>(</span>Wang et¬†al.<span class=ltx_text id=S5.T2.7.1.2.1.6.2.2.1.1 style=font-size:70%>, </span><a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib36 title>2024b</a><span class=ltx_text id=S5.T2.7.1.2.1.6.3.3 style=font-size:70%>)</span></cite></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column" id=S5.T2.7.1.2.1.7><cite class="ltx_cite ltx_citemacro_citep"><span class=ltx_text id=S5.T2.7.1.2.1.7.1.1 style=font-size:70%>(</span>Brooks et¬†al.<span class=ltx_text id=S5.T2.7.1.2.1.7.2.2.1.1 style=font-size:70%>, </span><a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib3 title>2023</a><span class=ltx_text id=S5.T2.7.1.2.1.7.3.3 style=font-size:70%>)</span></cite></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column" id=S5.T2.7.1.2.1.8><cite class="ltx_cite ltx_citemacro_citep"><span class=ltx_text id=S5.T2.7.1.2.1.8.1.1 style=font-size:70%>(</span>Zhang et¬†al.<span class=ltx_text id=S5.T2.7.1.2.1.8.2.2.1.1 style=font-size:70%>, </span><a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib41 title>2023a</a><span class=ltx_text id=S5.T2.7.1.2.1.8.3.3 style=font-size:70%>)</span></cite></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T2.7.1.3.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S5.T2.7.1.3.1.1 rowspan=4><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.3.1.1.1 style=font-size:90%>Image-Only Tasks</span></th><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S5.T2.7.1.3.1.2><span class=ltx_text id=S5.T2.7.1.3.1.2.1 style=font-size:90%>1‚Äì2 subtasks</span></th><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.3.1.3><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.3.1.3.1 style=font-size:90%>0.94</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.3.1.4><span class=ltx_text id=S5.T2.7.1.3.1.4.1 style=font-size:90%>0.88</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.3.1.5><span class=ltx_text id=S5.T2.7.1.3.1.5.1 style=font-size:90%>0.91</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.3.1.6><span class=ltx_text id=S5.T2.7.1.3.1.6.1 style=font-size:90%>0.93</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.3.1.7><span class=ltx_text id=S5.T2.7.1.3.1.7.1 style=font-size:90%>0.87</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.3.1.8><span class=ltx_text id=S5.T2.7.1.3.1.8.1 style=font-size:90%>0.92</span></td></tr><tr class=ltx_tr id=S5.T2.7.1.4.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S5.T2.7.1.4.2.1><span class=ltx_text id=S5.T2.7.1.4.2.1.1 style=font-size:90%>3‚Äì4 subtasks</span></th><td class="ltx_td ltx_align_center" id=S5.T2.7.1.4.2.2><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.4.2.2.1 style=font-size:90%>0.93</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.4.2.3><span class=ltx_text id=S5.T2.7.1.4.2.3.1 style=font-size:90%>0.76</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.4.2.4><span class=ltx_text id=S5.T2.7.1.4.2.4.1 style=font-size:90%>0.77</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.4.2.5><span class=ltx_text id=S5.T2.7.1.4.2.5.1 style=font-size:90%>0.85</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.4.2.6><span class=ltx_text id=S5.T2.7.1.4.2.6.1 style=font-size:90%>0.74</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.4.2.7><span class=ltx_text id=S5.T2.7.1.4.2.7.1 style=font-size:90%>0.78</span></td></tr><tr class=ltx_tr id=S5.T2.7.1.5.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S5.T2.7.1.5.3.1><span class=ltx_text id=S5.T2.7.1.5.3.1.1 style=font-size:90%>5‚Äì6 subtasks</span></th><td class="ltx_td ltx_align_center" id=S5.T2.7.1.5.3.2><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.5.3.2.1 style=font-size:90%>0.93</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.5.3.3><span class=ltx_text id=S5.T2.7.1.5.3.3.1 style=font-size:90%>0.62</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.5.3.4><span class=ltx_text id=S5.T2.7.1.5.3.4.1 style=font-size:90%>0.63</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.5.3.5><span class=ltx_text id=S5.T2.7.1.5.3.5.1 style=font-size:90%>0.71</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.5.3.6><span class=ltx_text id=S5.T2.7.1.5.3.6.1 style=font-size:90%>0.55</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.5.3.7><span class=ltx_text id=S5.T2.7.1.5.3.7.1 style=font-size:90%>0.51</span></td></tr><tr class=ltx_tr id=S5.T2.7.1.6.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S5.T2.7.1.6.4.1><span class=ltx_text id=S5.T2.7.1.6.4.1.1 style=font-size:90%>7‚Äì8 subtasks</span></th><td class="ltx_td ltx_align_center" id=S5.T2.7.1.6.4.2><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.6.4.2.1 style=font-size:90%>0.95</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.6.4.3><span class=ltx_text id=S5.T2.7.1.6.4.3.1 style=font-size:90%>0.46</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.6.4.4><span class=ltx_text id=S5.T2.7.1.6.4.4.1 style=font-size:90%>0.45</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.6.4.5><span class=ltx_text id=S5.T2.7.1.6.4.5.1 style=font-size:90%>0.61</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.6.4.6><span class=ltx_text id=S5.T2.7.1.6.4.6.1 style=font-size:90%>0.38</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.6.4.7><span class=ltx_text id=S5.T2.7.1.6.4.7.1 style=font-size:90%>0.46</span></td></tr><tr class=ltx_tr id=S5.T2.7.1.7.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S5.T2.7.1.7.5.1 rowspan=3><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.7.5.1.1 style=font-size:90%>Text+Image Tasks</span></th><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S5.T2.7.1.7.5.2><span class=ltx_text id=S5.T2.7.1.7.5.2.1 style=font-size:90%>2‚Äì3 subtasks</span></th><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.7.5.3><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.7.5.3.1 style=font-size:90%>0.93</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.7.5.4><span class=ltx_text id=S5.T2.7.1.7.5.4.1 style=font-size:90%>0.61</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.7.5.5><span class=ltx_text id=S5.T2.7.1.7.5.5.1 style=font-size:90%>0.63</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.7.5.6><span class=ltx_text id=S5.T2.7.1.7.5.6.1 style=font-size:90%>0.67</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.7.5.7><span class=ltx_text id=S5.T2.7.1.7.5.7.1 style=font-size:90%>0.48</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.7.5.8><span class=ltx_text id=S5.T2.7.1.7.5.8.1 style=font-size:90%>0.62</span></td></tr><tr class=ltx_tr id=S5.T2.7.1.8.6><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S5.T2.7.1.8.6.1><span class=ltx_text id=S5.T2.7.1.8.6.1.1 style=font-size:90%>4‚Äì5 subtasks</span></th><td class="ltx_td ltx_align_center" id=S5.T2.7.1.8.6.2><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.8.6.2.1 style=font-size:90%>0.94</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.8.6.3><span class=ltx_text id=S5.T2.7.1.8.6.3.1 style=font-size:90%>0.50</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.8.6.4><span class=ltx_text id=S5.T2.7.1.8.6.4.1 style=font-size:90%>0.51</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.8.6.5><span class=ltx_text id=S5.T2.7.1.8.6.5.1 style=font-size:90%>0.61</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.8.6.6><span class=ltx_text id=S5.T2.7.1.8.6.6.1 style=font-size:90%>0.42</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.8.6.7><span class=ltx_text id=S5.T2.7.1.8.6.7.1 style=font-size:90%>0.40</span></td></tr><tr class=ltx_tr id=S5.T2.7.1.9.7><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S5.T2.7.1.9.7.1><span class=ltx_text id=S5.T2.7.1.9.7.1.1 style=font-size:90%>6‚Äì8 subtasks</span></th><td class="ltx_td ltx_align_center" id=S5.T2.7.1.9.7.2><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.9.7.2.1 style=font-size:90%>0.94</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.9.7.3><span class=ltx_text id=S5.T2.7.1.9.7.3.1 style=font-size:90%>0.38</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.9.7.4><span class=ltx_text id=S5.T2.7.1.9.7.4.1 style=font-size:90%>0.36</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.9.7.5><span class=ltx_text id=S5.T2.7.1.9.7.5.1 style=font-size:90%>0.56</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.9.7.6><span class=ltx_text id=S5.T2.7.1.9.7.6.1 style=font-size:90%>0.31</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.9.7.7><span class=ltx_text id=S5.T2.7.1.9.7.7.1 style=font-size:90%>0.26</span></td></tr><tr class=ltx_tr id=S5.T2.7.1.10.8><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id=S5.T2.7.1.10.8.1 rowspan=3><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.10.8.1.1 style=font-size:90%>Overall Accuracy</span></th><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S5.T2.7.1.10.8.2><span class=ltx_text id=S5.T2.7.1.10.8.2.1 style=font-size:90%>Image Tasks</span></th><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.10.8.3><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.10.8.3.1 style=font-size:90%>0.94</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.10.8.4><span class=ltx_text id=S5.T2.7.1.10.8.4.1 style=font-size:90%>0.69</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.10.8.5><span class=ltx_text id=S5.T2.7.1.10.8.5.1 style=font-size:90%>0.70</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.10.8.6><span class=ltx_text id=S5.T2.7.1.10.8.6.1 style=font-size:90%>0.78</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.10.8.7><span class=ltx_text id=S5.T2.7.1.10.8.7.1 style=font-size:90%>0.64</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.1.10.8.8><span class=ltx_text id=S5.T2.7.1.10.8.8.1 style=font-size:90%>0.67</span></td></tr><tr class=ltx_tr id=S5.T2.7.1.11.9><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S5.T2.7.1.11.9.1><span class=ltx_text id=S5.T2.7.1.11.9.1.1 style=font-size:90%>Text+Image Tasks</span></th><td class="ltx_td ltx_align_center" id=S5.T2.7.1.11.9.2><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.11.9.2.1 style=font-size:90%>0.93</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.11.9.3><span class=ltx_text id=S5.T2.7.1.11.9.3.1 style=font-size:90%>0.49</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.11.9.4><span class=ltx_text id=S5.T2.7.1.11.9.4.1 style=font-size:90%>0.50</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.11.9.5><span class=ltx_text id=S5.T2.7.1.11.9.5.1 style=font-size:90%>0.61</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.11.9.6><span class=ltx_text id=S5.T2.7.1.11.9.6.1 style=font-size:90%>0.40</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.1.11.9.7><span class=ltx_text id=S5.T2.7.1.11.9.7.1 style=font-size:90%>0.43</span></td></tr><tr class=ltx_tr id=S5.T2.7.1.12.10><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S5.T2.7.1.12.10.1><span class=ltx_text id=S5.T2.7.1.12.10.1.1 style=font-size:90%>All Tasks</span></th><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.7.1.12.10.2><span class="ltx_text ltx_font_bold" id=S5.T2.7.1.12.10.2.1 style=font-size:90%>0.94</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.7.1.12.10.3><span class=ltx_text id=S5.T2.7.1.12.10.3.1 style=font-size:90%>0.62</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.7.1.12.10.4><span class=ltx_text id=S5.T2.7.1.12.10.4.1 style=font-size:90%>0.63</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.7.1.12.10.5><span class=ltx_text id=S5.T2.7.1.12.10.5.1 style=font-size:90%>0.73</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.7.1.12.10.6><span class=ltx_text id=S5.T2.7.1.12.10.6.1 style=font-size:90%>0.56</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.7.1.12.10.7><span class=ltx_text id=S5.T2.7.1.12.10.7.1 style=font-size:90%>0.59</span></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 2 presents a detailed comparison of the accuracy achieved by the proposed CoSTA* model against several state-of-the-art baseline methods. The comparison is broken down by task type (image-only vs. text+image) and further categorized by the number of subtasks involved in each task (1-2, 3-4, 5-6, 7-8). This breakdown allows for a nuanced analysis of how each method performs under varying levels of complexity. The results highlight CoSTA*&rsquo;s superior performance, especially in complex scenarios with multiple subtasks and both image and text data. This demonstrates the efficacy of the CoSTA* approach in managing challenging, multi-step image editing tasks.</p><details><summary>read the caption</summary>Table 2: Accuracy comparison of CoSTA‚àó with baselines across task types and categories. CoSTA‚àó excels in complex workflows with A‚àó search and a diverse set of tools, ensuring higher accuracy.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S5.T3.1.1><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T3.1.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=S5.T3.1.1.1.1.1>Metric</th><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.1.1.1.1.2>CLIP Similarity Score</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.1.1.1.1.3>Human Evaluation Accuracy</td></tr><tr class=ltx_tr id=S5.T3.1.1.2.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id=S5.T3.1.1.2.2.1>Average (50 Tasks)</th><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T3.1.1.2.2.2>0.96</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T3.1.1.2.2.3>0.78</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of CLIP (Contrastive Language‚ÄìImage Pre-training) similarity scores against human evaluation scores for 50 image editing tasks. The tasks involved multimodal and multi-step edits, meaning they required multiple AI tools to complete and combined image and text modifications. The comparison highlights the limitations of using CLIP similarity as a sole metric for assessing the quality of complex image manipulations, particularly those involving semantic nuances and holistic image understanding, where human evaluation is necessary for more accurate assessment.</p><details><summary>read the caption</summary>Table 3: Comparison of CLIP Similarity vs. Human Evaluation on 50 tasks to assess CLIP similarity against human judgments in multimodal and multi-step editing.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S5.T4.5><thead class=ltx_thead><tr class=ltx_tr id=S5.T4.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id=S5.T4.1.1.2>Metric</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T4.1.1.3>Correlation Coefficient</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T4.1.1.1><math alttext="p" class="ltx_Math" display="inline" id="S5.T4.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.m1.1a"><mi id="S5.T4.1.1.1.m1.1.1" xref="S5.T4.1.1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1">ùëù</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.m1.1d">italic_p</annotation></semantics></math>-value</th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T4.3.3><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T4.2.2.1>Spearman‚Äôs<math alttext="\rho" class="ltx_Math" display="inline" id="S5.T4.2.2.1.m1.1"><semantics id="S5.T4.2.2.1.m1.1a"><mi id="S5.T4.2.2.1.m1.1.1" xref="S5.T4.2.2.1.m1.1.1.cmml">œÅ</mi><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.1.m1.1b"><ci id="S5.T4.2.2.1.m1.1.1.cmml" xref="S5.T4.2.2.1.m1.1.1">ùúå</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.1.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.1.m1.1d">italic_œÅ</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.3.3.3>0.59</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.3.3.2><math alttext="6.07\times 10^{-5}" class="ltx_Math" display="inline" id="S5.T4.3.3.2.m1.1"><semantics id="S5.T4.3.3.2.m1.1a"><mrow id="S5.T4.3.3.2.m1.1.1" xref="S5.T4.3.3.2.m1.1.1.cmml"><mn id="S5.T4.3.3.2.m1.1.1.2" xref="S5.T4.3.3.2.m1.1.1.2.cmml">6.07</mn><mo id="S5.T4.3.3.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.T4.3.3.2.m1.1.1.1.cmml">√ó</mo><msup id="S5.T4.3.3.2.m1.1.1.3" xref="S5.T4.3.3.2.m1.1.1.3.cmml"><mn id="S5.T4.3.3.2.m1.1.1.3.2" xref="S5.T4.3.3.2.m1.1.1.3.2.cmml">10</mn><mrow id="S5.T4.3.3.2.m1.1.1.3.3" xref="S5.T4.3.3.2.m1.1.1.3.3.cmml"><mo id="S5.T4.3.3.2.m1.1.1.3.3a" xref="S5.T4.3.3.2.m1.1.1.3.3.cmml">‚àí</mo><mn id="S5.T4.3.3.2.m1.1.1.3.3.2" xref="S5.T4.3.3.2.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.2.m1.1b"><apply id="S5.T4.3.3.2.m1.1.1.cmml" xref="S5.T4.3.3.2.m1.1.1"><times id="S5.T4.3.3.2.m1.1.1.1.cmml" xref="S5.T4.3.3.2.m1.1.1.1"></times><cn id="S5.T4.3.3.2.m1.1.1.2.cmml" type="float" xref="S5.T4.3.3.2.m1.1.1.2">6.07</cn><apply id="S5.T4.3.3.2.m1.1.1.3.cmml" xref="S5.T4.3.3.2.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T4.3.3.2.m1.1.1.3.1.cmml" xref="S5.T4.3.3.2.m1.1.1.3">superscript</csymbol><cn id="S5.T4.3.3.2.m1.1.1.3.2.cmml" type="integer" xref="S5.T4.3.3.2.m1.1.1.3.2">10</cn><apply id="S5.T4.3.3.2.m1.1.1.3.3.cmml" xref="S5.T4.3.3.2.m1.1.1.3.3"><minus id="S5.T4.3.3.2.m1.1.1.3.3.1.cmml" xref="S5.T4.3.3.2.m1.1.1.3.3"></minus><cn id="S5.T4.3.3.2.m1.1.1.3.3.2.cmml" type="integer" xref="S5.T4.3.3.2.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.2.m1.1c">6.07\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S5.T4.3.3.2.m1.1d">6.07 √ó 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math></td></tr><tr class=ltx_tr id=S5.T4.5.5><td class="ltx_td ltx_align_left ltx_border_bb" id=S5.T4.4.4.1>Kendall‚Äôs<math alttext="\tau" class="ltx_Math" display="inline" id="S5.T4.4.4.1.m1.1"><semantics id="S5.T4.4.4.1.m1.1a"><mi id="S5.T4.4.4.1.m1.1.1" xref="S5.T4.4.4.1.m1.1.1.cmml">œÑ</mi><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.1.m1.1b"><ci id="S5.T4.4.4.1.m1.1.1.cmml" xref="S5.T4.4.4.1.m1.1.1">ùúè</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S5.T4.4.4.1.m1.1d">italic_œÑ</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.5.5.3>0.47</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.5.5.2><math alttext="5.83\times 10^{-5}" class="ltx_Math" display="inline" id="S5.T4.5.5.2.m1.1"><semantics id="S5.T4.5.5.2.m1.1a"><mrow id="S5.T4.5.5.2.m1.1.1" xref="S5.T4.5.5.2.m1.1.1.cmml"><mn id="S5.T4.5.5.2.m1.1.1.2" xref="S5.T4.5.5.2.m1.1.1.2.cmml">5.83</mn><mo id="S5.T4.5.5.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.T4.5.5.2.m1.1.1.1.cmml">√ó</mo><msup id="S5.T4.5.5.2.m1.1.1.3" xref="S5.T4.5.5.2.m1.1.1.3.cmml"><mn id="S5.T4.5.5.2.m1.1.1.3.2" xref="S5.T4.5.5.2.m1.1.1.3.2.cmml">10</mn><mrow id="S5.T4.5.5.2.m1.1.1.3.3" xref="S5.T4.5.5.2.m1.1.1.3.3.cmml"><mo id="S5.T4.5.5.2.m1.1.1.3.3a" xref="S5.T4.5.5.2.m1.1.1.3.3.cmml">‚àí</mo><mn id="S5.T4.5.5.2.m1.1.1.3.3.2" xref="S5.T4.5.5.2.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.5.5.2.m1.1b"><apply id="S5.T4.5.5.2.m1.1.1.cmml" xref="S5.T4.5.5.2.m1.1.1"><times id="S5.T4.5.5.2.m1.1.1.1.cmml" xref="S5.T4.5.5.2.m1.1.1.1"></times><cn id="S5.T4.5.5.2.m1.1.1.2.cmml" type="float" xref="S5.T4.5.5.2.m1.1.1.2">5.83</cn><apply id="S5.T4.5.5.2.m1.1.1.3.cmml" xref="S5.T4.5.5.2.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T4.5.5.2.m1.1.1.3.1.cmml" xref="S5.T4.5.5.2.m1.1.1.3">superscript</csymbol><cn id="S5.T4.5.5.2.m1.1.1.3.2.cmml" type="integer" xref="S5.T4.5.5.2.m1.1.1.3.2">10</cn><apply id="S5.T4.5.5.2.m1.1.1.3.3.cmml" xref="S5.T4.5.5.2.m1.1.1.3.3"><minus id="S5.T4.5.5.2.m1.1.1.3.3.1.cmml" xref="S5.T4.5.5.2.m1.1.1.3.3"></minus><cn id="S5.T4.5.5.2.m1.1.1.3.3.2.cmml" type="integer" xref="S5.T4.5.5.2.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.5.2.m1.1c">5.83\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S5.T4.5.5.2.m1.1d">5.83 √ó 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a correlation analysis comparing the CLIP (Contrastive Language‚ÄìImage Pre-training) similarity scores against human evaluation scores for 40 image editing tasks. The analysis reveals a weak correlation between automatic CLIP scores and human judgments, highlighting the limitations of relying solely on CLIP for evaluating the quality and accuracy of complex image editing tasks. Human evaluation remains essential for capturing nuanced aspects of image quality, including subtle errors and semantic coherence issues that automatic metrics often miss. The table shows the correlation coefficients (Spearman&rsquo;s œÅ and Kendall&rsquo;s œÑ) and their corresponding p-values to quantify the strength and statistical significance of the correlation between CLIP and human scores.</p><details><summary>read the caption</summary>Table 4: Correlation Analysis of CLIP vs Human Evaluation on 40 tasks, which indicates that human evaluation is still necessary.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S5.T5.3.1><thead class=ltx_thead><tr class=ltx_tr id=S5.T5.3.1.1><th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id=S5.T5.3.1.1.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=S5.T5.3.1.1.2.1><span class=ltx_p id=S5.T5.3.1.1.2.1.1 style=width:216.8pt><span class="ltx_text ltx_font_bold" id=S5.T5.3.1.1.2.1.1.1>Feature</span></span></span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T5.3.1.1.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S5.T5.3.1.1.1.1>CoSTA<sup class=ltx_sup id=S5.T5.3.1.1.1.1.1><span class="ltx_text ltx_font_medium" id=S5.T5.3.1.1.1.1.1.1>‚àó</span></sup></span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T5.3.1.1.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S5.T5.3.1.1.3.1>CLOVA</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T5.3.1.1.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S5.T5.3.1.1.4.1>GenArtist</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T5.3.1.1.5 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S5.T5.3.1.1.5.1>VisProg</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T5.3.1.1.6 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S5.T5.3.1.1.6.1>Instruct Pix2Pix</span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T5.3.1.2.1><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=S5.T5.3.1.2.1.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=S5.T5.3.1.2.1.1.1><span class=ltx_p id=S5.T5.3.1.2.1.1.1.1 style=width:216.8pt>Integration of LLM with A* Path Optimization</span></span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T5.3.1.2.1.2 style=padding-top:1pt;padding-bottom:1pt>‚úì</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T5.3.1.2.1.3 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T5.3.1.2.1.4 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T5.3.1.2.1.5 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T5.3.1.2.1.6 style=padding-top:1pt;padding-bottom:1pt>√ó</td></tr><tr class=ltx_tr id=S5.T5.3.1.3.2><td class="ltx_td ltx_align_justify ltx_align_top" id=S5.T5.3.1.3.2.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=S5.T5.3.1.3.2.1.1><span class=ltx_p id=S5.T5.3.1.3.2.1.1.1 style=width:216.8pt>Automatic Feedback Integration (Real-Time)</span></span></td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.3.2.2 style=padding-top:1pt;padding-bottom:1pt>‚úì</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.3.2.3 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.3.2.4 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.3.2.5 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.3.2.6 style=padding-top:1pt;padding-bottom:1pt>√ó</td></tr><tr class=ltx_tr id=S5.T5.3.1.4.3><td class="ltx_td ltx_align_justify ltx_align_top" id=S5.T5.3.1.4.3.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=S5.T5.3.1.4.3.1.1><span class=ltx_p id=S5.T5.3.1.4.3.1.1.1 style=width:216.8pt>Real-Time Cost-Quality Tradeoff</span></span></td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.4.3.2 style=padding-top:1pt;padding-bottom:1pt>‚úì</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.4.3.3 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.4.3.4 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.4.3.5 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.4.3.6 style=padding-top:1pt;padding-bottom:1pt>√ó</td></tr><tr class=ltx_tr id=S5.T5.3.1.5.4><td class="ltx_td ltx_align_justify ltx_align_top" id=S5.T5.3.1.5.4.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=S5.T5.3.1.5.4.1.1><span class=ltx_p id=S5.T5.3.1.5.4.1.1.1 style=width:216.8pt>User-Defined Cost-Quality Weightage</span></span></td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.5.4.2 style=padding-top:1pt;padding-bottom:1pt>‚úì</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.5.4.3 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.5.4.4 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.5.4.5 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.5.4.6 style=padding-top:1pt;padding-bottom:1pt>√ó</td></tr><tr class=ltx_tr id=S5.T5.3.1.6.5><td class="ltx_td ltx_align_justify ltx_align_top" id=S5.T5.3.1.6.5.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=S5.T5.3.1.6.5.1.1><span class=ltx_p id=S5.T5.3.1.6.5.1.1.1 style=width:216.8pt>Multimodality Support (Image + Text)</span></span></td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.6.5.2 style=padding-top:1pt;padding-bottom:1pt>‚úì</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.6.5.3 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.6.5.4 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.6.5.5 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.6.5.6 style=padding-top:1pt;padding-bottom:1pt>√ó</td></tr><tr class=ltx_tr id=S5.T5.3.1.7.6><td class="ltx_td ltx_align_justify ltx_align_top" id=S5.T5.3.1.7.6.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=S5.T5.3.1.7.6.1.1><span class=ltx_p id=S5.T5.3.1.7.6.1.1.1 style=width:216.8pt>Number of Tools for Task Accomplishment</span></span></td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.7.6.2 style=padding-top:1pt;padding-bottom:1pt>24</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.7.6.3 style=padding-top:1pt;padding-bottom:1pt>&lt;10</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.7.6.4 style=padding-top:1pt;padding-bottom:1pt>&lt;10</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.7.6.5 style=padding-top:1pt;padding-bottom:1pt>&lt;12</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.7.6.6 style=padding-top:1pt;padding-bottom:1pt>&lt;5</td></tr><tr class=ltx_tr id=S5.T5.3.1.8.7><td class="ltx_td ltx_align_justify ltx_align_top" id=S5.T5.3.1.8.7.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=S5.T5.3.1.8.7.1.1><span class=ltx_p id=S5.T5.3.1.8.7.1.1.1 style=width:216.8pt>Feedback-Based Retrying and Model Selection</span></span></td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.8.7.2 style=padding-top:1pt;padding-bottom:1pt>‚úì</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.8.7.3 style=padding-top:1pt;padding-bottom:1pt>‚úì</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.8.7.4 style=padding-top:1pt;padding-bottom:1pt>‚úì</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.8.7.5 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center" id=S5.T5.3.1.8.7.6 style=padding-top:1pt;padding-bottom:1pt>√ó</td></tr><tr class=ltx_tr id=S5.T5.3.1.9.8><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id=S5.T5.3.1.9.8.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=S5.T5.3.1.9.8.1.1><span class=ltx_p id=S5.T5.3.1.9.8.1.1.1 style=width:216.8pt>Dynamic Adjustment of Heuristic Values</span></span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T5.3.1.9.8.2 style=padding-top:1pt;padding-bottom:1pt>‚úì</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T5.3.1.9.8.3 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T5.3.1.9.8.4 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T5.3.1.9.8.5 style=padding-top:1pt;padding-bottom:1pt>√ó</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T5.3.1.9.8.6 style=padding-top:1pt;padding-bottom:1pt>√ó</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 5 presents a comparative analysis of key features across several image editing methods, including CoSTA*. The table highlights the substantial advantages of CoSTA* over other baselines. These advantages stem from CoSTA*&rsquo;s unique combination of features such as its integration of a large language model (LLM) with A* search, its capability for path optimization, its real-time feedback integration, and its support for both image and text-based editing. The table explicitly shows that CoSTA* possesses functionalities absent in other methods, such as the ability to perform cost-quality tradeoffs and dynamically adjust its heuristic values. This comprehensive feature set contributes to CoSTA*&rsquo;s superior performance in complex image editing tasks.</p><details><summary>read the caption</summary>Table 5: Comparison of key features across methods, highlighting the extensive set of capabilities supported by CoSTA‚àó, which are absent in baselines and contribute to its superior performance.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S5.T6.6.2><thead class=ltx_thead><tr class=ltx_tr id=S5.T6.6.2.3.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=S5.T6.6.2.3.1.1 style=padding-top:1pt;padding-bottom:1pt>Approach</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T6.6.2.3.1.2 style=padding-top:1pt;padding-bottom:1pt>Average Accuracy</th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T6.5.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S5.T6.5.1.1.1 style=padding-top:1pt;padding-bottom:1pt><math alttext="h(x)" class="ltx_Math" display="inline" id="S5.T6.5.1.1.1.m1.1"><semantics id="S5.T6.5.1.1.1.m1.1a"><mrow id="S5.T6.5.1.1.1.m1.1.2" xref="S5.T6.5.1.1.1.m1.1.2.cmml"><mi id="S5.T6.5.1.1.1.m1.1.2.2" xref="S5.T6.5.1.1.1.m1.1.2.2.cmml">h</mi><mo id="S5.T6.5.1.1.1.m1.1.2.1" xref="S5.T6.5.1.1.1.m1.1.2.1.cmml">‚Å¢</mo><mrow id="S5.T6.5.1.1.1.m1.1.2.3.2" xref="S5.T6.5.1.1.1.m1.1.2.cmml"><mo id="S5.T6.5.1.1.1.m1.1.2.3.2.1" stretchy="false" xref="S5.T6.5.1.1.1.m1.1.2.cmml">(</mo><mi id="S5.T6.5.1.1.1.m1.1.1" xref="S5.T6.5.1.1.1.m1.1.1.cmml">x</mi><mo id="S5.T6.5.1.1.1.m1.1.2.3.2.2" stretchy="false" xref="S5.T6.5.1.1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T6.5.1.1.1.m1.1b"><apply id="S5.T6.5.1.1.1.m1.1.2.cmml" xref="S5.T6.5.1.1.1.m1.1.2"><times id="S5.T6.5.1.1.1.m1.1.2.1.cmml" xref="S5.T6.5.1.1.1.m1.1.2.1"></times><ci id="S5.T6.5.1.1.1.m1.1.2.2.cmml" xref="S5.T6.5.1.1.1.m1.1.2.2">‚Ñé</ci><ci id="S5.T6.5.1.1.1.m1.1.1.cmml" xref="S5.T6.5.1.1.1.m1.1.1">ùë•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.5.1.1.1.m1.1c">h(x)</annotation><annotation encoding="application/x-llamapun" id="S5.T6.5.1.1.1.m1.1d">italic_h ( italic_x )</annotation></semantics></math> Only</th><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T6.5.1.1.2 style=padding-top:1pt;padding-bottom:1pt>0.798</td></tr><tr class=ltx_tr id=S5.T6.6.2.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S5.T6.6.2.2.1 style=padding-top:1pt;padding-bottom:1pt><math alttext="h(x)+g(x)" class="ltx_Math" display="inline" id="S5.T6.6.2.2.1.m1.2"><semantics id="S5.T6.6.2.2.1.m1.2a"><mrow id="S5.T6.6.2.2.1.m1.2.3" xref="S5.T6.6.2.2.1.m1.2.3.cmml"><mrow id="S5.T6.6.2.2.1.m1.2.3.2" xref="S5.T6.6.2.2.1.m1.2.3.2.cmml"><mi id="S5.T6.6.2.2.1.m1.2.3.2.2" xref="S5.T6.6.2.2.1.m1.2.3.2.2.cmml">h</mi><mo id="S5.T6.6.2.2.1.m1.2.3.2.1" xref="S5.T6.6.2.2.1.m1.2.3.2.1.cmml">‚Å¢</mo><mrow id="S5.T6.6.2.2.1.m1.2.3.2.3.2" xref="S5.T6.6.2.2.1.m1.2.3.2.cmml"><mo id="S5.T6.6.2.2.1.m1.2.3.2.3.2.1" stretchy="false" xref="S5.T6.6.2.2.1.m1.2.3.2.cmml">(</mo><mi id="S5.T6.6.2.2.1.m1.1.1" xref="S5.T6.6.2.2.1.m1.1.1.cmml">x</mi><mo id="S5.T6.6.2.2.1.m1.2.3.2.3.2.2" stretchy="false" xref="S5.T6.6.2.2.1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo id="S5.T6.6.2.2.1.m1.2.3.1" xref="S5.T6.6.2.2.1.m1.2.3.1.cmml">+</mo><mrow id="S5.T6.6.2.2.1.m1.2.3.3" xref="S5.T6.6.2.2.1.m1.2.3.3.cmml"><mi id="S5.T6.6.2.2.1.m1.2.3.3.2" xref="S5.T6.6.2.2.1.m1.2.3.3.2.cmml">g</mi><mo id="S5.T6.6.2.2.1.m1.2.3.3.1" xref="S5.T6.6.2.2.1.m1.2.3.3.1.cmml">‚Å¢</mo><mrow id="S5.T6.6.2.2.1.m1.2.3.3.3.2" xref="S5.T6.6.2.2.1.m1.2.3.3.cmml"><mo id="S5.T6.6.2.2.1.m1.2.3.3.3.2.1" stretchy="false" xref="S5.T6.6.2.2.1.m1.2.3.3.cmml">(</mo><mi id="S5.T6.6.2.2.1.m1.2.2" xref="S5.T6.6.2.2.1.m1.2.2.cmml">x</mi><mo id="S5.T6.6.2.2.1.m1.2.3.3.3.2.2" stretchy="false" xref="S5.T6.6.2.2.1.m1.2.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T6.6.2.2.1.m1.2b"><apply id="S5.T6.6.2.2.1.m1.2.3.cmml" xref="S5.T6.6.2.2.1.m1.2.3"><plus id="S5.T6.6.2.2.1.m1.2.3.1.cmml" xref="S5.T6.6.2.2.1.m1.2.3.1"></plus><apply id="S5.T6.6.2.2.1.m1.2.3.2.cmml" xref="S5.T6.6.2.2.1.m1.2.3.2"><times id="S5.T6.6.2.2.1.m1.2.3.2.1.cmml" xref="S5.T6.6.2.2.1.m1.2.3.2.1"></times><ci id="S5.T6.6.2.2.1.m1.2.3.2.2.cmml" xref="S5.T6.6.2.2.1.m1.2.3.2.2">‚Ñé</ci><ci id="S5.T6.6.2.2.1.m1.1.1.cmml" xref="S5.T6.6.2.2.1.m1.1.1">ùë•</ci></apply><apply id="S5.T6.6.2.2.1.m1.2.3.3.cmml" xref="S5.T6.6.2.2.1.m1.2.3.3"><times id="S5.T6.6.2.2.1.m1.2.3.3.1.cmml" xref="S5.T6.6.2.2.1.m1.2.3.3.1"></times><ci id="S5.T6.6.2.2.1.m1.2.3.3.2.cmml" xref="S5.T6.6.2.2.1.m1.2.3.3.2">ùëî</ci><ci id="S5.T6.6.2.2.1.m1.2.2.cmml" xref="S5.T6.6.2.2.1.m1.2.2">ùë•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.6.2.2.1.m1.2c">h(x)+g(x)</annotation><annotation encoding="application/x-llamapun" id="S5.T6.6.2.2.1.m1.2d">italic_h ( italic_x ) + italic_g ( italic_x )</annotation></semantics></math></th><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T6.6.2.2.2 style=padding-top:1pt;padding-bottom:1pt>0.923</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 6 presents the results of an ablation study comparing the performance of the CoSTA* model with and without real-time feedback. The study focuses on 35 high-risk tasks (defined as those where the initial heuristic alone might lead to suboptimal tool selection), measuring the accuracy achieved in each case. This allows for a direct assessment of how the incorporation of real-time cost and quality feedback (represented by g(x)) impacts the overall accuracy of the multi-step image editing process.</p><details><summary>read the caption</summary>Table 6: Comparison of accuracy with and without g‚Å¢(x)ùëîùë•g(x)italic_g ( italic_x ) on 35 high-risk tasks to analyze the impact of real-time feedback g‚Å¢(x)ùëîùë•g(x)italic_g ( italic_x ).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S5.T7.9.1><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T7.9.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=S5.T7.9.1.1.1.1 style=padding-top:1pt;padding-bottom:1pt>Metric</th><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T7.9.1.1.1.2 style=padding-top:1pt;padding-bottom:1pt>Image Editing Tools</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T7.9.1.1.1.3 style=padding-top:1pt;padding-bottom:1pt>CoSTA*</td></tr><tr class=ltx_tr id=S5.T7.9.1.2.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id=S5.T7.9.1.2.2.1 style=padding-top:1pt;padding-bottom:1pt>Average Accuracy (30 Tasks)</th><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T7.9.1.2.2.2 style=padding-top:1pt;padding-bottom:1pt>0.48</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T7.9.1.2.2.3 style=padding-top:1pt;padding-bottom:1pt>0.93</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of the performance of CoSTA* against other image editing tools specifically on tasks involving text manipulation within images. The results demonstrate that CoSTA* significantly outperforms the image-only editing tools by achieving a substantially higher average accuracy across 30 text-based image editing tasks. This highlights CoSTA*&rsquo;s superior capabilities in handling complex tasks requiring both image and text processing. The superior performance stems from CoSTA*&rsquo;s ability to seamlessly integrate multiple tools and dynamically adapt its approach based on real-time feedback and quality assessment.</p><details><summary>read the caption</summary>Table 7: Comparison of image editing tools vs. CoSTA‚àó for text-based tasks. CoSTA‚àó outperforms image-only tools.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=A2.T8.1.1><thead class=ltx_thead><tr class=ltx_tr id=A2.T8.1.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id=A2.T8.1.1.1.1.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A2.T8.1.1.1.1.1.1>Task Type</span></th><th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id=A2.T8.1.1.1.1.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A2.T8.1.1.1.1.2.1><span class=ltx_p id=A2.T8.1.1.1.1.2.1.1 style=width:256.1pt><span class="ltx_text ltx_font_bold" id=A2.T8.1.1.1.1.2.1.1.1>Evaluation Criteria</span></span></span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=A2.T8.1.1.1.1.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A2.T8.1.1.1.1.3.1>Assigned Score</span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=A2.T8.1.1.2.1><td class="ltx_td ltx_align_left ltx_border_t" id=A2.T8.1.1.2.1.1 rowspan=6 style=padding-top:1pt;padding-bottom:1pt><span class=ltx_text id=A2.T8.1.1.2.1.1.1>Image-Only Tasks</span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=A2.T8.1.1.2.1.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A2.T8.1.1.2.1.2.1><span class=ltx_p id=A2.T8.1.1.2.1.2.1.1 style=width:256.1pt>Minor artifacts, barely noticeable distortions</span></span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T8.1.1.2.1.3 style=padding-top:1pt;padding-bottom:1pt>0.9</td></tr><tr class=ltx_tr id=A2.T8.1.1.3.2><td class="ltx_td ltx_align_justify ltx_align_top" id=A2.T8.1.1.3.2.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A2.T8.1.1.3.2.1.1><span class=ltx_p id=A2.T8.1.1.3.2.1.1.1 style=width:256.1pt>Some visible artifacts, but main content is unaffected</span></span></td><td class="ltx_td ltx_align_center" id=A2.T8.1.1.3.2.2 style=padding-top:1pt;padding-bottom:1pt>0.8</td></tr><tr class=ltx_tr id=A2.T8.1.1.4.3><td class="ltx_td ltx_align_justify ltx_align_top" id=A2.T8.1.1.4.3.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A2.T8.1.1.4.3.1.1><span class=ltx_p id=A2.T8.1.1.4.3.1.1.1 style=width:256.1pt>Noticeable distortions, but retains basic correctness</span></span></td><td class="ltx_td ltx_align_center" id=A2.T8.1.1.4.3.2 style=padding-top:1pt;padding-bottom:1pt>0.7</td></tr><tr class=ltx_tr id=A2.T8.1.1.5.4><td class="ltx_td ltx_align_justify ltx_align_top" id=A2.T8.1.1.5.4.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A2.T8.1.1.5.4.1.1><span class=ltx_p id=A2.T8.1.1.5.4.1.1.1 style=width:256.1pt>Significant artifacts or blending issues</span></span></td><td class="ltx_td ltx_align_center" id=A2.T8.1.1.5.4.2 style=padding-top:1pt;padding-bottom:1pt>0.5</td></tr><tr class=ltx_tr id=A2.T8.1.1.6.5><td class="ltx_td ltx_align_justify ltx_align_top" id=A2.T8.1.1.6.5.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A2.T8.1.1.6.5.1.1><span class=ltx_p id=A2.T8.1.1.6.5.1.1.1 style=width:256.1pt>Major distortions or loss of key content</span></span></td><td class="ltx_td ltx_align_center" id=A2.T8.1.1.6.5.2 style=padding-top:1pt;padding-bottom:1pt>0.3</td></tr><tr class=ltx_tr id=A2.T8.1.1.7.6><td class="ltx_td ltx_align_justify ltx_align_top" id=A2.T8.1.1.7.6.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A2.T8.1.1.7.6.1.1><span class=ltx_p id=A2.T8.1.1.7.6.1.1.1 style=width:256.1pt>Output is almost unusable, but some attempt is visible</span></span></td><td class="ltx_td ltx_align_center" id=A2.T8.1.1.7.6.2 style=padding-top:1pt;padding-bottom:1pt>0.1</td></tr><tr class=ltx_tr id=A2.T8.1.1.8.7><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=A2.T8.1.1.8.7.1 rowspan=6 style=padding-top:1pt;padding-bottom:1pt><span class=ltx_text id=A2.T8.1.1.8.7.1.1>Text+Image Tasks</span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=A2.T8.1.1.8.7.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A2.T8.1.1.8.7.2.1><span class=ltx_p id=A2.T8.1.1.8.7.2.1.1 style=width:256.1pt>Text is correctly placed but slightly misaligned</span></span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T8.1.1.8.7.3 style=padding-top:1pt;padding-bottom:1pt>0.9</td></tr><tr class=ltx_tr id=A2.T8.1.1.9.8><td class="ltx_td ltx_align_justify ltx_align_top" id=A2.T8.1.1.9.8.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A2.T8.1.1.9.8.1.1><span class=ltx_p id=A2.T8.1.1.9.8.1.1.1 style=width:256.1pt>Font or color inconsistencies, but legible</span></span></td><td class="ltx_td ltx_align_center" id=A2.T8.1.1.9.8.2 style=padding-top:1pt;padding-bottom:1pt>0.8</td></tr><tr class=ltx_tr id=A2.T8.1.1.10.9><td class="ltx_td ltx_align_justify ltx_align_top" id=A2.T8.1.1.10.9.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A2.T8.1.1.10.9.1.1><span class=ltx_p id=A2.T8.1.1.10.9.1.1.1 style=width:256.1pt>Noticeable alignment or formatting issues</span></span></td><td class="ltx_td ltx_align_center" id=A2.T8.1.1.10.9.2 style=padding-top:1pt;padding-bottom:1pt>0.7</td></tr><tr class=ltx_tr id=A2.T8.1.1.11.10><td class="ltx_td ltx_align_justify ltx_align_top" id=A2.T8.1.1.11.10.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A2.T8.1.1.11.10.1.1><span class=ltx_p id=A2.T8.1.1.11.10.1.1.1 style=width:256.1pt>Some missing or incorrect words but mostly readable</span></span></td><td class="ltx_td ltx_align_center" id=A2.T8.1.1.11.10.2 style=padding-top:1pt;padding-bottom:1pt>0.5</td></tr><tr class=ltx_tr id=A2.T8.1.1.12.11><td class="ltx_td ltx_align_justify ltx_align_top" id=A2.T8.1.1.12.11.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A2.T8.1.1.12.11.1.1><span class=ltx_p id=A2.T8.1.1.12.11.1.1.1 style=width:256.1pt>Major formatting errors or loss of intended meaning</span></span></td><td class="ltx_td ltx_align_center" id=A2.T8.1.1.12.11.2 style=padding-top:1pt;padding-bottom:1pt>0.3</td></tr><tr class=ltx_tr id=A2.T8.1.1.13.12><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id=A2.T8.1.1.13.12.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A2.T8.1.1.13.12.1.1><span class=ltx_p id=A2.T8.1.1.13.12.1.1.1 style=width:256.1pt>Text placement is incorrect, missing, or unreadable</span></span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=A2.T8.1.1.13.12.2 style=padding-top:1pt;padding-bottom:1pt>0.1</td></tr></tbody></table></table></figure><blockquote><p>üîº This table outlines the criteria used for assigning partial correctness scores during human evaluation of image editing tasks. It specifies the score (on a scale from 0.1 to 0.9) assigned to different levels of correctness based on the presence of artifacts, distortions, and other issues in the generated image. Scores are provided separately for image-only tasks and those involving both text and images, reflecting the different criteria applied in each case. The table clarifies what constitutes minor artifacts versus major distortions or loss of key information, enabling consistent evaluations across various tasks and levels of complexity.</p><details><summary>read the caption</summary>Table 8: Predefined Rules for Assigning Partial Correctness Scores in Human Evaluation</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=A4.T9.1><thead class=ltx_thead><tr class=ltx_tr id=A4.T9.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=A4.T9.1.1.1.1 style="padding:.9pt 4pt"><span class="ltx_text ltx_font_bold" id=A4.T9.1.1.1.1.1 style=font-size:90%>Subtask</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=A4.T9.1.1.1.2 style="padding:.9pt 4pt"><span class="ltx_text ltx_font_bold" id=A4.T9.1.1.1.2.1 style=font-size:90%>Avg Similarity Score</span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=A4.T9.1.2.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=A4.T9.1.2.1.1 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.2.1.1.1 style=font-size:90%>Object Replacement</span></th><td class="ltx_td ltx_align_center ltx_border_t" id=A4.T9.1.2.1.2 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.2.1.2.1 style=font-size:90%>0.98</span></td></tr><tr class=ltx_tr id=A4.T9.1.3.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A4.T9.1.3.2.1 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.3.2.1.1 style=font-size:90%>Object Recoloration</span></th><td class="ltx_td ltx_align_center" id=A4.T9.1.3.2.2 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.3.2.2.1 style=font-size:90%>0.99</span></td></tr><tr class=ltx_tr id=A4.T9.1.4.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A4.T9.1.4.3.1 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.4.3.1.1 style=font-size:90%>Object Addition</span></th><td class="ltx_td ltx_align_center" id=A4.T9.1.4.3.2 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.4.3.2.1 style=font-size:90%>0.97</span></td></tr><tr class=ltx_tr id=A4.T9.1.5.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A4.T9.1.5.4.1 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.5.4.1.1 style=font-size:90%>Object Removal</span></th><td class="ltx_td ltx_align_center" id=A4.T9.1.5.4.2 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.5.4.2.1 style=font-size:90%>0.97</span></td></tr><tr class=ltx_tr id=A4.T9.1.6.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A4.T9.1.6.5.1 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.6.5.1.1 style=font-size:90%>Image Captioning</span></th><td class="ltx_td ltx_align_center" id=A4.T9.1.6.5.2 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.6.5.2.1 style=font-size:90%>0.92</span></td></tr><tr class=ltx_tr id=A4.T9.1.7.6><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A4.T9.1.7.6.1 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.7.6.1.1 style=font-size:90%>Outpainting</span></th><td class="ltx_td ltx_align_center" id=A4.T9.1.7.6.2 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.7.6.2.1 style=font-size:90%>0.99</span></td></tr><tr class=ltx_tr id=A4.T9.1.8.7><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A4.T9.1.8.7.1 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.8.7.1.1 style=font-size:90%>Changing Scenery</span></th><td class="ltx_td ltx_align_center" id=A4.T9.1.8.7.2 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.8.7.2.1 style=font-size:90%>0.96</span></td></tr><tr class=ltx_tr id=A4.T9.1.9.8><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A4.T9.1.9.8.1 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.9.8.1.1 style=font-size:90%>Text Removal</span></th><td class="ltx_td ltx_align_center" id=A4.T9.1.9.8.2 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.9.8.2.1 style=font-size:90%>0.98</span></td></tr><tr class=ltx_tr id=A4.T9.1.10.9><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=A4.T9.1.10.9.1 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.10.9.1.1 style=font-size:90%>QA on Text</span></th><td class="ltx_td ltx_align_center ltx_border_bb" id=A4.T9.1.10.9.2 style="padding:.9pt 4pt"><span class=ltx_text id=A4.T9.1.10.9.2.1 style=font-size:90%>0.96</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the average CLIP (Contrastive Language‚ÄìImage Pre-training) similarity scores achieved for the outputs generated by CoSTA* on subtasks known to exhibit variability in their results (those with potential randomness in the output). The scores indicate the consistency of CoSTA*&rsquo;s performance across multiple runs for each subtask, demonstrating its robustness. Lower scores suggest higher variability in the outputs. The subtasks assessed include object replacement, recoloration, addition, removal, image captioning, outpainting, scenery changes, and text operations.</p><details><summary>read the caption</summary>Table 9: Average CLIP Similarity Scores for Outputs of Randomness-Prone Subtasks</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=A5.T10.1.1><thead class=ltx_thead><tr class=ltx_tr id=A5.T10.1.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=A5.T10.1.1.1.1.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A5.T10.1.1.1.1.1.1>Model</span></th><th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id=A5.T10.1.1.1.1.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.1.1.2.1><span class=ltx_p id=A5.T10.1.1.1.1.2.1.1 style=width:142.3pt><span class="ltx_text ltx_font_bold" id=A5.T10.1.1.1.1.2.1.1.1>Tasks Supported</span></span></span></th><th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id=A5.T10.1.1.1.1.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.1.1.3.1><span class=ltx_p id=A5.T10.1.1.1.1.3.1.1 style=width:142.3pt><span class="ltx_text ltx_font_bold" id=A5.T10.1.1.1.1.3.1.1.1>Inputs</span></span></span></th><th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id=A5.T10.1.1.1.1.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.1.1.4.1><span class=ltx_p id=A5.T10.1.1.1.1.4.1.1 style=width:142.3pt><span class="ltx_text ltx_font_bold" id=A5.T10.1.1.1.1.4.1.1.1>Outputs</span></span></span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=A5.T10.1.1.2.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=A5.T10.1.1.2.1.1 style=padding-top:1pt;padding-bottom:1pt>Grounding DINO<cite class="ltx_cite ltx_citemacro_citep">(Liu et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib22 title>2024</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=A5.T10.1.1.2.1.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.2.1.2.1><span class=ltx_p id=A5.T10.1.1.2.1.2.1.1 style=width:142.3pt>Object Detection</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=A5.T10.1.1.2.1.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.2.1.3.1><span class=ltx_p id=A5.T10.1.1.2.1.3.1.1 style=width:142.3pt>Input Image</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=A5.T10.1.1.2.1.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.2.1.4.1><span class=ltx_p id=A5.T10.1.1.2.1.4.1.1 style=width:142.3pt>Bounding Boxes</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.3.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.3.2.1 style=padding-top:1pt;padding-bottom:1pt>YOLOv7<cite class="ltx_cite ltx_citemacro_citep">(Wang et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib32 title>2022</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.3.2.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.3.2.2.1><span class=ltx_p id=A5.T10.1.1.3.2.2.1.1 style=width:142.3pt>Object Detection</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.3.2.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.3.2.3.1><span class=ltx_p id=A5.T10.1.1.3.2.3.1.1 style=width:142.3pt>Input Image</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.3.2.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.3.2.4.1><span class=ltx_p id=A5.T10.1.1.3.2.4.1.1 style=width:142.3pt>Bounding Boxes</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.4.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.4.3.1 style=padding-top:1pt;padding-bottom:1pt>SAM<cite class="ltx_cite ltx_citemacro_citep">(Kirillov et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib16 title>2023b</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.4.3.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.4.3.2.1><span class=ltx_p id=A5.T10.1.1.4.3.2.1.1 style=width:142.3pt>Object Segmentation</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.4.3.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.4.3.3.1><span class=ltx_p id=A5.T10.1.1.4.3.3.1.1 style=width:142.3pt>Bounding Boxes</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.4.3.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.4.3.4.1><span class=ltx_p id=A5.T10.1.1.4.3.4.1.1 style=width:142.3pt>Segmentation Masks</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.5.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.5.4.1 style=padding-top:1pt;padding-bottom:1pt>DALL-E<cite class="ltx_cite ltx_citemacro_citep">(Ramesh et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib25 title>2021</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.5.4.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.5.4.2.1><span class=ltx_p id=A5.T10.1.1.5.4.2.1.1 style=width:142.3pt>Object Replacement</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.5.4.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.5.4.3.1><span class=ltx_p id=A5.T10.1.1.5.4.3.1.1 style=width:142.3pt>Segmentation Masks</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.5.4.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.5.4.4.1><span class=ltx_p id=A5.T10.1.1.5.4.4.1.1 style=width:142.3pt>Edited Image</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.6.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.6.5.1 style=padding-top:1pt;padding-bottom:1pt>DALL-E<cite class="ltx_cite ltx_citemacro_citep">(Ramesh et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib25 title>2021</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.6.5.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.6.5.2.1><span class=ltx_p id=A5.T10.1.1.6.5.2.1.1 style=width:142.3pt>Text Removal</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.6.5.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.6.5.3.1><span class=ltx_p id=A5.T10.1.1.6.5.3.1.1 style=width:142.3pt>Text Region Bounding Box</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.6.5.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.6.5.4.1><span class=ltx_p id=A5.T10.1.1.6.5.4.1.1 style=width:142.3pt>Image with Removed Text</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.7.6><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.7.6.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion Erase<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib29 title>2022b</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.7.6.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.7.6.2.1><span class=ltx_p id=A5.T10.1.1.7.6.2.1.1 style=width:142.3pt>Text Removal</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.7.6.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.7.6.3.1><span class=ltx_p id=A5.T10.1.1.7.6.3.1.1 style=width:142.3pt>Text Region Bounding Box</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.7.6.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.7.6.4.1><span class=ltx_p id=A5.T10.1.1.7.6.4.1.1 style=width:142.3pt>Image with Removed Text</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.8.7><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.8.7.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion Inpaint<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib29 title>2022b</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.8.7.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.8.7.2.1><span class=ltx_p id=A5.T10.1.1.8.7.2.1.1 style=width:142.3pt>Object Replacement, Object Recoloration, Object Removal</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.8.7.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.8.7.3.1><span class=ltx_p id=A5.T10.1.1.8.7.3.1.1 style=width:142.3pt>Segmentation Masks</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.8.7.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.8.7.4.1><span class=ltx_p id=A5.T10.1.1.8.7.4.1.1 style=width:142.3pt>Edited Image</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.9.8><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.9.8.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion Erase<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib29 title>2022b</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.9.8.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.9.8.2.1><span class=ltx_p id=A5.T10.1.1.9.8.2.1.1 style=width:142.3pt>Object Removal</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.9.8.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.9.8.3.1><span class=ltx_p id=A5.T10.1.1.9.8.3.1.1 style=width:142.3pt>Segmentation Masks</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.9.8.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.9.8.4.1><span class=ltx_p id=A5.T10.1.1.9.8.4.1.1 style=width:142.3pt>Edited Image</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.10.9><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.10.9.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion 3<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib29 title>2022b</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.10.9.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.10.9.2.1><span class=ltx_p id=A5.T10.1.1.10.9.2.1.1 style=width:142.3pt>Changing Scenery</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.10.9.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.10.9.3.1><span class=ltx_p id=A5.T10.1.1.10.9.3.1.1 style=width:142.3pt>Input Image</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.10.9.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.10.9.4.1><span class=ltx_p id=A5.T10.1.1.10.9.4.1.1 style=width:142.3pt>Edited Image</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.11.10><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.11.10.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion Outpaint<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib29 title>2022b</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.11.10.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.11.10.2.1><span class=ltx_p id=A5.T10.1.1.11.10.2.1.1 style=width:142.3pt>Outpainting</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.11.10.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.11.10.3.1><span class=ltx_p id=A5.T10.1.1.11.10.3.1.1 style=width:142.3pt>Input Image</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.11.10.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.11.10.4.1><span class=ltx_p id=A5.T10.1.1.11.10.4.1.1 style=width:142.3pt>Expanded Image</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.12.11><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.12.11.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion Search & Recolor<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib29 title>2022b</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.12.11.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.12.11.2.1><span class=ltx_p id=A5.T10.1.1.12.11.2.1.1 style=width:142.3pt>Object Recoloration</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.12.11.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.12.11.3.1><span class=ltx_p id=A5.T10.1.1.12.11.3.1.1 style=width:142.3pt>Input Image</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.12.11.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.12.11.4.1><span class=ltx_p id=A5.T10.1.1.12.11.4.1.1 style=width:142.3pt>Recolored Image</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.13.12><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.13.12.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion Remove Background<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib29 title>2022b</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.13.12.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.13.12.2.1><span class=ltx_p id=A5.T10.1.1.13.12.2.1.1 style=width:142.3pt>Background Removal</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.13.12.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.13.12.3.1><span class=ltx_p id=A5.T10.1.1.13.12.3.1.1 style=width:142.3pt>Input Image</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.13.12.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.13.12.4.1><span class=ltx_p id=A5.T10.1.1.13.12.4.1.1 style=width:142.3pt>Edited Image</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.14.13><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.14.13.1 style=padding-top:1pt;padding-bottom:1pt>Text Removal (Painting)</th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.14.13.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.14.13.2.1><span class=ltx_p id=A5.T10.1.1.14.13.2.1.1 style=width:142.3pt>Text Removal</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.14.13.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.14.13.3.1><span class=ltx_p id=A5.T10.1.1.14.13.3.1.1 style=width:142.3pt>Text Region Bounding Box</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.14.13.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.14.13.4.1><span class=ltx_p id=A5.T10.1.1.14.13.4.1.1 style=width:142.3pt>Image with Removed Text</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.15.14><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.15.14.1 style=padding-top:1pt;padding-bottom:1pt>DeblurGAN<cite class="ltx_cite ltx_citemacro_citep">(Kupyn et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib18 title>2018</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.15.14.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.15.14.2.1><span class=ltx_p id=A5.T10.1.1.15.14.2.1.1 style=width:142.3pt>Image Deblurring</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.15.14.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.15.14.3.1><span class=ltx_p id=A5.T10.1.1.15.14.3.1.1 style=width:142.3pt>Input Image</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.15.14.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.15.14.4.1><span class=ltx_p id=A5.T10.1.1.15.14.4.1.1 style=width:142.3pt>Deblurred Image</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.16.15><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.16.15.1 style=padding-top:1pt;padding-bottom:1pt>LLM (GPT-4o)</th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.16.15.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.16.15.2.1><span class=ltx_p id=A5.T10.1.1.16.15.2.1.1 style=width:142.3pt>Image Captioning</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.16.15.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.16.15.3.1><span class=ltx_p id=A5.T10.1.1.16.15.3.1.1 style=width:142.3pt>Input Image</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.16.15.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.16.15.4.1><span class=ltx_p id=A5.T10.1.1.16.15.4.1.1 style=width:142.3pt>Image Caption</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.17.16><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.17.16.1 style=padding-top:1pt;padding-bottom:1pt>LLM (GPT-4o)</th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.17.16.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.17.16.2.1><span class=ltx_p id=A5.T10.1.1.17.16.2.1.1 style=width:142.3pt>Question Answering based on text, Sentiment Analysis</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.17.16.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.17.16.3.1><span class=ltx_p id=A5.T10.1.1.17.16.3.1.1 style=width:142.3pt>Extracted Text, Font Style Label</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.17.16.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.17.16.4.1><span class=ltx_p id=A5.T10.1.1.17.16.4.1.1 style=width:142.3pt>New Text, Text Region Bounding Box, Text Sentiment, Answers to Questions</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.18.17><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.18.17.1 style=padding-top:1pt;padding-bottom:1pt>Google Cloud Vision<cite class="ltx_cite ltx_citemacro_citep">(Google Cloud, <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib9 title>2024</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.18.17.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.18.17.2.1><span class=ltx_p id=A5.T10.1.1.18.17.2.1.1 style=width:142.3pt>Landmark Detection</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.18.17.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.18.17.3.1><span class=ltx_p id=A5.T10.1.1.18.17.3.1.1 style=width:142.3pt>Input Image</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.18.17.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.18.17.4.1><span class=ltx_p id=A5.T10.1.1.18.17.4.1.1 style=width:142.3pt>Landmark Label</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.19.18><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.19.18.1 style=padding-top:1pt;padding-bottom:1pt>CRAFT<cite class="ltx_cite ltx_citemacro_citep">(Baek et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib2 title>2019b</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.19.18.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.19.18.2.1><span class=ltx_p id=A5.T10.1.1.19.18.2.1.1 style=width:142.3pt>Text Detection</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.19.18.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.19.18.3.1><span class=ltx_p id=A5.T10.1.1.19.18.3.1.1 style=width:142.3pt>Input Image</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.19.18.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.19.18.4.1><span class=ltx_p id=A5.T10.1.1.19.18.4.1.1 style=width:142.3pt>Text Bounding Box</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.20.19><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.20.19.1 style=padding-top:1pt;padding-bottom:1pt>CLIP<cite class="ltx_cite ltx_citemacro_citep">(Radford et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib24 title>2021</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.20.19.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.20.19.2.1><span class=ltx_p id=A5.T10.1.1.20.19.2.1.1 style=width:142.3pt>Caption Consistency Check</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.20.19.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.20.19.3.1><span class=ltx_p id=A5.T10.1.1.20.19.3.1.1 style=width:142.3pt>Extracted Text</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.20.19.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.20.19.4.1><span class=ltx_p id=A5.T10.1.1.20.19.4.1.1 style=width:142.3pt>Consistency Score</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.21.20><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.21.20.1 style=padding-top:1pt;padding-bottom:1pt>DeepFont<cite class="ltx_cite ltx_citemacro_citep">(Wang et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib34 title>2015</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.21.20.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.21.20.2.1><span class=ltx_p id=A5.T10.1.1.21.20.2.1.1 style=width:142.3pt>Text Style Detection</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.21.20.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.21.20.3.1><span class=ltx_p id=A5.T10.1.1.21.20.3.1.1 style=width:142.3pt>Text Bounding Box</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.21.20.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.21.20.4.1><span class=ltx_p id=A5.T10.1.1.21.20.4.1.1 style=width:142.3pt>Font Style Label</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.22.21><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.22.21.1 style=padding-top:1pt;padding-bottom:1pt>EasyOCR<cite class="ltx_cite ltx_citemacro_citep">(Kittinaradorn et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib17 title>2022</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.22.21.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.22.21.2.1><span class=ltx_p id=A5.T10.1.1.22.21.2.1.1 style=width:142.3pt>Text Extraction</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.22.21.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.22.21.3.1><span class=ltx_p id=A5.T10.1.1.22.21.3.1.1 style=width:142.3pt>Text Bounding Box</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.22.21.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.22.21.4.1><span class=ltx_p id=A5.T10.1.1.22.21.4.1.1 style=width:142.3pt>Extracted Text</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.23.22><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.23.22.1 style=padding-top:1pt;padding-bottom:1pt>MagicBrush<cite class="ltx_cite ltx_citemacro_citep">(Zhang et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib42 title>2024a</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.23.22.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.23.22.2.1><span class=ltx_p id=A5.T10.1.1.23.22.2.1.1 style=width:142.3pt>Object Addition</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.23.22.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.23.22.3.1><span class=ltx_p id=A5.T10.1.1.23.22.3.1.1 style=width:142.3pt>Input Image</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.23.22.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.23.22.4.1><span class=ltx_p id=A5.T10.1.1.23.22.4.1.1 style=width:142.3pt>Edited Image with Object</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.24.23><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.24.23.1 style=padding-top:1pt;padding-bottom:1pt>pix2pix<cite class="ltx_cite ltx_citemacro_citep">(Isola et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib14 title>2018</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.24.23.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.24.23.2.1><span class=ltx_p id=A5.T10.1.1.24.23.2.1.1 style=width:142.3pt>Changing Scenery (Day2Night)</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.24.23.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.24.23.3.1><span class=ltx_p id=A5.T10.1.1.24.23.3.1.1 style=width:142.3pt>Input Image</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.24.23.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.24.23.4.1><span class=ltx_p id=A5.T10.1.1.24.23.4.1.1 style=width:142.3pt>Edited Image</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.25.24><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.25.24.1 style=padding-top:1pt;padding-bottom:1pt>Real-ESRGAN<cite class="ltx_cite ltx_citemacro_citep">(Wang et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib33 title>2021</a>)</cite></th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.25.24.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.25.24.2.1><span class=ltx_p id=A5.T10.1.1.25.24.2.1.1 style=width:142.3pt>Image Upscaling</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.25.24.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.25.24.3.1><span class=ltx_p id=A5.T10.1.1.25.24.3.1.1 style=width:142.3pt>Input Image</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.25.24.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.25.24.4.1><span class=ltx_p id=A5.T10.1.1.25.24.4.1.1 style=width:142.3pt>High-Resolution Image</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.26.25><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.26.25.1 style=padding-top:1pt;padding-bottom:1pt>Text Writing using Pillow (For Addition)</th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.26.25.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.26.25.2.1><span class=ltx_p id=A5.T10.1.1.26.25.2.1.1 style=width:142.3pt>Text Addition</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.26.25.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.26.25.3.1><span class=ltx_p id=A5.T10.1.1.26.25.3.1.1 style=width:142.3pt>New Text, Text Region Bounding Box</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.26.25.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.26.25.4.1><span class=ltx_p id=A5.T10.1.1.26.25.4.1.1 style=width:142.3pt>Image with Text Added</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.27.26><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.27.26.1 style=padding-top:1pt;padding-bottom:1pt>Text Writing using Pillow</th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.27.26.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.27.26.2.1><span class=ltx_p id=A5.T10.1.1.27.26.2.1.1 style=width:142.3pt>Text Replacement, Keyword Highlighting</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.27.26.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.27.26.3.1><span class=ltx_p id=A5.T10.1.1.27.26.3.1.1 style=width:142.3pt>Image with Removed Text</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.27.26.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.27.26.4.1><span class=ltx_p id=A5.T10.1.1.27.26.4.1.1 style=width:142.3pt>Image with Text Added</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.28.27><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T10.1.1.28.27.1 style=padding-top:1pt;padding-bottom:1pt>Text Redaction (Code-based)</th><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.28.27.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.28.27.2.1><span class=ltx_p id=A5.T10.1.1.28.27.2.1.1 style=width:142.3pt>Text Redaction</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.28.27.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.28.27.3.1><span class=ltx_p id=A5.T10.1.1.28.27.3.1.1 style=width:142.3pt>Text Region Bounding Box</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top" id=A5.T10.1.1.28.27.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.28.27.4.1><span class=ltx_p id=A5.T10.1.1.28.27.4.1.1 style=width:142.3pt>Image with Redacted Text</span></span></td></tr><tr class=ltx_tr id=A5.T10.1.1.29.28><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=A5.T10.1.1.29.28.1 style=padding-top:1pt;padding-bottom:1pt>MiDaS</th><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id=A5.T10.1.1.29.28.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.29.28.2.1><span class=ltx_p id=A5.T10.1.1.29.28.2.1.1 style=width:142.3pt>Depth Estimation</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id=A5.T10.1.1.29.28.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.29.28.3.1><span class=ltx_p id=A5.T10.1.1.29.28.3.1.1 style=width:142.3pt>Input Image</span></span></td><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id=A5.T10.1.1.29.28.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_inline-block ltx_align_top" id=A5.T10.1.1.29.28.4.1><span class=ltx_p id=A5.T10.1.1.29.28.4.1.1 style=width:142.3pt>Image with Depth of Objects</span></span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table details the models used in the CoSTA* system. For each model, it lists the subtasks it can perform, any dependencies on outputs from other models as inputs, and the outputs it produces. This information is crucial for understanding how the CoSTA* system orchestrates different models to accomplish complex multi-turn image editing tasks. The table facilitates the automatic construction of the Tool Dependency Graph (TDG) which is used in the algorithm.</p><details><summary>read the caption</summary>Table 10: Model Description Table (MDT). Each model is listed with its supported subtasks, input dependencies, and outputs.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=A7.T11.3.1><thead class=ltx_thead><tr class=ltx_tr id=A7.T11.3.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id=A7.T11.3.1.1.1.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A7.T11.3.1.1.1.1.1>Model Name</span></th><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id=A7.T11.3.1.1.1.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A7.T11.3.1.1.1.2.1>Subtask</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=A7.T11.3.1.1.1.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A7.T11.3.1.1.1.3.1>Accuracy</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id=A7.T11.3.1.1.1.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A7.T11.3.1.1.1.4.1>Time (s)</span></th><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id=A7.T11.3.1.1.1.5 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A7.T11.3.1.1.1.5.1>Source</span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=A7.T11.3.1.2.1><td class="ltx_td ltx_align_left ltx_border_t" id=A7.T11.3.1.2.1.1 style=padding-top:1pt;padding-bottom:1pt>DeblurGAN<cite class="ltx_cite ltx_citemacro_citep">(Kupyn et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib18 title>2018</a>)</cite></td><td class="ltx_td ltx_align_left ltx_border_t" id=A7.T11.3.1.2.1.2 style=padding-top:1pt;padding-bottom:1pt>Image Deblurring</td><td class="ltx_td ltx_align_center ltx_border_t" id=A7.T11.3.1.2.1.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=A7.T11.3.1.2.1.4 style=padding-top:1pt;padding-bottom:1pt>0.8500</td><td class="ltx_td ltx_align_left ltx_border_t" id=A7.T11.3.1.2.1.5 style=padding-top:1pt;padding-bottom:1pt><cite class="ltx_cite ltx_citemacro_citep">(Kupyn et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib18 title>2018</a>)</cite></td></tr><tr class=ltx_tr id=A7.T11.3.1.3.2><td class="ltx_td ltx_align_left" id=A7.T11.3.1.3.2.1 style=padding-top:1pt;padding-bottom:1pt>MiDaS<cite class="ltx_cite ltx_citemacro_citep">(Ranftl et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib26 title>2020</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.3.2.2 style=padding-top:1pt;padding-bottom:1pt>Depth Estimation</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.3.2.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.3.2.4 style=padding-top:1pt;padding-bottom:1pt>0.7100</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.3.2.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.4.3><td class="ltx_td ltx_align_left" id=A7.T11.3.1.4.3.1 style=padding-top:1pt;padding-bottom:1pt>YOLOv7<cite class="ltx_cite ltx_citemacro_citep">(Wang et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib32 title>2022</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.4.3.2 style=padding-top:1pt;padding-bottom:1pt>Object Detection</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.4.3.3 style=padding-top:1pt;padding-bottom:1pt>0.82</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.4.3.4 style=padding-top:1pt;padding-bottom:1pt>0.0062</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.4.3.5 style=padding-top:1pt;padding-bottom:1pt><cite class="ltx_cite ltx_citemacro_citep">(Wang et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib32 title>2022</a>)</cite></td></tr><tr class=ltx_tr id=A7.T11.3.1.5.4><td class="ltx_td ltx_align_left" id=A7.T11.3.1.5.4.1 style=padding-top:1pt;padding-bottom:1pt>Grounding DINO<cite class="ltx_cite ltx_citemacro_citep">(Liu et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib22 title>2024</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.5.4.2 style=padding-top:1pt;padding-bottom:1pt>Object Detection</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.5.4.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.5.4.4 style=padding-top:1pt;padding-bottom:1pt>0.1190</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.5.4.5 style=padding-top:1pt;padding-bottom:1pt>Accuracy: <cite class="ltx_cite ltx_citemacro_citep">(Liu et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib22 title>2024</a>)</cite>, Time: Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.6.5><td class="ltx_td ltx_align_left" id=A7.T11.3.1.6.5.1 style=padding-top:1pt;padding-bottom:1pt>CLIP<cite class="ltx_cite ltx_citemacro_citep">(Radford et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib24 title>2021</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.6.5.2 style=padding-top:1pt;padding-bottom:1pt>Caption Consistency Check</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.6.5.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.6.5.4 style=padding-top:1pt;padding-bottom:1pt>0.0007</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.6.5.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.7.6><td class="ltx_td ltx_align_left" id=A7.T11.3.1.7.6.1 style=padding-top:1pt;padding-bottom:1pt>SAM<cite class="ltx_cite ltx_citemacro_citep">(Ravi et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib27 title>2024</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.7.6.2 style=padding-top:1pt;padding-bottom:1pt>Object Segmentation</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.7.6.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.7.6.4 style=padding-top:1pt;padding-bottom:1pt>0.0460</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.7.6.5 style=padding-top:1pt;padding-bottom:1pt>Accuracy: Evaluation on 137 instances of this subtask, Time: <cite class="ltx_cite ltx_citemacro_citep">(Ravi et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib27 title>2024</a>)</cite></td></tr><tr class=ltx_tr id=A7.T11.3.1.8.7><td class="ltx_td ltx_align_left" id=A7.T11.3.1.8.7.1 style=padding-top:1pt;padding-bottom:1pt>CRAFT<cite class="ltx_cite ltx_citemacro_citep">(Baek et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib2 title>2019b</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.8.7.2 style=padding-top:1pt;padding-bottom:1pt>Text Detection</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.8.7.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.8.7.4 style=padding-top:1pt;padding-bottom:1pt>1.2700</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.8.7.5 style=padding-top:1pt;padding-bottom:1pt>Accuracy: <cite class="ltx_cite ltx_citemacro_citep">(Baek et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib2 title>2019b</a>)</cite>, Time: Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.9.8><td class="ltx_td ltx_align_left" id=A7.T11.3.1.9.8.1 style=padding-top:1pt;padding-bottom:1pt>Google Cloud Vision<cite class="ltx_cite ltx_citemacro_citep">(Google Cloud, <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib9 title>2024</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.9.8.2 style=padding-top:1pt;padding-bottom:1pt>Landmark Detection</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.9.8.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.9.8.4 style=padding-top:1pt;padding-bottom:1pt>1.2000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.9.8.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.10.9><td class="ltx_td ltx_align_left" id=A7.T11.3.1.10.9.1 style=padding-top:1pt;padding-bottom:1pt>EasyOCR<cite class="ltx_cite ltx_citemacro_citep">(Kittinaradorn et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib17 title>2022</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.10.9.2 style=padding-top:1pt;padding-bottom:1pt>Text Extraction</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.10.9.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.10.9.4 style=padding-top:1pt;padding-bottom:1pt>0.1500</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.10.9.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.11.10><td class="ltx_td ltx_align_left" id=A7.T11.3.1.11.10.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion Erase<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib28 title>2022a</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.11.10.2 style=padding-top:1pt;padding-bottom:1pt>Object Removal</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.11.10.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.11.10.4 style=padding-top:1pt;padding-bottom:1pt>13.8000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.11.10.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.12.11><td class="ltx_td ltx_align_left" id=A7.T11.3.1.12.11.1 style=padding-top:1pt;padding-bottom:1pt>DALL-E<cite class="ltx_cite ltx_citemacro_citep">(Ramesh et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib25 title>2021</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.12.11.2 style=padding-top:1pt;padding-bottom:1pt>Object Replacement</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.12.11.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.12.11.4 style=padding-top:1pt;padding-bottom:1pt>14.1000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.12.11.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.13.12><td class="ltx_td ltx_align_left" id=A7.T11.3.1.13.12.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion Inpaint<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib28 title>2022a</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.13.12.2 style=padding-top:1pt;padding-bottom:1pt>Object Removal</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.13.12.3 style=padding-top:1pt;padding-bottom:1pt>0.93</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.13.12.4 style=padding-top:1pt;padding-bottom:1pt>12.1000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.13.12.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.14.13><td class="ltx_td ltx_align_left" id=A7.T11.3.1.14.13.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion Inpaint<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib28 title>2022a</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.14.13.2 style=padding-top:1pt;padding-bottom:1pt>Object Replacement</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.14.13.3 style=padding-top:1pt;padding-bottom:1pt>0.97</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.14.13.4 style=padding-top:1pt;padding-bottom:1pt>12.1000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.14.13.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.15.14><td class="ltx_td ltx_align_left" id=A7.T11.3.1.15.14.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion Inpaint<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib28 title>2022a</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.15.14.2 style=padding-top:1pt;padding-bottom:1pt>Object Recoloration</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.15.14.3 style=padding-top:1pt;padding-bottom:1pt>0.89</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.15.14.4 style=padding-top:1pt;padding-bottom:1pt>12.1000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.15.14.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.16.15><td class="ltx_td ltx_align_left" id=A7.T11.3.1.16.15.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion Search & Recolor<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib28 title>2022a</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.16.15.2 style=padding-top:1pt;padding-bottom:1pt>Object Recoloration</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.16.15.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.16.15.4 style=padding-top:1pt;padding-bottom:1pt>14.7000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.16.15.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.17.16><td class="ltx_td ltx_align_left" id=A7.T11.3.1.17.16.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion Outpaint<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib28 title>2022a</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.17.16.2 style=padding-top:1pt;padding-bottom:1pt>Outpainting</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.17.16.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.17.16.4 style=padding-top:1pt;padding-bottom:1pt>12.7000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.17.16.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.18.17><td class="ltx_td ltx_align_left" id=A7.T11.3.1.18.17.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion Remove Background<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib28 title>2022a</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.18.17.2 style=padding-top:1pt;padding-bottom:1pt>Background Removal</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.18.17.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.18.17.4 style=padding-top:1pt;padding-bottom:1pt>12.5000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.18.17.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.19.18><td class="ltx_td ltx_align_left" id=A7.T11.3.1.19.18.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion 3<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib28 title>2022a</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.19.18.2 style=padding-top:1pt;padding-bottom:1pt>Changing Scenery</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.19.18.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.19.18.4 style=padding-top:1pt;padding-bottom:1pt>12.9000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.19.18.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.20.19><td class="ltx_td ltx_align_left" id=A7.T11.3.1.20.19.1 style=padding-top:1pt;padding-bottom:1pt>pix2pix<cite class="ltx_cite ltx_citemacro_citep">(Isola et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib14 title>2018</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.20.19.2 style=padding-top:1pt;padding-bottom:1pt>Changing Scenery (Day2Night)</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.20.19.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.20.19.4 style=padding-top:1pt;padding-bottom:1pt>0.7000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.20.19.5 style=padding-top:1pt;padding-bottom:1pt>Accuracy: <cite class="ltx_cite ltx_citemacro_citep">(Isola et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib14 title>2018</a>)</cite>, Time: Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.21.20><td class="ltx_td ltx_align_left" id=A7.T11.3.1.21.20.1 style=padding-top:1pt;padding-bottom:1pt>Real-ESRGAN<cite class="ltx_cite ltx_citemacro_citep">(Wang et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib33 title>2021</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.21.20.2 style=padding-top:1pt;padding-bottom:1pt>Image Upscaling</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.21.20.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.21.20.4 style=padding-top:1pt;padding-bottom:1pt>1.7000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.21.20.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.22.21><td class="ltx_td ltx_align_left" id=A7.T11.3.1.22.21.1 style=padding-top:1pt;padding-bottom:1pt>LLM (GPT-4o)</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.22.21.2 style=padding-top:1pt;padding-bottom:1pt>Question Answering based on Text</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.22.21.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.22.21.4 style=padding-top:1pt;padding-bottom:1pt>6.2000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.22.21.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.23.22><td class="ltx_td ltx_align_left" id=A7.T11.3.1.23.22.1 style=padding-top:1pt;padding-bottom:1pt>LLM (GPT-4o)</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.23.22.2 style=padding-top:1pt;padding-bottom:1pt>Sentiment Analysis</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.23.22.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.23.22.4 style=padding-top:1pt;padding-bottom:1pt>6.1500</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.23.22.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.24.23><td class="ltx_td ltx_align_left" id=A7.T11.3.1.24.23.1 style=padding-top:1pt;padding-bottom:1pt>LLM (GPT-4o)</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.24.23.2 style=padding-top:1pt;padding-bottom:1pt>Image Captioning</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.24.23.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.24.23.4 style=padding-top:1pt;padding-bottom:1pt>6.3100</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.24.23.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.25.24><td class="ltx_td ltx_align_left" id=A7.T11.3.1.25.24.1 style=padding-top:1pt;padding-bottom:1pt>DeepFont<cite class="ltx_cite ltx_citemacro_citep">(Wang et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib34 title>2015</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.25.24.2 style=padding-top:1pt;padding-bottom:1pt>Text Style Detection</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.25.24.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.25.24.4 style=padding-top:1pt;padding-bottom:1pt>1.8000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.25.24.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.26.25><td class="ltx_td ltx_align_left" id=A7.T11.3.1.26.25.1 style=padding-top:1pt;padding-bottom:1pt>Text Writing - Pillow</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.26.25.2 style=padding-top:1pt;padding-bottom:1pt>Text Replacement</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.26.25.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.26.25.4 style=padding-top:1pt;padding-bottom:1pt>0.0380</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.26.25.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.27.26><td class="ltx_td ltx_align_left" id=A7.T11.3.1.27.26.1 style=padding-top:1pt;padding-bottom:1pt>Text Writing - Pillow</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.27.26.2 style=padding-top:1pt;padding-bottom:1pt>Text Addition</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.27.26.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.27.26.4 style=padding-top:1pt;padding-bottom:1pt>0.0380</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.27.26.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.28.27><td class="ltx_td ltx_align_left" id=A7.T11.3.1.28.27.1 style=padding-top:1pt;padding-bottom:1pt>Text Writing - Pillow</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.28.27.2 style=padding-top:1pt;padding-bottom:1pt>Keyword Highlighting</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.28.27.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.28.27.4 style=padding-top:1pt;padding-bottom:1pt>0.0380</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.28.27.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.29.28><td class="ltx_td ltx_align_left" id=A7.T11.3.1.29.28.1 style=padding-top:1pt;padding-bottom:1pt>MagicBrush<cite class="ltx_cite ltx_citemacro_citep">(Zhang et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib41 title>2023a</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.29.28.2 style=padding-top:1pt;padding-bottom:1pt>Object Addition</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.29.28.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.29.28.4 style=padding-top:1pt;padding-bottom:1pt>12.8000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.29.28.5 style=padding-top:1pt;padding-bottom:1pt>Accuracy: <cite class="ltx_cite ltx_citemacro_citep">(Zhang et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib41 title>2023a</a>)</cite>, Time: Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.30.29><td class="ltx_td ltx_align_left" id=A7.T11.3.1.30.29.1 style=padding-top:1pt;padding-bottom:1pt>Text Redaction</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.30.29.2 style=padding-top:1pt;padding-bottom:1pt>Text Redaction</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.30.29.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.30.29.4 style=padding-top:1pt;padding-bottom:1pt>0.0410</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.30.29.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.31.30><td class="ltx_td ltx_align_left" id=A7.T11.3.1.31.30.1 style=padding-top:1pt;padding-bottom:1pt>Text Removal by Painting</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.31.30.2 style=padding-top:1pt;padding-bottom:1pt>Text Removal (Fallback)</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.31.30.3 style=padding-top:1pt;padding-bottom:1pt>0.20</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.31.30.4 style=padding-top:1pt;padding-bottom:1pt>0.0450</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.31.30.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.32.31><td class="ltx_td ltx_align_left" id=A7.T11.3.1.32.31.1 style=padding-top:1pt;padding-bottom:1pt>DALL-E<cite class="ltx_cite ltx_citemacro_citep">(Ramesh et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib25 title>2021</a>)</cite></td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.32.31.2 style=padding-top:1pt;padding-bottom:1pt>Text Removal</td><td class="ltx_td ltx_align_center" id=A7.T11.3.1.32.31.3 style=padding-top:1pt;padding-bottom:1pt>1.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=A7.T11.3.1.32.31.4 style=padding-top:1pt;padding-bottom:1pt>14.2000</td><td class="ltx_td ltx_align_left" id=A7.T11.3.1.32.31.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr><tr class=ltx_tr id=A7.T11.3.1.33.32><td class="ltx_td ltx_align_left ltx_border_bb" id=A7.T11.3.1.33.32.1 style=padding-top:1pt;padding-bottom:1pt>Stable Diffusion Erase<cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2503.10613v1#bib.bib28 title>2022a</a>)</cite></td><td class="ltx_td ltx_align_left ltx_border_bb" id=A7.T11.3.1.33.32.2 style=padding-top:1pt;padding-bottom:1pt>Text Removal</td><td class="ltx_td ltx_align_center ltx_border_bb" id=A7.T11.3.1.33.32.3 style=padding-top:1pt;padding-bottom:1pt>0.97</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=A7.T11.3.1.33.32.4 style=padding-top:1pt;padding-bottom:1pt>13.8000</td><td class="ltx_td ltx_align_left ltx_border_bb" id=A7.T11.3.1.33.32.5 style=padding-top:1pt;padding-bottom:1pt>Evaluation on 137 instances of this subtask</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 11 presents benchmark results for various tools used in image editing tasks, focusing on accuracy and execution time. The data is sourced from published benchmarks whenever available. For tools lacking pre-existing benchmark data, the table includes results derived from evaluating the tools on 137 instances of each relevant subtask, using a dataset of 121 images. This extensive evaluation ensures a robust assessment across diverse conditions.</p><details><summary>read the caption</summary>Table 11: Benchmark Table for Accuracy and Execution Time. Accuracy and execution time for each tool-task pair are obtained from cited sources where available. For tools without prior benchmarks, evaluation was conducted over 137 instances of the specific subtask on 121 images from the dataset, ensuring a robust assessment across varied conditions.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-87aba902ce6da2c2cf184015dc81bc88 class=gallery><img src=https://ai-paper-reviewer.com/2503.10613/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.10613/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10613/&amp;title=CoSTA$%07st$:%20Cost-Sensitive%20Toolpath%20Agent%20for%20Multi-turn%20Image%20Editing" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10613/&amp;text=CoSTA$%07st$:%20Cost-Sensitive%20Toolpath%20Agent%20for%20Multi-turn%20Image%20Editing" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10613/&amp;subject=CoSTA$%07st$:%20Cost-Sensitive%20Toolpath%20Agent%20for%20Multi-turn%20Image%20Editing" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2503.10613/index.md",oid_likes="likes_paper-reviews/2503.10613/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2503.10637/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Distilling Diversity and Control in Diffusion Models</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-03-13T00:00:00+00:00>13 March 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2503.10391/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-03-13T00:00:00+00:00>13 March 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>