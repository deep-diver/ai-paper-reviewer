[{"figure_path": "https://arxiv.org/html/2503.10613/x1.png", "caption": "Figure 1: CoSTA\u2217 with different cost-quality trade-off coefficients \u03b1\ud835\udefc\\alphaitalic_\u03b1 vs. four recent image-editing models/agents. CoSTA\u2217 achieves Pareto optimality and dominates baselines on both metrics.", "description": "This figure presents a comparison of CoSTA*\u2019s performance against four other state-of-the-art image editing models/agents, varying the cost-quality trade-off coefficient (\u03b1).  The x-axis represents the cost (in seconds), and the y-axis represents the quality. Each line represents a different \u03b1 value for CoSTA*, demonstrating its ability to achieve various cost-quality tradeoffs. The Pareto front highlights the optimal trade-off between cost and quality. CoSTA* consistently outperforms other methods, achieving Pareto optimality and dominating the baselines on both metrics (cost and quality).", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.10613/x2.png", "caption": "Figure 2: Comparison of CoSTA\u2217 with State-of-the-Art image editing models/agents, which include GenArtist (Wang et\u00a0al., 2024b), MagicBrush (Zhang et\u00a0al., 2024a), InstructPix2Pix (Brooks et\u00a0al., 2023), and CLOVA (Gao et\u00a0al., 2024). The input images and prompts are shown on the left of the figure. The outputs generated by each method illustrate differences in accuracy, visual coherence, and the ability to multimodal tasks. Figure\u00a09 shows examples of step-by-step editing using CoSTA\u2217with intermediate subtask outputs presented.", "description": "Figure 2 showcases a comparison of CoSTA* against several state-of-the-art image editing models and agents.  Three complex multi-turn image editing tasks are presented. For each task, the input image and user prompt are displayed alongside the results generated by CoSTA* and four other methods: GenArtist, MagicBrush, InstructPix2Pix, and CLOVA.  The comparison highlights the differences in accuracy, visual coherence, and the ability of each method to handle multimodal tasks. CoSTA* demonstrates superior performance across all three tasks, showing a greater ability to correctly interpret and execute the complex, multi-step instructions.", "section": "2. Related Work"}, {"figure_path": "https://arxiv.org/html/2503.10613/x3.png", "caption": "Figure 3: Comparison of CoSTA\u2217 with other planning agents. LLM-only planning is efficient but prone to failure and heuristics. Search algorithms like A\u2217 guarantee optimal paths but are computationally expensive. CoSTA\u2217 balances cost and quality by first pruning the subtask tree using an LLM, which reduces the graph of tools we conduct fine-grained A\u2217 search on.", "description": "This figure compares three different approaches to multi-turn image editing task planning: LLM-only planning, traditional search algorithms (like A*), and the proposed CoSTA* method.  LLM-only planning is fast but unreliable due to limitations in its understanding of tool capabilities and costs, leading to suboptimal plans and frequent failures. A* search guarantees optimal solutions but is computationally expensive and impractical for complex tasks with numerous tools. CoSTA* combines the strengths of both by using an LLM to generate a subtask tree, which significantly reduces the search space for A*. Then, A* is applied to this smaller, more manageable subgraph to find a cost-efficient toolpath that balances quality and cost. This hybrid approach allows CoSTA* to achieve efficient and high-quality results.", "section": "4. CoSTA*: Cost-Sensitive Toolpath Agent"}, {"figure_path": "https://arxiv.org/html/2503.10613/x4.png", "caption": "Figure 4: Tool Dependency Graph (TDG). A directed graph where nodes represent tools and edges indicate dependencies. An edge (v1,v2)subscript\ud835\udc631subscript\ud835\udc632(v_{1},v_{2})( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) means v1subscript\ud835\udc631v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT\u2019s output is a legal input of v2subscript\ud835\udc632v_{2}italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. It enables toolpath search for multi-turn image-editing tasks with composite instructions.", "description": "The figure depicts a Tool Dependency Graph (TDG), a directed acyclic graph that visually represents the relationships and dependencies between various AI tools used in multi-turn image editing. Each node in the graph represents a specific AI tool (e.g., an object detector, an image inpainter, a text generator), and a directed edge from node  `v1` to node `v2` signifies that the output of tool `v1` serves as a valid input for tool `v2`. This graph is crucial for the CoSTA* algorithm because it allows for efficient searching of optimal toolpaths (sequences of tools) to accomplish complex image editing tasks described in natural language.  The TDG facilitates the selection of appropriate tools for each subtask within a multi-turn image editing workflow, considering the dependencies between tools to ensure a seamless and logical editing process.", "section": "3. Foundations of CoSTA*"}, {"figure_path": "https://arxiv.org/html/2503.10613/x5.png", "caption": "Figure 5: Three stages in CoSTA\u2217: (1) an LLM generates a subtask tree based on the input and task dependencies; (2) the subtask tree spans a tool subgraph that maintains tool dependencies; and (3) A\u2217 search finds the best toolpath balancing efficiency and quality.", "description": "This figure illustrates the three-stage process of the CoSTA* algorithm for multi-turn image editing.  Stage 1 shows an LLM generating a subtask tree from the input image and task instructions, breaking down the complex task into smaller, manageable subtasks.  Each branch of the tree represents a sequence of tool uses to achieve the overall goal. Stage 2 shows how the LLM's subtask tree translates into a tool subgraph, which is a subset of the complete tool dependency graph. This subgraph only includes the tools and relationships necessary for executing the subtasks identified in Stage 1, making the search space smaller and more efficient. Finally, Stage 3 depicts an A* search algorithm operating on this tool subgraph.  The A* search uses a cost function that balances execution time (efficiency) and output quality to find the optimal path (sequence of tool uses) through the subgraph, ultimately leading to the final edited image.", "section": "4. CoSTA*: Cost-Sensitive Toolpath Agent"}, {"figure_path": "https://arxiv.org/html/2503.10613/x6.png", "caption": "Figure 6: Distribution of image-only (left) and text+image tasks (middle) in our proposed benchmark, and quality comparison of different methods on the benchmark (right). CoSTA\u2217 excels in complex multimodal tasks and outperforms all the baselines.", "description": "This figure presents a breakdown of the benchmark dataset used to evaluate CoSTA* and other image editing methods. The leftmost panel shows the distribution of tasks involving only image manipulation, categorized by the number of subtasks. The middle panel displays the distribution of tasks involving both image and text manipulation.  The rightmost panel compares the performance of various models, including CoSTA*,  across these tasks. It highlights CoSTA*'s superior performance on complex multi-modal tasks where image and text editing is required, outperforming all baseline methods.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.10613/x7.png", "caption": "Figure 7: Comparison of a task with h\u2062(x)\u210e\ud835\udc65h(x)italic_h ( italic_x ) and h\u2062(x)+g\u2062(x)\u210e\ud835\udc65\ud835\udc54\ud835\udc65h(x)+g(x)italic_h ( italic_x ) + italic_g ( italic_x ), showing how real-time feedback improves path selection and execution.", "description": "Figure 7 demonstrates the impact of incorporating real-time feedback (g(x)) into the A* search algorithm within CoSTA*.  The left side shows the results when only the heuristic cost (h(x)) is used to guide path selection, while the right side shows the results when both the heuristic and the actual execution cost (h(x) + g(x)) are considered. The figure visually highlights how the real-time feedback mechanism improves the algorithm's ability to select more efficient paths that also deliver higher quality results.  Specifically, it shows how integrating g(x) leads to a path closer to the Pareto-optimal front (where both quality and cost are effectively balanced).", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.10613/x8.png", "caption": "Figure 8: Qualitative comparison of image editing tools vs. CoSTA\u2217 for text-based tasks, highlighting the advantages of our multimodal support in preserving visual and textual fidelity.", "description": "Figure 8 showcases a qualitative comparison between traditional image editing tools and CoSTA* when handling tasks involving text manipulation within images.  The figure demonstrates CoSTA*'s superior performance in maintaining both visual quality and textual accuracy.  Traditional methods often struggle to accurately edit text within an image while preserving other visual elements, leading to inconsistencies. In contrast, CoSTA*'s multimodal approach, integrating multiple specialized models, enables more precise and comprehensive text editing within the image, while preserving other visual elements. This highlights the effectiveness of CoSTA*'s approach in preserving both visual and textual fidelity.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.10613/x9.png", "caption": "Figure 9: Step-by-step execution of editing tasks using CoSTA\u2217. Each row illustrates an input image, the corresponding subtask breakdown, and intermediate outputs at different stages of the editing process. This visualization highlights how CoSTA\u2217 systematically refines outputs by leveraging specialized models for each subtask, ensuring greater accuracy and consistency in multimodal tasks.", "description": "This figure showcases the step-by-step process of CoSTA* in handling multi-turn image editing tasks. Each row displays an example, starting with the initial image and prompt. Then, the subtasks identified by CoSTA* are shown, along with the intermediate outputs generated by each subtask's execution. This visualization demonstrates the sequential nature of CoSTA*'s operations. Each step refines the output, leading to the final edited image. This step-by-step approach highlights CoSTA*'s ability to systematically improve accuracy and consistency, especially in complex tasks involving multiple modalities (text and image).", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2503.10613/x10.png", "caption": "Figure 10: Distribution of the number of instances for each subtask in the dataset.", "description": "This bar chart visualizes the frequency distribution of various subtasks within the benchmark dataset used in the CoSTA* model evaluation. Each bar represents a specific subtask (e.g., object detection, text replacement, image upscaling), and its height indicates the number of instances of that subtask present in the dataset. This figure provides insights into the dataset composition, showing which subtasks are more frequent and which are less frequent. This is useful for understanding the types of image editing tasks the dataset covers and the relative difficulty or complexity of those tasks.", "section": "5.1 Experimental Settings"}, {"figure_path": "https://arxiv.org/html/2503.10613/x11.png", "caption": "Figure 11: An overview of the dataset used for evaluation, showcasing representative input images and prompts across different task categories. The top section presents examples from image-only tasks, while the bottom section includes text+image tasks. These examples illustrate the diversity of tasks in our dataset, highlighting the range of modifications required for both visual and multimodal editing scenarios.", "description": "Figure 11 presents a curated selection of image editing tasks from the proposed benchmark dataset.  The top half showcases examples involving solely image manipulation, highlighting the range of complexity from simple object edits to more involved scene changes. The bottom half demonstrates tasks requiring both image and text editing. The examples illustrate the multifaceted nature of multi-turn image editing, encompassing modifications such as object removal, addition, recoloring, text manipulation, scene alterations, and more, underscoring the diverse challenges addressed by the CoSTA* method.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.10613/x12.png", "caption": "Figure 12: Visualization of the Retry Mechanism", "description": "This figure illustrates the retry mechanism used in CoSTA*.  When a node (representing a tool-subtask pair) fails to meet the quality threshold during the A* search, the retry mechanism is triggered.  The diagram shows the adjustments made to the cost and quality of the node, and the re-evaluation of potential paths to completion. The 'Retry' arrow indicates that the process repeats, attempting to fulfill the subtask by adjusting parameters or exploring alternative models. If the subtask still fails to meet the quality threshold after a retry, this node is excluded from further consideration. The figures demonstrate a case where the initial path is blocked by a node failing the quality check. Then, the A* search is re-triggered, this time avoiding the failed node, to explore other possible paths.", "section": "I. A* Execution Strategy"}, {"figure_path": "https://arxiv.org/html/2503.10613/x13.png", "caption": "Figure 13: Scatter plot of CLIP scores vs. human accuracy across 40 tasks. The weak correlation (Spearman\u2019s \u03c1=0.59\ud835\udf0c0.59\\rho=0.59italic_\u03c1 = 0.59, Kendall\u2019s \u03c4=0.47\ud835\udf0f0.47\\tau=0.47italic_\u03c4 = 0.47) highlights CLIP\u2019s limitations in capturing nuanced inaccuracies, particularly in complex, multi-step tasks.", "description": "This scatter plot visualizes the correlation between CLIP similarity scores and human-evaluated accuracy across 40 multi-turn image editing tasks.  Each point represents a single task, plotting its CLIP score against its human-assigned accuracy score. The weak positive correlation (Spearman's \u03c1 = 0.59, Kendall's \u03c4 = 0.47) indicates that CLIP scores are not a reliable predictor of human judgment, especially for complex, multi-step tasks.  The plot demonstrates that high CLIP scores do not guarantee high human accuracy, highlighting CLIP's limitations in capturing subtle errors or nuances often present in complex editing scenarios.", "section": "5. Experiments"}]