[{"figure_path": "https://arxiv.org/html/2503.06573/x1.png", "caption": "Figure 1: WildIFEval description. On the left, an example for a constrained generation task, and its decomposition into constraints. On the right, the judge decides whether each of the constraints is fulfilled.", "description": "This figure illustrates the WILDIFEVAL dataset's structure.  The left side shows a sample user instruction for a text generation task, broken down into its individual constraints (e.g., length, style). The right side depicts how a human judge assesses whether the generated text satisfies each constraint, ultimately determining the task's overall score. This process reveals the challenges LLMs face in fulfilling multiple constraints simultaneously.", "section": "The WILDIFEVAL Dataset"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/heatmap_of_co-occurrences_in_tasks.png", "caption": "(a)", "description": "The figure shows the distribution of constraint types in the WILDIFEVAL dataset.  It's a bar chart visualizing the frequency of different constraint types, offering insights into the prevalence of various types of constraints in real-world user instructions. This helps understand the characteristics and complexity of user requests in terms of the types of constraints they impose on LLMs.", "section": "3 Into the Wild: A Data Expedition"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/domain_piechart.png", "caption": "(b)", "description": "This t-SNE projection of constraint embeddings shows clusters corresponding to certain constraint types, such as format and structure, length, and style and tone constraints, while others, like focus/emphasis and include/avoid, are more spread out.", "section": "3 Into the Wild: A Data Expedition"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/lex_div_2.png", "caption": "(c)", "description": "This figure displays the Kendall's Tau correlation coefficients between the model rankings generated by different constraint types within the WILDIFEVAL benchmark. Values closer to 1 indicate stronger agreement in model rankings between the constraint types. This visualization helps to understand the relationship and potential overlap between various constraints, revealing whether models' abilities to handle one type of constraint correlate with their abilities on other constraint types.", "section": "3 Into the Wild: A Data Expedition"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/bar_plot_of_mean_score.png", "caption": "(d)", "description": "This figure shows the relative co-occurrence of constraint categories within tasks in the WILDIFEVAL dataset.  It uses a heatmap to visualize how often pairs of constraint types appear together in the same task, compared to what would be expected based on their individual frequencies. Values above 1 indicate that constraints co-occur more frequently than expected by chance, suggesting potential relationships between constraint types.  For example, a strong co-occurrence might indicate that certain types of constraints are often requested together by users.", "section": "3 Into the Wild: A Data Expedition"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/line_plot_of_mean_score_by_num_constraints_normalized.png", "caption": "Figure 2: Analysis of constraints in WildIFEval. (a) Distribution of constraint types. (b) A tSNE projection van\u00a0der Maaten and Hinton (2008) of the embeddings of constraints, colored by their type. For convenience, we randomly subsample 1k data points. We observe some red, brown, and grey clusters, corresponding to Format and Structure, Length, and Style and Tone constraints, aligning with the generic nature of these types. This is in contrast to content-oriented types like Focus/Emphasis and Include/Avoid (green and purple), which are more spread out.", "description": "Figure 2 visualizes the analysis of constraints within the WildIFEval dataset. Panel (a) presents a bar chart illustrating the distribution of various constraint types, showing their relative frequencies.  Panel (b) shows a t-distributed Stochastic Neighbor Embedding (t-SNE) projection of constraint embeddings, where each point represents a constraint and is colored according to its type.  The t-SNE visualization reveals clustering patterns: constraints related to 'Format and Structure', 'Length', and 'Style and Tone' form distinct clusters, while 'Focus/Emphasis' and 'Include/Avoid' constraints are more dispersed. This suggests that the former group possesses a more inherent, generic nature than the latter content-oriented group.", "section": "3 Into the Wild: A Data Expedition"}, {"figure_path": "https://arxiv.org/html/2503.06573/x2.png", "caption": "Figure 3: Relative co-occurrence of constraint categories within tasks. Values above 1111 indicate that constraints co-occur more than expected by their overall type frequencies. Refer to Appendix\u00a0B.2 for details.", "description": "This figure shows the relative co-occurrence of different constraint categories within the tasks in the WILDIFEVAL dataset.  It compares the observed co-occurrence frequency of constraint pairs to what would be expected based on the individual frequencies of each constraint type. Values above 1 indicate that pairs of constraints appear together more frequently than expected by chance alone.  The Appendix provides further details on the statistical calculations used.", "section": "3 Into the Wild: A Data Expedition"}, {"figure_path": "https://arxiv.org/html/2503.06573/x3.png", "caption": "Figure 4: Distribution of task domains in WildIFEval.", "description": "This figure shows the distribution of different domains present in the WildIFEVAL dataset.  It provides a visual representation of the variety of real-world tasks included in the dataset, categorized by their subject matter. This allows readers to quickly understand the range of topics the dataset covers, which influences the generality of conclusions drawn from the dataset. The visual representation helps in assessing the diversity and real-world applicability of the benchmark.", "section": "3 Into the Wild: A Data Expedition"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/line_plot_of_mean_score_by_num_constraints.png", "caption": "Figure 5: Constraint lexical diversity (opening verbs).", "description": "This figure shows the distribution of the most frequent verbs used at the beginning of constraints within the WILDIFEVAL dataset.  It illustrates the lexical diversity of the constraints by showing how often certain verbs are used to initiate user-specified requirements. The visualization allows for a better understanding of the common ways in which users express constraints in natural language.", "section": "3.2 Data Diversity"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/histogram_of_num_constraints.png", "caption": "Figure 6: Mean scores across all tasks in WildIFEval.", "description": "This figure displays the average performance scores of 14 different Large Language Models (LLMs) across all tasks within the WildIFEval benchmark dataset.  Each bar represents an LLM, showing its mean score, indicating the overall effectiveness of the model in fulfilling multiple, real-world constraints presented in user instructions.  It provides a high-level view of the relative performance of these models on the challenging multi-constraint instruction-following tasks.", "section": "LLM Benchmarking"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/frequency_of_constraints.png", "caption": "Figure 7: Performance on constraints as a function of the number of constraints in each task.", "description": "This figure displays the relationship between the number of constraints in a given task and the model's performance in fulfilling those constraints.  The x-axis represents the number of constraints, while the y-axis shows the fraction of constraints successfully met by the models.  Multiple lines are present, each representing a different large language model (LLM), allowing for a comparison of their performance across varying constraint complexities. The graph reveals how the model's success rate generally declines as the number of constraints increases, indicating that more complex tasks pose greater challenges to LLMs.", "section": "LLM Benchmarking"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/bar_plot_of_mean_score_by_category.png", "caption": "Figure 8: Correlation (Kendall\u2019s Tau) between model rankings induced by different constraint types.", "description": "This figure displays the correlation between model rankings generated using different constraint types from the WILDIFEVAL dataset.  Kendall's Tau, a measure of rank correlation, is used to quantify the agreement between these rankings.  A higher correlation indicates a greater similarity in how different models perform across various constraint types.  This helps to understand whether models struggle consistently across all constraint types, or if their performance varies significantly depending on the specific type of constraint.", "section": "4 LLM Benchmarking"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/frequency_of_categories_1.png", "caption": "Figure 9: Mean constraint-following performance by constraint category. Here we focus on a subset of large models, the full version is in Figure\u00a012 in the Appendix.", "description": "This figure displays the average performance of several large language models (LLMs) across eight different constraint categories in a constrained text generation task.  The y-axis represents the fraction of fulfilled constraints (a measure of how well the LLMs satisfied the specified requirements), and the x-axis lists the eight constraint types.  The figure highlights the relative strengths and weaknesses of different LLMs when handling various types of constraints. Note that this figure presents a subset of the models analyzed; the complete results, including smaller models, are found in Figure 12 of the Appendix.", "section": "LLM Benchmarking"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/frequency_of_categories_2.png", "caption": "Figure 10: Mean score of samples as a function of constraints in each task description. Not normalized by diversity.", "description": "This figure displays the average performance of different language models on tasks with varying numbers of constraints. The x-axis represents the number of constraints in a task, and the y-axis shows the mean score achieved by the models on those tasks.  The scores are not adjusted for the diversity of constraint types within each task. This visualization helps to understand how the difficulty of a task increases with the number of constraints imposed.", "section": "LLM Benchmarking"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/frequency_of_categories_3.png", "caption": "(a)", "description": "The figure shows the distribution of constraint types in the WILDIFEVAL dataset.  The bar chart visually represents the frequency of each constraint type, offering a clear understanding of their prevalence in real-world user instructions.  This provides insight into the common types of constraints users employ when interacting with LLMs, highlighting the most frequent and least frequent types.", "section": "3 Into the Wild: A Data Expedition"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/frequency_of_categories_4.png", "caption": "(b)", "description": "This figure is a t-SNE projection of the embeddings of constraints from the WILDIFEVAL dataset, colored by their type.  It visually represents the relationships between different types of constraints.  Constraints of similar types cluster together, while those of different types are more spread out. This visualization helps to understand the semantic similarity and diversity within the different constraint categories in the dataset.", "section": "3 Into the Wild: A Data Expedition"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/frequency_of_categories_5.png", "caption": "Figure 11: Analysis of constraints in WildIFEval. (a) Distribution of the number of constraints per task. This histogram shows how many constraints are typically assigned to individual tasks. (b) Frequency of unique constraints across the dataset. This plot illustrates how often each distinct constraint appears in different tasks.", "description": "Figure 11 presents a dual analysis of constraints within the WildIFEval dataset. The first sub-figure (a) displays a histogram showing the distribution of the number of constraints per task, illustrating the typical number of constraints found in each task within the dataset.  The second sub-figure (b) presents a bar chart illustrating the frequency distribution of unique constraints across all tasks. This visualizes how often each individual constraint appears in the dataset, highlighting the prevalence of various types of constraints and indicating whether certain types are more common than others.", "section": "3 Into the Wild: A Data Expedition"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/frequency_of_categories_6.png", "caption": "Figure 12: Mean constraint-following performance, by constraint category.", "description": "This figure displays the average performance of various LLMs across different constraint categories in the WILDIFEVAL benchmark. Each bar represents a constraint type (e.g., length, style, focus), and the bar height shows the mean proportion of successfully fulfilled constraints of that type by the different LLMs.  The x-axis lists the different constraint types, and the y-axis shows the average performance. It helps to illustrate which constraint types are more challenging for LLMs to satisfy.", "section": "LLM Benchmarking"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/frequency_of_categories_7.png", "caption": "(a)", "description": "Distribution of constraint types in the WILDIFEVAL dataset.  The bar chart shows the frequency of each of eight constraint types found in the dataset.  This visualization helps to understand the relative prevalence of different types of constraints within real-world user instructions for text generation tasks.", "section": "3 Into the Wild: A Data Expedition"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/frequency_of_categories_8.png", "caption": "(b)", "description": "This figure shows a t-SNE projection of the embeddings of constraints from the WILDIFEVAL dataset, colored by their type.  It visually represents the semantic similarity between different constraint types.  Constraints of similar types (e.g., Format and Structure, Length, Style and Tone) cluster together, indicating semantic closeness. In contrast, constraints related to content (e.g., Include/Avoid, Focus/Emphasis) are more spread out, highlighting their diversity and less structured nature.", "section": "3 Into the Wild: A Data Expedition"}, {"figure_path": "https://arxiv.org/html/2503.06573/extracted/6264346/images/lex_div_full_data.png", "caption": "(c)", "description": "This figure displays the Kendall's Tau correlation coefficients between the model rankings produced by WILDIFEVAL and several other established benchmarks.  Higher values indicate stronger agreement between the rankings. The benchmarks compared are IFEval, GPQA, ARC-C, MMLU, and HumanEval, showing a high degree of correlation with WILDIFEVAL, suggesting that the benchmark effectively measures similar model capabilities.", "section": "C Correlation Analysis with Existing Benchmarks"}]