[{"content": "The following markdown table is a reformat of the provided HTML table. Note that the image paths have been updated to use the provided arxiv ID (2412.05237) and replace the svg images with a placeholder:  \n\n| Category | Dataset | Dataset | Dataset | Dataset | Dataset |\n|---|---|---|---|---|---| \n| General (15.4%) | [ALLaVA](https://arxiv.org/html/2412.05237/allava.png) | [SVITCore](https://arxiv.org/html/2412.05237/svitcore.png) | [ALLaVA-zh](https://arxiv.org/html/2412.05237/allava-zh.png) | [ShareGPT4V](https://arxiv.org/html/2412.05237/sharegpt4v.png) |  | \n|  | [CLlava Instruct](https://arxiv.org/html/2412.05237/cllava_instruct.png) | [idefics375k](https://arxiv.org/html/2412.05237/idefics375k.png) | [LVIS-InstructV4](https://arxiv.org/html/2412.05237/lvis-instructv4.png) | [WildVision Chat](https://arxiv.org/html/2412.05237/wildvision_chat.png) | [GQA](https://arxiv.org/html/2412.05237/gqa.png) |\n|  | [AlfWorld](https://arxiv.org/html/2412.05237/alfworld.png) | [IDK](https://arxiv.org/html/2412.05237/idk.png) | [GPT4V77](https://arxiv.org/html/2412.05237/gpt4v77.png) | [Laion GPT4V](https://arxiv.org/html/2412.05237/laion_gpt4v.png) | [Sherlock](https://arxiv.org/html/2412.05237/sherlock.png) |\n|  | [Irv-Normal](https://arxiv.org/html/2412.05237/irv-normal.png) | [LLaVA-zh](https://arxiv.org/html/2412.05237/llava-zh.png) | [SVITCore](https://arxiv.org/html/2412.05237/svitcore.png) | [Cambrian (Filter)](https://arxiv.org/html/2412.05237/cambrian_filter.png) | [Visual7W](https://arxiv.org/html/2412.05237/visual7w.png) |\n| Chart (15.4%) | [mPLUG-DocOwlchart](https://arxiv.org/html/2412.05237/mplug-doc owlchart.png) | [Ureader Chart](https://arxiv.org/html/2412.05237/ureader_chart.png) | [Ureader QA](https://arxiv.org/html/2412.05237/ureader_qa.png) | [DVQA](https://arxiv.org/html/2412.05237/dvqa.png) |  |\n|  | [ArXiv-Chart-GPT4o](https://arxiv.org/html/2412.05237/arxiv-chart-gpt4o.png) | [PlotQA](https://arxiv.org/html/2412.05237/plotqa.png) | [ArxivQA](https://arxiv.org/html/2412.05237/arxivqa.png) | [InfographicVQA](https://arxiv.org/html/2412.05237/infographicvqa.png) | [Robut-WTQ](https://arxiv.org/html/2412.05237/robot-wtq.png) |\n|  | [Robut-SQA](https://arxiv.org/html/2412.05237/robot-sqa.png) | [Hitab](https://arxiv.org/html/2412.05237/hitab.png) | [TAT-QA](https://arxiv.org/html/2412.05237/tat-qa.png) | [FinQA](https://arxiv.org/html/2412.05237/finqa.png) | [Vistext](https://arxiv.org/html/2412.05237/vistext.png) |\n|  | [ChartQA](https://arxiv.org/html/2412.05237/chartqa.png) | [Robut-WikiSQL](https://arxiv.org/html/2412.05237/robot-wikisql.png) | [Ureader KG](https://arxiv.org/html/2412.05237/ureader_kg.png) | [Chart2Text](https://arxiv.org/html/2412.05237/chart2text.png) | [Irv-Chart](https://arxiv.org/html/2412.05237/irv-chart.png) |\n| OCR (13.7%) | [MultiUI](https://arxiv.org/html/2412.05237/multiui.png) | [OCRVQA](https://arxiv.org/html/2412.05237/ocrvqa.png) | [ScreenQA](https://arxiv.org/html/2412.05237/screenqa.png) | [TextVQA](https://arxiv.org/html/2412.05237/textvqa.png) |  |\n|  | [TextOCR](https://arxiv.org/html/2412.05237/textocr.png) | [LLaVAR GPT4](https://arxiv.org/html/2412.05237/llavar_gpt4.png) | [ReCTs](https://arxiv.org/html/2412.05237/rects.png) | [Chrome-Writing](https://arxiv.org/html/2412.05237/chrome-writing.png) | [IAM](https://arxiv.org/html/2412.05237/iam.png) |\n|  | [UreaderOCR](https://arxiv.org/html/2412.05237/ureaderocr.png) | [ST-VQA](https://arxiv.org/html/2412.05237/st-vqa.png) | [DocVQA](https://arxiv.org/html/2412.05237/docvqa.png) | [RenderedText](https://arxiv.org/html/2412.05237/renderedtext.png) | [VisualMRC](https://arxiv.org/html/2412.05237/visualmrc.png) |\n| Caption (10.9%) | [ShareGPT4v](https://arxiv.org/html/2412.05237/sharegpt4v.png) | [ShareGPT4o](https://arxiv.org/html/2412.05237/sharegpt4o.png) | [Sharegpt4v (SAM)](https://arxiv.org/html/2412.05237/sharegpt4v_sam.png) | [Infographic](https://arxiv.org/html/2412.05237/infographic.png) |  |\n|  | [Sharegpt4v (COCO)](https://arxiv.org/html/2412.05237/sharegpt4v_coco.png) | [Sharegpt4v (LLAVA)](https://arxiv.org/html/2412.05237/sharegpt4v_llava.png) |  |  |  |\n| Language (16%) | [Orca](https://arxiv.org/html/2412.05237/orca.png) | [NuminaMath](https://arxiv.org/html/2412.05237/numina_math.png) | [MathInstruct](https://arxiv.org/html/2412.05237/mathinstruct.png) | [Orca Math](https://arxiv.org/html/2412.05237/orca_math.png) |  |\n|  | [Magpie Pro(L3 MT)](https://arxiv.org/html/2412.05237/magpie_pro_l3_mt.png) | [Magpie Pro(L3 ST)](https://arxiv.org/html/2412.05237/magpie_pro_l3_st.png) | [Others](https://arxiv.org/html/2412.05237/others.png) |  |  |\n| Code/Math (8.3%) | [MAVIS Geo](https://arxiv.org/html/2412.05237/mavis_geo.png) | [MAVIS Metagen](https://arxiv.org/html/2412.05237/mavis_metagen.png) | [Geometry3K](https://arxiv.org/html/2412.05237/geometry3k.png) | [GeomVerse](https://arxiv.org/html/2412.05237/geomverse.png) |  |\n|  | [Super-CLEVR](https://arxiv.org/html/2412.05237/super-clevr.png) | [TabMWP](https://arxiv.org/html/2412.05237/tabmwp.png) | [VizWiz](https://arxiv.org/html/2412.05237/vizwiz.png) | [Geo170K](https://arxiv.org/html/2412.05237/geo170k.png) | [MathVision](https://arxiv.org/html/2412.05237/mathvision.png) |\n|  | [GEOS](https://arxiv.org/html/2412.05237/geos.png) | [GeoQA+](https://arxiv.org/html/2412.05237/geoqa.png) | [IconQA(Math)](https://arxiv.org/html/2412.05237/iconqa_math.png) | [PMC-VQA](https://arxiv.org/html/2412.05237/pmc-vqa.png) | [UniGeo](https://arxiv.org/html/2412.05237/unigeo.png) |\n|  | [CLEVR-Math](https://arxiv.org/html/2412.05237/clevr-math.png) | [MapQA](https://arxiv.org/html/2412.05237/mapqa.png) | [RAVEN(M)](https://arxiv.org/html/2412.05237/raven_m.png) | [Design2Code](https://arxiv.org/html/2412.05237/design2code.png) |  |\n| Domain-specific (8.9%) | [WIT](https://arxiv.org/html/2412.05237/wit.png) | [M3IT+FLAN](https://arxiv.org/html/2412.05237/m3itflan.png) | [ScienceQA(Nona)](https://arxiv.org/html/2412.05237/scienceqa_nona.png) | [Vision Flan](https://arxiv.org/html/2412.05237/vision_flan.png) |  |\n|  | [PathVQA](https://arxiv.org/html/2412.05237/pathvqa.png) | [TQA](https://arxiv.org/html/2412.05237/tqa.png) | [A-OKVQA](https://arxiv.org/html/2412.05237/a-okvqa.png) | [WebSight](https://arxiv.org/html/2412.05237/websight.png) | [ViQuAE](https://arxiv.org/html/2412.05237/viquae.png) |\n|  | [ShareGPT4V(Knowledge)](https://arxiv.org/html/2412.05237/sharegpt4v_knowledge.png) | [AI2D(4V)](https://arxiv.org/html/2412.05237/ai2d_4v.png) |  |  |  |\n| Detection (3.2%) | [CLEVR](https://arxiv.org/html/2412.05237/clevr.png) | [VisualGenome](https://arxiv.org/html/2412.05237/visualgenome.png) | [TallyQA](https://arxiv.org/html/2412.05237/tallyqa.png) | [VSR](https://arxiv.org/html/2412.05237/vsr.png) |  |\n| Multi-Image (5.8%) | [NLVR2](https://arxiv.org/html/2412.05237/nlvr2.png) | [Mimic CGD](https://arxiv.org/html/2412.05237/mimic_cg.png) | [Coinstruct](https://arxiv.org/html/2412.05237/coinstruct.png) | [HQ-Edit](https://arxiv.org/html/2412.05237/hq-edit.png) |  |\n|  | [Raven](https://arxiv.org/html/2412.05237/raven.png) | [IconQA](https://arxiv.org/html/2412.05237/iconqa.png) | [VIST](https://arxiv.org/html/2412.05237/vist.png) | [Contrast-Caption](https://arxiv.org/html/2412.05237/contrast-caption.png) | [FlintstonesSV](https://arxiv.org/html/2412.05237/flintstonesv.png) |\n|  | [PororoSV](https://arxiv.org/html/2412.05237/pororosv.png) | [Others](https://arxiv.org/html/2412.05237/others.png) |  |  |  |\n| Video (2.5%) | [L-Video](https://arxiv.org/html/2412.05237/l-video.png) | [M4 Instruct Video](https://arxiv.org/html/2412.05237/m4_instruct_video.png) | [L-Video-ActivityNetQA](https://arxiv.org/html/2412.05237/l-video-activitynetqa.png) | [L-Hound](https://arxiv.org/html/2412.05237/l-hound.png) |  |\n|  | [L-Video-NeXT-QA](https://arxiv.org/html/2412.05237/l-video-next-qa.png) | [VideoChatGPT](https://arxiv.org/html/2412.05237/videochatgpt.png) | [Video-MME](https://arxiv.org/html/2412.05237/video-mme.png) | [L-Video-PerceptionTest](https://arxiv.org/html/2412.05237/l-video-perceptiontest.png) | [EgoSchema](https://arxiv.org/html/2412.05237/egoschema.png) |", "caption": "Table 1: Detailed configuration for each training stage of the MAmmoTH-VL-8B model.", "description": "This table details the hyperparameters and settings used during the three-stage training process of the MAmmoTH-VL-8B multimodal large language model.  It specifies the resolution, number of tokens, dataset used, number of samples, vision tower architecture, LLM backbone, trainable model parameters, batch size, maximum model length, and learning rates for the vision and language model components for each training stage.  The stages represent distinct phases in the model's training:  Language-Image Alignment, Visual Instruction Tuning (Single Image), and Visual Instruction Tuning (One Vision).", "section": "4 Experiments"}, {"content": "| Stage-1 | Stage-2 | Stage-3 |\n|---|---|---|\n| **Resolution** | 384 | 384 \u00d7 {1\u00d71, \u2026} | 384 \u00d7 {1\u00d71, \u2026} |\n| **#Tokens** | 729 | Max 729\u00d75 | Max 729\u00d75 |\n| **Dataset** | LCS | Single Image | Single, Multi-Image & Video |\n| **#Samples** | 558K | 10M | 2M |\n| **Vision Tower** | siglip-so400m-patch14-384 | siglip-so400m-patch14-384 | siglip-so400m-patch14-384 |\n| **LLM Backbone** | Qwen2.5-7B-Instruct | Qwen2.5-7B-Instruct | Qwen2.5-7B-Instruct |\n| **Trainable Model Parameters** | Projector: 20.0M | Full Model: 8.0B | Full Model: 8.0B |\n| **Batch Size** | 512 | 256 | 256 |\n| **Model Max Length** | 8192 | 8192 | 16384 |\n| **Learning Rate: \u03c8<sub>vision</sub>** | 1\u00d710<sup>-3</sup> | 2\u00d710<sup>-6</sup> | 2\u00d710<sup>-6</sup> |\n| **Learning Rate: {\u03b8<sub>proj</sub>,\u03a6<sub>LLM</sub>}** | 1\u00d710<sup>-3</sup> | 1\u00d710<sup>-5</sup> | 1\u00d710<sup>-5</sup> |\n| **Epoch** | 1 | 1 | 1 |", "caption": "Table 2: Performance on multi-discipline knowledge and mathematical reasoning benchmarks. We highlight different groups of models with different colors: closed-source models, open weights but closed training details, and fully open-source models. Results are from official sources or running with lmms-eval package if unavailable.", "description": "Table 2 presents the performance comparison of various large language models (LLMs) across a suite of benchmark tests evaluating multi-disciplinary knowledge and mathematical reasoning capabilities.  The benchmarks cover diverse tasks requiring complex reasoning and problem-solving skills.  Models are categorized into three groups based on their accessibility and transparency: closed-source (proprietary), open-weight (model weights are publicly available but training details are not), and fully open-source (both weights and training details are open).  Performance metrics were sourced either from the official publications of the respective LLMs or calculated using the lmms-eval package. This table is crucial for illustrating the significant improvement achieved by the proposed MAmmoTH-VL-8B model, particularly when compared to fully open-source models of a similar size.", "section": "4 Experiments"}, {"content": "| Model | MMStar | MMMU | MMMU-Pro | SeedBench | MMBench | MMVet | MathVerse | MathVista |\n|---|---|---|---|---|---|---|---|---|\n| **Multi-Discipline Knowledge and Mathematical Reasoning** |  |  |  |  |  |  |  |  |\n| **Model** | **MMStar** | **MMMU** | **MMMU-Pro** | **SeedBench** | **MMBench** | **MMVet** | **MathVerse** | **MathVista** |\n|  | test | val | vision | test | en-test | test | mini-vision | testmini |\n| GPT-4o (OpenAI, 2024) | 64.7 | 69.1 | 49.7 | 76.2 | 82.1 | 76.2 | 50.2 | 63.8 |\n| Gemini-1.5-Pro (Gemini Team, 2023) | 59.1 | 65.8 | 44.4 | 76.0 | 73.9 | 64.0 | - | 63.9 |\n| Claude-3.5-Sonnet (Anthropic, 2024) | 62.2 | 68.3 | 48.0 | 72.2 | 79.7 | 75.4 | - | 67.7 |\n| InternVL2-76B (Chen et al., 2023b) | 67.1 | 58.2 | 38.0 | 77.6 | 86.5 | 64.4 | - | 65.5 |\n| Qwen2-VL-72B (Wang et al., 2024c) | 68.6 | 64.5 | 37.1 | 77.9 | 86.9 | 73.9 | 37.3 | 70.5 |\n| LLaVA-OV-72B (SI) (Li et al., 2024b) | 65.2 | 57.4 | 26.0 | 77.6 | 86.6 | 60.0 | 37.7 | 66.5 |\n| LLaVA-OV-72B (Li et al., 2024b) | 66.1 | 56.8 | 24.0 | 78.0 | 85.9 | 63.7 | 39.1 | 67.5 |\n| MiniCPM-V-2.6-8B (Yao et al., 2024) | 57.5 | 49.8 | 21.7 | 74.0 | 81.5 | 60.0 | - | 60.6 |\n| INXComp-2.5-7B (Zhang et al., 2024b) | 59.9 | 42.9 | - | 75.4 | 74.4 | 51.7 | 20.0 | 59.6 |\n| Llama-3.2-11B-Vision-Ins. (Meta, 2024b) | 49.8 | 50.7 | 23.7 | 72.7 | 73.2 | 57.6 | 23.6 | 51.5 |\n| InternVL-2-8B (Chen et al., 2023b) | 59.4 | 49.3 | 25.4 | 76.0 | 81.7 | 60.0 | 27.5 | 58.3 |\n| Qwen2-VL-7B-Ins. (Wang et al., 2024c) | 60.7 | 52.1 | 26.9 | 74.3 | 83.0 | 62.0 | 28.2 | 58.2 |\n| Cambrian-1-8B (Tong et al., 2024) | - | 42.7 | 14.7 | 73.3 | 74.6 | 48.0 | - | 49.0 |\n| Llava-CoT-11B (Xu et al., 2024b) | 57.6 | 48.9 | 18.5 | 75.2 | 75.0 | 60.3 | 24.2 | 54.8 |\n| Molmo-8B-D (Deitke et al., 2024) | 50.5 | 45.3 | 18.9 | 74.1 | 73.6 | 58.0 | 21.5 | 51.6 |\n| LLaVA-OV-7B (SI) (Li et al., 2024b) | 60.9 | 47.3 | 16.8 | 74.8 | 80.5 | 58.8 | 26.9 | 56.1 |\n| LLaVA-OV-7B (Li et al., 2024b) | 61.7 | 48.8 | 18.7 | 75.4 | 80.8 | 58.6 | 26.2 | 63.2 |\n| MAmmoTH-VL-8B (SI) | 55.4 | 49.4 | 26.0 | 73.3 | 83.0 | 60.6 | 35.0 | 67.6 |\n| MAmmoTH-VL-8B | 63.0 | 50.8 | 25.3 | 76.0 | 83.4 | 62.3 | 34.2 | 67.6 |\n| \u0394 Over Best Open-Source (~10B Scale) | +1.3 | +1.9 | +7.1 | +0.6 | +2.6 | +2.0 | +8.1 | +4.4 |", "caption": "Table 3: Main results on Chart, Diagram, and Document Understanding, and Real-world Multimodal Interactions and Human Preferences benchmarks. Follow the same settings as in\u00a0Table\u00a02.", "description": "Table 3 presents the performance of various models on a range of benchmarks focused on Chart & Doc Understanding, and Multimodal Interactions & Preferences.  These benchmarks evaluate the models' abilities to comprehend and reason with charts, diagrams, documents, and real-world multimodal scenarios, measuring their accuracy and overall performance in nuanced interaction tasks.  Results are compared using consistent evaluation settings as established in Table 2.", "section": "4.3 Single-Image Performance"}, {"content": "| Model | AI2D | ChartQA | InfoVQA | DocVQA | RealWorldQA | WildVision | L-Wilder |\n|---|---|---|---|---|---|---|---| \n| **Chart & Doc Understanding** |  |  |  |  |  |  |  |\n|  | **AI2D** | **ChartQA** | **InfoVQA** | **DocVQA** | **RealWorldQA** | **WildVision** | **L-Wilder** |\n|  | test | test | test | test | test | 0617 | small |\n| GPT-4o (OpenAI, 2024) | 94.2 | 85.7 | 79.2 | 92.8 | 76.5 | 89.4 | 85.9 |\n| Gemini-1.5-Pro (Gemini Team, 2023) | 94.4 | 87.2 | 81.0 | 93.1 | 70.4 | - | - |\n| Claude-3.5-Sonnet (Anthropic, 2024) | 94.7 | 90.8 | 49.7 | 95.2 | 60.1 | 50.0 | 83.1 |\n| InternVL2-76B (Chen et al., 2023b) | 88.4 | 88.4 | 82.0 | 94.1 | 72.7 | - | - |\n| Qwen2-VL-72B (Wang et al., 2024c) | 88.1 | 88.3 | 84.5 | 96.5 | 77.8 | 52.3 | 53.6 |\n| LLaVA-OV-72B (SI) (Li et al., 2024b) | 85.1 | 84.9 | 74.6 | 91.8 | 73.8 | 49.5 | 72.9 |\n| LLaVA-OV-72B (Li et al., 2024b) | 85.6 | 83.7 | 74.9 | 91.3 | 71.9 | 52.3 | 72.0 |\n| MiniCPM-V-2.6-7B (Yao et al., 2024) | 82.1 | 82.4 | - | 90.8 | 65.0 | 11.7 | - |\n| INXComp-2.5-7B (Zhang et al., 2024b) | 81.5 | 82.2 | 70.0 | 90.9 | 67.8 | - | 61.4 |\n| Llama-3.2-11B-Vision-Ins (Meta, 2024b) | 77.3 | 83.4 | 65.0 | 88.4 | 63.3 | 49.7 | 62.0 |\n| InternVL-2-8B (Chen et al., 2023b) | 83.8 | 83.3 | 74.8 | 91.6 | 64.4 | 51.5 | 62.5 |\n| Qwen2-VL-7B-Ins (Wang et al., 2024c) | 83.0 | 83.0 | 76.5 | 94.5 | 70.1 | 44.0 | 66.3 |\n| Cambrian-1-8B (Tong et al., 2024) | 73.3 | 73.3 | 41.6 | 77.8 | 64.2 | - | 34.1 |\n| Llava-CoT-11B (Xu et al., 2024b) | - | 67.0 | 44.8 | - | - | - | 65.3 |\n| Molmo-7B-D (Deitke et al., 2024) | 81.0 | 84.1 | 72.6 | 92.2 | 70.7 | 40.0 | - |\n| LLaVA-OV-7B (SI) (Li et al., 2024b) | 81.6 | 78.8 | 65.3 | 86.9 | 65.5 | 39.2 | 69.1 |\n| LLaVA-OV-7B (Li et al., 2024b) | 81.4 | 80.0 | 68.8 | 87.5 | 66.3 | 53.8 | 67.8 |\n| MAmmoTH-VL-8B (SI) | 83.4 | 85.9 | 74.8 | 93.8 | 71.3 | 51.9 | 71.3 |\n| MAmmoTH-VL-8B | 84.0 | 86.2 | 73.1 | 93.7 | 69.9 | 51.1 | 70.8 |\n| **\u0394 Over Best Open-Source (~10B Scale)** | +2.4 | +2.1 | +2.2 | +1.6 | +0.6 | -1.9 | +2.2 |", "caption": "Table 4: Main results on Multi-Image and Video benchmarks. Follow the same settings as in\u00a0Table\u00a02.", "description": "This table presents the performance of various models on benchmarks involving multiple images and videos.  It compares different models' performance across several datasets, showing their scores and highlighting the relative strengths and weaknesses of each model in handling more complex, multi-modal data. This allows for a comparison to assess which models are best suited for tasks that demand processing of rich visual information from multiple sources.", "section": "4.3 Multi-Image and Video Performance"}, {"content": "| Model | MuirBench | MEGABench | EgoSchema | PerceptionTest | SeedBench | MLVU | MVBench | VideoMME |\n|---|---|---|---|---|---|---|---|---|\n| **Multi-Image and Video** |  |  |  |  |  |  |  |  |\n| **Model** | **MuirBench** | **MEGABench** | **EgoSchema** | **PerceptionTest** | **SeedBench** | **MLVU** | **MVBench** | **VideoMME** |\n|  | test | test | test | test | video | dev | test | w/o subs |\n| GPT-4o (OpenAI, 2024) | 68.0 | 54.2 | - | - | - | 64.6 | - | 71.9 |\n| GPT-4V (OpenAI, 2023) | 62.3 | - | - | - | 60.5 | 49.2 | 43.5 | 59.9 |\n| LLaVA-OV-72B (SI) (Li et al., 2024b) | 33.2 | - | 58.6 | 62.3 | 60.9 | 60.9 | 57.1 | 64.8 |\n| LLaVA-OV-72B (Li et al., 2024b) | 54.8 | 33.8 | 62.0 | 66.9 | 62.1 | 66.4 | 59.4 | 66.2 |\n| InternVL-2-8B (Chen et al., 2023b) | 59.4 | 27.7 | 54.2 | 57.4 | 54.9 | 30.2 | 66.4 | 54.0 |\n| Qwen2-VL-7B-Ins. (Wang et al., 2024c) | 41.6 | 36.0 | 66.7 | 62.3 | 55.3 | 58.6 | 67.0 | 63.3 |\n| LLaVA-OV-7B (SI) (Li et al., 2024b) | 32.7 | 22.1 | 52.9 | 54.9 | 51.1 | 60.2 | 51.2 | 55.0 |\n| LLaVA-OV-7B (Li et al., 2024b) | 41.8 | 23.9 | 60.1 | 57.1 | 56.9 | 64.7 | 56.7 | 58.2 |\n| MAmmoTH-VL-8B | 55.1 | 28.2 | 58.5 | 59.3 | 57.1 | 64.7 | 59.1 | 58.8 |\n| \u0394 Over Best Open-Source ~10B Scale | +13.3 | +4.3 | -1.6 | +2.2 | +0.2 | +0 | +2.4 | +0.6 |", "caption": "Table A1: Performance Comparison of Models Trained on Filtered versus Unfiltered Data Across Multiple Benchmarks.", "description": "This table presents a quantitative comparison of the performance achieved by models trained on filtered versus unfiltered datasets across a range of benchmarks.  It highlights the impact of the data filtering process on model accuracy, providing a detailed breakdown of performance differences across various evaluation metrics. The benchmarks likely include diverse tasks and datasets, showcasing a comprehensive assessment of the models' capabilities after training on data subjected to different preprocessing techniques. The table allows for an understanding of the effectiveness of data filtering in improving the overall model performance and the influence of data quality on the outcome.", "section": "A Additional Results of Ablation Study"}, {"content": "| Bench Name | Before Filter | After Filter |\n|---|---|---|\n| MMMU | 39.6 | **40.9** |\n| MMStar | 14.0 | **44.6** |\n| SeedBench | 66.4 | **67.9** |\n| MMMU-Pro Vision | **15.5** | 13.7 |\n| MathVista | 39.5 | **42.0** |\n| MMBench EN | 58.6 | **65.1** |\n| MMVet | 40.5 | **43.9** |\n| MathVerse | 19.3 | **22.6** |\n| AI2D | 56.9 | **61.8** |\n| ChartQA | 26.8 | **63.0** |\n| InfoVQA | 41.5 | **48.0** |\n| DocVQA | 71.7 | **76.5** |\n| L-Wilder Small | 58.8 | **59.8** |\n| WildVision | 40.2 | **42.2** |\n| RealWorldQA | 50.3 | **56.0** |\n| Avg | 42.6 | **49.9** |", "caption": "Table A2: Filter Rates Of Different Data Types After Data Filtering.", "description": "This table presents the percentage of data retained after filtering for different data types within the MAmmoTH-VL dataset.  The filtering process aimed to remove low-quality or hallucinated data, focusing on data sets related to Charts, Diagrams, and Documents.  The filter rate represents the proportion of data deemed acceptable and retained for model training after this quality-control process. The data types are categorized and analyzed individually to show where the filtering process had a greater effect, indicating potential areas for future data improvements or improvements in the model's ability to filter such data.", "section": "Additional Results of Ablation Study"}, {"content": "| Data Type | Before Filter | After Filter | Filter Rate |\n|---|---|---|---| \n| OCR | 1104960 | 498337 | 54.9 |\n| Chart | 7326189 | 3782029 | 48.4 |\n| GeneralQA | 1726180 | 1584308 | 8.2 |\n| Caption | 244874 | 199853 | 18.3 |\n| Math | 590894 | 518393 | 12.3 |\n| Other | 1315039 | 1178275 | 10.4 |", "caption": "Table A3: Benchmark Performance Of Models Trained On Data With Different Mix Ratios.", "description": "This table presents a comparison of the performance of models trained on datasets with varying ratios of original and rewritten data.  It shows how the model's performance changes across multiple benchmarks as the proportion of rewritten data in the training set increases. The benchmarks used are likely for a multimodal large language model (MLLM). The different mix ratios show how the model performs using only original data, only rewritten data, and combinations of both.", "section": "5.2 Effect of Data Mixing Ratio"}, {"content": "| Bench Name | Rewrite | Original | Mix 3:7 | Mix 7:3 | Mix 5:5 |\n|---|---|---|---|---|---| \n| MMMU | 40.9 | **41.9** | 41.5 | 41.3 | 41.7 |\n| MMStar | **44.6** | 43.3 | 43.4 | 42.3 | 43.7 |\n| SeedBench | 67.9 | **69.9** | 68.7 | 69.3 | 68.9 |\n| MMMU-Pro Vision | 13.7 | 13.0 | **13.8** | 13.5 | 13.5 |\n| MathVista | **42.0** | 40.4 | 41.8 | 40.6 | 39.5 |\n| MMBench EN | 65.1 | 67.8 | 66.1 | **67.9** | 66.4 |\n| MMVet | 43.9 | 37.3 | **45.5** | 40.7 | 38.9 |\n| MathVerse | **22.6** | 19.8 | 21.4 | 21.0 | 20.4 |\n| AI2D | 61.8 | **63.1** | 62.9 | 62.5 | 62.8 |\n| ChartQA | **63.1** | 56.5 | 61.1 | 56.8 | 56.6 |\n| InfoVQA | 48.0 | 47.3 | **49.0** | 45.7 | 45.6 |\n| DocVQA | 76.5 | 76.6 | **77.4** | 76.0 | 75.7 |\n| L-Wilder Small | 59.8 | 56.4 | **60.9** | 56.8 | 57.4 |\n| WildVision | **42.2** | 34.9 | 38.7 | 34.5 | 36.7 |\n| RealworldQA | 56.0 | 56.1 | **57.1** | 55.7 | 54.8 |\n| Avg | 49.9 | 48.3 | **50.0** | 48.3 | 48.2 |", "caption": "Table A4: Performance On Different Benchmarks Of Models Trained On Data Rewritten By Different Models", "description": "This table presents a comparison of the performance achieved by various models trained on datasets created using different rewriting techniques.  The models were evaluated across multiple benchmarks, allowing for a comprehensive assessment of how rewriting methods impact performance. Each model's performance is presented as a score for each benchmark, allowing for a direct comparison across different rewriting approaches.", "section": "A Additional Results of Ablation Study"}, {"content": "| Bench Name | Original | Rewrite (Qwen2-VL-7B) | Rewrite (InternVL2-8B) | Rewrite (InternVL2-76B) |\n|---|---|---|---|---|\n| MMMU | 40.4 | 40.6 | **40.9** | 40.78 |\n| MMStar | 40.9 | 41.7 | **41.7** | 37.9 |\n| SeedBench | 50.6 | 52.1 | 65.0 | **67.0** |\n| MMMU-Pro Vision | 12.3 | 12.9 | 12.9 | **15.3** |\n| MathVista | 36.4 | 38.8 | 37.4 | **39.0** |\n| MMBench EN | **65.8** | 59.1 | 60.1 | 58.3 |\n| MMVet | 38.6 | 38.1 | 38.6 | **41.1** |\n| MathVerse | 17.6 | **21.6** | 19.8 | 20.6 |\n| AI2D | 61.8 | **62.3** | 61.7 | 59.6 |\n| ChartQA | 49.4 | 48.1 | 50.6 | **58.7** |\n| InfoVQA | 43.8 | 43.1 | 43.7 | **44.3** |\n| DocVQA | **73.4** | 70.8 | 71.3 | 72.2 |\n| L-Wilder Small | 44.5 | 55.7 | 55.7 | **60.5** |\n| WildVision | 32.7 | 32.0 | 30.8 | **41.7** |\n| RealWorldQA | 56.5 | 55.1 | **56.8** | 53.5 |\n| Avg | 46.8 | 47.3 | 48.4 | **50.0** |", "caption": "Table A5: Kappa Value Between Any Two.", "description": "This table presents the inter-rater reliability scores (Cohen's Kappa) comparing the model's filtering decisions against three human evaluators.  The values show how consistently the model's automated filtering process agrees with human judgment in identifying high-quality data entries.", "section": "Analysis of the Filtering Step"}, {"content": "| \\ | Model | Evaluator1 | Evaluator2 | Evaluator3 |\n|---|---|---|---|---|\n| Model | - | 0.73 | 0.70 | 0.63 |\n| Evaluator1 | 0.73 | - | 0.70 | 0.42 |\n| Evaluator2 | 0.70 | 0.70 | - | 0.53 |\n| Evaluator3 | 0.63 | 0.42 | 0.53 | - |", "caption": "Table A6: Comparison of Original and Rewrite Average Content and Relevance Scores", "description": "This table presents a quantitative comparison of the quality of original and rewritten multimodal instruction data.  Specifically, it shows the average content and relevance scores for each dataset before and after the rewriting process.  Higher scores indicate better quality data, implying richer information content and stronger alignment between the visual and textual components. The scores were obtained using an MLLM (large language model) as a judge.", "section": "A Additional Results of Ablation Study"}]