{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-18", "reason": "This paper introduces a foundational visual model that significantly advances the capabilities of multimodal large language models by leveraging natural language supervision for visual learning."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "publication_date": "2023-09-16", "reason": "This paper presents Qwen, a large language model that serves as a crucial backbone for the MAmmoTH-VL model, providing the core language capabilities that are integrated with visual information."}, {"fullname_first_author": "Jiacheng Chen", "paper_title": "InternVL2-Llama3-76B", "publication_date": "2023-12-06", "reason": "This paper introduces InternVL2-Llama3-76B, a multimodal model that plays a vital role in both the rewriting and filtering stages of the dataset creation pipeline, significantly improving the quality and effectiveness of the dataset."}, {"fullname_first_author": "Bo Li", "paper_title": "LLaVA-OneVision: Easy visual task transfer", "publication_date": "2024-08-03", "reason": "This paper introduces LLaVA-OneVision, the architecture upon which MAmmoTH-VL is based, providing the fundamental framework for integrating vision and language models."}, {"fullname_first_author": "Xiang Yue", "paper_title": "MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale", "publication_date": "2024-12-06", "reason": "This is the main publication of the current work, introducing a new dataset and model that improve multimodal reasoning capabilities and achieve state-of-the-art results on various benchmarks."}]}