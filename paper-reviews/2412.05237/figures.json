[{"figure_path": "https://arxiv.org/html/2412.05237/x9.png", "caption": "Figure 1: Scaling effects of MAmmoTH-VL-8B on eight multimodal evaluation datasets. A simple rewriting approach using open models improves the quality of visual instruction data by eliciting chain-of-thought (CoT) reasoning. Training on this rewritten data demonstrates significant performance gains through increased model scale. Llava-OneVision-7B&72B\u00a0(Li et\u00a0al., 2024b) and Llava-CoT\u00a0(Xu et\u00a0al., 2024a) are included as references.", "description": "Figure 1 presents a performance comparison of the MAmmoTH-VL-8B model against several baseline models across eight multimodal datasets.  The key finding is that using a simple rewriting technique with open-source language models significantly improves the quality of visual instruction data.  This rewriting method encourages chain-of-thought (CoT) reasoning.  Training MAmmoTH-VL-8B on this enhanced data leads to substantial gains in performance that scale with the model size. LLaVA-OneVision and LLaVA-CoT models serve as baselines for comparison.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2412.05237/x13.png", "caption": "Figure 2: Overview of our simple yet scalable visual instruction data rewriting pipeline with three steps: manual data source collection, rewriting using MLLMs/LLMs, and filtering via the same MLLM as a judge. Examples below illustrate transformations in math and science categories, showcasing detailed, step-by-step responses.", "description": "Figure 2 illustrates the three-step pipeline used to create the MAmmoTH-VL instruction dataset.  First, existing open-source multimodal datasets are manually collected and categorized. Second, these datasets are rewritten using large language models (LLMs) and multimodal LLMs (MLLMs) to generate more complex questions and answers with detailed, step-by-step reasoning.  This rewriting process elicits Chain-of-Thought (CoT) reasoning, improving the quality of the instructions. Finally, the same MLLM acts as a judge to filter out low-quality or hallucinated entries.  The figure provides examples illustrating how simple question-answer pairs are transformed into more detailed, step-by-step CoT responses in both math and science domains.", "section": "2 Method"}, {"figure_path": "https://arxiv.org/html/2412.05237/x14.png", "caption": "Figure 3: The data distribution of MAmmoTH-VL-Instruct (12M). Left: Category distribution. Right: Details of data sources.", "description": "Figure 3 presents a comprehensive overview of the MAmmoTH-VL-Instruct dataset, which comprises 12 million multimodal instruction-response pairs. The left panel displays the category distribution within the dataset, illustrating the proportion of data points belonging to ten major categories: General, OCR, Chart, Caption, Domain-specific, Code&Math, Language, Detection, Multi-Image, and Video.  Each category represents a distinct set of tasks or scenarios covered by the data. The right panel provides detailed information about the individual data sources used to build the MAmmoTH-VL-Instruct dataset, demonstrating the breadth and diversity of the data collection process.  The figure clearly highlights both the diversity of the tasks covered and the range of sources that contributed to the construction of this large-scale dataset.", "section": "2 Method"}, {"figure_path": "https://arxiv.org/html/2412.05237/x15.png", "caption": "Figure 4: Comparison of original and rewritten data across two metrics: (1) Content and Relevance Scores judged by MLLMs show that rewritten data scores higher, indicating improved quality; (2) Token Length distribution suggests that rewritten data tends to be longer, including more tokens for rationales.", "description": "Figure 4 presents a comparison of the quality of original and rewritten multimodal instruction data.  Two key metrics are shown: (1) A model-based evaluation of Content and Relevance scores.  This shows that the rewritten data receives higher scores from the language model judge, indicating improved quality.  (2) A comparison of Token Length distributions.  The rewritten data shows longer token lengths than the original data, demonstrating that the rewriting process has added detailed rationales to the instruction-response pairs.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2412.05237/x16.png", "caption": "Figure 5: The t-SNE data distribution plot demonstrates how the rewritten data expands beyond the original dataset, increasing topic diversity and enhancing coverage of complex queries and reasoning.", "description": "This t-SNE plot visualizes the difference in topic distribution between the original and rewritten multimodal instruction datasets.  The original dataset's points cluster more tightly, indicating less diversity in the types of instructions and questions. The rewritten dataset shows a wider spread of points, demonstrating a significant increase in the variety of tasks and the complexity of reasoning required to answer them.  The expansion beyond the original dataset's clusters indicates that the rewriting process successfully generated new and diverse instructions that go beyond the scope of existing datasets, improving coverage of complex reasoning tasks.", "section": "Analysis of MAmmoTH-VL-Instruct"}, {"figure_path": "https://arxiv.org/html/2412.05237/x17.png", "caption": "Figure 6: The filter rates of different data types after filtering, with a lower filtering rate seen in categories like GeneralQA and Math, while OCR and Chart data experience more extensive filtering.", "description": "This figure shows the percentage of data filtered out during the data cleaning process for different categories of data. The filtering aimed to remove low-quality or hallucinated data entries.  The results reveal that certain data types, like those involving general question answering (GeneralQA) and mathematical problems (Math), had lower filter rates, indicating higher initial data quality. In contrast, Optical Character Recognition (OCR) and chart-related data (Chart) experienced significantly more extensive filtering, suggesting that these categories contained a higher proportion of problematic data points requiring removal.", "section": "Analysis of MAmmoTH-VL-Instruct"}, {"figure_path": "https://arxiv.org/html/2412.05237/x33.png", "caption": "Figure 7: Data filtering significantly improves the quality of generated data, particularly in chart and document understanding, where hallucinations are more frequent.", "description": "This figure presents a bar chart comparing the performance of a model trained on filtered and unfiltered data.  The chart shows that filtering significantly improves the model's accuracy, especially in tasks involving chart and document understanding.  This improvement is attributed to the reduction of hallucinations, which are errors where the model generates incorrect information that seems plausible but isn't supported by the data.  The results highlight the importance of data filtering in improving the overall quality and reliability of datasets for training multimodal large language models (MLLMs).", "section": "Ablation Study"}]