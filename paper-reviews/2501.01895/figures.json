[{"figure_path": "https://arxiv.org/html/2501.01895/x1.png", "caption": "Figure 1: The overview of our proposed EnerVerse model, consisting of three key components. First, Initial Reconstruction uses observation images from cameras mounted on the robot to build an initial 3D point cloud, with anchor views set to adapt to the environment and meet task-specific requirements. Second, Free Anchor View Renders generates rendered images from these anchor perspectives to provide comprehensive scene representations. Finally, Chunk-wise Autoregressive Generation employs a multi-view video diffusion to produce image sequences in chunks based on task instructions. When integrated with a policy head, this module can generate robotic actions to execute the given task.", "description": "EnerVerse is composed of three main parts: initial reconstruction, free anchor view rendering, and chunk-wise autoregressive generation.  Initial reconstruction creates a 3D point cloud from camera images mounted on the robot, using anchor views to adapt to the environment and specific task needs.  Free anchor view rendering then generates images from these anchor points, offering a comprehensive view of the scene. Finally, chunk-wise autoregressive generation uses a multi-view video diffusion model to create sequences of images in chunks, guided by task instructions.  A policy head can then be added to generate the robot actions needed to complete the task.", "section": "3 Methods"}, {"figure_path": "https://arxiv.org/html/2501.01895/x2.png", "caption": "Figure 2: The architecture of our proposed next-chunk diffusion model. As shown in Figure (a), a sequence of observational frames, captured by camera i\ud835\udc56iitalic_i and accompanied by the corresponding ray direction map, is utilized as observation priors. Leveraging these camera observations, an initial 3D reconstruction is obtained through depth wrapping and rendering\u00a0Lassner and Zollhofer (2021), then several Free Anchor Views are established accordingly. In addition to camera observational frames, a render frame from the FAV is also employed as context priors for the subsequent chunk diffusion models. To synthesize the anchor view i+1\ud835\udc561i+1italic_i + 1 sequence, the respective ray direction map is concatenated with the video latent. Notably, the observational image from the camera is optional and used only when the camera is static. If all sensors are in motion, the rendered image alone can serve as the context prior. In the context of the chunk-wise autoregressive training process, as depicted in Figure (b), clean frames selected at random from consecutive sequences are concatenated with noisy frames to forecast denoised latents. During the inference phase, once denoised frames are produced, they are utilized as the new set of clean frames for the following inference step. This iterative process persists until the predefined End-Of-Sequence (EOS) frame is encountered. Notably, we visualize only one view in Figure (b) to simplify the demonstration of the autoregressive generation process, but multi-view generation is fully supported by the model.", "description": "Figure 2 illustrates the ENERVERSE model's next-chunk diffusion process.  Panel (a) details how multi-view observations and ray direction maps from camera \ud835\udc56 are used to generate initial 3D reconstructions. These reconstructions form the basis for creating several Free Anchor Views (FAVs).  A rendered frame from a FAV serves as context for subsequent chunks.  The ray direction map is concatenated with the video latent to synthesize the next FAV sequence.  Camera observations are optional, only used when cameras are static.  Panel (b) shows the chunk-wise autoregressive training process.  Randomly selected clean frames from consecutive sequences are combined with noisy frames to predict denoised latents iteratively until an End-of-Sequence (EOS) frame is reached.  While (b) shows one view for simplicity, the model supports multi-view generation.", "section": "3.1 Next Chunk Diffusion"}, {"figure_path": "https://arxiv.org/html/2501.01895/x3.png", "caption": "Figure 3: The pipeline for EnerVerse as a data engine. Observation images captured from multiple cameras, along with rendered images from anchor views, are processed by the multi-view video generator to produce denoised multi-view videos. These videos, paired with their corresponding camera poses, are utilized in 4D Gaussian Splatting (4D GS) for 4D scene reconstruction. The reconstructed content is rendered from anchor views to generate high-precision images, which are iteratively fed back into the pipeline to enhance motion consistency and reconstruction quality. This iterative loop combines geometric consistency with generative refinement, delivering high-fidelity outputs for tasks such as robotic manipulation.", "description": "EnerVerse uses a data engine pipeline to iteratively improve the quality of its training data.  The pipeline starts with real-world observations from multiple cameras and rendered images from virtual anchor views.  These images are fed into a multi-view video generator to produce denoised videos.  The videos, along with camera pose information, are then used by 4D Gaussian Splatting (4DGS) to reconstruct a 4D scene representation.  This 4D model is then rendered from the anchor views to create high-precision images, which are fed back into the pipeline. This iterative process refines the data, improving motion consistency and reconstruction quality, and bridges the sim-to-real gap. The resulting high-fidelity data is suitable for training robotic manipulation tasks.", "section": "3.2 4D Generation"}, {"figure_path": "https://arxiv.org/html/2501.01895/x4.png", "caption": "Figure 4: Visualization of FAVs generation on the LIBERO benchmark. Anchor View 1 represents the observation image captured by a mounted camera. Anchor View 2 and Anchor View 3 are generated by rendering from a point cloud reconstructed from Anchor View 1 using depth wrapping.", "description": "Figure 4 illustrates the generation of Free Anchor Views (FAVs) within the LIBERO benchmark.  A single observation image (Anchor View 1) is captured by a camera mounted on the robot. This image is then processed to reconstruct a 3D point cloud representing the scene.  Using this point cloud, two additional synthetic views (Anchor Views 2 and 3) are rendered from different perspectives. These synthetic views demonstrate the ability of the system to generate varied viewpoints from a single observation, enriching the robot's understanding of the scene beyond what is visible from a single camera angle.", "section": "3.3 Application"}, {"figure_path": "https://arxiv.org/html/2501.01895/x5.png", "caption": "Figure 5: Qualitative comparison for single view video generation between EnerVerse and DynamiCrafter(FN) on RT-1 dataset. Since EnerVerse predict EOS frame at 42th frame for this task, we visualize 8th, 16th, 24th and 41th frame sampled from both generated sequence. The sequences generated by DynamiCrafter(FN) did not maintain the logic of the long-range task, producing many hallucinations as the sequence grew. In contrast, the sequence generated by EnerVerse was logically coherent, continuously and completely generating the future space of the entire task, and accurately predicting the EOS (End of Sequence) frame.", "description": "Figure 5 presents a qualitative comparison of single-view video generation between EnerVerse and DynamiCrafter (with FreeNoise) using the RT-1 dataset.  EnerVerse correctly predicts the end-of-sequence (EOS) frame at the 42nd frame. The figure shows frames 8, 16, 24, and 41 from both models' generated sequences. DynamiCrafter's output loses logical coherence and contains many hallucinations as the sequence length increases. In contrast, EnerVerse generates a logically coherent and complete future space for the entire task, correctly predicting the EOS frame, highlighting its superior performance in long-range tasks.", "section": "4.2 Comparison Results"}, {"figure_path": "https://arxiv.org/html/2501.01895/x6.png", "caption": "Figure 6: Qualitative results for multi anchor view generation on LIBERO benchmark (left) and real-world manipulation data (right), collected from AgiBot World\u00a0AgiBot (2024). One view is overlapped with a fixed RGB sensor and other views are manully set. Visualized Frames are uniformly sampled from generated sequence. We emphasize the consistency of objects across views by highlighting them with a red rectangle.", "description": "Figure 6 presents a qualitative comparison of multi-anchor view video generation.  The left side shows results from the LIBERO benchmark, a simulated robotics environment, while the right side showcases results from real-world robotic manipulation tasks collected using the AgiBot World dataset. In both scenarios, one camera view is fixed (an RGB sensor), and several generated anchor views offer additional perspectives. The generated frames are evenly sampled from the full sequence. To highlight the consistency of the generated objects across different views, red rectangles are used to outline the objects.", "section": "4.3 Further Studies"}, {"figure_path": "https://arxiv.org/html/2501.01895/x7.png", "caption": "Figure 7: Ablation results for context memory mechanism in video generation. Providing history information to the generation model with consecutive context (first line) often leads to unexpected model collapse while the model with sparse memory (second line) shows robust performance and save mush computing resources.", "description": "This figure demonstrates the impact of using a sparse memory mechanism versus a consecutive memory mechanism on video generation.  The top row shows the results of using consecutive frames as context for the generation model.  This approach leads to instability and model collapse, as evidenced by the degradation in video quality and coherence.  The bottom row shows the improved results of using a sparse memory mechanism, which selectively retains only the most important historical frames. This approach significantly enhances the robustness of the generation model and avoids model collapse, resulting in significantly improved video quality.  Furthermore, the sparse method reduces computational resources.", "section": "4.3 Further Studies"}, {"figure_path": "https://arxiv.org/html/2501.01895/x8.png", "caption": "Figure 8: Attention maps from different attention heads and layers of the model. The y-axis represents the predicted action space (the Query), spanning 8 steps, while the x-axis represents the Key-Value space. The first 4 columns in the KV space correspond to information from the Sparse Memory space, while the last 8 columns correspond to the predicted future space. These maps highlight how the model attends to sparse memory conditions (left) and future space conditions (right) when predicting actions. The bright yellow indicates a higher attention score while dark red indicates a lower one.", "description": "This figure visualizes attention maps from different layers and heads within the model's architecture.  Each map shows the model's attention distribution when predicting actions over eight time steps. The y-axis represents the predicted action space (Query), while the x-axis represents the Key-Value space containing information from both the sparse memory (first four columns) and the predicted future space (last eight columns).  The color intensity (yellow to dark red) represents the attention weight, with yellow indicating high attention and dark red indicating low attention. These visualizations demonstrate how the model balances its attention between previously stored information (sparse memory) and future predictions when making action decisions.  The shift in attention from sparse memory to future predictions over time is also highlighted.", "section": "4.3 Further Studies"}]