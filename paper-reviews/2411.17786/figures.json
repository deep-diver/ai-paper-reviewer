[{"figure_path": "https://arxiv.org/html/2411.17786/x1.png", "caption": "Figure 1: DreamCache is a finetuning-free personalized image generation method that achieves an optimal balance between subject fidelity, memory efficiency, and adherence to text prompts.", "description": "Figure 1 illustrates DreamCache's performance compared to other methods. It highlights DreamCache's ability to generate personalized images efficiently, accurately reflecting the subject while adhering to textual prompts and requiring minimal memory. Unlike other methods, which often trade off between these factors, DreamCache achieves a superior balance, showing its effectiveness and efficiency.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.17786/x2.png", "caption": "Figure 2: Personalized generations by DreamCache. The first column contains reference images. The generated images correspond to the text prompts above each column.", "description": "This figure showcases the capabilities of DreamCache in generating personalized images.  The leftmost column displays the reference images used as input to the model. Subsequent columns show images generated by DreamCache based on different text prompts, demonstrating how the model can adapt the reference subject to various contexts and styles while retaining its core features.  This illustrates the model's ability to personalize image generation according to textual instructions, even with no fine-tuning of the model.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.17786/x3.png", "caption": "Figure 3: Overview of DreamCache. Original U-Net layers are shown in violet, while the novel components introduced by DreamCache\u00a0are highlighted in green. During personalization, features from selected layers of the diffusion denoiser are cached from a single timestep, using a null text prompt. These cached features serve as reference-specific information. During generation, conditioning adapters inject the cached features into the denoiser, modulating the features of the generated image to create a personalized output.", "description": "DreamCache uses a pretrained diffusion model.  During personalization, features from specific layers of the diffusion model's U-Net are extracted at a single timestep, without using a text prompt. These cached features represent the reference subject.  A set of trainable conditioning adapters then use these cached features to modulate the generated image features, creating a personalized output. The original U-Net layers are shown in violet, while the added components are in green.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.17786/x4.png", "caption": "Figure 4: Visual comparison. Personalized generations on sample concepts. DreamCache\u00a0preserves reference concept appearance and does not suffer from background interference. BLIP-D [14] and Kosmos-G [20] cannot faithfully preserve visual details from the reference.", "description": "Figure 4 presents a visual comparison of personalized image generation results from three different methods: DreamCache, BLIP-Diffusion, and Kosmos-G.  Each method was tasked with generating images of various subjects (a dog, a can, a toy) in different contexts. The results demonstrate that DreamCache successfully preserves the appearance and details of the reference subject while avoiding background interference.  In contrast, BLIP-Diffusion and Kosmos-G struggle to faithfully reproduce the visual details of the reference subjects.", "section": "4. Experimental Results"}]