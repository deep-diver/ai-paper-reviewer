[{"content": "| Method | FT-free | Enc-free | Plug&Play | Ref-UNet-free | Extra Params | Train Params | # Dataset | Train Time |\n|---|---|---|---|---|---|---|---|---|\n| Textual Inversion [7] | \u2717 | \u2713 | \u2713 | \u2713 | 768* | 768* | 3-5* | 50 min* |\n| DreamBooth [27] | \u2717 | \u2713 | \u2717 | \u2713 | - | 0.9B* | 3-5* | 10 min* |\n| Custom Diffusion [13] | \u2717 | \u2713 | \u2717 | \u2713 | - | 57M* | 3-5* | 10 min* |\n| ELITE [36] | \u2713 | \u2717 | \u2717 | \u2713 | 457M | 77M | 125K | 14 days |\n| BLIP-Diffusion [14] | \u2713 | \u2717 | \u2713 | \u2713 | 380M | 1.5B | 129M | 96 days |\n| IP-Adapter [39] | \u2713 | \u2717 | \u2713 | \u2713 | 402M | 22M | 10M | 28 days |\n| Kosmos-G [20] | \u2713 | \u2717 | \u2713 | \u2713 | 1.6B | 1.6B | 9M | - |\n| JeDi [40] | \u2713 | \u2713 | \u2717 | \u2717 | - | 0.9B | 3M | 48 days |\n| SuTI [5] | \u2713 | \u2717 | \u2717 | \u2713 | 400M | 2.5B | 500K | - |\n| Subject-Diffusion [16] | \u2713 | \u2717 | \u2713 | \u2717 | 700M | 700M | 76M | - |\n| BootPig [23] | \u2713 | \u2713 | \u2717 | \u2717 | 0.95B | 0.95B | 200K | 18 hours |\n| ToffeeNet [42] | \u2713 | \u2717 | \u2717 | \u2713 | 632M | 0.9B | 5M | - |\n| CAFE [41] | \u2713 | \u2717 | \u2717 | \u2713 | 14B | 1B | 355K | - |\n| DreamCache (ours) | \u2713 | \u2713 | \u2713 | \u2713 | 25M | 25M | 400K | 40 hours |", "caption": "Table 1: Methods overview. Our DreamCache\u00a0achieves state-of-the-art generation quality at reduced computational costs. *: value refers to the personalization stage for each personal subject.", "description": "This table compares different methods for personalized image generation, highlighting their key characteristics.  It shows whether each method is finetuning-free, whether it requires an encoder, if it uses a plug-and-play approach, if it requires a reference U-Net, the number of extra parameters added, the size of training parameters used, the number of images in the training dataset, and the total training time. The table emphasizes that DreamCache achieves state-of-the-art results at a significantly lower computational cost compared to existing methods.", "section": "1. Introduction"}, {"content": "| \u201cA dragon\u2026\u201d | \u201cas street graffiti\u201d | \u201cplaying with fire\u201d | \u201cas a plushie\u201d | \u201cworking as a barista\u201d |\n|---|---|---|---|---|\n| [https://arxiv.org/html/2411.17786/figures/flux_dragon.png](https://arxiv.org/html/2411.17786/figures/flux_dragon.png) | [https://arxiv.org/html/2411.17786/figures/graffiti_dragon.jpg](https://arxiv.org/html/2411.17786/figures/graffiti_dragon.jpg) | [https://arxiv.org/html/2411.17786/figures/playing_fire.png](https://arxiv.org/html/2411.17786/figures/playing_fire.png) | [https://arxiv.org/html/2411.17786/figures/plushie_dragon2.jpg](https://arxiv.org/html/2411.17786/figures/plushie_dragon2.jpg) | [https://arxiv.org/html/2411.17786/figures/barista.png](https://arxiv.org/html/2411.17786/figures/barista.png) |\n| \u201cA cat\u2026\u201d | \u201cin Ukiyo-e style\u201d | \u201cwith a rainbow scarf\u201d | \u201cVan Gogh painting\u201d | \u201cwearing a diploma hat\u201d |\n|---|---|---|---|---|\n| [https://arxiv.org/html/2411.17786/figures/cat_ref.jpg](https://arxiv.org/html/2411.17786/figures/cat_ref.jpg) | [https://arxiv.org/html/2411.17786/figures/ukiyo-cat.jpg](https://arxiv.org/html/2411.17786/figures/ukiyo-cat.jpg) | [https://arxiv.org/html/2411.17786/figures/rainbow_scarf_cat.jpg](https://arxiv.org/html/2411.17786/figures/rainbow_scarf_cat.jpg) | [https://arxiv.org/html/2411.17786/figures/van_gogh_cat.jpg](https://arxiv.org/html/2411.17786/figures/van_gogh_cat.jpg) | [https://arxiv.org/html/2411.17786/figures/cat_diploma.jpg](https://arxiv.org/html/2411.17786/figures/cat_diploma.jpg) |", "caption": "Table 2: Quantitative results on DreamBooth. DreamCache\u00a0obtains a better balance between DINO score and CLIP-T compared to all baselines,\nwhile also offering a more efficient computational tradeoff (see Table\u00a01).", "description": "Table 2 presents a quantitative comparison of DreamCache against other state-of-the-art methods for personalized image generation, focusing on DreamBooth as a baseline.  The evaluation uses three metrics: DINO (evaluates image similarity to reference images), CLIP-I (measures image-text similarity by comparing generated images to reference images using CLIP), and CLIP-T (assesses text alignment between generated images and the input text prompts using CLIP). DreamCache demonstrates a superior balance between DINO and CLIP-T scores compared to other models. It also showcases significantly improved computational efficiency when compared to other models (refer to Table 1 for detailed information on the computational trade-offs).", "section": "4. Experimental Results"}, {"content": "| Method | Backbone | #Ref | DINO (\u2191) | CLIP-I (\u2191) | CLIP-T (\u2191) |\n|---|---|---|---|---|---| \n| <img src=\"https://arxiv.org/html/2411.17786/test-time finetuning.png\" style=\"width:20.6pt;height:56.9pt;vertical-align:-0.0pt;\"> | DreamBooth [27] | Imagen | 3-5 | 0.696 | 0.812 | 0.306 |\n|  | DreamBooth [27] | SD 1.5 | 3-5 | 0.668 | 0.803 | 0.305 |\n|  | Textual Inversion [7] | SD 1.5 | 3-5 | 0.569 | 0.780 | 0.255 |\n|  | Custom Diffusion [13] | SD 1.5 | 3-5 | 0.643 | 0.790 | 0.305 |\n|  | BLIP-Diffusion (FT) [14] | SD 1.5 | 3-5 | 0.670 | 0.805 | 0.302 |\n| <img src=\"https://arxiv.org/html/2411.17786/finetuning free.png\" style=\"width:16.3pt;height:56.9pt;vertical-align:-0.0pt;\"> | ELITE [36] | SD 1.5 | 1 | 0.621 | 0.771 | 0.293 |\n|  | BLIP-Diffusion [14] | SD 1.5 | 1 | 0.594 | 0.779 | 0.300 |\n|  | IP-Adapter [39] | SD 1.5 | 1 | 0.667 | 0.813 | 0.289 |\n|  | Kosmos-G [20] | SD 1.5 | 1 | 0.694 | 0.847 | 0.287 |\n|  | Jedi [40] | SD 1.5 | 1 | 0.619 | 0.782 | 0.304 |\n|  | **DreamCache (ours)** | SD 1.5 | 1 | 0.713 | 0.810 | 0.298 |\n|  | Re-Imagen [4] | Imagen | 1-3 | 0.600 | 0.740 | 0.270 |\n|  | SuTI [5] | Imagen | 1-3 | 0.741 | 0.819 | 0.304 |\n|  | Subject-Diffusion [16] | SD 2.1 | 1 | 0.771 | 0.779 | 0.293 |\n|  | BootPig [23] | SD 2.1 | 3 | 0.674 | 0.797 | 0.311 |\n|  | ToffeeNet [42] | SD 2.1 | 1 | 0.728 | 0.817 | 0.306 |\n|  | CAFE [41] | SD 2.1 | 1 | 0.715 | 0.827 | 0.294 |\n|  | **DreamCache (ours)** | SD 2.1 | 1 | 0.767 | 0.816 | 0.301 |", "caption": "Table 3: Computational comparison. *: time to generate an image with 100 timesteps, evaluated on a single NVIDIA A100 GPU.", "description": "This table compares the computational efficiency of different methods for generating personalized images. It shows the inference time (in seconds) required to generate a single image with 100 timesteps using a single NVIDIA A100 GPU, as well as the size of the additional parameters (in MB) required for each method. The table highlights DreamCache's efficiency compared to other state-of-the-art methods.", "section": "4. Experimental Results"}, {"content": "| Description |  |  A dog wearing a santa hat with the Eiffel Tower in the background |  |  |  |  |\n|---|---|---|---|---|---|---|---|\n| Reference |  | BLIP-D | Kosmos-G | DreamCache | BLIP-D | Kosmos-G | DreamCache |\n| [https://arxiv.org/html/2411.17786/figures/comparison/dog_reference.png](https://arxiv.org/html/2411.17786/figures/comparison/dog_reference.png) |  | [https://arxiv.org/html/2411.17786/figures/comparison/blip_dog_2.png](https://arxiv.org/html/2411.17786/figures/comparison/blip_dog_2.png) | [https://arxiv.org/html/2411.17786/figures/comparison/kosmos_g_dog_santa.png](https://arxiv.org/html/2411.17786/figures/comparison/kosmos_g_dog_santa.png) | [https://arxiv.org/html/2411.17786/figures/comparison/dog_santa.png](https://arxiv.org/html/2411.17786/figures/comparison/dog_santa.png) | [https://arxiv.org/html/2411.17786/figures/comparison/blip_eiffel_dog.png](https://arxiv.org/html/2411.17786/figures/comparison/blip_eiffel_dog.png) | [https://arxiv.org/html/2411.17786/figures/comparison/kosmos_g_dog_eiffel_2.png](https://arxiv.org/html/2411.17786/figures/comparison/kosmos_g_dog_eiffel_2.png) | [https://arxiv.org/html/2411.17786/figures/comparison/eiffel_dog_1.png](https://arxiv.org/html/2411.17786/figures/comparison/eiffel_dog_1.png) |\n| Description |  |  A can floating on top of water with a mountain in the background |  |  |  |  |\n|---|---|---|---|---|---|---|---|\n| Reference |  | BLIP-D | Kosmos-G | DreamCache | BLIP-D | Kosmos-G | DreamCache |\n| [https://arxiv.org/html/2411.17786/figures/comparison/can_reference.png](https://arxiv.org/html/2411.17786/figures/comparison/can_reference.png) |  | [https://arxiv.org/html/2411.17786/figures/comparison/can_blipd.png](https://arxiv.org/html/2411.17786/figures/comparison/can_blipd.png) | [https://arxiv.org/html/2411.17786/figures/comparison/kosmos_g_can.png](https://arxiv.org/html/2411.17786/figures/comparison/kosmos_g_can.png) | [https://arxiv.org/html/2411.17786/figures/comparison/can_floating_2.png](https://arxiv.org/html/2411.17786/figures/comparison/can_floating_2.png) | [https://arxiv.org/html/2411.17786/figures/comparison/blip_can_mountain.png](https://arxiv.org/html/2411.17786/figures/comparison/blip_can_mountain.png) | [https://arxiv.org/html/2411.17786/figures/comparison/can_mountain_kosmos.png](https://arxiv.org/html/2411.17786/figures/comparison/can_mountain_kosmos.png) | [https://arxiv.org/html/2411.17786/figures/comparison/can_mountain_1.png](https://arxiv.org/html/2411.17786/figures/comparison/can_mountain_1.png) |\n| Description |  |  A toy on the beach on top of a white rug |  |  |  |  |\n|---|---|---|---|---|---|---|---|\n| Reference |  | BLIP-D | Kosmos-G | DreamCache | BLIP-D | Kosmos-G | DreamCache |\n| [https://arxiv.org/html/2411.17786/figures/comparison/monster_toy_reference.png](https://arxiv.org/html/2411.17786/figures/comparison/monster_toy_reference.png) |  | [https://arxiv.org/html/2411.17786/figures/comparison/blip_monster_beach.png](https://arxiv.org/html/2411.17786/figures/comparison/blip_monster_beach.png) | [https://arxiv.org/html/2411.17786/figures/comparison/monster_kosmos_beach.png](https://arxiv.org/html/2411.17786/figures/comparison/monster_kosmos_beach.png) | [https://arxiv.org/html/2411.17786/figures/comparison/monster_beach.png](https://arxiv.org/html/2411.17786/figures/comparison/monster_beach.png) | [https://arxiv.org/html/2411.17786/figures/comparison/blip_monster_white_rug2.png](https://arxiv.org/html/2411.17786/figures/comparison/blip_monster_white_rug2.png) | [https://arxiv.org/html/2411.17786/figures/comparison/monster_white_rug_kosmos.png](https://arxiv.org/html/2411.17786/figures/comparison/monster_white_rug_kosmos.png) | [https://arxiv.org/html/2411.17786/figures/comparison/monster_white_rug.png](https://arxiv.org/html/2411.17786/figures/comparison/monster_white_rug.png) |", "caption": "Table 4: Reference feature integration. DreamCache\u00a0uses the best tradeoff between accuracy and complexity.", "description": "Table 4 presents a comparison of different methods for integrating reference features into the image generation process. It shows that DreamCache achieves the best balance between the quality of generated images (measured by CLIP-I and CLIP-T scores) and the number of parameters used.  The methods compared include Textual Sum, Spatial Sum, Decoupled Blocks, and DreamCache's Spatial Concat method. The table helps to justify the design choices made in DreamCache by demonstrating its superior performance compared to other approaches while maintaining efficiency.", "section": "3. Method"}, {"content": "| Method | Inference Time* | Extra Params Size |\n|---|---|---|\n| ELITE [36] | 6.24 s | 914 MB |\n| BLIP-Diffusion [14] | 3.92 s | 760 MB |\n| BootPig [23] | 7.55 s | 1900 MB |\n| **DreamCache (ours)** | 3.88 s | 42 MB |", "caption": "Table 5: Cache positioning in the U-Net backbone offers a further tradeoff between accuracy and complexity.", "description": "This table presents the results of an ablation study that explores different cache positions within the U-Net backbone of the DreamCache model. It investigates how the choice of layers from which to extract and cache features affects the tradeoff between the model's accuracy in generating personalized images and the overall complexity of the model. By testing different combinations of middle and decoder layers from which to extract features, this table helps to optimize DreamCache for performance while maintaining efficiency.", "section": "3. Method"}, {"content": "| Method | CLIP-I (\u2191) | CLIP-T (\u2191) | Params |\n|---|---|---|---| \n| Textual Sum [39] | 0.788 | 0.282 | 19M |\n| Spatial Sum | 0.812 | 0.293 | 16M |\n| Decoupled Blocks [9] | 0.808 | 0.300 | 61M |\n| **Spatial Concat (ours)** | 0.810 | 0.298 | 25M |", "caption": "Table 6: Caching with text is not influential and adds complexity.", "description": "Table 6 presents an ablation study on the impact of using text prompts during the feature caching process in DreamCache. It compares the performance of DreamCache when text input is used versus when no text input is used during caching. The results show that using text during caching doesn't improve performance and adds unnecessary complexity to the model.", "section": "3. Method"}, {"content": "| Encoder | Middle | Decoder | CLIP-I (<img src=\"https://arxiv.org/html/2411.17786/uparrow.png\" alt=\"\u2191\">) | CLIP-T (<img src=\"https://arxiv.org/html/2411.17786/uparrow.png\" alt=\"\u2191\">) | Params |\n|---|---|---|---|---|---| \n| \u2713 | \u2717 | \u2717 | 0.721 | 0.303 | 11M |\n| \u2713 | \u2713 | \u2717 | 0.749 | 0.306 | 19M |\n| \u2717 | \u2713 | \u2717 | 0.716 | 0.302 | 8M |\n| \u2717 | \u2717 | \u2713 | 0.799 | 0.296 | 17M |\n| \u2717 | \u2713 | \u2713 | 0.810 | 0.298 | 25M |\n| \u2713 | \u2713 | \u2713 | 0.813 | 0.297 | 36M |", "caption": "Table 7: Dataset impact for both synthetic and real data.", "description": "This table presents the quantitative results achieved by training the DreamCache model's conditioning adapters on various datasets.  It shows how the model's performance on image and text alignment tasks varies depending on the size and type of training data used.  Specifically, it compares the performance using synthetic datasets of 50K, 200K, and 400K samples, as well as a real-world dataset, LAION-5M, to demonstrate the impact of dataset size and the nature of the data (synthetic vs. real) on the model's generalization capabilities.", "section": "4. Experimental Results"}, {"content": "| Text-Free | CLIP-I (\u2191) | CLIP-T (\u2191) |\n|---|---|---|\n| \u2717 | 0.811 | 0.295 |\n| \u2713 | 0.810 | 0.298 |", "caption": "Table S1: Masked metrics quantitative evaluation.", "description": "This table presents a quantitative comparison of several methods for image generation, focusing on the accuracy of preserving the subject in generated images while minimizing interference from the background.  It uses masked versions of standard image similarity metrics (MCLIP-I and MDINO) to evaluate the quality of subject preservation.  The metrics assess the similarity between generated images and the masked portions of the corresponding reference images, isolating the subject from the background and thus providing a more precise evaluation of personalization.", "section": "S2. Additional Evaluations"}, {"content": "| Dataset | CLIP-I (\u2191) | CLIP-T (\u2191) |\n|---|---|---|\n| Synthetic-50K | 0.781 | 0.304 |\n| Synthetic-200K | 0.797 | 0.301 |\n| Synthetic-400K | 0.810 | 0.298 |\n| LAION-5M | 0.814 | 0.242 |", "caption": "Table S2: Reference features ablation study.", "description": "This table presents the results of an ablation study investigating the impact of different numbers and selections of reference features on the performance of DreamCache. It shows how varying the number of reference features and using only middle or respective layers impacts the CLIP-I and CLIP-T scores, which measure image and text alignment respectively. This analysis helps determine the optimal strategy for selecting reference features to balance model accuracy and efficiency.", "section": "S2. Additional Evaluations"}, {"content": "|---|---|---|---|\n| ![Refer to caption](https://arxiv.org/html/2411.17786/figures/attn_ref_16x16_sneaker.png) | ![Refer to caption](https://arxiv.org/html/2411.17786/figures/attn_maps_16x16_sneaker.png) | ![Refer to caption](https://arxiv.org/html/2411.17786/figures/ref_image_with_rect_2ref.png) | ![Refer to caption](https://arxiv.org/html/2411.17786/figures/superimposed_image_2ref.png) |\n| ![Refer to caption](https://arxiv.org/html/2411.17786/figures/attn_ref_16x16_backpack.png) | ![Refer to caption](https://arxiv.org/html/2411.17786/figures/attn_maps_16x16_backpack.png) | ![Refer to caption](https://arxiv.org/html/2411.17786/figures/attn_ref_32x32_backpack.png) | ![Refer to caption](https://arxiv.org/html/2411.17786/figures/attn_maps_32x32_backpack.png) |", "caption": "Table S3: Encoding timestep ablation study.", "description": "This table presents the results of an ablation study on the effect of different encoding timesteps on the performance of DreamCache.  It shows how varying the timestep used to extract and cache reference features impacts the model's ability to align generated images with both reference images (CLIP-I) and text prompts (CLIP-T). The study evaluates three different timesteps: 1, 150, and 300, demonstrating the optimal timestep for obtaining a good balance between visual fidelity and textual coherence.", "section": "4.3. Ablation Studies"}]