[{"figure_path": "https://arxiv.org/html/2503.10568/extracted/6278367/fig/samples.png", "caption": "Figure 1: Visual Autoregressive Modeling with Randomized Parallel Generation (ARPG): A high-quality and efficient framework for image synthesis. ARPG enables (a) class-conditional generation with just 64-step parallel decoding and (b) outperform recent representative works in this line (e.g., VAR\u00a0[39], LlamaGen\u00a0[35]) in throughput, memory consumption, and quality. It further supports (c) controllable generation and zero-shot generalization, including (d) class-conditional editing, inpainting, outpainting, and (e) resolution expansion.", "description": "This figure showcases the capabilities of the ARPG model for autoregressive image generation.  Panel (a) demonstrates class-conditional image generation, achieving high quality with only 64 parallel decoding steps. Panel (b) presents a quantitative comparison, highlighting ARPG's superior performance over state-of-the-art methods (VAR and LlamaGen) in terms of speed (throughput), memory efficiency, and image quality (FID).  The remaining panels illustrate the model's versatility: (c) controllable generation, (d) zero-shot capabilities (editing, inpainting, and outpainting), and (e) resolution expansion.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.10568/extracted/6278367/fig/control.png", "caption": "Figure 2: Comparison of different methods. RAR\u00a0[50]: The position of the next predicted token is fused into the current token via additive positional embedding. Its joint probability distribution is defined as \u220fi=1np\u2062(x\u03c4i\u2223x\u03c41+p\u03c42,\u2026,x\u03c4i\u22121+p\u03c4i)superscriptsubscriptproduct\ud835\udc561\ud835\udc5b\ud835\udc5dconditionalsubscript\ud835\udc65subscript\ud835\udf0f\ud835\udc56subscript\ud835\udc65subscript\ud835\udf0f1subscript\ud835\udc5dsubscript\ud835\udf0f2\u2026subscript\ud835\udc65subscript\ud835\udf0f\ud835\udc561subscript\ud835\udc5dsubscript\ud835\udf0f\ud835\udc56\\prod_{i=1}^{n}p\\big{(}x_{\\tau_{i}}\\mid x_{\\tau_{1}}+p_{\\tau_{2}},\\dots,x_{%\n\\tau_{i-1}}+p_{\\tau_{i}}\\big{)}\u220f start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_p ( italic_x start_POSTSUBSCRIPT italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2223 italic_x start_POSTSUBSCRIPT italic_\u03c4 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_p start_POSTSUBSCRIPT italic_\u03c4 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_\u03c4 start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_p start_POSTSUBSCRIPT italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ). RandAR\u00a0[25]: The position of the next predicted token is explicitly inserted as an instructional token within the input sequence. This yields the probability distribution \u220fi=1np\u2062(x\u03c4i\u2223x\u03c41,p\u03c42,\u2026,x\u03c4i\u22121,p\u03c4i)superscriptsubscriptproduct\ud835\udc561\ud835\udc5b\ud835\udc5dconditionalsubscript\ud835\udc65subscript\ud835\udf0f\ud835\udc56subscript\ud835\udc65subscript\ud835\udf0f1subscript\ud835\udc5dsubscript\ud835\udf0f2\u2026subscript\ud835\udc65subscript\ud835\udf0f\ud835\udc561subscript\ud835\udc5dsubscript\ud835\udf0f\ud835\udc56\\prod_{i=1}^{n}p\\big{(}x_{\\tau_{i}}\\mid x_{\\tau_{1}},p_{\\tau_{2}},\\dots,x_{%\n\\tau_{i-1}},p_{\\tau_{i}}\\big{)}\u220f start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_p ( italic_x start_POSTSUBSCRIPT italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2223 italic_x start_POSTSUBSCRIPT italic_\u03c4 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_\u03c4 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_\u03c4 start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ). ARPG (ours): The position of the next predicted token is integrated as a query within the attention mechanism. The corresponding probabilistic model, formalized in Eq.\u00a0(8).", "description": "Figure 2 illustrates the differences in how three different autoregressive models (RAR, RandAR, and ARPG) handle the positional information of the next predicted token during image generation. RAR fuses positional information into the current token using additive positional embedding. RandAR explicitly inserts the position as an instructional token in the input sequence.  ARPG integrates positional information into the query of the attention mechanism, decoupling positional guidance from content representation. The corresponding probability models for RAR and RandAR are shown, highlighting their differences in incorporating position information.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.10568/x2.png", "caption": "(a) Architecture", "description": "This figure shows the architecture of the ARPG model, a two-pass decoder architecture. The first pass uses self-attention layers to extract contextual representations of image tokens as key-value pairs.  The second pass uses cross-attention layers with target-aware queries that attend to these key-value pairs to guide the prediction. During training, the number of queries matches the number of key-value pairs. The positional embedding of each key reflects its actual position, while the positional embedding of each query is shifted right to align with its target position.  During inference, multiple queries are input simultaneously, sharing a common KV cache to enable parallel decoding. This architecture enables training and inference in fully random token order.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.10568/x3.png", "caption": "(b) Training and inference details of guided-decoding", "description": "This figure illustrates the training and inference mechanisms of the guided decoding process in ARPG.  During training, the number of queries matches the number of key-value pairs.  Each query's positional embedding is right-shifted to align with its target position, while each key's positional embedding reflects its actual position. This allows the model to learn to predict tokens at random positions.  During inference, however, multiple queries are input concurrently, sharing a common KV cache to enable parallel decoding. This shared cache drastically improves efficiency.", "section": "3.2. Randomized & Parallelized AR Model"}, {"figure_path": "https://arxiv.org/html/2503.10568/x4.png", "caption": "Figure 3: 3(a) ARPG architecture: We employ a Two-Pass Decoder architecture. In the first pass, N\ud835\udc41Nitalic_N self-attention layers extract contextual representations of the image token sequence as global key-value pairs. In the second pass, N\ud835\udc41Nitalic_N cross-attention layers use target-aware queries that attend to these global key-value pairs to guide prediction.\n3(b) During training, the number of queries matches the number of key-value pairs. Each key\u2019s positional embedding reflects its actual position, while each query\u2019s positional embedding is right-shifted to align with its target position.\nDuring inference, multiple queries are input simultaneously, sharing a common KV cache to enable parallel decoding.", "description": "Figure 3 illustrates the ARPG architecture, a two-pass decoder model.  The first pass uses self-attention layers to process the image token sequence and generate global key-value pairs representing contextual information. The second pass employs cross-attention layers with 'target-aware queries' to focus on specific parts of the global key-value pairs, guiding the prediction process. During training, the number of queries equals the number of key-value pairs.  Each key's positional embedding indicates its position in the sequence, while each query's embedding is offset to point to the position of the token to be predicted.  This design allows for parallel decoding during inference because multiple queries can be processed simultaneously using a shared key-value cache.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.10568/extracted/6278367/fig/supp-cls.png", "caption": "(a) Class-Conditional Image Generation", "description": "This figure shows several images generated by the ARPG model given different class labels.  It demonstrates the model's ability to generate high-quality, diverse images conditioned on class information. Each image in the grid represents a different class.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.10568/extracted/6278367/fig/supp-ctrl.png", "caption": "(b) Controllable Image Generation", "description": "This figure shows example images generated by the ARPG model under the control of additional information, such as canny edges and depth maps.  The model is able to generate images that incorporate these additional inputs, demonstrating its capability for controllable image synthesis.  This showcases the flexibility of the ARPG approach in creating images beyond simple class-conditional generation.", "section": "4. Class-Conditional Image Generation"}, {"figure_path": "https://arxiv.org/html/2503.10568/extracted/6278367/fig/supp-outpaint.png", "caption": "Figure 4: Generated samples of ARPG on ImageNet-1K 256\u00d7\\times\u00d7256: 4(a) Class-conditional image generation. 4(b) Controllable image generation with canny edges and depth maps as conditions respectively. All images are sampled using 64 steps.", "description": "Figure 4 presents example images generated by the ARPG model.  Subfigure 4(a) showcases class-conditional image generation, where the model generates images based solely on a given class label. Subfigure 4(b) demonstrates controllable image generation, in which the model generates images guided by both a class label and additional conditional information, specifically canny edges and depth maps. This illustrates the model's ability to incorporate external cues to influence the image synthesis process.  All images shown were generated with 64 sampling steps.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.10568/extracted/6278367/fig/supp-inpaint.png", "caption": "Figure 5: Samples of zero-shot inference. Top: Inpainting and class-conditional editing. Bottom: Outpainting.", "description": "Figure 5 showcases the model's zero-shot capabilities on image manipulation tasks without explicit training. The top row demonstrates inpainting (filling missing parts of an image) and class-conditional editing (modifying an existing image based on a class label). The bottom row shows outpainting (extending an image beyond its original boundaries). This highlights the model's ability to generalize to various tasks beyond the standard image generation.", "section": "4.4. Zero-Shot Generalization"}, {"figure_path": "https://arxiv.org/html/2503.10568/extracted/6278367/fig/supp-expansion.png", "caption": "Figure A: Implementation Details of Controllable Image Generation. For clarity, we illustrate the process using raster-order as an example. The figure only illustrates the interaction between query and key in the attention mechanism and its output, omitting the value for simplicity. [CLS]: Class token.", "description": "Figure A illustrates the process of controllable image generation using the ARPG model.  It focuses on how the model handles the interaction between queries and keys in the attention mechanism, specifically highlighting how these elements guide the generation process. To simplify the illustration, raster-order generation is used as an example, and the 'value' component of the attention mechanism is omitted.  The class token ([CLS]) is also shown.", "section": "A. Details of Controllable Image Generation"}]