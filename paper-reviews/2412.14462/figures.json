[{"figure_path": "https://arxiv.org/html/2412.14462/x1.png", "caption": "Figure 1: Pipeline of constructing the SAM-FB dataset.\nThe background is inpainted and high-quality foreground objects are preserved through a data quality control stage.", "description": "The SAM-FB dataset creation process is depicted in this diagram.  First, the SA-1B dataset is used to generate foreground-background pairs by identifying objects using Segment Anything Model (SAM) and masking them out. The masked-out regions representing the background are then inpainted using a model like LAMA, resulting in a complete background image.  A data quality control step filters out low-quality foregrounds and backgrounds to ensure high image quality.  The end result is a dataset consisting of foreground images, background images, positional prompts (points, bounding boxes, masks, or null), and corresponding ground truth images.", "section": "3. Dataset"}, {"figure_path": "https://arxiv.org/html/2412.14462/x2.png", "caption": "Figure 2: The framework of MADD. Foreground objects are encoded using a DINOv2 encoder, serving as the guidance signal through the cross-attention mechanism. The position prompt encoder unifies different types of position prompts, which are then concatenated with the latent mask \ud835\udc26\ud835\udc2dsubscript\ud835\udc26\ud835\udc2d\\mathbf{m_{t}}bold_m start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT. The background is encoded using a VAE encoder and then concatenated with the latent image \ud835\udc33\ud835\udc2dsubscript\ud835\udc33\ud835\udc2d\\mathbf{z_{t}}bold_z start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT. We use a dual branch structure to denoise RGB image \ud835\udc33\ud835\udc33\\mathbf{z}bold_z and object mask \ud835\udc26\ud835\udc26\\mathbf{m}bold_m simultaneously.", "description": "The Mask-Aware Dual Diffusion (MADD) model uses a dual-branch architecture. One branch processes the RGB image, and the other branch processes the object mask.  Both branches use diffusion models to denoise their respective inputs.  The foreground object is encoded using a DINOv2 encoder, creating a feature representation that acts as guidance through a cross-attention mechanism.  Positional information (points, bounding boxes, masks, or even no prompt) is encoded using a dedicated encoder. This information is combined with the latent mask representation at timestep t (m<sub>t</sub>). The background is encoded via a Variational Autoencoder (VAE) and its latent representation (z<sub>t</sub>) is also incorporated. The dual branches work in parallel, denoising both the RGB image (z) and object mask (m) to generate a final composed image.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2412.14462/x3.png", "caption": "Figure 3: Qualitative results of MADD on the SAM-FB test set. Each row corresponds to one type of prompt, i.e., point, bounding box, mask, and null, respectively. Our MADD simultaneously predicts the RGB image and the object mask.", "description": "This figure showcases the qualitative results of the Mask-Aware Dual Diffusion (MADD) model on the SAM-FB test dataset.  The figure displays four rows, each representing a different type of input prompt used to guide the object insertion: point, bounding box, mask, and null (no explicit position guidance). For each prompt type, the figure presents the original background image, the foreground object, and the results generated by the MADD model.  The MADD model simultaneously predicts both the RGB image with the inserted object and the corresponding object mask, demonstrating its ability to accurately place objects in diverse scenarios and based on different positional cues.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.14462/x4.png", "caption": "(a) Location Adjustment", "description": "The figure demonstrates the model's ability to adjust the location of a foreground object to achieve affordance-aware insertion.  Given an ambiguous prompt (e.g., a point), the model intelligently positions the object within the scene, considering factors like background semantics and spatial relationships to ensure the object's placement is realistic and natural.", "section": "5.2. Result on In-the-wild Images"}, {"figure_path": "https://arxiv.org/html/2412.14462/x5.png", "caption": "(b) View Adjustment", "description": "The figure demonstrates the model's ability to adjust the viewpoint of the inserted object to maintain consistency with the background scene. The example shows how the model alters the orientation of a car to align with the lane markings, showcasing an understanding of perspective and spatial relationships.", "section": "5.2 Result on In-the-wild Images"}, {"figure_path": "https://arxiv.org/html/2412.14462/x6.png", "caption": "(c) Size Adjustment", "description": "The model adjusts the size of the coffee beans to match the background scene, even though only a point prompt and a reference image of the statue are provided. This demonstrates the model's ability to understand and adjust object size to match the context of the scene.", "section": "5.2. Result on In-the-wild Images"}, {"figure_path": "https://arxiv.org/html/2412.14462/x7.png", "caption": "(d) Automatic Localization", "description": "When no explicit position prompt is given, the model can automatically determine an appropriate location to place the object, demonstrating its ability to understand and utilize affordances in object-scene composition.", "section": "5.2 Result on In-the-wild Images"}, {"figure_path": "https://arxiv.org/html/2412.14462/extracted/6077377/imgs/diverse.png", "caption": "Figure 4: We test ambiguous prompts (points and blank) on the in-the-wild images. When providing the prompt of point, 4(a), 4(b), and 4(c) show that our model can adjust properties of foreground objects to achieve the affordance insertion.4(d) illustrates that the model could find the suitable position to insert the object.", "description": "This figure demonstrates the model's ability to handle ambiguous position prompts for object insertion in real-world scenes.  It showcases four scenarios, all using in-the-wild images: (a) the model adjusts the foreground object's location to achieve a natural placement given a point prompt; (b) it adjusts the foreground object's viewpoint; (c) it adjusts the foreground object's size; and (d) it automatically determines the best insertion location in the absence of any explicit positional guidance (null prompt).  These examples highlight the model's ability to understand affordances and create realistic compositions.", "section": "5.2 Result on In-the-wild Images"}, {"figure_path": "https://arxiv.org/html/2412.14462/x8.png", "caption": "Figure 5: MADD can give different feasible solutions for ambiguous prompts such as point and blank.", "description": "This figure showcases the model's ability to generate multiple plausible outputs even with vague positional guidance.  The top row illustrates different outputs when only a single point is provided as input.  The bottom row demonstrates the model's ability to place objects appropriately even without explicit positional cues, highlighting its capacity for creative and contextually aware object insertion.", "section": "5.2 Result on In-the-wild Images"}, {"figure_path": "https://arxiv.org/html/2412.14462/extracted/6077377/imgs/criteria.png", "caption": "(a) Rank distribution for different methods. Our method has the highest proportion of rank 1 and the least proportion of rank 5.", "description": "This figure displays a bar chart comparing the performance of different image composition methods, including the proposed method, across five ranking criteria.  The x-axis represents the different methods, while the y-axis shows the percentage of times each method received a specific rank (1 to 5). A higher proportion of rank 1 indicates superior performance.  The chart visually demonstrates that the proposed method achieved the highest percentage of rank 1 and the lowest percentage of rank 5, signifying its superior overall performance compared to other methods.", "section": "5.1 Results on the SAM-FB Test Set"}, {"figure_path": "https://arxiv.org/html/2412.14462/x9.png", "caption": "(b) Rank-1 distribution for each criterion. Each pie chart represents the proportion of times each model achieved Rank-1 for a specific evaluation criterion. Our method dominates every metric.", "description": "This figure presents a breakdown of the Rank-1 results for each evaluation metric across different models.  Each pie chart shows the percentage of times a specific model achieved the top rank (Rank-1) for a particular evaluation criterion. The criteria used are: Foreground and Background Integration, Foreground Clarity and Detail, Foreground Consistency with Reference, Lighting and Shadows on Foreground, and Color Consistency. The results clearly indicate that the proposed model outperforms other state-of-the-art models in all categories.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.14462/x10.png", "caption": "Figure 6: Human evaluation results on in-the-wild Images. We compared 10 groups of images according to different criteria. Our MADD model outperforms SDXL\u00a0[39], GLI-GEN\u00a0[29], ObjectStitch\u00a0[42] and PBE\u00a0[47] on overall ranking and each criteria.", "description": "This figure displays the results of a human evaluation comparing the performance of the MADD model against several other state-of-the-art image editing models.  Ten sets of images were generated by each model, and human evaluators judged these images using five criteria:  Foreground and Background Integration, Foreground Clarity and Detail, Foreground Consistency with Reference, Lighting and Shadows on Foreground, and Color Consistency. The results show that the MADD model consistently outperforms the others across all evaluation criteria, receiving the highest rank in the majority of cases for each category.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.14462/extracted/6077377/imgs/FID-CLIP.png", "caption": "Figure 7: MADD can work on images of higher resolution, generating sharper edges, clearer reflections, and improved texture details.", "description": "This figure demonstrates the scalability of the MADD model to higher-resolution images (512x512 pixels).  The results show significant improvements in image quality compared to lower resolution inputs (256x256 pixels), including sharper edges, more realistic reflections, and enhanced texture details in the inserted objects. This highlights the model's ability to handle higher-resolution images while maintaining consistent performance in generating high-quality and visually pleasing outputs.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.14462/extracted/6077377/imgs/sample.png", "caption": "Figure 8: More in-the-wild examples with null prompts. The model can generate an affordance-feasible solution to insert the foreground objects according to the background scene.", "description": "Figure 8 presents several examples of the MADD model's ability to perform affordance-aware object insertion without explicit positional guidance.  Each example shows a background image and a foreground object.  The model successfully integrates the foreground object into the background, making realistic and contextually appropriate adjustments to its position, size, and orientation. This highlights the model's capacity for autonomous and contextually aware object placement.", "section": "5.2 Result on In-the-wild Images"}, {"figure_path": "https://arxiv.org/html/2412.14462/extracted/6077377/imgs/category.png", "caption": "Figure 9: FID-CLIP score curve on 128\u00d7128128128128\\times 128128 \u00d7 128 resolution with different guidance scale [1.0,3.0,4.0,5.0,6.0,7.0]1.03.04.05.06.07.0[1.0,3.0,4.0,5.0,6.0,7.0][ 1.0 , 3.0 , 4.0 , 5.0 , 6.0 , 7.0 ].", "description": "This figure shows the relationship between FID (Fr\u00e9chet Inception Distance) and CLIP (Contrastive Language\u2013Image Pre-training) scores, which are used to evaluate the quality of generated images and the alignment between the generated image and the prompt, respectively.  The x-axis represents different guidance scales (1.0 through 7.0) used during the image generation process with the Classifier-Free Guidance approach.  The y-axis shows the FID and CLIP scores for each guidance scale.  The plot illustrates how the FID and CLIP scores change as the guidance scale increases, providing insights into the optimal guidance scale for balancing image quality and prompt adherence when using the Mask-Aware Dual Diffusion (MADD) model. The resolution of the generated images for this analysis is 128x128 pixels.", "section": "4.4. Implementation Details"}, {"figure_path": "https://arxiv.org/html/2412.14462/x11.png", "caption": "(a) Examples for foreground quality control", "description": "This figure shows examples of foreground image samples that were filtered out during the data quality control process. The top row displays low-quality samples, such as images with incomplete or blurry objects, while the bottom row showcases high-quality foreground samples retained for the SAM-FB dataset after the filtering process.", "section": "3. Dataset"}, {"figure_path": "https://arxiv.org/html/2412.14462/x12.png", "caption": "(b) Word cloud of foreground categories", "description": "This word cloud visualizes the frequency of different foreground object categories present in the SAM-FB dataset.  The size of each word corresponds to its frequency, providing a quick overview of the dataset's object category distribution.  Categories range from common objects like 'person' and 'car' to less frequent ones like 'figurine' and 't-shirt'. This figure highlights the diversity and richness of the SAM-FB dataset in terms of object types included.", "section": "A. More Implementation Details"}, {"figure_path": "https://arxiv.org/html/2412.14462/x13.png", "caption": "Figure 10: 10(a) shows the candidate foreground samples in the pipeline. The upper row shows four low-quality samples. The lower row shows the samples after data quality control. 10(b) shows the word cloud of foreground categories in the SAM-FB dataset.", "description": "Figure 10(a) illustrates the data quality control process for the SAM-FB dataset.  The top row displays examples of low-quality foreground object masks rejected due to issues like incompleteness, background mask contamination, and small object size. The bottom row presents the same images after passing the quality control filters, highlighting the improvement in mask quality. Figure 10(b) provides a visualization of the diverse range of object categories present in the SAM-FB dataset using a word cloud, emphasizing its richness and variety.", "section": "A. More Implementation Details"}, {"figure_path": "https://arxiv.org/html/2412.14462/x14.png", "caption": "Figure 11: Example of objects with details. Our model could keep the appearance better even with some details compared with SD\u00a0[39], GLI-GEN\u00a0[29] and PBE\u00a0[47]. The first row demonstrates the ability to keep some image texture, and the second row illustrates the ability to keep text texture.", "description": "Figure 11 presents a comparison of image editing results, focusing on the preservation of detailed textures. The comparison involves four different methods: the authors' proposed model (MADD), Stable Diffusion [39], GLI-GEN [29], and PBE [47]. Two rows of examples are shown to highlight the models' capabilities in maintaining image texture (first row) and text texture (second row) during image editing.  The results demonstrate that MADD excels at preserving fine details compared to the other methods.", "section": "C.1. Details Maintaining"}]