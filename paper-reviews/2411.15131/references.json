{"references": [{"fullname_first_author": "M. Ahn", "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances", "publication_date": "2022-04-01", "reason": "This paper is foundational for the concept of grounding language in robotic affordances, a key element in WildLMa's language-conditioned imitation learning."}, {"fullname_first_author": "Z. Fu", "paper_title": "Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation", "publication_date": "2024-00-00", "reason": "This paper directly addresses the challenge of mobile manipulation using a similar approach to WildLMa, making it a crucial comparative study."}, {"fullname_first_author": "X. Cheng", "paper_title": "Open-television: Teleoperation with immersive active visual feedback", "publication_date": "2024-00-00", "reason": "WildLMa leverages VR teleoperation for data collection, and this paper provides a relevant and comparable method for improving teleoperation efficiency."}, {"fullname_first_author": "C. Chi", "paper_title": "Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots", "publication_date": "2024-00-00", "reason": "WildLMa uses CLIP for language-conditioned imitation learning, and this paper offers a similar approach with a focus on creating a universal manipulation interface."}, {"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "WildLMa relies on CLIP for visual representation, and this paper introduces CLIP, a crucial foundation for WildLMa's approach."}]}