[{"content": "| Method | Tabletop Grasping |  | Button Pressing |  | Ground Grasping |  | Avg. Succ. |\n|---|---|---|---|---|---|---|---|---|\n|  | I.D. | O.O.D | I.D. | O.O.D | I.D. | O.O.D |  |\n| WildLMa (Ours) | **94.4%** | 75% | **80%** | **57.5%** | **60%** | **60%** | **71.2%** |\n| ACT (Mobile ALOHA) [fu2024mobilealoha] | 77.8% | 19.4% | 55% | 25% | 60% | 30% | 40.8% |\n| OpenTV [cheng2024-opentv] | 88.9% | **77.8%** | 75% | 25% | 50% | 50% | 64.4% |\n| VBC [liu2024-vbc] | 50%* | 50%* | NA\u2020 | NA\u2020 | 43.8%* | 43.8%* | 46.9% |\n| GeFF [qiu2024-geff] | 55.6%* | 55.6%* | NA\u2020 | NA\u2020 | NA\u2020 | NA\u2020 | 55.6% |", "caption": "TABLE I: Success rate of autonomous skill execution. Imitation learning methods outperform RL\u00a0[liu2024-vbc] and zero-shot method\u00a0[qiu2024-geff] on comparable tasks. Both OpenTV and WildLMa achieve noticeably higher success rates in the challenging O.O.D. setting. \u2020: methods involve learned/manual policies that are not trivially applicable to the task settings. \u2217: Method does not differentiate object sets and success rates are averaged on I.D. and O.O.D. object sets.", "description": "Table I presents a comparison of the success rates achieved by different methods in performing three autonomous robotic manipulation skills: tabletop grasping, button pressing, and ground grasping.  The methods are evaluated under both in-distribution (I.D.) and out-of-distribution (O.O.D.) conditions.  In-distribution means the test conditions are similar to the training data, while out-of-distribution means the testing conditions are more challenging and different from the training data.  The table highlights that imitation learning methods generally outperform reinforcement learning (RL) and zero-shot approaches.  WildLMa and OpenTV show significantly better performance in the O.O.D. setting.  Specific notes clarify that some methods use pre-defined policies not readily applicable to all task scenarios, and that some methods do not distinguish between I.D. and O.O.D. data for success rate reporting, therefore providing an average.", "section": "IV. Experiments"}, {"content": "| Pipeline | Collect & Drop Trash | Shelf Rearrangement |\n|---|---|---|\n| WildLMa (Ours) | **7/10** | **3/10** |\n| ACT [fu2024mobilealoha, zhao2023aloha] | 0/10 | 0/10 |", "caption": "TABLE II: Evaluation of long-horizon execution. Given a few training demonstrations (10), WildLMa improves long-horizon task success rate via (1) improved generalizability of single skill and (2) divide-and-conquer.", "description": "Table II presents the results of evaluating WildLMa's performance on long-horizon tasks, which involve a sequence of actions to achieve a complex goal.  The experiment was conducted with only 10 training demonstrations, highlighting WildLMa's efficiency in learning from limited data. The table compares WildLMa's success rate with that of ACT [17, 69] on two tasks: collecting and dropping trash and rearranging items on a shelf.  WildLMa demonstrates a significantly higher success rate than ACT, showcasing its improved generalizability (ability to handle variations in object configurations and environments) in single skills and its divide-and-conquer approach (breaking down complex tasks into smaller, manageable sub-tasks) for improved long-horizon task execution.", "section": "IV. Experiments"}, {"content": "| Backbone | In Dist. | Out of Dist. | Avg. Succ. |\n|---|---|---|---|\n| CLIP [radford2021-CLIP] | 83.3% | 69.4% | 76.4% |\n| ResNet [zhao2023aloha]\u22c6 | 77.8% | 19.4% | 48.6% |\n| DinoV2 [oquab2023-dinov2] | **88.9%** | **77.8%** | **83.3%** |", "caption": "TABLE III: Ablation of different visual encoders pre-trained with different objectives. The evaluation is done on the object-grasping tasks. \u22c6: we followed ACT\u00a0[fu2024mobilealoha, zhao2023aloha] to use ImageNet-pretrained ResNet-18 as the encoder, which has fewer parameters.", "description": "This table presents an ablation study comparing the performance of different visual encoders, pre-trained with varying objectives, on object grasping tasks.  The encoders' impact on task success rate is evaluated across in-distribution (ID) and out-of-distribution (OOD) object sets. One encoder uses a ResNet-18 model pre-trained on ImageNet, chosen for its smaller parameter count following methods from cited works. The results illustrate the effect of different encoder architectures and pre-training strategies on the robustness and generalization capability of the object grasping model.", "section": "IV. Experiments"}, {"content": "| Metric | Whole-body (Ours) Ground Grasping | Whole-body (Ours) Rearrange Shelf | Decoupled Control Ground Grasping | Decoupled Control Rearrange Shelf | W/o Whole-body (Arm Only) Ground Grasping | W/o Whole-body (Arm Only) Rearrange Shelf |\n|---|---|---|---|---|---|---|\n| Average Time | 21.87s | 27.25s | 37.35s | 29.81s | - | 27.88s |\n| Success Rate | 95% | 70% | 80% | 40% | 0% | 70% |", "caption": "TABLE IV: Comparison of success rate and completion time for our whole-body controller, decoupled control with manual base pitching and arm control implemented via Unitree SDK, and arm-only policies. Four teleoperators are tasked to manipulate objects at various heights for three trials in each task.", "description": "Table IV presents a comparison of the success rates and completion times achieved by three different control methods in a mobile manipulation task.  The methods compared are: a whole-body controller (the authors' method), decoupled control (manual base pitching and arm control using the Unitree SDK), and arm-only control.  The evaluation involved four teleoperators performing the same manipulation tasks on objects at various heights, with each teleoperator completing three trials per task. The table highlights the performance differences between these three control methods.", "section": "IV. Experiments"}, {"content": "| Camera | Tabletop Grasping | Button Pressing | Door Opening |\n|---|---|---|---|\n| Head + Wrist | **94.4%** | 80% | **70%** |\n| Head Only | 27.8% | 75% | 30% |\n| Wrist Only | 83.3% | **85%** | 10% |", "caption": "TABLE V: Ablation of input visual modality. Tasks that involve occlusion significantly benefit from multi-view setup.", "description": "This table presents the results of an ablation study on the input visual modality used for WildLMa, focusing on tasks where occlusion is a factor.  It compares the success rates of different tasks (Tabletop Grasping, Button Pressing, Door Opening) using various combinations of camera views: head camera only, wrist camera only, and both head and wrist cameras. The results demonstrate the significant advantage of using both cameras, especially in scenarios with significant occlusions, highlighting the importance of multi-view setup for robust performance.", "section": "IV. Experiments"}, {"content": "| Backbone | In Dist. | Out of Dist. | Avg. Succ. |\n|---|---|---|---| \n| w/ cross-attention (Ours) | **94.4%** | **75%** | **84.7%** |\n| w/o cross-attention | 83.3% | 69.4% | 76.4% |", "caption": "TABLE VI: Ablation of cross-attention on the object-grasping tasks. Cross-attention improves both I.D. and O.O.D. setting by using additional task-specific information.", "description": "Table VI presents an ablation study evaluating the impact of cross-attention on the performance of object-grasping tasks.  The study compares the success rates of the model with and without cross-attention, under both in-distribution (I.D.) and out-of-distribution (O.O.D.) settings. The results demonstrate that incorporating cross-attention, which leverages additional task-specific textual information, significantly enhances the model's ability to perform object-grasping tasks, particularly in challenging, out-of-distribution scenarios.", "section": "IV. Experiments"}]