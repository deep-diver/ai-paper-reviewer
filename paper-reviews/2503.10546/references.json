{"references": [{"fullname_first_author": "J. Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-01-01", "reason": "This paper introduces BERT, a foundational transformer model crucial for advancing natural language understanding, which is essential for open-vocabulary robotic systems' instruction interpretation."}, {"fullname_first_author": "A. Radford", "paper_title": "Improving language understanding by generative pre-training", "publication_date": "2018-01-01", "reason": "This paper describes Generative Pre-Training (GPT), a significant approach for enhancing language comprehension which plays a vital role in processing language instructions for robot tasks."}, {"fullname_first_author": "T. B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This paper focuses on few-shot learning capabilities in language models, a key aspect that allows robotic systems to adapt quickly to new manipulation tasks from limited examples."}, {"fullname_first_author": "J. Achiam", "paper_title": "Gpt-4 technical report", "publication_date": "2023-01-01", "reason": "This paper details the technical specifications for GPT-4 which is used for code generation to specify keypoint-based target specification."}, {"fullname_first_author": "A. Kirillov", "paper_title": "Segment anything", "publication_date": "2023-01-01", "reason": "This paper introduces Segment Anything, which facilitates the segmentation of semantic masks from RGB observations, enabling keypoint sampling in the KUDA framework."}]}