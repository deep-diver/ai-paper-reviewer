[{"figure_path": "https://arxiv.org/html/2503.08102/extracted/6269827/figures/hybrid.png", "caption": "Figure 1: Hybrid Architecture of Second Me", "description": "This figure illustrates the hybrid architecture of Second Me, a system designed to function as an AI-native persistent memory offload.  It shows how Second Me acts as an intermediary between the user and external systems (such as devices, applications, and the internet). The user interacts with Second Me through a personalized interface, posing queries or requests.  Second Me then processes these requests using a combination of internal memory layers (L0, L1, and L2), an agent model for reasoning, and various knowledge bases and tools.  This architecture enables Second Me to autonomously retrieve, organize, and apply user-specific knowledge to generate context-aware responses and facilitate seamless interactions.", "section": "2 An Overview of SECOND ME"}, {"figure_path": "https://arxiv.org/html/2503.08102/extracted/6269827/figures/train_structure.png", "caption": "Figure 2: Automated Personal Model pipeline with LLM as a Judge and LLM as data synthesizer", "description": "This figure illustrates the automated pipeline used to train the SECOND ME model.  It begins with raw user data (like documents, audio, or website interactions) that undergoes data cleaning and mining to extract relevant entities and topics. This data is then filtered, synthesized using techniques like self-location reinforcement and memory cognition enhancement, and used for supervised fine-tuning (SFT) and direct preference optimization (DPO) of the LLM. An LLM acts as a 'judge' to evaluate the model's performance, while another LLM serves as a 'data synthesizer' to generate synthetic training data for further refinement.  The entire process is iterative, with the model's performance evaluated continuously to ensure optimal results.", "section": "3 SECOND ME: Practice and Result"}, {"figure_path": "https://arxiv.org/html/2503.08102/extracted/6269827/figures/multi-agent.png", "caption": "Figure 3: Given same query, here are three synthetic responses using different COT strategies.", "description": "This figure displays three different responses generated by a large language model (LLM) to the same query, each using a different Chain-of-Thought (COT) strategy.  The COT strategies vary in the level of detail and structure in the reasoning process before providing a final answer. This illustrates how different COT strategies can influence the LLM's response style and the quality of the generated text, in this case showing the progression from less structured (Weak COT) to more structured (Strong COT) reasoning.", "section": "3 SECOND ME: Practice and Result"}]