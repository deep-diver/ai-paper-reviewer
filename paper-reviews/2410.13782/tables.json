[{"figure_path": "2410.13782/tables/table_8_0.html", "caption": "Table 2: Benchmarking comparison of unconditional protein generation, in terms of structure-sequence co-generation, backbone-only generation, and sequence-only generation. For each method, we generate 100 samples for lengths in [100, 200, 300, 400, 500]. * denotes Multiflow variants retrained by us using different dataset \u2013 native PDB data without ProteinMPNN distillation and the same training data as DPLM-2 (i.e., PDB+SwissProt), respectively.", "description": "Table 2 compares the performance of DPLM-2 with other state-of-the-art methods for unconditional protein generation across various metrics, including structure-sequence compatibility, diversity, and novelty.", "section": "4.1 UNCONDITIONAL PROTEIN GENERATION"}, {"figure_path": "2410.13782/tables/table_9_0.html", "caption": "Table 8: Ablation study on the self-mixup training strategy.", "description": "Table 8 shows the ablation study results on the self-mixup training strategy, demonstrating its effectiveness in improving the diversity of generated protein samples.", "section": "A DPLM-2 TRAINING"}, {"figure_path": "2410.13782/tables/table_9_1.html", "caption": "Table 4: Structure prediction performance comparison between DPLM-2 and different baseline approaches on CAMEO 2022 datasets. \u2020: PVQD results are quoted from Liu et al. (2023).", "description": "Table 4 presents a comparison of the structure prediction performance of DPLM-2 against several other models on the CAMEO 2022 and PDB datasets, using RMSD and TMscore metrics.", "section": "4.2 FORWARD FOLDING (SEQUENCE-CONDITIONED STRUCTURE PREDICTION)"}, {"figure_path": "2410.13782/tables/table_10_0.html", "caption": "Table 5: Comparison on inverse folding task.", "description": "Table 5 presents the results of inverse folding task using different models, showing the amino acid recovery (AAR) and structure consistency (scTM) for CAMEO 2022 and PDB datasets.", "section": "4.3 INVERSE FOLDING (STRUCTURE-CONDITIONED SEQUENCE GENERATION)"}, {"figure_path": "2410.13782/tables/table_10_1.html", "caption": "Table 6: Performance on various protein predictive downstream tasks. \u2020: benchmarked results are quoted from Su et al. (2023).", "description": "Table 6 presents the performance comparison of different models on various protein predictive downstream tasks, including thermostability, HumanPPI, metal ion binding, EC, GO (MF, BP, CC), DeepLoc (subcellular and binary).", "section": "4.5 EVALUATION OF PROTEIN REPRESENTATION LEARNING"}, {"figure_path": "2410.13782/tables/table_16_0.html", "caption": "Table 8: Ablation study on the self-mixup training strategy.", "description": "Table 8 shows the ablation study results on the self-mixup training strategy, demonstrating that the self-mixup training strategy effectively enhances the diversity of samples.", "section": "A DPLM-2 TRAINING"}, {"figure_path": "2410.13782/tables/table_17_0.html", "caption": "Table 2: Benchmarking comparison of unconditional protein generation, in terms of structure-sequence co-generation, backbone-only generation, and sequence-only generation. For each method, we generate 100 samples for lengths in [100, 200, 300, 400, 500]. * denotes Multiflow variants retrained by us using different dataset \u2013 native PDB data without ProteinMPNN distillation and the same training data as DPLM-2 (i.e., PDB+SwissProt), respectively.", "description": "Table 2 provides a quantitative comparison of DPLM-2's unconditional protein generation performance against various baselines across different metrics, including quality, novelty, and diversity, for different protein lengths and generation methods.", "section": "4.1 UNCONDITIONAL PROTEIN GENERATION"}, {"figure_path": "2410.13782/tables/table_18_0.html", "caption": "Table 2: Benchmarking comparison of unconditional protein generation, in terms of structure-sequence co-generation, backbone-only generation, and sequence-only generation. For each method, we generate 100 samples for lengths in [100, 200, 300, 400, 500]. * denotes Multiflow variants retrained by us using different dataset \u2013 native PDB data without ProteinMPNN distillation and the same training data as DPLM-2 (i.e., PDB+SwissProt), respectively.", "description": "Table 2 presents a quantitative comparison of unconditional protein generation performance metrics (quality, novelty, and diversity) across different models, including variations of Multiflow and DPLM-2, for various protein lengths.", "section": "4.1 UNCONDITIONAL PROTEIN GENERATION"}]