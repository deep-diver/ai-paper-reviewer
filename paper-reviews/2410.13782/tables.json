[{"figure_path": "2410.13782/tables/table_8_0.html", "caption": "Table 2: Benchmarking comparison of unconditional protein generation, in terms of structure-sequence co-generation, backbone-only generation, and sequence-only generation. For each method, we generate 100 samples for lengths in [100, 200, 300, 400, 500]. * denotes Multiflow variants retrained by us using different dataset \u2013 native PDB data without ProteinMPNN distillation and the same training data as DPLM-2 (i.e., PDB+SwissProt), respectively.", "description": "Table 2 compares the performance of DPLM-2 against other unconditional protein generation methods across various metrics, including quality, novelty, and diversity of generated sequences and structures.", "section": "4.1.1 DPLM-2 ENABLES HIGH-QUALITY, DIVERSE AND NOVEL PROTEIN SEQUENCE AND STRUCTURE GENERATION"}, {"figure_path": "2410.13782/tables/table_9_0.html", "caption": "Table 3: Ablation study on the sequence pre-training and training data augmentation.", "description": "Table 3 shows the ablation study results on the effects of sequence pre-training and data augmentation on the unconditional protein generation performance of DPLM-2, demonstrating that both strategies significantly improve the model's performance, especially in generating long proteins.", "section": "4.1.3 ABLATION STUDY"}, {"figure_path": "2410.13782/tables/table_9_1.html", "caption": "Table 4: Structure prediction performance comparison between DPLM-2 and different baseline approaches on CAMEO 2022 datasets. \u2020: PVQD results are quoted from Liu et al. (2023).", "description": "Table 4 compares the structure prediction performance of DPLM-2 against several other state-of-the-art methods on the CAMEO 2022 dataset, using RMSD and TMscore as evaluation metrics.", "section": "4.2 FORWARD FOLDING (SEQUENCE-CONDITIONED STRUCTURE PREDICTION)"}, {"figure_path": "2410.13782/tables/table_10_0.html", "caption": "Table 5: Comparison on inverse folding task.", "description": "Table 5 presents the performance comparison of different models on the inverse folding task, evaluating amino acid recovery (AAR) and structure consistency (scTM).", "section": "4.3 INVERSE FOLDING (STRUCTURE-CONDITIONED SEQUENCE GENERATION)"}, {"figure_path": "2410.13782/tables/table_10_1.html", "caption": "Table 6: Performance on various protein predictive downstream tasks. \u2020: benchmarked results are quoted from Su et al. (2023).", "description": "Table 6 presents the performance comparison of different protein language models on various protein predictive downstream tasks, including thermostability, HumanPPI, metal ion binding, EC, GO (MF, BP, CC), DeepLoc (subcellular and binary).", "section": "4.5 EVALUATION OF PROTEIN REPRESENTATION LEARNING"}, {"figure_path": "2410.13782/tables/table_16_0.html", "caption": "Table 8: Ablation study on the self-mixup training strategy.", "description": "Table 8 shows the ablation study results on the self-mixup training strategy, demonstrating its effect on the diversity of generated protein samples at different lengths.", "section": "A DPLM-2 TRAINING"}, {"figure_path": "2410.13782/tables/table_17_0.html", "caption": "Table 2: Benchmarking comparison of unconditional protein generation, in terms of structure-sequence co-generation, backbone-only generation, and sequence-only generation. For each method, we generate 100 samples for lengths in [100, 200, 300, 400, 500]. * denotes Multiflow variants retrained by us using different dataset \u2013 native PDB data without ProteinMPNN distillation and the same training data as DPLM-2 (i.e., PDB+SwissProt), respectively.", "description": "Table 2 compares the performance of DPLM-2 with other unconditional protein generation methods in terms of quality, novelty, and diversity of generated protein sequences and structures.", "section": "4.1 UNCONDITIONAL PROTEIN GENERATION"}, {"figure_path": "2410.13782/tables/table_18_0.html", "caption": "Table 2: Benchmarking comparison of unconditional protein generation, in terms of structure-sequence co-generation, backbone-only generation, and sequence-only generation. For each method, we generate 100 samples for lengths in [100, 200, 300, 400, 500]. * denotes Multiflow variants retrained by us using different dataset \u2013 native PDB data without ProteinMPNN distillation and the same training data as DPLM-2 (i.e., PDB+SwissProt), respectively.", "description": "Table 2 benchmarks the performance of DPLM-2 against other methods on unconditional protein generation tasks, evaluating various metrics across different protein lengths and generation approaches.", "section": "4.1 UNCONDITIONAL PROTEIN GENERATION"}]