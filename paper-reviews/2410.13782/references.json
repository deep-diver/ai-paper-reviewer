{"references": [{" publication_date": "2021", "fullname_first_author": "John Jumper", "paper_title": "Highly accurate protein structure prediction with Alphafold", "reason": "This paper is highly influential because it introduces AlphaFold, a groundbreaking deep learning model that revolutionized protein structure prediction, achieving unprecedented accuracy.  Its impact extends beyond the field of protein structure prediction, inspiring further developments in protein language models and generative protein modeling.  The accuracy achieved by AlphaFold serves as a benchmark for assessing other protein structure prediction methods.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This paper is foundational for diffusion models, introducing a novel approach to generative modeling based on denoising diffusion processes.  Diffusion models have gained significant popularity and achieved great success in various generative tasks, including protein structure modeling, because of this paper's contribution. The methodology proposed in this paper has had a direct and substantial impact on the development of protein language models, particularly diffusion-based models.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Alexander Rives", "paper_title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences", "reason": "This paper introduced ESM, a series of protein language models that achieved significant advancements in protein sequence representation and understanding, using large-scale evolutionary data.  ESM's impact is profound; it paved the way for many subsequent protein language models and advanced our understanding of protein sequence-structure relationships, forming a basis for many subsequent improvements in the field.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Jacob Austin", "paper_title": "Structured denoising diffusion models in discrete state-spaces", "reason": "This work significantly advanced the field of discrete diffusion models, a crucial framework for many generative models, including DPLM.  Its contributions lay in proposing a method to perform structured denoising in discrete state spaces, providing an improved framework for learning complex relationships between variables, and forming the basis for the discrete diffusion approach in DPLM and consequently DPLM-2.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Aaron Van Den Oord", "paper_title": "Neural discrete representation learning", "reason": "This paper is highly influential as it introduced Vector Quantized-Variational Autoencoders (VQ-VAEs), a powerful technique for tokenizing continuous data into discrete representations. This work significantly influenced how DPLM-2 handles continuous structural data, converting them into a discrete token sequence that can be processed by a language model. The use of VQ-VAE or its variants is common practice in many multimodal generative modeling tasks.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Lijun Yu", "paper_title": "Language model beats diffusion-tokenizer is key to visual generation", "reason": "This paper introduces Lookup-Free Quantizer (LFQ), a highly efficient and effective quantization method that significantly improves the quality and speed of visual tokenization.  DPLM-2 adopts LFQ as a key component for its structure tokenizer, demonstrating its suitability for converting 3D protein coordinates into discrete tokens, overcoming the challenges associated with traditional VQ-VAEs.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "reason": "This paper introduced LoRA, a highly efficient and effective technique for adapting large language models.  LoRA significantly reduces the computational cost and memory requirements of fine-tuning large language models, which is particularly important when handling multimodal data. DPLM-2 leverages LoRA for warm-up from pre-trained sequence-based DPLM, efficiently transferring knowledge to the multimodal model.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Xinyou Wang", "paper_title": "Diffusion language models are versatile protein learners", "reason": "This paper introduces DPLM, a discrete diffusion-based protein language model, which is the foundation upon which DPLM-2 is built.  DPLM's strengths in handling protein sequences and its performance in generation and representation learning are crucial for DPLM-2's efficient warm-up strategy.  The architecture and methodology of DPLM directly informs the development and design of DPLM-2.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Justas Dauparas", "paper_title": "Robust deep learning-based protein sequence design using proteinmpnn", "reason": "This paper introduces ProteinMPNN, a powerful model for protein design that is widely used as a baseline in many protein generative modeling tasks.  ProteinMPNN's strength in generating high-quality protein structures serves as a key benchmark for evaluating DPLM-2's performance in various tasks, including inverse folding and structure generation.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Zeming Lin", "paper_title": "Language models of protein sequences at the scale of evolution enable accurate structure prediction", "reason": "This paper demonstrates that large-scale protein language models, trained on evolutionary data, implicitly capture structural information that facilitates accurate protein structure prediction. DPLM-2 leverages this insight by incorporating a warm-up strategy from pre-trained sequence-based DPLM, effectively transferring this evolutionary knowledge into the multimodal model, which improves performance on structure-related tasks.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "John Jumper", "paper_title": "Highly accurate protein structure prediction with alphafold", "reason": "AlphaFold's remarkable accuracy in protein structure prediction makes it a crucial tool for evaluating generative protein models. DPLM-2 uses AlphaFold's predicted structures for comparison and validation in several experiments, particularly for assessing the quality and compatibility of generated sequences and structures.  The structural accuracy provided by AlphaFold serves as a benchmark for evaluating the generation capability of DPLM-2.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Tomas Hayes", "paper_title": "Simulating 500 million years of evolution with a language model", "reason": "This paper introduces ESM3, another highly influential multimodal protein generative language model.  A comparative analysis is included in the paper, highlighting the different approaches and capabilities of DPLM-2 and ESM3 in protein generation, enabling a deeper understanding of the respective model strengths and limitations.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Andrew Campbell", "paper_title": "Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design", "reason": "This paper introduces Multiflow, a multimodal generative model for protein co-generation.  Multiflow serves as a key comparative model for DPLM-2 in unconditional and conditional protein generation experiments. Comparing DPLM-2 and Multiflow helps to highlight the advantages and novelties in DPLM-2's approach, especially in terms of its capability to co-generate protein sequence and structure.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Joseph L Watson", "paper_title": "De novo design of protein structure and function with rfdiffusion", "reason": "This paper introduces RFDiffusion, a state-of-the-art generative model focusing on inverse folding and protein design tasks. RFDiffusion is compared with DPLM-2 on several downstream tasks, including inverse folding and motif-scaffolding, providing valuable insights into DPLM-2's performance relative to a strong baseline.  This comparative study demonstrates the strengths and limitations of each model, providing crucial context for interpreting DPLM-2's results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jason Yim", "paper_title": "Improved motif-scaffolding with se (3) flow matching", "reason": "This work significantly advances the field of motif-scaffolding, an important protein engineering application.  The experimental setup and evaluation metrics established in this paper are directly adopted in evaluating DPLM-2 on the motif scaffolding task, facilitating fair comparison and enabling robust assessment of DPLM-2's capability in this area. The experimental results directly contrast the performance of DPLM-2 against state-of-the-art methods.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Jin Su", "paper_title": "Saprot: Protein language modeling with structure-aware vocabulary", "reason": "This paper introduces SaProt, a strong baseline in protein predictive tasks, which is used for evaluating DPLM-2's capability in this area.  Comparing DPLM-2 to SaProt provides insights into the effectiveness of DPLM-2's structure-aware representations and reveals areas where further improvements might be necessary.", "section_number": 4}, {" publication_date": "2016", "fullname_first_author": "Marc'Aurelio Ranzato", "paper_title": "Sequence level training with recurrent neural networks", "reason": "This paper highlights the exposure bias problem in sequence modeling, a challenge that DPLM-2 addresses.  Understanding the exposure bias is crucial for developing effective training strategies for sequence models, and the insights from this paper inform DPLM-2's self-mixup training strategy, which mitigates the exposure bias problem.", "section_number": 5}, {" publication_date": "2015", "fullname_first_author": "Samy Bengio", "paper_title": "Scheduled sampling for sequence prediction with recurrent neural networks", "reason": "This paper also discusses the exposure bias problem in sequence models, a challenge that DPLM-2 addresses.  Scheduled sampling, a technique introduced in this paper to mitigate the exposure bias problem, is closely related to DPLM-2's self-mixup training strategy, helping to maintain consistency between the training and inference process.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Alexander E Chu", "paper_title": "An all-atom protein generative model", "reason": "This paper provides an alternative approach to protein generative modeling using all-atom representation.  It is relevant to the discussion because the model architecture and generation approach are different from DPLM-2's approach. This paper helps to highlight the strengths and weaknesses of different approaches to protein generative modeling and provides context for evaluating DPLM-2's approach.", "section_number": 5}]}