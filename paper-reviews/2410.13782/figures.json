[{"figure_path": "2410.13782/figures/figures_2_0.png", "caption": "Figure 1: Overall illustration of DPLM-2. (A) Structure tokenization consists of a GVP-based encoder to yield invariant backbone geometric features, a lookup-free quantizer (LFQ) to discretize encoded structural features into structure tokens within a codebook, and an IPA-based decoder as de-tokenizer to convert structure tokens back to backbone atomic coordinates. (B) Multimodal learning and generation of protein structure and sequence with DPLM-2. (C) Various applications of DPLM-2 as a protein foundation model: (1) unconditional protein sequence-structure mixed-modal co-generation; (2) protein sequence-structure joint representation for predictive tasks; (3) structure prediction; (4) fixed-backbone sequence generation; (5) conditional protein generation with structure-sequence mixed-modal input and output.", "description": "Figure 1 illustrates the overall architecture of DPLM-2, including structure tokenization, multimodal training and sampling, and various applications of DPLM-2 as a protein foundation model.", "section": "3 DPLM-2: A MULTIMODAL DIFFUSION PROTEIN LANGUAGE MODEL"}, {"figure_path": "2410.13782/figures/figures_4_0.png", "caption": "Figure 1: Overall illustration of DPLM-2. (A) Structure tokenization consists of a GVP-based encoder to yield invariant backbone geometric features, a lookup-free quantizer (LFQ) to discretize encoded structural features into structure tokens within a codebook, and an IPA-based decoder as de-tokenizer to convert structure tokens back to backbone atomic coordinates. (B) Multimodal learning and generation of protein structure and sequence with DPLM-2. (C) Various applications of DPLM-2 as a protein foundation model: (1) unconditional protein sequence-structure mixed-modal co-generation; (2) protein sequence-structure joint representation for predictive tasks; (3) structure prediction; (4) fixed-backbone sequence generation; (5) conditional protein generation with structure-sequence mixed-modal input and output.", "description": "Figure 1 illustrates the overall architecture of DPLM-2, including structure tokenization, multimodal training and sampling, and various applications of DPLM-2 as a protein foundation model.", "section": "3 DPLM-2: A MULTIMODAL DIFFUSION PROTEIN LANGUAGE MODEL"}, {"figure_path": "2410.13782/figures/figures_7_0.png", "caption": "Figure 1: Overall illustration of DPLM-2. (A) Structure tokenization consists of a GVP-based encoder to yield invariant backbone geometric features, a lookup-free quantizer (LFQ) to discretize encoded structural features into structure tokens within a codebook, and an IPA-based decoder as de-tokenizer to convert structure tokens back to backbone atomic coordinates. (B) Multimodal learning and generation of protein structure and sequence with DPLM-2. (C) Various applications of DPLM-2 as a protein foundation model: (1) unconditional protein sequence-structure mixed-modal co-generation; (2) protein sequence-structure joint representation for predictive tasks; (3) structure prediction; (4) fixed-backbone sequence generation; (5) conditional protein generation with structure-sequence mixed-modal input and output.", "description": "Figure 1 illustrates the overall architecture of DPLM-2, showing its structure tokenization, multimodal training and sampling process, and various applications in protein generation and prediction tasks.", "section": "3 DPLM-2: A MULTIMODAL DIFFUSION PROTEIN LANGUAGE MODEL"}, {"figure_path": "2410.13782/figures/figures_7_1.png", "caption": "Figure 1: Overall illustration of DPLM-2. (A) Structure tokenization consists of a GVP-based encoder to yield invariant backbone geometric features, a lookup-free quantizer (LFQ) to discretize encoded structural features into structure tokens within a codebook, and an IPA-based decoder as de-tokenizer to convert structure tokens back to backbone atomic coordinates. (B) Multimodal learning and generation of protein structure and sequence with DPLM-2. (C) Various applications of DPLM-2 as a protein foundation model: (1) unconditional protein sequence-structure mixed-modal co-generation; (2) protein sequence-structure joint representation for predictive tasks; (3) structure prediction; (4) fixed-backbone sequence generation; (5) conditional protein generation with structure-sequence mixed-modal input and output.", "description": "Figure 1 illustrates the overall architecture of DPLM-2, including structure tokenization, multimodal training and sampling, and various applications in protein generation and prediction tasks.", "section": "3 DPLM-2: A MULTIMODAL DIFFUSION PROTEIN LANGUAGE MODEL"}, {"figure_path": "2410.13782/figures/figures_7_2.png", "caption": "Figure 3: Evaluation of DPLM-2 on unconditional structure-sequence co-generation. Here for designability of co-generated proteins, we use ESMFold to obtain refolded structure of DPLM-2-generated sequence and measure the structural similarity between DPLM-2-generated structure and the refolded structure, which aims to measure the compatibility of the co-generated structure and sequence pairs.", "description": "Figure 3 shows the evaluation results of DPLM-2 on unconditional protein generation, illustrating its capability to generate high-quality, diverse, and novel protein sequences and structures simultaneously.", "section": "4.1 UNCONDITIONAL PROTEIN GENERATION"}]