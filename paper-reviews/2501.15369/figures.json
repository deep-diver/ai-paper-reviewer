[{"figure_path": "https://arxiv.org/html/2501.15369/x1.png", "caption": "Figure 1: Comparison of latency and accuracy between our iFormer and other existing methods on ImageNet-1k. The latency is measured on an iPhone 13. Our iFormer is Pareto-optimal.", "description": "This figure compares the latency and accuracy of various lightweight neural networks, including the novel iFormer model, on the ImageNet-1k image classification benchmark.  The latency is measured using an iPhone 13, providing a real-world performance metric relevant to mobile applications.  The results show iFormer achieving a superior balance between low latency and high accuracy compared to other state-of-the-art models, demonstrating its Pareto optimality (meaning improvements in latency cannot be made without sacrificing accuracy, and vice versa).", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.15369/x2.png", "caption": "Figure 2: Illustration of the evolution from the ConvNeXt baseline towards the lightweight iFormer. The orange bars are model accuracies and the light blue bars are model latencies. We also include a red latency outline for better visualization.", "description": "This figure illustrates the iterative improvements made to the ConvNeXt model to create the lightweight iFormer architecture.  Each bar represents a stage in the development process.  The orange bars show the Top-1 accuracy achieved at each stage, while the light blue bars represent the model latency (inference time) on an iPhone 13. A red line is overlaid to better visualize the latency improvements.  The figure visually demonstrates the trade-off between improving model accuracy and maintaining low latency, a key design goal of iFormer.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2501.15369/x4.png", "caption": "Figure 4: Overview of iFormer architecture, detailed convolutional stem, block design, and SHMA. The hatched area in SHMA indicates extra memory-intensive reshaping operations that are eliminated by SHMA. S\u2062(\u22c5)\ud835\udc46\u22c5S(\\cdot)italic_S ( \u22c5 ) denotes the softmax function. R\ud835\udc45Ritalic_R is the ratio for reducing channels of query and key. It is set to 2 in iFormer. We omit BN following project or convolution for simplicity.", "description": "Figure 4 illustrates the architecture of the iFormer model, a hybrid convolutional and transformer network designed for mobile applications.  It shows a hierarchical structure with four stages.  Early stages use efficient convolutional blocks (derived from ConvNeXt) to extract local features quickly.  Later stages incorporate a novel single-head modulation attention (SHMA) mechanism, depicted in detail.  SHMA is designed to replace the computationally expensive multi-head self-attention, as shown by the hatched area representing operations removed in SHMA.  The figure highlights the key components: a convolutional stem, convolutional blocks, attention blocks (using SHMA), and a final classification head. The use of a channel reduction ratio (R) in SHMA is also explained, set to 2 in this instance to improve efficiency.  Batch Normalization (BN) is omitted from the diagram for simplicity.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2501.15369/x5.png", "caption": "Figure 5: Comparison of SHMA and SHA in SHViT. In SHViT, r\u2062C\ud835\udc5f\ud835\udc36rCitalic_r italic_C channels are utilized for spatial attention, where r\ud835\udc5fritalic_r is set to 14.6714.67\\frac{1}{4.67}divide start_ARG 1 end_ARG start_ARG 4.67 end_ARG. SHMA projects the input into a higher dimension of 1212\\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARGC (i.e., R=2) and avoids split and concatenation operations.", "description": "Figure 5 compares the single-head modulation attention (SHMA) proposed in the iFormer model with the single-head attention (SHA) used in the SHViT model.  Both methods aim for efficient attention mechanisms in vision transformers, but they differ in their approach. SHViT uses only a fraction (1/4.67) of the input channels for spatial attention, requiring channel splitting and concatenation operations, which can be computationally expensive. In contrast, SHMA projects the input to a higher dimension (1.5 times the original) before the attention operation, avoiding these extra steps. This design choice enhances efficiency and reduces memory consumption.", "section": "3.3 SINGLE HEAD MODULATION ATTENTION"}]