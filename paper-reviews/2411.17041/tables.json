[{"content": "| Baseline | Free<sup>2</sup>Guide | Baseline | Free<sup>2</sup>Guide |\n|---|---|---|---|", "caption": "Table 1: Qualitative comparison between ensemble methods.", "description": "This table presents a qualitative comparison of different ensemble methods used to combine multiple reward models for enhancing text-to-video alignment in a diffusion model.  It compares the performance of three ensemble methods: Weighted Sum, Normalized Sum, and Consensus.  Each method combines the scores from two reward models: one using Large Vision-Language Models (LVLMs) for temporal awareness, and one using existing large-scale image models. The table shows the average scores for each ensemble method, allowing a comparison of their relative effectiveness in improving video generation.", "section": "4.2. Ensembling Reward Functions"}, {"content": "|---|---|---|---|\n| https://arxiv.org/html/2411.17041/Video/1_1/0015.png | https://arxiv.org/html/2411.17041/Video/1_2/0015.png | https://arxiv.org/html/2411.17041/Video/2_1/0015.png | https://arxiv.org/html/2411.17041/Video/2_2/0015.png |", "caption": "Table 2: \nQuantitative evaluation on text alignment. Higher numbers indicate better alignment with the text prompt. The numbers in parentheses denote the performance difference from the baseline.", "description": "This table presents a quantitative analysis of text-to-video alignment performance for different models.  It uses the VBench benchmark to evaluate alignment across six key dimensions: Appearance Style, Temporal Style, Human Action, Multiple Objects, Spatial Relationship, and Overall Consistency. Higher scores indicate stronger alignment between the generated video and the text prompt.  The numbers in parentheses show the improvement or decrease in performance compared to the baseline model for each dimension.", "section": "5. Experiments"}, {"content": "| Description | Description |\n|---|---| \n| \"A person is strumming guitar\" | \"A dog and a horse\" |", "caption": "Table 3: \nComparison of the general quality of the generated video independent of the text prompt. Higher numbers indicate better video quality. The numbers in parentheses denote the performance difference from the baselines.", "description": "This table presents a quantitative comparison of generated video quality across different methods, irrespective of text prompt alignment.  It assesses various aspects of video quality, including the consistency of subjects and backgrounds, smoothness of motion, the dynamic range of the video, the aesthetic appeal, and overall imaging quality.  Higher scores indicate better overall video quality. The values in parentheses show the improvement or decline compared to the baseline model for each metric.", "section": "5. Experiments"}, {"content": "| Baseline | Free<sup>2</sup>Guide | Baseline | Free<sup>2</sup>Guide |\n|---|---|---|---|", "caption": "Table 4: Average results by assessment policy using LVLM.", "description": "This table presents a comparison of the average text alignment and general video quality scores obtained using different assessment methods with the Large Vision-Language Model (LVLM).  Specifically, it contrasts the results of using a binary ('yes'/'no') versus a 1-9 scale rating system for evaluating the alignment of generated videos with text prompts. The results highlight the impact of the assessment method on the overall performance and help to illustrate how nuanced scoring schemes (1-9 scale) can improve the evaluation accuracy for text-video alignment.", "section": "5.1. Results"}, {"content": "|---|---|---|---|\n|https://arxiv.org/html/2411.17041/Video/1_3/0015.png|https://arxiv.org/html/2411.17041/Video/1_4/0015.png|https://arxiv.org/html/2411.17041/Video/2_3/0015.png|https://arxiv.org/html/2411.17041/Video/2_4/0015.png|", "caption": "Table 7: Comparison on reward function of generated video. Bold: best, underline: second best.", "description": "This table presents a comparison of the performance of different reward functions used in the Free\u00b2Guide framework for text-to-video generation.  It shows the average scores achieved across different metrics (CLIP, ImageReward, and GPT-40) when using various combinations of these reward functions with the LaVie and VideoCrafter2 models.  The bold and underlined values highlight the best and second-best performing reward function combinations, respectively, for each model and metric. This helps to illustrate the relative effectiveness of different reward models and strategies in guiding the generation of high-quality videos that are well-aligned with the corresponding text prompts.", "section": "C. Additional Analysis"}, {"content": "| Text | Text |\n|---|---| \n| \"A happy fuzzy panda playing guitar nearby a campfire, snow mountain in the background\" | \"The bund Shanghai, vibrant color\" |", "caption": "Table 8: \nComparison with video-based reward model. Higher numbers indicate better video quality. The numbers in parentheses denote the performance difference from the baselines.", "description": "This table presents a comparison of video quality metrics between using image-based reward models (CLIP and ImageReward) and a video-based reward model (ViCLIP) for guiding video generation.  It shows scores for various aspects of video quality, such as subject consistency, background consistency, motion smoothness, dynamic degree, aesthetic quality, and imaging quality. Higher scores signify better video quality.  The numbers in parentheses indicate the improvement or degradation in each metric relative to the baseline (LaVie) model.", "section": "5.1. Results"}]