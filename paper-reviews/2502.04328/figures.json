[{"figure_path": "https://arxiv.org/html/2502.04328/x2.png", "caption": "Figure 1: Ola pushes the frontiers of the omni-modal language model across image, video and audio understanding benchmarks.  We compare Ola with existing state-of-the-art open-sourced multimodal models and GPT-4o on their abilities in mainstream image, video, and audio benchmarks. For fair comparisons, we select around 7B versions of existing MLLMs. Ola can achieve outperforming performance against omni-modal and specialized MLLMs in all modalities thanks to our progressive alignment strategy. \u201c\u00d7\\times\u00d7\u201d indicates that the model is not capable of the task and \u201c\u2212--\u201d indicates the result is lacking. The score for LibriSpeech is inverted as lower is better for the WER metric.", "description": "Figure 1 compares the performance of the Ola omni-modal language model against other state-of-the-art open-source multimodal models and GPT-4 on various image, video, and audio benchmarks.  To ensure a fair comparison, models with approximately 7 billion parameters were selected. The results demonstrate that Ola significantly outperforms other models across all modalities, attributed to its progressive modality alignment training strategy.  The '\u00d7' symbol indicates a model's inability to perform a specific task, while '-' signifies missing results.  Note that the LibriSpeech benchmark uses a reversed scoring system where lower values represent better performance (lower Word Error Rate).", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.04328/x3.png", "caption": "Figure 2: Ola Architecture. Ola supports omni-modal inputs including text, image, video, and audio, capable of processing the inputs simultaneously with competitive performance on understanding tasks for all these modalities. Meanwhile, Ola supports user-friendly real-time streaming decoding for texts and speeches thanks to the text detokenizer and the speech decoder.", "description": "The Ola architecture diagram shows its ability to process text, image, video, and audio inputs simultaneously.  Each modality's input is fed through its respective encoder (visual encoder, audio encoder, etc.).  These encodings are then combined and processed by the Ola Omni-Modal Language Model, which generates tokens. These tokens are then decoded into text output via a text detokenizer and converted to speech output using a speech decoder.  This enables real-time streaming decoding for text and speech, providing a user-friendly experience.", "section": "3.1 Ola Architecture"}, {"figure_path": "https://arxiv.org/html/2502.04328/x4.png", "caption": "Figure 3: Progressive modality alignment helps to learn better omni-modal models.  We compare our progressive alignment strategy with two baseline training pipelines on Image QA(MMBench\u00a0[40]), Video QA(VideoMME\u00a0[21]), and ASR(LibriSpeech\u00a0[54]): 1) direct mixing where all instruction tuning data is merged and trained in a single stage, and 2) balanced sampling where we upsample certain sources to make the training data more balanced among modalities. The experiment is conducted on a subsampled training set for efficiency and we train models for the same number of steps for fair comparisons. The score is normalized based on the score of progressive alignment to calculate the relative score and the ASR score is inverted as lower is better for the WER metric.", "description": "This figure compares three different training strategies for an omni-modal language model: progressive modality alignment (the proposed method), direct mixing of data, and balanced sampling of data.  The results are shown on three tasks: Image QA (using the MMBench benchmark), Video QA (using the VideoMME benchmark), and Automatic Speech Recognition (ASR) (using the LibriSpeech benchmark). The progressive alignment strategy shows consistent improvements across all three tasks compared to the baselines. The direct mixing approach merges all data types and trains at once, while the balanced sampling approach oversamples smaller datasets to balance the dataset size across all modalities.  The figure highlights the benefits of the progressive approach, which incrementally adds modalities to improve performance by maintaining relatively small cross-modal alignment data.", "section": "3. Ola: Omni-Modal Understanding"}, {"figure_path": "https://arxiv.org/html/2502.04328/x5.png", "caption": "Figure 4: Illustrations of the Ola Progressive Modality Alignment. We visualize the relationships among modalities in the left part. Speech acts as the connection between language and audio knowledge, while video constructs the bridge with highly relevant visual and audio information. Therefore, we design the progressive alignment training strategy from primary to periphery. Furthermore, we design the cross-modality video-audio data to better capture the relationships among modalities.", "description": "Figure 4 illustrates the Ola model's progressive modality alignment training strategy.  The left side depicts the relationships between modalities: text connects to image directly, while speech acts as a bridge between text and audio, and video connects text, image, and audio. The training process starts with aligning text and image (Stage 1), then adds video data (Stage 2) and finally incorporates audio and video data (Stage 3) to fully integrate all modalities. This progressive approach is chosen because it leverages existing strong text-image models and incrementally adds more complex modalities.  Specifically, cross-modal video-audio data was created to enhance learning of the relationships between audio and visual information. The figure visually represents this step-wise training process.", "section": "3. Ola: Omni-Modal Understanding"}, {"figure_path": "https://arxiv.org/html/2502.04328/x6.png", "caption": "Figure 5: Generative results on speech and visual understanding tasks. We illustrate results on speech and video understanding and show the strong ability of omni-modal Ola compared with conventional vision-language models.", "description": "Figure 5 presents a comparison of Ola's performance on speech and video understanding tasks against conventional vision-language models.  The figure showcases examples where Ola leverages its omni-modal capabilities (processing text, audio, and visual data simultaneously) to generate more comprehensive and accurate responses compared to models limited to vision and language only.  Specifically, it highlights Ola's superior ability to understand nuanced details and contexts from audio inputs within a video, resulting in superior responses.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.04328/x7.png", "caption": "Figure 6: Showcases on Text and Audio Understanding.", "description": "Figure 6 presents examples showcasing Ola's capabilities in understanding and generating text based on various audio inputs.  It demonstrates Ola's ability to accurately interpret and respond to diverse audio types, including music, speech, and environmental sounds. The figure highlights Ola's comprehension of both the semantic content and the emotional context within the audio, and its ability to generate relevant and coherent textual descriptions or responses.  This showcases Ola's capacity for robust cross-modal understanding and generation.", "section": "4. More Showcases"}]