[{"figure_path": "https://arxiv.org/html/2412.17589/extracted/6090997/PC_Agent_Images/animation.png", "caption": "Figure 1: Overview of our framework, consisting of three key components: (1) PC Tracker, a lightweight infrastructure that collects human-computer interaction trajectories by recording user actions and state observations; (2) a two-stage cognition completion that converts raw interaction data into cognitive trajectories through data refinement and human cognition completion, including action semantics and thought processes; and (3) a multi-agent system comprising a planning agent for action decision-making and a grounding agent for click position grounding.", "description": "The figure illustrates the PC Agent framework's architecture, detailing its three main components.  PC Tracker, a lightweight system, captures human-computer interaction data by recording user actions and the resulting screen state. This raw data is then processed by the Cognition Completion module, a two-stage pipeline.  First, data refinement cleans and standardizes the raw data. Second, the system infers the user's cognitive processes (thoughts and intentions behind each action) by completing the action semantics. Finally, a multi-agent system utilizes the completed cognitive trajectories.  This system integrates a planning agent, which makes high-level decisions about the work to be done, and a grounding agent, which performs low-level actions, such as precisely clicking GUI elements. The overall goal is to transform raw interaction data into rich cognitive information for training an AI system capable of mimicking human work performance.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.17589/extracted/6090997/PC_Agent_Images/overview.png", "caption": "Figure 2: Key features of PC Tracker", "description": "PC Tracker is a lightweight infrastructure for efficiently collecting large-scale human-computer interaction trajectories.  Its key features are:  Lightweight Collection (efficient event-based tracking, not bulky video recording); High Usability (seamless background operation, non-disruptive to user workflow); Scalability (supports unlimited data collection); Unified Action Space (combines fragmented actions into a unified representation, reducing recorded events); and Data Transparency (local data storage, user control over recording, transparent markdown visualization).", "section": "4 PC Tracker: Human-Computer Interaction Data Collection Infrastructure"}, {"figure_path": "https://arxiv.org/html/2412.17589/extracted/6090997/PC_Agent_Images/Key_Feature.png", "caption": "Figure 3: An example trajectory collected by PC Tracker. Red marks on the screenshots indicate the positions of click-related actions.", "description": "This figure showcases a sample user interaction trajectory captured by the PC Tracker system. The trajectory visually depicts a sequence of events during a user's interaction with their computer, focusing on the creation of a PowerPoint slide. Each screenshot represents a moment within the interaction, showing the state of the user's applications and the position of the mouse cursor.  Red marks highlight the precise locations of click-related actions to indicate specific steps performed by the user.  This visual representation illustrates how PC Tracker collects comprehensive data on user behavior, going beyond simple screen recordings to provide a detailed record of human-computer interaction, including the sequential nature of actions and their contextual settings.", "section": "4 PC Tracker: Human-Computer Interaction Data Collection Infrastructure"}, {"figure_path": "https://arxiv.org/html/2412.17589/extracted/6090997/PC_Agent_Images/trajectory_example.png", "caption": "Figure 4: Action space \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A of PC Tracker.", "description": "The figure shows the action space of PC Tracker, a system designed to collect large-scale human-computer interaction trajectories.  The action space encompasses a variety of actions that can be taken during the recording of interactions, including mouse clicks (single, double, right-click), mouse drags, scrolling, keyboard key presses, hotkey combinations, text entry, pauses (waits), task completion, and task failures.  This comprehensive set of actions ensures that a wide range of user behaviors during computer interactions can be captured and analyzed.", "section": "4 PC Tracker: Human-Computer Interaction Data Collection Infrastructure"}, {"figure_path": "https://arxiv.org/html/2412.17589/extracted/6090997/PC_Agent_Images/Dual_Mode_Collection.png", "caption": "Figure 5: Example of type encapsulation.", "description": "This figure illustrates how PC Tracker encapsulates multiple raw keyboard events into a single, unified \"type text\" action.  The left column shows the sequence of raw actions, including individual key presses, capitalization changes, and backspaces.  The right column shows how these are consolidated into a single, semantically meaningful action representing the user's intended typing of the text \"Hello\". This demonstrates the process of data refinement within PC Tracker, which improves data efficiency and facilitates downstream cognition completion.", "section": "4 PC Tracker: Human-Computer Interaction Data Collection Infrastructure"}, {"figure_path": "https://arxiv.org/html/2412.17589/extracted/6090997/PC_Agent_Images/congnition_completion.png", "caption": "Figure 6: An example of the output from get\u2062_\u2062element\u2062_\u2062info\u2062_\u2062at\u2062_\u2062position\u2061(x,y)get_element_info_at_position\ud835\udc65\ud835\udc66\\operatorname{get\\_element\\_info\\_at\\_position}(x,y)start_OPFUNCTION roman_get _ roman_element _ roman_info _ roman_at _ roman_position end_OPFUNCTION ( italic_x , italic_y ) when the user clicks Chrome icon at (1161,1065)11611065(1161,1065)( 1161 , 1065 ).", "description": "The figure displays the output from the `get_element_info_at_position(x, y)` function, which is called when a user clicks on a specific GUI element. The function returns information about the clicked element, including its name and bounding box coordinates.  In this example, the user clicked on the Chrome icon at coordinates (1161, 1065). The figure shows how the function retrieves the element's name (\"Google Chrome\") and its bounding box coordinates, which define the location and size of the icon on the screen. This data is crucial for the PC Agent system to accurately identify and interact with GUI elements during complex work scenarios.", "section": "4 PC Tracker: Human-Computer Interaction Data Collection Infrastructure"}, {"figure_path": "https://arxiv.org/html/2412.17589/extracted/6090997/PC_Agent_Images/collaboration_arch.png", "caption": "Figure 7: Natural flow of human interaction: Observe, Think, Act.", "description": "This figure illustrates the natural flow of human interaction when engaging with a computer or other digital system. It emphasizes the cyclical nature of the process: a user begins by observing their environment (the screen display, available tools, etc.), then considers this information to formulate a plan of action (thinking stage), and finally executes this plan through interaction (acting stage).  This cycle repeats continuously as humans interact to complete tasks, adapt to situations, and resolve problems.", "section": "4 PC Tracker: Human-Computer Interaction Data Collection Infrastructure"}, {"figure_path": "https://arxiv.org/html/2412.17589/extracted/6090997/PC_Agent_Images/Train_Data_Example.jpg", "caption": "Figure 8: An overview of the dual-mode collection design", "description": "PC Tracker offers two data collection modes: task-oriented and non-task-oriented.  The task-oriented mode is further divided into given-task and free-task sub-modes. In the given-task mode, users are provided with predefined tasks to complete, while in the free-task mode, they can use the computer freely.  In both sub-modes, task descriptions are recorded. The non-task-oriented mode collects data without any specific tasks, enabling large-scale data collection for general behavior analysis. This dual-mode design allows for both focused, supervised data collection for fine-tuning and broader, unsupervised data collection for pre-training and broader understanding of human computer interaction.", "section": "4 PC Tracker: Human-Computer Interaction Data Collection Infrastructure"}, {"figure_path": "https://arxiv.org/html/2412.17589/extracted/6090997/PC_Agent_Images/grounding_validation.png", "caption": "Figure 9: Visualization of our cognition completion process for a click action. (Left) Raw click event recorded by PC Tracker. (Center) Action semantic completion converts coordinates (717, 387) into a semantic description \u201cthe search box at the top center of the TripAdvisor page\u201d. (Right) Thought completion reconstructs the human intention behind this action - finding high-rated restaurants near the Eiffel Tower by broadening the search scope.", "description": "This figure illustrates the two-stage cognition completion pipeline. The left panel shows the raw click event data captured by PC Tracker, including coordinates (717, 387). The center panel displays the result of action semantic completion, converting the raw coordinates into a more descriptive and semantically rich representation: \u201cthe search box at the top center of the TripAdvisor page\u201d. Finally, the right panel demonstrates the outcome of thought completion, which interprets the user's intention behind the click. In this instance, the user aimed to find highly-rated restaurants near the Eiffel Tower by expanding the search scope, demonstrating an understanding of the task's context and the user's cognitive goals.", "section": "Cognition Completion"}, {"figure_path": "https://arxiv.org/html/2412.17589/extracted/6090997/PC_Agent_Images/task_number.png", "caption": "Figure 10: Illustration of multi-agent workflow. The planning agent initially attempts to click a nonexistent element The \u2018Images\u2019 button, which is reported by the grounding agent. Upon receiving this feedback, the planning agent reformulates its plan, and the grounding agent generates coordinates of the new click target. The workflow illustrates the error correction mechanism between agents.", "description": "This figure illustrates the collaborative workflow of a multi-agent system, specifically highlighting the error correction mechanism between the planning and grounding agents. Initially, the planning agent attempts to click the 'Images' button. However, the grounding agent detects that this element does not exist and informs the planning agent.  This feedback prompts the planning agent to revise its plan. Subsequently, the grounding agent generates the coordinates for a new, valid click target. This iterative process showcases how the agents collaborate to ensure successful task completion, even when unexpected situations arise.", "section": "6 PC Agent: Cognitive Agent for Complex Computer Work"}, {"figure_path": "https://arxiv.org/html/2412.17589/extracted/6090997/PC_Agent_Images/ppt_example_border.png", "caption": "Figure 11: Training data example showing query and response structure.", "description": "Figure 11 shows an example of the training data used for the planning agent.  The training data consists of several parts: a system prompt that explains the overall task to the agent, a task description that details the specific task to perform, and a history of previous actions and thoughts of the agent.  Finally, the figure shows an example response of the agent. The response is separated into a thought process section and an action that the agent proposes to execute. This structure is crucial to the approach described in the paper because it allows the agent to learn not only the specific actions needed for task completion but also the cognitive processes behind those actions.", "section": "6.2 Training Planning Agent via Cognitive Trajectories"}, {"figure_path": "https://arxiv.org/html/2412.17589/extracted/6090997/PC_Agent_Images/task_description.png", "caption": "Figure 12: Illustration of the grounding agent\u2019s self-validation mechanism.", "description": "The grounding agent receives a click target description. It first attempts to generate the coordinates of the target in the screenshot. If the target doesn't exist, the agent outputs \"there are none\". If it successfully outputs the coordinates, it retrieves corresponding element information. Then, it annotates the screenshot with red marks indicating element boundary and click position. This, along with the retrieved element name, is used to determine if the generated position matches the initial target description.  If inconsistent, the process reattempts output until a match is found or a retry limit is reached.", "section": "6.3 Robust Visual Grounding with Self-Validation"}]