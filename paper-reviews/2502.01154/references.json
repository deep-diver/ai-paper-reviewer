{"references": [{"fullname_first_author": "Gabriel Alon", "paper_title": "Detecting language model attacks with perplexity", "publication_date": "2023-08-14", "reason": "This paper introduces a defense mechanism against jailbreaking attacks by detecting abnormal inputs through perplexity filtering, which is directly relevant to the paper's defense methodology."}, {"fullname_first_author": "Moustafa Farid Alzantot", "paper_title": "Generating natural language adversarial examples", "publication_date": "2018-04-07", "reason": "This early work on adversarial attacks in NLP laid the groundwork for subsequent research on jailbreaking, providing a foundational understanding of vulnerabilities in language models."}, {"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-05-14", "reason": "This paper highlights the remarkable ability of LLMs to generalize across various tasks without fine-tuning, a key characteristic that influences the approach to jailbreaking and defense strategies discussed in the paper."}, {"fullname_first_author": "Andy Zou", "paper_title": "Universal and transferable adversarial attacks on aligned language models", "publication_date": "2023-07-15", "reason": "This paper focuses on universal attacks, which directly addresses the paper's goal of developing a universal method for jailbreaking LLMs, making it highly relevant to the core of the research."}, {"fullname_first_author": "Lianmin Zheng", "paper_title": "Judging LLM-as-a-judge with MT-bench and Chatbot Arena", "publication_date": "2023-06-05", "reason": "This paper introduces MT-bench and Chatbot Arena, which are used in the paper's evaluation methodology, making this a crucial reference for understanding the metrics and benchmarks utilized in the study."}]}