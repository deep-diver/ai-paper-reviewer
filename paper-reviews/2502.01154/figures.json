[{"figure_path": "https://arxiv.org/html/2502.01154/x1.png", "caption": "Figure 1: Framework of our proposed method, JUMP. We perform a universal jailbreak attack by optimizing universal multi-prompts, framed by a red dashed line. We decompose our training pipeline into four stages: Selector, Mutator, Constraints, and Evaluator, which are detailed in Section\u00a03.3.", "description": "This figure illustrates the JUMP framework, a method for universal jailbreaking of large language models (LLMs) using multi-prompts.  It shows a four-stage pipeline: 1) *Sampling*: K candidate templates are randomly selected from the adversarial templates. 2) *Mutation*:  New beam candidates are generated from each template by replacing placeholders with harmful instructions. The attacker model predicts the next tokens to extend these candidates. 3) *Evaluation*: Each beam candidate is evaluated using the loss function on the victim model. 4) *Selection*:  The best candidates are selected based on minimal loss and their perplexity. The process iterates until an optimal set of universal multi-prompts is obtained. The red dashed line highlights the core of the method, focusing on optimizing these universal multi-prompts.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.01154/x2.png", "caption": "Figure 2: Tradeoffs between perplexity and ASR under different settings.", "description": "This figure illustrates the relationship between the attack success rate (ASR) and perplexity when using different temperature settings in the JUMP model.  The x-axis represents the perplexity score and the y-axis represents the ASR. Multiple lines represent different temperature settings. The plot shows that there's a trade-off; higher temperatures may lead to higher ASR but also higher perplexity, which makes the attacks more easily detectable. Lower temperatures increase the stealthiness of the attacks but may reduce the ASR.", "section": "3.4 Adding perplexity constraints to JUMP*"}, {"figure_path": "https://arxiv.org/html/2502.01154/x3.png", "caption": "Figure 3: Ablations on the performance of three prompting methods (including JUMP++) under different types of initialization.", "description": "This figure displays the ablation study on the performance of three different prompting methods: AutoDAN, GPTFuzzer, and JUMP++.  The study focuses on how the choice of initialization for each method affects its success rate.  The x-axis represents the three different initialization methods, and the y-axis shows the Attack Success Rate (ASR) for each method on Llama 2-7b and Llama 3-8b language models.  The results illustrate the impact of initialization on model performance, highlighting the relative effectiveness of different strategies in generating effective prompts.", "section": "4.5 Sensitivity to different choices of initial templates"}, {"figure_path": "https://arxiv.org/html/2502.01154/x4.png", "caption": "Figure 4: ASR curves against AutoDAN for the three defense settings: No Defense, SmoothLLM, and DUMP. Solid lines represent ASR evaluated by String Matching, while dashed lines represent ASR evaluated by Llama Guard.", "description": "This figure displays the attack success rates (ASR) of the AutoDAN attack against three different defense mechanisms: No Defense, SmoothLLM, and DUMP.  The x-axis represents the number of queries made during the attack, while the y-axis shows the ASR. Separate curves are plotted for each defense method, and ASRs are calculated using two different evaluation metrics: String Matching (solid lines) and Llama Guard (dashed lines). This allows for a comparison of the effectiveness of different defense strategies under varying attack conditions and evaluation criteria.", "section": "5 Defenses against Individual Attacks"}]