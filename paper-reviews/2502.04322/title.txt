Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions