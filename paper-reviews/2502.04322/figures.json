[{"figure_path": "https://arxiv.org/html/2502.04322/x1.png", "caption": "Figure 1: Process flow of human evaluation to identify attributes contributing to jailbreak harmfulness. We collect and curate 10101010 harmful base examples that meet all four attributes and augment each response into 16161616 variants with different attribute combinations using GPT-4. Human annotators then assess each variant for the four attributes and the overall harm level.", "description": "This figure illustrates the process of a human evaluation study designed to determine which attributes of a Large Language Model (LLM) response contribute most to its potential for harm.  Ten examples of harmful LLM responses were initially selected. Each of these responses was then modified to create sixteen variations exhibiting different combinations of four key attributes (actionability, coherence, conciseness, and informativeness).  These variations were evaluated by human annotators who scored each response for the presence or absence of the four attributes, as well as assessing the overall level of harm associated with the response. This process allowed the researchers to quantify the relationship between specific attributes and the perceived harmfulness of LLM outputs.", "section": "3. What Constitutes a Harmful Jailbreak?"}, {"figure_path": "https://arxiv.org/html/2502.04322/x2.png", "caption": "Figure 2: Real-world human-LLM interactions from WildVis. The left example illustrates a multi-step user-LLM interaction with a malicious query and subsequent follow-ups.\nIn the right example, the multilingual LLM provides step-by-step instructions in response to a malicious query in Spanish.", "description": "This figure showcases two real-world examples of human-LLM interactions from the WildVis dataset, highlighting how simple interactions can be exploited for malicious purposes. The left panel displays a multi-step interaction where a user gradually leads the LLM towards providing instructions for harmful actions through a series of seemingly innocuous queries.  The right panel demonstrates a multilingual interaction, illustrating how users can circumvent safety mechanisms by phrasing their malicious requests in a different language (Spanish, in this case).  Both examples illustrate the vulnerabilities of LLMs to harmful jailbreaks.", "section": "4. Jailbreaks Through Simple Interactions"}, {"figure_path": "https://arxiv.org/html/2502.04322/x3.png", "caption": "Figure 3: Our Speak Easy jailbreak framework.\nGiven a malicious query, we (1) decompose it into multiple steps of seemingly harmless subqueries and (2) translate each subquery into a set of predefined languages from different resource groups.\nWe then (3) prompt multilingual LLMs with the translated subqueries at each step.\nAfter collecting the responses, we (4) translate them back into English and (5) select the most actionable and informative response for each subquery using our response selection models.\nFinally, (6) the selected responses are combined to form a complete response to the original malicious query.", "description": "The SPEAK EASY framework elicits harmful jailbreaks from LLMs using a multi-step, multilingual approach.  First, a malicious query is broken down into smaller, seemingly harmless sub-queries.  These sub-queries are then translated into multiple languages.  Next, a multilingual LLM is prompted with these translated sub-queries. The responses are translated back into English. Then, a response selection model chooses the most actionable and informative response for each sub-query. Finally, these selected responses are combined to create a complete, potentially harmful response to the original malicious query.", "section": "4. Jailbreaks Through Simple Interactions"}, {"figure_path": "https://arxiv.org/html/2502.04322/x4.png", "caption": "Figure 4: Jailbreak performance measured by ASR and HarmScore before and after integrating Speak Easy into the baselines, with the shaded bars highlighting the difference. Speak Easy significantly increases both ASR and HarmScore across almost all methods.\nSee Table\u00a010 for full numerical values.", "description": "Figure 4 presents a comparison of the attack success rate (ASR) and HARMSCORE (a novel metric assessing the harmfulness of jailbreak responses) before and after incorporating the SPEAK EASY framework.  The results are displayed for three distinct Large Language Models (LLMs): GPT-40, Qwen2-72B-Instruct, and Llama-3.3-70B-Instruct. Each LLM is tested across four different jailbreak benchmarks using three baseline methods: Direct Request (DR), Greedy Coordinate Gradient-Transfer (GCG-T), and Tree of Attacks with Pruning-Transfer (TAP-T). For each LLM and baseline method, the figure shows the ASR and HARMSCORE before SPEAK EASY is applied (unshaded bars) and after its integration (shaded bars). The shaded bars visually represent the increase in both ASR and HARMSCORE resulting from the application of the SPEAK EASY framework. The caption notes that SPEAK EASY consistently and significantly improves both ASR and HARMSCORE for almost all methods and benchmarks tested, indicating a significant increase in the effectiveness of these attacks when the SPEAK EASY method is used.  Table 10 provides detailed numerical values for all shown data points.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.04322/x5.png", "caption": "Figure 5: Top: Average actionability and informativeness scores; Bottom: Selection rates for each language, both for n=6\ud835\udc5b6n=6italic_n = 6.\nEach color theme represents a language resource level.\nWhile informativeness remains consistent across languages, actionability and selection rate decreases with resource level.", "description": "Figure 5 presents a dual analysis of language performance in a multilingual LLM setting, specifically exploring actionability and informativeness of model responses. The top section displays the average actionability and informativeness scores across six different languages.  Each language is categorized by its resource level (high, medium, or low).  Actionability scores show a correlation with resource levels, where high-resource languages exhibit higher scores than low-resource languages.  Informativeness scores, however, show consistent high values across all languages, indicating that the model generates informative outputs regardless of the language's resource level. The bottom section illustrates the selection rate (frequency with which the response was chosen by the model) for each language when the model is given a choice of six languages. This reinforces the trend observed in actionability scores, demonstrating that high-resource languages are more frequently selected than low-resource ones.", "section": "5.5. Language Usage in SPEAK EASY"}, {"figure_path": "https://arxiv.org/html/2502.04322/x10.png", "caption": "Figure 6: Prompts used to instruct GPT-4o to augment responses by removing each attribute from the response. [QUERY] and [RESPONSE] are replaced with the original query-response pairs from Table\u00a07, respectively.", "description": "This figure details the prompts used to instruct GPT-4 to modify the responses from Table 7 by removing one attribute at a time.  Each prompt guides GPT-4 to generate response variations lacking a specific attribute (actionability, informativeness, coherence, conciseness).  The goal is to create a set of responses with all possible combinations of these attributes to study their impact on harmfulness.  The prompts ensure GPT-4 maintains the overall structure and context of the original response while systematically removing each attribute.", "section": "A.3. Response Augmentation"}, {"figure_path": "https://arxiv.org/html/2502.04322/x11.png", "caption": "Figure 7: Annotation questionnaire for assessing the relationship between the four identified attributes and the harm in jailbreak responses.", "description": "This figure displays the questionnaire used in a human evaluation study to assess the relationship between four attributes (actionability, informativeness, coherence, and conciseness) of LLM responses and the perceived harmfulness of those responses.  The questionnaire guides annotators to rate each attribute for a given response (yes/no for each attribute, except harmfulness, which uses a high/moderate/low scale), providing definitions for each attribute for clarity. The goal was to determine which attributes contribute most strongly to the perception of harm in jailbroken LLM outputs.", "section": "3.1. Human Evaluation on Jailbreak Attributes"}, {"figure_path": "https://arxiv.org/html/2502.04322/x12.png", "caption": "Figure 8: Annotation instructions and example instances for comparing alignment results between ASR and HarmScore.", "description": "Figure 8 shows the instructions given to human annotators for evaluating the harmfulness of LLM responses.  It also provides example query-response pairs. The goal is to compare how well two different metrics, Attack Success Rate (ASR) and HARMSCORE, align with human judgment of the harmfulness of the LLM's output. The instructions guide annotators to focus on the potential real-world harm of the response, considering its actionability and informativeness.  This process helps to assess whether HARMSCORE accurately reflects human perceptions of harmful responses.", "section": "5.2. Human Evaluation for HARMSCORE"}, {"figure_path": "https://arxiv.org/html/2502.04322/x13.png", "caption": "Figure 9: The prompt used to decompose the harmful query, along with four in-context examples. [NUMBER OF SUBQUERIES] and [HARMFUL QUERY] are replaced with the number of subqueries and the jailbreak query during test time. By default, [NUMBER OF SUBQUERIES] is set to 3.", "description": "This figure shows the prompt used to decompose a harmful query into multiple benign subqueries.  The prompt includes four in-context examples of benign questions to guide the LLM in breaking down the harmful query.  The number of subqueries and the specific harmful query are placeholders that change during testing, with a default of 3 subqueries.", "section": "4.1. The SPEAK EASY Jailbreak Framework"}, {"figure_path": "https://arxiv.org/html/2502.04322/x14.png", "caption": "(a) Prompt used to summarize paragraph-length questions in the Stack-Exchange-Preferences dataset into a single sentence.", "description": "This prompt instructs GPT-4 to transform multi-sentence questions from the Stack-Exchange-Preferences dataset into concise, single-sentence questions. This is a crucial preprocessing step for creating a dataset suitable for training response selection models, as it ensures that all questions are uniformly formatted, and only the essential information is retained.  The prompt takes the original question as input and outputs a single-sentence question that maintains the core meaning while eliminating unnecessary details or context. This ensures consistency and efficiency during the subsequent training phase.", "section": "B.1. Query Decomposition in SPEAK EASY"}, {"figure_path": "https://arxiv.org/html/2502.04322/x15.png", "caption": "(b) Prompt used to determine whether a question from the HH-RLHF or Stack-Exchange-Preferences datasets can be answered with an actionable response to filter out irrelevant questions.", "description": "This prompt is used to filter questions from the HH-RLHF or Stack-Exchange-Preferences datasets that can be answered with an actionable response.  It instructs a model to determine if a question requests instructions on how to do something, focusing on whether the answer would provide concrete, actionable steps, thus filtering out questions that are not suitable for this specific evaluation.", "section": "B. Response Selection Models"}, {"figure_path": "https://arxiv.org/html/2502.04322/x16.png", "caption": "(c) Prompt used to determine whether a question from the HH-RLHF or Stack-Exchange-Preferences datasets can be answered with an informative response to filter out irrelevant questions.", "description": "This prompt filters out irrelevant questions from the HH-RLHF and Stack-Exchange-Preferences datasets by determining whether a given question can be answered with an informative response.  It provides a definition of 'informative' and asks for a simple yes/no response, ensuring only relevant questions are included in subsequent analysis.", "section": "B.2. Response Selection Models"}, {"figure_path": "https://arxiv.org/html/2502.04322/x17.png", "caption": "(d) Prompt used to determine whether a query-response pair is actionable or informative.", "description": "This prompt is used in the process of creating the dataset used to train the response selection models.  The goal is to determine whether a query-response pair exhibits actionability or informativeness. The prompt provides definitions of each attribute (actionable and informative) and asks for a binary (yes/no) response indicating if the given answer meets the criteria of the defined attribute.  This process is crucial to filter and label appropriate query-response pairs for training the models that are used to select the best response in the SPEAK EASY framework.", "section": "A.3 Response Augmentation"}]