[{"content": "| Method | SSIM\u2191 | MS-SSIM\u2191 | CW-SSIM\u2191 | L-PIPS\u2193 | L-FID\u2193 | CLIP-FID\u2193 | DI-KID\u2193 | DI-STS\u2193 |\n|---|---|---|---|---|---|---|---|---|\n| GAN-Pose [36] | 77.4 | 63.8 | 32.5 | 44.2 | 73.2 | 30.9 | 55.8 | 30.4 |\n| ViscoNet [7] | 58.5 | 50.7 | 28.9 | 54.0 | 42.3 | 12.1 | 25.5 | 31.2 |\n| OOTDiff. [57] | 65.1 | 50.6 | 26.1 | 49.5 | 54.0 | 17.5 | 33.2 | 32.4 |\n| CatVTON [9] | 72.8 | 56.9 | 32.0 | 45.9 | 31.4 | 9.7 | 17.8 | 28.2 |\n| Ours:TryOffDiff | **79.5** | **70.4** | **46.2** | **32.4** | **25.1** | **9.4** | **8.9** | **23.0** |", "caption": "Table 1: Quantitative comparison. Evaluation metrics for various methods on VITON-HD-test dataset in the VTOFF task.", "description": "This table presents a quantitative comparison of different virtual try-off (VTOFF) methods.  It evaluates their performance on the VITON-HD test dataset using a range of metrics that assess both reconstruction accuracy (how well the generated image matches the ground truth) and perceptual quality (how natural and visually appealing the generated image is to human observers). The metrics used include Structural Similarity Index Measure (SSIM) and its variants (MS-SSIM, CW-SSIM), Perceptual Image Patch Similarity (LPIPS), Fr\u00e9chet Inception Distance (FID), and its CLIP variant (CLIP-FID), Kernel Inception Distance (KID), and Deep Image Structure and Texture Similarity (DISTS). The table allows readers to compare the performance of various methods for generating standardized garment images from single reference images of clothed individuals.", "section": "4. Experiments"}, {"content": "| Method | VAE | Img. Encoder | Emb.shape | Adapter | Cond.shape | Sched. | Prec. | Steps |\n|---|---|---|---|---|---|---|---|---|\n| Autoencoder | - | - | - | - | - | - | fp32 | 290k |\n| PixelModel | - | SigLIP-B/16 | (1024,768) | Linear+LN | (64,768) | DDPM | fp16 | 300k |\n| LDM-1 | SD3 | CLIP ViT-B/32 | (50,768) | - | (50,768) | DDPM | fp16 | 180k |\n| LDM-2 | SD3 | SigLIP-B/16 | (1024,768) | Linear+LN | (64,768) | DDPM | fp16 | 320k |\n| LDM-3 | SD3 | SigLIP-B/16 | (1024,768) | Linear+LN | (64,768) | DDPM | fp32 | 120k |\n| TryOffDiff | SD1.4 | SigLIP-B/16 | (1024,768) | Trans.+Linear+LN | (77,768) | PNDM | fp32 | 220k |", "caption": "Table 2: Training configurations of ablations.", "description": "This table details the different configurations used in the ablation study for the TryOffDiff model.  It lists the model type (Autoencoder, PixelModel, various Latent Diffusion Models (LDMs), and the final TryOffDiff model), the image encoder used (VAE, SigLIP), the embedding shape, the adapter type (Linear+LN or Transformer+Linear+LN), conditional shape, the scheduler used (DDPM or PNDM), the precision (fp16 or fp32), and the number of training steps.", "section": "3. Methodology"}, {"content": "| Method | Sched. | s | n | SSIM \u2191 | MS-SSIM \u2191 | CW-SSIM \u2191 | LPIPS \u2193 | FID \u2193 | CLIP-FID \u2193 | KID \u2193 | DISTS \u2193 |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| Autoencoder | - | - | - | **81.4** | <ins>72.0</ins> | 37.3 | 39.5 | 108.7 | 31.7 | 66.8 | 32.5 |\n| PixelModel | DDPM | - | 50 | 76.0 | 66.3 | 37.0 | 52.1 | 75.4 | 20.7 | 56.4 | 32.6 |\n| LDM-1 | DDPM | - | 50 | 79.6 | 70.5 | 42.0 | 33.0 | 26.6 | 9.14 | 11.5 | 24.3 |\n| LDM-2 | DDPM | - | 50 | <ins>80.2</ins> | **72.3** | **48.3** | **31.8** | <ins>18.9</ins> | **7.5** | **5.4** | **21.8** |\n| LDM-3 | DDPM | - | 50 | 79.5 | 71.3 | 46.9 | <ins>32.6</ins> | **18.6** | **7.5** | <ins>6.7</ins> | 22.7 |\n| TryOffDiff | PNDM | 2.0 | 50 | 79.4 | 71.5 | <ins>47.2</ins> | 33.2 | 20.2 | <ins>8.3</ins> | 6.8 | <ins>22.5</ins> |", "caption": "Table 3: Quantitative comparison. Evaluation metrics for different methods on VITON-HD-test dataset for VTOFF task. Results are reported on raw predictions, with no background removal. Note that while LDM-2 may achieve better performance metrics, we still choose TryOffDiff over LDM-2 due to its better subjective visual quality in garment image generation, see also Figure\u00a08.", "description": "This table presents a quantitative comparison of different models on the Virtual Try-Off (VTOFF) task using the VITON-HD test dataset.  The models are evaluated using several metrics including SSIM (structural similarity index), MS-SSIM (multi-scale SSIM), CW-SSIM (complex wavelet SSIM), LPIPS (Learned Perceptual Image Patch Similarity), FID (Fr\u00e9chet Inception Distance), CLIP-FID, KID (Kernel Inception Distance), and DISTS (Deep Image Structure and Texture Similarity).  The results are based on raw model outputs without any post-processing for background removal. Although one model (LDM-2) achieves slightly better numerical results in some metrics, TryOffDiff is preferred due to its superior subjective visual quality, which is supported by a visual comparison shown in Figure 8 of the paper.", "section": "4.3. Quantitative Results"}, {"content": "| Method |  |  |  | FID \u2193 |  | CLIP-FID \u2193 |  | KID \u2193 |\n|---|---|---|---|---|---|---|---|---|\n| CatVTON |  |  |  | 12.0 |  | 3.5 |  | 3.9 |\n| OOTDiffusion + GT |  |  |  | **10.8** |  | **2.8** |  | **2.0** |\n| OOTDiffusion + TryOffDiff |  |  |  | <u>12.0</u> |  | <u>3.5</u> |  | <u>2.5</u> |", "caption": "Table 4: Quantitative comparison of Virtual Try-On models.\nWe compare the results of OOTDiffusion when ground truth\u00a0(GT) garment is used\nand when the garment predicted by TryOffDiff is used. We further show\nthe results of CatVTON, a specialized person-to-person try-on model.\nOur TryOffDiff model in combination with VTON model achieves competitive\nperformance in person-to-person VTON.", "description": "This table compares the performance of different virtual try-on (VTON) models on a person-to-person try-on task.  It shows the results using three metrics (FID, CLIP-FID, KID) for three different approaches: 1) OOTDiffusion with ground truth garment images, 2) OOTDiffusion using garment images generated by TryOffDiff, and 3) CatVTON (a state-of-the-art person-to-person VTON model).  The comparison highlights the effectiveness of using TryOffDiff's generated garment images as input for VTON, demonstrating competitive performance compared to using ground truth images and a dedicated person-to-person VTON model.", "section": "7. Person-to-person Try-On"}]