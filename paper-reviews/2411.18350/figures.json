[{"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/VTONvsVTOFF.png", "caption": "Figure 1: Virtual try-off results generated by our method. The first row shows the input reference image, the second row our model\u2019s prediction, and the third row the ground truth. Our approach naturally renders the garment against a clean background, preserving the standard pose and capturing complex details of the target garment, such as patterns and logos, from a single reference image.", "description": "This figure showcases the performance of the TryOffDiff model in generating standardized garment images from a single reference image of a person wearing the garment. Each set of three rows represents a single example: the first row displays the input reference image, the second row shows the model's prediction of the garment isolated on a clean background, maintaining a consistent pose, and the third row presents the ground truth image. The results demonstrate the model's ability to capture intricate details such as patterns and logos, showcasing its high-fidelity garment reconstruction capabilities.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/metric_failures/ssim_4.jpg", "caption": "Figure 2: Illustration of the differences between Virtual Try-On and Virtual Try-Off.\nTop: Basic inference pipeline of a Virtual Try-On model, which takes an image of a clothed person as reference and an image of a garment to generate an image of the same person but wearing the specified garment. Bottom: Virtual Try-Off setup, where the objective is to predict the canonical form of the garment from a single input reference image.", "description": "This figure illustrates the core difference between the tasks of Virtual Try-On (VTON) and Virtual Try-Off (VTOFF). The top half shows the VTON pipeline, where the model takes two inputs: an image of a person wearing clothes and an image of the garment to be virtually 'tried on'. The output is an image of the person wearing the specified garment. The bottom half depicts the VTOFF pipeline, which only uses a single image of the clothed person as input and aims to produce a standardized, catalog-like image of the garment itself, focusing on accuracy of reconstruction rather than style or pose.", "section": "2. Related Work"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/metric_failures/ssim_5.jpg", "caption": "(a) 82.4\u2062\u00a0/\u00a0\u206220.682.4\u00a0/\u00a020.682.4\\text{ / }20.682.4 / 20.6", "description": "The figure demonstrates the inadequacy of using SSIM (structural similarity index measure) alone for evaluating the quality of generated garment images in the context of virtual try-off.  SSIM, while providing a numerical score, fails to capture the visual discrepancies apparent to the human eye. The figure presents a series of image comparisons. Each comparison shows the SSIM score and DISTS (Deep Image Structure and Texture Similarity) score. The DISTS score provides a perceptually-aware measurement of image similarity that aligns better with human judgments.  In the top row, we see various examples where the SSIM score is deceptively high despite major differences in image quality, while the DISTS score reflects these quality differences more accurately. The bottom row shows a similar trend with clear visual differences that are better reflected in the DISTS scores compared to SSIM.  This highlights the necessity of supplementing SSIM with perceptually-aware metrics like DISTS for comprehensive evaluation of image generation quality, specifically in the context of garment reconstruction tasks.", "section": "3.1 Virtual Try-Off"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/metric_failures/ssim_6.jpg", "caption": "(b) 96.8\u2062\u00a0/\u00a0\u206217.996.8\u00a0/\u00a017.996.8\\text{ / }17.996.8 / 17.9", "description": "This figure shows a comparison of different images using two metrics, SSIM and DISTS.  The first metric (SSIM), measures structural similarity, while the second (DISTS), evaluates perceptual similarity. The image shows that the SSIM metric gives a high score of 96.8 even with an image having color change. However, DISTS, which better reflects human perception, assigns a lower score of 17.9 to that image.", "section": "3.1. Virtual Try-Off"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/metric_failures/ssim_1.jpg", "caption": "(c) 88.3\u2062\u00a0/\u00a0\u206220.388.3\u00a0/\u00a020.388.3\\text{ / }20.388.3 / 20.3", "description": "This figure shows an example image used to demonstrate the suitability of different performance metrics for evaluating virtual try-on and virtual try-off tasks. It compares a reference image with various modified versions of the image, and shows how different metrics respond to these changes. The reference image is compared with (a) an image with masked-out garment, (b) an image with changed garment color, and (c) an image with color jittering applied. The second row compares a garment image to different altered images. These include (d) a plain white image, (e) a slightly rotated image, and (f) a randomly posterized image. This helps illustrate which metrics provide results that more closely correlate with human visual assessment of quality.", "section": "Performance Measures"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/metric_failures/ssim_2.jpg", "caption": "(d) 86.0\u2062\u00a0/\u00a0\u206270.386.0\u00a0/\u00a070.386.0\\text{ / }70.386.0 / 70.3", "description": "This figure shows a comparison of a garment image against different transformations to demonstrate the limitations of SSIM and the advantages of DISTS in assessing reconstruction quality. The top row shows a reference image and three variations, while the bottom shows a garment and three transformations. SSIM scores remain high regardless of transformation, but the DISTS score reflects visual quality.", "section": "3.1 Virtual Try-Off"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/metric_failures/ssim_3.jpg", "caption": "(e) 75.0\u2062\u00a0/\u00a0\u20628.275.0\u00a0/\u00a08.275.0\\text{ / }8.275.0 / 8.2", "description": "This figure is part of the performance evaluation in the paper. It shows a garment image (left) compared to a randomly posterized version of the same image (right). The posterized image uses fewer bits for each color channel, reducing the image quality.  The comparison highlights the limitations of the SSIM metric (structural similarity index measure) compared to the DISTS metric (Deep Image Structure and Texture Similarity) for evaluating image reconstruction quality.  While SSIM shows relatively high similarity between images even when there's a significant loss of fine details, DISTS provides a more accurate reflection of the perceptual difference, aligning better with human judgment. This shows the importance of using perception-based metrics like DISTS for assessing reconstruction fidelity in image generation tasks.", "section": "3.1 Virtual Try-Off"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/TryOffDiff.png", "caption": "(f) 86.4\u2062\u00a0/\u00a0\u206224.786.4\u00a0/\u00a024.786.4\\text{ / }24.786.4 / 24.7", "description": "This figure shows a comparison of a garment image against several variations. It demonstrates the limitations of common image quality metrics such as SSIM, which achieves consistently high scores across all examples, including failure cases, unlike the DISTS score which more accurately reflects variations aligned with human judgement. Specifically, the variations shown are (a) an image with a masked-out garment; (b) an image with changed colors of the garment; (c) an image after applying color jittering; (d) a plain white image; (e) a slightly rotated image; and (f) a randomly posterized image (reducing the number of bits per color channel).", "section": "3.1 Virtual Try-Off"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/baseline_pose2.jpg", "caption": "Figure 3: Examples demonstrating the un/suitability of performance metrics\u00a0(SSIM\u2191\u00a0/\u00a0DISTS\u2193) to VTON and VTOFF. In the top row, a reference image is compared against:\n(a) an image with a masked-out garment;\n(b) an image with changed colors of the model;\n(c) and an image after applying color jittering.\nIn the bottom row, a garment image is compared against:\n(d) a plain white image;\n(e) a slightly rotated image;\n(f) and a randomly posterized image (reducing the number of bits for each color channel).\nWhile the SSIM score achieves consistently high across all examples, in particular including failure cases, the DISTS score more accurately reflects variations aligned with human judgment.", "description": "This figure demonstrates the limitations of using SSIM (Structural Similarity Index) alone to evaluate image quality, particularly in the context of virtual try-on (VTON) and the novel virtual try-off (VTOFF) task.  The top row shows a reference image compared to three modified versions: one with the garment masked, one with altered colors, and one with color jittering.  The bottom row compares a garment image to three significantly different images: a plain white image, a slightly rotated version, and a posterized version (with reduced color depth). While SSIM scores remain high even for the heavily altered images, highlighting its insensitivity to perceptually significant changes, the DISTS (Deep Image Structure and Texture Similarity) scores are much lower for these, indicating better alignment with human perception of quality.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/baseline_visconet.jpg", "caption": "Figure 4: Overview of TryOffDiff. The SigLIP image encoder [59] extracts features from the reference image, which are subsequently processed by adapter modules. These extracted image features are embedded into a pre-trained text-to-image Stable Diffusion-v1.4 [35] by replacing the original text features in the cross-attention layers. By conditioning on image features in place of text features, TryOffDiff directly targets the VTOFF task. Simultaneous training of the adapter layers and the diffusion model enables effective garment transformation.", "description": "TryOffDiff is a model that adapts Stable Diffusion for the task of Virtual Try-Off (VTOFF).  The figure shows the architecture of this model. It starts with a SigLIP image encoder that processes a reference image of a person wearing clothing, extracting visual features. These features are fed into adapter modules, which refine the features before they are input into a pre-trained Stable Diffusion v1.4 model. Importantly, the adapter modules replace the text-based conditioning typically used in Stable Diffusion with image-based conditioning. By directly using image features, the model learns to reconstruct garment images without needing a text description. This direct approach is key to achieving high-fidelity garment reconstruction in the VTOFF task. The model's adapter modules and the Stable Diffusion model are trained simultaneously to ensure effective garment transformation and high-quality output.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/baseline_ootd.jpg", "caption": "(a) Left to right: reference image, fixed pose heatmap derived from target image, initial model output, SAM prompts, and final processed output.", "description": "This figure shows the process of adapting an existing state-of-the-art method (GAN-Pose) to the VTOFF task.  It starts with a reference image of a person wearing the garment. A fixed pose heatmap is then created, showing the ideal neutral pose desired for the standardized garment image. The initial model output shows the garment reconstruction attempt using only the reference image and pose heatmap.  Next, SAM (Segment Anything Model) prompts refine the mask of the garment and guide further processing. The final processed output is the improved garment image, showing the garment isolated on a clean background in a standardized pose and with enhanced accuracy. This illustrates the intermediate steps and refinement processes required to make existing methods suitable for the new virtual try-off task.", "section": "4.2. Baseline Approaches"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/baseline_catvton.jpg", "caption": "(b) Left to right: masked conditioning image, mask image, pose image, initial model output with SAM prompts, and final processed output.", "description": "This figure shows the adaptation of the ViscoNet model to the VTOFF task.  It details the five steps involved in generating a standardized garment image from a real-world image. (1) A masked conditioning image is used to isolate the garment. (2) A mask image highlights the garment region. (3) Pose information is provided to align the output with a neutral pose. (4) An initial garment reconstruction is generated using the ViscoNet model, guided by SAM prompts. (5) The final result shows the garment image after post-processing ( likely using the SAM prompts to refine the output and create a clean background).", "section": "4.2. Baseline Approaches"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/preds/1-input.jpg", "caption": "(c) Left to right: masked garment image, model image, masked model image, initial model output with SAM prompts, and final processed output.", "description": "This figure shows the adaptation of the OOTDiffusion model to the VTOFF task.  The process begins with a masked garment image (leftmost), representing the target garment without the background or other elements. Next, a generic model image (2nd from left) provides a basic structural shape. This model image is then masked (3rd from left), to match the initial masking used for the garment image. This masked model image is used as input to the OOTDiffusion model which generates an initial model output (4th from left) but often requires further refinement. Finally, Segment Anything (SAM) prompts are applied to enhance the image, resulting in a final processed output that features a complete and clean garment (rightmost).", "section": "4.2. Baseline Approaches"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/preds/1-pred_pose.jpg", "caption": "(d) Left to right: conditioning garment image, blank model image, mask image, initial model output with SAM prompts, final processed output.", "description": "This figure shows the adaptation of the CatVTON model for the virtual try-off task.  The process starts with a conditioning garment image (a clean image of the garment). A blank model image (a plain image of the same size as the output) and a mask image (specifying the area where the garment should appear) are used as inputs. The initial model output is generated using the CatVTON model with Segment Anything (SAM) prompts to guide the placement of the garment, resulting in an initial try-off image. Post-processing with SAM prompts refines the image, leading to the final processed output, which is a standardized image of the garment.", "section": "4.2. Baseline Approaches"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/preds/1-pred_visco.jpg", "caption": "Figure 5: Adapting existing state-of-the-art methods to VTOFF. (a) GAN-Pose [36] and (b) ViscoNet [7] are approaches based on pose transfer and view synthesis, respectively, (c) OOTDiffusion [57] and (d) CatVTON [9] are based on recent virtual try-on methods.", "description": "Figure 5 demonstrates adaptations of existing state-of-the-art methods for the virtual try-off task.  Specifically, it shows how four different models were adapted: GAN-Pose and ViscoNet (using pose transfer and view synthesis, respectively), and OOTDiffusion and CatVTON (both recent virtual try-on models).  The figure visually presents the input images and the results of each model's adaptation, highlighting the challenges and differences in their approaches to the virtual try-off problem.", "section": "4.2. Baseline Approaches"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/preds/1-pred_ootd.jpg", "caption": "(a) Reference", "description": "This figure shows a comparison of garment reconstruction results from different methods. Column (a) displays the reference image, showing a person wearing a garment. Subsequent columns show results from GAN-Pose, ViscoNet, OOTDiffusion, CatVTON, and TryOffDiff, each representing a different approach to garment image generation. The final column shows the ground truth garment image. The figure illustrates the performance of each model in terms of detail preservation and accuracy of reconstruction.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/preds/1-pred_cat.jpg", "caption": "(b) Gan-Pose", "description": "This figure shows the results of applying the GAN-Pose model for the Virtual Try-Off task. GAN-Pose is a pose transfer method that takes an image and pose heatmaps as input.  Here, the model is modified to predict a canonical garment image from a person wearing the garment by estimating a heatmap for a neutral pose, and then applying a pose transfer to align the output to a standard product image view.  The results show that GAN-Pose can manage to approximate the main color and shape of the garment, although some detail is missing, and the predicted images often contain inconsistencies, showing parts of the garment missing, and a somewhat unnatural appearance. ", "section": "4.2. Baseline Approaches"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/preds/1-pred_tryoffdiff.jpg", "caption": "(c) ViscoNet", "description": "This figure shows the results of adapting the ViscoNet model for the Virtual Try-Off task. ViscoNet, originally designed for view synthesis, requires a text prompt, a pose, a mask, and multiple masked conditioning images as input.  For this adaptation, a generic text prompt describing clothing is used, along with a neutral pose derived from the VITON-HD dataset and a mask generated using an off-the-shelf fashion parser. The resulting images demonstrate ViscoNet's performance in reconstructing garments from limited information.", "section": "4.2. Baseline Approaches"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/preds/1-gt.jpg", "caption": "(d) OOTDiffusion", "description": "This figure is part of an ablation study comparing different model architectures for virtual try-off. It shows the results of using the OOTDiffusion model, a baseline method adapted from virtual try-on, on the VTOFF task. The images depict the model's reconstruction of garments from reference images of people wearing those garments. This visualization aids in understanding the model's ability to capture the details and texture of the garments.", "section": "6. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/ssim_failures/ae-1.jpg", "caption": "(e) CatVTON", "description": "This figure shows the results of the CatVTON model, a virtual try-on method adapted for the virtual try-off task. It showcases how this particular method attempts garment reconstruction given an input image, but highlights limitations such as the struggle to accurately capture the garment's shape, occasionally producing deformed representations. Also, it demonstrates a tendency towards generating long sleeves, regardless of the actual garment design, and often lacks fine textural details.", "section": "4.2. Baseline Approaches"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/ssim_failures/ae-2.jpg", "caption": "(f) TryOffDiff", "description": "This figure shows the results of the TryOffDiff model, which is the main method proposed in this paper. It showcases examples of garments generated by the model based on input reference images.  The figure is used to demonstrate the model's ability to reconstruct garment details such as texture, patterns, and logos, achieving high fidelity and generating standardized product images from single images of clothed individuals. ", "section": "4. Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/ssim_failures/ae-3.jpg", "caption": "(g) Target", "description": "This image shows the ground truth garment image corresponding to the reference image used as input. It is the canonical, standardized garment image that the model aims to generate as output.  This serves as the target for evaluation of the model's performance.", "section": "4. Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/ssim_failures/tod-1.jpg", "caption": "Figure 6: Qualitative comparison. In comparison to the baseline approaches, TryOffDiff is capable of generating garment images with accurate structural details as well as fine textural details.", "description": "Figure 6 presents a qualitative comparison of garment image generation results between TryOffDiff and several baseline methods.  The figure visually demonstrates TryOffDiff's superior ability to accurately reconstruct both the overall shape (structural details) and fine details like logos and patterns (textural details) of garments.  This contrasts with the baseline methods, which often exhibit inaccuracies in either shape or texture.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/ssim_failures/tod-2.jpg", "caption": "(a) 81.9\u2062\u00a0/\u00a0\u206236.281.9\u00a0/\u00a036.281.9\\text{ / }36.281.9 / 36.2", "description": "This figure presents a qualitative comparison of garment reconstruction results obtained using different model configurations within the TryOffDiff framework. Each column represents a different model variant, showcasing the generated garment image alongside the corresponding ground truth image.  The numbers provided (e.g., \"81.9 / 36.2\") likely represent the SSIM and DISTS scores for each respective model configuration, reflecting reconstruction accuracy and perceptual similarity to the ground truth.", "section": "6. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/ssim_failures/tod-3.jpg", "caption": "(b) 81.5\u2062\u00a0/\u00a0\u206240.481.5\u00a0/\u00a040.481.5\\text{ / }40.481.5 / 40.4", "description": "This figure shows the qualitative results for a pixel-space diffusion model, a variation of the TryOffDiff model.  The top row displays the ground truth images of various garments, and each subsequent row presents the results from different model configurations.  The numerical values (e.g., \"81.5 / 40.4\") likely represent the SSIM and DISTS scores, indicating the model's performance in terms of structural similarity and perceptual quality.  The aim is to visually demonstrate the impact of various model configurations on the quality of garment image reconstruction.", "section": "6.1 Impact of TryOffDiff configurations"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/ablations/pred_xunet.jpg", "caption": "(c) 81.7\u2062\u00a0/\u00a0\u206239.781.7\u00a0/\u00a039.781.7\\text{ / }39.781.7 / 39.7", "description": "The figure shows qualitative results from an ablation study comparing different model configurations for the task of garment reconstruction. Specifically, it visualizes the outputs of several models, including an autoencoder, pixel-based diffusion model, three latent diffusion models (LDM-1, LDM-2, LDM-3), and the final TryOffDiff model. Each row displays the results for a different garment, with the ground truth image shown on the far right. The models' outputs vary in quality, with some showing only basic shapes while others capturing finer details like logos and patterns. The TryOffDiff model demonstrates the best reconstruction quality overall, capturing a high level of detail and visual fidelity.", "section": "6. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/ablations/pred_pixelmodel.jpg", "caption": "(d) 80.3\u2062\u00a0/\u00a0\u206224.280.3\u00a0/\u00a024.280.3\\text{ / }24.280.3 / 24.2", "description": "This figure shows a qualitative comparison of garment reconstruction results.  It displays the results from six different methods: (a) Autoencoder, (b) PixelModel, (c) LDM-1, (d) LDM-2, (e) LDM-3, and (f) TryOffDiff. Each method's reconstruction of a specific garment is shown alongside the corresponding ground truth garment image (g).  The caption numbers (e.g., 80.3/24.2) represent evaluation metrics (SSIM/DISTS) for each generated image, illustrating the relative success of each model in achieving visual fidelity compared to the ground truth.", "section": "6.1 Impact of TryOffDiff configurations"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/ablations/pred_ldm1.jpg", "caption": "(e) 75.3\u2062\u00a0/\u00a0\u206225.075.3\u00a0/\u00a025.075.3\\text{ / }25.075.3 / 25.0", "description": "This figure shows a comparison of garment reconstruction results between different model configurations. The image displays a series of garments, with each row representing a different configuration of the model, and each column showing the results for a specific garment. It showcases the effect of various configurations such as Autoencoder, PixelModel, LDM-1, LDM-2, LDM-3, and the final TryOffDiff model. The numerical values displayed are SSIM scores and DISTS scores. The caption is too short to adequately describe the figure, but it shows that the TryOffDiff approach outperforms other baseline approaches.", "section": "6.1 Impact of TryOffDiff configurations"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/ablations/pred_ldm2.jpg", "caption": "(f) 80.3\u2062\u00a0/\u00a0\u206219.480.3\u00a0/\u00a019.480.3\\text{ / }19.480.3 / 19.4", "description": "The image shows a comparison between the ground truth garment image and the garment image generated by the TryOffDiff model. The ground truth image is on the left, and the generated image is on the right.  The numbers '80.3 / 19.4' likely represent evaluation metrics (e.g., SSIM and DISTS scores) comparing the generated image to the ground truth.", "section": "Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/ablations/pred_ldm3.jpg", "caption": "Figure 7: Examples demonstrating the un-/suitability of performance metrics\u00a0(SSIM\u2191\u00a0/\u00a0DISTS\u2193) and an Autoencoer model applied to VTOFF.\nIn each figure, left image is the ground truth image and the right image is the model prediction of Autoencoder\u00a0(top, a-c) and TryOffDiff\u00a0(bottom, d-f). Notice the higher SSIM scores for the Autoencoder compared to TryOffDiff despite poor visual quality of reconstructed garment images.", "description": "Figure 7 demonstrates the limitations of using SSIM (Structural Similarity Index) alone to evaluate image quality in the context of virtual try-off.  It compares the results of an autoencoder and the TryOffDiff model.  The autoencoder achieves higher SSIM scores, but produces images of significantly lower visual quality compared to TryOffDiff. The figure showcases that while SSIM might indicate high similarity in terms of pixel-level values, it fails to capture the overall perceptual quality and fidelity of the garment reconstruction.  The images show that TryOffDiff produces visually superior garments despite lower SSIM scores, highlighting the need for a metric that considers both structural similarity and visual quality, such as DISTS (Deep Image Structure and Texture Similarity).", "section": "6. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/ablations/pred_tryoffdiff.jpg", "caption": "(a) Autoencoder", "description": "This figure displays qualitative results comparing different model configurations. Specifically, it contrasts the output of an Autoencoder model with other models (PixelModel, LDM-1, LDM-2, LDM-3, and TryOffDiff). Each row shows a different garment from the dataset and the corresponding outputs for the various models. This comparison helps assess the impact of different model architectures and hyperparameters on garment reconstruction quality.", "section": "6.1 Impact of TryOffDiff configurations"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/ablations/gt.jpg", "caption": "(b) PixelModel", "description": "This figure shows the qualitative results of the PixelModel, a diffusion model operating in pixel space, from the ablation study.  It displays generated garment images alongside the corresponding ground truth images.  The model attempts to reconstruct the garments from a given input, illustrating its performance in capturing fine details and overall reconstruction accuracy. The results show the model has improved pixel-level details but suffers from slow inference.", "section": "6.1 Impact of TryOffDiff configurations"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/fig_guidance.png", "caption": "(c) LDM-1", "description": "This figure shows qualitative results for an ablation study comparing different model configurations for the Virtual Try-Off (VTOFF) task.  Specifically, it presents the results of using a Latent Diffusion Model (LDM) with Stable Diffusion v3 and SigLIP-based image features.  The image displays generated garment images alongside the corresponding ground truth images, allowing for a visual comparison of the model's performance under this specific configuration.", "section": "6. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/fig_step.png", "caption": "(d) LDM-2", "description": "This figure shows qualitative results for the LDM-2 model, one of the ablation studies comparing different model architectures.  LDM-2 is a latent diffusion model using a SigLIP-B/16 image encoder, a linear plus layer normalization adapter, and DDPM scheduler. The figure shows generated images and corresponding ground truth images of various garments, illustrating the model's performance in terms of garment reconstruction fidelity and detail preservation.", "section": "6. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/VTON-ablation-1.jpg", "caption": "(e) LDM-3", "description": "This figure shows the qualitative results of the LDM-3 model, one of the ablation study configurations.  LDM-3 is a Latent Diffusion Model using Stable Diffusion v3, the SigLIP-B/16 image encoder, and a linear layer with layer normalization for the adapter. The results display the model's performance on several different clothing items, showcasing generated images alongside the corresponding ground truth images. The visual comparison highlights the model's success in reconstructing the garment and its details, while also revealing its limitations and room for improvement.", "section": "6. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/VTON-ablation-2.jpg", "caption": "(f) TryOffDiff", "description": "This figure shows the results of the TryOffDiff model on a garment image. TryOffDiff is a novel model proposed in this paper to generate standardized garment images from single photos of clothed individuals. The figure showcases the high-fidelity garment reconstruction capabilities of TryOffDiff, accurately capturing garment shape, texture, and intricate patterns.", "section": "4. Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/2-pred_pose.jpg", "caption": "(g) Target", "description": "This figure shows the ground truth garment image corresponding to the reference image used as input in the virtual try-off task.  It displays the garment on a neutral background, in a standardized pose, showcasing the garment's true details and appearance without any variations from other images or stylistic choices.", "section": "4.4. Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/2-pred_visco.jpg", "caption": "Figure 8: Qualitative comparison between different configurations explored in our ablation study.\nSee also Table\u00a02 for more details.", "description": "This figure provides a qualitative comparison of the results obtained using various configurations explored in the ablation study described in the paper.  The ablation study investigated the impact of different components of TryOffDiff, including the image encoder, adapter design, and training methods. Each row presents results for a specific garment type. The columns represent different model configurations: (a) Autoencoder, (b) PixelModel, (c) LDM-1, (d) LDM-2, (e) LDM-3, (f) TryOffDiff, and (g) the ground truth.  By visually comparing the generated images across different configurations, the reader can assess the effectiveness of each component and the overall performance of the proposed TryOffDiff model. More quantitative details on these configurations are available in Table 2.", "section": "6. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/2-pred_ootd.jpg", "caption": "(a) Guidance Scale", "description": "This figure shows the impact of varying guidance scales on FID and DISTS scores.  The x-axis represents the guidance scale, while the y-axis shows the FID and DISTS scores. Different lines represent the FID and DISTS values at different guidance scales. The results reveal how the choice of guidance scale affects both the image quality (FID) and the structural and textural similarity to the ground truth (DISTS).  A proper balance needs to be found between the two, as very low guidance scales result in blurry images (poor FID and higher DISTS) and very high guidance scales make the images too similar to the reference images (good FID and higher DISTS).", "section": "6. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/2-pred_cat.jpg", "caption": "(b) Inference steps", "description": "The figure shows the impact of varying the number of inference steps on the FID and DISTS scores.  The experiment uses the TryOffDiff model with the DDIM noise scheduler. The x-axis represents the number of inference steps, while the y-axis shows the FID and DISTS scores.  The graph illustrates how the model's performance changes as the number of inference steps increases. This helps to determine an optimal balance between computational cost and image quality.", "section": "6. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/2-pred_tryoffdiff.png", "caption": "Figure 9: Ablation study on the impact of guidance scale\u00a0(s\ud835\udc60sitalic_s) and inference steps\u00a0(n\ud835\udc5bnitalic_n) on DISTS and FID scores. Experiments are conducted on VITON-HD-test with TryOffDiff using the DDIM\u00a0[41]\nnoise scheduler.", "description": "This ablation study analyzes how the guidance scale and the number of inference steps used in the TryOffDiff model affect the FID and DISTS scores.  The experiment uses the DDIM noise scheduler and is performed on the VITON-HD test dataset.", "section": "6. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/2-gt.png", "caption": "Figure 10: Qualitative results for different guidance. Left: no guidance applied (s=0\ud835\udc600s=0italic_s = 0). Middle: varying guidance scale (s\u2208[1.2,1.5,1.8,2.0,2.5,3.0,3.5]\ud835\udc601.21.51.82.02.53.03.5s\\in[1.2,1.5,1.8,2.0,2.5,3.0,3.5]italic_s \u2208 [ 1.2 , 1.5 , 1.8 , 2.0 , 2.5 , 3.0 , 3.5 ]). Right: ground-truth.", "description": "This figure displays the impact of different guidance scales on the generated garment images. The leftmost column shows the results without any guidance, demonstrating the baseline reconstruction performance. The center columns showcase results across a range of guidance scales, from 1.2 to 3.5, illustrating how increasing guidance scales affects image detail, realism, and adherence to the reference. The rightmost column displays the ground truth images for comparison, highlighting the quality of reconstruction achieved with different levels of guidance.", "section": "4.4. Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/whole_test_1.jpg", "caption": "Figure 11: Sample Variations. While minor variations in shape and pattern may occur with complex garments, the overall output of TryOffDiff demonstrates consistent garment reconstructions across multiple inference runs with different random seeds.", "description": "This figure shows the results of multiple inference runs using the TryOffDiff model on the same input image.  The goal is to demonstrate the model's consistency in generating garment reconstructions. While minor variations in shape and pattern might appear, particularly with complex garments, the overall output is highly consistent across multiple runs, showcasing the model's reliability despite different random seed initializations for each run.", "section": "Additional Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2411.18350/extracted/6017521/figures/suppl/whole_test_2.jpg", "caption": "Figure 12: Sample Variations. While minor variations in shape and pattern may occur with complex garments, the overall output of TryOffDiff demonstrates consistent garment reconstructions across multiple inference runs with different random seeds.", "description": "This figure shows the results of multiple inference runs of the TryOffDiff model on the same input image.  Despite using different random seeds for each run, the generated garment images exhibit high consistency in terms of overall shape and pattern. Minor variations are observed for complex garment designs but these are minimal and do not impact the overall reconstruction quality.  This consistency demonstrates the robustness and reliability of the TryOffDiff model.", "section": "8. Additional Qualitative Results"}]