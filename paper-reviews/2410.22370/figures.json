[{"figure_path": "https://arxiv.org/html/2410.22370/x1.png", "caption": "Figure 1: Prompt vs Inputs (Sec. 2.3): A visual summary of the distinction between prompts and inputs. A prompt is a user-guided interaction where the user asks the system to complete a task. Whereas the input is the piece of data, information, or content that the prompt is acting upon.", "description": "This figure illustrates the difference between a prompt and an input within the context of generative AI.  A prompt is a user instruction requesting the AI to perform a specific task.  The input, on the other hand, is the data or resource that the AI uses to fulfill the request made in the prompt. The example shown depicts an audio editing task. The prompt is the user's textual instructions, while the input is the actual audio file the instructions are applied to.", "section": "2 Background & Preliminaries"}, {"figure_path": "https://arxiv.org/html/2410.22370/x2.png", "caption": "Figure 2: Modalities: A high-level visual summary of the different modalities that generative AIs use (Sec.\u00a02.3).", "description": "Generative AI models can utilize different modalities for both input and output.  This figure provides a visual overview of the common modalities used in generative AI systems. It shows three main categories: Text (including natural language, data, and code), Visual (including images, videos, and visual interactions), and Sound (including audio and speech). Each category is further broken down into more specific examples. This visualization helps to understand the diverse ways that humans can interact with and receive information from generative AI systems.", "section": "2.3 Input Modalities"}, {"figure_path": "https://arxiv.org/html/2410.22370/x3.png", "caption": "Figure 3: Taxonomy of works by their input/output modalities.", "description": "This figure provides a comprehensive overview of the different generative AI systems and their capabilities based on the modalities they support for both input and output.  It presents a table where each row represents a specific generative AI system, and each column indicates the type of modality it handles (text, visual, or sound). A checkmark indicates the system's ability to process or generate data in that specific modality. This visualization helps understand the range of functionalities offered by different generative AI systems and their suitability for various applications.", "section": "2.3 Input Modalities"}, {"figure_path": "https://arxiv.org/html/2410.22370/x4.png", "caption": "Figure 4: User-Guided Interaction Taxonomy.\nGenerative AI systems and tools are summarized using the proposed user-guided interaction taxonomy (Sec.\u00a03).", "description": "Figure 4 is a table that categorizes various generative AI systems and tools based on the user-guided interaction taxonomy introduced in Section 3 of the paper.  The taxonomy breaks down user interactions into four main types: Prompting, Selection Techniques, System & Parameter Manipulation, and Object Manipulation & Transformation. Each row in the table represents a specific generative AI system or tool. Each column indicates whether that system supports a particular type of user interaction from the taxonomy. A checkmark indicates that the system supports the interaction. This visualization helps readers quickly understand the range of interaction methods used by different generative AI systems and how these methods are classified within the proposed taxonomy.", "section": "3 User-Guided Interactions"}, {"figure_path": "https://arxiv.org/html/2410.22370/x5.png", "caption": "(a) Text-based \u200bPrompt (\u00a7.\u200b3.1.1)", "description": "This figure shows an example of a text-based prompt interaction in generative AI. The user provides a natural language instruction to the system. In the example shown, the user asks the system to generate a story about a dog in space. The system's response is displayed below the prompt, showcasing text-based interaction as a method of prompting.", "section": "3.1 Prompting"}, {"figure_path": "https://arxiv.org/html/2410.22370/x6.png", "caption": "(b) Visual Prompts (\u00a7.3.1.2)", "description": "This figure shows an example of a visual prompt.  Visual prompts are user-guided interactions where users use visual communication, like images or gestures, to prompt the system to complete a certain task. The example in the figure shows a user providing an image of two puppies to the system as a prompt. This is a way to instruct the system to generate new content related to the image, such as a similar picture, descriptions of the picture, or a story about the puppies.", "section": "3.1 Prompting"}, {"figure_path": "https://arxiv.org/html/2410.22370/x7.png", "caption": "(c) Audio Prompts (\u00a7.3.1.3)", "description": "This figure shows an example of an audio prompt interaction within a generative AI system. The user provides an audio input, for example an audio clip of a piano intro, and then prompts the system to complete the audio using either text or audio prompts. The system's response, a finished song, is shown next to the prompt.", "section": "3.1 Prompting"}, {"figure_path": "https://arxiv.org/html/2410.22370/x8.png", "caption": "(d) Multi-Modal \u200bPrompts (\u00a7.\u200b3.1.4)", "description": "This figure shows an example of a multimodal prompt in a generative AI system.  Multimodal prompts combine different input modalities (text, visuals, audio) to guide the AI's generation process. In this particular example, the user might be providing a text description, a visual input (perhaps an image or sketch), and an audio clip to create a specific output.  The combination of inputs allows for richer and more nuanced instructions compared to using just a single modality.", "section": "3.1 Prompting"}, {"figure_path": "https://arxiv.org/html/2410.22370/x9.png", "caption": "Figure 5: Prompting Visual Summary (Sec. 3.1): An overview of the four main prompting subcategories. Prompting is a user-guided interaction where a user asks or \"prompts\" the generative AI system to complete a certain task.", "description": "This figure provides a visual summary of the four main prompting subcategories discussed in Section 3.1 of the paper.  These subcategories are: 1) Text-based prompts, where users type text instructions; 2) Visual prompts, where users provide visual input (like images) to guide the generation; 3) Audio prompts, where users provide audio input; and 4) Multi-modal prompts, combining elements of the previous three methods. The figure visually shows example user prompts and system responses for each type of prompting interaction, illustrating the diversity of ways users can guide generative AI systems towards completing a task.", "section": "3.1 Prompting"}, {"figure_path": "https://arxiv.org/html/2410.22370/x10.png", "caption": "(a) Single Selection", "description": "This figure shows an example of single selection in a generative AI system.  The user is given several options for a story title, and single selection allows the user to select just one of the choices to proceed further. This contrasts with multi-selection where several options could be chosen at once. This simple interaction highlights a key way a user can provide refined control to a generative system, allowing for iterative refinement.", "section": "Selection Techniques"}, {"figure_path": "https://arxiv.org/html/2410.22370/x11.png", "caption": "(b) Multi-Selection", "description": "In the context of generative AI systems, multi-selection involves choosing or highlighting multiple UI elements simultaneously to further interact with them.  This allows for more complex interactions, such as selecting multiple words to apply a uniform change (e.g., replace with synonyms) or selecting components from different outputs to create something new (e.g., combining elements from different dress designs to create a unique garment).  It contrasts with single-selection, where only one element is selected at a time.", "section": "3.2 Selection Techniques"}, {"figure_path": "https://arxiv.org/html/2410.22370/x12.png", "caption": "(c) Lasso and Brush Selection", "description": "This figure shows an example of lasso and brush selection in a generative AI system.  Lasso and brush selection techniques allow for the precise selection of parts of a larger element (e.g., an image or a document), giving the user finer control over how the generative model processes that content.  The user can use a brush tool or lasso tool to select a specific area to manipulate or apply specific parameters.  In this case, a brush is used to select parts of an image to add a hat to, enabling a specific editing task only to the selected section.", "section": "3.2 Selection Techniques"}, {"figure_path": "https://arxiv.org/html/2410.22370/x13.png", "caption": "Figure 6: Selection Techniques (Sec. 3.2): Selecting, in terms of generative AI systems, consists of choosing or highlighting a specific UI element in order to further interact with it.", "description": "This figure illustrates the concept of selection techniques in generative AI user interfaces.  Selecting, in the context of generative AI, involves choosing or highlighting a specific UI element (a button, an image, text, etc.) to trigger further interaction with the system. The figure showcases three examples: single selection, where a single element is chosen; multi-selection, where multiple elements are chosen; and lasso/brush selection, where a region is selected using lasso or brush tools. This highlights how users can directly manipulate UI elements to guide the generative AI's output, providing a more precise and controlled interaction compared to simply providing textual prompts.", "section": "3.2 Selection Techniques"}, {"figure_path": "https://arxiv.org/html/2410.22370/x14.png", "caption": "(a) Menus", "description": "This figure shows an example of a menu UI element in a generative AI system.  Menus allow users to select from preset options or input their own parameters to modify the generative process.  The menu in the figure presents different choices, presumably to change certain aspects of the generated output. The various options suggest that the AI system offers customizable features.", "section": "3.3 System and Parameter Manipulation"}, {"figure_path": "https://arxiv.org/html/2410.22370/x15.png", "caption": "(b) Sliders", "description": "This figure shows how sliders can be used to adjust the parameters of a generative AI system. Sliders are visual UI elements that allow for the manipulation of parameters by adjusting their values.  The example in the figure likely displays a slider that controls some aspect of a generative model, perhaps influencing a visual output, the settings for a text generation, or parameters in an audio editor. The specific parameter being adjusted by the slider is not explicitly stated in the caption.", "section": "3.3 System and Parameter Manipulation"}, {"figure_path": "https://arxiv.org/html/2410.22370/x16.png", "caption": "(c) Explicit Feedback", "description": "This figure shows an example of explicit feedback in the context of generative AI systems.  Explicit feedback involves users directly communicating their satisfaction or dissatisfaction with a generated output. This is not implicit feedback where the system infers user satisfaction or dissatisfaction based on indirect cues.  The example shows a user providing textual feedback to critique the AI's response and suggest improvements for future interactions. The user's feedback is explicitly communicated to the system.", "section": "3.3 System and Parameter Manipulation"}, {"figure_path": "https://arxiv.org/html/2410.22370/x17.png", "caption": "Figure 7: System and Parameter Manipulation (Sec. 3.3): User interaction techniques that allow the user to adjust the parameters, settings, or functions of an overall generative AI system.", "description": "This figure illustrates three types of user interaction techniques that allow users to modify the parameters, settings, or functions of a generative AI system.  These techniques are:\n\n1. **Menus:**  Users select options from menus (dropdowns, etc.) to alter settings or parameters. The example shows a revenue graph with menus for selecting different metrics (total revenue, tone, mood, language, time period) to be displayed.\n2. **Sliders:**  Users adjust sliders to control parameters and settings.  The example showcases how sliders can be used to control values like range and increments of a revenue graph.\n3. **Explicit Feedback:** Users provide direct feedback (thumbs up/down, written critiques, etc.) to fine-tune the system's behavior. The example shows a user providing feedback about the information shown in the system's response to a query.", "section": "3.3 System and Parameter Manipulation"}, {"figure_path": "https://arxiv.org/html/2410.22370/x18.png", "caption": "(a) Drag and Drop", "description": "This figure shows an example of a drag-and-drop interaction within a generative AI system. Drag-and-drop interactions allow users to directly manipulate UI elements by dragging them to a specific location or another element. This manipulation can trigger actions within the system, such as creating or connecting elements, altering parameters, or prompting the system to perform a task.  The example illustrates how the user might combine prompts by dragging and dropping them onto each other. This specific example is from the Object Manipulation and Transformation section of the paper.", "section": "Object Manipulation and Transformation"}, {"figure_path": "https://arxiv.org/html/2410.22370/x19.png", "caption": "(b) Connecting", "description": "This figure shows an example of connecting UI elements within a generative AI system.  Users can combine UI elements that represent different system instructions (or parts of prompts) by connecting them visually. This process creates a combined prompt or instruction by combining the individual components.  In the example shown, UI elements containing parts of a prompt are connected. The system understands the combined meaning of these connected elements, resulting in a combined prompt such as, \u201cCreate a poem about a spaceship set in the modern age\u201d. This technique facilitates prompt creation by enabling users to combine modular units of instructions rather than writing a complete prompt from scratch.", "section": "3.4 Object Manipulation and Transformation"}, {"figure_path": "https://arxiv.org/html/2410.22370/x20.png", "caption": "(c) Resizing", "description": "This figure shows an example of the object manipulation and transformation interaction technique, specifically resizing. The user is shown to be able to resize an object in the system. Resizing an object changes the size of that object, and depending on the generative AI system that is used, can change the object's function.", "section": "3.4 Object Manipulation and Transformation"}, {"figure_path": "https://arxiv.org/html/2410.22370/x21.png", "caption": "Figure 8:  Object Manipulation and Transformation (Sec. 3.4): User interaction techniques that modify, adjust, and/or transform a specific UI element, like a building block, puzzle piece, or similar entity.", "description": "Figure 8 shows three types of user interaction techniques in Generative AI systems that involve directly manipulating visual UI elements.  These techniques allow users to modify, adjust, or transform a specific element.  The examples shown illustrate: (a) Drag and Drop: moving an element to a new position or using it to modify the system's generative process. (b) Connecting: linking UI elements together to create a composite input or prompt. (c) Resizing: changing the size of an element to alter its effects on the system.  These interactions are useful for giving users a more nuanced control over the generative process.", "section": "3.4 Object Manipulation and Transformation"}, {"figure_path": "https://arxiv.org/html/2410.22370/x22.png", "caption": "Figure 9: Conversational UI: A conversational UI is structured so that a user interacts with the user prompt/input box. From there, their output(s) and output history exist in a larger space within the UI (Sec.\u00a04.1).", "description": "This figure illustrates the structure of a conversational user interface (UI) in generative AI applications.  It shows how the UI is designed to mimic a human conversation. The user interacts with a designated prompt/input box, where they enter their queries or instructions.  The system's responses and the history of the entire conversation are then displayed in a larger area within the UI, making it easy for the user to follow the interaction flow and refer to previous exchanges. This structure facilitates a turn-based conversation between the user and the AI.", "section": "4 User Interface Layouts for Generative AI"}, {"figure_path": "https://arxiv.org/html/2410.22370/x23.png", "caption": "Figure 10: Canvas User Interface: A UI structure with a central canvas area that houses the primary content. The generative and other tools are often in the periphery or off to the side. (Sec.\u00a04.2).", "description": "This figure illustrates the layout of a Canvas User Interface, a common design pattern for generative AI applications.  The core element is a large central canvas area where the primary generated content (e.g., an image, a text, a video) is displayed.  Surrounding this canvas, in the periphery, are various tools and controls related to the generative process.  These peripheral elements might include options for adjusting parameters, selecting from different styles, adding new elements, modifying the generated content, and so on.  This arrangement keeps the focus on the main generated content, making it easy for users to view and interact with the generated output while providing convenient access to tools that enable adjustments and modifications.", "section": "4 User Interface Layouts for Generative AI"}]