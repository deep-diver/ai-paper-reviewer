[{"content": "|                     |             | phi-4 14b | phi-3 14b | Qwen 2.5 14b instruct | GPT 4o-mini | Llama-3.3 70b instruct | Qwen 2.5 72b instruct | GPT 4o | \n| :------------------ | :---------- | :--------: | :--------: | :---------------------: | :----------: | :---------------------: | :---------------------: | :------: | \n| <img src=\"https://arxiv.org/html/2412.08905/simple-evals.png\" width=8.0pt height=46.9pt> | **simple-evals** |             |             |             |             |             |             |             | \n| MMLU                |             |   84.8     |   77.9     |      79.9            |    81.8     |      86.3            |      85.3            |  **88.1** | \n| GPQA                 |             |  **56.1**  |   31.2     |      42.9            |    40.9     |      49.1            |      49.0            |   50.6   | \n| MATH                 |             |  **80.4**  |   44.6     |      75.6            |    73.0     |      66.3            |      80.0            |   74.6   | \n| HumanEval            |             |   82.6     |   67.8     |      72.1            |    86.2     |      78.9            |      80.4            |  **90.6** | \n| MGSM                 |             |   80.6     |   53.5     |      79.6            |    86.5     |      89.1            |      87.3            |  **90.4** | \n| SimpleQA             |             |    3.0     |    7.6     |       5.4             |     9.9     |      20.9            |      10.2            |  **39.4** | \n| DROP                 |             |   75.5     |   68.3     |      85.5            |    79.3     |     **90.2**         |      76.7            |   80.9   | \n| MMLUPro              |             |   70.4     |   51.3     |      63.2            |    63.4     |      64.4            |      69.6            |  **73.0** | \n| HumanEval+           |             |   82.8     |   69.2     |      79.1            |    82.0     |      77.9            |      78.4            |  **88.0** | \n| ArenaHard            |             |   75.4     |   45.8     |      70.2            |    76.2     |      65.5            |     **78.4**         |   75.6   | \n| LiveBench            |             |   47.6     |   28.1     |      46.6            |    48.1     |     **57.6**         |      55.3            |  **57.6** | \n| IFEval               |             |   63.0     |   57.9     |      78.7            |    80.0     |     **89.3**         |      85.0            |   84.8   | \n| PhiBench (internal) |             |   56.2     |   43.9     |      49.8            |    58.7     |      57.1            |      64.6            |  **72.4** | ", "caption": "Table 1: Performance of phi-4\u00a0 on a set of standard benchmarks. The first set of benchmarks uses OpenAI\u2019s simple-evals framework\u00a0[24], specifying the prompts/extraction/temperature=0.5. We compare to small models of similar inference cost, as well as to larger models.", "description": "This table presents the performance of the phi-4 language model on a range of standard benchmarks, comparing it against other models with similar computational cost.  The benchmarks include tasks assessing various language capabilities, such as question answering, reasoning, and code generation.  The evaluation methodology uses OpenAI's simple-evals framework, and the results are reported with specific prompt engineering and temperature settings.", "section": "1 Introduction"}, {"content": "| Model | MMLU | MMLU pro | GSM8k | Human-Eval | ARCC | MBPP | MATH | TQA |\n|---|---|---|---|---|---|---|---|---|\n| phi-4 (4k) | +3.0 | +10.3 | +2.2 | +7.8 | +1.1 | +6.8 | +8.9 | -0.7 |\n| phi-4 (16k) | +2.7 | +8.9 | +1.2 | +9.0 | +0.9 | +9.6 | +8.4 | -1.5 |", "caption": "Table 2: Pretraining benchmarks for phi-4\u00a0compared to its predecessor, phi-3-medium after pretraining.", "description": "This table presents a comparison of the phi-4 model's performance against its predecessor, phi-3-medium, across various pretraining benchmarks.  The improvements shown highlight the effectiveness of the changes made in phi-4's training process, particularly concerning data quality and curriculum.", "section": "3 Pretraining details"}, {"content": "|                   | MMLU | MMLU pro | GSM8k | Human-Eval | ARCC | MBPP | MATH | TQA |\n|-------------------|-------|-----------|--------|-------------|-------|-------|-------|------|\n| Synthetic          | +0.8  | +4.0      | +2.2   | +12.1       | 0.0   | +5.0  | +4.9  | -14.8 |\n| Synthetic + Web Rewrites | +0.3  | +4.1      | +1.8   | +13.3       | +3.0  | +7.6  | +8.1  | -7.7 |", "caption": "Table 3: Benchmark performance of 13131313B models (used for ablations only) trained on data mixtures containing no web data. The respective training tokens are either from synthetic sources, or an equal share of synthetic data and web rewrites. All numbers are reported relative to the performance of phi-3-medium, which has seen a combination of web and synthetic data.", "description": "This ablation study compares the performance of 13B parameter models trained exclusively on synthetic data versus models trained on a mix of synthetic and \"web rewrite\" data.  The results are presented as a relative improvement or decline compared to a baseline phi-3-medium model, which utilized both synthetic and web data during training.  The goal is to assess the impact of different data sources on model capabilities and guide the selection of optimal data mixtures for future model development.", "section": "3.2 Data Mixture"}, {"content": "|                | MMLU   | MATH   | GSM8k   | Human-Eval | ARCC   | MBPP   | TQA    | MMLU pro | Average |\n|----------------|--------|--------|---------|------------|--------|--------|--------|----------|---------|\n| Uniform         | -3.3   | -5.4   | -5.8    | -1.2       | +0.6   | -2.0   | +3.3   | -3.6     | -2.2    |\n| S               | +3.3   | +4.0   | +2.1    | -6.1       | +1.9   | +0.4   | -3.0   | +3.7     | +0.8    |\n| S + WR          | +0.6   | +1.2   | +1.5    | -1.2       | +1.6   | +1.6   | -3.7   | +1.2     | +0.4    |\n| S + W           | -0.6   | -0.7   | -0.7    | -4.3       | +0.3   | -2.0   | +6.9   | +0.9     | 0.0     |", "caption": "Table 4: Ablations on the allocation of 75%percent7575\\%75 % of training tokens to synthetic (S), filtered web (W), and web rewrite (WR) categories, while other data sources are held constant in the remaining 25%percent2525\\%25 % token budget. All benchmark numbers are measured relative to the final data mixture used for training phi-4.", "description": "This table presents ablation studies on the data mixture used for training the phi-4 model.  The experiments focus on varying the proportions of synthetic data (S), filtered web data (W), and web rewrite data (WR), while keeping the remaining data sources constant. The results are shown as performance differences relative to the final data mixture used in training phi-4, for various benchmark tasks. This allows researchers to understand the relative contributions of different data types to the model's overall performance and to optimize the data mixture for better results.", "section": "3.2 Data Mixture"}, {"content": "| Data | Fraction of Training | Unique Token Count | Number of Epochs |\n|---|---|---|---| \n| Web | 15% | 1.3T | 1.2 |\n| Web rewrites | 15% | 290B | 5.2 |\n| Synthetic | 40% | 290B | 13.8 |\n| Code data | 20% | 820B | 2.4 |\n| Acquired sources | 10% | 580B | 1.7 |", "caption": "Table 5: Data mixture for pretraining.", "description": "This table details the composition of the data used for pretraining the phi-4 language model.  It breaks down the proportion of the training data coming from different sources: web data, web rewrites (synthetic data generated based on web content), synthetic data, code data, and acquired sources (e.g., academic data, books).  The table shows the percentage of total training tokens from each source, the count of unique tokens in each data subset, and the number of epochs the model trained on each data source.", "section": "3 Pretraining details"}, {"content": "| Model | Max Length | Recall | RAG | ICL | Re-rank | QA | Summ |\n|---|---|---|---|---|---|---|---| \n| phi-4 | 8K | 100.0 | 58.1 | 68.0 | 65.3 | 26.7 | 38.3 |\n| Qwen-2.5-14B | 8K | 100.0 | 62.2 | 67.8 | 58.2 | 24.7 | 37.2 |\n| Llama-3.3-70B | 8K | 92.0 | 65.3 | 69.4 | 64.4 | 30.0 | 37.8 |\n| GPT-4o-mini | 8K | 99.2 | 65.8 | 74.4 | 69.4 | 31.3 | 38.5 |\n| GPT-4o | 8K | 100.0 | 66.9 | 83.0 | 75.1 | 37.3 | 43.0 |\n| phi-4 | 16K | 99.0 | 57.1 | 77.0 | 54.4 | 36.0 | 40.5 |\n| Qwen-2.5-14B | 16K | 100.0 | 59.1 | 67.6 | 50.3 | 29.7 | 42.3 |\n| Llama-3.3-70B | 16K | 92.0 | 62.2 | 70.0 | 63.3 | 36.7 | 41.9 |\n| GPT-4o-mini | 16K | 100.0 | 63.6 | 78.4 | 63.9 | 36.0 | 45.2 |\n| GPT-4o | 16K | 100.0 | 66.7 | 85.6 | 73.8 | 43.7 | 46.3 |", "caption": "Table 6: Evaluation results on the long-context benchmark HELMET\u00a0[35].", "description": "Table 6 presents a comparison of various large language models' performance on the HELMET benchmark, which evaluates long-context capabilities.  The models are evaluated across multiple tasks including recall, retrieval augmented generation (RAG), in-context learning (ICL), re-ranking, question answering (QA), and summarization.  The table shows the performance of each model on each task, using relevant metrics like SubEM, nDCG@10, F1, and GPT-4 scores.  Different maximum context lengths (8K and 16K) are tested to assess the models' performance in handling different context window sizes.", "section": "6 Performance on Key Benchmarks"}, {"content": "| Dataset Name | Sample Count |\n|---|---| \n| unknown + safety data | 3,000 |\n| generic multiple-choice Q&A | 132,859 |\n| math data | 76,552 |\n| python data | 16,080 |\n| cpp, go, java, js, rust data | 21,806 |", "caption": "Table 9: Performance through the post-training process. DPO stage 1 is pivotal token DPO, and DPO stage 2 is more standard judge-guided DPO. Each also has 1-5% hallucination and safety data mixed in.", "description": "Table 9 presents the performance of the phi-4 model on various benchmark tasks at different stages of post-training.  The model was initially fine-tuned using supervised fine-tuning (SFT).  Then, two stages of direct preference optimization (DPO) were applied. The first DPO stage utilized the novel pivotal token method, while the second stage employed a more standard judge-guided approach.  Each SFT and DPO stage included 1-5% of data focused on mitigating hallucinations and improving model safety. The table shows the performance improvement across each stage for multiple benchmarks including MMLU, GPQA, MATH, and HumanEval.", "section": "4 Post-Training"}, {"content": "| Dataset Name | Sample Count |\n|---|---| \n| unknown + safety data | 43,842 |\n| any vs any overall | 266,000 |\n| any vs any accuracy | 532,000 |", "caption": "Table 10: Performance comparison across models. Lower scores are better, except for \u201cGrounding,\u201d where a higher score is better. phi-4\u00a0 values are bold for readability.", "description": "Table 10 presents a quantitative comparison of phi-4's performance against several other large language models across various safety and robustness benchmarks. The metrics employed assess grounding (higher scores are better), harmful content generation, and jailbreaking attempts (lower scores are better). This comparison highlights phi-4's relative strengths and weaknesses in safety and responsible AI (RAI) aspects compared to its peers.", "section": "7 Safety"}]