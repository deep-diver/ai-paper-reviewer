[{"content": "| Method | Keys | Clean | Rotate | JPEG | C&S | Blur | Noise | Bright | Avg \u2191 |\n|---|---|---|---|---|---|---|---|---|---| \n| Tree-Ring | 32 | 0.790 | 0.020 | 0.420 | 0.040 | 0.610 | 0.530 | 0.420 | 0.404 |\n|  | 128 | 0.450 | 0.010 | 0.120 | 0.020 | 0.280 | 0.230 | 0.170 | 0.183 |\n|  | 2048 | 0.200 | 0.000 | 0.040 | 0.000 | 0.090 | 0.070 | 0.060 | 0.066 |\n| RingID | 32 | **1.000** | **1.000** | **1.000** | 0.530 | 0.990 | **1.000** | 0.960 | 0.926 |\n|  | 128 | **1.000** | 0.980 | **1.000** | 0.280 | 0.980 | **1.000** | 0.940 | 0.883 |\n|  | 2048 | **1.000** | 0.860 | **1.000** | 0.080 | 0.970 | 0.950 | 0.870 | 0.819 |\n| WIND<sub>fast<sub>128</sub></sub> | 100000 | **1.000** | 0.780 | **1.000** | 0.470 | **1.000** | **1.000** | 0.960 | 0.887 |\n| WIND<sub>fast<sub>2048</sub></sub> | 100000 | **1.000** | 0.870 | 0.960 | 0.060 | 0.960 | 0.950 | 0.900 | 0.814 |\n| WIND<sub>full<sub>128</sub></sub> | 100000 | **1.000** | 0.780 | **1.000** | 0.850 | **1.000** | **1.000** | **1.000** | 0.947 |\n| WIND<sub>full<sub>2048</sub></sub> | 100000 | **1.000** | 0.880 | **1.000** | **0.930** | **1.000** | 0.990 | 0.980 | **0.969** |", "caption": "Table 1: Comparison of correct watermark detection accuracy between WIND and previous image watermarking approaches under various image transformation attacks. WINDMsubscriptWIND\ud835\udc40\\text{WIND}_{M}WIND start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT denotes the use of M\ud835\udc40Mitalic_M groups, with the total number of noises (N\ud835\udc41Nitalic_N) specified in the \u201cKeys\u201d column. A broader comparison with additional methods can be found in Table\u00a016.", "description": "This table compares the accuracy of watermark detection for the proposed method (WIND) and two existing methods (Tree-Ring and RingID) across various image transformations (clean, rotation, JPEG compression, cropping and scaling, blurring, noise addition, brightness adjustment).  The accuracy is evaluated for different numbers of initial noises used for the WIND method, denoted as WINDM (where M is the number of groups of noises).  The table highlights the robustness of each method against common attacks that aim to remove or distort the watermark.", "section": "Experiments"}, {"content": "| Condition | Mean | STD |\n|---|---|---|\n| Original Image | 0.888 | 0.053 |\n| Attacked Image | 0.824 | 0.062 |\n| Unrelated Image | 0.000 | 0.008 |", "caption": "Table 4: Cosine similarity between the first initial noise used for generation and the inversed noise obtained through three inversion approaches. \u201cPrivate\u201d refers to models owner\u2019s model, while \u201cPublic\u201d denotes external model.", "description": "This table presents a comparison of cosine similarity scores between the initial noise used during image generation and the reconstructed noise obtained using different inversion methods.  Three scenarios are compared: 1) using the model owner's private model for both generation and inversion; 2) using the owner's private model for generation and a public model for inversion; and 3) comparing the initial noise against completely random noise. This comparison helps demonstrate the robustness of using initial noise as a distortion-free watermark, highlighting its resilience to attacks that might attempt to reconstruct the noise using external models.", "section": "2.2 Diffusion Models Inversion"}, {"content": "| Method | FID \u2193 |\n|---|---| \n| DwtDctSvd | 25.01 |\n| RivaGAN | 24.51 |\n| Tree-Ring | 25.93 |\n| RingID | 26.13 |\n| WIND | **24.33** |", "caption": "Table 5: Notations used in the paper.", "description": "This table lists the notations used throughout the paper.  For each notation, it provides a short description to clarify its meaning within the context of the paper's mathematical and algorithmic descriptions.", "section": "A Notation"}, {"content": "| Approach | Mean | Std |\n|---|---|---|\n| Gen (private) \u2192 Rev (private) | 0.888 | 0.053 |\n| Gen (private) \u2192 Rev (public) \u2192 Gen (public) \u2192 Rev (private) | 0.166 | 0.063 |\n| Random Noise | 0.000 | 0.053 |", "caption": "Table 6: Inpainting correct watermark detection accuracy.", "description": "This table presents the accuracy of watermark detection after applying various image transformations (Clean, Rotate, JPEG, C&S, Blur, Noise, Bright) to images that have been watermarked using the inpainting method.  The accuracy is measured as the percentage of correctly identified watermarked images after each transformation.", "section": "5 Experiments"}, {"content": "| Before | After | Before | After |\n|---|---|---|---|\n| ![Refer to caption](https://arxiv.org/html/2412.04653/org1.png) | ![Refer to caption](https://arxiv.org/html/2412.04653/wm1.png) | ![Refer to caption](https://arxiv.org/html/2412.04653/org2.png) | ![Refer to caption](https://arxiv.org/html/2412.04653/wm2.png) |\n| ![Refer to caption](https://arxiv.org/html/2412.04653/org3.png) | ![Refer to caption](https://arxiv.org/html/2412.04653/wm3.png) | ![Refer to caption](https://arxiv.org/html/2412.04653/org4.png) | ![Refer to caption](https://arxiv.org/html/2412.04653/wm4.png) |", "caption": "Table 7: Impact of different inference steps on detection accuracy.", "description": "This table presents the results of an experiment evaluating the effect of varying the number of inference steps during the image generation and reconstruction processes on the accuracy of watermark detection.  Different numbers of steps were tested (20, 50, 100, 200), and the detection accuracy is reported for several image transformation attacks (Clean, Rotate, JPEG, C&S, Blur, Noise, Bright) to assess the robustness of the watermarking method under different conditions.", "section": "5 Experiments"}, {"content": "| Parameter | Description |\n|---|---| \n| N | Number of initial noises |\n| M | Number groups |\n| s | Secret salt for cryptographic security |\n| i | Index of initial noise: i\u2208[N] |\n| g | Index of group: g=i%M |\n| hash | A cryptographic hash function |\n| **z** | Initial noise |\n| \u03c4 | Threshold for declaring an image is watermarked |\n| T | Number of diffusion steps |\n| \u0398 | Weights of a diffusion model |\n| p | Text prompt for diffusion |\n| G<sub>\u0398</sub> | Diffusion model with weights \u0398 |\n| G<sup>-1</sup><sub>\u0398</sub> | Inverse diffusion model with weights \u0398 |", "caption": "Table 8: Error bars of WIND.", "description": "This table presents the error bars, specifically the Area Under the Curve (AUC) and True Positive Rate at 1% False Positive Rate (TPR@1%FPR), for the WIND watermarking method.  These metrics provide a more comprehensive understanding of the accuracy and reliability of the WIND method in detecting watermarks, showing the variability and range of its performance.", "section": "Experiments"}, {"content": "| Clean | Rotate | JPEG | C&S | Blur | Noise | Bright | Avg \u2191 |\n|---|---|---|---|---|---|---|---| \n| 1.000 | 1.000 | 1.000 | 0.880 | 1.000 | 0.950 | 0.950 | 0.969 |", "caption": "Table 9: Success rate of additional attacks.", "description": "This table presents the success rates of several additional attacks against the proposed watermarking method, WIND.  These attacks are designed to test the robustness of WIND beyond the standard image transformation attacks.  The attacks included are WeVade, Random Search, Transfer Attack, and NES Query. Each attack aims to remove or forge the watermark in different ways. The results show the percentage of successful attacks for each method.", "section": "5 Experiments"}, {"content": "| Steps | Clean | Rotate | JPEG | C&S | Blur | Noise | Bright | Avg \u2191 |\n|---|---|---|---|---|---|---|---|---|\n| 20 | **1.000** | 0.780 | **1.000** | 0.880 | 0.920 | **1.000** | 0.960 | 0.934 |\n| 50 | **1.000** | **0.930** | **1.000** | **0.940** | **1.000** | 0.980 | 0.980 | 0.976 |\n| 100 | **1.000** | **0.930** | **1.000** | **0.940** | **1.000** | **1.000** | 0.990 | **0.980** |\n| 200 | **1.000** | 0.850 | **1.000** | **0.940** | **1.000** | **1.000** | **1.000** | 0.970 |", "caption": "Table 10: Effect of WIND on CLIP score.", "description": "This table presents the CLIP (Contrastive Language\u2013Image Pre-training) scores for images before and after applying the WIND watermarking technique.  The CLIP score measures the alignment between an image and its textual description, indicating the image's quality and semantic consistency.  By comparing the scores before and after watermarking, we can assess the impact of the watermark on the image's perceptual quality and semantic meaning. A negligible difference suggests that WIND's watermarking process does not significantly affect image quality.", "section": "Experiments"}, {"content": "| AUC | TP@1% |\n|---|---| \n| 0.971 | 1.000 |", "caption": "Table 11: SSIM and PSNR values of initial noise-based watermarking approaches. WINDw/ow/o{}_{\\text{w/o}}start_FLOATSUBSCRIPT w/o end_FLOATSUBSCRIPT refers to the method without group identifiers", "description": "This table presents a comparison of the structural similarity index (SSIM) and peak signal-to-noise ratio (PSNR) values for several different image watermarking techniques.  It shows how well each method preserves the quality of the watermarked images compared to the originals.  The methods compared are WIND (with and without group identifiers), Tree-Ring, RingID, RivaGAN, SSL, and StegaStamp.  SSIM and PSNR are common metrics for evaluating the perceptual quality and distortion of an image. Higher values for both metrics generally indicate better image quality after watermarking.", "section": "Additional Results"}, {"content": "| Method | WeVade | Random Search | Transfer Attack | NES Query |\n|---|---|---|---|---|\n| 1% | 2% | 3% | 2% |", "caption": "Table 12: Detection time (second)", "description": "This table presents the time taken for watermark detection in seconds using three different methods: WIND, Tree-Ring, and RingID.  The detection times are compared to show the relative computational efficiency of each watermarking technique.", "section": "5 Experiments"}, {"content": "| CLIP Before Watermark | CLIP After Watermark |\n|---|---| \n| 0.366 | 0.360 |", "caption": "Table 13: Accuracy of retrieving the initial noise from 10,000 noise samples, divided into varying numbers of groups, under different image transformation attacks.", "description": "This table presents the accuracy of retrieving the initial noise from a set of 10,000 samples.  The 10,000 samples were divided into different numbers of groups (32, 128, 512, and 2048), and each group was subjected to various image transformation attacks (clean, rotate, JPEG compression, cropping and scaling, blur, noise addition, and brightness adjustment). The results demonstrate how the number of groups affects the accuracy of retrieving the original noise after applying these transformations.", "section": "Experiments"}, {"content": "| WIND | Tree-Ring | RingID |\n|---|---|---|\n| 22 | 20 | 14 |", "caption": "Table 14: Correct watermark detection after iterative regeneration attack.", "description": "This table presents the results of a robustness test against iterative regeneration attacks.  The experiment involved repeatedly applying a regeneration attack to watermarked images and then measuring the accuracy of the watermark detection system. The table shows the cosine similarity between the original and reconstructed noise after each iteration of the attack and the detection rate (percentage of correctly identified watermarks). This demonstrates the resilience of the watermarking method against this specific type of attack.", "section": "5 Experiments"}, {"content": "| Groups | Clean | Rotate | JPEG | C&S | Blur | Noise | Bright | Avg \u2191 |\n|---|---|---|---|---|---|---|---|---|\n| 32 | **1.000** | 0.540 | **1.000** | 0.700 | **1.000** | 0.990 | 0.960 | 0.884 |\n| 128 | **1.000** | 0.810 | **1.000** | 0.820 | **1.000** | **1.000** | 0.980 | 0.944 |\n| 512 | **1.000** | 0.890 | **1.000** | 0.880 | **1.000** | 0.980 | **1.000** | 0.964 |\n| 2048 | **1.000** | **0.930** | **1.000** | **0.940** | **1.000** | 0.980 | 0.980 | **0.976** |", "caption": "Table 15: SSIM and PSNR values for non-synthetic image watermarking approaches.", "description": "This table presents a comparison of the Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR) values achieved by different watermarking methods when applied to non-synthetic images.  SSIM and PSNR are common metrics used to assess the quality of an image after a watermark is added. Higher values indicate better image quality, meaning the watermarking method has less impact on the original image's appearance. The table allows for a quantitative comparison of the trade-off between watermarking robustness and perceptual quality across several methods.", "section": "Additional Results"}, {"content": "| Iteration | Cosine Similarity | Detection Rate |\n|---|---|---|\n| 10 | 0.493 | 100% |\n| 20 | 0.342 | 100% |\n| 30 | 0.243 | 100% |\n| 40 | 0.170 | 100% |\n| 50 | 0.121 | 100% |", "caption": "Table 16: Comparison of correct watermark detection accuracy between WIND and previous image watermarking approaches under various image transformation attacks. WINDMsubscriptWIND\ud835\udc40\\text{WIND}_{M}WIND start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT denotes the use of M\ud835\udc40Mitalic_M groups, with the total number of noises (N\ud835\udc41Nitalic_N) specified in the \u201cKeys\u201d column.", "description": "This table compares the accuracy of watermark detection for the WIND method against several existing image watermarking techniques.  The comparison is done across a range of common image manipulations (transformations), including rotation, JPEG compression, cropping and scaling, blurring, adding noise, and changing brightness.  The accuracy of watermark detection is presented as an average across these various attacks. The WIND method is tested with different numbers of initial noise groups (M) and a total number of noises (N) to demonstrate the impact of these parameters on robustness.  Results show how well each watermarking technique preserves the watermark integrity under different attacks.", "section": "5 Experiments"}]