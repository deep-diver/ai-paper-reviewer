{"importance": "This paper is crucial for researchers in LMM interpretability.  It introduces a novel framework for understanding and controlling internal representations of LMMs, thereby addressing a critical challenge in the field. Its findings regarding emotion features and methods to correct for hallucinations have significant implications for future LMM development and safety, opening new research avenues for increased transparency and reliability.", "summary": "Large multimodal models' inner workings are demystified using a novel framework that identifies, interprets, and even steers their internal features, opening the door to safer, more reliable AI.", "takeaways": ["A new framework is presented for interpreting open-semantic features within large multimodal models (LMMs).", "Sparse Autoencoders (SAEs) are employed to disentangle LMM representations and large language models (LLMs) are used to interpret them.", "The framework enables steering LMM behavior by manipulating these features, demonstrating potential for rectifying issues like hallucinations."], "tldr": "Large multimodal models (LMMs) are powerful but opaque.  Understanding their internal representations is crucial for improving their reliability and safety, but their high dimensionality and polysemantic nature pose significant challenges. Existing methods struggle to effectively decompose complex neural representations into interpretable components, limiting our understanding of LMM decision-making. Furthermore, directly analyzing the vast number of features in LMMs is impractical.\nThis research introduces a novel framework to address these issues.  **Sparse Autoencoders (SAEs)** are used to disentangle LMM representations, generating more human-understandable features.  **A unique pipeline leverages another LLM to automatically interpret these features**, identifying their meanings and enabling the influence of LMM behavior. This novel framework contributes to a deeper understanding of LMMs, offering insights into their decision-making processes and potential methods to steer behavior. Experimental results showcase the effectiveness in analyzing and correcting LMM responses, particularly regarding emotionally-charged outputs and hallucinations.", "affiliation": "NTU, Singapore", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2411.14982/podcast.wav"}