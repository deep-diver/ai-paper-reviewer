[{"figure_path": "https://arxiv.org/html/2411.14982/x2.png", "caption": "Figure 1: a) The Sparse Autoencoder (SAE) is trained on LLaVA-NeXT data by integrating it into a specific layer of the model, with all other components frozen. b) The features learned by the SAE are subsequently interpreted through the proposed auto-explanation pipeline, which analyzes the visual features based on their activation regions. c) It is demonstrated that these features can be employed to steer the model\u2019s behavior by clamping them to high values.", "description": "Figure 1 illustrates the process of interpreting and controlling the internal representations of a Large Multimodal Model (LMM).  (a) shows how a Sparse Autoencoder (SAE) is trained to extract features from the LMM by integrating it into a specific layer while keeping other parts of the LMM frozen. This disentangles complex representations into more easily understandable features. (b) depicts the auto-explanation pipeline used to interpret the extracted features. This involves analyzing which visual features strongly activate the SAE's learned features. (c) demonstrates the ability to use these extracted and understood features to influence the model's output by directly manipulating their activation values (clamping them to high values). This shows that the learned features have a causal relationship to the model's behavior.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.14982/x3.png", "caption": "Figure 2: The overview of the explanation pipeline, where images are forwarded through the LMM with the integrated SAE, and the activations of the top 256 most activated features are cached. For each feature, the top 5 images with the highest activations are selected, followed by the execution of zero-shot image explanations using a large LMM.", "description": "This figure illustrates the process of interpreting learned features from a Sparse Autoencoder (SAE) within a Large Multimodal Model (LMM).  The pipeline begins by caching the activations of the top 256 most active features from the SAE.  For each of these features, the five images that exhibit the strongest activation are selected.  These images, along with their corresponding activated regions are then input into a larger LLM for zero-shot explanation.  This allows the model itself to interpret the meaning behind these learned features, providing insights into the semantic representations within the LMM.", "section": "2. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.14982/x4.png", "caption": "Figure 3: An overview of the evaluation pipeline for calculating IOU scores. Initially, a small LLM is used to refine the explanation into a concise description, which is then employed to generate the segmentation mask. The IOU score is subsequently computed by comparing the mask to the binarized activated region.", "description": "This figure details the process of calculating Intersection over Union (IoU) scores to evaluate the quality of feature explanations.  First, a smaller language model (LLM) refines the feature's initial explanation into a concise description. This description is then input into a segmentation model (GroundingDino-SAM), which generates a segmentation mask highlighting the predicted region. Finally, the IoU score is calculated by comparing this generated mask with a binarized mask representing the actual activated region of the feature, providing a quantitative measure of the explanation's accuracy.", "section": "2.3. Steering the Neural Representation"}, {"figure_path": "https://arxiv.org/html/2411.14982/x5.png", "caption": "Figure 4: A comparison of several visual concepts and their activated areas. We compare several visual concepts and their corresponding activated areas, showcasing one example for each concept across different features. For each feature, we calculate the IOU by averaging the IOUs from the top-5 activated images. Although some features yield relatively low IOU scores, we find that the explanations are still semantically accurate with respect to the activated regions.", "description": "Figure 4 visualizes examples of several visual concepts and their corresponding activated regions within a large multimodal model.  For each concept, a representative image with its activated region is shown. The figure demonstrates that the model's internal representations can be associated with various open-semantic concepts. The Intersection over Union (IOU) score for each feature is calculated by averaging the IOU scores obtained from the top 5 most activated images.  While some features exhibit relatively low IOU scores, their generated explanations still align semantically with the activated image regions, indicating the model's ability to capture relevant visual information, even if the spatial alignment is not always perfect.", "section": "3.2. Interpretaion Pipeline Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.14982/x6.png", "caption": "Figure 5: The feature that relates to sad. We probe and find out the feature that activated on \u201dsad\u201d. By clamping this feature, we can enforce the model to share the feeling of sad", "description": "This figure demonstrates a feature within a large multimodal model (LMM) associated with the emotion of sadness.  The researchers used a technique called \"clamping\" to artificially increase the activation of this sadness feature.  The figure shows example image prompts and the model's responses both before and after clamping the sadness feature.  The before response is neutral; the after response reflects a feeling of sadness, illustrating how directly manipulating a specific internal feature in the LMM can influence its behavioral output, specifically its emotional expression.", "section": "4.1. Case Studies of Emotion Feature"}, {"figure_path": "https://arxiv.org/html/2411.14982/x7.png", "caption": "Figure 6: The feature that relates to happy. We find out that the feature is related with joy and celebrate action that relate to happiness. By clamping this feature, we can enforce the model to share the feeling happiness with others.", "description": "This figure demonstrates a feature within a large multimodal model (LMM) associated with the emotion of happiness.  The left panel shows examples of images that strongly activate this feature; commonalities include scenes of joy and celebration. The right panel shows how manipulating this feature (by clamping its activation value to a high level) influences the model's generated text.  Specifically, the example shows that clamping this feature causes the model to express happiness in its response to a prompt, even if the prompt itself is neutral. This highlights the model's ability to associate and generate emotional responses based on internal feature representations.", "section": "4.1. Case Studies of Emotion Feature"}, {"figure_path": "https://arxiv.org/html/2411.14982/x8.png", "caption": "Figure 7: A feature that relates to the concept \u201deat\u201d. We further investigate about the concept behind this feature and find out that model can reason from a visual action \u201deat\u201d into the concept \u201dconcept\u201d and \u201dgreedy\u201d", "description": "This figure demonstrates a feature in a large multimodal model (LMM) that is activated by images depicting the action of eating.  Further analysis reveals that the model connects this visual feature not only to the simple concept of 'eating' but also to higher-level abstract concepts such as 'greed' and 'hunger'. The model's ability to connect a basic visual action to complex abstract ideas is illustrated by showing examples of its responses when this specific feature is activated.", "section": "4.1. Case Studies of Emotion Feature"}, {"figure_path": "https://arxiv.org/html/2411.14982/x9.png", "caption": "Figure 8: An example of the hallucination on LLaVA. Bolivia is not shown on the image but the model still answer yes.", "description": "The figure showcases a failure case of the LLaMA model where it hallucinates an answer despite the visual input lacking evidence.  The question is whether Bolivia is in the Amazon basin, and the model answers \"yes,\" even though the map image provided does not show Bolivia or provide any indication of its presence within the Amazon Basin. This highlights the limitations of the model to accurately interpret visual data and its tendency towards hallucination.", "section": "4.3. Localizing the Cause for Model Behaviors"}, {"figure_path": "https://arxiv.org/html/2411.14982/x10.png", "caption": "Figure 9: Feature that relates to the text \u201dBarcelona\u201d. By clamping this feature to high value, we intervene the reasoning steps and hallucination in\u00a0Fig.\u00a08 disappears.", "description": "Figure 9 demonstrates how manipulating a specific feature within a large multimodal model (LMM) can impact its reasoning process and correct hallucinated outputs. The figure focuses on a feature strongly associated with the text \"Barcelona\". By increasing the activation value of this \"Barcelona\" feature, the model's tendency to produce a hallucinated response is mitigated, illustrating the possibility of intervening in the model's internal mechanisms to refine its output.", "section": "4.4. Localizing the Cause for Model Behaviors"}, {"figure_path": "https://arxiv.org/html/2411.14982/x11.png", "caption": "Figure 10: The high attribution area of different images and on the text. For images, we observe that features with high attribution mostly activate on positions that relate to key information about the question. For text, we observe that the \u201dBolivia\u201d token contributes the most to the answer \u201dyes\u201d", "description": "Figure 10 visualizes the results of an attribution analysis, highlighting which parts of an image and text contribute most to a model's incorrect answer.  The heatmap for image attribution shows that regions containing key geographical information (like map legends and country boundaries) strongly activate relevant features.  In contrast, the text attribution heatmap highlights the word \"Bolivia\" as the most influential factor driving the incorrect \"yes\" response, even though Bolivia's presence in the Amazon Basin isn't actually depicted in the image.", "section": "4.3. Localizing the Cause for Model Behaviors"}, {"figure_path": "https://arxiv.org/html/2411.14982/x12.png", "caption": "Figure 11: Feature that relates to the text \u201dLos\u201d. We validate our assumption by finding another feature that relates to text and mitigate the hallucination.", "description": "This figure demonstrates how manipulating a specific feature in a large multimodal model (LMM) can correct a hallucination.  The model initially hallucinates that Bolivia is part of the Amazon Basin based on an image, even though the image doesn't explicitly show that. By identifying and activating a feature related to the text \"Los\" (likely referring to text on signage within the image), the researchers steer the model's reasoning process, leading to a corrected, non-hallucinatory response.", "section": "4.3. Localizing the Cause for Model Behaviors"}, {"figure_path": "https://arxiv.org/html/2411.14982/x13.png", "caption": "Figure 12: The feature related to money and its steering effect.", "description": "This figure shows a feature from a sparse autoencoder trained on the LLaVA-NeXT dataset.  The feature is identified as relating to the concept of \"money.\" The top row displays example images which strongly activate this feature.  The bottom row shows a prompt given to the LLaVA-NeXT model, along with the model's original response (left) and a modified response obtained when the 'money' feature's activation is artificially increased (right). The modified response demonstrates the feature's ability to influence the model's output, steering it towards a narrative concerning financial resources.", "section": "4. Case Studies of Emotion Feature"}, {"figure_path": "https://arxiv.org/html/2411.14982/x14.png", "caption": "Figure 13: The feature related to speech and its steering effect.", "description": "This figure demonstrates a feature identified within a large multimodal model (LMM) that strongly responds to the concept of 'speech' or 'speeches'. The figure showcases example images and their corresponding activations within this feature.  To illustrate the effect of the feature, the model's response to a prompt is shown under two conditions: the original response and a steered response where the 'speech' feature has been artificially increased. By comparing these responses, we can see how this specific feature directly influences the model's output, providing insight into how features can modulate the LMM's behavior and generate different kinds of text in response to the same prompt.", "section": "4. Case Studies of Emotion Feature"}, {"figure_path": "https://arxiv.org/html/2411.14982/x15.png", "caption": "Figure 14: The feature related to unix and its steering effect.", "description": "This figure shows a specific feature learned by a Sparse Autoencoder (SAE) within a Large Multimodal Model (LMM) that is strongly activated by the presence of Unix/Linux-related elements in images.  The original caption of the figure simply states that the feature is related to Unix and demonstrates its steering effect. Steering involves artificially increasing the activation of this specific feature and observing the impact on the LMM's output. The figure demonstrates how manipulating this feature causes the LLM to generate a text response that includes information about Ubuntu, a popular Unix-like operating system, showing the effect of this specific feature on the model's generated text.", "section": "4. Case Studies of Emotion Feature"}, {"figure_path": "https://arxiv.org/html/2411.14982/x16.png", "caption": "Figure 15: The feature related to chair and its steering effect.", "description": "This figure shows a feature in a large multi-modal model (LMM) that is associated with the concept of 'chair'. The top row displays example images from the dataset that strongly activate this particular feature.  The bottom row shows the model's response to a prompt (\"Tell me a story about Alice and Bob\") under two conditions: (1) without any manipulation of the feature, and (2) with the feature's activation value clamped to a high value (steering). By comparing the responses, one can observe how manipulating a specific feature within the LMM can influence the model's generated text, demonstrating the feature's ability to influence the output.", "section": "4. Case Studies of Emotion Feature"}, {"figure_path": "https://arxiv.org/html/2411.14982/x17.png", "caption": "Figure 16: The feature related to money and its steering effect.", "description": "This figure showcases a specific feature within a large multimodal model (LMM) that strongly responds to the visual concept of 'money' or 'currency'.  The left panel displays example images that highly activate this feature\u2014these are images that the model's internal representation processes as strongly related to money.  The right panel demonstrates the effect of 'steering'\u2014manipulating the activation strength of this feature\u2014on the model's generated text.  By artificially increasing the activation of this feature, the model's generated story shifts to prominently include themes and details centered around financial matters, illustrating how the model's behavior can be influenced by directly altering its internal representations of specific visual concepts.", "section": "4. Case Studies of Emotion Feature"}, {"figure_path": "https://arxiv.org/html/2411.14982/x18.png", "caption": "Figure 17: Low level features in the LMM. These features activate in most of the images and showcase the model\u2019s basic cognition and perception abilities.", "description": "Figure 17 showcases examples of low-level features identified within the Large Multimodal Model (LMM). Unlike high-level semantic features representing complex concepts, these low-level features respond to basic visual elements such as grid structures, shapes (e.g., stars), surface textures, lines, and color patterns. Their consistent activation across a wide range of images highlights the model's fundamental capacity for basic visual perception and cognition.  These features are foundational building blocks for understanding more complex visual information.", "section": "4. Low Level Perception Features"}]