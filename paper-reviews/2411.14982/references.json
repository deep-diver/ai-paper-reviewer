{"references": [{"fullname_first_author": "Alekh Agarwal", "paper_title": "Learning sparsely used overcomplete dictionaries via alternating minimization", "publication_date": "2014-01-01", "reason": "This paper introduces sparse autoencoders (SAEs), a key technique used in the current research to disentangle and interpret multimodal representations."}, {"fullname_first_author": "David Bau", "paper_title": "Network dissection: Quantifying interpretability of deep visual representations", "publication_date": "2017-07-01", "reason": "This paper presents a method for analyzing and interpreting visual features within deep learning models which is highly relevant to the current paper\u2019s goal of interpreting multimodal models."}, {"fullname_first_author": "Steven Bills", "paper_title": "Language models can explain neurons in language models", "publication_date": "2023-01-01", "reason": "This paper demonstrates a method for using larger language models to interpret the inner workings of smaller ones, an approach mirrored in the current research for multimodal models."}, {"fullname_first_author": "Trenton Bricken", "paper_title": "Towards monosemanticity: Decomposing language models with dictionary learning", "publication_date": "2023-01-01", "reason": "This paper proposes dictionary learning to decompose complex representations in language models into simpler semantic components, a technique similar to that applied to multimodal models in the current study."}, {"fullname_first_author": "Michael Elad", "paper_title": "Sparse and redundant representations: from theory to applications in signal and image processing", "publication_date": "2010-01-01", "reason": "This paper provides background on sparse coding and its applications in signal and image processing, which is crucial to understanding the use of sparse autoencoders in the current research for multimodal representations."}]}