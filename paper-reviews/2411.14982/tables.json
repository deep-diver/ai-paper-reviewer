[{"content": "Concept|Metric|Random|V-Interp (Ours)\n---|---|---|---\nscene|IOU (\u2191)|0.007\u00b11\u00d710\u207b\u00b3|0.20\n|CS (\u2191)|18.1\u00b16\u00d710\u207b\u00b2|24.4\nobject|IOU (\u2191)|0.005\u00b15\u00d710\u207b\u2074|0.19\n|CS (\u2191)|18.2\u00b12\u00d710\u207b\u00b2|24.0\npart|IOU (\u2191)|0.007\u00b12\u00d710\u207b\u00b3|0.21\n|CS (\u2191)|18.1\u00b15\u00d710\u207b\u00b2|23.5\nmaterial|IOU (\u2191)|0.01\u00b18\u00d710\u207b\u00b3|0.39\n|CS (\u2191)|18.1\u00b11\u00d710\u207b\u00b9|24.1\ntexture|IOU (\u2191)|0.007\u00b12\u00d710\u207b\u00b3|0.21\n|CS (\u2191)|18.4\u00b16\u00d710\u207b\u00b2|20.9\ncolour|IOU (\u2191)|0.005\u00b12\u00d710\u207b\u00b3|0.10\n|CS (\u2191)|19.6\u00b17\u00d710\u207b\u00b2|20.3\nTotal|IOU (\u2191)|0.005\u00b12\u00d710\u207b\u2074|0.20\n|CS (\u2191)|18.2\u00b11\u00d710\u207b\u00b2|23.6", "caption": "Table 1: The Intersection over Union (IoU) and CLIP scores for each concept are computed based on the top-5 most activated images.", "description": "This table presents the Intersection over Union (IoU) and CLIP scores for several visual concepts.  The IoU score measures the overlap between the predicted segmentation mask (generated from the model's explanation of the activated feature) and the ground truth activation region.  The CLIP score measures the semantic similarity between the model's generated explanation and the concept being evaluated. Both scores are calculated using the top 5 images with the highest activation for each feature.", "section": "3.2 Interpretaion Pipeline Evaluation"}, {"content": "|           | scene | object | part | material | texture | colour | Total |\n|---|---|---|---|---|---|---|---| \n| GPT-4o | 0.93 | 0.84 | 0.9 | 1.0 | 0.85 | 0.92 | 0.89 |\n| Human | 0.70 | 0.85 | 0.6 | 0.95 | 0.80 | 0.60 | 0.75 |", "caption": "Table 2: Consistency scores for our explanation, computed based on 100 samples per concept for GPT-4o consistency and 10 samples for human consistency, highlighting the agreement between GPT-4o-generated and human-generated explanations.", "description": "This table presents a quantitative evaluation of the consistency between the model's automatically generated explanations of visual features and human-generated explanations.  For each concept (scene, object, part, material, texture, color), the model's consistency was evaluated using two methods: GPT-4, which assessed 100 samples per concept, and human evaluation, which assessed 10 samples per concept.  The scores highlight the level of agreement between the model's interpretation and human understanding of the features.", "section": "3.2 Interpretaion Pipeline Evaluation"}]