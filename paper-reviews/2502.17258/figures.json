[{"figure_path": "https://arxiv.org/html/2502.17258/x1.png", "caption": "Figure 1: VideoGrain enables multi-grained video editing across class, instance, and part levels.", "description": "This figure showcases VideoGrain's capability to perform multi-grained video editing.  It demonstrates three levels of editing: class-level (editing objects within the same class, for example, changing multiple men into Spidermen), instance-level (editing individual distinct objects, such as transforming one man into Spiderman and another into Batman), and part-level (modifying attributes of objects or adding new ones, for instance, adding a hat to Spiderman). The figure visually illustrates these three levels with examples from videos, highlighting the precision and semantic understanding achieved by VideoGrain.", "section": "ABSTRACT"}, {"figure_path": "https://arxiv.org/html/2502.17258/x2.png", "caption": "Figure 2: Definition of multi-grained video editing and comparison on instance editing", "description": "The figure illustrates the concept of multi-grained video editing, which involves modifications at three levels: class, instance, and part.  Class-level editing changes all objects within a specific class (e.g., changing all men to Spiderman). Instance-level editing modifies individual objects separately (e.g., changing one man to Spiderman and another to a polar bear). Finally, part-level editing focuses on modifying specific attributes of an object (e.g., adding sunglasses to a polar bear). The figure also highlights the challenges of existing instance editing methods, which often mix features of different instances during editing, leading to artifacts. This contrasts with the goal of multi-grained editing to provide precise control over each level of modification.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2502.17258/x3.png", "caption": "Figure 3: Analysis of why the diffusion model failed in instance-level video editing. Our goal is to edit left man into \u201cIron Man,\u201d right man into \u201cSpiderman,\u201d and trees into \u201ccherry blossoms.\u201d In (b), we apply K-Means on self-attention, and in (d), we visualize the 32x32 cross-attention map.", "description": "This figure analyzes the failure of a diffusion model in instance-level video editing. The objective was to transform the left man into Iron Man, the right man into Spiderman, and the trees into cherry blossoms.  Subfigure (a) shows the input video.  Subfigure (b) demonstrates the application of K-Means clustering to the self-attention features, revealing a semantic layout but failing to distinguish between distinct instances. Subfigure (d) visualizes the 32x32 cross-attention map generated when the model attempts the edit using the prompt \u201cAn Iron Man and a Spiderman are jogging under cherry blossoms,\u201d highlighting the issue of feature mixing and misalignment between textual prompts and corresponding visual regions.", "section": "3.1 MOTIVATION"}, {"figure_path": "https://arxiv.org/html/2502.17258/x4.png", "caption": "Figure 4: VideoGrain pipeline.\n(1) we integrate ST-Layout Attn into the frozen SD for multi-grained editing, where we modulate self- and cross-attention in a unified manner.\n(2) In cross-attention, we view each local prompt and its location as positive pairs, while the prompt and outside-location areas are negative pairs, enabling text-to-region control.\n(3) In self-attention,\nwe enhance positive awareness within intra-regions and restrict negative interactions between inter-regions across frames, making each query only attend to the target region and keep feature separation. In the bottom two figures, p\ud835\udc5dpitalic_p denotes original attention score and w,i\ud835\udc64\ud835\udc56w,iitalic_w , italic_i denotes the word and frame index.", "description": "Figure 4 illustrates the VideoGrain pipeline, a novel framework for multi-grained video editing.  It shows how spatial-temporal layout-guided attention (ST-Layout Attn) is integrated into a pre-trained Stable Diffusion model. This integration modulates both cross-attention and self-attention mechanisms for finer control. Cross-attention is modulated to ensure that each local prompt focuses on its correct spatial region (positive pairs), while ignoring irrelevant areas (negative pairs), achieving precise text-to-region control. Self-attention is modulated to amplify intra-region interactions and suppress inter-region interactions across frames. This improves feature separation and helps maintain temporal consistency. The bottom of the figure visually explains the modulation process, showing how attention weights are adjusted using positive and negative pair scores.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2502.17258/x5.png", "caption": "Figure 5: Qualitative results. VideoGrain achieves multi-grained video editing, including class-level, instance-level, and part-level. We refer the reader to our project page for full-video results.", "description": "Figure 5 presents example results demonstrating VideoGrain's capability for multi-grained video editing.  It showcases edits at three levels of granularity: class-level (modifying objects within the same class), instance-level (modifying specific instances of objects), and part-level (adding new objects or modifying attributes of existing objects). The figure visually demonstrates the versatility and precision of the proposed VideoGrain model.  For a more comprehensive view of the editing results including video clips and more examples, refer to the project page linked in the paper.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2502.17258/x6.png", "caption": "Figure 6: Qualitative comparisons.\nWe refer the reader to our project page for detailed assessment.", "description": "Figure 6 presents a qualitative comparison of VideoGrain's performance against other state-of-the-art methods across class, instance, and part levels of video editing.  The figure showcases examples of animal and human instance edits, and part-level modifications, highlighting VideoGrain's ability to achieve more precise and accurate results compared to baselines.  For a detailed analysis of the results, including video demonstrations, please refer to the project page.", "section": "4.2 Results"}, {"figure_path": "https://arxiv.org/html/2502.17258/x7.png", "caption": "Figure 7: Attention weight distribution.", "description": "This figure visualizes the attention weight distribution before and after applying the Spatial-Temporal Layout-Guided Attention (ST-Layout Attn) module.  It showcases how ST-Layout Attn refines attention weights to improve the precision of multi-grained text-to-region control and feature separation. The \"before\" visualization demonstrates attention leakage, highlighting the feature mixing problem prevalent in diffusion models without ST-Layout Attn. The \"after\" visualization, in contrast, shows how ST-Layout Attn focuses attention to relevant regions for each target while suppressing attention to irrelevant areas, thus effectively addressing feature mixing and enhancing the accuracy and quality of the edits.", "section": "3.4 SPATIAL-TEMPORAL LAYOUT-GUIDED ATTENTION"}, {"figure_path": "https://arxiv.org/html/2502.17258/x8.png", "caption": "Figure 8: Ablation of cross- and self-modulation in ST-Layout Attn.", "description": "This figure shows an ablation study on the impact of cross-attention and self-attention modulation within the Spatial-Temporal Layout Attention (ST-Layout Attn) module of the VideoGrain model. It demonstrates how modulating these attention mechanisms improves the model's ability to perform multi-grained video editing. The results show that both cross-attention and self-attention modulation are essential for accurate and high-quality edits, particularly when handling multiple instances or complex edits.  The figure presents several edited video frames alongside quantitative metrics to support the findings.", "section": "3.4 SPATIAL-TEMPORAL LAYOUT-GUIDED ATTENTION"}, {"figure_path": "https://arxiv.org/html/2502.17258/x9.png", "caption": "Figure 9: VideoP2P joint and sequential edit with SAM-Track masks", "description": "This figure compares the performance of VideoP2P, a video editing method using SAM-Track instance masks, with the proposed VideoGrain method.  It demonstrates two editing approaches: (1) joint editing, where multiple regions are modified simultaneously in a single denoising step, and (2) sequential editing, where each region is modified sequentially in separate denoising steps. The results show that VideoGrain outperforms VideoP2P in both accuracy and consistency, particularly in scenarios with complex edits.", "section": "A EVALUATE: SAM-TRACK MASKS' IMPACT"}, {"figure_path": "https://arxiv.org/html/2502.17258/x10.png", "caption": "Figure 10: Ground-A-Video joint edit with instance information", "description": "This figure demonstrates the limitations of the Ground-A-Video method in performing joint edits with instance information.  Despite providing instance-level grounding (information about the location and identity of objects), Ground-A-Video struggles to correctly modify multiple instances simultaneously in a single edit pass.  The figure highlights the failures of this approach in contrast to the capabilities of the VideoGrain model, which successfully handles multi-grained video editing.", "section": "4.2 RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.17258/x11.png", "caption": "Figure 11: Our method without additional SAM-Track masks", "description": "Figure 11 demonstrates the capability of the proposed VideoGrain method to perform multi-grained video editing without relying on additional instance segmentation masks from SAM-Track.  It showcases the results of editing a video of Batman playing tennis on a snowy court before an iced wall, using three different approaches: (1) The input video, (2) Results obtained using Ground-A-Video, showing its limitations in multi-grained editing, (3) Results from applying DDIM inversion cluster masks to identify regions for editing, demonstrating the method\u2019s reliance on inherent semantic layout information rather than external masks. Finally, (4) displays the results from VideoGrain, illustrating its ability to successfully edit the video.  This illustrates that VideoGrain does not strictly depend on SAM-Track, but leverages the diffusion model\u2019s self-attention features for multi-grained video editing.", "section": "3.1 MOTIVATION"}, {"figure_path": "https://arxiv.org/html/2502.17258/x12.png", "caption": "Figure 12: Soely edit on specific subjects, without background changed", "description": "This figure demonstrates the capability of the VideoGrain model to edit specific subjects within a video while leaving the background unchanged.  The leftmost image shows the original video frame. The next three images show the results of editing only the left subject, only the right subject, and both subjects simultaneously.  This showcases the model's ability to perform selective edits with precision.", "section": "C SOLELY EDIT ON SPECIFIC SUBJECTS, WITHOUT BACKGROUND CHANGED"}, {"figure_path": "https://arxiv.org/html/2502.17258/x13.png", "caption": "Figure 13: Part-level modifications on humans and animals", "description": "Figure 13 presents examples of part-level modifications achievable with the VideoGrain method.  The left side shows modifications on humans, demonstrating changes such as altering a shirt's color from gray to blue, or changing the shirt style from a half-sleeve to a full suit. The right side shows modifications on animals, specifically changing a cat's head and body color while retaining other features. These examples showcase VideoGrain's capability to make fine-grained edits to specific parts of objects within a video frame, enhancing its versatility for detailed video manipulation.", "section": "Part-Level Modification Examples"}, {"figure_path": "https://arxiv.org/html/2502.17258/x14.png", "caption": "Figure 14: Temporal Focus of ST-Layout Attn", "description": "This figure compares the results of different approaches to handling temporal consistency in video editing.  Specifically, it contrasts using a per-frame approach, a sparse-causal approach (considering only the current and immediately preceding frame), and the proposed ST-Layout Attention (ST-Layout Attn) method.  The goal is to show how ST-Layout Attn effectively avoids flickering and maintains consistent visual details across frames during multi-grained video editing, unlike the other methods that might produce inconsistencies.", "section": "3.4 SPATIAL-TEMPORAL LAYOUT-GUIDED ATTENTION"}, {"figure_path": "https://arxiv.org/html/2502.17258/x15.png", "caption": "Figure 15: ControlNet ablation", "description": "This figure demonstrates the impact of ControlNet on the model's performance. ControlNet is a technique that uses additional information, such as depth or pose maps, to guide the image generation process. The figure shows that even without ControlNet (using only the textual descriptions), the model can still edit videos with multiple regions simultaneously. However, there might be some inconsistencies between edited and original objects without ControlNet due to lack of explicit structural guidance.", "section": "F ControlNet Ablation"}, {"figure_path": "https://arxiv.org/html/2502.17258/x16.png", "caption": "Figure 16: More general objects instance editing (animals) and shape editing (cars) results.", "description": "This figure showcases the versatility of the VideoGrain model in handling more complex editing tasks involving general objects and shape changes.  The top row demonstrates instance editing on animals, successfully replacing animals with others while maintaining the background context. The bottom row demonstrates shape editing on cars, effectively changing the make and model of a vehicle while preserving the overall scene. This highlights the model's ability to perform multi-grained editing, seamlessly integrating changes into existing video content, regardless of the complexity of the scene or the type of objects involved.", "section": "More General Objects and Shape Editing"}, {"figure_path": "https://arxiv.org/html/2502.17258/x17.png", "caption": "Figure 17: More frames ablation of ST-Layout Attn\u2019s effects on attention weight distribution.", "description": "This ablation study investigates the impact of using ST-Layout Attention (ST-Layout Attn) with varying numbers of frames on attention weight distribution.  It compares the attention weight distribution when ST-Layout Attn is applied to (1) the first frame only, (2) each frame individually, and (3) across the entire video sequence. The goal is to demonstrate how ST-Layout Attn helps maintain consistency in attention weights across frames and improve the temporal coherence of the generated video, mitigating issues like flickering or inconsistencies that could arise from processing individual frames in isolation.", "section": "4.4 ABLATION STUDY"}]