{"importance": "This paper is crucial for researchers in NLP and computational linguistics because it addresses the critical need for large, high-quality multilingual corpora, especially for minority languages.  **GlotCC offers a valuable resource for developing and evaluating language technologies**, and the open-source pipeline allows researchers to build upon this work and adapt it to other languages or domains.  This work significantly contributes to bridging the digital divide in language technologies and fostering linguistic diversity in research.", "summary": "GlotCC: Open multilingual corpus & pipeline for minority languages, exceeding 1000 languages.", "takeaways": ["GlotCC, a 2TB multilingual corpus covering over 1000 languages, derived from Common Crawl is introduced.", "A new language identification model, GlotLID v3.0, with improved accuracy and broader coverage is presented.", "An open-source pipeline for creating and cleaning multilingual corpora is made available, fostering reproducibility and further research."], "tldr": "Many existing language corpora are skewed towards high-resource languages, leaving many under-resourced languages underserved. This imbalance hinders the development of language technologies that can benefit diverse communities.  Furthermore, existing methods for collecting and cleaning web data often struggle with minority languages.  This results in noisy, unreliable data unsuitable for machine learning tasks. \nTo address these problems, this paper introduces GlotCC, a massive multilingual corpus covering more than 1000 languages.  **GlotCC is generated using a novel, open-source pipeline that incorporates a sophisticated language identification model (GlotLID v3.0) designed for high accuracy and broad language coverage.** This pipeline also employs several robust filtering methods to remove noisy data, producing a high-quality and reliable corpus suitable for many natural language processing tasks. The researchers also share their pipeline and improved language identification model, enhancing the reproducibility of their work and encouraging future research and development in this field.", "affiliation": "LMU Munich & Munich Center for Machine Learning", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}}