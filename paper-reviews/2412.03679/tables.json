[{"content": "| Domain | Data Generation Method | Seed Data | Seed Data Size | Benchmark |\n|---|---|---|---|---|\n| **Math** | **Instance Generation** | GSM8K, MATH (train set) | 14,856 | GSM8K, MATH (test set) |\n|  | **Response Generation** | Magpie-Reasoning (math) | 10,000 | GSM8K, MATH (test set) |\n|  | **Quality Enhancement** | WebInstruct (math) | 10,000 | GSM8K, MATH (test set) |\n| **Code** | **Instance Generation** | MBPP (train set), xP3x | 874 | MBPP, HumanEval (test set) |\n|  | **Response Generation** | Magpie-Reasoning (code) | 10,000 | MBPP, HumanEval (test set) |\n|  | **Quality Enhancement** | CoNaLa | 10,000 | MBPP, HumanEval (test set) |\n| **Inst. Follow** | **Instance Generation** | LIMA | 503 | AlpacaEval 2.0, Arena-Hard |\n|  | **Response Generation** | Magpie-Pro | 10,000 | AlpacaEval 2.0, Arena-Hard |\n|  | **Quality Enhancement** | WebInstruct (code) | 10,000 | AlpacaEval 2.0, Arena-Hard |", "caption": "Table 1: AgoraBench Settings: For each of the nine settings, an LM being evaluated generates 10K instances with the same meta-prompt and seed data. Note that the seed dataset is also used for training in instance generation.", "description": "The table details the experimental setup of AGORABENCH, a benchmark designed to evaluate language models as synthetic data generators.  Each of the nine settings involves a specific domain (math, code, or instruction following), a data generation method (instance generation, response generation, or quality enhancement), a corresponding seed dataset, and a benchmark for evaluating the generated data.  For each setting, the same meta-prompt is used, and each language model (LM) generates 10,000 instances.  Crucially, for instance generation, the seed dataset is also used as the training data for the student model. This controlled setup allows for a fair comparison of different LMs' data generation capabilities.", "section": "3 Experimental Setting of AGORABENCH"}, {"content": "| Data Generator | Math | Code | Inst. | Avg | Math | Code | Inst. | Avg | Math | Code | Inst. | Avg |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| GPT-4o | **20.6** | **73.6** | **46.1** | **46.8** | **46.7** | 28.5 | **30.3** | **35.2** | **21.9** | -8.8 | 7.1 | **6.7** |\n| GPT-4o-mini | **16.1** | 41.9 | 18.0 | **25.3** | **48.1** | 18.9 | **13.7** | 26.9 | **17.8** | -11.2 | **9.9** | 5.5 |\n| Claude-3.5-Sonnet | 8.9 | 23.4 | **40.1** | 24.1 | 29.0 | **44.5** | 12.7 | **28.8** | 15.7 | **16.1** | **21.8** | **17.9** |\n| Llama-3.1-405B | 10.4 | 12.6 | 7.4 | 10.1 | 31.7 | 35.4 | 4.9 | 24.0 | -11.8 | 7.5 | 3.6 | -0.2 |\n| Llama-3.1-70B | 9.6 | **58.7** | 6.5 | 24.9 | 23.0 | **37.1** | 4.5 | 21.5 | -21.8 | 6.9 | 2.7 | -4.1 |\n| Llama-3.1-8B | 6.5 | 55.7 | 6.2 | 22.8 | 27.6 | 25.8 | 5.0 | 19.4 | -1.7 | **15.4** | 3.0 | 5.6 |", "caption": "Table 2: AgoraBench Results: How much performance could you recover by generating 10K instances with your LLM, compared to Meta\u2019s post-training process for training Llama-3.1-8B-Instruct from Llama-3.1-8B? The best comparable performances (%) are bolded, and the second-best performances (%) are underlined. Note that the Llama models are instruction-tuned versions and that \u2018Inst.\u2019 denotes instruction-following.", "description": "This table presents the results of the AGORABENCH benchmark, which evaluates the effectiveness of different Language Models (LLMs) in generating synthetic data for post-training.  It shows the percentage improvement (Performance Gap Recovered or PGR) achieved by a student model (Llama-3.1-8B) trained on synthetic data generated by six different LLMs (GPT-40, GPT-40-mini, Claude-3.5-Sonnet, Llama-3.1-405B-Instruct, Llama-3.1-70B-Instruct, Llama-3.1-8B-Instruct) compared to the same model trained without synthetic data. The results are broken down by three data generation methods (instance generation, response generation, quality enhancement) and three domains (math, code, instruction following).  The best and second-best performance for each LLM in each category are highlighted. Note that the Llama models used are instruction-tuned versions.", "section": "4 Experimental Results of AGORABENCH"}, {"content": "| Data Generator | API Cost |  | Prob. | Data |\n|---|---|---|---|---| \n|  | Solv. | Gen. |  |  |\n|---|---|---|---|---| \n| GPT-4o | $2.50 | $10.00 | 80.9 | 29.5% |\n| GPT-4o-mini | $0.15 | $0.60 | 75.4 | 19.2% |\n| Claude-3.5-Sonnet | $3.00 | $15.00 | 80.5 | 23.6% |\n| Llama-3.1-405B | $1.79 | $1.79 | 75.0 | 11.3% |\n| Llama-3.1-70B | $0.35 | $0.40 | 69.6 | 14.1% |\n| Llama-3.1-8B | $0.055 | $0.055 | 50.2 | 15.9% |", "caption": "Table 3: Comparison of API costs, problem-solving ability, and data generation ability: Our findings reveal that neither the strength nor the cost of an LM guarantees its effectiveness as a data generator. Note that the Llama models are instruction-tuned versions, the specific results of the LMs on each benchmark (averaged as \u2018Problem Solving average\u2019) is in Appendix\u00a0C, and the AgoraBench results are averaged from Table\u00a02.", "description": "This table compares the API costs, problem-solving abilities, and data generation capabilities of six different Language Models (LMs).  It highlights that a more powerful or expensive LM does not automatically translate to better performance as a data generator.  The table shows the average API cost (for generating data) for each LM, its average problem-solving score across various benchmarks (detailed in Appendix C), and its average data generation score from the AgoraBench benchmark (averaged from Table 2).  This demonstrates the need to consider multiple factors when selecting an LM for data generation purposes and shows that even less expensive, less powerful models can be surprisingly effective at the task.", "section": "3 Experimental Setting of AGORABENCH"}, {"content": "| Intrinsic Metric | Loading Strength | Contribution |\n|---|---|---|\n| Prometheus Score (R.Q.) | 0.256 | 12.18% |\n| Response Perplexity | 0.252 | 12.00% |\n| GPT-4o Score (R.Q.) | 0.246 | 11.71% |\n| Problem-solving Ability | 0.240 | 11.42% |\n| Skywork-RM Score (R.Q.) | 0.239 | 11.38% |\n| Prometheus Score (I.D.) | 0.230 | 10.95% |\n| Diversity (I.D.) | 0.226 | 10.76% |\n| GPT-4o Score (I.D.) | 0.223 | 10.61% |\n| Diversity (R.Q.) | 0.189 | 9.00% |", "caption": "Table 4: Mean Contributions of Intrinsic Metrics to Principal Components: Each loading strength represents the average magnitude of a feature\u2019s loadings across all principal components and the contribution are normalized values to represent the relative percentage of each feature\u2019s loading strength in the overall component structure. \u2018I.D.\u2019 refers to metrics measuring instruction difficulty and \u2018R.Q.\u2019 refers to metrics measuring response quality. All intrinsic evaluation metrics show substantial contributions (0.189-0.256) to the principal components.", "description": "Table 4 shows the contribution of different intrinsic evaluation metrics to the principal components derived from a principal component analysis (PCA).  The \"loading strength\" represents the average magnitude of each metric's influence across all principal components.  The \"contribution\" is the normalized percentage of each metric's loading strength within the total variance explained by the principal components.  Metrics related to instruction difficulty (I.D.) and response quality (R.Q.) are both significant contributors to the principal components, as indicated by the substantial loading strengths (0.189-0.256). This suggests that the combination of instruction difficulty and response quality are important factors in determining data generation capability.", "section": "5.2 Can we predict the student model's improvement by looking into the data?"}, {"content": "| Data Generator | AgoraBench Meta-prompt Math | AgoraBench Meta-prompt Code | AgoraBench Meta-prompt Inst. | AgoraBench Meta-prompt Avg | Unoptimized Meta-prompt Math | Unoptimized Meta-prompt Code | Unoptimized Meta-prompt Inst. | Unoptimized Meta-prompt Avg | JSON-format Meta-prompt Math | JSON-format Meta-prompt Code | JSON-format Meta-prompt Inst. | JSON-format Meta-prompt Avg |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Instance Generation** |\n| GPT-4o-mini | 16.1 | 41.9 | 18.0 | 25.3 | 12.4 | 36.8 | 17.6 | 22.3 | 13.8 | 20.5 | 19.5 | 17.9 |\n| Llama-3.1-70B | 9.6 | 58.7 | 6.5 | 24.9 | 7.0 | 46.8 | 5.8 | 19.9 | 8.7 | 33.5 | 6.1 | 16.1 |\n| Llama-3.1-8B | 6.5 | 55.7 | 6.2 | 22.8 | 0.7 | 43.6 | 4.5 | 16.3 | 6.7 | 31.4 | 4.4 | 14.2 |\n| **Quality Enhancement** |\n| GPT-4o-mini | 17.8 | -11.2 | 9.9 | 5.5 | 13.0 | -6.3 | 9.4 | 5.4 | 15.4 | -13.0 | 9.2 | 3.8 |\n| Llama-3.1-70B | -21.8 | 6.9 | 2.7 | -4.1 | -20.5 | -5.5 | 2.3 | -7.9 | -18.3 | 6.5 | 2.4 | -3.1 |\n| Llama-3.1-8B | -1.7 | 15.4 | 3.0 | 5.6 | -6.6 | 3.7 | 3.5 | 0.2 | -2.7 | 12.0 | 3.9 | 4.4 |", "caption": "Table 5: Performance Gap Recovered (%) results with different meta-prompts on instance generation and quality enhancement. Llama models are instruction-tuned versions and that \u2018Inst.\u2019 denotes instruction-following.", "description": "This table presents the results of an experiment comparing the effectiveness of different meta-prompts when using language models to generate synthetic training data.  The experiment focuses on two data generation methods: instance generation (creating new training examples) and quality enhancement (improving existing examples).  The table shows the \"Performance Gap Recovered (PGR)\" for each method, indicating how much the performance of a model trained on the synthetic data improved compared to a baseline model. Three different Language Models (LLMs), all instruction-tuned versions of Llama, are compared.  Results are presented for three domains: math, code and instruction following. The performance is evaluated by calculating the percentage of the performance improvement (PGR).", "section": "6 Further Analysis Experiments"}, {"content": "| Inference Hyper-parameter |  | \n|---|---| \n| **Temperature** | 0.2 (math) & 0.0 (other domains) | \n| **Top_p** | 0.95 | \n| **Max New Tokens** | 1024 | \n| **Repetition Penalty** | 1.03 | \n| Training Hyper-parameter |  | \n|---|---| \n| **Base Model** | meta-llama/Llama-3.1-8B | \n| **Torch dtype** | bfloat16 | \n| **Epoch** | 5 | \n| **Max Seq Length** | 4096 | \n| **Learning Rate** | 1e-5 | \n| **Train Batch Size** | 4 | \n| **Gradient Accumulation** | 8 | \n| **GPU** | H100 (80GB) x 4 | \n| **Random Seed** | 42 | \n| **Training Method** | Supervised Fine-tuning | ", "caption": "Table 6: Hyper-parameters used for inference.", "description": "This table details the hyperparameters employed during the inference stage of the AGORABENCH experiments.  It lists values for parameters such as temperature, top_p, maximum new tokens, and repetition penalty, offering a clear view of the settings used for generating predictions from the language models.", "section": "3 Experimental Setting of AGORABENCH"}, {"content": "| Data Generator | GSM8K | MATH | MBPP | Human | Alpaca | Arena | Average |\n|---|---|---|---|---|---|---|---| \n| **GPT-4o** | 96.1 | 76.6 | 86.2 | 91.5 | 57.5 | 77.9 | 80.9 |\n| **GPT-4o-mini** | 93.2 | 70.2 | 85.7 | 88.4 | 50.7 | 64.2 | 75.4 |\n| **Claude-3.5-Sonnet** | 96.4 | 71.1 | 89.2 | 92.0 | 52.4 | 82.0 | 80.5 |\n| **Llama-3.1-405B** | 96.8 | 73.8 | 84.5 | 89.0 | 39.3 | 66.8 | 75.0 |\n| **Llama-3.1-70B** | 95.1 | 68.0 | 84.2 | 80.5 | 38.1 | 51.6 | 69.6 |\n| **Llama-3.1-8B** | 78.9 | 34.6 | 68.5 | 69.5 | 24.2 | 25.5 | 50.2 |", "caption": "Table 7: Problem-solving abilities of LMs measured by benchmark scores.", "description": "This table presents the problem-solving capabilities of six different Large Language Models (LLMs) across six benchmark datasets.  The benchmarks cover three domains: mathematics (GSM8K, MATH), code (MBPP, HumanEval), and instruction following (AlpacaEval 2.0, Arena-Hard).  The scores represent the percentage of correct answers each LLM achieved on each benchmark, providing a comprehensive assessment of their problem-solving abilities across various tasks and complexities.", "section": "4 Experimental Results of AGORABENCH"}, {"content": "| Data Generator | Math | Code | Inst. Follow | Avg | Math | Code | Inst. Follow | Avg | Math | Code | Inst. Follow | Avg |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Instruction Difficulty (LLM-as-a-Judge; GPT-4o Score) |  |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o (2024-08-06) | 2.92 | 3.48 | 3.06 | 3.16 | 2.27 | 2.21 | 1.41 | 1.97 | 2.44 | 1.51 | 1.79 | 1.91 |\n| GPT-4o-mini (2024-07-18) | 2.38 | 3.42 | 2.89 | 2.90 | 2.27 | 2.21 | 1.41 | 1.97 | 2.47 | 1.38 | 1.81 | 1.89 |\n| Claude-3.5-Sonnet (2024-06-20) | 3.24 | 4.03 | 3.54 | 3.60 | 2.27 | 2.21 | 1.41 | 1.97 | 2.47 | 1.52 | 1.83 | 1.94 |\n| Llama-3.1-405B-Instruct | 2.74 | 3.50 | 2.87 | 3.04 | 2.27 | 2.21 | 1.41 | 1.97 | 2.45 | 1.47 | 1.85 | 1.92 |\n| Llama-3.1-70B-Instruct | 2.87 | 3.45 | 2.96 | 3.09 | 2.27 | 2.21 | 1.41 | 1.97 | 2.48 | 1.49 | 1.87 | 1.95 |\n| Llama-3.1-8B-Instruct | 3.00 | 3.52 | 3.08 | 3.20 | 2.27 | 2.21 | 1.41 | 1.97 | 2.43 | 1.49 | 1.83 | 1.92 |\n| Instruction Difficulty (LLM-as-a-Judge; Prometheus-2-8x7B Score) |  |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o (2024-08-06) | 3.73 | 3.57 | 3.95 | 3.75 | 3.00 | 2.76 | 2.24 | 2.67 | 3.37 | 2.14 | 2.50 | 2.67 |\n| GPT-4o-mini (2024-07-18) | 3.44 | 3.38 | 3.94 | 3.59 | 3.00 | 2.76 | 2.24 | 2.67 | 3.36 | 1.98 | 2.53 | 2.63 |\n| Claude-3.5-Sonnet (2024-06-20) | 4.11 | 4.51 | 4.45 | 4.36 | 3.00 | 2.76 | 2.24 | 2.67 | 3.38 | 2.24 | 2.61 | 2.74 |\n| Llama-3.1-405B-Instruct | 3.63 | 3.27 | 3.84 | 3.58 | 3.00 | 2.76 | 2.24 | 2.67 | 3.35 | 2.11 | 2.64 | 2.70 |\n| Llama-3.1-70B-Instruct | 3.72 | 3.43 | 3.94 | 3.69 | 3.00 | 2.76 | 2.24 | 2.67 | 3.32 | 2.21 | 2.76 | 2.76 |\n| Llama-3.1-8B-Instruct | 3.86 | 3.48 | 3.99 | 3.78 | 3.00 | 2.76 | 2.24 | 2.67 | 3.30 | 2.09 | 2.67 | 2.68 |\n| Instruction Difficulty (Perplexity) |  |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o (2024-08-06) | 2.13 | 1.28 | 3.44 | 2.28 | 2.26 | 4.23 | 3.41 | 3.30 | 2.03 | 3.60 | 3.83 | 3.15 |\n| GPT-4o-mini (2024-07-18) | 2.05 | 1.31 | 3.32 | 2.23 | 2.28 | 2.12 | 3.20 | 2.53 | 2.08 | 5.50 | 3.97 | 3.85 |\n| Claude-3.5-Sonnet (2024-06-20) | 2.04 | 1.34 | 3.18 | 2.19 | 2.16 | 3.48 | 3.63 | 3.09 | 1.99 | 2.46 | 3.04 | 2.50 |\n| Llama-3.1-405B-Instruct | 1.96 | 1.29 | 2.19 | 1.81 | 1.90 | 1.91 | 2.42 | 2.08 | 2.10 | 3.10 | 3.90 | 3.03 |\n| Llama-3.1-70B-Instruct | 1.78 | 1.27 | 2.19 | 1.74 | 1.86 | 1.72 | 2.52 | 2.03 | 2.12 | 2.84 | 3.98 | 2.98 |\n| Llama-3.1-8B-Instruct | 1.83 | 1.33 | 2.08 | 1.74 | 1.98 | 1.81 | 2.48 | 2.09 | 2.06 | 3.17 | 3.98 | 3.07 |\n| Response Quality (LLM-as-a-Judge; GPT-4o Score) |  |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o (2024-08-06) | 3.72 | 3.95 | 4.42 | 4.03 | 3.99 | 3.79 | 4.44 | 4.07 | 3.62 | 3.66 | 3.99 | 3.76 |\n| GPT-4o-mini (2024-07-18) | 3.96 | 3.96 | 4.35 | 4.09 | 3.85 | 3.76 | 4.41 | 4.01 | 3.57 | 3.22 | 3.96 | 3.58 |\n| Claude-3.5-Sonnet (2024-06-20) | 3.39 | 4.03 | 4.34 | 3.92 | 3.80 | 3.75 | 4.24 | 3.93 | 3.64 | 3.77 | 4.29 | 3.90 |\n| Llama-3.1-405B-Instruct | 3.20 | 3.74 | 4.13 | 3.69 | 3.51 | 3.76 | 4.29 | 3.85 | 3.36 | 3.37 | 3.80 | 3.51 |\n| Llama-3.1-70B-Instruct | 2.97 | 3.59 | 4.12 | 3.56 | 3.31 | 3.65 | 4.22 | 3.72 | 3.23 | 3.22 | 3.80 | 3.42 |\n| Llama-3.1-8B-Instruct | 1.99 | 2.51 | 3.82 | 2.77 | 2.90 | 3.26 | 4.17 | 3.44 | 3.05 | 2.76 | 3.52 | 3.11 |\n| Response Quality (LLM-as-a-Judge; Prometheus-2-8x7B Score) |  |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o (2024-08-06) | 3.93 | 3.49 | 4.07 | 3.83 | 4.02 | 3.28 | 3.97 | 3.76 | 3.98 | 3.28 | 3.69 | 3.65 |\n| GPT-4o-mini (2024-07-18) | 4.05 | 3.46 | 4.04 | 3.85 | 3.96 | 3.39 | 3.93 | 3.76 | 3.92 | 3.04 | 3.73 | 3.57 |\n| Claude-3.5-Sonnet (2024-06-20) | 3.95 | 3.37 | 4.04 | 3.78 | 3.94 | 3.29 | 3.83 | 3.69 | 4.00 | 3.48 | 4.03 | 3.84 |\n| Llama-3.1-405B-Instruct | 3.76 | 3.24 | 3.92 | 3.64 | 3.81 | 3.42 | 3.91 | 3.71 | 3.78 | 3.23 | 3.66 | 3.56 |\n| Llama-3.1-70B-Instruct | 3.68 | 3.36 | 3.91 | 3.65 | 3.73 | 3.37 | 3.86 | 3.65 | 3.73 | 3.20 | 3.62 | 3.52 |\n| Llama-3.1-8B-Instruct | 3.22 | 3.06 | 3.81 | 3.36 | 3.62 | 3.24 | 3.88 | 3.58 | 3.68 | 3.08 | 3.49 | 3.42 |\n| Response Quality (Reward Model; Skywork-RM-8B Score) |  |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o (2024-08-06) | 13.90 | 23.20 | 27.79 | 21.63 | 8.82 | -0.10 | 8.60 | 5.77 | 4.86 | -7.48 | -4.73 | -2.45 |\n| GPT-4o-mini (2024-07-18) | 5.74 | -1.18 | 7.80 | 4.12 | 13.71 | 23.74 | 20.71 | 19.39 | 3.42 | -12.93 | -5.16 | -4.89 |\n| Claude-3.5-Sonnet (2024-06-20) | 10.67 | 20.22 | 18.20 | 16.36 | 5.56 | 1.67 | 0.17 | 2.46 | 6.29 | -5.31 | 10.76 | 3.92 |\n| Llama-3.1-405B-Instruct | -0.50 | 19.54 | 13.63 | 10.89 | -1.23 | 2.68 | 10.65 | 4.03 | -1.89 | -10.43 | -7.35 | -6.56 |\n| Llama-3.1-70B-Instruct | -2.17 | 20.42 | 11.22 | 9.83 | -3.26 | 2.17 | 5.85 | 1.59 | -3.04 | -11.54 | -8.60 | -7.72 |\n| Llama-3.1-8B-Instruct | -7.71 | 7.16 | 3.45 | 0.97 | -3.89 | -1.53 | 8.72 | 1.10 | -3.68 | -12.61 | -10.15 | -8.81 |\n| Instruction Diversity (c-dist) |  |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o (2024-08-06) | 0.4170 | 0.4640 | 0.3047 | 0.3952 | 0.3737 | 0.3958 | 0.3386 | 0.3694 | 0.4263 | 0.4870 | 0.2943 | 0.4025 |\n| GPT-4o-mini (2024-07-18) | 0.4091 | 0.5127 | 0.3013 | 0.4077 | 0.3737 | 0.3958 | 0.3386 | 0.3694 | 0.4270 | 0.4670 | 0.2956 | 0.3965 |\n| Claude-3.5-Sonnet (2024-06-20) | 0.4124 | 0.4872 | 0.2940 | 0.3979 | 0.3737 | 0.3958 | 0.3386 | 0.3694 | 0.4307 | 0.4903 | 0.2921 | 0.4044 |\n| Llama-3.1-405B-Instruct | 0.3996 | 0.5411 | 0.2789 | 0.4065 | 0.3737 | 0.3958 | 0.3386 | 0.3694 | 0.4344 | 0.4796 | 0.3033 | 0.4058 |\n| Llama-3.1-70B-Instruct | 0.4003 | 0.5015 | 0.2682 | 0.3900 | 0.3737 | 0.3958 | 0.3386 | 0.3694 | 0.4232 | 0.4756 | 0.3018 | 0.4022 |\n| Llama-3.1-8B-Instruct | 0.4201 | 0.4785 | 0.2956 | 0.3981 | 0.3737 | 0.3958 | 0.3386 | 0.3694 | 0.4302 | 0.4619 | 0.2984 | 0.3968 |\n| Response Diversity (c-dist) |  |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o (2024-08-06) | 0.4564 | 0.5347 | 0.2918 | 0.4276 | 0.4126 | 0.4719 | 0.2714 | 0.3853 | 0.4455 | 0.5065 | 0.2271 | 0.3930 |\n| GPT-4o-mini (2024-07-18) | 0.4558 | 0.5719 | 0.2814 | 0.4364 | 0.4095 | 0.4726 | 0.2811 | 0.3877 | 0.4577 | 0.5184 | 0.2257 | 0.4006 |\n| Claude-3.5-Sonnet (2024-06-20) | 0.4719 | 0.5648 | 0.3220 | 0.4529 | 0.4156 | 0.4647 | 0.2788 | 0.3864 | 0.4610 | 0.5141 | 0.2325 | 0.4025 |\n| Llama-3.1-405B-Instruct | 0.4490 | 0.6122 | 0.2523 | 0.4378 | 0.4037 | 0.4737 | 0.2551 | 0.3775 | 0.4633 | 0.5155 | 0.2239 | 0.4009 |\n| Llama-3.1-70B-Instruct | 0.4520 | 0.5771 | 0.2596 | 0.4296 | 0.4012 | 0.4784 | 0.2530 | 0.3775 | 0.4571 | 0.5134 | 0.2233 | 0.3979 |\n| Llama-3.1-8B-Instruct | 0.4768 | 0.5651 | 0.2660 | 0.4360 | 0.4077 | 0.4778 | 0.2530 | 0.3795 | 0.4738 | 0.5143 | 0.2254 | 0.4045 |", "caption": "Table 8: Intrinsic evaluation results of AgoraBench.", "description": "Table 8 presents the detailed results of the intrinsic evaluation conducted as part of the AgoraBench benchmark.  It shows the values for several metrics across different large language models (LLMs) and for three data generation methods (instance generation, response generation, quality enhancement). The metrics include those assessing instruction difficulty (using LLM-as-a-judge scores from both GPT-40 and Prometheus-2-8x7B, and perplexity) and response quality (LLM-as-a-judge scores from GPT-40 and Prometheus-2-8x7B, and Skywork-RM score), as well as instruction and response diversity (using cosine distance). The table offers a granular view of the data quality generated by each LLM, facilitating a deeper understanding of their strengths and weaknesses in various data generation tasks.", "section": "5.2 Can we predict the student model's improvement by looking into the data?"}]