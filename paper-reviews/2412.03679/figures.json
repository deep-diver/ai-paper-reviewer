[{"figure_path": "https://arxiv.org/html/2412.03679/x1.png", "caption": "Figure 1: Illustration of the motivation of AgoraBench: Prior works focused on developing new methods to generate synthetic data. In contrast, our work focuses on systematically comparing different LMs as data generators based on existing data generation methods. Further explanation of data generation methods are covered in Section\u00a02.", "description": "Figure 1 illustrates the core difference between AgoraBench and previous research in using large language models (LLMs) to generate synthetic data.  Prior studies primarily concentrated on creating new data generation techniques, often showcasing their effectiveness through case studies with varying experimental setups. In contrast, AgoraBench systematically compares the data generation capabilities of six different LLMs, using pre-existing data generation methods. This controlled approach allows for a direct comparison of the LLMs' performance, providing valuable insights into their relative strengths and weaknesses as synthetic data generators.  Details on the data generation methods are provided in Section 2 of the paper.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.03679/x2.png", "caption": "Figure 2: AgoraBench tests three data generation methods: generating new instruction and response pairs (left), generating responses (middle), and enhancing the quality of the instruction and/or the response (right).", "description": "The figure illustrates the three data generation methods used in AgoraBench.  The left panel shows instance generation, where a small number of human-written instruction-response pairs are used as seed data to generate many more new pairs. The middle panel depicts response generation, which starts with many instructions without responses and generates responses for each one. The right panel shows quality enhancement, taking a large set of existing instruction-response pairs and improving the quality of either the instructions or responses.", "section": "2 Preliminaries: Measuring Data Generation Capabilities of LMs"}, {"figure_path": "https://arxiv.org/html/2412.03679/x3.png", "caption": "Figure 3: Illustration of Performance Gap Recovered metric: The performance gap recovered metric captures the relative improvement of SDGsubscript\ud835\udc46subscript\ud835\udc37\ud835\udc3aS_{D_{G}}italic_S start_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT end_POSTSUBSCRIPT with respect to Sr\u2062e\u2062fsubscript\ud835\udc46\ud835\udc5f\ud835\udc52\ud835\udc53S_{ref}italic_S start_POSTSUBSCRIPT italic_r italic_e italic_f end_POSTSUBSCRIPT where SDGsubscript\ud835\udc46subscript\ud835\udc37\ud835\udc3aS_{D_{G}}italic_S start_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT end_POSTSUBSCRIPT and Sr\u2062e\u2062fsubscript\ud835\udc46\ud835\udc5f\ud835\udc52\ud835\udc53S_{ref}italic_S start_POSTSUBSCRIPT italic_r italic_e italic_f end_POSTSUBSCRIPT is both trained from S\u00d8subscript\ud835\udc46\u00d8S_{\\text{\\O}}italic_S start_POSTSUBSCRIPT \u00d8 end_POSTSUBSCRIPT.", "description": "The figure illustrates the Performance Gap Recovered (PGR) metric.  PGR measures the relative improvement in performance of a student model (SDG) trained on synthetic data generated by a language model (LM), compared to a reference model (Sref) trained on a different dataset. Both SDG and Sref are derived from the same base model (S\u00d8). The formula highlights how much improvement SDG gains relative to Sref, demonstrating the effectiveness of the LM as a synthetic data generator.", "section": "Preliminaries: Measuring Data Generation Capabilities of LMs"}, {"figure_path": "https://arxiv.org/html/2412.03679/x4.png", "caption": "Figure 4: Problem-solving and data generation capabilities do not strongly correlate:  Linear regression between problem-solving ability and data generation ability scores at multiple granularity levels yields either low R2superscript\ud835\udc452R^{2}italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT values (R2<0.1superscript\ud835\udc4520.1R^{2}<0.1italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < 0.1) or non-significant relationships (p>0.05\ud835\udc5d0.05p>0.05italic_p > 0.05), which indicates that it is hard to predict data generation capabilities only using problem-solving capabilities.", "description": "This figure displays the results of a correlation analysis between a language model's problem-solving ability and its data generation capability. Two types of analyses were performed: a coarse-grained analysis using average scores across all settings, and a fine-grained analysis using individual scores per domain and generation setting.  The results show weak correlations (low R-squared values) and lack statistical significance (p-values > 0.05). This suggests that a model's problem-solving ability is a poor predictor of its data generation performance.", "section": "5 What makes an effective data generator?"}, {"figure_path": "https://arxiv.org/html/2412.03679/x5.png", "caption": "Figure 5: Through a PCA analysis on multiple intrinsic evaluation metrics, we find that there exists interpretable low-dimension principal components that explain the variance of data generation capabilities up to 93.4%.", "description": "Principal Component Analysis (PCA) was performed on several intrinsic metrics to analyze the data generated by different large language models (LLMs).  The PCA revealed that the top five principal components account for 93.4% of the variance in data generation capabilities. These components are interpretable and capture aspects like instruction difficulty, response quality, and diversity.  This analysis indicates that a small number of key factors determine the effectiveness of an LLM for synthetic data generation.", "section": "5.2 Can we predict the student model's improvement by looking into the data?"}, {"figure_path": "https://arxiv.org/html/2412.03679/x6.png", "caption": "Figure 6: Principal Components from Intrinsic Metrics Show Stronger Correlation with Data Generation ability: Linear regression using the weighted top-5 principal components yields a higher explained variance (R2superscript\ud835\udc452R^{2}italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0.325) and statistical significance (p<0.001\ud835\udc5d0.001p<0.001italic_p < 0.001) compared to using problem-solving ability scores alone (R2<0.1superscript\ud835\udc4520.1R^{2}<0.1italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < 0.1 or p>0.05\ud835\udc5d0.05p>0.05italic_p > 0.05; see Figure\u00a04).", "description": "This figure demonstrates that a linear regression model using the top five principal components derived from intrinsic data metrics achieves a significantly higher explained variance (R-squared = 0.325) and statistical significance (p<0.001) in predicting data generation capabilities compared to using problem-solving ability scores alone.  The use of problem-solving ability scores alone resulted in either low R-squared values (less than 0.1) or insignificant results (p>0.05), highlighting the stronger predictive power of the intrinsic metrics. This suggests that analyzing intrinsic properties of generated data provides better insights into a language model's effectiveness as a data generator.", "section": "5 What makes an effective data generator?"}, {"figure_path": "https://arxiv.org/html/2412.03679/x7.png", "caption": "Figure 7: With a fixed budget, generating large amounts of data with weaker LMs could sometimes be more effective and cheaper than generating a few instances with stronger LMs: Since GPT-4o-mini is 17 times cheaper than GPT-4o, generating 50K instances is 3.4 times cheaper than generating 10K instances with GPT-4o. Yet, generating 50K instances with GPT-4o-mini achieves higher PGR in instruction following and math domains compared to generating 10K instances with GPT-4o.", "description": "This figure compares the cost-effectiveness of using different language models (LMs) to generate synthetic training data.  It shows that, while stronger models like GPT-4 might produce higher-quality data with fewer instances, weaker but cheaper models like GPT-4-mini can achieve comparable performance improvements (measured by Performance Gap Recovered or PGR) when generating a much larger dataset. The cost savings from using GPT-4-mini are substantial (17x cheaper than GPT-4), making it a more practical choice for certain scenarios, especially when considering the gains in instruction following and mathematical problem-solving tasks demonstrated in the graph.  The x-axis shows the number of instances generated, and the y-axis displays the performance gain (PGR).", "section": "Further Analysis Experiments"}]