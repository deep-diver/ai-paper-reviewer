[{"figure_path": "https://arxiv.org/html/2503.08525/x2.png", "caption": "Figure 1: Zoom in for more details. Illustration of thought collapse occurring when training a VLM agent to solve 24 points through RL. With RL training, the thoughts generated by the VLM agent quickly lose their diversity, turn out to be incorrect, and lead to invalid actions and negative rewards. For example, checkpoints \u278c and \u278d erroneously predict the same thought (\u201cI should append \u201810\u2019 to the current formula\u201d) and action \u201c10\u201d even in the face of different environment states, catastrophically degrading the agent\u2019s decision-making capabilities and RL\u2019s sample efficiency. We propose Guided Thought Reinforcement (GTR) to prevent this problem.", "description": "Figure 1 illustrates the phenomenon of \"thought collapse\" during reinforcement learning (RL) training of a vision-language model (VLM) agent designed to solve the 24 points card game.  The figure shows that as RL training progresses, the VLM's reasoning process loses diversity.  The model begins producing repetitive and incorrect thoughts, even when presented with different game states.  This leads to invalid actions and negative rewards, severely hindering the agent's ability to learn effectively.  Specifically, checkpoints 3 and 4 highlight this issue, where the same thought and action are generated despite distinct starting card arrangements. This demonstrates the inefficiency of RL training in this context and motivates the need for the proposed Guided Thought Reinforcement (GTR) method to address this problem.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.08525/x3.png", "caption": "Figure 2: Thought collapse persists for larger model scales and training budgets. We train LLaVA 7b and 13b models for 30k RL steps but still observe the degraded performance.", "description": "Figure 2 presents the results of training two different sized Large Language Models (LLMs) using reinforcement learning.  The models, LLaVa 7B and LLaVa 13B, were trained for 30,000 reinforcement learning steps.  The graphs display both the episode return (a measure of cumulative reward) and success rate (the percentage of successful task completions) over the training process.  Despite the increased model size and extensive training, both models exhibit a phenomenon called \"thought collapse\", where their reasoning capabilities degrade over time.  The success rate plateaus or even decreases, suggesting that longer training does not mitigate this issue, and the episode return also shows suboptimal performance. This highlights the limitations of relying solely on outcome-based rewards in training LLMs.", "section": "3. Thought Collapse"}, {"figure_path": "https://arxiv.org/html/2503.08525/x4.png", "caption": "Figure 3: Overview of the GTR framework. We modify the RL finetuning (orange region) of VLM agents by introducing automated thought correction (green region) as guidance, leveraging an off-the-shelf VLM model as a corrector (purple region). GTR performs SFT updating for the agent\u2019s thought tokens and PPO updating for its action tokens, thereby training thoughts and actions simultaneously.", "description": "This figure illustrates the Guided Thought Reinforcement (GTR) framework, which enhances the training of Vision-Language Models (VLMs) in reinforcement learning.  The core idea is to guide the VLM's learning process by incorporating automated thought correction.  The framework modifies standard RL finetuning by adding a VLM-based corrector (purple).  This corrector evaluates the VLM agent's (orange) thoughts (green) and provides refined thoughts.  The GTR framework uses supervised fine-tuning (SFT) to update the agent's thought tokens and Proximal Policy Optimization (PPO) to update its action tokens, thus simultaneously training both the reasoning process and action selection.", "section": "4. Guided Thought Reinforcement"}, {"figure_path": "https://arxiv.org/html/2503.08525/x7.png", "caption": "Figure 4: Comparison among different process guidance methods in the 24 points card game. Trivial numerical rewards provided by the VLM judge and rule-based evaluation cannot incentivize the agent\u2019s reasoning thoughts and higher success rate.", "description": "Figure 4 presents a comparison of different process guidance methods within the context of a 24 points card game.  The graph displays the success rate and discounted episode return achieved by three different approaches: pure reinforcement learning (RL), using a VLM-as-a-corrector model (GTR), and utilizing a VLM-as-a-judge model.  The key takeaway is that simply providing numerical rewards (as in the VLM-as-a-judge and pure RL methods), without explicit guidance on the reasoning process, is insufficient to encourage diverse and effective chain-of-thought reasoning. The GTR method, which incorporates an automated corrector for thought refinement, significantly outperforms the other methods in terms of both success rate and cumulative reward.", "section": "4.2 Process Guidance from a VLM Corrector"}, {"figure_path": "https://arxiv.org/html/2503.08525/x8.png", "caption": "Figure 5: Training curves on the 24 points game environment. Compared to the baseline methods, our GTR framework integrating process guidance with RL achieves better performance while maintaining a rational reasoning process. Curves are smoothed for better readability. Since GTR and SFT-only employ truncation strategies, we plot \u03b3=0.9\ud835\udefe0.9\\gamma=0.9italic_\u03b3 = 0.9 discounted returns in the figure for a fair comparison.", "description": "Figure 5 presents a comparison of training curves for different methods on the 24 Points game.  The x-axis represents training steps, and the y-axis shows both success rate and discounted episode return (with a discount factor of 0.9).  The plot showcases that the GTR (Guided Thought Reinforcement) framework significantly outperforms baseline methods (pure RL, SFT-only, and RL4VLM), achieving higher success rates and returns.  The curves are smoothed for clarity.  Importantly,  because GTR and SFT-only use truncation strategies (ending episodes early in certain situations), the discounted returns are used for a fairer comparison with methods that do not employ truncation.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.08525/x9.png", "caption": "Figure 6: Evaluation result of different models on the Points24 task. GTR demonstrates significant advantages over other methods in both task success rate and episode returns. SR - success rate, ER - episode return, * - reported in previous work.", "description": "Figure 6 presents a bar chart comparing the performance of various models on the Points24 task.  The chart displays two key metrics: success rate (SR) and episode return (ER).  GTR significantly outperforms other methods such as RL4VLM, Gemini, GPT4-V, and GPT40, achieving a substantially higher success rate and episode return.  The asterisk (*) indicates results reported in previous studies, highlighting GTR's superior performance compared to existing state-of-the-art models.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.08525/x10.png", "caption": "Figure 7: Comparison of training curves between GTR and RL4VLM in the ALFWorld environment. RL4VLM fails to effectively facilitate model learning, leading to thought collapse. GTR, however, enables the agent\u2019s performance to improve steadily, ultimately achieving superior results.", "description": "Figure 7 presents a comparison of the training performance between two reinforcement learning (RL) approaches: GTR (Guided Thought Reinforcement) and RL4VLM, within the ALFWorld environment.  The x-axis represents training steps, and the y-axis shows both success rate and discounted episode return.  The plots reveal that RL4VLM, lacking process guidance, suffers from \"thought collapse\"\u2014a phenomenon where the model's reasoning becomes erratic and ineffective, resulting in stagnant or declining performance. In contrast, GTR, which incorporates automated thought correction, shows a steady improvement in both success rate and episode return, demonstrating that this approach mitigates thought collapse and enhances model learning. The superior performance of GTR highlights the importance of providing process guidance during RL training for visual language models (VLMs).", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.08525/x11.png", "caption": "Figure 8: Comparison of success rates across different models in the ALFWorld environment. We present the peak performance in the training curve for RL methods. \u2714/\u2717 denotes whether the environment gives textual descriptions of the current observation alongside visual images, which assists the agent\u2019s decision-making. * - reported in previous work.", "description": "This figure compares the success rates of various models on tasks within the ALFWorld environment.  The peak performance achieved during training is shown for reinforcement learning (RL) models.  The '\u2714/\u2717' symbol indicates whether the environment provided both visual images and textual descriptions of the current state; textual descriptions make the task easier. Results from previous research are marked with an asterisk (*).", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.08525/x12.png", "caption": "Figure 9: Ablation study of the GTR framework. Its superior performance highlights the importance of full-process thought guidance, task-specific knowledge, and DAgger.", "description": "Figure 9 presents an ablation study analyzing the impact of different components within the Guided Thought Reinforcement (GTR) framework. The results demonstrate that all three components\u2014full-process thought guidance, task-specific knowledge, and the DAgger algorithm\u2014are crucial for GTR's superior performance. Removing any one of them significantly degrades the model's ability to achieve high success rates and rewards.", "section": "5.3. Ablation Study"}]