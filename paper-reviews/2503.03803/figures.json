[{"figure_path": "https://arxiv.org/html/2503.03803/x2.png", "caption": "Figure 1: \nThe Overview of EgoLife Project. The EgoLife project features six participants living together for a week to prepare an Earth Day celebration. Each participant wears Meta Aria glasses\u00a0[1], recording approximately 8 hours of egocentric video and signals daily. In addition, 15 cameras and 2 mmWave devices provide synchronized third-person perspective data (detailed in Figure\u00a02). These comprehensive annotations enable the development of state-of-the-art multimodal egocentric AI assistants and introduce novel tasks to advance long-term egocentric life assistance, as illustrated in the EgoLife task board.", "description": "The EgoLife project involves six participants living together for a week, documenting their daily lives using Meta Aria glasses to capture egocentric video and sensor data.  Fifteen additional cameras and two mmWave devices simultaneously record third-person views, creating a rich multimodal dataset.  The data supports development of advanced egocentric AI assistants and introduces new research tasks focused on long-term life assistance.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.03803/x25.png", "caption": "Figure 2: 3D reconstruction of the shared house using Aria Multi-MPS\u00a0[1], showcasing the locations of 15 Exo cameras in the common area and 2 mmWave devices (highlighted in red) on the second floor. Color-coded 10-minute participant traces are also displayed.", "description": "This 3D model of the EgoLife house shows the placement of 15 standard cameras and two mmWave radar devices within the house.  The cameras are spread throughout the common areas to capture third-person perspectives of participant activities. The mmWave radar devices, highlighted in red, are located on the second floor for additional motion capture data. Color-coded lines represent 10-minute movement tracks for each participant, indicating their movements within the space during the study.", "section": "3. Data Collection"}, {"figure_path": "https://arxiv.org/html/2503.03803/x26.png", "caption": "Figure 3: The Activity Timeline of the EgoLife Dataset. It visualizes the activity timeline of six participants over one week. Each block represents a 20-minute interval, color-coded and marked with icons for different activities. The legend shows 14 activity categories with their total occurrence counts. The categorization is automatically performed using GPT-4o on visual-audio captions with timestamps.", "description": "This figure presents a visual overview of the activities undertaken by six participants over a week-long period within the EgoLife project.  Each horizontal band represents a single participant's activities, and the entire timeline spans seven days.  The timeline is divided into 20-minute intervals, with each interval color-coded and labeled with an icon to represent the type of activity (e.g., social, housekeeping, cooking, etc.).  A legend provides a complete list of the 14 activity categories used, along with the total number of times each activity occurred throughout the week. Notably, the activity categorization was automatically determined using GPT-4, which analyzed both the audio and video data associated with each 20-minute interval.", "section": "3. Data Collection"}, {"figure_path": "https://arxiv.org/html/2503.03803/x27.png", "caption": "Figure 4: The Overview of Data Process Pipeline. The pipeline synchronizes multi-source data (video, audio, IMU) from Aria glasses and GoPro cameras using EgoSync codebase, processes them through privacy protection (EgoBlur), dense captioning (EgoCaption), and transcription (EgoTranscript) modules, ultimately feeding into the EgoLifeQA system.", "description": "The figure illustrates the data processing pipeline for the EgoLife project.  It begins with the synchronization of data streams from Meta Aria glasses and GoPro cameras using the EgoSync codebase.  This synchronized data (video, audio, and IMU) then undergoes privacy protection via the EgoBlur module.  Next, the data is processed by EgoCaption for dense caption generation and by EgoTranscript for transcription. Finally, the processed and annotated data is fed into the EgoLifeQA system for further analysis and task completion.", "section": "3. Data Collection"}, {"figure_path": "https://arxiv.org/html/2503.03803/x28.png", "caption": "Figure 5: Question Types and Examples in the EgoLifeQA Benchmark. We design five types of questions to evaluate egocentric assistants\u2019 capabilities in entity logging, event recall, task tracking, and human-centric problems (habit analysis and relationship understanding). Each example includes a multiple-choice Q&A with supporting evidence from timestamps at least 5 minutes prior to the question. Black vertical lines indicate question timestamps, while colored curved lines connect to relevant evidence timestamps.", "description": "This figure showcases the five question types within the EgoLifeQA benchmark designed to assess an egocentric AI assistant's capabilities.  These question types cover entity logging (tracking objects), event recall (remembering past events), task tracking (monitoring ongoing tasks), and two human-centric problem areas: habit analysis (identifying personal habits) and relationship understanding (analyzing social interactions). Each example displays a multiple-choice question with its answer, along with timestamps linking the answer back to relevant evidence in the recorded data.  The timestamps are at least 5 minutes before the question to demonstrate the AI's ability to use long-term contextual information.  Vertical black lines mark question timestamps; colored arcs connect to relevant evidence timestamps.", "section": "3.5 EgoLifeQA Annotations"}, {"figure_path": "https://arxiv.org/html/2503.03803/x29.png", "caption": "Figure 6: Statistics of EgoLifeQA. We gathered 500 long-context QAs per participant, totaling 3K QAs. The sum of QAs for each question type is reported. In the pie chart, darker segments indicate the proportion of questions requiring audio. The bar chart presents the daily count of QAs per question type, with brightness levels reflecting 4-level certification length\u00a0[11] (from <<<2h to >>>24h).", "description": "The figure presents a statistical overview of the EgoLifeQA dataset, showing the distribution of different question types and their temporal characteristics.  It features two visualizations: a pie chart illustrating the proportion of questions that require audio input and a bar chart depicting the daily distribution of questions across question types, with color intensity indicating the certification length (time span between the question and relevant evidence).  The dataset comprises 3000 QAs (500 per participant).", "section": "3. The EgoLife Dataset & Benchmark"}, {"figure_path": "https://arxiv.org/html/2503.03803/x30.png", "caption": "Figure 7: The EgoBulter Architecture. The system comprises (a) a Captioning Stage powered by EgoGPT for dense visual-audio understanding of egocentric clips, and (b) a Question Answering Stage utilizing EgoRAG for memory retrieval and response generation. The example demonstrates temporal reasoning across multiple days, with keyword extraction, evidence retrieval, and context-aware answer generation for a breakfast-related query.", "description": "EgoButler is a two-stage system. First, EgoGPT processes egocentric video clips to generate detailed visual-audio captions. Then, EgoRAG retrieves relevant information from these captions based on user queries, enabling it to answer questions requiring temporal reasoning across multiple days. The figure shows an example of how EgoButler answers a breakfast-related question by extracting keywords, retrieving evidence from past days' captions, and generating a context-aware response.", "section": "4. EgoButler: Agentic Egocentric Life Assistant"}, {"figure_path": "https://arxiv.org/html/2503.03803/x31.png", "caption": "Figure 8: Qualitative Comparison of EgoGPT and Gemini-1.5-Pro under the EgoButler Framework. The top section compares captions from two models on a 30-second clip: EgoGPT excels in personalization and hallucinates less on the egocentric videos. The bottom section features a question that is answered by the clip, showcasing EgoRAG\u2019s skill in pinpointing relevant time slots and key clues.", "description": "Figure 8 demonstrates a qualitative comparison of EgoGPT and Gemini-1.5-Pro within the EgoButler framework.  The top half shows a side-by-side comparison of captions generated by each model for the same 30-second egocentric video clip. This highlights EgoGPT's superior performance in generating personalized captions that accurately reflect the content of the video, while also exhibiting fewer hallucinations (inventions of facts not present in the source video). The bottom half presents a question-answering scenario.  It illustrates how EgoRAG, a retrieval-augmented generation component, leverages temporal information from the video to correctly identify and utilize relevant time segments to answer the question accurately.", "section": "4. EgoButler: Agentic Egocentric Life Assistant"}, {"figure_path": "https://arxiv.org/html/2503.03803/x32.png", "caption": "Figure A1: The Overview of Egocentric Datasets. The figure summarizes the domain, modality, annotation type, release time, dataset statistics, and other aspects of datasets, providing a comprehensive view of existing egocentric datasets.", "description": "This figure provides a comprehensive overview of existing egocentric datasets.  It presents a timeline of dataset releases, highlighting the evolution of data modalities (video, RGB-D, gaze, IMU, 3D scans, touch, point clouds), annotation types (activity labels, bounding boxes, pixel-level masks, timestamps, narrations, QA pairs), and the expansion of domains captured (sports, kitchen activities, assembly, daily life, interactions). The figure also summarizes key features and statistics of each dataset, such as the number of videos, duration, and types of annotations. This visualization helps readers understand the context of the EgoLife dataset by showing its position relative to other egocentric datasets in terms of scope, complexity, and features.", "section": "G. History of Egocentric Datasets"}]