[{"figure_path": "https://arxiv.org/html/2501.04575/x1.png", "caption": "Figure 1: \nInfiGUIAgent is trained in two stages. Stage 1 cultivates fundamental abilities using diverse datasets covering GUI understanding (element recognition and layout comprehension), question answering, instruction grounding, general knowledge, and tool usage. Stage 2 introduces native advanced reasoning, employed during both training and inference. This stage follows a cyclical process at each step, consisting of Reflection, Hierarchical Reasoning (strategic and tactical layers), Action, and Expectation. Each step receives the overall task, the history of previous screenshots and reasoning, and the current environment as input. Reflection assesses the previous action\u2019s outcome against its expectation, while Expectation predicts the outcome of the current action for subsequent reflection.", "description": "InfiGUIAgent uses a two-stage training process. Stage 1 focuses on fundamental skills like GUI understanding, question answering, and instruction grounding using diverse datasets.  Stage 2 introduces advanced reasoning capabilities (hierarchical and expectation-reflection reasoning).  During inference, the agent iteratively performs reflection (comparing results to expectations), hierarchical reasoning (strategic and tactical planning), actions, and expectation setting (predicting the next outcome). Each step uses the overall task, past interactions, and current context as input.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2501.04575/x2.png", "caption": "Figure 2: Case of GUI Understanding.", "description": "This figure showcases the InfiGUIAgent's ability to understand a GUI. The screenshot displays the settings menu of an Android device, and the caption details key UI components and their locations, demonstrating the agent's capacity to accurately interpret and describe the interface elements.", "section": "A.1 Stage 1: Fundamental Abilities"}, {"figure_path": "https://arxiv.org/html/2501.04575/x3.png", "caption": "Figure 3: Case of Grounding.", "description": "The figure demonstrates the agent's ability to ground actions within a GUI.  The instruction asks the agent to identify where to send a message. The agent correctly identifies the messaging app icon at the bottom-left of the home screen and explains its function in relation to sending messages.", "section": "A Cases: Stage 1: Fundamental Abilities"}, {"figure_path": "https://arxiv.org/html/2501.04575/x4.png", "caption": "Figure 4: Case of Question Answering.", "description": "This figure showcases the InfiGUIAgent's ability to answer questions based on the visual information provided in a screenshot.  The screenshot shows a portion of an Android phone's home screen displaying the time and date. The question posed is 'What day is it tomorrow?', and the model correctly identifies the current day (Sunday) from the screen and answers 'Monday'.  The answer highlights the InfiGUIAgent's ability to extract relevant information from the visual UI and perform simple reasoning tasks.", "section": "A Cases. A.1 Stage 1: Fundamental Abilities"}, {"figure_path": "https://arxiv.org/html/2501.04575/x5.png", "caption": "Figure 5: Case of Native Advanced Reasoning. The agent\u2019s goal is to reply to a message", "description": "This figure demonstrates the InfiGUIAgent's advanced reasoning capabilities in a messaging app scenario.  The agent must identify the relevant UI element (the 'Start chat' button) and perform the correct action (tapping the button) to initiate a reply.  The figure displays the agent's reasoning process, detailing the steps involved: reflection on the previous action, hierarchical reasoning to determine the next steps, and expectation of the outcome. This illustrates the agent's ability to understand context, plan actions, and adapt its behavior to achieve the goal of replying to a message.", "section": "A.2 Stage 2: Native Reasoning"}, {"figure_path": "https://arxiv.org/html/2501.04575/x6.png", "caption": "Figure 6: Case of Native Advanced Reasoning. The agent\u2019s goal is to create a new contact.", "description": "This figure illustrates an example of InfiGUIAgent performing advanced reasoning to achieve the goal of creating a new contact.  It shows the agent's reasoning process, broken down into strategic and tactical layers, as it navigates the phone app's interface to locate and tap the 'Contacts' tab, and subsequently initiates the 'Create new contact' process.  The steps highlight how the agent uses hierarchical reasoning to decompose the task and combines this with grounding, referencing specific visual elements to perform actions.", "section": "A.2 Stage 2: Native Reasoning"}, {"figure_path": "https://arxiv.org/html/2501.04575/x7.png", "caption": "Figure 7: Case of Native Advanced Reasoning. The agent\u2019s goal is to create a new contact.", "description": "This figure shows a screenshot of a smartphone screen, illustrating the step-by-step process of creating a new contact using InfiGUIAgent.  The agent first successfully navigated to the 'Contacts' section, and in this specific step (K+1), it proceeds to initiate the process by tapping the 'Create new contact' button. This illustrates the agent's hierarchical reasoning and grounding capabilities by showing the action and its expected outcome within the broader context of creating a contact.  Details like the reasoning process, the grounding details of the button's location and visual features, and the action itself as a JSON formatted instruction, are all displayed in the caption.", "section": "A.2 Stage 2: Native Reasoning"}]