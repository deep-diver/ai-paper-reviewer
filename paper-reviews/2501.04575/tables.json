[{"content": "| ![logo](https://arxiv.org/html/2501.04575/logo.png) | **InfiGUIAgent: A Multimodal Generalist GUI Agent** | \n| | **with Native Reasoning and Reflection** |", "caption": "Table 1: Training datasets used in stage 1 of supervised fine-tuning.", "description": "This table lists the datasets used in Stage 1 of the InfiGUIAgent training process.  It details the platform (Webpage, Mobile, or both) each dataset was collected from, the category of the data (GUI Understanding, Grounding, Question Answering, General Knowledge, or Tool Usage), and the number of samples available in each dataset. The datasets cover various aspects of GUI interaction, including visual understanding, task execution, and general knowledge.", "section": "3.1 Stage 1: Training for Fundamental Abilities"}, {"content": "| Dataset | Platform | Category | # of Samples |\n|---|---|---|---| \n| _GUI-related Datasets_ |  |  |  |\n| GUIEnv (Chen et al., 2024) | Webpage | Grounding | 150,000 |\n| RICO Semantic Annotation (Sunkara et al., 2022) | Mobile | Grounding | 150,000 |\n| SeeClick-Web (Cheng et al., 2024) | Webpage | Grounding | 100,000 |\n| RICO SCA (Li et al., 2020a) | Mobile | Grounding | 100,000 |\n| Widget Caption (Li et al., 2020b) | Mobile | Grounding | 70,000 |\n| GUIChat (Chen et al., 2024) | Webpage | QA | 40,000 |\n| ScreenQA (Hsiao et al., 2022) | Mobile | QA | 17,000 |\n| UIBert Reference Expression (Bai et al., 2021) | Mobile & Mobile | Grounding | 16,000 |\n| Screen2Words (Wang et al., 2021) | Mobile | Understanding | 12,000 |\n| Complex QA (Yin et al., 2023) | Mobile | QA | 11,000 |\n| Screen Annotation (Baechler et al., 2024) | Mobile | Understanding | 5,400 |\n| OmniAct-Single Click (Kapoor et al., 2024) | Webpage & Desktop | Grounding | 4,800 |\n| _Non-GUI Datasets_ |  |  |  |\n| LLaVA-OneVision (Li et al., 2024a) | - | General | 250,000 |\n| PixMo (MDeitke et al., 2024) | - | General | 68,800 |\n| Glaive-function-calling (Glaive AI, 2024) | - | Tool Usage | 5,000 |", "caption": "Table 2: UI action reasoning datasets used in the training process", "description": "This table lists the datasets used in Stage 2 of the InfiGUIAgent training process, focusing on enhancing advanced reasoning capabilities.  The datasets provide examples of hierarchical and expectation-reflection reasoning, crucial for effective GUI interaction. Each dataset includes information on the platform (Webpage or Mobile) and the number of samples provided, contributing to the agent's ability to perform complex multi-step tasks.", "section": "3.2 Stage 2: Training for Native Reasoning"}, {"content": "| Dataset | Platform | # of Samples |\n|---|---|---|\n| GUIAct (Chen et al., 2024) | Webpage & Mobile | 10,000 |\n| AMEX (Chai et al., 2024) | Mobile | 3,000 |\n| Android in the Zoo (Zhang et al., 2024a) | Mobile | 2,000 |\n| Composition: Stage 1-aligned | - | 30,000 |", "caption": "Table 3: Categorization of actions in the action space.", "description": "This table categorizes the different types of actions that an agent can take within the GUI environment.  It breaks down actions into six categories based on their characteristics: single-point operations (e.g., tap, click), two-point operations (e.g., swipe, select text), directional operations (e.g., scroll), text input operations, parameterless operations (e.g., remember, enter), and state settings (e.g., set task status).  Each category lists specific examples of actions it includes. This provides a structured understanding of the action space available to the GUI agent, showing how various interactions are classified and what parameters they may take.", "section": "3.2.4 Modular Action Space"}, {"content": "| Category | Operations |\n|---|---| \n| **Single-point operations** | `tap`, `click`, `hover`, `select` |\n| **Two-point operations** | `swipe`, `select_text` |\n| **Directional operations** | `scroll` |\n| **Text input** | `input`, `point_input` |\n| **Parameterless operations** | `remember`, `enter`, `home`, `back` |\n| **State settings** | `set_task_status` |", "caption": "Table 4: Performances on various platforms (Mobile, Desktop, Web) on Screenshot. All experiments were conducted using raw screenshot information. Results marked in bold represent the best performance, and those underlined indicate the second-best performance.", "description": "This table presents a comparison of the performance of different models on the ScreenSpot benchmark across various platforms (Mobile, Desktop, and Web) and for different element types (Text and Icon).  The accuracy of each model is evaluated using raw screenshot information, without any additional metadata or augmentations.  The best performing model for each category is highlighted in bold, while the second-best is underlined. This highlights the effectiveness of the models in understanding and interacting with graphical user interfaces on different device types.", "section": "4.2 Main Results"}, {"content": "| Model | Mobile Text | Mobile Icon | Desktop Text | Desktop Icon | Web Text | Web Icon | Avg. |\n|---|---|---|---|---|---|---|---| \n| *Proprietary Models* |  |  |  |  |  |  |  |\n| GPT-4o<sup>1</sup> (OpenAI, 2024) | 30.5 | 23.2 | 20.6 | 19.4 | 11.1 | 7.8 | 18.8 |\n| Gemini-1.5-pro<sup>2</sup> (Team et al., 2024) | 76.2 | 54.1 | 65.5 | 39.3 | 52.2 | 32.0 | 53.2 |\n| *Open-source Models* |  |  |  |  |  |  |  |\n| Qwen2-VL-2B (Wang et al., 2024) | 24.2 | 10.0 | 1.4 | 9.3 | 8.7 | 2.4 | 9.3 |\n| Qwen2-VL-7B (Wang et al., 2024) | 61.3 | 39.3 | 52.0 | 45.0 | 33.0 | 21.8 | 42.9 |\n| CogAgent (Hong et al., 2024) | 67.0 | 24.0 | 74.2 | 20.0 | 70.4 | 28.6 | 47.4 |\n| SeeClick (Cheng et al., 2024) | 78.0 | 52.0 | 72.2 | 30.0 | 55.7 | 32.5 | 53.4 |\n| UGround-7B (Gou et al., 2024) | 82.8 | 60.3 | 82.5 | 63.6 | 80.4 | 70.4 | 73.3 |\n| ShowUI-2B (Lin et al., 2024) | 92.3 | 75.5 | 76.3 | 61.1 | 81.7 | 63.6 | 75.1 |\n| *Ours* |  |  |  |  |  |  |  |\n| **InfiGUIAgent-2B** | 88.6 | 74.7 | 85.6 | 65.0 | 79.1 | 64.6 | 76.3 |", "caption": "Table 5: Performances on AndroidWorld.", "description": "This table presents a comparison of the success rates achieved by different models on the AndroidWorld benchmark.  The benchmark consists of 116 programmatic tasks across 20 real-world Android applications.  The table shows the success rates broken down by task difficulty level (Easy, Middle, Hard) for several models including InfiGUIAgent, as well as several open-source baselines.  The overall success rate is also shown for each model.  Note that the experiments were conducted using raw screenshots without any additional GUI metadata.", "section": "4.2 Main Results"}]