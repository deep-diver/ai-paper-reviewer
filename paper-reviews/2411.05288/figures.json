[{"figure_path": "https://arxiv.org/html/2411.05288/x1.png", "caption": "Figure 1: Repeating pattern in an imbalanced pipeline. Bubbles are incurred due to an extra output layer in the last pipeline stage.", "description": "Figure 1 illustrates the repeating pattern of an imbalanced pipeline caused by an extra output layer in the final stage.  This extra layer leads to an uneven distribution of workload across pipeline stages.  The stages with fewer layers have less computation, creating idle time or \"bubbles\" in the pipeline. This reduces overall efficiency and throughput.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2411.05288/x2.png", "caption": "Figure 2: Ratio of compute and memory of vocabulary layers compared to transformer layers in Gemma2-9B.", "description": "This figure shows a comparison of the computational and memory requirements of vocabulary layers relative to transformer layers in the Gemma2-9B language model.  It illustrates how the compute and memory demands of the vocabulary layers scale significantly with increasing vocabulary size, underscoring the memory imbalance issue highlighted in the paper.  This imbalance is more pronounced in larger vocabulary scenarios, demonstrating the need for the proposed Vocabulary Parallelism method.", "section": "2 RELATED WORK"}, {"figure_path": "https://arxiv.org/html/2411.05288/x5.png", "caption": "Figure 3: Transformer Layer Redistribution for a 7B GPT-like model with vocabulary size 128k. In this case, each stage has 2 transformer layers, while output layer is equivalent to 2.4x of transformer layer on compute and 2.6x on parameter memory.", "description": "This figure illustrates how transformer layers are redistributed in a 7B parameter GPT-like model with a vocabulary size of 128k to balance the computational load across pipeline stages.  The redistribution aims to mitigate the imbalance caused by the vocabulary layers, which typically have disproportionately high computational and memory requirements compared to the transformer layers.  The bar chart visually represents the compute requirements (in terms of time) and memory usage (parameter memory and activation memory) for each pipeline stage.  We can observe that, after redistribution, each stage has roughly two transformer layers, ensuring a relatively even distribution of workload, while the output layer remains slightly more computationally expensive than an average transformer layer.", "section": "Balancing Vocabulary Layers"}, {"figure_path": "https://arxiv.org/html/2411.05288/x6.png", "caption": "Figure 4: Computation graph of the output layer after partitioning across the vocabulary dimension. There are three all-reduce communications across all devices.", "description": "This figure illustrates the computation graph of the output layer after it's been partitioned across multiple devices based on the vocabulary dimension.  The process involves three steps.  First, each device performs a matrix multiplication independently. Second, the maximum and sum of logits are computed via all-reduce operations, which require communication between all devices.  Finally, the softmax function is calculated, followed by another all-reduce, and the weight gradient is computed. This highlights how the vocabulary layer's parallelization introduces significant communication overhead.", "section": "Vocabulary Parallelism in Pipeline Parallelism"}, {"figure_path": "https://arxiv.org/html/2411.05288/x7.png", "caption": "Figure 5: Overlapping all-reduce communication with transformer layer computation.", "description": "This figure illustrates how the all-reduce communication barriers inherent in the vocabulary layer computations can be overlapped with the computations of the transformer layers. By strategically placing these communications in a separate stream (Stream 2), as shown in the figure, the idle time caused by waiting for all-reduce operations is minimized, thereby improving the overall efficiency of the pipeline. Stream 1 shows transformer layer computations, while Stream 2 depicts all-reduce operations within the vocabulary layer. This technique is crucial in balancing pipeline parallelism with vocabulary parallelism, leading to reduced activation memory overhead and enhanced throughput.", "section": "3 VOCABULARY PARALLELISM IN PIPELINE PARALLELISM"}, {"figure_path": "https://arxiv.org/html/2411.05288/x29.png", "caption": "Figure 6: Scheduling dependencies in the na\u00efve output layer implementation.", "description": "This figure illustrates the computational and communication dependencies in a naive implementation of the output layer, specifically focusing on the impact of partitioning the layer across multiple devices within a pipeline parallel system.  The figure visually demonstrates how all-reduce communication barriers between devices, arising from operations like computing the maximum and sum of logits, create sequential dependencies that hinder efficient parallel processing and can lead to increased activation memory consumption.  Each box represents a computational operation or communication barrier, and the arrows depict dependencies and the flow of data.  The figure highlights the need for optimization strategies (as presented in later sections of the paper) to reduce or eliminate these communication barriers and improve the efficiency of the pipeline parallel system.", "section": "4 VOCABULARY PASSES CONSTRUCTION"}, {"figure_path": "https://arxiv.org/html/2411.05288/x30.png", "caption": "Figure 7: Computation order in the output layer for a single microbatch, corresponding to the na\u00efve implementation, Algorithm 1 and Algorithm 2 respectively.", "description": "Figure 7 illustrates the computation flow within the output layer for a single microbatch, comparing three different approaches: the naive method, Algorithm 1, and Algorithm 2.  It highlights how each algorithm handles the computation and communication dependencies (specifically all-reduce operations) within the output layer to improve efficiency. The figure shows the order in which the computational steps (F1, F2, B, etc.) and communication steps (broadcast and all-reduce) are executed. It visualizes the differences in computational flow and barrier locations resulting from various optimization strategies implemented in Algorithms 1 and 2, contrasted with the naive approach.", "section": "4 Vocabulary Passes Construction"}, {"figure_path": "https://arxiv.org/html/2411.05288/x31.png", "caption": "Figure 8: Scheduling Dependencies in Algorithms 1 and 2.", "description": "Figure 8 illustrates the scheduling dependencies for a single microbatch using Algorithms 1 and 2, which are methods for optimizing the output layer in pipeline parallelism.  Algorithm 1 introduces two communication barriers (C1 and C2), while Algorithm 2 optimizes to only one barrier (C1). The figure shows how the forward (F) and backward (B) passes of the transformer layer interact with the vocabulary layer passes (S and T) within each algorithm. It highlights the dependencies between these passes and demonstrates how the number of communication barriers impacts the overall scheduling.", "section": "5 Pipeline Scheduling"}]