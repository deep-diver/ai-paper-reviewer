[{"content": "| Pipelines (GPUs) | 8 | 16 | 32 |\n|---|---|---|---|\n| Model Size | \u2248 4B | \u2248 10B | \u2248 21B |\n| Layers | 32 | 48 | 64 |\n| Attention Heads | 24 | 32 | 40 |\n| Hidden Size | 3072 | 4096 | 5120 |\n| Sequence Length | 2048 / 4096 | 2048 / 4096 | 2048 / 4096 |\n| Microbatch Size | 1 | 1 | 1 |\n| Number of Microbatches | 128 | 128 | 128 |\n| Vocabulary Size | 32k / 64k / 128k / 256k | 32k / 64k / 128k / 256k | 32k / 64k / 128k / 256k |", "caption": "Table 1: Settings used in experiments on 1F1B schedule.", "description": "This table details the configurations used in the experiments based on the 1F1B pipeline scheduling.  It lists the number of GPUs used, model sizes (approximate parameter count), number of layers, attention heads, hidden size, sequence length, microbatch size, number of microbatches, and vocabulary size for various experimental settings. This information is crucial for understanding the scale and scope of the experiments conducted in the study.", "section": "6 Experiments"}, {"content": "| Pipelines (GPUs) | 16 | 24 | 32 |\n|---|---|---|---|\n| Model Size | \u2248 7B | \u2248 16B | \u2248 30B |\n| Layers | 32 | 48 | 64 |\n| Attention Heads | 32 | 40 | 48 |\n| Hidden Size | 4096 | 5120 | 6144 |\n| Sequence Length | 2048 / 4096 | 2048 / 4096 | 2048 / 4096 |\n| Microbatch Size | 1 | 1 | 1 |\n| Number of Microbatches | 128 | 128 | 128 |\n| Vocabulary Size | 32k / 64k / 128k / 256k | 32k / 64k / 128k / 256k | 32k / 64k / 128k / 256k |", "caption": "Table 2: Settings used in experiments on V-Half schedule.", "description": "This table details the configurations used in the experiments conducted using the V-Half scheduling algorithm.  It specifies the number of GPUs (pipeline parallelism), the model size, the number of layers, attention heads, hidden size, sequence length, micro-batch size, number of micro-batches, and vocabulary size used in the various experimental runs. These parameters define the different scales and configurations at which the performance of the V-Half schedule was evaluated.", "section": "6.2 Setup"}, {"content": "| Seq | Layer | 8GPU | 16GPU | 32GPU |\n|---|---|---|---|---|\n| 2048 | Output-Vocab-1 | 91.29% | 84.22% | 80.59% |\n|  | Output-Vocab-2 | 86.72% | 79.84% | 75.93% |\n|  | Input | 39.99% | 28.85% | 15.18% |\n| 4096 | Output-Vocab-1 | 93.21% | 88.02% | 85.24% |\n|  | Output-Vocab-2 | 88.36% | 83.42% | 79.66% |\n|  | Input | 27.69% | 15.52% | 8.35% |", "caption": "Table 3: The scaling factor of vocabulary layer computation relative to linear scaling on sequence lengths 2048 and 4096.", "description": "This table presents the scaling efficiency of vocabulary layer computations (both input and output) in the Vocabulary Parallelism method.  It compares the achieved throughput of these computations against a theoretical ideal of perfect linear scaling. The results are broken down by the number of GPUs (8, 16, and 32), sequence length (2048 and 4096), and whether the forward-only (VOCAB-1) or forward-backward (VOCAB-2) pass optimization was used.  The values represent the percentage of the ideal linear speedup obtained.", "section": "6.5 Scaling Analysis of Vocabulary Layers"}, {"content": "| Layer Type | Compute FLOPs | Param Memory |\n|---|---|---|\n| Transformer | bsh(72h+12s) | 24h<sup>2</sup> |\n| Input | 3bsh | 2hV |\n| Output | 6bshV | 2hV |", "caption": "Table 4: Compute and memory cost of vocabulary and transformer layers", "description": "This table presents a quantitative analysis of the computational and memory costs associated with vocabulary layers compared to transformer layers in large language models.  It breaks down the FLOPs (floating-point operations) for computation and the memory usage for parameters in each layer type, providing insights into the computational and memory efficiency of different components within these models.", "section": "A Quantitative Analysis of Vocabulary Layers"}, {"content": "| Setup | Method | MFU (%) 32k | MFU (%) 64k | MFU (%) 128k | MFU (%) 256k | Peak Memory (GB) 32k | Peak Memory (GB) 64k | Peak Memory (GB) 128k | Peak Memory (GB) 256k |\n|---|---|---|---|---|---|---|---|---|---| \n| 8GPU, Seq Length 2048 | Baseline | 46.16 | 40.48 | 33.11 | 25.23 | 14.86 | 16.32 | 19.25 | 25.64 |\n| 8GPU, Seq Length 2048 | Redis | 46.01 | 46.37 | 44.22 | 38.91 | 14.86 | 16.32 | 19.25 | 25.64 |\n| 8GPU, Seq Length 2048 | Vocab-1 | 50.42 | 50.28 | 49.93 | 50.12 | 15.63 | 16.02 | 16.84 | 18.59 |\n| 8GPU, Seq Length 2048 | Vocab-2 | 50.23 | 50.18 | 49.82 | 49.69 | 14.83 | 15.23 | 16.04 | 17.78 |\n| 8GPU, Seq Length 2048 | Interlaced | 51.18 | 50.94 | 50.97 | 50.92 | 17.20 | 17.57 | 18.43 | 20.17 |\n| 8GPU, Seq Length 4096 | Baseline | 47.05 | 41.87 | 35.00 | 26.75 | 21.39 | 22.85 | 25.78 | 31.64 |\n| 8GPU, Seq Length 4096 | Redis | 46.93 | 46.78 | 47.44 | 43.01 | 21.39 | 22.85 | 25.78 | 31.64 |\n| 8GPU, Seq Length 4096 | Vocab-1 | 50.98 | 50.98 | 50.83 | 50.66 | 24.04 | 24.47 | 25.41 | 27.34 |\n| 8GPU, Seq Length 4096 | Vocab-2 | 50.93 | 50.75 | 50.56 | 50.40 | 22.44 | 22.89 | 23.80 | 25.73 |\n| 8GPU, Seq Length 4096 | Interlaced | 51.41 | 51.82 | 51.32 | 51.38 | 27.20 | 27.64 | 28.60 | 30.53 |\n| 16GPU, Seq Length 2048 | Baseline | 45.66 | 40.09 | 32.44 | 24.21 | 24.03 | 25.98 | 29.92 | 38.71 |\n| 16GPU, Seq Length 2048 | Redis | 45.56 | 42.82 | 38.65 | 36.98 | 24.03 | 25.98 | 29.92 | 38.71 |\n| 16GPU, Seq Length 2048 | Vocab-1 | 49.02 | 50.62 | 50.54 | 50.66 | 24.37 | 24.63 | 25.14 | 26.26 |\n| 16GPU, Seq Length 2048 | Vocab-2 | 48.90 | 50.49 | 50.46 | 50.46 | 23.57 | 23.83 | 24.35 | 25.47 |\n| 16GPU, Seq Length 2048 | Interlaced | 48.94 | 48.97 | 49.19 | 49.52 | 29.23 | 29.47 | 29.97 | 31.10 |\n| 16GPU, Seq Length 4096 | Baseline | 47.56 | 41.21 | 33.88 | 25.33 | 36.99 | 38.94 | 42.85 | 50.90 |\n| 16GPU, Seq Length 4096 | Redis | 47.41 | 43.07 | 43.15 | 40.15 | 36.99 | 38.94 | 42.85 | 50.90 |\n| 16GPU, Seq Length 4096 | Vocab-1 | 50.93 | 50.97 | 50.71 | 51.22 | 39.46 | 39.73 | 40.31 | 41.53 |\n| 16GPU, Seq Length 4096 | Vocab-2 | 50.97 | 50.80 | 50.68 | 50.90 | 37.89 | 38.18 | 38.77 | 39.92 |\n| 16GPU, Seq Length 4096 | Interlaced | 49.52 | 49.53 | 49.77 | 49.84 | 49.16 | 49.44 | 50.05 | 51.28 |\n| 32GPU, Seq Length 2048 | Baseline | 42.81 | 37.28 | 28.97 | 20.86 | 33.45 | 35.89 | 41.17 | 52.16 |\n| 32GPU, Seq Length 2048 | Redis | 43.48 | 37.29 | 36.32 | 29.16 | 33.45 | 35.89 | 41.17 | 52.16 |\n| 32GPU, Seq Length 2048 | Vocab-1 | 45.85 | 45.92 | 45.90 | 46.11 | 33.38 | 33.55 | 33.86 | 34.51 |\n| 32GPU, Seq Length 2048 | Vocab-2 | 45.54 | 45.86 | 45.86 | 46.16 | 32.72 | 32.88 | 33.20 | 33.84 |\n| 32GPU, Seq Length 2048 | Interlaced | 42.40 | 42.43 | 42.75 | 43.25 | 42.94 | 43.09 | 43.40 | 44.07 |\n| 32GPU, Seq Length 4096 | Baseline | 43.68 | 38.11 | 30.05 | 21.63 | 54.97 | 57.41 | 62.29 | 73.05 |\n| 32GPU, Seq Length 4096 | Redis | 44.01 | 38.12 | 37.87 | 31.03 | 54.97 | 57.41 | 62.29 | 73.05 |\n| 32GPU, Seq Length 4096 | Vocab-1 | 46.41 | 46.44 | 46.68 | 46.83 | 57.41 | 57.56 | 57.88 | 58.58 |\n| 32GPU, Seq Length 4096 | Vocab-2 | 46.23 | 46.35 | 46.55 | 46.84 | 56.09 | 56.26 | 56.61 | 57.31 |\n| 32GPU, Seq Length 4096 | Interlaced | - | - | - | - | - | - | - | - |", "caption": "Table 5: Comparison of Methods on 1F1B.", "description": "This table presents a comparison of different methods for training large language models using the 1F1B pipeline parallelism schedule.  The methods compared include a baseline approach, a layer redistribution technique, two versions of the proposed Vocabulary Parallelism method (Vocab-1 and Vocab-2), and an interlaced pipeline method. For several model sizes and varying numbers of GPUs, the table shows the achieved model FLOPs utilization (MFU) and peak memory usage for each method. This allows for a quantitative assessment of the effectiveness of each method in improving training throughput and memory efficiency.", "section": "6.3 Comparison of Methods"}, {"content": "| Setup | Method | MFU (%) 32k | MFU (%) 64k | MFU (%) 128k | MFU (%) 256k | Peak Memory (GB) 32k | Peak Memory (GB) 64k | Peak Memory (GB) 128k | Peak Memory (GB) 256k | \n|---|---|---|---|---|---|---|---|---|---| \n| 16GPU, Seq Length 2048 | Baseline | 46.41 | 38.52 | 28.75 | 19.99 | 15.57 | 19.77 | 28.55 | 46.77 | \n|  | Vocab-1 | **52.82** | **53.11** | **53.41** | **52.89** | **13.20** | **13.46** | **13.98** | **15.02** | \n| 16GPU, Seq Length 4096 | Baseline | 50.01 | 41.17 | 31.36 | 21.90 | 21.22 | 25.61 | 34.56 | 53.11 | \n|  | Vocab-1 | **58.69** | **58.56** | **58.44** | **57.59** | **20.14** | **20.41** | **20.96** | **22.06** | \n| 24GPU, Seq Length 2048 | Baseline | 51.07 | 43.13 | 32.38 | 22.54 | 23.94 | 29.12 | 39.98 | 61.71 | \n|  | Vocab-1 | **56.70** | **56.50** | **55.72** | **54.86** | **21.08** | **21.29** | **21.72** | **22.57** | \n| 24GPU, Seq Length 4096 | Baseline | 54.53 | 45.96 | 34.99 | 24.31 | 33.60 | 38.97 | 49.90 | 72.60 | \n|  | Vocab-1 | **60.09** | **60.09** | **59.42** | **58.22** | **32.55** | **32.78** | **33.22** | **34.12** | \n| 32GPU, Seq Length 2048 | Baseline | 52.80 | 45.56 | 35.69 | - | 34.11 | 40.28 | 53.22 | - | \n|  | Vocab-1 | **57.70** | **57.62** | **57.69** | **57.80** | **30.85** | **31.04** | **31.42** | **32.18** | \n| 32GPU, Seq Length 4096 | Baseline | 56.06 | 48.17 | 37.85 | - | 48.84 | 55.19 | 68.12 | - | \n|  | Vocab-1 | **60.10** | **60.14** | **60.72** | **59.82** | **47.99** | **48.19** | **48.59** | **49.38** | ", "caption": "Table 6: Comparison of Methods on V-Half.", "description": "This table presents a comparison of different methods' performance on the V-Half pipeline scheduling algorithm.  It shows the achieved FLOPs utilization (MFU) and peak memory usage for various model sizes and vocabulary sizes across different numbers of GPUs.  The methods compared include the baseline (naive) approach and the proposed Vocabulary Parallelism (Vocab-1) method.  The table helps to demonstrate the effectiveness of Vocabulary Parallelism in improving throughput and reducing memory consumption, especially for larger models and vocabularies.", "section": "6.4 Memory-Balanced Schedule"}]