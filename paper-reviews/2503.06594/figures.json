[{"figure_path": "https://arxiv.org/html/2503.06594/x1.png", "caption": "Figure 1: Architectures of machine translation models. In standard NMT models, an encoder is used to encode the source-language sequence x, and a decoder is used to generate the target-language sequence y from left to right. In LLMs, the decoder-only architecture is adopted. Both x and y, along with the prompt c, are represented as a single sequence, which is processed by a large decoding network. In the LaMaTE model, an LLM serves as the encoder. The output of the LLM is transformed into the input to the NMT decoder through an adaptor. The NMT decoder then generates the target-language sequence as usual.", "description": "This figure illustrates three different machine translation model architectures.  (a) shows a standard Neural Machine Translation (NMT) model, using a separate encoder to process the source language sequence (x) and a decoder to generate the target language sequence (y). (b) depicts a Large Language Model (LLM) approach, where both x and y, along with an optional prompt (c), are fed into a single decoder network. (c) presents the LaMaTE architecture, which combines LLMs and NMT.  LaMaTE uses an LLM as its encoder. The LLM's output then passes through an adaptor before being input to the NMT decoder, which produces the translated output (y).", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.06594/x2.png", "caption": "Figure 2: Architecture of the NMT Encoder, NMT Decoder, and LLM Decoder. We omit the layer normalization and residual connections for simplicity.", "description": "This figure shows the architecture of three different decoder models used in machine translation: a standard neural machine translation (NMT) encoder, a standard NMT decoder, and a large language model (LLM) decoder.  The NMT encoder processes the input sequence bidirectionally using self-attention and feed-forward neural networks. The NMT decoder generates the target sequence unidirectionally, also using self-attention, but incorporating cross-attention with the encoder's output for better context. The LLM decoder is a decoder-only model that processes both source and target sequences as a single sequence, leveraging causal self-attention.  Layer normalization and residual connections are omitted for clarity.", "section": "3.1 Model Architecture"}, {"figure_path": "https://arxiv.org/html/2503.06594/x3.png", "caption": "Figure 3: The architecture of LaMaTE, where the Adaptor consists of three components: Fusion combines the representations of layer groups \ud835\udc20ksubscript\ud835\udc20\ud835\udc58\\mathbf{g}_{k}bold_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, MLP reduces the representations\u2019s dimensionality, and EncStack learns bidirectional representations. The training process consists of two stages: the first stage trains the Adaptor and Decoder, and the second stage trains all model parameters.", "description": "The figure illustrates the LaMaTE model architecture, which uses a large language model (LLM) as the encoder and a neural machine translation (NMT) decoder.  A key component is the adaptor, which sits between the LLM and the NMT decoder. This adaptor has three parts: a fusion layer that combines the LLM's hidden states from multiple layers, a multilayer perceptron (MLP) that reduces the dimensionality of the combined representation, and a stack of encoder layers (EncStack) that creates bidirectional representations.  The model is trained in two stages: the first stage trains only the adaptor and decoder, and the second stage trains all model parameters, including the LLM.", "section": "3 LaMaTE"}, {"figure_path": "https://arxiv.org/html/2503.06594/x4.png", "caption": "Figure 4: Our comprehensive translation dataset, ComMT, includes diverse translation-related tasks. The table presents the training set statistics for ComMT.", "description": "Figure 4 presents the ComMT dataset, a comprehensive benchmark for evaluating machine translation models' capabilities across various tasks.  It is not limited to simple sentence-level translation; ComMT includes tasks like document-level translation, which tests coherence and context maintenance in longer texts, and domain-specific translation, assessing performance within specific terminology and styles (medical, legal, IT, colloquial, literature).  Additionally, it evaluates performance on constrained translation (where specific terminology must be used) and automatic post-editing (correcting machine-generated translations). The table within Figure 4 details the amount of training data available for each of these tasks across multiple languages (German, Czech, Russian, and Chinese).", "section": "ComMT"}, {"figure_path": "https://arxiv.org/html/2503.06594/x5.png", "caption": "Figure 5: Comparison of performance across three datasets\u2014WMT17-20, TowerBlock, and ComMT\u2014fine-tuned on Llama3-8B and evaluated on the WMT23 test set.", "description": "This figure compares the performance of the Llama3-8B model fine-tuned on three different datasets: WMT17-20, TowerBlock, and ComMT.  The model's performance is then evaluated on the WMT23 test set using BLEU scores. This allows for a comparison of how well the model generalizes across different datasets and task types.  The x-axis represents the dataset used for fine-tuning (WMT17-20, TowerBlock, or ComMT), and the y-axis shows the BLEU score achieved on the WMT23 test set.  The figure includes separate bars for English-to-other-language translation (En\u2192X) and other-language-to-English translation (X\u2192En), illustrating performance differences in each direction. This visualization helps assess the model's ability to transfer knowledge from various training data sources and its adaptability for various translation scenarios.", "section": "Quality Verification"}, {"figure_path": "https://arxiv.org/html/2503.06594/x6.png", "caption": "Figure 6: Evaluation of LLM translation capabilities in 0-shot and 3-shot settings using the WMT23 test set.", "description": "This figure displays the results of evaluating the translation capabilities of five different large language models (LLMs) in both zero-shot (no prior training on the translation task) and three-shot (three examples provided during inference) settings.  The models were tested on eight different translation directions involving English and four other languages (German, Czech, Russian, and Chinese). The y-axis represents the BLEU score, a common metric for evaluating machine translation quality, and shows the performance for each translation direction and model type (zero-shot vs three-shot). The x-axis shows the five different language models: XGLM-7.5B, BLOOM-7B, Falcon-7B, Qwen2-7B, and Llama-3-8B.  The figure illustrates how providing a few examples (three-shot) significantly improves the translation quality for most of the LLMs tested, and highlights Llama-3-8B's consistently superior performance across all settings.", "section": "Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2503.06594/x7.png", "caption": "Figure 7: Comparison of decoder-only (Llama3-8B and TowerInstruct-7B) and encoder-decoder (mT5-large and LaMaTE) models on off-target rate (OTR), unaligned source words (USW), and unaligned target words (UTW).", "description": "Figure 7 presents a comparison of four different machine translation models across three metrics that evaluate translation quality. Specifically, it compares the Llama3-8B and TowerInstruct-7B (both decoder-only models) against mT5-large and LaMaTE (both encoder-decoder models). The three metrics used are off-target rate (OTR), which measures how often a model generates translations in an unintended language; unaligned source words (USW), which is the proportion of words in the source language that are not properly translated in the target language; and unaligned target words (UTW), which is the number of words that appear in the target language translation but do not have a proper equivalent in the source language.  The figure helps demonstrate how different model architectures can affect the overall quality and alignment of the machine translations.", "section": "Misalignment Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.06594/x8.png", "caption": "Figure 8: Comparison of efficiency: The left chart displays the decoding speedup ratio of LaMaTE versus Llama3-8B under varying source sequence lengths and batch sizes, and the right chart shows the theoretical KV cache size factor for each model.", "description": "Figure 8 presents a comparison of the efficiency of LaMaTE against Llama3-8B.  The left chart shows the speedup ratio achieved by LaMaTE in decoding, considering varying source sequence lengths and batch sizes.  This illustrates how LaMaTE's decoding speed improves as the complexity of the input increases. The right chart visualizes the theoretical reduction in KV cache memory usage achieved by LaMaTE compared to Llama3-8B. This demonstrates LaMaTE's improved memory efficiency.", "section": "Efficiency Analysis"}, {"figure_path": "https://arxiv.org/html/2503.06594/x9.png", "caption": "Figure 9: The impact of EncStack and decoder depth on model performance and efficiency.", "description": "This figure analyzes how the depth of the EncStack (encoder stack within the adaptor) and the decoder affects both the model's performance and efficiency.  The left panel shows the impact of increasing the number of EncStack layers, while keeping the decoder's depth fixed. The right panel reverses this, showing the effect of increasing the number of decoder layers, with the EncStack's depth fixed.  The figure plots translation quality (COMET score) and decoding speed against the number of layers for both the EncStack and decoder. This allows for a direct comparison of how changes in depth for each component influence the model's translation capabilities and inference speed, enabling a discussion on optimal architectural choices regarding depth for balancing performance and efficiency.", "section": "6.3 Depth vs Performance"}, {"figure_path": "https://arxiv.org/html/2503.06594/x10.png", "caption": "Figure 10: The Encoder-Decoder and Decoder-only architecture.", "description": "This figure compares two different neural network architectures commonly used in machine translation: the encoder-decoder model and the decoder-only model. The encoder-decoder model consists of two main components: an encoder that processes the input sequence and a decoder that generates the output sequence based on the encoder's output.  The decoder-only model, on the other hand, uses only a decoder to process both the input and output sequences. This figure visually depicts the key structural differences between these two architectures.", "section": "A Encoder-Decoder vs Decoder-Only"}, {"figure_path": "https://arxiv.org/html/2503.06594/x11.png", "caption": "Figure 11: Three variants of decoders: Cross Decoder is the standard decoder, while Concat Decoder and Prefix Decoder remove the cross-attention sublayer, integrating source information through self-attention and early fusion methods, respectively.", "description": "This figure illustrates three decoder architectures used in machine translation. The standard 'Cross Decoder' utilizes cross-attention to integrate source information. The 'Concat Decoder' omits cross-attention and integrates source information directly into the self-attention mechanism, while the 'Prefix Decoder' integrates source information early via fusion methods before self-attention.", "section": "3.1 Model Architecture"}]