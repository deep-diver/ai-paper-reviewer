{"importance": "This paper is crucial for researchers aiming to enhance machine translation efficiency while retaining quality. By combining LLMs with NMT, it presents a balanced approach that addresses computational costs and scalability, offering new directions for future research. The insights on generalization across tasks are valuable for developing more versatile translation systems.", "summary": "LLMs as MT encoders enhance efficiency & generalization!", "takeaways": ["LLMs can be effectively used as encoders in NMT models, improving translation quality and efficiency.", "The LaMaTE model achieves significant speedups and memory reduction compared to traditional LLMs.", "A new benchmark, ComMT, is introduced to evaluate the generalization capabilities of MT systems across various tasks."], "tldr": "The paper addresses the rising computational costs of neural machine translation (NMT) due to large language models (LLMs). Traditional encoder-decoder architectures have been overshadowed by single Transformer decoders in NLP. The research explores efficient translation models by integrating LLMs into NMT encoding, retaining the original NMT decoder to create a universal and efficient model. Methods to adapt LLMs for better NMT decoder compatibility are also developed. The paper introduces a new dataset for assessing machine translation generalization across tasks. \n\nThe study presents the LaMaTE model, which combines LLMs as NMT encoders with adaptations for improved decoder compatibility. Evaluations on WMT and a new dataset show that the method matches or exceeds translation quality baselines, while achieving 2.4~6.5x inference speedups and a 75% reduction in KV cache memory. The new benchmark, ComMT, is introduced for assessing machine translation generalization across tasks. This approach demonstrates strong generalization across translation-related tasks.", "affiliation": "NLP Lab, Northeastern University, Shenyang, China", "categories": {"main_category": "Natural Language Processing", "sub_category": "Machine Translation"}, "podcast_path": "2503.06594/podcast.wav"}