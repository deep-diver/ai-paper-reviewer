{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "This paper is foundational as it provides a detailed technical report on GPT-4, a large language model that serves as a benchmark for LLM quantization.  Understanding the architectural details and performance characteristics of GPT-4 is essential for evaluating the effectiveness of quantization techniques like FLATQUANT, which aims to improve the efficiency of such models.  The paper's comprehensive analysis of GPT-4's capabilities and limitations provides valuable context for assessing the impact and potential benefits of FLATQUANT.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Saleh Ashkboos", "paper_title": "Towards end-to-end 4-bit inference on generative large language models", "reason": "This paper is highly relevant as it explores end-to-end 4-bit inference, directly addressing the goal of efficient LLM quantization.  It investigates techniques for minimizing quantization errors and improving inference speed. The paper's focus on low-bit quantization makes it a crucial reference for comparing and contrasting the approach of FLATQUANT, which also targets efficient low-bit quantization of LLMs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Saleh Ashkboos", "paper_title": "Quarot: Outlier-free 4-bit inference in rotated Ilms", "reason": "QuaRot is a state-of-the-art method in LLM quantization, directly compared against in FLATQUANT's experiments. This paper's results and techniques are fundamental for establishing a benchmark and demonstrating the improvements achieved by FLATQUANT.  Understanding QuaRot's strengths and weaknesses provides crucial context for evaluating the novelty and significance of FLATQUANT's contributions.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Haoli Bai", "paper_title": "Binarybert: Pushing the limit of bert quantization", "reason": "This paper is significant because it explores techniques for quantizing BERT models, a predecessor to LLMs. The methods and insights gained from quantizing BERT models, a simpler architecture, are valuable for understanding the challenges and approaches that can be applied to quantizing the more complex LLMs, such as those evaluated in FLATQUANT.  It provides a historical perspective on the evolution of LLM quantization techniques.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Yonatan Bisk", "paper_title": "Piqa: Reasoning about physical commonsense in natural language", "reason": "This paper introduces a benchmark dataset (PIQA) used for evaluating commonsense reasoning capabilities in LLMs.  Since FLATQUANT's experimental evaluation includes commonsense reasoning tasks, understanding the PIQA dataset and its properties is crucial for interpreting and comparing the performance of FLATQUANT against other methods on this specific type of task.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jerry Chee", "paper_title": "Quip: 2-bit quantization of large language models with guarantees", "reason": "This work is directly relevant as it proposes a quantization method with guarantees, a crucial aspect when dealing with the numerical accuracy of quantized LLMs.  FLATQUANT is compared against this method, so understanding its approach and performance is essential to assessing the improvements offered by FLATQUANT.  The focus on guarantees highlights the importance of accuracy and reliability in the context of LLM quantization.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Brian Chmiel", "paper_title": "Robust quantization: One model to rule them all", "reason": "This paper addresses the general problem of robust quantization for neural networks, providing insights into techniques for mitigating the impact of outliers and maintaining model accuracy under quantization. The principles and methods discussed are relevant to the LLM quantization challenge addressed by FLATQUANT, offering a broader theoretical framework for understanding the problem.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Peter Clark", "paper_title": "Think you have solved question answering? try arc, the ai2 reasoning challenge", "reason": "This paper introduces the ARC dataset, which is used as a benchmark in FLATQUANT's zero-shot question answering experiments.  Understanding the ARC dataset, including its characteristics and the types of reasoning it requires, is essential for interpreting and comparing FLATQUANT's performance against other methods in the context of zero-shot question answering.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Tim Dettmers", "paper_title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale", "reason": "This paper is crucial for understanding the state-of-the-art in low-bit matrix multiplication for transformers, a fundamental operation in LLMs. FLATQUANT benefits from efficient low-bit matrix operations, so understanding the advancements made by this paper provides context for evaluating FLATQUANT's efficiency improvements.  It directly addresses the performance challenges in quantizing large transformer models.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces the LLaMA-3 family of models, which are directly used in FLATQUANT's experiments.   Understanding the architectural characteristics and performance metrics of LLaMA-3 models is crucial for properly evaluating and interpreting the results obtained by FLATQUANT, particularly the state-of-the-art performance achieved on this model family.  The paper forms a key foundation for the experimental setup.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Elias Frantar", "paper_title": "Optq: Accurate quantization for generative pre-trained transformers", "reason": "This paper presents a method for accurate quantization of generative pre-trained transformers, directly addressing the core problem of FLATQUANT. Comparing the performance and techniques of OptQ with FLATQUANT's approach provides valuable insights into the advantages and limitations of each method and helps to position FLATQUANT within the current state-of-the-art.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Leo Gao", "paper_title": "A framework for few-shot language model evaluation", "reason": "This paper introduces a framework for evaluating few-shot performance in language models.  FLATQUANT's zero-shot evaluation uses this framework, making this paper crucial for understanding the methodology and interpreting the results.  Without understanding the framework and metrics used in the evaluation, it's impossible to fully appreciate the significance of FLATQUANT's zero-shot performance.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Coleman Hooper", "paper_title": "Kvquant: Towards 10 million context length llm inference with kv cache quantization", "reason": "This paper addresses KV cache quantization, a specific aspect of LLM quantization that FLATQUANT also addresses.  The approaches and results of KvQuant offer a relevant comparison point for evaluating FLATQUANT's techniques and performance improvements in the context of KV cache quantization.  The work shares the goal of efficient, low-bit quantization.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "The Mistral 7B model is a large language model and serves as a significant benchmark for comparing LLM quantization techniques.  FLATQUANT is not directly compared against Mistral 7B but the results on other models like LLaMA provide context for extrapolating FLATQUANT's potential performance on diverse large language models, especially in the realm of low-bit quantization.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Benoit Jacob", "paper_title": "Quantization and training of neural networks for efficient integer-arithmetic-only inference", "reason": "This foundational work explores the early techniques for quantizing neural networks and addresses various aspects of quantization, such as the effects of different bit-widths. The insights and challenges presented are highly relevant to the LLM quantization problem targeted by FLATQUANT, as it sets the stage for understanding the background and evolution of quantization techniques.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Ji Lin", "paper_title": "Awq: Activation-aware weight quantization for Ilm compression and acceleration", "reason": "This paper proposes AWQ, a weight quantization technique, which is relevant to FLATQUANT because it also addresses LLM compression and acceleration. Comparing and contrasting FLATQUANT's approach with AWQ's provides valuable insights into the different strategies and their effectiveness.  The focus on activation-aware quantization highlights the importance of considering the interaction between weights and activations during quantization.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yuhang Li", "paper_title": "Brecq: Pushing the limit of post-training quantization by block reconstruction", "reason": "Brecq is a post-training quantization method and is related to FLATQUANT in its approach.  Comparing the techniques, advantages, and limitations of Brecq with those of FLATQUANT helps understand FLATQUANT's approach within the broader context of post-training quantization methods and shows its advantages in terms of speed and accuracy.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Ruikang Liu", "paper_title": "Intactkv: Improving large language model quantization by keeping pivot tokens intact", "reason": "This paper is highly relevant as it focuses on improving LLM quantization by preserving crucial \"pivot tokens.\" The concept of pivot tokens and their significance for model accuracy are important for understanding the challenges and limitations of traditional quantization methods.  FLATQUANT addresses these challenges indirectly by improving flatness, which may indirectly contribute to better handling of pivot tokens.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuexiao Ma", "paper_title": "Affinequant: Affine transformation quantization for large language models", "reason": "AffineQuant is a post-training quantization method directly compared against in FLATQUANT's experimental evaluation. It utilizes affine transformations, similar to FLATQUANT, and understanding its performance helps in assessing the improvement achieved by FLATQUANT.  The direct comparison allows for highlighting FLATQUANT's advantages in terms of efficiency, accuracy, and scalability.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "Stephen Merity", "paper_title": "Pointer sentinel mixture models", "reason": "This paper introduces a pointer sentinel mixture model which is relevant as it relates to the underlying architecture of the LLMs considered in this work.  Understanding the model architecture and its properties provides valuable context for interpreting the challenges and solutions presented in FLATQUANT for quantizing such LLMs.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Markus Nagel", "paper_title": "Up or down? adaptive rounding for post-training quantization", "reason": "This paper focuses on adaptive rounding techniques for post-training quantization which is highly relevant to the overall problem addressed by FLATQUANT. It explores different strategies for quantization and this understanding provides context for understanding the trade-offs involved in choosing different quantization methods and the design choices in FLATQUANT.", "section_number": 1}]}