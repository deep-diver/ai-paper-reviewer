[{"figure_path": "2410.09426/figures/figures_5_0.png", "caption": "Figure 3: The overall framework of FLATQUANT. (a): necessary notations of FLATQUANT; (b): the integration of FLATQUANT with a conventional LLaMA layer, where merged parameters are grouped in red, online transformation and quantization functions in blue, and merged scaling vectors in green; (c): the exemplary view of FLATQUANT applied for the down-projection layer, where the scaling vector diag(c) over X is merged to Wu in practice.", "description": "Figure 3 illustrates the overall framework of FLATQUANT, including the notations, integration with a LLaMA layer, and an example of its application to a down-projection layer.", "section": "3 Method"}, {"figure_path": "2410.09426/figures/figures_16_0.png", "caption": "Figure 3: The overall framework of FLATQUANT. (a): necessary notations of FLATQUANT; (b): the integration of FLATQUANT with a conventional LLaMA layer, where merged parameters are grouped in red, online transformation and quantization functions in blue, and merged scaling vectors in green; (c): the exemplary view of FLATQUANT applied for the down-projection layer, where the scaling vector diag(c) over X is merged to Wu in practice.", "description": "The figure illustrates the overall framework of FLATQUANT, including its integration with a conventional LLaMA layer and its application to the down-projection layer.", "section": "3 Method"}, {"figure_path": "2410.09426/figures/figures_21_0.png", "caption": "Figure 1: Distributions of weights and inputs from LLaMA-3-8B and LLaMA-3-70B, sorted by the channel magnitudes (i.e., the Frobenius norm) in descending order. In a Transformer layer, W\u3002 and X, denote the weight matrix and input of the output projection layer in the self-attention layer, respectively. Wg and X, denote the weight and input of the gated linear layer of the feed-forward network, respectively. More visualizations can be found in Appendix D.", "description": "The figure visualizes the distributions of weights and inputs from LLaMA-3-8B and LLaMA-3-70B models, demonstrating the impact of different pre-quantization transformations on weight and activation flatness.", "section": "2 MOTIVATION"}, {"figure_path": "2410.09426/figures/figures_23_0.png", "caption": "Figure 2: The mean squared error (MSE) of quantization across Transformer layers and input sequence in LLaMA-3-8B. Figure 2a-2c plot the MSE surface of each method, while Figure 2d overlays these surfaces by dividing each MSE with that of FLATQUANT. More details and visualizations can be found in Appendix D.", "description": "Figure 2 shows the mean squared error (MSE) of quantization across Transformer layers and input tokens for three different pre-quantization methods, highlighting the superior flatness of FLATQUANT in reducing error propagation.", "section": "2 MOTIVATION"}, {"figure_path": "2410.09426/figures/figures_23_1.png", "caption": "Figure 2: The mean squared error (MSE) of quantization across Transformer layers and input sequence in LLaMA-3-8B. Figure 2a-2c plot the MSE surface of each method, while Figure 2d overlays these surfaces by dividing each MSE with that of FLATQUANT. More details and visualizations can be found in Appendix D.", "description": "The figure shows the mean squared error (MSE) of quantization across Transformer layers and input tokens for three different methods (per-channel scaling, Hadamard transform, and FLATQUANT) and a stacked view comparing the three methods.", "section": "2 MOTIVATION"}, {"figure_path": "2410.09426/figures/figures_23_2.png", "caption": "Figure 2: The mean squared error (MSE) of quantization across Transformer layers and input sequence in LLaMA-3-8B. Figure 2a-2c plot the MSE surface of each method, while Figure 2d overlays these surfaces by dividing each MSE with that of FLATQUANT. More details and visualizations can be found in Appendix D.", "description": "The figure shows the mean squared error (MSE) of quantization across Transformer layers and input tokens for three different quantization methods (per-channel scaling, Hadamard transform, and FLATQUANT), visualizing how flatness affects error propagation.", "section": "2 MOTIVATION"}, {"figure_path": "2410.09426/figures/figures_23_3.png", "caption": "Figure 2: The mean squared error (MSE) of quantization across Transformer layers and input sequence in LLaMA-3-8B. Figure 2a-2c plot the MSE surface of each method, while Figure 2d overlays these surfaces by dividing each MSE with that of FLATQUANT. More details and visualizations can be found in Appendix D.", "description": "The figure shows the mean squared error (MSE) of quantization across Transformer layers and input tokens for three different quantization methods (per-channel scaling, Hadamard transform, and FLATQUANT) and an overlay of the MSEs, normalized by FLATQUANT's MSE.", "section": "The Flatness of Quantization Error Landscape"}]