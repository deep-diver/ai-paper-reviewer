[{"figure_path": "2410.09426/charts/charts_3_0.png", "caption": "Figure 1: Distributions of weights and inputs from LLaMA-3-8B and LLaMA-3-70B, sorted by the channel magnitudes (i.e., the Frobenius norm) in descending order. In a Transformer layer, W\u3002 and X, denote the weight matrix and input of the output projection layer in the self-attention layer, respectively. Wg and X, denote the weight and input of the gated linear layer of the feed-forward network, respectively. More visualizations can be found in Appendix D.", "description": "The chart displays the distributions of original and transformed weights and activations of LLAMA models, illustrating the impact of different transformations on flatness.", "section": "2 MOTIVATION"}, {"figure_path": "2410.09426/charts/charts_3_1.png", "caption": "Figure 2: The mean squared error (MSE) of quantization across Transformer layers and input sequence in LLaMA-3-8B. Figure 2a-2c plot the MSE surface of each method, while Figure 2d overlays these surfaces by dividing each MSE with that of FLATQUANT. More details and visualizations can be found in Appendix D.", "description": "The chart visualizes the mean squared error (MSE) of quantization across Transformer layers and input tokens for three different methods (Per-channel Scaling, Hadamard Transform, and FLATQUANT) and shows how FLATQUANT reduces the MSE.", "section": "2 MOTIVATION"}, {"figure_path": "2410.09426/charts/charts_9_0.png", "caption": "Figure 4: Prefill and decoding speedup of LLaMA-2-7B model across different batch sizes. We decode 256 tokens after the prefill on a sequence length of 2048.", "description": "The chart displays the prefill and decoding speedup of the LLaMA-2-7B model with different batch sizes, showing FLATQUANT's performance improvements over INT4 and QuaRot.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/charts/charts_9_1.png", "caption": "Figure 5: Prefill speedup and WikiText2 PPL results of different decomposed matrix sizes on LLaMA-2-7B model. We decompose the hidden dimension 4096 into n\u2081 \u00d7 n\u2082 and range n\u2081 from 1 to 2048, where n\u2081 = 1 amounts to maintaining a full-size transformation matrix. More details can be found in Appendix C.3.", "description": "The chart displays the prefill speedup and WikiText2 perplexity (PPL) of the LLaMA-2-7B model with varying sizes of decomposed matrices, showing the impact of Kronecker decomposition on model performance and speedup.", "section": "3 METHOD"}, {"figure_path": "2410.09426/charts/charts_9_2.png", "caption": "Figure 6: Prefill speedup of LLaMA-2-7B on a sequence length of 2048 under a batch size of 64 after applying different online transformations. We incorporate different online transformations sequentially to gauge their impact on the final speedup. Each point on the x-axis indicates adding a new online transformation.", "description": "The chart displays the prefill speedup of the LLaMA-2-7B model with different online transformations applied sequentially, showing the impact of each transformation on overall speedup.", "section": "3.3 EFFICIENT KERNEL DESIGN"}, {"figure_path": "2410.09426/charts/charts_20_0.png", "caption": "Figure 4: Prefill and decoding speedup of LLaMA-2-7B model across different batch sizes. We decode 256 tokens after the prefill on a sequence length of 2048.", "description": "The chart displays the prefill and decoding speedup of the LLaMA-2-7B model across different batch sizes, showing FLATQUANT's superior performance.", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.09426/charts/charts_20_1.png", "caption": "Figure 4: Prefill and decoding speedup of LLaMA-2-7B model across different batch sizes. We decode 256 tokens after the prefill on a sequence length of 2048.", "description": "The chart displays the prefill and decoding speedup of the LLaMA-2-7B model across different batch sizes, showing FLATQUANT's improved speed compared to baselines.", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.09426/charts/charts_22_0.png", "caption": "Figure 1: Distributions of weights and inputs from LLaMA-3-8B and LLaMA-3-70B, sorted by the channel magnitudes (i.e., the Frobenius norm) in descending order. In a Transformer layer, W\u3002 and X, denote the weight matrix and input of the output projection layer in the self-attention layer, respectively. Wg and X, denote the weight and input of the gated linear layer of the feed-forward network, respectively. More visualizations can be found in Appendix D.", "description": "The chart displays the distributions of weights and activations from LLaMA-3-8B and LLaMA-3-70B models before and after applying different transformations (original, per-channel scaling, Hadamard, and FLATQUANT).", "section": "2 MOTIVATION"}, {"figure_path": "2410.09426/charts/charts_22_1.png", "caption": "Figure 1: Distributions of weights and inputs from LLaMA-3-8B and LLaMA-3-70B, sorted by the channel magnitudes (i.e., the Frobenius norm) in descending order. In a Transformer layer, W\u3002 and X, denote the weight matrix and input of the output projection layer in the self-attention layer, respectively. Wg and X, denote the weight and input of the gated linear layer of the feed-forward network, respectively. More visualizations can be found in Appendix D.", "description": "Figure 1 shows the distributions of weights and activations from LLaMA-3-8B and LLaMA-3-70B models after applying different transformations, illustrating the flatness achieved by FLATQUANT.", "section": "2 MOTIVATION"}, {"figure_path": "2410.09426/charts/charts_22_2.png", "caption": "Figure 1: Distributions of weights and inputs from LLaMA-3-8B and LLaMA-3-70B, sorted by the channel magnitudes (i.e., the Frobenius norm) in descending order. In a Transformer layer, W\u3002 and X, denote the weight matrix and input of the output projection layer in the self-attention layer, respectively. Wg and X, denote the weight and input of the gated linear layer of the feed-forward network, respectively. More visualizations can be found in Appendix D.", "description": "The chart displays the distributions of weights and activations from LLaMA-3-8B and LLaMA-3-70B models before and after applying different pre-quantization transformations, illustrating the effect on flatness.", "section": "2 MOTIVATION"}, {"figure_path": "2410.09426/charts/charts_22_3.png", "caption": "Figure 1: Distributions of weights and inputs from LLaMA-3-8B and LLaMA-3-70B, sorted by the channel magnitudes (i.e., the Frobenius norm) in descending order. In a Transformer layer, W\u3002 and X, denote the weight matrix and input of the output projection layer in the self-attention layer, respectively. Wg and X, denote the weight and input of the gated linear layer of the feed-forward network, respectively. More visualizations can be found in Appendix D.", "description": "Figure 1 shows the distributions of weights and activations from LLaMA-3-8B and LLaMA-3-70B models before and after applying different pre-quantization transformations, illustrating the effectiveness of FLATQUANT in achieving flatter weight and activation distributions.", "section": "2 MOTIVATION"}, {"figure_path": "2410.09426/charts/charts_22_4.png", "caption": "Figure 1: Distributions of weights and inputs from LLaMA-3-8B and LLaMA-3-70B, sorted by the channel magnitudes (i.e., the Frobenius norm) in descending order. In a Transformer layer, W\u3002 and X, denote the weight matrix and input of the output projection layer in the self-attention layer, respectively. Wg and X, denote the weight and input of the gated linear layer of the feed-forward network, respectively. More visualizations can be found in Appendix D.", "description": "Figure 1 shows the distributions of weights and inputs from LLaMA-3-8B and LLaMA-3-70B models after applying different pre-quantization transformations, including original, per-channel scaling, Hadamard, and FLATQUANT.", "section": "2 MOTIVATION"}, {"figure_path": "2410.09426/charts/charts_22_5.png", "caption": "Figure 1: Distributions of weights and inputs from LLaMA-3-8B and LLaMA-3-70B, sorted by the channel magnitudes (i.e., the Frobenius norm) in descending order. In a Transformer layer, W\u3002 and X, denote the weight matrix and input of the output projection layer in the self-attention layer, respectively. Wg and X, denote the weight and input of the gated linear layer of the feed-forward network, respectively. More visualizations can be found in Appendix D.", "description": "Figure 1 shows the distributions of weights and activations from LLaMA-3-8B and LLaMA-3-70B models after applying different transformations, illustrating their flatness.", "section": "2 MOTIVATION"}]