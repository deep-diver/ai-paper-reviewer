[{"figure_path": "2410.09426/tables/table_7_0.html", "caption": "Table 1: WikiText-2 and C4 perplexity of 4-bit weight & activation quantized LLaMA models.", "description": "Table 1 presents the perplexity results for different LLAMA models that are quantized to 4-bit weight and activation using various methods.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/tables/table_8_1.html", "caption": "Table 3: MT-Bench results of 4-bit weight & activation quantized LLaMA-3.1-8B-Instruct model.", "description": "Table 3 presents the performance of different quantization methods on the MT-Bench benchmark using the LLaMA-3.1-8B-Instruct model with 4-bit weight and activation quantization.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/tables/table_10_0.html", "caption": "Table 1: WikiText-2 and C4 perplexity of 4-bit weight & activation quantized LLaMA models.", "description": "Table 1 presents the perplexity scores on WikiText-2 and C4 datasets for various 4-bit quantized LLAMA models using different quantization methods.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/tables/table_10_1.html", "caption": "Table 1: WikiText-2 and C4 perplexity of 4-bit weight & activation quantized LLaMA models.", "description": "Table 1 presents the perplexity results for different 4-bit weight and activation quantized LLaMA models on WikiText-2 and C4 datasets, comparing various quantization methods.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/tables/table_12_0.html", "caption": "Table 1: WikiText-2 and C4 perplexity of 4-bit weight & activation quantized LLaMA models.", "description": "Table 1 shows the perplexity scores on WikiText-2 and C4 datasets for different LLaMA models with 4-bit weight and activation quantization using various methods.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/tables/table_13_0.html", "caption": "Table 1: WikiText-2 and C4 perplexity of 4-bit weight & activation quantized LLaMA models.", "description": "Table 1 presents the perplexity scores on WikiText-2 and C4 datasets for different LLaMA models with 4-bit weight and activation quantization using various methods.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/tables/table_15_0.html", "caption": "Table 1: WikiText-2 and C4 perplexity of 4-bit weight & activation quantized LLaMA models.", "description": "Table 1 presents the perplexity scores on WikiText-2 and C4 datasets for various 4-bit quantized LLAMA models using different quantization methods.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/tables/table_15_1.html", "caption": "Table 1: WikiText-2 and C4 perplexity of 4-bit weight & activation quantized LLaMA models.", "description": "Table 1 presents the perplexity results for WikiText-2 and C4 datasets, comparing the performance of different quantization methods on various LLaMA models with 4-bit weight and activation quantization.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/tables/table_16_0.html", "caption": "Table 1: WikiText-2 and C4 perplexity of 4-bit weight & activation quantized LLaMA models.", "description": "Table 1 presents the perplexity results for different quantization methods on the WikiText-2 and C4 datasets using 4-bit weight and activation quantization on various LLaMA models.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/tables/table_17_0.html", "caption": "Table 1: WikiText-2 and C4 perplexity of 4-bit weight & activation quantized LLaMA models.", "description": "Table 1 presents the perplexity results for various 4-bit quantized LLAMA models on WikiText-2 and C4 datasets, comparing different quantization methods and weight quantizers.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/tables/table_17_1.html", "caption": "Table 1: WikiText-2 and C4 perplexity of 4-bit weight & activation quantized LLaMA models.", "description": "Table 1 presents the perplexity results for WikiText-2 and C4 datasets, comparing different quantization methods on various LLaMA models with 4-bit weight and activation quantization.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/tables/table_18_0.html", "caption": "Table 1: WikiText-2 and C4 perplexity of 4-bit weight & activation quantized LLaMA models.", "description": "Table 1 presents the perplexity results for different 4-bit weight and activation quantized LLAMA models on WikiText-2 and C4 datasets, comparing various quantization methods.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/tables/table_18_1.html", "caption": "Table 1: WikiText-2 and C4 perplexity of 4-bit weight & activation quantized LLaMA models.", "description": "Table 1 presents the perplexity results for different 4-bit weight and activation quantized LLAMA models on WikiText-2 and C4 datasets.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/tables/table_18_2.html", "caption": "Table 1: WikiText-2 and C4 perplexity of 4-bit weight & activation quantized LLaMA models.", "description": "Table 1 presents the perplexity results for different quantization methods on the WikiText-2 and C4 datasets using LLaMA models with 4-bit weight and activation quantization.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/tables/table_19_1.html", "caption": "Table 1: WikiText-2 and C4 perplexity of 4-bit weight & activation quantized LLaMA models.", "description": "Table 1 presents the perplexity results for various 4-bit weight and activation quantized LLAMA models on WikiText-2 and C4 datasets, comparing different quantization methods.", "section": "4.2 Main Results"}, {"figure_path": "2410.09426/tables/table_22_0.html", "caption": "Table 1: WikiText-2 and C4 perplexity of 4-bit weight & activation quantized LLaMA models.", "description": "Table 1 presents the perplexity results on WikiText-2 and C4 datasets for different 4-bit weight and activation quantized LLAMA models using various methods.", "section": "4.2 Main Results"}]