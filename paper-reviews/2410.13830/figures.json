[{"figure_path": "2410.13830/figures/figures_1_0.png", "caption": "Figure 1: Customized video generation results of Dream Video-2. Our method precisely generates customized subjects at specified positions without fine-tuning at inference time.", "description": "The figure shows example results of customized video generation using DreamVideo-2, demonstrating precise control over subject placement and motion without requiring fine-tuning.", "section": "ABSTRACT"}, {"figure_path": "2410.13830/figures/figures_4_0.png", "caption": "Figure 2: Overall framework of DreamVideo-2. During training, a random video frame is segmented to obtain the subject image with a blank background. The bounding boxes extracted from the training video are converted into binary box masks. Then, the subject image is treated as a single-frame video and processed in parallel with the video by masked reference attention that incorporates blended masks to learn the subject appearance. Meanwhile, box masks are fed into a motion module that includes a spatiotemporal encoder and a ControlNet for motion control. Both the masked reference attention and motion module are trained using a reweighted diffusion loss.", "description": "The figure illustrates the overall architecture of DreamVideo-2, detailing how subject appearance and motion are learned and controlled during training via masked reference attention and a mask-guided motion module respectively.", "section": "4 Methodology"}, {"figure_path": "2410.13830/figures/figures_6_0.png", "caption": "Figure 3: Illustration of motion control domination in DreamVideo-2. As seen in (b) and (c), motion control tends to dominate over subject learning during training, causing the degradation of subject identity. In (d), our method ensures a balance between subject and motion control.", "description": "Figure 3 shows that simple joint training of subject and motion leads to motion control dominating over subject learning, while the proposed method balances them effectively.", "section": "4.3 BALANCING SUBJECT LEARNING AND MOTION CONTROL"}, {"figure_path": "2410.13830/figures/figures_8_0.png", "caption": "Figure 4: Qualitative comparison of joint subject customization and motion control. DreamVideo-2 generates videos with customized subjects and precise motion trajectory control, while other methods suffer from control conflicts, especially when trained on a single image.", "description": "Figure 4 shows a qualitative comparison of DreamVideo-2 against other methods for jointly customizing video subjects and their motion trajectories.", "section": "5.2 Main Results"}, {"figure_path": "2410.13830/figures/figures_9_0.png", "caption": "Figure 1: Customized video generation results of Dream Video-2. Our method precisely generates customized subjects at specified positions without fine-tuning at inference time.", "description": "Figure 1 shows example results of DreamVideo-2, demonstrating its ability to generate customized videos with specific subjects and motions.", "section": "ABSTRACT"}, {"figure_path": "2410.13830/figures/figures_9_1.png", "caption": "Figure 6: Qualitative comparison of motion control. Our DreamVideo-2 achieves precise motion trajectory control and effectively maintains subjects within the specified bounding boxes.", "description": "The figure qualitatively compares DreamVideo-2's motion control with other methods, showcasing its precision in maintaining subjects within specified bounding boxes and achieving accurate trajectory control.", "section": "5.2 Main Results"}, {"figure_path": "2410.13830/figures/figures_10_0.png", "caption": "Figure 8: Qualitative ablation studies on each component and blended mask weight.", "description": "The figure shows qualitative and quantitative ablation study results on each component of DreamVideo-2 and the effect of blended mask weight on the model's performance.", "section": "4.3 BALANCING SUBJECT LEARNING AND MOTION CONTROL"}, {"figure_path": "2410.13830/figures/figures_10_1.png", "caption": "Figure 1: Customized video generation results of Dream Video-2. Our method precisely generates customized subjects at specified positions without fine-tuning at inference time.", "description": "The figure shows example results of DreamVideo-2, demonstrating its ability to generate videos with customized subjects and motions at specified locations without requiring further fine-tuning.", "section": "Abstract"}, {"figure_path": "2410.13830/figures/figures_18_0.png", "caption": "Figure 9: Pipeline of dataset construction.", "description": "The figure illustrates the process of constructing the dataset, including video caption extraction, subject word identification, mask generation using Grounding DINO, SAM, and DEVA, and human evaluation.", "section": "A.1 DATASET CONSTRUCTION"}, {"figure_path": "2410.13830/figures/figures_20_0.png", "caption": "Figure 1: Customized video generation results of Dream Video-2. Our method precisely generates customized subjects at specified positions without fine-tuning at inference time.", "description": "Figure 1 shows example results of DreamVideo-2, demonstrating its ability to generate customized videos with specified subjects and motions.", "section": "ABSTRACT"}, {"figure_path": "2410.13830/figures/figures_21_0.png", "caption": "Figure 1: Customized video generation results of DreamVideo-2. Our method precisely generates customized subjects at specified positions without fine-tuning at inference time.", "description": "Figure 1 shows example results of DreamVideo-2, demonstrating its ability to generate videos with customized subjects and motion trajectories from a single image and bounding box sequence, respectively, without requiring any fine-tuning.", "section": "ABSTRACT"}]