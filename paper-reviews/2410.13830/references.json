{"references": [{" publication_date": "2022", "fullname_first_author": "Jonathan Ho", "paper_title": "Video diffusion models", "reason": "This paper is foundational for the DreamVideo-2 model, introducing the core concept of video diffusion models (VDMs). VDMs are crucial for the proposed approach, enabling the generation of high-quality videos by iteratively removing noise from a latent representation. The training objective focuses on minimizing the difference between the original noise and the noise predicted by the model.  Understanding VDMs is critical for appreciating the overall methodology of DreamVideo-2 and its improvements.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Mathilde Caron", "paper_title": "Emerging properties in self-supervised vision transformers", "reason": "This paper introduces the self-supervised vision transformer model which is the backbone of the DreamVideo-2 model. Understanding the self-supervised vision transformer architecture is necessary to appreciate the technical details of the proposed method and its effectiveness in learning robust representations for video generation.  This paper's contributions to vision transformer architectures is highly relevant because the proposed method leverages them for video customization, especially in the attention mechanism which is crucial in balancing subject and motion control.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This paper is fundamental to DreamVideo-2 as it introduces the denoising diffusion probabilistic models.  These models are crucial for generating high-quality and diverse video content.  The paper's core concepts, including the forward and reverse diffusion processes, along with the training objective, are essential components of the DreamVideo-2 method, contributing to its ability to effectively manage noise during video synthesis.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Andreas Blattmann", "paper_title": "Stable video diffusion: Scaling latent video diffusion models to large datasets", "reason": "This work provides significant advancements in latent video diffusion models, directly influencing the design and performance of DreamVideo-2.  Improving the stability and scalability of latent video diffusion models is crucial for handling larger datasets and generating more realistic and consistent videos.  The paper's focus on improving latent space representations is important for the successful implementation of DreamVideo-2's architecture.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Haoxin Chen", "paper_title": "Videocrafter1: Open diffusion models for high-quality video generation", "reason": "This paper introduces the VideoCrafter model, a significant advancement in text-to-video generation.  VideoCrafter's ability to generate high-quality videos directly impacts the effectiveness of DreamVideo-2 because it offers a strong foundation for customized video generation. By leveraging the advancements in VideoCrafter, DreamVideo-2 is able to focus on improving subject learning and motion control without sacrificing overall video quality.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "This is a highly influential paper for its advancements in high-resolution image synthesis using latent diffusion models.  DreamVideo-2 builds upon this foundation to adapt the latent diffusion model for video generation.  The techniques developed for high-resolution image synthesis are directly applicable to generating high-quality videos.  The advancements in handling high-resolution images are crucial to the success of DreamVideo-2.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Eyal Molad", "paper_title": "Dreamix: Video diffusion models are general video editors", "reason": "This paper presents Dreamix, a method that utilizes video diffusion models for general video editing. This is highly relevant to DreamVideo-2 because it demonstrates the versatility of video diffusion models in manipulating and customizing video content. DreamVideo-2 uses similar techniques but focuses on specific aspects like subject learning and motion control.  Understanding Dreamix is essential for analyzing the design choices and contributions of DreamVideo-2.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Nataniel Ruiz", "paper_title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation", "reason": "This paper introduces Dreambooth, a method for fine-tuning text-to-image diffusion models for subject-driven generation. This is highly relevant to DreamVideo-2 as it provides a basis for subject customization. DreamVideo-2 takes a different approach that avoids fine-tuning, but Dreambooth is a significant predecessor, demonstrating the potential of customizing video generation.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Max Bain", "paper_title": "Frozen in time: A joint video and image encoder for end-to-end retrieval", "reason": "This paper presents a novel approach to video and image encoding which forms the basis for DreamVideo-2. By leveraging this encoder, the model is able to effectively learn subject appearances and motion trajectories simultaneously. This paper is fundamental to the methodology of DreamVideo-2 as it provides a framework for injecting image information for subject learning and combining it with bounding boxes for motion control.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yujie Wei", "paper_title": "DreamVideo: Composing your dream videos with customized subject and motion", "reason": "This paper introduces DreamVideo, a framework for video generation with customized subject and motion. This is highly relevant to DreamVideo-2 because it addresses a similar problem. DreamVideo-2 builds upon this foundation but introduces novel techniques that eliminate the need for test-time fine-tuning and effectively balances subject learning and motion control, leading to improved performance.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tianxing Wu", "paper_title": "MotionBooth: Motion-aware customized text-to-video generation", "reason": "This work tackles the problem of motion control in text-to-video generation. It's highly relevant to DreamVideo-2 because both approaches focus on controlling motion within generated videos.  However, DreamVideo-2 introduces innovative techniques like masked reference attention and reweighted diffusion loss, enabling it to achieve better subject preservation and motion control in a zero-shot manner.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Shiwei Zhang", "paper_title": "I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models", "reason": "This work contributes significantly to image-to-video synthesis, which is directly relevant to the subject learning aspect of DreamVideo-2. The use of cascaded diffusion models for generating high-quality videos provides a strong technical basis for DreamVideo-2.  The image-to-video synthesis capabilities greatly influence the effectiveness of the reference attention component in DreamVideo-2. ", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuming Jiang", "paper_title": "Videobooth: Diffusion-based video generation with image prompts", "reason": "This paper is highly relevant to the DreamVideo-2 paper as it also addresses the problem of zero-shot video customization.  VideoBooth employs a tuning-free framework for subject customization, however, it lacks comprehensive motion control.  By analyzing VideoBooth, we can better appreciate DreamVideo-2's advancements in balancing subject learning and motion control and achieving more precise motion trajectory generation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper introduces CLIP (Contrastive Language\u2013Image Pre-training), a powerful model for connecting image and text embeddings.  This is directly used in DreamVideo-2's evaluation metrics, particularly CLIP-T, CLIP-I, and R-CLIP, which assess the overall quality of generated videos, including subject and text alignment. CLIP is essential for establishing the quality benchmarks used in evaluating DreamVideo-2's performance.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Alexander Kirillov", "paper_title": "Segment anything", "reason": "This paper introduces the Segment Anything Model (SAM), a highly versatile model for image segmentation.  It directly contributes to the DreamVideo-2 dataset construction by enabling the precise segmentation of subject images, which is a crucial step in training the model.  The accuracy and efficiency of SAM's segmentation capabilities greatly influence the quality of the resulting training data and thus the overall performance of DreamVideo-2.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Shilong Liu", "paper_title": "Grounding dino: Marrying dino with grounded pre-training for open-set object detection", "reason": "This paper introduces Grounding DINO, a powerful model for object detection and grounding, which is used in DreamVideo-2's dataset construction for annotating subject bounding boxes in video frames.  The accuracy and efficiency of Grounding DINO's object detection capabilities are crucial for the quality and reliability of the dataset used to train and evaluate DreamVideo-2.", "section_number": 5}, {" publication_date": "2021", "fullname_first_author": "Jordi Pont-Tuset", "paper_title": "The 2017 davis challenge on video object segmentation", "reason": "This paper introduces the DAVIS dataset, which is used as a reference for bounding boxes in DreamVideo-2's experimental setup.  The DAVIS dataset is particularly important for assessing the model's motion control capabilities.  Using the DAVIS dataset's established benchmark enables a fair and robust comparison with other methods for motion control.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Haonan Qiu", "paper_title": "Freetraj: Tuning-free trajectory control in video diffusion models", "reason": "This paper introduces FreeTraj, a model for motion control in video generation, used as a baseline in the experiments of DreamVideo-2.  FreeTraj demonstrates a specific approach to motion control that differs from DreamVideo-2's methods. The comparison with FreeTraj's approach enables evaluating DreamVideo-2's improvements and establishing its relative advantages in terms of efficiency and the trade-off between subject learning and motion control.  The evaluation metrics using FreeTraj showcase the efficacy and advantages of DreamVideo-2.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Shiyuan Yang", "paper_title": "Direct-a-video: Customized video generation with user-directed camera movement and object motion", "reason": "This paper introduces Direct-a-Video, a method for video customization that allows users to direct camera movement and object motion.  It's highly relevant to DreamVideo-2, as it addresses similar issues in customizing video generation. DreamVideo-2 improves upon Direct-a-Video's techniques and addresses the issue of motion control dominance in existing methods, resulting in a more balanced learning approach for subject appearance and motion control.", "section_number": 5}]}