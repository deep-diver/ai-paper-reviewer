[{"figure_path": "https://arxiv.org/html/2501.11067/extracted/6137699/figures/overview.png", "caption": "Figure 1: System diagram. (1) Given a chatbot prompt and a Schema DB, the system generates an event that targets a subset of policies, which includes a user request and a system DB state. (2) For each event the system simulates a conversation between the user and the chatbot. (3) A fine-grained report on the chatbot performances is generated.", "description": "The IntellAgent system takes as input a chatbot prompt and database schema.  It uses this information to generate an event, which simulates a user request and defines the initial state of the system's database. The event is designed to test a specific subset of predefined policies.  Then, a simulated conversation between the user and the chatbot is conducted based on that generated event. Finally, IntellAgent produces a detailed report that analyzes the chatbot's performance in handling the event, focusing on fine-grained metrics related to policy adherence and overall effectiveness.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2501.11067/extracted/6137699/figures/event_generator_architecture.png", "caption": "Figure 2: Model success rates across different challenge levels. While all models show reduced performance as the challenge level increases, they exhibit distinct patterns of decline, differing in both the onset level and the magnitude of the decrease.", "description": "This figure displays the success rates of several conversational AI models across varying challenge levels.  Each model's performance is plotted against increasing challenge levels. The graph illustrates that, while all models show decreased success rates as challenge levels increase, their decline rates and starting points differ significantly. Some models start showing decreased performance at lower challenge levels compared to others, and some exhibit steeper performance drops as challenge levels escalate than others.  This variation highlights the distinct capabilities and limitations of each model in handling complex conversational tasks.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2501.11067/extracted/6137699/figures/dialog_architecture.png", "caption": "Figure 3: Comparison of the success rates of the top four models across various policy categories, highlighting that some categories are more challenging than others. Additionally, the relative performance order of different models varies across categories.", "description": "This figure presents a bar chart comparing the success rates of four top-performing conversational AI models across different policy categories.  The x-axis represents various policy categories (e.g., authentication, escalation, payment handling), while the y-axis shows the success rate for each model within each category.  The chart highlights that success rates vary significantly across different policy categories, indicating that some categories pose greater challenges than others for these models.  Additionally, it shows that the relative performance ranking of the four models is not consistent across all policy categories; a model performing well in one category might perform poorly in another.  This demonstrates that different models have different strengths and weaknesses with respect to specific policies, emphasizing the need for comprehensive evaluation across a variety of scenarios.", "section": "4.2 Results"}]