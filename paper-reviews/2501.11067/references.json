{"references": [{"fullname_first_author": "M. Abbasian", "paper_title": "Conversational health agents: A personalized LLM-powered agent framework", "publication_date": "2023", "reason": "This paper is foundational to the field of conversational AI systems and their applications in healthcare, making it one of the most important references."}, {"fullname_first_author": "L. Alberts", "paper_title": "Curate: Benchmarking personalized alignment of conversational AI assistants", "publication_date": "2025", "reason": "This paper provides a benchmark for evaluating the alignment of conversational AI agents, addressing a critical challenge in the field."}, {"fullname_first_author": "S. Arcadinho", "paper_title": "Automated test generation to evaluate tool-augmented LLMs as conversational AI agents", "publication_date": "2024", "reason": "This work introduces a novel automated method for evaluating conversational AI agents, which enhances the efficiency and scalability of the evaluation process."}, {"fullname_first_author": "D. Banerjee", "paper_title": "Benchmarking LLM powered chatbots: Methods and metrics", "publication_date": "2023", "reason": "This paper focuses on benchmarking LLM-powered chatbots and provides metrics and methods to assess the performance of these models, addressing a crucial aspect of evaluating conversational AI systems."}, {"fullname_first_author": "D. Castillo-Bolado", "paper_title": "Beyond prompts: Dynamic conversational benchmarking of large language models", "publication_date": "2024", "reason": "This paper introduces a dynamic benchmarking approach that goes beyond static prompts, offering a more realistic and comprehensive evaluation methodology for conversational AI systems."}]}