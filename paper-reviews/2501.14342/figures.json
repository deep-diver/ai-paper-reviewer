[{"figure_path": "https://arxiv.org/html/2501.14342/x1.png", "caption": "(a) Test-time scaling behavior of CoRAG.", "description": "This figure shows how the performance of the CoRAG model scales with the increase of the average number of tokens consumed during testing. The x-axis represents the average number of tokens consumed per test instance, which sums both prompt and generated tokens. The y-axis shows the exact match (EM) score, a common metric for evaluating the accuracy of question-answering models. The curve shows that increasing token consumption initially leads to significant improvement in the EM score, indicating better performance by allowing for more extensive reasoning steps. However, the gains gradually diminish as the token consumption increases beyond a certain point.", "section": "3.3 Test-time Scaling"}, {"figure_path": "https://arxiv.org/html/2501.14342/x2.png", "caption": "(b) An example of CoRAG in action.", "description": "The figure shows an example of a multi-hop question answering process using the Chain-of-Retrieval Augmented Generation (CoRAG) model.  The user asks a question, \"Where did the star of Dark Hazard study?\", and the system uses multiple retrieval steps to find the answer. First, it retrieves the star's name, \"Edward G. Robinson\". Then, it reformulates the query to \"What college did Edward G. Robinson attend?\" and retrieves the answer, \"City College of New York\". This illustrates the dynamic reformulation of queries and the step-by-step retrieval process of CoRAG, highlighting its capability in addressing complex questions requiring multiple hops of reasoning.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.14342/x3.png", "caption": "Figure 1: Overview of CoRAG.\nRejection sampling is utilized to augment QA-only datasets with retrieval chains.\nEach chain starts with the original query,\nfollowed by a sequence of sub-queries and sub-answers.\nAn open-source LLM is then fine-tuned to predict the next action based on the current state.\nDuring inference,\nmultiple decoding strategies are available to control the test-time compute.", "description": "CoRAG (Chain-of-Retrieval Augmented Generation) is a novel approach for training RAG models that retrieve and reason over relevant information step by step before generating the final answer.  The figure illustrates the CoRAG framework. It begins with rejection sampling to augment standard QA datasets by creating retrieval chains. Each chain starts with the original query and iteratively generates sub-queries and corresponding sub-answers.  These chains are used to train an open-source large language model (LLM) to predict the next action (sub-query or sub-answer) based on the current state of the chain. During the inference stage (when generating answers to new questions), multiple decoding strategies are employed to control the computational cost and balance it with the desired accuracy.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2501.14342/x4.png", "caption": "Figure 2: Scaling test-time compute on multi-hop QA datasets.\nThe Pareto frontier is in the form of y=a\u00d7log\u2061(x+b)+c\ud835\udc66\ud835\udc4e\ud835\udc65\ud835\udc4f\ud835\udc50y=a\\times\\log(x+b)+citalic_y = italic_a \u00d7 roman_log ( italic_x + italic_b ) + italic_c fitted on the Pareto optimal points.\nA point is considered Pareto optimal if no other point achieves a higher EM score with less token consumption.\nThe metric \u201c# Avg. Tokens\u201d represents the average number of tokens consumed per test instance,\nsumming up both the prompt and generated tokens.", "description": "This figure examines the trade-off between model performance (measured by Exact Match score) and computational cost (measured by the average number of tokens used per query) in multi-hop question answering.  It shows how different decoding strategies (greedy, best-of-N sampling, tree search) and retrieval chain lengths affect this trade-off.  The Pareto frontier is visualized, illustrating the optimal balance between performance and cost for each strategy. Points on the Pareto frontier represent situations where no other strategy could achieve a higher EM score with fewer tokens. The relationship between tokens and EM score is modeled by a logarithmic function, y = a \u00d7 log(x + b) + c, where y represents EM and x represents average tokens.", "section": "3.3 Test-time Scaling"}, {"figure_path": "https://arxiv.org/html/2501.14342/x5.png", "caption": "Figure 3: Scaling test-time compute across three datasets from the KILT benchmark.\nWe report scores on the public validation set.", "description": "This figure shows how varying the retrieval chain length impacts the performance and computational cost of the CoRAG model on three different tasks from the KILT benchmark.  The x-axis represents the length of the retrieval chain (number of retrieval steps), and the y-axis represents the performance (accuracy or exact match) achieved on the tasks. Different lines represent various decoding strategies employed by the model.  The figure highlights the trade-off between achieving higher accuracy and managing computational cost by adjusting the retrieval chain length. The scores are those achieved on the public validation set of the KILT benchmark.", "section": "Scaling Test-Time Compute"}, {"figure_path": "https://arxiv.org/html/2501.14342/x6.png", "caption": "Figure 4: Learning to stop at test time.\nLarger logit bias values result in earlier stopping.\nL=6\ud835\udc3f6L=6italic_L = 6 correspond to always performing 6666 retrieval steps,\nwhile L=0\ud835\udc3f0L=0italic_L = 0 indicate no intermediate retrieval steps.", "description": "This figure illustrates the effect of a learnable \"stop\" mechanism that determines when to halt the iterative retrieval process within the Chain-of-Retrieval Augmented Generation (CoRAG) model.  The experiment varies the logit bias associated with the 'stop' prediction. A higher logit bias incentivizes the model to stop earlier, reducing the number of retrieval steps. The results show the trade-off between the model's performance (measured by Exact Match score) and the number of tokens consumed for different stopping strategies.  The x-axis represents the logit bias of the stop decision, and the y-axis depicts the Exact Match score and token consumption. The different colored lines represent different numbers of retrieval steps: L=6 (always using 6 retrieval steps) and L=0 (no intermediate retrieval steps).  This allows for examination of how a learnable stopping mechanism affects efficiency without sacrificing accuracy.", "section": "Scaling Test-time Compute"}, {"figure_path": "https://arxiv.org/html/2501.14342/x7.png", "caption": "Figure 5: Scaling rejection sampling compute for training data generation.\nWe vary the number of sampled chains from 4444 to 16161616 while maintaining all other hyperparameters fixed.", "description": "This figure illustrates the impact of varying the number of retrieval chains sampled during the rejection sampling process on the performance of the model.  The experiment systematically varies the number of sampled chains, increasing it from 4 to 16, while keeping all other hyperparameters constant.  The x-axis represents the number of sampled chains, and the y-axis displays the resulting EM (Exact Match) score on four different multi-hop question answering datasets.  The purpose is to investigate the effect of the quantity of training data generated through rejection sampling on model performance.", "section": "5.1 Iterative Rejection Sampling"}, {"figure_path": "https://arxiv.org/html/2501.14342/x8.png", "caption": "Figure 6: Effects of varying the sampling temperature on multi-hop QA datasets.", "description": "This figure displays the impact of different sampling temperatures on the performance of the CoRAG model across four multi-hop question answering datasets (2WikiMultihopQA, HotpotQA, Bamboogle, and MuSiQue).  Each dataset has a separate subplot showing the Exact Match (EM) score on the y-axis and the sampling temperature on the x-axis. Different lines represent different decoding strategies (greedy, best-of-4, best-of-8).  The plots reveal how the choice of sampling temperature influences the balance between diversity and quality in the retrieval chains generated during training, which in turn affects the final model's performance on these datasets. The results suggest that there is no universally optimal sampling temperature, and its effectiveness can vary based on the dataset.", "section": "5.3 Does Chain-of-Retrieval Always Help?"}]