{"references": [{"fullname_first_author": "Daryna Dementieva", "paper_title": "Overview of the multilingual text detoxification task at PAN 2024", "publication_date": "2024-09-12", "reason": "This paper introduces the MultiParaDetox dataset, a benchmark for multilingual text detoxification, which is used for comparison in the current research."}, {"fullname_first_author": "Varvara Logacheva", "paper_title": "Paradetox: Detoxification with parallel data", "publication_date": "2022-05-27", "reason": "This paper is a key reference because it introduces a successful approach to text detoxification using parallel data and large language models, which is relevant to the current work."}, {"fullname_first_author": "David Dale", "paper_title": "Text detoxification using large pre-trained neural models", "publication_date": "2021-11-07", "reason": "This paper explores unsupervised methods for text detoxification using large pre-trained language models, providing a foundation for the current research."}, {"fullname_first_author": "Katherine Atwell", "paper_title": "APPDIA: A discourse-aware transformer-based style transfer model for offensive social media conversations", "publication_date": "2022-10-12", "reason": "This paper presents a model that addresses the task of text detoxification in social media conversations, demonstrating an approach to mitigating online toxicity."}, {"fullname_first_author": "Daniil Moskovskiy", "paper_title": "LLMs to replace crowdsourcing for parallel data creation? The case of text detoxification", "publication_date": "2024-11-16", "reason": "This paper explores the use of LLMs to generate synthetic parallel data for text detoxification, a technique used in the current work, and compares performance against human-annotated datasets."}]}