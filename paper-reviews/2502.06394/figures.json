[{"figure_path": "https://arxiv.org/html/2502.06394/x1.png", "caption": "\u0420\u0438\u0441. 1: An illustration of the proposed approach for collecting and generating the multilingual text detoxification dataset SynthDetoxM.", "description": "This figure illustrates the process of creating the SynthDetoxM multilingual text detoxification dataset. It starts with collecting multilingual toxic data from various sources.  This data is then processed using several modern large language models (LLMs) via few-shot prompting to generate corresponding detoxified text.  The LLM-generated detoxification candidates are scored and filtered based on quality metrics such as toxicity and semantic similarity to the original text.  Finally, the best detoxification candidates are selected to form the SynthDetoxM dataset.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.06394/x2.png", "caption": "\u0420\u0438\u0441. 2: Number of accepted samples in the final SynthDetoxM\u00a0dataset with respect to the LLM by language.", "description": "This figure shows the number of samples accepted into the final SynthDetoxM dataset for each language (German, Spanish, French, Russian) and for each of the nine Large Language Models (LLMs) used in its creation. The bar chart allows for a comparison of the contribution of each LLM to the overall size of the dataset for each language.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.06394/x3.png", "caption": "\u0420\u0438\u0441. 3: Distribution of STA toxicity scores of toxic and neutral examples in the dataset. The original toxic texts are in orange, while detoxified texts are in blue. For readability we apply Gaussian smoothing.", "description": "Figure 3 presents the distribution of toxicity scores (STA) for both original toxic and their corresponding detoxified versions across four different languages.  The x-axis represents the STA score, ranging from 0 (non-toxic) to 1 (highly toxic). The y-axis represents the frequency or count of texts within a given STA score range. Original toxic texts are displayed in orange, while their detoxified counterparts are shown in blue. A Gaussian smoothing technique has been applied to enhance the visual clarity and readability of the distributions. This visualization helps illustrate the effectiveness of the detoxification process in reducing the toxicity levels of the text.", "section": "3.3 Data Quality Evaluation Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.06394/", "caption": "\u0420\u0438\u0441. 4: Side-by-side comparison of model outputs across all languages, evaluated by GPT-4o. The results highlight the relative performance of the models in generating detoxified text for German, Russian, and Spanish. The notation is similar to the notation from Table\u00a03.", "description": "Figure 4 presents a side-by-side comparison of the outputs generated by different models when detoxifying text in German, Russian, and Spanish.  GPT-4 acted as an evaluator to determine which of the two model outputs (one from a model trained on the SynthDetoxM dataset and the other from a model trained on MultiParaDetox) produced a better detoxification result for each example. The results visualize the relative performance of models trained on SynthDetoxM compared to models trained on the smaller MultiParaDetox dataset, illustrating the effectiveness of SynthDetoxM in training high-performing detoxification models. The color-coding and bar lengths in the chart represent the percentage of times each model was preferred by GPT-4, and the notation aligns with that of Table 3.", "section": "4 Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2502.06394/x5.png", "caption": "\u0420\u0438\u0441. 5: Detoxification prompt we use for synthetic parallel data generation. {toxic_text} stands for a placeholder for a given toxic text being prompted into LLM. In few-shot setting we add few examples of detoxification before last two lines and write: Here are few examples:.", "description": "This figure illustrates the prompt used to generate synthetic parallel data for text detoxification.  The prompt instructs a large language model (LLM) to rewrite a given toxic text into a non-toxic version while preserving the original meaning. The {toxic_text} placeholder represents the input toxic sentence.  The prompt also includes a few-shot learning component, where a few example pairs of toxic and detoxified texts are provided to guide the model's generation. The instruction to provide only the generated text (and not the input text again) helps maintain data quality and avoid unnecessary repetition.", "section": "3.1.1 Parallel Data Generation Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.06394/x6.png", "caption": "\u0420\u0438\u0441. 6: Detoxification prompt we use for mT0.", "description": "This figure shows the text of the prompt used for fine-tuning the mT0 model in the paper. The prompt instructs the model to rewrite toxic text into non-toxic text while maintaining the original meaning and style as much as possible.  The few-shot learning setting is implied, though not explicitly stated in the prompt itself.  This is a crucial component of the methodology described in the paper, as it details how the model is trained to perform the text detoxification task.", "section": "3.1.2 Few-Shot Example Mining"}, {"figure_path": "https://arxiv.org/html/2502.06394/x7.png", "caption": "\u0420\u0438\u0441. 7: Refusal generation prompt for synthetic refusals dataset.", "description": "This figure shows the prompt used to generate synthetic refusal data for training a refusal classification model.  The prompt instructs a large language model (LLM) to politely refuse to answer a given input text and to provide a reason for the refusal. The refusal should be relevant to the input text. The prompt ensures the LLM's response is concise and focuses only on the refusal itself without adding unrelated information.", "section": "3.1.1 Parallel Data Generation Pipeline"}]