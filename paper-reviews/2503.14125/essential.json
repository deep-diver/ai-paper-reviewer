{"importance": "This paper introduces Frac-Connections, an efficient alternative to Hyper-Connections, potentially impacting deep learning architecture design. It offers a novel approach to balancing model performance and memory usage, and opens new avenues for research in optimizing deep learning models.", "summary": "Frac-Connections: An efficient alternative to Hyper-Connections that divides hidden states into fractions.", "takeaways": ["Frac-Connections offer a way to retain the benefits of Hyper-Connections while reducing memory consumption.", "Experiments on language tasks, including training a 7B MoE model, show Frac-Connections outperform residual connections.", "The simplicity, scalability, and efficiency of Frac-Connections may enable their widespread adoption."], "tldr": "Residual connections are crucial in deep learning, but can cause issues like gradient vanishing and representation collapse. Hyper-Connections were introduced to address these problems by using multiple connection strengths. However, Hyper-Connections increase memory access costs. To solve the trade-off between memory usage and expressiveness of connections, this paper introduces Frac-Connections.\n\nFrac-Connections divide hidden states into multiple parts instead of expanding their width. This method retains partial benefits of Hyper-Connections but reduces memory consumption. Experiments on large language tasks, including a 7B MoE model trained on 3T tokens, demonstrate that Frac-Connections **significantly outperform residual connections**. Frac-Connections shows better training stability and downstream task performance across various NLP benchmarks.", "affiliation": "ByteDance Seed", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "2503.14125/podcast.wav"}