[{"figure_path": "https://arxiv.org/html/2503.14125/x1.png", "caption": "Figure 1: Comparison of Frac-Connections and Hyper-Connections based on their expansion rates. Frac-Connections correspond to n\u22641\ud835\udc5b1n\\leq 1italic_n \u2264 1, while Hyper-Connections are defined by n\u22651\ud835\udc5b1n\\geq 1italic_n \u2265 1. The two connection types become identical when the expansion rate is n=1\ud835\udc5b1n=1italic_n = 1.", "description": "This figure compares Frac-Connections and Hyper-Connections, two types of residual connections used in deep learning models.  The key difference lies in their expansion rate (n). Frac-Connections have an expansion rate of n \u2264 1, meaning they divide the hidden state into multiple parts (the number of parts increases as n decreases towards 0).  Hyper-Connections, on the other hand, have an expansion rate of n \u2265 1, where they replicate the hidden state multiple times (the number of replications is n). When n = 1, both connection types are identical.  The figure visually represents the relationship between the expansion rate and the type of connection.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.14125/x2.png", "caption": "Figure 2: Cosine similarity between the input of the current and the previous layers for the OLMoE-7B models. The curve represents the median of similarity, while the shaded area indicates the range between the 5th and 95th percentiles.", "description": "Figure 2 shows the cosine similarity between the input of each layer and its previous layer for three OLMoE-7B models: a baseline model, a model with Hyper-Connections, and a model with Frac-Connections. The x-axis represents the layer index, and the y-axis represents the cosine similarity. The solid line represents the median cosine similarity, while the shaded area shows the interquartile range (IQR), which is the range between the 5th and 95th percentiles of the cosine similarity.  This visualization demonstrates how the different connection methods influence the similarity between adjacent layers.  Lower similarity is desirable because it indicates more diverse representations between adjacent layers, which can help to avoid representation collapse.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.14125/x18.png", "caption": "Figure 3: Figure 2. Frac-connections (FC) with an expansion rate of n=1/2\ud835\udc5b12n=1/2italic_n = 1 / 2.\n(a) Residual connections.\n(b) Hyper-connections: \u03b21subscript\ud835\udefd1\\beta_{1}italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, \u03b22subscript\ud835\udefd2\\beta_{2}italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, \u03b10,0subscript\ud835\udefc00\\alpha_{0,0}italic_\u03b1 start_POSTSUBSCRIPT 0 , 0 end_POSTSUBSCRIPT, \u03b10,1subscript\ud835\udefc01\\alpha_{0,1}italic_\u03b1 start_POSTSUBSCRIPT 0 , 1 end_POSTSUBSCRIPT, \u03b11,0subscript\ud835\udefc10\\alpha_{1,0}italic_\u03b1 start_POSTSUBSCRIPT 1 , 0 end_POSTSUBSCRIPT, \u03b11,1subscript\ud835\udefc11\\alpha_{1,1}italic_\u03b1 start_POSTSUBSCRIPT 1 , 1 end_POSTSUBSCRIPT, \u03b12,1subscript\ud835\udefc21\\alpha_{2,1}italic_\u03b1 start_POSTSUBSCRIPT 2 , 1 end_POSTSUBSCRIPT, and \u03b12,2subscript\ud835\udefc22\\alpha_{2,2}italic_\u03b1 start_POSTSUBSCRIPT 2 , 2 end_POSTSUBSCRIPT are learnable scalars or scalars predicted by the network, depending on the specific HC version.\n(c) Frac-connections: Frac-connections split the hidden representations into smaller fractions and process each fraction independently. The scalars \u03b31,2subscript\ud835\udefe12\\gamma_{1,2}italic_\u03b3 start_POSTSUBSCRIPT 1 , 2 end_POSTSUBSCRIPT, \u03b32,1subscript\ud835\udefe21\\gamma_{2,1}italic_\u03b3 start_POSTSUBSCRIPT 2 , 1 end_POSTSUBSCRIPT, and \u03b32,2subscript\ud835\udefe22\\gamma_{2,2}italic_\u03b3 start_POSTSUBSCRIPT 2 , 2 end_POSTSUBSCRIPT are either learnable or predicted by the network, similar to hyper-connections. These fractions are concatenated (denoted as Cat) after processing, followed by integration into the main network pipeline.", "description": "This figure compares residual connections, hyper-connections, and the proposed frac-connections.  Panel (a) shows a standard residual connection. Panel (b) illustrates hyper-connections, which introduce multiple learnable scalar weights connecting different depths of the network. Panel (c) presents frac-connections, a modification that divides the hidden states into multiple parts before processing, resulting in a more memory-efficient way to model multiple connection strengths. The learnable parameters are shown in the figure.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2503.14125/x19.png", "caption": "Figure 4: Training loss (0.999 EMA smoothed) loss for OLMoE-1.3B models.", "description": "Figure 4 presents the training loss curves for the OLMoE-1.3B model with various configurations of Frac-Connections.  It shows how the loss changes over the course of training (in billions of tokens) for different versions of the model: baseline (no Frac-Connections), Static Frac-Connections (SFC) with different fractional rates (SFCx2 and SFCx4), and Dynamic Frac-Connections (DFC) with different fractional rates (DFCx2 and DFCx4). Additionally, ablation studies are performed on the DFC model by removing normalization, the tanh activation function, or rescaling to analyze the impact of each component on performance. The loss is smoothed using a 0.999 Exponential Moving Average (EMA) filter for better visualization. The purpose of this figure is to demonstrate the improved training stability and efficiency of the Frac-Connection methods compared to the baseline and to understand the role of each component within the DFC model.", "section": "5.1 Ablation Study"}]