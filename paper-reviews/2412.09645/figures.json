[{"figure_path": "https://arxiv.org/html/2412.09645/x1.png", "caption": "Figure 1: An Example of Evaluation Agent. Existing evaluation methods typically assess visual generative models by extensively sampling from a fixed benchmark. In contrast, our Evaluation Agent framework requires only a small number of sampled images or videos, tailored to the user\u2019s specific evaluation request. Additionally, it goes beyond providing a simple numerical score by offering detailed explanations to the evaluation conclusions.", "description": "Figure 1 contrasts traditional evaluation methods with the proposed Evaluation Agent framework.  Traditional methods rely on extensive sampling from fixed benchmarks and provide numerical scores without detailed explanations.  The Evaluation Agent, however, uses a human-like strategy, requiring only a few samples tailored to a user's specific request.  It offers a multi-round, dynamic evaluation process with detailed explanations, enhancing efficiency and user understanding. The figure visually illustrates this through an example user query about a generative model's ability to generate objects and their attributes.  The traditional approach would require extensive sampling and provide only a numerical score (e.g., 0.89). The Evaluation Agent, in contrast, uses a thought process based on a few samples in multiple rounds.  It first evaluates simple prompts, then progresses to more complex evaluations like color attributes and spatial relationships. Finally, it provides a summary explanation of the model's strengths and weaknesses regarding object and attribute generation, rather than just a score.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.09645/x2.png", "caption": "Figure 2: Overview of Evaluation Agent Framework. This framework leverages LLM-powered agents for efficient and flexible visual model assessments. As shown, it consists of two stages: (a) the Proposal Stage, where user queries are decomposed into sub-aspects, and prompts are generated, and (b) the Execution Stage, where visual content is generated and evaluated using an Evaluation Toolkit. The two stages interact iteratively to dynamically assess models based on user queries.", "description": "The Evaluation Agent framework uses LLM-powered agents for efficient and flexible visual model assessments. It consists of two stages:\n1. **Proposal Stage**: User queries are decomposed into sub-aspects, and prompts are generated.\n2. **Execution Stage**: Visual content is generated and evaluated using an Evaluation Toolkit.  These two stages interact iteratively, creating a dynamic assessment process based on user queries, intermediate results, and feedback.", "section": "3.2 The Evaluation Agent Framework"}, {"figure_path": "https://arxiv.org/html/2412.09645/extracted/6071683/figures/fig_prompts_abl.png", "caption": "Figure 3: Validation on VBench Percentage Dimensions. We conducted additional validation experiments on VBench by increasing the number of prompts in each evaluation. For each model and dimension, lighter bars represent results with the original settings, darker bars with increased sample size. Hatched portions indicate predictions within the exact range, and solid portions within an error margin of one range. Specific numerical results are provided in Table\u00a06", "description": "This figure compares the Evaluation Agent's performance against VBench on four dimensions: Human Action, Scene, Color, and Object Class, using two models: Latte-1 and ModelScope.  Two bar styles are used for each model-dimension pair: lighter bars represent performance using the original, smaller set of prompts; darker bars represent performance with an increased number of prompts (30). Within each bar, the hatched section indicates the percentage of predictions where the agent's rating precisely matched VBench, and the solid section represents predictions within one rating level of VBench. This visualization demonstrates how the Evaluation Agent's performance improves with increased samples, especially for metrics relying on binary sample-level evaluations, and aims to show its effectiveness can be comparable to full benchmark pipelines with more samples.  The exact percentages for each setting are detailed in Table 6.", "section": "4.1.2 Results Analysis"}, {"figure_path": "https://arxiv.org/html/2412.09645/x3.png", "caption": "Figure 4: A Case of Open-Ended User Query Evaluation. For open-ended user queries, the Evaluation Agent systematically explores the model\u2019s capabilities in specific areas, starting from basic aspects and gradually delving deeper, culminating in a detailed analysis and summary. Please refer to the Appendix\u00a0E.2 for the complete results.", "description": "This figure shows an example of how the Evaluation Agent assesses a visual generative model based on an open-ended user query.  The user asks if the model can generate variations of existing artwork while maintaining the original style. The agent explores this by testing the model's ability to replicate basic art styles (like Impressionism), maintain style consistency in detail-oriented artworks (like replicating a Van Gogh with a skyline), and blend styles by merging elements from different artistic traditions (like adding elements of Indigenous Australian dot painting to Monet's Water Lilies). The results are then summarized to provide the user with a comprehensive analysis of the model\u2019s ability to handle variations within and across different artistic styles.", "section": "4.2 Experiments on Open-Ended User Query"}, {"figure_path": "https://arxiv.org/html/2412.09645/extracted/6071683/figures/open_dataset_stats.png", "caption": "Figure 5: Data Distribution of Open-Ended User Query Dataset. We analyze the constructed open-ended user query dataset from three aspects: General/Specific, Ability, and Specific Domain. The results indicate that our dataset exhibits a relatively balanced distribution across these dimensions.", "description": "This figure analyzes the distribution of the open-ended user query dataset across three dimensions: General/Specific, Ability, and Specific Domain. The General/Specific dimension categorizes queries as either addressing general model capabilities or focusing on specific functionalities. The Ability dimension labels the type of model capability being evaluated, such as prompt following, visual quality, creativity, knowledge, or other. The Specific Domain dimension identifies the area of application for the query, including law, film and entertainment, fashion, game design, architecture and interior design, medical, science and education, history and culture, and others. The distribution reveals a relatively balanced representation across these dimensions, indicating the dataset's diversity and suitability for evaluating a wide range of model capabilities.", "section": "4.2 Experiments on Open-Ended User Query"}, {"figure_path": "https://arxiv.org/html/2412.09645/extracted/6071683/figures/fig_performance_ind.png", "caption": "Figure 6: Performance Comparison across VBench Dimensions for Different Base Models. This visualization highlights the performance of all backbone models, including GPT-4o and Claude models, providing a comprehensive comparison in each dimension for different backbone models. Hatched portions indicate predictions within the exact range, and solid portions within an error margin of one range. Specific numerical results are provided in Table\u00a0C.2 and Table\u00a08", "description": "This figure visually compares the performance of different base models (GPT-40 and Claude) across various dimensions of the VBench benchmark for text-to-video generation. Each dimension's bar chart shows the accuracy of the model's predictions, with hatched sections representing predictions within the exact target range and solid sections showing predictions within one range of the target. The comparison demonstrates how different LLMs perform as the backbone of the evaluation agent, affecting its ability to assess video generation models.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09645/x4.png", "caption": "Figure 7: A Common Failure Pattern in Tool Selection. As shown in the figure, Gemini frequently selected an incorrect tool for evaluation. In this case, the model should have selected the \u201cAesthetic Quality\u201d tool, but it incorrectly chose \u201cSubject Consistency,\u201d leading to inaccuracies in subsequent assessments.", "description": "This figure shows an example where Gemini selected an incorrect tool for evaluation. Asked to assess aesthetics, it chose the \"Subject Consistency\" tool instead of \"Aesthetic Quality,\" leading to inaccurate assessments. This illustrates Gemini's challenges in tool selection during visual model evaluation.", "section": "E.1 Experiments on Different Base Models"}, {"figure_path": "https://arxiv.org/html/2412.09645/x5.png", "caption": "Figure 8: Common Failures in Generating Sub-Aspects and Finalizing Responses. The figure highlights two critical failures: first, Gemini fails to propose new sub-aspects based on observations from previous rounds, instead engaging in repetitive and meaningless loops without strictly adhering to the provided instructions. Second, this repetitive behavior leads to a non-stopping loop, ultimately failing to generate a meaningful final response to the user\u2019s query.", "description": "Gemini, when used as the backbone of the Evaluation Agent, exhibits two main failure modes.  First, it fails to propose new sub-aspects, engaging in repetitive loops without adapting to feedback. Second, these loops often prevent it from generating a final response to the user's query.", "section": "Supplementary -> A Detailed Explanation of Pipeline -> A.3 Dynamic Looping"}, {"figure_path": "https://arxiv.org/html/2412.09645/x6.png", "caption": "Figure 9: A Case of Open-Ended User Query Evaluation. This figure illustrates the Evaluation Agent\u2019s response to the user query, \u201cCan the model generate variations of existing artwork while maintaining the original style?\u201d", "description": "The Evaluation Agent assesses a model's ability to generate variations of existing artwork while maintaining the original style. It begins by replicating basic art styles (Impressionism), then tests style consistency in detail-oriented artworks (Van Gogh's Starry Night with a city skyline), and finally explores blending styles (integrating indigenous Australian dot painting into Monet's Water Lilies) and complex style integration (combining Japanese ukiyo-e with African tribal patterns). The results reveal proficiency in single-style variations but limitations in blending diverse styles.", "section": "4.2 Experiments on Open-Ended User Query"}, {"figure_path": "https://arxiv.org/html/2412.09645/x7.png", "caption": "Figure 10: A Case of Open-Ended User Query Evaluation. This figure illustrates the Evaluation Agent\u2019s response to the user query, \u201cHow precisely can the user specify object relationships?\u201d", "description": "This figure showcases an example of the Evaluation Agent's response to an open-ended user query concerning the precision of object relationships. The agent conducts multiple rounds of evaluations, starting with simple relationships and progressing to complex and abstract ones. Each round involves generating images, posing questions about the generated image to a VLM (Visual Language Model), and analyzing the VLM\u2019s answers to refine subsequent prompts.  The agent's thought process and the generated images provide insight into its evaluation strategy and its capabilities and limitations in understanding and generating spatial arrangements.", "section": "4.2 Experiments on Open-Ended User Query"}, {"figure_path": "https://arxiv.org/html/2412.09645/x8.png", "caption": "Figure 11: A Case of Open-Ended User Query Evaluation. This figure illustrates the Evaluation Agent\u2019s response to the user query, \u201cHow well the model can generate a specific number of objects?\u201d", "description": "This figure details the step-by-step evaluation process performed by the Evaluation Agent when responding to the open-ended user query, \"How well can the model generate a specific number of objects?\"  The figure presents a series of sub-aspects explored by the agent, each with corresponding prompts, generated images, questions posed to the VLM about the images, the VLM's answers, and finally, a summary of the findings. The exploration begins with simple scenarios involving small numbers of identical objects and progresses to more complex situations with varied object types and environments. The goal is to assess the model's capability to accurately generate the requested number of objects under diverse conditions.", "section": "4.2 Experiments on Open-Ended User Query"}]