{"references": [{"fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "publication_date": "2021-00-00", "reason": "This paper introduces VQGAN, a foundational model for visual tokenization that the current research builds upon and improves."}, {"fullname_first_author": "Doyup Lee", "paper_title": "Autoregressive image generation using residual quantization", "publication_date": "2022-00-00", "reason": "This paper presents RQ-VAE, a significant advancement in visual tokenization that directly addresses some of the limitations of VQGAN, which the current work aims to further improve upon."}, {"fullname_first_author": "Peize Sun", "paper_title": "Autoregressive model beats diffusion: Llama for scalable image generation", "publication_date": "2024-00-00", "reason": "This paper introduces LlamaGen, a state-of-the-art autoregressive image generation model that the current work uses as a baseline and aims to surpass."}, {"fullname_first_author": "Zhuoyan Luo", "paper_title": "Open-MAGVIT2: An open-source project toward democratizing auto-regressive visual generation", "publication_date": "2024-00-00", "reason": "This paper introduces Open-MAGVIT2, another state-of-the-art model in autoregressive image generation that uses a novel lookup-free quantization approach which is compared with the proposed method."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "This paper introduces CLIP, a crucial model for representation learning that is used in the current research to improve the semantic richness of the visual tokens."}]}