[{"content": "| Method | Downsample | Codebook Size | Code Dim | rFID \u2193 | PSNR \u2191 |\n|---|---|---|---|---|---| \n| VQGAN [6] | 16 | 16384 | 256 | 4.98 | - |\n| SD-VQGAN [23] | 16 | 16384 | 4 | 5.15 | - |\n| RQ-VAE [12] | 16 | 16384 | 256 | 3.20 | - |\n| LlamaGen [25] | 16 | 16384 | 8 | 2.19 | 20.79 |\n| Titok-B [36] | - | 4096 | 12 | 1.70 | - |\n| VQGAN-LC [41] | 16 | 100000 | 8 | 2.62 | 23.80 |\n| VQ-KD [30] | 16 | 8192 | 32 | 3.41 | - |\n| VILA-U [31] | 16 | 16384 | 256 | 1.80 | - |\n| Open-MAGVIT2 [15] | 16 | 262144 | 1 | 1.17 | 21.90 |\n| **FQGAN-Dual** | 16 | 16384 \u00d7 2 | 8 | **0.94** | 22.02 |\n| **FQGAN-Triple** | 16 | 16384 \u00d7 3 | 8 | **0.76** | **22.73** |\n| SD-VAE \u2020 [23] | 8 |  | 4 | 0.74 | 25.68 |\n| SDXL-VAE \u2020 [19] | 8 | - | 4 | 0.68 | 26.04 |\n| ViT-VQGAN [33] | 8 | 8192 | 32 | 1.28 | - |\n| VQGAN * [6] | 8 | 16384 | 4 | 1.19 | 23.38 |\n| SD-VQGAN * [23] | 8 | 16384 | 4 | 1.14 | - |\n| OmniTokenizer [29] | 8 | 8192 | 8 | 1.11 | - |\n| LlamaGen [25] | 8 | 16384 | 8 | 0.59 | 25.45 |\n| Open-MAGVIT2 [15] | 8 | 262144 | 1 | 0.34 | 26.19 |\n| **FQGAN-Dual** | 8 | 16384 \u00d7 2 | 8 | **0.32** | **26.27** |\n| **FQGAN-Triple** | 8 | 16384 \u00d7 3 | 8 | **0.24** | **27.58** |", "caption": "Table 1: Comparisons with other image tokenziers. Reconstruction performance of different tokenizers on 256\u00d7256256256256\\times 256256 \u00d7 256 ImageNet 50k validation set. All models are trained on ImageNet, except \u201c\u2217*\u2217\u201d on OpenImages and \u201c\u2020\u2020\\dagger\u2020\u201d on unknown training data.\nBold denotes the best scores; underline denotes the second place.", "description": "This table compares the reconstruction performance of various visual tokenizers on the ImageNet 50k validation set.  The models were evaluated using 256x256 pixel images.  The table shows the reconstruction performance metrics (rFID, PSNR) for each model, along with relevant hyperparameters, like codebook size and dimensionality, downsampling ratio, and the number of parameters. Note that some models were trained on datasets other than ImageNet (indicated by * and \u2020), and the best and second-best results are highlighted in bold and underlined, respectively.", "section": "4. Experiment"}, {"content": "| Type | Model | #Para. | FID\u2193 | IS\u2191 | Precision\u2191 | Recall\u2191 |\n|---|---|---|---|---|---|---|\n| Diffusion | ADM [5] | 554M | 10.94 | 101.0 | 0.69 | 0.63 |\n|  | CDM [10] | - | 4.88 | 158.7 | - | - |\n|  | LDM-4 [23] | 400M | 3.60 | 247.7 | - | - |\n|  | DiT-XL/2 [18] | 675M | 2.27 | 278.2 | 0.83 | 0.57 |\n| LFQ AR | Open-MAGVIT2-B [15] | 343M | 3.08 | 258.26 | 0.85 | 0.51 |\n|  | Open-MAGVIT2-L [15] | 804M | 2.51 | 271.70 | 0.84 | 0.54 |\n| VQ AR | VQGAN [6] | 227M | 18.65 | 80.4 | 0.78 | 0.26 |\n|  | VQGAN [6] | 1.4B | 15.78 | 74.3 | - | - |\n|  | VQGAN-re [6] | 1.4B | 5.20 | 280.3 | - | - |\n|  | ViT-VQGAN [33] | 1.7B | 4.17 | 175.1 | - | - |\n|  | ViT-VQGAN-re [33] | 1.7B | 3.04 | 227.4 | - | - |\n|  | RQTran. [12] | 3.8B | 7.55 | 134.0 | - | - |\n|  | RQTran.-re [12] | 3.8B | 3.80 | 323.7 | - | - |\n|  | LlamaGen-L [25] | 343M | 3.80 | 248.28 | 0.83 | 0.51 |\n|  | LlamaGen-XL [25] | 775M | 3.39 | 227.08 | 0.81 | 0.54 |\n|  | **FAR-Base** | 415M | 3.38 | 248.26 | 0.81 | 0.54 |\n|  | **FAR-Large** | 898M | 3.08 | 272.52 | 0.82 | 0.54 |", "caption": "Table 2: \nClass-conditional generation on 256\u00d7256256256256\\times 256256 \u00d7 256 ImageNet.\nModels with the suffix \u201c-re\u201d use rejection sampling.\nThe evaluation protocol and implementation follow ADM\u00a0[5].\nOur model employs a cfg-scale of 2.0.", "description": "Table 2 presents a comparison of class-conditional image generation results on the 256x256 ImageNet dataset.  It shows various metrics, including FID (Fr\u00e9chet Inception Distance) which measures the quality of generated images by comparing them to real images, IS (Inception Score) indicating the diversity and quality of generated samples, Precision, and Recall. Models are categorized based on their architecture and whether or not they utilize rejection sampling, indicated by the \u201c-re\u201d suffix. The evaluation methodology is consistent with the ADM [5] approach and a cfg-scale of 2.0 is used for our model.", "section": "4.3 Comparison on Generation Models"}, {"content": "| Model | Codebook Size | Dis. | Rep. | rFID \u2193 | IS \u2191 | PSNR \u2191 | Usage \u2191 |\n|---|---|---|---|---|---|---|---| \n| VQGAN | 16384 | - | - | 3.71 | 50.05 | 20.56 | 98% |\n|  | 32768 | - | - | 3.60 | 50.60 | 20.56 | 84% |\n| FQGAN | 16384 \u00d7 2 | \u2717 | \u2717 | 2.00 | 54.72 | 22.21 | 97% |\n|  | 16384 \u00d7 2 | \u2713 | \u2717 | 1.84 | 55.04 | 22.04 | 98% |\n|  | 16384 \u00d7 2 | \u2717 | \u2713 | 1.73 | 55.00 | 21.61 | 98% |\n|  | 16384 \u00d7 2 | \u2713 | \u2713 | 1.66 | 55.21 | 21.62 | 98% |", "caption": "Table 3: \nAblation study on different components of the proposed factorized quantization, using the FQGAN-Dual variant.", "description": "This table presents an ablation study analyzing the impact of different components of the proposed factorized quantization method on the performance of the FQGAN-Dual model.  It compares the reconstruction quality (measured by rFID, IS, PSNR) and codebook usage rate across various configurations, including the use of a single codebook (as in standard VQGAN), a factorized codebook without disentanglement or representation learning, and finally the full FQGAN-Dual model which incorporates both disentanglement regularization and representation learning. This allows for a quantitative assessment of the contribution of each proposed technique to the overall improvement in performance.", "section": "3. Method"}, {"content": "| Generation Model Head | Top-k Sampling | gFID\u2193 |\n|---|---|---|\n| k Linear Classifiers | 4096 | 5.19 |\n|  | 8192 | 6.90 |\n| k MLP Classifiers | 4096 | 5.59 |\n|  | 8192 | 8.88 |\n| Factorized AR Head | 4096 | 4.37 |\n|  | 8192 | 3.74 |", "caption": "Table 4: \nAblation study on the generation model head design with the proposed FQGAN tokenizer. We use FAR-Large model with cfg-scale=1.75 in this study.", "description": "This table presents an ablation study on different design choices for the autoregressive (AR) head in the context of the proposed Factorized Quantization Generative Adversarial Network (FQGAN).  Specifically, it investigates the impact of various AR head architectures on the quality of image generation.  The study uses the FAR-Large model with a classifier-free guidance (CFG) scale of 1.75, comparing different head designs: multiple linear classifiers, multiple multi-layer perceptrons (MLPs), and the proposed factorized AR head. The results are evaluated using the Fr\u00e9chet Inception Distance (FID) metric, a lower FID indicating better image generation quality.  The experiment helps determine the optimal design for the AR head that effectively handles multiple sub-tokens produced by the factorized tokenizer in FQGAN.", "section": "3.3 Auto-Regressive Model"}]