[{"figure_path": "https://arxiv.org/html/2411.16681/x1.png", "caption": "Figure 1: \nPerformance comparison of popular tokenizers at various codebook sizes, including VQ (Taming)\u00a0[6], VQ (LlamaGen)\u00a0[25], VQ-LC\u00a0[41], LFQ (OpenMAGVIT2)\u00a0[15], and FQGAN.\nLower rFID values indicate better performance.", "description": "The figure showcases a comparison of reconstruction performance (measured by rFID) across several popular visual tokenizers as their codebook size varies.  The tokenizers compared are VQ (from the Taming-Transformers paper), VQ (from LlamaGen), VQ-LC, LFQ (from OpenMAGVIT2), and the novel FQGAN model introduced in this paper. Lower rFID values represent better performance, indicating a higher quality of image reconstruction from the discrete tokens produced by the tokenizer. The graph illustrates how the reconstruction quality changes as the codebook size increases, highlighting the relative strengths and weaknesses of each tokenizer in terms of scalability and accuracy.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.16681/x2.png", "caption": "Figure 2: \nIllustration of the our method.\nThe left part shows FQGAN-Dual, the factorized tokenizer design in an example scenario when k=2\ud835\udc582k=2italic_k = 2.\nThis framework is extendable to factorization of more codebooks.\nThe right part demonstrate how we leverage an additional AR head to accommodate the factorized sub-codes based on standard AR generative transformer.", "description": "Figure 2 illustrates the architecture of the proposed Factorized Quantization (FQ) method. The left side shows the FQGAN-Dual model, a specific example of FQ with two sub-codebooks (k=2).  It demonstrates how a large codebook is broken down into smaller, independent sub-codebooks, reducing the computational complexity of quantization and improving stability.  Each sub-codebook uses its own encoder and quantizer to process the input features and generate sub-tokens.  These sub-tokens are concatenated before being fed to the decoder for image reconstruction. The right side of the figure shows how the factorized tokens are handled in an autoregressive (AR) image generation model.  A factorized AR head is added that predicts multiple sub-tokens simultaneously for each image patch, utilizing the output of an AR transformer backbone.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.16681/x3.png", "caption": "Figure 3: \nVisualization of standard reconstruction by FQGAN-Dual and reconstruction using only a single sub-codebook.", "description": "This figure shows a comparison of image reconstructions. The leftmost image is the original image. The next three images show reconstructions using the FQGAN-Dual model, which uses two sub-codebooks. The first reconstruction uses both sub-codebooks. The second and third use only one sub-codebook each, highlighting the individual contributions of each sub-codebook to the overall reconstruction quality.  The results visually demonstrate the effectiveness of the factorized approach in capturing diverse visual features.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.16681/x4.png", "caption": "Figure 4: \nT-SNE visualization of VQ codes from different sub-codebooks in FQGAN-Dual.", "description": "This figure visualizes the distribution of vector quantized (VQ) codes learned by the two sub-codebooks in the Factorized Quantization Generative Adversarial Network (FQGAN-Dual) model.  t-SNE (t-distributed Stochastic Neighbor Embedding) is used to reduce the dimensionality of the VQ code representations for visualization in a 2D space.  The plot shows how the VQ codes from each sub-codebook cluster based on their semantic content. One sub-codebook focuses on low-level visual features while the other, guided by CLIP embeddings, focuses on higher-level semantic information. This visualization demonstrates the effectiveness of the disentanglement regularization in FQGAN-Dual, showing that each sub-codebook learns distinct and complementary image features.", "section": "3.2.2 Representation Learning"}, {"figure_path": "https://arxiv.org/html/2411.16681/x5.png", "caption": "Figure 5: \nQualitative examples generated by our FAR model.", "description": "Figure 5 showcases several example images generated by the Factorized Auto-Regressive (FAR) model. These examples highlight the model's ability to produce high-quality and diverse images, demonstrating its effectiveness in autoregressive visual generation tasks.  The images likely represent a range of different classes or objects, showcasing the breadth of the model's capabilities.", "section": "4.3 Comparison on Generation Models"}]