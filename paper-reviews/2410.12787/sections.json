[{"page_end_idx": 1, "page_start_idx": 1, "section_number": 1, "section_title": "Introduction", "details": {"details": "Large Multimodal Models (LMMs) have significantly advanced, showing substantial improvements across diverse tasks by effectively integrating and processing various data modalities.  While integrating modalities like image and text has yielded considerable performance gains, especially in generating contextually accurate textual outputs, there is a growing trend towards incorporating additional modalities such as audio and video. However, a critical issue, known as hallucination, arises where the model's generated outputs don't accurately reflect the multimodal inputs.  This discrepancy can severely undermine LMM reliability and applicability, particularly in tasks requiring precise and factual content generation. Object hallucination, a key focus in LMMs dealing with image and text, occurs when the model generates semantically coherent but factually unaligned content with the objects in the input image.  Various benchmarks and mitigation techniques have been proposed, including process refinement, post-hoc correction, etc., but adding more modalities such as audio and video exacerbates the problem and increases hallucinations. The core issue of this paper is to systematically examine how LMMs produce hallucinations while integrating language, visual, and audio inputs.", "first_cons": "The introduction section primarily focuses on the problem of hallucination in LMMs without providing specific solutions or approaches to address the issue. It lays the groundwork for the research but doesn't offer concrete steps to improve the situation.", "first_pros": "The introduction clearly highlights the rapid advancements in LMMs and their increasing complexity, along with the critical challenge of hallucinations.  This sets a strong foundation for the research by establishing the context and importance of the problem.", "keypoints": ["Rapid advancement of Large Multimodal Models (LMMs) with improvements across a wide range of tasks", "Integration of multiple modalities (language, visual, audio) driving significant improvements but also creating challenges", "Hallucination as a critical issue where generated outputs don't reflect the input, limiting applicability", "Object hallucination as a key focus, particularly in image and text LMMs", "Accommodating additional modalities (audio, video) exacerbates alignment and fusion difficulties and leads to increased hallucinations"], "second_cons": "The section lacks concrete examples illustrating the severity and various types of hallucinations.  Providing specific examples would have strengthened the impact and clarity of the introduction.", "second_pros": "The introduction effectively establishes the context and significance of the research problem, highlighting the growing importance of LMMs and the critical need to address the issue of hallucination. The concise writing style makes it easy to understand for a broad audience.", "summary": "The introduction section details the rapid advancements in large multimodal models (LMMs) and the significant improvements they offer across numerous tasks. However, it emphasizes the critical problem of hallucinations, where the model's generated output deviates from the actual multimodal input, limiting the models' practical use.  The section notes that while progress has been made in addressing this with image-text models, the addition of more modalities only increases the complexity and the frequency of these hallucinations."}}, {"page_end_idx": 5, "page_start_idx": 2, "section_number": 2, "section_title": "Analyzing Hallucinations Across Language, Visual, and Audio", "details": {"details": "This section delves into the root causes of hallucinations in Large Multimodal Models (LMMs) that process language, visual, and audio inputs.  It identifies two primary contributors: overreliance on unimodal priors and spurious inter-modality correlations.  Overreliance on unimodal priors occurs when the model favors information from one modality (language, visual, or audio) over others, even when contradictory evidence exists in other modalities. This leads to outputs that align with familiar patterns within the dominant modality but not the overall multimodal input. Three distinct types are identified: Language Dominance, Visual Dominance, and Audio Dominance.  Spurious inter-modality correlations refer to statistically significant but meaningless associations learned during training between different modalities. These spurious links lead to plausible but factually incorrect outputs. Three sub-types of spurious correlations are discussed: Visual-Language, Audio-Language, and Visual-Audio-Language. The section uses qualitative demonstrations and statistical analysis to support these claims, providing empirical evidence for the extent to which these factors influence LMM reliability.  Validation experiments are conducted by manipulating the dominant modality to assess the impact on hallucination rates, which corroborates the findings.  Specific examples highlight the dominance of language models in some cases, where even obvious visual contradictions are ignored by the model.", "first_cons": "The analysis in this section focuses primarily on identifying and categorizing the causes of hallucinations rather than offering concrete solutions for mitigating them. While the identification of the two key factors is valuable, the section lacks a detailed discussion on practical strategies for addressing these issues.", "first_pros": "The section provides a clear and well-structured explanation of the two main factors contributing to hallucinations in LMMs, supported by illustrative examples and statistical analysis. This clear explanation is useful for researchers to better understand the underlying challenges associated with multimodal learning.", "keypoints": ["Two main contributors to LMM hallucinations are identified: overreliance on unimodal priors and spurious inter-modality correlations.", "Overreliance on unimodal priors manifests in three forms: Language Dominance, Visual Dominance, and Audio Dominance.", "Spurious inter-modality correlations appear in three forms: Visual-Language, Audio-Language, and Visual-Audio-Language.", "Validation experiments show that reducing information from the dominant modality decreases hallucination rates, supporting the analysis."], "second_cons": "The section primarily uses qualitative examples and a limited set of quantitative data to support its claims.  A more comprehensive quantitative evaluation using a larger and more diverse dataset would strengthen the findings and provide more robust evidence for the conclusions drawn.", "second_pros": "The section presents a novel and insightful framework for understanding the sources of hallucinations in LMMs,  providing a valuable contribution to the field of multimodal learning and a basis for further research into this critical issue. The categorization of hallucination types into unimodal priors and spurious correlations is a very useful contribution.", "summary": "This section investigates the root causes of hallucinations in Large Multimodal Models (LMMs), pinpointing two key factors: overreliance on unimodal priors and spurious inter-modality correlations.  Overreliance on unimodal priors occurs when the model prioritizes one modality (language, visual, or audio) over the others, leading to outputs aligning with familiar patterns from the dominant modality despite contradictory evidence. Three types are detailed: Language, Visual, and Audio Dominance. Spurious inter-modality correlations involve statistically significant but meaningless associations between different modalities learned during training, resulting in plausible yet incorrect outputs. Three subtypes are explored: Visual-Language, Audio-Language, and Visual-Audio-Language.  Qualitative demonstrations and statistical analyses support the claims, with validation experiments showing that reducing information from the dominant modality lowers hallucination rates."}}, {"page_end_idx": 7, "page_start_idx": 5, "section_number": 3, "section_title": "CMM Benchmark: The Curse of Multi-Modalities", "details": {"details": "The Curse of Multi-Modalities (CMM) benchmark is introduced to systematically evaluate hallucinations in Large Multimodal Models (LMMs).  It focuses on two main contributors to hallucinations: overreliance on unimodal priors and spurious inter-modality correlations.  CMM is composed of 1200 samples and 2400 probing questions, categorized into sub-categories based on modality dominance and correlation types (Visual-Language, Audio-Language, Visual-Audio-Language).  The benchmark uses Perception Accuracy (PA) and Hallucination Resistance (HR) as evaluation metrics.  The data construction process involved creating targeted probing queries to isolate and evaluate these two primary hallucination sources. Object-level and event-level probing questions were used to evaluate at a finer granularity.  Evaluation results revealed that  models struggle with hallucinations stemming from both sources, highlighting the challenges in achieving reliable multimodal integration.  A breakdown of results by specific subcategories was given to allow for better diagnosis of LMM vulnerabilities and to pinpoint areas for future improvement.", "first_cons": "The benchmark focuses primarily on two aspects of LMM hallucinations, which may not encompass all possible causes.  More diverse factors influencing hallucination need further investigation.", "first_pros": "CMM offers a systematic and comprehensive evaluation framework specifically designed to pinpoint the root causes of hallucinations in LMMs, making diagnosis of vulnerabilities more precise.", "keypoints": ["CMM benchmark systematically evaluates hallucinations in LMMs, focusing on two key contributors: overreliance on unimodal priors and spurious inter-modality correlations.", "The benchmark comprises 1200 samples and 2400 probing questions, categorized into sub-categories to enable fine-grained assessment.", "Perception Accuracy (PA) and Hallucination Resistance (HR) are used as evaluation metrics.", "The data construction involved creating targeted probing questions to isolate and evaluate the two primary hallucination sources."], "second_cons": "The reliance on manual annotation for data construction could be a bottleneck, potentially limiting the scalability and breadth of the benchmark.", "second_pros": "The detailed analysis and discussion of results provide valuable insights into the limitations of current LMMs and suggest potential research directions for improving reliability.", "summary": "The Curse of Multi-Modalities (CMM) benchmark provides a systematic evaluation of hallucinations in Large Multimodal Models (LMMs) by focusing on two key factors: overreliance on unimodal priors and spurious inter-modality correlations.  It uses 1200 samples and 2400 probing questions, categorized into various sub-categories and evaluated using Perception Accuracy (PA) and Hallucination Resistance (HR) metrics. The detailed analysis helps understand LMM vulnerabilities and suggests research directions for improvement."}}, {"page_end_idx": 8, "page_start_idx": 7, "section_number": 4, "section_title": "Experiments and Discussions", "details": {"details": "This section presents the experimental results and discussions of the study, focusing on the performance of various Large Multimodal Models (LMMs) across different tasks.  The analysis is categorized into three groups based on their modality capabilities: Visual-Audio LMMs, Visual-only LMMs, and Audio-only LMMs. The key metrics used for evaluation are Perception Accuracy (PA) and Hallucination Resistance (HR). Visual-Audio LMMs generally exhibit high PA scores (over 80) but struggle with hallucination resistance, particularly in Audio-Language correlations (HR scores around 14.5).  Visual-only and Audio-only LMMs demonstrate superior PA scores in their respective domains but still fall short in mitigating hallucinations caused by spurious inter-modality correlations.   The results highlight the challenges in balancing multimodal integration and avoiding overreliance on single modalities.  Different models also exhibit varied response tendencies; some are overconfident while others err on the side of caution.  The section concludes with suggestions for future research, focusing on creating more balanced datasets, improving multimodal fusion, mitigating biases, and refining safety alignment strategies.", "first_cons": "The analysis primarily focuses on a limited set of models, which may not fully represent the range of capabilities and limitations of different LMM architectures.", "first_pros": "The study provides a comprehensive evaluation of LMMs across different tasks and modalities, employing two key metrics (PA and HR) to assess both perceptual accuracy and hallucination resistance. The results and analysis are well-structured and easy to follow.", "keypoints": ["Visual-Audio LMMs achieve high Perception Accuracy (PA) scores above 80, but their Hallucination Resistance (HR) scores are significantly lower, especially in Audio-Language correlations (around 14.5).", "Visual-only and Audio-only LMMs demonstrate higher PA but still struggle with hallucinations, particularly in Audio-Language and Language Dominance tasks.", "Different LMMs exhibit different response tendencies, some overconfident and others cautious, suggesting a need for more balanced model training strategies.", "The study identifies key vulnerabilities, including unbalanced cross-modal integration, over-reliance on single modalities, and spurious correlations, thus providing insights for improvements."], "second_cons": "The study lacks a detailed explanation of how the different models were trained and configured which limits the reproducibility and generalizability of the findings.", "second_pros": "The study offers valuable insights into the challenges and limitations of current LMMs, suggesting potential future research directions for creating more robust and reliable multimodal systems. The categorized analysis allows for a detailed understanding of various model limitations and their underlying causes.", "summary": "This section presents a comprehensive evaluation of Large Multimodal Models (LMMs) across various visual, audio, and language tasks.  The results reveal that while LMMs generally achieve high perception accuracy, they struggle with hallucination resistance, especially in scenarios involving audio and language correlations.  The study identifies key limitations including unbalanced multimodal integration, reliance on single modalities, and spurious correlations learned during pretraining.  Finally, the study proposes potential research directions to improve the robustness and reliability of future LMMs."}}]