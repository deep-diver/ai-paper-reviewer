{"references": [{" publication_date": "2022", "fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "This paper is highly relevant due to its introduction of Flamingo, a visual language model that excels at few-shot learning.  Its relevance to the current research stems from the fact that Flamingo's architecture and capabilities are directly applicable to the study of LMMs and their propensity for hallucinations, as the authors explore how models like Flamingo handle visual information, which directly relates to the challenges in managing visual information in multimodal contexts as discussed by the authors in the paper.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Anna Rohrbach", "paper_title": "Object hallucination in image captioning", "reason": "This paper is crucial because it directly addresses the problem of object hallucination in vision-language models.  The concepts and findings presented, which focus on the causes and effects of hallucinations in image-text models, provide a strong foundation for understanding the broader issue of hallucinations in LMMs and serve as a comparison point for the current research, which expands the scope to encompass the challenges in managing multimodal inputs as mentioned in the paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yifan Li", "paper_title": "Evaluating object hallucination in large vision-language models", "reason": "This paper is important because it provides a comprehensive evaluation of object hallucination in large vision-language models. The paper introduces several benchmarks and evaluation metrics that are relevant to the current research as they have a bearing on the assessment of the various LMMs in the paper.", "section_number": 1}, {" publication_date": "2014", "fullname_first_author": "Tsung-Yi Lin", "paper_title": "Microsoft coco: Common objects in context", "reason": "This work is foundational to the research presented because of its introduction of the COCO dataset which is widely used in training visual models. The current research draws extensively on the COCO dataset or similar datasets which makes this citation relevant.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Anna Rohrbach", "paper_title": "Object hallucination in image captioning", "reason": "This paper's focus on object hallucination in image captioning is directly relevant because the current research expands upon this work. The paper presents a deep study of overreliance on unimodal priors which has a direct bearing on the problems of hallucination in LMMs discussed in the paper.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Chris Dongjoo Kim", "paper_title": "Audiocaps: Generating captions for audios in the wild", "reason": "This paper is important due to its creation of the AudioCaps dataset, a valuable resource for the research.  This dataset is directly relevant to the research presented due to its use in studying audio-visual relationships and understanding the effects of inter-modality correlations on hallucinations.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Max Bain", "paper_title": "Frozen in time: A joint video and image encoder for end-to-end retrieval", "reason": "This work is critical due to its exploration of joint video and image encoding in the context of retrieval tasks.  The current research expands on this work by incorporating additional modalities, which adds complexity to the problem of hallucination.  The joint encoding techniques explored in this citation offer valuable insights into techniques used to address multimodal issues which the current paper utilizes.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Fuxiao Liu", "paper_title": "Mitigating hallucination in large multi-modal models via robust instruction tuning", "reason": "This work is highly relevant due to its direct focus on mitigating hallucination in large multimodal models. The paper presents several techniques that can be used to reduce the prevalence of hallucinations. Given the focus of the current paper, this is a vital citation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Holy Lovenia", "paper_title": "Negative object presence evaluation (nope) to measure object hallucination in vision-language models", "reason": "This paper is important for its introduction of a novel evaluation metric for assessing object hallucination. The current paper extends this work by focusing on more modalities, and their results could be benchmarked against the metrics presented in this citation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Wenbin An", "paper_title": "Agla: Mitigating object hallucinations in large vision-language models with assembly of global and local attention", "reason": "This work is significant because it introduces a novel approach to mitigate object hallucinations in large vision-language models.  The techniques explored in this paper are directly applicable to the current research and provide another solution to the issue being studied.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Lin Chen", "paper_title": "Sharegpt4video: Improving video understanding and generation with better captions", "reason": "This work is relevant due to its focus on video understanding and generation, which is directly applicable to the current research. The paper's model uses captions, which is one of the main input types studied in the current research. The results from the current research could be benchmarked against the model presented in this citation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zesen Cheng", "paper_title": "Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms", "reason": "This is a significant contribution to the field as it explores advancements in video-LLMs which is an important area of study for this research. The paper addresses challenges in handling spatio-temporal information as well as audio understanding which are both pertinent to the current research.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Paul Pu Liang", "paper_title": "Foundations & trends in multimodal machine learning: Principles, challenges, and open questions", "reason": "This paper offers a broad overview of multimodal machine learning which sets the stage for a deeper understanding of the current research. The paper covers principles, challenges, and open questions which can guide future research based on the current research findings.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "This technical report is significant due to its comprehensive overview of the GPT-4 model and its abilities. This large language model architecture is compared against other LMMs in the current research, making this citation relevant.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Guangzhi Sun", "paper_title": "Fine-grained audio-visual joint representations for multimodal large language models", "reason": "This paper's relevance comes from its exploration of fine-grained audio-visual joint representations in multimodal large language models. These representations are directly relevant to the current research and could be used to evaluate the LMMs' ability to extract relevant information from different modalities.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Lin Chen", "paper_title": "Unified hallucination detection for multimodal large language models", "reason": "This paper offers a unified approach to hallucination detection in multimodal large language models, which directly relates to the current research.  The methods explored in this citation could be adapted or compared to the current work.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Bo Li", "paper_title": "Llava-onevision: Easy visual task transfer", "reason": "This paper is highly relevant due to its exploration of a model that can easily transfer its learning across visual tasks. This is directly applicable to the current research's study of LMMs and their capabilities to handle various modalities. The findings and techniques presented in this work could help guide future research on this subject.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Hang Zhang", "paper_title": "Video-llama: An instruction-tuned audio-visual language model for video understanding", "reason": "This paper's introduction of Video-LLaMa, an instruction-tuned audio-visual language model, is highly relevant due to its focus on video understanding.  The methods used in this model could be compared against other similar models in the paper's study.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Pan Zhang", "paper_title": "Internlm-xcomposer-2.5: A versatile large vision language model supporting long-contextual input and output", "reason": "This work is significant because of its introduction of a versatile large vision language model. This is directly applicable to the current research and allows for a comparison of its performance in terms of hallucination with other LMMs in the study.", "section_number": 4}]}