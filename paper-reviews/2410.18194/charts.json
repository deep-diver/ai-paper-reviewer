[{"figure_path": "2410.18194/charts/charts_2_0.png", "caption": "Figure 2: Code Generation: ZIP-FIT accelerates cross-entropy loss reduction, even in code-specialized models like CodeGemma-2B. The plots show cross-entropy test loss versus the number of training tokens for Gemma2-2B (top row) and CodeGemma-2B (bottom row) across different token selection sizes. ZIP-FIT (blue) consistently reduces loss faster than DSIR (green) and D4 (red), achieving up to 85.11% speed improvement at lower token counts. These results demonstrate ZIP-FIT's efficiency in data selection for fine-tuning models on code-geneation tasks.", "description": "The chart displays the cross-entropy test loss versus the number of training tokens for two code generation models, showing that ZIP-FIT consistently achieves lower loss faster than competing methods.", "section": "5 COMPARATIVE EVALUATION OF ZIP-FIT FOR EFFICIENT FINE-TUNING"}, {"figure_path": "2410.18194/charts/charts_3_0.png", "caption": "Figure 3: Higher ZIP-FIT alignment correlates with lower cross-entropy loss. The relationship between ZIP-FIT alignment and cross-entropy (CE) loss for (a) GPT-2 trained on 22k tokens (R2 = 0.90, p = 0.001) and (b) Mistral7B trained on 22k tokens (R2 = 0.75, p = 0.025). Each point represents a dataset, with its position reflecting the dataset's ZIP-FIT alignment score against the ProofNet test set and the resulting CE loss. The dashed red line indicates the linear regression fit, while the dashed grey line shows the pretrained CE loss. Higher alignment scores correspond to lower CE losses, demonstrating that training on better aligned data yields better performance.", "description": "The chart shows a strong negative correlation between higher ZIP-FIT alignment scores and lower cross-entropy loss for GPT-2 and Mistral7B language models, indicating that training on better-aligned data leads to improved performance.", "section": "Higher Alignment Interventionally Achieves Better Model Performance"}, {"figure_path": "2410.18194/charts/charts_3_1.png", "caption": "Figure 3: Higher ZIP-FIT alignment correlates with lower cross-entropy loss. The relationship between ZIP-FIT alignment and cross-entropy (CE) loss for (a) GPT-2 trained on 22k tokens (R2 = 0.90, p = 0.001) and (b) Mistral7B trained on 22k tokens (R2 = 0.75, p = 0.025). Each point represents a dataset, with its position reflecting the dataset's ZIP-FIT alignment score against the ProofNet test set and the resulting CE loss. The dashed red line indicates the linear regression fit, while the dashed grey line shows the pretrained CE loss. Higher alignment scores correspond to lower CE losses, demonstrating that training on better aligned data yields better performance.", "description": "Higher ZIP-FIT alignment scores correlate with lower cross-entropy loss, indicating that training on better-aligned data leads to improved model performance.", "section": "3 HIGHER ALIGNMENT INTERVENTIONALLY ACHIEVES BETTER MODEL PERFORMANCE"}, {"figure_path": "2410.18194/charts/charts_5_0.png", "caption": "Figure 4: Highly aligned data lowers cross-entropy loss more efficiently. The x-axis shows the number of training tokens, and the y-axis represents the cross-entropy (CE) test loss. Different curves correspond to datasets filtered by different alignment scores, indicating their relevance to the target domain. The most aligned data reduce Test CE loss significantly faster than less aligned data. The left panel depicts results using GPT-2, and the right panel uses Mistral7B, demonstrating that using highly aligned data not only accelerates training but also achieves better model performance, validating the effectiveness of ZIP-FIT for data selection in fine-tuning.", "description": "The chart shows that highly aligned data leads to faster convergence and lower cross-entropy loss during model training for both GPT-2 and Mistral7B language models.", "section": "Higher Alignment Interventionally Achieves Better Model Performance"}, {"figure_path": "2410.18194/charts/charts_6_0.png", "caption": "Figure 5: AutoFormalization: ZIP-FIT consistently achieves lower test loss more quickly than D4 and DSIR, demonstrating its efficiency in data selection. The plots show cross-entropy test loss versus the number of training tokens for three models (InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B) across different token selection sizes. ZIP-FIT (blue line) consistently outperforms both DSIR (green line) and D4 (red line) across all model and token size configurations, highlighting its ability to process data more efficiently. The percentage labels in each plot indicate the relative speedup of ZIP-FIT over DSIR in reaching the lowest cross-entropy loss, reinforcing the method's scalability and adaptability for domain-specific fine-tuning.", "description": "The chart shows that ZIP-FIT consistently achieves lower cross-entropy test loss faster than DSIR and D4 across various models and token selection sizes for the AutoFormalization task, demonstrating its efficiency in data selection.", "section": "5.1 AutoFormalization"}, {"figure_path": "2410.18194/charts/charts_8_0.png", "caption": "Figure 6: Selective data filtering with ZIP-FIT allows us to achieve better cross-entropy test loss faster than training on all the data, resulting in improved performance and efficiency. The x-axis represents the number of training tokens, while the y-axis shows the cross-entropy test loss. The curves represent models fine-tuned (FT) on datasets filtered by varying alignment thresholds (>0.1, >0.2, >0.3). The dashed line indicates the baseline performance of the pretrained Mistral7B model. Training on data filtered with higher alignment thresholds leads to superior performance, demonstrating the effectiveness of removing misaligned data in fine-tuning.", "description": "The chart displays the relationship between the number of training tokens and cross-entropy test loss for different data alignment thresholds, demonstrating that using more aligned data leads to faster convergence and lower test loss.", "section": "6 IMPACT OF DATA MISALIGNMENT ON MODEL PERFORMANCE"}, {"figure_path": "2410.18194/charts/charts_17_0.png", "caption": "Figure 7: ZIP-FIT consistently achieves a lower test loss at a faster rate compared to D4 and DSIR for Autoformalization. The plots show the cross-entropy test loss against the number of training tokens for three models (InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B) across various token selection sizes. ZIP-FIT (blue line) consistently surpasses both DSIR (green line) and D4 (red line) across all model and token size configurations, emphasizing its superior data processing efficiency. The percentage labels in each plot denote the relative speedup of ZIP-FIT over DSIR in attaining the lowest cross-entropy loss, further underscoring the method's scalability and adaptability for domain-specific fine-tuning.", "description": "The chart displays the cross-entropy test loss for three different models across various token selection sizes, demonstrating that ZIP-FIT consistently achieves lower test loss at a faster rate compared to D4 and DSIR.", "section": "5.1 Autoformalization"}, {"figure_path": "2410.18194/charts/charts_17_1.png", "caption": "Figure 7: ZIP-FIT consistently achieves a lower test loss at a faster rate compared to D4 and DSIR for Autoformalization. The plots show the cross-entropy test loss against the number of training tokens for three models (InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B) across various token selection sizes. ZIP-FIT (blue line) consistently surpasses both DSIR (green line) and D4 (red line) across all model and token size configurations, emphasizing its superior data processing efficiency. The percentage labels in each plot denote the relative speedup of ZIP-FIT over DSIR in attaining the lowest cross-entropy loss, further underscoring the method's scalability and adaptability for domain-specific fine-tuning.", "description": "The chart compares the performance of ZIP-FIT, DSIR, and D4 in reducing cross-entropy loss during the fine-tuning of three different language models for the Autoformalization task, demonstrating ZIP-FIT's superior efficiency and speed.", "section": "5.1 Autoformalization"}, {"figure_path": "2410.18194/charts/charts_17_2.png", "caption": "Figure 7: ZIP-FIT consistently achieves a lower test loss at a faster rate compared to D4 and DSIR for Autoformalization. The plots show the cross-entropy test loss against the number of training tokens for three models (InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B) across various token selection sizes. ZIP-FIT (blue line) consistently surpasses both DSIR (green line) and D4 (red line) across all model and token size configurations, emphasizing its superior data processing efficiency. The percentage labels in each plot denote the relative speedup of ZIP-FIT over DSIR in attaining the lowest cross-entropy loss, further underscoring the method's scalability and adaptability for domain-specific fine-tuning.", "description": "The chart compares the cross-entropy test loss of three different language models fine-tuned using data selected by ZIP-FIT, DSIR, and D4 across varying numbers of training tokens, demonstrating ZIP-FIT's superior efficiency and faster convergence.", "section": "5.1 Autoformalization"}, {"figure_path": "2410.18194/charts/charts_17_3.png", "caption": "Figure 7: ZIP-FIT consistently achieves a lower test loss at a faster rate compared to D4 and DSIR for Autoformalization. The plots show the cross-entropy test loss against the number of training tokens for three models (InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B) across various token selection sizes. ZIP-FIT (blue line) consistently surpasses both DSIR (green line) and D4 (red line) across all model and token size configurations, emphasizing its superior data processing efficiency. The percentage labels in each plot denote the relative speedup of ZIP-FIT over DSIR in attaining the lowest cross-entropy loss, further underscoring the method's scalability and adaptability for domain-specific fine-tuning.", "description": "The chart displays the cross-entropy test loss of three different language models using different data selection methods (ZIP-FIT, DSIR, and D4) across varying numbers of training tokens, demonstrating ZIP-FIT's superior performance and efficiency.", "section": "5.1 Autoformalization"}, {"figure_path": "2410.18194/charts/charts_17_4.png", "caption": "Figure 2: Code Generation: ZIP-FIT accelerates cross-entropy loss reduction, even in code-specialized models like CodeGemma-2B. The plots show cross-entropy test loss versus the number of training tokens for Gemma2-2B (top row) and CodeGemma-2B (bottom row) across different token selection sizes. ZIP-FIT (blue) consistently reduces loss faster than DSIR (green) and D4 (red), achieving up to 85.11% speed improvement at lower token counts. These results demonstrate ZIP-FIT's efficiency in data selection for fine-tuning models on code-geneation tasks.", "description": "The chart displays the cross-entropy test loss versus the number of training tokens for different models and token selection sizes, showing that ZIP-FIT consistently reduces loss faster than other methods.", "section": "5 COMPARATIVE EVALUATION OF ZIP-FIT FOR EFFICIENT FINE-TUNING"}, {"figure_path": "2410.18194/charts/charts_17_5.png", "caption": "Figure 7: ZIP-FIT consistently achieves a lower test loss at a faster rate compared to D4 and DSIR for Autoformalization. The plots show the cross-entropy test loss against the number of training tokens for three models (InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B) across various token selection sizes. ZIP-FIT (blue line) consistently surpasses both DSIR (green line) and D4 (red line) across all model and token size configurations, emphasizing its superior data processing efficiency. The percentage labels in each plot denote the relative speedup of ZIP-FIT over DSIR in attaining the lowest cross-entropy loss, further underscoring the method's scalability and adaptability for domain-specific fine-tuning.", "description": "The chart compares the cross-entropy test loss of three different language models (InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B) trained on data selected by ZIP-FIT, DSIR, and D4 across various token selection sizes, demonstrating ZIP-FIT's superior performance and efficiency.", "section": "5.1 Autoformalization"}, {"figure_path": "2410.18194/charts/charts_17_6.png", "caption": "Figure 7: ZIP-FIT consistently achieves a lower test loss at a faster rate compared to D4 and DSIR for Autoformalization. The plots show the cross-entropy test loss against the number of training tokens for three models (InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B) across various token selection sizes. ZIP-FIT (blue line) consistently surpasses both DSIR (green line) and D4 (red line) across all model and token size configurations, emphasizing its superior data processing efficiency. The percentage labels in each plot denote the relative speedup of ZIP-FIT over DSIR in attaining the lowest cross-entropy loss, further underscoring the method's scalability and adaptability for domain-specific fine-tuning.", "description": "The chart compares the performance of ZIP-FIT, DSIR, and D4 in reducing cross-entropy loss during the fine-tuning of three different language models for the Autoformalization task, showing ZIP-FIT's superior efficiency and faster convergence.", "section": "5.1 Autoformalization"}, {"figure_path": "2410.18194/charts/charts_17_7.png", "caption": "Figure 7: ZIP-FIT consistently achieves a lower test loss at a faster rate compared to D4 and DSIR for Autoformalization. The plots show the cross-entropy test loss against the number of training tokens for three models (InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B) across various token selection sizes. ZIP-FIT (blue line) consistently surpasses both DSIR (green line) and D4 (red line) across all model and token size configurations, emphasizing its superior data processing efficiency. The percentage labels in each plot denote the relative speedup of ZIP-FIT over DSIR in attaining the lowest cross-entropy loss, further underscoring the method's scalability and adaptability for domain-specific fine-tuning.", "description": "The chart compares the performance of ZIP-FIT, DSIR, and D4 in terms of cross-entropy test loss and training speed for different model sizes and datasets in the Autoformalization task, demonstrating ZIP-FIT's superiority.", "section": "5.1 Autoformalization"}, {"figure_path": "2410.18194/charts/charts_17_8.png", "caption": "Figure 7: ZIP-FIT consistently achieves a lower test loss at a faster rate compared to D4 and DSIR for Autoformalization. The plots show the cross-entropy test loss against the number of training tokens for three models (InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B) across various token selection sizes. ZIP-FIT (blue line) consistently surpasses both DSIR (green line) and D4 (red line) across all model and token size configurations, emphasizing its superior data processing efficiency. The percentage labels in each plot denote the relative speedup of ZIP-FIT over DSIR in attaining the lowest cross-entropy loss, further underscoring the method's scalability and adaptability for domain-specific fine-tuning.", "description": "The chart compares the cross-entropy test loss of three different language models (InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B) trained using data selected by ZIP-FIT, DSIR, and D4 across various token selection sizes, demonstrating ZIP-FIT's superior efficiency and faster convergence in achieving lower test loss.", "section": "5.1 Autoformalization"}, {"figure_path": "2410.18194/charts/charts_18_0.png", "caption": "Figure 8: ZIP-FIP demonstrates lower cross-entropy and lower run time during data selection than competing DSIR and D4 methods. ZIP-FIT is cheaper, faster, and better performing. The run times do no include fine-tuning time, since it's a constant offset across all models. D4's data selection (not shown) takes 5hs because it uses an embedding model (opt-125m Zhang et al. (2022)), the same one as the original paper Tirumala et al. (2023).", "description": "The chart compares the cross-entropy test loss and data selection time of ZIP-FIT against DSIR for different models and token sizes, showing that ZIP-FIT achieves lower cross-entropy and faster data selection.", "section": "D DATA SELECTION PROFILING (RUN TIMES)"}]