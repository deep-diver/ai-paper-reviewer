{"references": [{" publication_date": "2020", "fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "reason": "This foundational paper demonstrates the potential of large language models (LLMs) for few-shot learning, highlighting their capacity to generalize across diverse tasks.  This concept is central to the current work, which focuses on improving LLM performance through targeted data selection, implicitly building upon the understanding of LLMs' ability to learn from limited examples.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Suchin Gururangan", "paper_title": "Don't stop pretraining: Adapt language models to domains and tasks", "reason": "This paper emphasizes the importance of adapting pre-trained language models to specific domains and tasks, which directly addresses the core challenge of the current paper. The current work presents a novel method for enhancing the adaptation process through careful data selection, building upon the insights of adapting pre-trained models.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Amro Abbas", "paper_title": "Semdedup: Data-efficient learning at web-scale through semantic deduplication", "reason": "This paper introduces a technique for efficient data selection focusing on removing semantically similar or duplicate data points. This method complements ZIP-FIT which also focuses on efficiency by directly using compression to improve efficiency. In essence, the methods tackle the efficient data selection problem from slightly different angles.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Kushal Tirumala", "paper_title": "D4: Improving llm pretraining via document de-duplication and diversification", "reason": "This work provides a baseline method (D4) against which ZIP-FIT is compared.  The paper's focus on data deduplication and diversification is directly related to the current paper's emphasis on data quality and task alignment; ZIP-FIT builds upon this, offering a more efficient and task-aware data selection strategy.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Ben Sorscher", "paper_title": "Beyond neural scaling laws: beating power law scaling via data pruning", "reason": "This work explores data pruning techniques to improve model performance which is complementary to data selection. The work's focus on achieving better scaling is an important consideration in the current work, since improving data selection often yields computational efficiency gains as demonstrated by ZIP-FIT.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Sang Michael Xie", "paper_title": "Data selection for language models via importance resampling", "reason": "This paper introduces DSIR, a baseline data selection algorithm compared to ZIP-FIT in this work.  Both papers address similar problems but through different methods, allowing for direct comparison of efficiency and effectiveness.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Alycia Lee", "paper_title": "Beyond scale: the diversity coefficient as a data quality metric demonstrates llms are pre-trained on formally diverse data", "reason": "This paper investigates data diversity in the context of language model training, which is a significant consideration for efficient data selection. This paper examines the diversity in the dataset used for model pre-training; high diversity can lead to better generalization but poor task-specific performance. The current work builds upon this, emphasizing the importance of selecting data relevant to the target task.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Brando Miranda", "paper_title": "Beyond scale: The diversity coefficient as a data quality metric for variability in natural language data", "reason": "This paper introduces a new metric for assessing the quality of language model training data, focusing on diversity. The work's focus on the importance of data diversity highlights the complexity of data quality. High diversity alone does not guarantee effective model performance, which is addressed by ZIP-FIT through efficient task-aware data selection.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Zhiying Jiang", "paper_title": "\u201clow-resource", "reason": "This study provides a foundational understanding of how compression can aid in text classification which is crucial for efficient data selection. The paper shows that normalized compression distance outperforms traditional neural embeddings. This paper offers a theoretical justification for the use of compression in data selection, which is a key component of the proposed ZIP-FIT.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "Leonardo De Moura", "paper_title": "The lean theorem prover (system description)", "reason": "The Lean theorem prover is mentioned as an example of a formal mathematical programming language.  This connection makes the Autoformalization task relevant to the paper, illustrating the application of ZIP-FIT to domain-specific problems where high-quality data is essential for success.  ZIP-FIT offers a way to select data for such tasks efficiently.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Piotr Bojanowski", "paper_title": "Enriching word vectors with subword information", "reason": "This paper provides background information on word embeddings, a technique frequently used in data selection for language models.  Although the current work proposes an embedding-free method, it's relevant to understand the context and limitations of existing embedding-based methods to better appreciate the novelty and advantages of the proposed ZIP-FIT approach.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "reason": "This paper introduces a unified text-to-text transformer model that improves the efficiency of language models.  Since ZIP-FIT also focuses on improving model training efficiency by selecting relevant data, it is relevant to review the current research and practices which contribute to better training efficiency.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Sang Michael Xie", "paper_title": "Doremi: Optimizing data mixtures speeds up language model pretraining", "reason": "This paper focuses on optimizing data mixtures in language model pre-training.  While the focus is on pre-training, the work addresses a related problem to efficient data selection which is the crux of this paper. ZIP-FIT complements DoReMi by focusing on efficient fine-tuning data selection instead of pre-training data selection. This is complementary research which adds to the body of work.", "section_number": 7}, {" publication_date": "2022", "fullname_first_author": "Nan Du", "paper_title": "Glam: Efficient scaling of language models with mixture-of-experts", "reason": "This paper examines efficient scaling of language models using Mixture of Experts (MoE).  Efficient scaling is important for addressing both the computational and data challenges in training large language models.  This paper informs the discussion of efficiency in the current work, as ZIP-FIT aims to improve training efficiency via data selection; MoE addresses efficiency in the model architecture itself.", "section_number": 7}, {" publication_date": "2023", "fullname_first_author": "Zhiying Jiang", "paper_title": "\u201clow-resource", "reason": "This paper explores low-resource text classification and provides a related parameter-free approach using compressors. This approach is foundational to the current paper.  The study highlights the connection between compression and text classification and explores how compression can be utilized for efficient text processing.  This aligns with the core concept of ZIP-FIT which leverages compression for efficient data selection.", "section_number": 7}, {" publication_date": "2024", "fullname_first_author": "Pablo Villalobos", "paper_title": "Will we run out of data? limits of Ilm scaling based on human-generated data", "reason": "This paper explores limitations in scaling language models due to data scarcity, directly addressing the current work's focus on efficient data utilization.  This paper investigates the relationship between data scale and LLM performance, providing a context for understanding the importance of effective data selection which is the core focus of this paper.", "section_number": 7}, {" publication_date": "2024", "fullname_first_author": "Alexander Wettig", "paper_title": "Qurating: Selecting high-quality data for training language models", "reason": "This paper focuses on selecting high-quality data for training language models, closely related to the current work. The paper uses methods such as sentence embeddings, providing insights into methods for selecting data that contribute to efficient training. ZIP-FIT offers a computationally less expensive method to accomplish this.", "section_number": 7}, {" publication_date": "2023", "fullname_first_author": "Kaiyu Yang", "paper_title": "LeanDojo: Theorem proving with retrieval-augmented language models", "reason": "This paper introduces LeanDojo, a dataset used in this paper's experiments, demonstrating the application of the proposed method to a specific domain.  Using a dataset for a real world task demonstrates the practical use of ZIP-FIT for efficient model fine-tuning, especially on niche tasks that lack sufficient data.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Huaiyuan Ying", "paper_title": "Internlm-math: Open math large language models toward verifiable reasoning", "reason": "This paper provides the model (InterLM-Math-Plus-1.8B) used in this paper's Autoformalization experiments. The model's specialization in mathematical reasoning makes the results obtained with ZIP-FIT on this model particularly relevant to the task of efficient data selection for specialized tasks.", "section_number": 5}]}