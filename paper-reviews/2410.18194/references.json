{"references": [{" publication_date": "2020", "fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is foundational in establishing the capabilities of large language models and the potential for few-shot learning.  It directly relates to the central theme of data selection for optimizing LM performance, as efficient data selection is crucial for achieving optimal results with these models in low-data regimes.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Suchin Gururangan", "paper_title": "Don't stop pretraining: Adapt language models to domains and tasks", "reason": "This work is highly relevant because it addresses the challenge of adapting pre-trained language models to specific domains and tasks.  This directly relates to the paper's focus on selecting task-relevant data for fine-tuning, as effective adaptation often relies on choosing appropriate data for the target domain.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "reason": "This study explores the trade-off between model size and computational resources, highlighting the importance of efficiency. This aligns directly with the current paper's emphasis on computationally efficient data selection methods, particularly relevant in resource-constrained environments for low-resource tasks.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Amro Abbas", "paper_title": "Semdedup: Data-efficient learning at web-scale through semantic deduplication", "reason": "This paper focuses on data efficiency and deduplication techniques to reduce training data size, a concern shared by ZIP-FIT. Comparing and contrasting their method with ZIP-FIT's approach to data selection is relevant to the current work's focus on efficiency and impact of data selection.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Sang Michael Xie", "paper_title": "Data selection for language models via importance resampling", "reason": "This work introduces a baseline data selection method (DSIR) directly compared against in the present paper. The direct comparison allows for the assessment of ZIP-FIT's advantages and improvement over state-of-the-art techniques in data selection.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Zhiying Jiang", "paper_title": "\u201clow-resource\u201d text classification: A parameter-free classification method with compressors", "reason": "This study explores the use of compression in text classification, providing a theoretical foundation related to the core idea of ZIP-FIT's compression-based alignment for data selection.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Kushal Tirumala", "paper_title": "D4: Improving llm pre-training via document de-duplication and diversification", "reason": "This work provides another important baseline method (D4) for comparison with ZIP-FIT.  Direct comparison allows for an assessment of improvements achieved by ZIP-FIT and shows how it surpasses existing methods.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "reason": "This seminal work introduces GPT-2, one of the models used for the experiments in the present study.  Understanding GPT-2's characteristics is vital for interpreting the results and evaluating the impact of ZIP-FIT on this particular model.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Albert Q. Jiang", "paper_title": "Mistral 7b", "reason": "Mistral-7B, introduced in this paper, is another model used for the experiments in the present study.  The use of this model allows for assessing the impact of ZIP-FIT across multiple models, strengthening the results and generalizability.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Kaiyu Yang", "paper_title": "LeanDojo: Theorem proving with retrieval-augmented language models", "reason": "This paper introduces the LeanDojo dataset used extensively in the Autoformalization experiments. Understanding its properties is important for interpreting the results and context of the experiments.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Zhangir Azerbayev", "paper_title": "Llemma: An open language model for mathematics", "reason": "This work introduces a language model specializing in mathematics, offering another relevant model for comparative analysis within the same domain as the Autoformalization experiments in the paper.", "section_number": 5}, {" publication_date": "2021", "fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "reason": "This work discusses program synthesis using large language models, relevant to the code generation experiments in the paper. It provides context regarding the task's complexity and the challenges in data selection for code-related tasks.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Alycia Lee", "paper_title": "Beyond scale: the diversity coefficient as a data quality metric demonstrates llms are pre-trained on formally diverse data", "reason": "This paper examines data diversity, a crucial aspect of data quality that is important for training high-performing language models.  It provides context and insight into data quality considerations for improving LMs\u2019 performance.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Pablo Villalobos", "paper_title": "Will we run out of data? limits of llm scaling based on human-generated data", "reason": "This work discusses the limitations of relying solely on human-generated data for training large language models, which is directly relevant to the focus on efficient data selection explored in the current paper.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Joshua Kazdan", "paper_title": "Collapse or thrive? perils and promises of synthetic data in a self-generating world", "reason": "This paper discusses the challenges of using synthetic data in training language models, relevant to the context of improving data selection and addressing data scarcity issues.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Sang Michael Xie", "paper_title": "Doremi: Optimizing data mixtures speeds up language model pretraining", "reason": "This work introduces a data selection method that focuses on weighting data from multiple domains to optimize pre-training. Comparing and contrasting this method with ZIP-FIT's individual data point selection is insightful for understanding the different approaches.", "section_number": 7}, {" publication_date": "2022", "fullname_first_author": "Ben Sorscher", "paper_title": "Beyond neural scaling laws: beating power law scaling via data pruning", "reason": "This study examines data pruning techniques, directly related to the core idea of selecting optimal subsets of training data in the current paper.  This provides context and background on improving the efficiency of using training data.", "section_number": 7}, {" publication_date": "2022", "fullname_first_author": "Niklas Muennighoff", "paper_title": "Sgpt: Gpt sentence embeddings for semantic search", "reason": "This paper explores sentence embeddings for semantic search, which is a related technique for data representation and selection. Comparing the computational cost and effectiveness of this embedding-based approach with ZIP-FIT's compression-based approach is relevant.", "section_number": 7}, {" publication_date": "2015", "fullname_first_author": "Leonardo De Moura", "paper_title": "The lean theorem prover (system description)", "reason": "This paper introduces the Lean theorem prover, a crucial tool in the Autoformalization task.  The relevance stems from the use of LeanDojo dataset in the experiments, which relies on the Lean theorem prover for its data.", "section_number": 7}]}