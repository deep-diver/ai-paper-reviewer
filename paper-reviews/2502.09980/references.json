{"references": [{"fullname_first_author": "Holger Caesar", "paper_title": "nuScenes: A multimodal dataset for autonomous driving", "publication_date": "2020-06-14", "reason": "This paper introduces a large-scale, real-world autonomous driving dataset used as the foundation for several other datasets mentioned in the paper, including the V2V-QA dataset."}, {"fullname_first_author": "Runsheng Xu", "paper_title": "V2V4Real: A real-world large-scale dataset for vehicle-to-vehicle cooperative perception", "publication_date": "2023-06-19", "reason": "This paper presents the V2V4Real dataset, which is the base dataset for the proposed V2V-QA dataset in the current paper, directly contributing to the creation of the new cooperative driving benchmark."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-12-01", "reason": "This paper introduces LLaVA, a multi-modal large language model, which serves as the backbone for the proposed V2V-LLM model, fundamentally impacting the architecture and capabilities of the system."}, {"fullname_first_author": "Alex H. Lang", "paper_title": "PointPillars: Fast encoders for object detection from point clouds", "publication_date": "2019-06-16", "reason": "PointPillars is the 3D object detection model used for feature extraction in the V2V-LLM model, representing a core component of the perception pipeline in this research."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-18", "reason": "CLIP, introduced in this paper, is the basis of the multi-modal LLM's image feature encoders in some baseline methods, contributing to the comparison and evaluation within the study."}]}