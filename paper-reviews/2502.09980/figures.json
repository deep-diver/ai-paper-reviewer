[{"figure_path": "https://arxiv.org/html/2502.09980/x1.png", "caption": "Figure 1: Overview of our problem setting of LLM-based cooperative autonomous driving. All CAVs share their perception information with the LLM. Any CAV can ask the LLM a question to obtain useful information for driving safety.", "description": "This figure illustrates the system architecture of V2V-LLM, a novel approach to cooperative autonomous driving that leverages large language models (LLMs).  Multiple connected autonomous vehicles (CAVs) simultaneously share their perception data (e.g., sensor readings, object detections) with a central LLM.  Any CAV can then query the LLM in natural language about aspects of the driving environment, such as potential hazards or optimal trajectories. The LLM processes the aggregated perception data from all CAVs to answer the query, providing helpful information to improve driving safety and decision-making.  This cooperative perception system aims to enhance the reliability of autonomous driving, especially in situations where individual vehicle sensors may be unreliable or limited.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.09980/x2.png", "caption": "(a) Q1: Grounding at a reference location.", "description": "This figure shows an example of a question-answer pair in the V2V-QA dataset for the task of grounding at a reference location.  The question asks if there is anything at a specific location (x1, y1). The answer confirms the presence of a car and provides its center location.  This task assesses the model's ability to identify objects at specified coordinates, which is crucial for safe and reliable autonomous driving.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/x3.png", "caption": "(b) Q2: Grounding behind a reference object at a location.", "description": "This figure demonstrates a question-answering pair related to grounding in a cooperative autonomous driving scenario.  The question asks whether any objects are present behind a specific reference object located at a particular location. The image visually depicts the scene, highlighting the reference object and the objects located behind it. This illustrates how the system uses multi-modal input from multiple vehicles to answer complex queries involving spatial reasoning and object occlusion.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/x4.png", "caption": "(c) Q3: Grounding behind a reference object in a direction.", "description": "This figure demonstrates a type of question-answer pair from the V2V-QA dataset.  The question asks if there is anything behind a reference object in a specific direction. The figure shows a visual representation of the scene, including the reference object, a car behind it in the specified direction, and the predicted location based on the question. This illustrates the challenge of grounding questions in cooperative autonomous driving, where the visibility of objects for each vehicle might be limited, and relying on information from multiple vehicles is crucial.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/x5.png", "caption": "(d) Q4: Notable object identification.", "description": "This figure shows an example of the \"Notable Object Identification\" question-answer pair in the V2V-QA dataset.  The question asks whether there are any notable objects that the autonomous vehicle needs to be aware of near its planned trajectory. The answer from the LLM includes the location of nearby objects of interest, providing context to inform the vehicle's planning system. This example demonstrates the use of multi-modal information from multiple connected autonomous vehicles (CAVs) to inform safe and efficient driving decisions.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/x6.png", "caption": "(e) Q5: Planning.", "description": "This figure shows an example of a planning question-answer pair in the V2V-QA dataset.  An autonomous vehicle (CAV) asks the central LLM:  'I am CAV. What is the suggested future trajectory...?'.  The LLM considers the fused perception information from multiple CAVs and generates an answer showing a suggested trajectory to avoid collisions.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/x7.png", "caption": "Figure 2: \nIllustration of V2V-QA\u2019s 5555 types of QA pairs. The arrows pointing at LLM indicate the perception data from CAVs.", "description": "This figure illustrates the five question-answering (QA) pair types included in the V2V-QA dataset.  These QAs are designed for cooperative driving scenarios.  The five types are:\n(a) Grounding at a reference location: Asks whether an object exists at a specified location.\n(b) Grounding behind a reference object at a location:  Asks whether an object is behind another object at a specific location.\n(c) Grounding behind a reference object in a direction: Asks whether an object exists in a specified direction behind a reference object.\n(d) Notable object identification: Asks to identify notable objects near planned future trajectories.\n(e) Planning: Asks for suggested future trajectories to avoid collisions. The arrows pointing toward the LLM in the diagram highlight that all connected autonomous vehicles (CAVs) share perception data with the LLM, enabling cooperative responses.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/x8.png", "caption": "Figure 3: Model diagram of our proposed V2V-LLM\u00a0for cooperative autonomous driving.", "description": "The figure illustrates the architecture of the Vehicle-to-Vehicle Large Language Model (V2V-LLM) for cooperative autonomous driving.  It shows multiple connected autonomous vehicles (CAVs) each independently extracting scene-level feature maps and object-level feature vectors from their LiDAR point cloud data using a 3D object detector. This information is then sent to a central Large Language Model (LLM). The LLM fuses the perception data from all CAVs and answers driving-related questions (provided as language input) using a projector network to align the visual and language embeddings. The final output is a natural language answer.", "section": "4. V2V-LLM"}, {"figure_path": "https://arxiv.org/html/2502.09980/x9.png", "caption": "(a) No fusion", "description": "This figure shows the architecture of the baseline method with no fusion.  In this approach, only a single CAV's LiDAR point cloud is fed to a 3D object detector to extract scene-level feature maps and object-level feature vectors. These are then directly used as the input to the LLM.  This method is expected to perform poorly compared to those using multi-CAV data because it ignores the sensor inputs from other CAVs.", "section": "5.1 Baseline Methods"}, {"figure_path": "https://arxiv.org/html/2502.09980/x10.png", "caption": "(b) Early fusion", "description": "This figure shows the early fusion method used as a baseline in the paper.  In early fusion, LiDAR point clouds from two CAVs (Connected Autonomous Vehicles) are merged before being processed by a 3D object detector. The resulting scene-level feature map and object-level feature vectors are then used as the visual input for the LLM (Large Language Model). This approach aims to leverage all sensor data but may be less efficient for large-scale deployments due to the high communication bandwidth required.", "section": "5.1. Baseline Methods"}, {"figure_path": "https://arxiv.org/html/2502.09980/x11.png", "caption": "(c) Intermediate fusion\u00a0[50, 51, 52]", "description": "This figure shows the intermediate fusion approach used as a baseline method in the paper.  It depicts how feature maps from multiple connected autonomous vehicles (CAVs) are merged. Unlike early fusion which merges raw LiDAR data from all CAVs before feature extraction, intermediate fusion first extracts scene-level feature maps and object-level feature vectors from each CAV individually.  Then, these features are combined using techniques like attention mechanisms (as seen in works cited [50, 51, 52]) to produce a unified representation for the LLM. This approach offers a balance between the computational cost of early fusion and the potential loss of information inherent in no fusion.", "section": "5.1. Baseline Methods"}, {"figure_path": "https://arxiv.org/html/2502.09980/x12.png", "caption": "Figure 4: \nFeature encoder diagrams of the baseline methods from different fusion approaches.", "description": "This figure illustrates the different feature extraction methods used in the baseline models for cooperative autonomous driving. It compares three approaches: no fusion, early fusion, and intermediate fusion.  The 'no fusion' approach processes each vehicle's LiDAR data independently, using a separate 3D object detector for each. The 'early fusion' approach merges the LiDAR point clouds from all vehicles before processing them with a single 3D object detector.  The 'intermediate fusion' approach uses a cooperative detector (like those explored in prior works) to process the data from each vehicle before passing features to the LLM. Each approach highlights a different way of combining data from multiple vehicles, demonstrating varying levels of computation and communication complexity.", "section": "4. V2V-LLM"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q1_x.png", "caption": "Figure 5: V2V-LLM\u2019s grounding results on V2V-QA\u2019s testing split.\u00a0Magenta \u2218\\circ\u2218: reference locations in questions. Yellow +++: model output locations. Green \u2218\\circ\u2218: ground-truth answers.", "description": "This figure visualizes the performance of the V2V-LLM model on the grounding subtask of the V2V-QA dataset.  Grounding involves identifying objects at specific locations.  Each row represents a different sample from the testing set. Magenta circles indicate the location specified in the question (query location).  Yellow crosses show the location predicted by the V2V-LLM model. Green circles indicate the ground truth location of the relevant object. The figure demonstrates the model's ability to accurately locate objects based on textual queries, highlighting instances where predictions align well with ground truth, while showing potential limitations of the model where the prediction is less accurate.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q1_y.png", "caption": "Figure 6: V2V-LLM\u2019s notable object identification and planning results on V2V-QA\u2019s testing split. For notable object identification,\u00a0Magenta curve: planned future trajectories in questions. Green \u2218\\circ\u2218: ground-truth notable object locations. Yellow +++ and Cyan \u00d7\\times\u00d7: model identification outputs corresponding to CAV_EGO and CAV_1, respectively.\nFor planning, Green line: future trajectories in ground-truth answers. Yellow curve and Cyan curve: model planning outputs corresponding to CAV_EGO and CAV_1, respectively.", "description": "Figure 6 presents a qualitative analysis of the V2V-LLM model's performance on notable object identification and trajectory planning tasks within the V2V-QA testing dataset. The top half illustrates the notable object identification results.  Magenta curves represent the planned future trajectories from the questions. Green circles denote the ground truth locations of notable objects. Yellow and cyan markings indicate the identified objects by V2V-LLM, distinguishing between the ego vehicle (CAV_EGO) and another vehicle (CAV_1). The bottom half showcases planning results.  Green lines depict ground truth future trajectories, while yellow and cyan curves show trajectories generated by V2V-LLM for CAV_EGO and CAV_1, respectively.", "section": "5. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q1_dist.png", "caption": "(a) x (meters)", "description": "This figure shows the distribution of ground-truth answer locations relative to the CAV (Connected Autonomous Vehicle) in V2V-QA's Q1, which is the grounding task at a reference location. The figure consists of four subplots: (a) shows the distribution of x-coordinates (meters), (b) shows the distribution of y-coordinates (meters), (c) shows the distribution of distances (meters) between the ground-truth answers and the CAV, and (d) shows the distribution of angles (degrees) between the ground-truth answers and the CAV.  The x-axis represents the coordinates and the distance, while the y-axis represents the probability density.  The plots illustrate the spatial distribution of objects relevant to the grounding questions within the context of the autonomous driving scenario.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q1_angle.png", "caption": "(b) y (meters)", "description": "This figure shows the distribution of ground-truth answer locations' y-coordinate relative to the CAV in V2V-QA's Q2, which is the question type of grounding behind a reference object at a location. The y-axis represents the probability density, and the x-axis represents the y-coordinate in meters.  The distribution is shown in a histogram with different color representing different ranges of distances. The figure helps to understand the spatial distribution of the answers relative to the CAV for this specific question type.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q2_x.png", "caption": "(c) distance (meters)", "description": "This figure shows the distribution of ground-truth answer locations relative to the CAV in V2V-QA's Q3: Grounding behind a reference object in a direction.  The x-axis represents the x-coordinate in meters, where positive x is in the direction the CAV is facing, and the y-axis is the y-coordinate in meters, with positive y to the right of the CAV. The plot shows the probability density function of the distance (in meters) between the CAV's location and the locations of answers. The distribution of the angle (in degrees) between the CAV's front direction and the direction of answer locations is also displayed. The distributions are shown in the same order as in Fig. 8.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q2_y.png", "caption": "(d) angle (degrees)", "description": "This histogram shows the distribution of the angle (in degrees) of the ground truth answer locations relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA (Vehicle-to-Vehicle Question Answering) dataset for question type Q3: Grounding behind a reference object in a direction. The angle is calculated based on the relative position of the answer location with respect to the reference object's position from the perspective of the CAV.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q2_dist.png", "caption": "Figure 7: \nThe distribution of ground-truth answer locations relative to CAV in V2V-QA\u2019s Q1: Grounding at a reference location.", "description": "This figure visualizes the distribution of ground truth answer locations relative to the autonomous vehicle (CAV) in the V2V-QA dataset's Q1 task, which focuses on grounding at a reference location.  The plots illustrate the distribution along the x-axis (front-facing direction of the CAV), the y-axis (right-facing direction of the CAV), the distance from the CAV, and the angle relative to the CAV. These distributions provide insights into the spatial characteristics of the ground truth answers within the dataset for this specific task, showing how they are distributed relative to the CAV's perspective.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q2_angle.png", "caption": "(a) x (meters)", "description": "This figure shows the distribution of ground-truth answer locations relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA dataset's Q1 task (Grounding at a reference location).  The x-axis represents the x-coordinate (meters) in the local coordinate system of the CAV, where x=0 is the CAV's front direction. Similarly, the y-axis represents the y-coordinate (meters), where y=0 is the CAV's right direction. The figure contains four subplots. (a) shows the distribution of x-coordinates, (b) shows the distribution of y-coordinates, (c) shows the distribution of distances (meters) between the CAV and the answer locations, and (d) shows the distribution of angles (degrees) of the answer locations relative to the CAV's front direction. This visualization helps to understand the spatial characteristics of the question-answer pairs in this specific grounding task.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q3_x.png", "caption": "(b) y (meters)", "description": "This figure shows the distribution of ground-truth answer locations' y-coordinates relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA dataset.  The y-axis represents the probability density, and the x-axis shows the y-coordinate in meters. The figure helps illustrate the spatial distribution of the answers in the dataset, indicating the range and frequency of y-coordinates of the answers relative to the CAVs.  This is crucial for understanding the data distribution and evaluating the performance of models on various spatial scenarios.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q3_y.png", "caption": "(c) distance (meters)", "description": "This figure shows the distribution of ground-truth answer locations relative to the CAV in V2V-QA's Q3: Grounding behind a reference object in a direction.  The x-axis represents the distance (in meters) between the CAV and the ground-truth answer location. The histogram visually represents the frequency or probability density of different distances observed in the dataset.", "section": "3.3. Question and Answer Pairs Curation"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q3_dist.png", "caption": "(d) angle (degrees)", "description": "This histogram shows the distribution of the angles of ground-truth answer locations relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA (Vehicle-to-Vehicle Question Answering) dataset for question type Q3: Grounding behind a reference object in a direction.  The angle is calculated as the direction of the ground truth answer location with respect to the CAV's heading.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q3_angle.png", "caption": "Figure 8: \nThe distribution of ground-truth answer locations relative to CAV in V2V-QA\u2019s Q2: Grounding behind a reference object at a location.", "description": "This figure shows the distribution of ground truth answer locations relative to the autonomous vehicle (CAV) in the V2V-QA dataset's Q2 task, which is 'Grounding behind a reference object at a location'.  The distributions are visualized in four subplots showing the x-coordinate, y-coordinate, distance, and angle of the ground truth answer locations relative to the CAV.  This provides insights into the spatial characteristics of the answers provided to this specific type of question within the dataset. The distributions help illustrate the range and frequency of various locations of objects behind reference objects, useful for understanding the complexity and variability of the cooperative perception task.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q4_x.png", "caption": "(a) x (meters)", "description": "This figure visualizes the distribution of ground-truth answer locations relative to the CAV (connected autonomous vehicle) in V2V-QA's Q1 (Grounding at a reference location). The x-axis represents the x-coordinate (meters) and the y-axis represents the probability density. Subfigure (a) displays the distribution along the x-axis (front direction of the vehicle), (b) displays the distribution along the y-axis (right direction of the vehicle), (c) shows the distribution of distances (meters) between the ground truth locations and the CAV, and (d) presents the angular distribution (degrees) of the locations relative to the CAV.  These distributions help characterize the spatial characteristics of the grounding task in the dataset.", "section": "3.3 Question and Answer Pairs Curation"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q4_y.png", "caption": "(b) y (meters)", "description": "This histogram shows the distribution of the y-coordinates of ground-truth answer locations relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA dataset's Q2 question type.  The y-axis represents the probability density, and the x-axis represents the y-coordinate in meters.  This visualization helps to understand the spatial distribution of objects relative to the CAV's position when answering grounding questions about objects located behind a reference object at a specific location. The distribution is centered around 0, reflecting the fact that most answers are near the reference object but spread across various Y locations.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q4_dist.png", "caption": "(c) distance (meters)", "description": "This figure shows the distribution of the distances between ground truth answer locations and the locations of the asking CAVs in the V2V-QA's Q3 dataset. The x-axis represents the distance in meters, and the y-axis represents the probability density. The distribution is shown for different perspectives, including the x-coordinate, y-coordinate, distance, and angle. The x-coordinate and y-coordinate show the distribution along the x-axis and y-axis of the CAV's local coordinate system. The distance shows the distribution of the distances between the ground truth answer locations and the asking CAVs. The angle shows the distribution of the angles between the ground truth answer locations and the asking CAVs. The figure helps to understand the spatial distribution of the objects relative to the asking CAVs.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q4_angle.png", "caption": "(d) angle (degrees)", "description": "This histogram shows the distribution of the angle (in degrees) of ground-truth answer locations relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA dataset's Q4: Notable Object Identification task.  The angle is measured as the direction from the CAV to the ground truth answer location.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q5_x.png", "caption": "Figure 9: \nThe distribution of ground-truth answer locations relative to CAV in V2V-QA\u2019s Q3: Grounding behind a reference object in a direction.", "description": "This figure shows the distribution of ground truth answer locations relative to the ego vehicle (CAV) for question type Q3 of the V2V-QA dataset.  Q3 questions are of the form: \"Is there anything behind the [direction] object?\"  The distributions are visualized for x and y coordinates (in meters), distance from the ego vehicle (in meters), and angle from the ego vehicle\u2019s forward direction (in degrees).  The distributions reveal the spatial spread of objects that satisfy Q3 queries in the dataset, which provides insight into the types of cooperative perception challenges addressed in the dataset and V2V-LLM model.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q5_y.png", "caption": "(a) x (meters)", "description": "This figure shows the distribution of ground-truth answer locations relative to the CAV (connected autonomous vehicle) in V2V-QA's Q1 (Grounding at a reference location).  The figure is composed of four subplots: (a) shows the distribution of x-coordinates (in meters), (b) shows the distribution of y-coordinates (in meters), (c) shows the distribution of distances (in meters) between the CAV and the answer location, and (d) shows the distribution of angles (in degrees) between the CAV's forward direction and the answer location.  Each subplot provides a histogram illustrating the frequency distribution of the respective metric. This visualization helps to understand the spatial characteristics and distribution of the answer locations relative to the CAV.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q5_dist.png", "caption": "(b) y (meters)", "description": "This figure displays the distribution of ground-truth answer locations along the y-axis relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA dataset.  The y-axis represents the lateral direction, with positive values indicating locations to the right of the CAV and negative values indicating locations to the left. The distribution is shown in a histogram, providing insights into the spatial distribution of objects relevant to the grounding questions in the dataset.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q5_angle.png", "caption": "(c) distance (meters)", "description": "This figure shows the distribution of the distance between the ground-truth answer locations and the asking CAV's location for question type Q3 (Grounding behind a reference object in a direction) in the V2V-QA dataset. The x-axis represents the distance in meters, and the y-axis represents the probability density. The distribution is shown as a histogram.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/x13.png", "caption": "(d) angle (degrees)", "description": "This histogram shows the distribution of the angles of ground-truth answer locations relative to the CAV's coordinate system in the V2V-QA's Q3 dataset. The angle is measured in degrees and represents the direction of the answer location with respect to the CAV's forward direction. This visualization helps to understand the spatial distribution of answers relative to the CAV, providing insights into the dataset's characteristics and the challenges involved in cooperative perception tasks.", "section": "3.3. Question and Answer Pairs Curation"}, {"figure_path": "https://arxiv.org/html/2502.09980/x14.png", "caption": "Figure 10: \nThe distribution of ground-truth answer locations relative to CAV in V2V-QA\u2019s Q4: Notable object identification.", "description": "This figure visualizes the distribution of ground truth answer locations relative to the ego vehicle (CAV) for question type Q4 in the V2V-QA dataset.  Specifically, it shows how the x and y coordinates (relative to the CAV), the distance from the CAV, and the angle relative to the CAV are distributed for the ground truth answers of question type Q4. This provides insights into the spatial characteristics of the notable objects that the model is expected to identify and helps to understand the difficulty and distribution of the data for this question type.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/x15.png", "caption": "(a) x (meters)", "description": "This figure shows the distribution of ground-truth answer locations relative to the CAV (connected autonomous vehicle) in V2V-QA's Q1 (Grounding at a reference location).  The figure consists of four subplots: (a) shows the x-coordinate distribution in meters, (b) shows the y-coordinate distribution in meters, (c) shows the distance distribution in meters, and (d) shows the angle distribution in degrees. Each subplot provides a histogram visualizing the frequency of different values for the corresponding metric. This helps understand the spatial distribution of the objects referenced in the dataset's grounding questions relative to the CAV's perspective.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/x16.png", "caption": "(b) y (meters)", "description": "This figure shows the distribution of ground-truth answer locations relative to the CAV in V2V-QA's Q2.  The y-axis represents the probability density, and the x-axis shows the y-coordinate of the ground truth answer location in meters. The figure provides four subplots: (a) x-coordinate distribution, (b) y-coordinate distribution, (c) distance distribution from the CAV to the answer location, and (d) angle distribution of the answer location relative to the CAV's heading.  These distributions are important to understand the characteristics of the dataset and the difficulty of the grounding task, as they show how the relevant information is distributed in the dataset.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/x17.png", "caption": "(c) distance (meters)", "description": "This figure shows the distribution of the ground-truth answer distances to the asking CAV in V2V-QA's Q3. The distance is calculated from the location of the ground-truth answer to the location of the asking CAV.  The distribution is shown as a histogram. The x-axis represents the distance in meters, and the y-axis represents the probability density.", "section": "3.3. Question and Answer Pairs Curation"}, {"figure_path": "https://arxiv.org/html/2502.09980/x18.png", "caption": "(d) angle (degrees)", "description": "This histogram shows the distribution of the angle (in degrees) of ground-truth answer locations relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA (Vehicle-to-Vehicle Question Answering) dataset.  The angle is measured from the CAV's forward direction, indicating the orientation of the objects being located relative to the vehicle's heading. This distribution helps illustrate the range of object locations relative to the CAV that the models are tasked with identifying in the dataset and in the experiment.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/x19.png", "caption": "Figure 11: \nThe distribution of ground-truth answer locations relative to CAV in V2V-QA\u2019s Q5: Planning.", "description": "This figure visualizes the distribution of ground truth answer locations relative to the ego vehicle (CAV) for the planning questions in the V2V-QA dataset.  Specifically, it shows the distribution of the x-coordinate, y-coordinate, distance, and angle of the ending waypoints from the ground truth trajectories, providing insights into the spatial characteristics of the planning task within the dataset.", "section": "3. V2V-QA Dataset"}, {"figure_path": "https://arxiv.org/html/2502.09980/x20.png", "caption": "Figure 12: V2V-LLM\u00a0and baseline methods\u2019 grounding results on V2V-QA\u2019s testing split.\u00a0Magenta \u2218\\circ\u2218: reference locations in questions. Yellow +++: model output locations. Green \u2218\\circ\u2218: ground-truth answers.", "description": "This figure compares the performance of V2V-LLM and several baseline methods (No Fusion, Early Fusion, AttFuse, V2X-VIT, COBEVT) on the grounding task of the V2V-QA dataset.  The grounding task involves identifying objects at specific locations.  Each row presents a different example. The image shows the point cloud data. Magenta circles indicate the reference location provided in the question.  Yellow plus signs (+) show the location predicted by the model. Green circles show the ground truth location of the object.", "section": "5.2.1. Grounding"}, {"figure_path": "https://arxiv.org/html/2502.09980/x21.png", "caption": "Figure 13: V2V-LLM\u00a0and baseline methods\u2019 grounding results on V2V-QA\u2019s testing split.\u00a0Magenta \u2218\\circ\u2218: reference locations in questions. Yellow +++: model output locations. Green \u2218\\circ\u2218: ground-truth answers.", "description": "Figure 13 presents a comparison of grounding results between the proposed V2V-LLM model and several baseline methods. The task is to identify objects at specified locations within a driving scene, using data from the V2V-QA testing split. The figure visualizes the results by showing LiDAR point cloud data for a section of the driving scene and highlighting relevant objects. Magenta circles denote the reference locations specified in the questions. Yellow plus symbols represent the locations predicted by each method. Ground truth object locations are shown as green circles. This comparison allows for an evaluation of the accuracy and effectiveness of each method in cooperative grounding tasks. In particular, it can assess how well each approach fuses information from multiple vehicles to accurately and reliably identify objects in the driving scene.", "section": "3. V2V-QA Dataset"}]