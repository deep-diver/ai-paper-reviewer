<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models &#183; HF Daily Paper Reviews by AI"><meta name=description content="V2V-LLM leverages multi-modal LLMs for safer cooperative autonomous driving by fusing perception data from multiple vehicles, answering driving-related questions, and improving trajectory planning."><meta name=keywords content="AI Applications,Autonomous Vehicles,üè¢ NVIDIA,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09980/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09980/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models"><meta property="og:description" content="V2V-LLM leverages multi-modal LLMs for safer cooperative autonomous driving by fusing perception data from multiple vehicles, answering driving-related questions, and improving trajectory planning."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-02-14T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-14T00:00:00+00:00"><meta property="article:tag" content="AI Applications"><meta property="article:tag" content="Autonomous Vehicles"><meta property="article:tag" content="üè¢ NVIDIA"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09980/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09980/cover.png"><meta name=twitter:title content="V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models"><meta name=twitter:description content="V2V-LLM leverages multi-modal LLMs for safer cooperative autonomous driving by fusing perception data from multiple vehicles, answering driving-related questions, and improving trajectory planning."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models","headline":"V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models","abstract":"V2V-LLM leverages multi-modal LLMs for safer cooperative autonomous driving by fusing perception data from multiple vehicles, answering driving-related questions, and improving trajectory planning.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2502.09980\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-02-14T00:00:00\u002b00:00","datePublished":"2025-02-14T00:00:00\u002b00:00","dateModified":"2025-02-14T00:00:00\u002b00:00","keywords":["AI Applications","Autonomous Vehicles","üè¢ NVIDIA"],"mainEntityOfPage":"true","wordCount":"6984"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-27</p></a><a href=/ai-paper-reviewer/2025-03-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-28</p></a><a href=/ai-paper-reviewer/2025-03-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-31</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-27</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-28</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-31</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2502.09980/cover_hu13552269461100231641.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2502.09980/>V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-02-14T00:00:00+00:00>14 February 2025</time><span class="px-2 text-primary-500">&#183;</span><span>6984 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">33 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2502.09980/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2502.09980/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/ai-applications/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Applications
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/autonomous-vehicles/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Autonomous Vehicles
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-nvidia/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ NVIDIA</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#v2v-llm-overview>V2V-LLM Overview</a></li><li><a href=#cooperative-perception>Cooperative Perception</a></li><li><a href=#v2v-qa-dataset>V2V-QA Dataset</a></li><li><a href=#llm-fusion-method>LLM Fusion Method</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#v2v-llm-overview>V2V-LLM Overview</a></li><li><a href=#cooperative-perception>Cooperative Perception</a></li><li><a href=#v2v-qa-dataset>V2V-QA Dataset</a></li><li><a href=#llm-fusion-method>LLM Fusion Method</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2502.09980</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Hsu-kuang Chiu et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-02-17</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2502.09980 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2502.09980 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2502.09980/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current autonomous driving systems heavily rely on individual vehicle sensors, leading to safety issues when sensors malfunction or are occluded. Cooperative perception methods using vehicle-to-vehicle communication improve this but are largely limited to detection and tracking, leaving overall cooperative planning performance underexplored. This paper aims to address these limitations.</p><p>The paper proposes V2V-LLM, a novel method that integrates a large language model (LLM) into vehicle-to-vehicle cooperative autonomous driving. It introduces a new dataset (V2V-QA) for this problem setting and benchmarks V2V-LLM against baseline methods with various fusion approaches. <strong>Results show that V2V-LLM effectively fuses perception information from multiple vehicles and outperforms other methods in critical tasks like notable object identification and planning</strong>, paving the way for safer and more efficient autonomous driving.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-51ee669755fe41c1458b0e864403d615></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-51ee669755fe41c1458b0e864403d615",{strings:[" A novel problem setting is proposed that integrates LLMs into cooperative autonomous driving. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-f7fd54f147048eaa7d8448a1d8e1ede1></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-f7fd54f147048eaa7d8448a1d8e1ede1",{strings:[" A new benchmark dataset (V2V-QA) is created for evaluating LLM-based cooperative autonomous driving approaches. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-9594c99a44a8674cd68ff0826b52c7f5></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-9594c99a44a8674cd68ff0826b52c7f5",{strings:[" The proposed V2V-LLM method shows promising results, outperforming other baseline methods in key tasks. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is important because <strong>it introduces a novel problem setting for cooperative autonomous driving using LLMs</strong>, which is a significant advancement in the field. It also presents <strong>a new benchmark dataset (V2V-QA) and a strong baseline method (V2V-LLM)</strong>, which will allow other researchers to easily compare their methods. Lastly, it opens up <strong>new research avenues</strong> for improving safety and efficiency of autonomous driving systems.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x1.png alt></figure></p><blockquote><p>üîº This figure illustrates the system architecture of V2V-LLM, a novel approach to cooperative autonomous driving that leverages large language models (LLMs). Multiple connected autonomous vehicles (CAVs) simultaneously share their perception data (e.g., sensor readings, object detections) with a central LLM. Any CAV can then query the LLM in natural language about aspects of the driving environment, such as potential hazards or optimal trajectories. The LLM processes the aggregated perception data from all CAVs to answer the query, providing helpful information to improve driving safety and decision-making. This cooperative perception system aims to enhance the reliability of autonomous driving, especially in situations where individual vehicle sensors may be unreliable or limited.</p><details><summary>read the caption</summary>Figure 1: Overview of our problem setting of LLM-based cooperative autonomous driving. All CAVs share their perception information with the LLM. Any CAV can ask the LLM a question to obtain useful information for driving safety.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S1.T1.2><tbody class=ltx_tbody><tr class=ltx_tr id=S1.T1.2.1.1><td class="ltx_td ltx_align_left ltx_border_tt" id=S1.T1.2.1.1.1 style=padding-left:4pt;padding-right:4pt>Dataset</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S1.T1.2.1.1.2 style=padding-left:4pt;padding-right:4pt>Publication</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S1.T1.2.1.1.3 style=padding-left:4pt;padding-right:4pt># CAVs</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S1.T1.2.1.1.4 style=padding-left:4pt;padding-right:4pt>Sim/Real</td><td class="ltx_td ltx_align_right ltx_border_tt" id=S1.T1.2.1.1.5 style=padding-left:4pt;padding-right:4pt># Frames</td><td class="ltx_td ltx_align_right ltx_border_tt" id=S1.T1.2.1.1.6 style=padding-left:4pt;padding-right:4pt># QA</td><td class="ltx_td ltx_align_right ltx_border_tt" id=S1.T1.2.1.1.7 style=padding-left:4pt;padding-right:4pt># QA/frame</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S1.T1.2.1.1.8 style=padding-left:4pt;padding-right:4pt>Point Cloud</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S1.T1.2.1.1.9 style=padding-left:4pt;padding-right:4pt>Planning</td></tr><tr class=ltx_tr id=S1.T1.2.2.2><td class="ltx_td ltx_align_left ltx_border_tt" id=S1.T1.2.2.2.1 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_italic" id=S1.T1.2.2.2.1.1 style=font-size:70%>AD</span></td><td class="ltx_td ltx_border_tt" id=S1.T1.2.2.2.2 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_tt" id=S1.T1.2.2.2.3 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_tt" id=S1.T1.2.2.2.4 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_tt" id=S1.T1.2.2.2.5 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_tt" id=S1.T1.2.2.2.6 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_tt" id=S1.T1.2.2.2.7 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_tt" id=S1.T1.2.2.2.8 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_tt" id=S1.T1.2.2.2.9 style=padding-left:4pt;padding-right:4pt></td></tr><tr class=ltx_tr id=S1.T1.2.3.3><td class="ltx_td ltx_align_left" id=S1.T1.2.3.3.1 style=padding-left:4pt;padding-right:4pt>NuScenes¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib3 title><span class=ltx_text style=font-size:90%>3</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S1.T1.2.3.3.2 style=padding-left:4pt;padding-right:4pt>CVPR 2020</td><td class="ltx_td ltx_align_center" id=S1.T1.2.3.3.3 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_center" id=S1.T1.2.3.3.4 style=padding-left:4pt;padding-right:4pt>Real</td><td class="ltx_td ltx_align_right" id=S1.T1.2.3.3.5 style=padding-left:4pt;padding-right:4pt>400K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.3.3.6 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_right" id=S1.T1.2.3.3.7 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_center" id=S1.T1.2.3.3.8 style=padding-left:4pt;padding-right:4pt>‚úì</td><td class=ltx_td id=S1.T1.2.3.3.9 style=padding-left:4pt;padding-right:4pt></td></tr><tr class=ltx_tr id=S1.T1.2.4.4><td class="ltx_td ltx_align_left" id=S1.T1.2.4.4.1 style=padding-left:4pt;padding-right:4pt>Waymo¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib38 title><span class=ltx_text style=font-size:90%>38</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S1.T1.2.4.4.2 style=padding-left:4pt;padding-right:4pt>CVPR 2020</td><td class="ltx_td ltx_align_center" id=S1.T1.2.4.4.3 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_center" id=S1.T1.2.4.4.4 style=padding-left:4pt;padding-right:4pt>Real</td><td class="ltx_td ltx_align_right" id=S1.T1.2.4.4.5 style=padding-left:4pt;padding-right:4pt>200K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.4.4.6 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_right" id=S1.T1.2.4.4.7 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_center" id=S1.T1.2.4.4.8 style=padding-left:4pt;padding-right:4pt>‚úì</td><td class=ltx_td id=S1.T1.2.4.4.9 style=padding-left:4pt;padding-right:4pt></td></tr><tr class=ltx_tr id=S1.T1.2.5.5><td class="ltx_td ltx_align_left ltx_border_t" id=S1.T1.2.5.5.1 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_italic" id=S1.T1.2.5.5.1.1 style=font-size:70%>Cooperative perception in AD</span></td><td class="ltx_td ltx_border_t" id=S1.T1.2.5.5.2 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S1.T1.2.5.5.3 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S1.T1.2.5.5.4 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S1.T1.2.5.5.5 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S1.T1.2.5.5.6 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S1.T1.2.5.5.7 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S1.T1.2.5.5.8 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S1.T1.2.5.5.9 style=padding-left:4pt;padding-right:4pt></td></tr><tr class=ltx_tr id=S1.T1.2.6.6><td class="ltx_td ltx_align_left" id=S1.T1.2.6.6.1 style=padding-left:4pt;padding-right:4pt>OPV2V¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib52 title><span class=ltx_text style=font-size:90%>52</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S1.T1.2.6.6.2 style=padding-left:4pt;padding-right:4pt>ICRA 2022</td><td class="ltx_td ltx_align_center" id=S1.T1.2.6.6.3 style=padding-left:4pt;padding-right:4pt>2-7</td><td class="ltx_td ltx_align_center" id=S1.T1.2.6.6.4 style=padding-left:4pt;padding-right:4pt>Sim</td><td class="ltx_td ltx_align_right" id=S1.T1.2.6.6.5 style=padding-left:4pt;padding-right:4pt>11K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.6.6.6 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_right" id=S1.T1.2.6.6.7 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_center" id=S1.T1.2.6.6.8 style=padding-left:4pt;padding-right:4pt>‚úì</td><td class=ltx_td id=S1.T1.2.6.6.9 style=padding-left:4pt;padding-right:4pt></td></tr><tr class=ltx_tr id=S1.T1.2.7.7><td class="ltx_td ltx_align_left" id=S1.T1.2.7.7.1 style=padding-left:4pt;padding-right:4pt>V2XSet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib51 title><span class=ltx_text style=font-size:90%>51</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S1.T1.2.7.7.2 style=padding-left:4pt;padding-right:4pt>ECCV 2022</td><td class="ltx_td ltx_align_center" id=S1.T1.2.7.7.3 style=padding-left:4pt;padding-right:4pt>2-5</td><td class="ltx_td ltx_align_center" id=S1.T1.2.7.7.4 style=padding-left:4pt;padding-right:4pt>Sim</td><td class="ltx_td ltx_align_right" id=S1.T1.2.7.7.5 style=padding-left:4pt;padding-right:4pt>11K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.7.7.6 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_right" id=S1.T1.2.7.7.7 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_center" id=S1.T1.2.7.7.8 style=padding-left:4pt;padding-right:4pt>‚úì</td><td class=ltx_td id=S1.T1.2.7.7.9 style=padding-left:4pt;padding-right:4pt></td></tr><tr class=ltx_tr id=S1.T1.2.8.8><td class="ltx_td ltx_align_left" id=S1.T1.2.8.8.1 style=padding-left:4pt;padding-right:4pt>V2V4Real¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib53 title><span class=ltx_text style=font-size:90%>53</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S1.T1.2.8.8.2 style=padding-left:4pt;padding-right:4pt>CVPR 2023</td><td class="ltx_td ltx_align_center" id=S1.T1.2.8.8.3 style=padding-left:4pt;padding-right:4pt>2</td><td class="ltx_td ltx_align_center" id=S1.T1.2.8.8.4 style=padding-left:4pt;padding-right:4pt>Real</td><td class="ltx_td ltx_align_right" id=S1.T1.2.8.8.5 style=padding-left:4pt;padding-right:4pt>20K<sup class=ltx_sup id=S1.T1.2.8.8.5.1>‚Ä†</sup></td><td class="ltx_td ltx_align_right" id=S1.T1.2.8.8.6 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_right" id=S1.T1.2.8.8.7 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_center" id=S1.T1.2.8.8.8 style=padding-left:4pt;padding-right:4pt>‚úì</td><td class=ltx_td id=S1.T1.2.8.8.9 style=padding-left:4pt;padding-right:4pt></td></tr><tr class=ltx_tr id=S1.T1.2.9.9><td class="ltx_td ltx_align_left" id=S1.T1.2.9.9.1 style=padding-left:4pt;padding-right:4pt>V2X-Real¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib48 title><span class=ltx_text style=font-size:90%>48</span></a>]</cite><sup class=ltx_sup id=S1.T1.2.9.9.1.1>‚Ä°</sup></td><td class="ltx_td ltx_align_center" id=S1.T1.2.9.9.2 style=padding-left:4pt;padding-right:4pt>ECCV 2024</td><td class="ltx_td ltx_align_center" id=S1.T1.2.9.9.3 style=padding-left:4pt;padding-right:4pt>2</td><td class="ltx_td ltx_align_center" id=S1.T1.2.9.9.4 style=padding-left:4pt;padding-right:4pt>Real</td><td class="ltx_td ltx_align_right" id=S1.T1.2.9.9.5 style=padding-left:4pt;padding-right:4pt>33K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.9.9.6 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_right" id=S1.T1.2.9.9.7 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_center" id=S1.T1.2.9.9.8 style=padding-left:4pt;padding-right:4pt>‚úì</td><td class=ltx_td id=S1.T1.2.9.9.9 style=padding-left:4pt;padding-right:4pt></td></tr><tr class=ltx_tr id=S1.T1.2.10.10><td class="ltx_td ltx_align_left ltx_border_t" id=S1.T1.2.10.10.1 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_italic" id=S1.T1.2.10.10.1.1 style=font-size:70%>LLM-based AD</span></td><td class="ltx_td ltx_border_t" id=S1.T1.2.10.10.2 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S1.T1.2.10.10.3 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S1.T1.2.10.10.4 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S1.T1.2.10.10.5 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S1.T1.2.10.10.6 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S1.T1.2.10.10.7 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S1.T1.2.10.10.8 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S1.T1.2.10.10.9 style=padding-left:4pt;padding-right:4pt></td></tr><tr class=ltx_tr id=S1.T1.2.11.11><td class="ltx_td ltx_align_left" id=S1.T1.2.11.11.1 style=padding-left:4pt;padding-right:4pt>NuScenes-QA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib35 title><span class=ltx_text style=font-size:90%>35</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S1.T1.2.11.11.2 style=padding-left:4pt;padding-right:4pt>AAAI 2024</td><td class="ltx_td ltx_align_center" id=S1.T1.2.11.11.3 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_center" id=S1.T1.2.11.11.4 style=padding-left:4pt;padding-right:4pt>Real</td><td class="ltx_td ltx_align_right" id=S1.T1.2.11.11.5 style=padding-left:4pt;padding-right:4pt>34K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.11.11.6 style=padding-left:4pt;padding-right:4pt>460K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.11.11.7 style=padding-left:4pt;padding-right:4pt>13.5</td><td class="ltx_td ltx_align_center" id=S1.T1.2.11.11.8 style=padding-left:4pt;padding-right:4pt>‚úì</td><td class=ltx_td id=S1.T1.2.11.11.9 style=padding-left:4pt;padding-right:4pt></td></tr><tr class=ltx_tr id=S1.T1.2.12.12><td class="ltx_td ltx_align_left" id=S1.T1.2.12.12.1 style=padding-left:4pt;padding-right:4pt>Lingo-QA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib33 title><span class=ltx_text style=font-size:90%>33</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S1.T1.2.12.12.2 style=padding-left:4pt;padding-right:4pt>ECCV 2024</td><td class="ltx_td ltx_align_center" id=S1.T1.2.12.12.3 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_center" id=S1.T1.2.12.12.4 style=padding-left:4pt;padding-right:4pt>Real</td><td class="ltx_td ltx_align_right" id=S1.T1.2.12.12.5 style=padding-left:4pt;padding-right:4pt>28K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.12.12.6 style=padding-left:4pt;padding-right:4pt>420K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.12.12.7 style=padding-left:4pt;padding-right:4pt>15.3</td><td class=ltx_td id=S1.T1.2.12.12.8 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_align_center" id=S1.T1.2.12.12.9 style=padding-left:4pt;padding-right:4pt>‚úì</td></tr><tr class=ltx_tr id=S1.T1.2.13.13><td class="ltx_td ltx_align_left" id=S1.T1.2.13.13.1 style=padding-left:4pt;padding-right:4pt>MAPLM-QA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib4 title><span class=ltx_text style=font-size:90%>4</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S1.T1.2.13.13.2 style=padding-left:4pt;padding-right:4pt>CVPR 2024</td><td class="ltx_td ltx_align_center" id=S1.T1.2.13.13.3 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_center" id=S1.T1.2.13.13.4 style=padding-left:4pt;padding-right:4pt>Real</td><td class="ltx_td ltx_align_right" id=S1.T1.2.13.13.5 style=padding-left:4pt;padding-right:4pt>14K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.13.13.6 style=padding-left:4pt;padding-right:4pt>61K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.13.13.7 style=padding-left:4pt;padding-right:4pt>4.4</td><td class="ltx_td ltx_align_center" id=S1.T1.2.13.13.8 style=padding-left:4pt;padding-right:4pt>‚úì</td><td class=ltx_td id=S1.T1.2.13.13.9 style=padding-left:4pt;padding-right:4pt></td></tr><tr class=ltx_tr id=S1.T1.2.14.14><td class="ltx_td ltx_align_left" id=S1.T1.2.14.14.1 style=padding-left:4pt;padding-right:4pt>DriveLM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib37 title><span class=ltx_text style=font-size:90%>37</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S1.T1.2.14.14.2 style=padding-left:4pt;padding-right:4pt>ECCV 2024</td><td class="ltx_td ltx_align_center" id=S1.T1.2.14.14.3 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_center" id=S1.T1.2.14.14.4 style=padding-left:4pt;padding-right:4pt>Sim+Real</td><td class="ltx_td ltx_align_right" id=S1.T1.2.14.14.5 style=padding-left:4pt;padding-right:4pt>69K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.14.14.6 style=padding-left:4pt;padding-right:4pt>2M</td><td class="ltx_td ltx_align_right" id=S1.T1.2.14.14.7 style=padding-left:4pt;padding-right:4pt>29.1</td><td class=ltx_td id=S1.T1.2.14.14.8 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_align_center" id=S1.T1.2.14.14.9 style=padding-left:4pt;padding-right:4pt>‚úì</td></tr><tr class=ltx_tr id=S1.T1.2.15.15><td class="ltx_td ltx_align_left" id=S1.T1.2.15.15.1 style=padding-left:4pt;padding-right:4pt>TOKEN¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib39 title><span class=ltx_text style=font-size:90%>39</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S1.T1.2.15.15.2 style=padding-left:4pt;padding-right:4pt>CoRL 2024</td><td class="ltx_td ltx_align_center" id=S1.T1.2.15.15.3 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_center" id=S1.T1.2.15.15.4 style=padding-left:4pt;padding-right:4pt>Real</td><td class="ltx_td ltx_align_right" id=S1.T1.2.15.15.5 style=padding-left:4pt;padding-right:4pt>28K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.15.15.6 style=padding-left:4pt;padding-right:4pt>434K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.15.15.7 style=padding-left:4pt;padding-right:4pt>15.5</td><td class=ltx_td id=S1.T1.2.15.15.8 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_align_center" id=S1.T1.2.15.15.9 style=padding-left:4pt;padding-right:4pt>‚úì</td></tr><tr class=ltx_tr id=S1.T1.2.16.16><td class="ltx_td ltx_align_left" id=S1.T1.2.16.16.1 style=padding-left:4pt;padding-right:4pt>OmniDrive-nuScenes¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib43 title><span class=ltx_text style=font-size:90%>43</span></a>]</cite></td><td class="ltx_td ltx_align_center" id=S1.T1.2.16.16.2 style=padding-left:4pt;padding-right:4pt>arXiv 2024</td><td class="ltx_td ltx_align_center" id=S1.T1.2.16.16.3 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_center" id=S1.T1.2.16.16.4 style=padding-left:4pt;padding-right:4pt>Real</td><td class="ltx_td ltx_align_right" id=S1.T1.2.16.16.5 style=padding-left:4pt;padding-right:4pt>34K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.16.16.6 style=padding-left:4pt;padding-right:4pt>450K</td><td class="ltx_td ltx_align_right" id=S1.T1.2.16.16.7 style=padding-left:4pt;padding-right:4pt>13.2</td><td class=ltx_td id=S1.T1.2.16.16.8 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_align_center" id=S1.T1.2.16.16.9 style=padding-left:4pt;padding-right:4pt>‚úì</td></tr><tr class=ltx_tr id=S1.T1.2.17.17><td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id=S1.T1.2.17.17.1 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_bold" id=S1.T1.2.17.17.1.1>V2V-QA¬†(Ours)</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S1.T1.2.17.17.2 style=padding-left:4pt;padding-right:4pt>-</td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S1.T1.2.17.17.3 style=padding-left:4pt;padding-right:4pt>2</td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S1.T1.2.17.17.4 style=padding-left:4pt;padding-right:4pt>Real</td><td class="ltx_td ltx_align_right ltx_border_b ltx_border_t" id=S1.T1.2.17.17.5 style=padding-left:4pt;padding-right:4pt>18K</td><td class="ltx_td ltx_align_right ltx_border_b ltx_border_t" id=S1.T1.2.17.17.6 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_bold" id=S1.T1.2.17.17.6.1>577K</span></td><td class="ltx_td ltx_align_right ltx_border_b ltx_border_t" id=S1.T1.2.17.17.7 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_bold" id=S1.T1.2.17.17.7.1>31.7</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S1.T1.2.17.17.8 style=padding-left:4pt;padding-right:4pt>‚úì</td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S1.T1.2.17.17.9 style=padding-left:4pt;padding-right:4pt>‚úì</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 1 compares the V2V-QA dataset with other relevant autonomous driving datasets. It details key characteristics including the number of autonomous vehicles involved in data collection (CAVs), whether the data is simulated or from real-world driving scenarios (Sim/Real), the total number of frames, the number of question-answer pairs, the number of questions per frame, whether point cloud data is included, and whether planning information is provided. Note that the frame count for V2V4Real includes the validation set which is not publicly available and that V2X-Real only made a subset of its data public.</p><details><summary>read the caption</summary>Table 1: Comparison between our V2V-QA¬†and recent related Autonomous Driving (AD) datasets. ‚Ä† This number of frames includes the validation split of V2V4Real¬†[53], which is not released to the public. We build our V2V-QA¬†upon the released training and testing splits of V2V4Real¬†[53]. ‚Ä° V2X-Real¬†[48] only releases a subset of data to the public.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">V2V-LLM Overview<div id=v2v-llm-overview class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#v2v-llm-overview aria-label=Anchor>#</a></span></h4><p>A hypothetical &lsquo;V2V-LLM Overview&rsquo; section would delve into the architecture and functionality of the Vehicle-to-Vehicle Large Language Model (V2V-LLM). It would likely begin by explaining how the system leverages <strong>multi-modal inputs</strong>, combining data from various sensors (LiDAR, cameras) of multiple autonomous vehicles (CAVs). The core of the overview would describe the <strong>LLM&rsquo;s role in fusing this data</strong>, highlighting its ability to integrate diverse perception streams and answer driving-related questions. Key features, such as its capacity for <strong>grounding, object identification, and planning</strong>, would be elaborated upon, showcasing the system&rsquo;s decision-making capabilities within a cooperative environment. Finally, a discussion of the <strong>advantages of V2V-LLM</strong> over traditional methods‚Äîlike improved safety and reliability due to the fusion of multiple perspectives‚Äîwould conclude this section. It would emphasize the novelty of using an LLM for cooperative driving and underscore the potential for increased safety and efficiency in autonomous vehicle systems.</p><h4 class="relative group">Cooperative Perception<div id=cooperative-perception class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#cooperative-perception aria-label=Anchor>#</a></span></h4><p>Cooperative perception in autonomous driving leverages the combined sensor data from multiple vehicles to achieve a more robust and complete understanding of the environment than relying on individual vehicle sensors alone. This is particularly crucial in scenarios with sensor occlusion or malfunction, where individual vehicles may fail to detect important objects or events. <strong>Communication protocols</strong> are key to enabling cooperative perception, with vehicles sharing sensor data, typically using V2V (vehicle-to-vehicle) communication. <strong>Data fusion techniques</strong> then integrate this diverse information, addressing challenges like differing sensor modalities, varying data rates, and potential inconsistencies across sources. The resulting enhanced perception improves the accuracy and reliability of object detection, tracking, and scene understanding. This enhanced situational awareness directly benefits the downstream tasks of planning and decision-making, leading to <strong>safer and more efficient autonomous driving</strong>. Future research directions include exploring more efficient communication strategies, developing robust fusion algorithms adaptable to diverse data quality and types, and investigating the security and privacy implications of sharing data between autonomous vehicles.</p><h4 class="relative group">V2V-QA Dataset<div id=v2v-qa-dataset class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#v2v-qa-dataset aria-label=Anchor>#</a></span></h4><p>The V2V-QA dataset represents a <strong>significant contribution</strong> to the field of cooperative autonomous driving. Its novelty lies in the integration of a large language model (LLM) within a cooperative perception framework, addressing a gap in existing datasets that primarily focus on perception tasks. By creating question-answer pairs around grounding, notable object identification, and planning scenarios, V2V-QA <strong>provides a comprehensive benchmark</strong> for evaluating LLM-based cooperative driving systems. The dataset&rsquo;s utilization of real-world data from V2V4Real enhances its practical relevance and allows for more realistic testing. The <strong>diversity of question types</strong> is also crucial, as it compels models to demonstrate understanding of context, spatial reasoning, and predictive capabilities crucial for safe and robust autonomous systems. The careful design of V2V-QA thus makes it a powerful tool for advancing research in this rapidly evolving area.</p><h4 class="relative group">LLM Fusion Method<div id=llm-fusion-method class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#llm-fusion-method aria-label=Anchor>#</a></span></h4><p>The concept of an &ldquo;LLM Fusion Method&rdquo; in the context of vehicle-to-vehicle (V2V) cooperative autonomous driving is a novel and powerful approach. It leverages the strengths of Large Language Models (LLMs) to integrate and reason over multi-modal data from multiple vehicles. Unlike traditional fusion techniques that primarily focus on low-level feature concatenation or aggregation, the LLM acts as a high-level reasoning engine. <strong>This allows the system to go beyond simple sensor fusion, incorporating contextual understanding, common-sense reasoning, and even uncertainty management into the decision-making process.</strong> The LLM&rsquo;s ability to handle diverse data types (e.g., raw sensor readings, object detections, and even natural language descriptions) is crucial for robust and safe cooperative driving, particularly in scenarios with sensor occlusions or malfunctions. <strong>The key is the LLM&rsquo;s capacity to learn complex relationships and patterns from vast amounts of training data,</strong> enabling it to synthesize information from various sources effectively. This approach potentially leads to improved safety and efficiency compared to methods relying solely on low-level data fusion strategies. <strong>However, challenges associated with LLM&rsquo;s computational cost and explainability must be carefully addressed.</strong> Moreover, data requirements for effective training are significant, demanding substantial and diverse V2V datasets. Further research on efficient LLM architectures and training methods tailored for autonomous driving is essential for practical implementation.</p><h4 class="relative group">Future Work<div id=future-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-work aria-label=Anchor>#</a></span></h4><p>Future research directions stemming from this V2V-LLM model could explore several promising avenues. <strong>Expanding the dataset to encompass more diverse driving scenarios</strong> and a wider range of cooperative driving tasks would significantly enhance the model&rsquo;s robustness and generalizability. <strong>Incorporating HD map data</strong> into the model&rsquo;s input could significantly improve the accuracy and safety of the generated trajectories, especially in complex intersections and challenging road conditions. Investigating <strong>alternative LLM architectures</strong> better suited for multi-modal fusion and real-time processing within the context of autonomous driving is also essential for optimizing performance. Finally, a detailed analysis of the model&rsquo;s limitations and failure cases, as identified in the qualitative evaluation, should inform the development of strategies for error mitigation and enhanced safety protocols. The integration of explainable AI techniques could provide valuable insights into the model&rsquo;s decision-making process, improving trust and facilitating broader adoption.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x2.png alt></figure></p><blockquote><p>üîº This figure shows an example of a question-answer pair in the V2V-QA dataset for the task of grounding at a reference location. The question asks if there is anything at a specific location (x1, y1). The answer confirms the presence of a car and provides its center location. This task assesses the model&rsquo;s ability to identify objects at specified coordinates, which is crucial for safe and reliable autonomous driving.</p><details><summary>read the caption</summary>(a) Q1: Grounding at a reference location.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x3.png alt></figure></p><blockquote><p>üîº This figure demonstrates a question-answering pair related to grounding in a cooperative autonomous driving scenario. The question asks whether any objects are present behind a specific reference object located at a particular location. The image visually depicts the scene, highlighting the reference object and the objects located behind it. This illustrates how the system uses multi-modal input from multiple vehicles to answer complex queries involving spatial reasoning and object occlusion.</p><details><summary>read the caption</summary>(b) Q2: Grounding behind a reference object at a location.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x4.png alt></figure></p><blockquote><p>üîº This figure demonstrates a type of question-answer pair from the V2V-QA dataset. The question asks if there is anything behind a reference object in a specific direction. The figure shows a visual representation of the scene, including the reference object, a car behind it in the specified direction, and the predicted location based on the question. This illustrates the challenge of grounding questions in cooperative autonomous driving, where the visibility of objects for each vehicle might be limited, and relying on information from multiple vehicles is crucial.</p><details><summary>read the caption</summary>(c) Q3: Grounding behind a reference object in a direction.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x5.png alt></figure></p><blockquote><p>üîº This figure shows an example of the &lsquo;Notable Object Identification&rsquo; question-answer pair in the V2V-QA dataset. The question asks whether there are any notable objects that the autonomous vehicle needs to be aware of near its planned trajectory. The answer from the LLM includes the location of nearby objects of interest, providing context to inform the vehicle&rsquo;s planning system. This example demonstrates the use of multi-modal information from multiple connected autonomous vehicles (CAVs) to inform safe and efficient driving decisions.</p><details><summary>read the caption</summary>(d) Q4: Notable object identification.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x6.png alt></figure></p><blockquote><p>üîº This figure shows an example of a planning question-answer pair in the V2V-QA dataset. An autonomous vehicle (CAV) asks the central LLM: &lsquo;I am CAV. What is the suggested future trajectory&mldr;?&rsquo;. The LLM considers the fused perception information from multiple CAVs and generates an answer showing a suggested trajectory to avoid collisions.</p><details><summary>read the caption</summary>(e) Q5: Planning.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x7.png alt></figure></p><blockquote><p>üîº This figure illustrates the five question-answering (QA) pair types included in the V2V-QA dataset. These QAs are designed for cooperative driving scenarios. The five types are: (a) Grounding at a reference location: Asks whether an object exists at a specified location. (b) Grounding behind a reference object at a location: Asks whether an object is behind another object at a specific location. (c) Grounding behind a reference object in a direction: Asks whether an object exists in a specified direction behind a reference object. (d) Notable object identification: Asks to identify notable objects near planned future trajectories. (e) Planning: Asks for suggested future trajectories to avoid collisions. The arrows pointing toward the LLM in the diagram highlight that all connected autonomous vehicles (CAVs) share perception data with the LLM, enabling cooperative responses.</p><details><summary>read the caption</summary>Figure 2: Illustration of V2V-QA‚Äôs 5555 types of QA pairs. The arrows pointing at LLM indicate the perception data from CAVs.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x8.png alt></figure></p><blockquote><p>üîº The figure illustrates the architecture of the Vehicle-to-Vehicle Large Language Model (V2V-LLM) for cooperative autonomous driving. It shows multiple connected autonomous vehicles (CAVs) each independently extracting scene-level feature maps and object-level feature vectors from their LiDAR point cloud data using a 3D object detector. This information is then sent to a central Large Language Model (LLM). The LLM fuses the perception data from all CAVs and answers driving-related questions (provided as language input) using a projector network to align the visual and language embeddings. The final output is a natural language answer.</p><details><summary>read the caption</summary>Figure 3: Model diagram of our proposed V2V-LLM¬†for cooperative autonomous driving.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x9.png alt></figure></p><blockquote><p>üîº This figure shows the architecture of the baseline method with no fusion. In this approach, only a single CAV&rsquo;s LiDAR point cloud is fed to a 3D object detector to extract scene-level feature maps and object-level feature vectors. These are then directly used as the input to the LLM. This method is expected to perform poorly compared to those using multi-CAV data because it ignores the sensor inputs from other CAVs.</p><details><summary>read the caption</summary>(a) No fusion</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x10.png alt></figure></p><blockquote><p>üîº This figure shows the early fusion method used as a baseline in the paper. In early fusion, LiDAR point clouds from two CAVs (Connected Autonomous Vehicles) are merged before being processed by a 3D object detector. The resulting scene-level feature map and object-level feature vectors are then used as the visual input for the LLM (Large Language Model). This approach aims to leverage all sensor data but may be less efficient for large-scale deployments due to the high communication bandwidth required.</p><details><summary>read the caption</summary>(b) Early fusion</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x11.png alt></figure></p><blockquote><p>üîº This figure shows the intermediate fusion approach used as a baseline method in the paper. It depicts how feature maps from multiple connected autonomous vehicles (CAVs) are merged. Unlike early fusion which merges raw LiDAR data from all CAVs before feature extraction, intermediate fusion first extracts scene-level feature maps and object-level feature vectors from each CAV individually. Then, these features are combined using techniques like attention mechanisms (as seen in works cited [50, 51, 52]) to produce a unified representation for the LLM. This approach offers a balance between the computational cost of early fusion and the potential loss of information inherent in no fusion.</p><details><summary>read the caption</summary>(c) Intermediate fusion¬†[50, 51, 52]</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x12.png alt></figure></p><blockquote><p>üîº This figure illustrates the different feature extraction methods used in the baseline models for cooperative autonomous driving. It compares three approaches: no fusion, early fusion, and intermediate fusion. The &rsquo;no fusion&rsquo; approach processes each vehicle&rsquo;s LiDAR data independently, using a separate 3D object detector for each. The &rsquo;early fusion&rsquo; approach merges the LiDAR point clouds from all vehicles before processing them with a single 3D object detector. The &lsquo;intermediate fusion&rsquo; approach uses a cooperative detector (like those explored in prior works) to process the data from each vehicle before passing features to the LLM. Each approach highlights a different way of combining data from multiple vehicles, demonstrating varying levels of computation and communication complexity.</p><details><summary>read the caption</summary>Figure 4: Feature encoder diagrams of the baseline methods from different fusion approaches.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q1_x.png alt></figure></p><blockquote><p>üîº This figure visualizes the performance of the V2V-LLM model on the grounding subtask of the V2V-QA dataset. Grounding involves identifying objects at specific locations. Each row represents a different sample from the testing set. Magenta circles indicate the location specified in the question (query location). Yellow crosses show the location predicted by the V2V-LLM model. Green circles indicate the ground truth location of the relevant object. The figure demonstrates the model&rsquo;s ability to accurately locate objects based on textual queries, highlighting instances where predictions align well with ground truth, while showing potential limitations of the model where the prediction is less accurate.</p><details><summary>read the caption</summary>Figure 5: V2V-LLM‚Äôs grounding results on V2V-QA‚Äôs testing split.¬†Magenta ‚àò\circ‚àò: reference locations in questions. Yellow +++: model output locations. Green ‚àò\circ‚àò: ground-truth answers.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q1_y.png alt></figure></p><blockquote><p>üîº Figure 6 presents a qualitative analysis of the V2V-LLM model&rsquo;s performance on notable object identification and trajectory planning tasks within the V2V-QA testing dataset. The top half illustrates the notable object identification results. Magenta curves represent the planned future trajectories from the questions. Green circles denote the ground truth locations of notable objects. Yellow and cyan markings indicate the identified objects by V2V-LLM, distinguishing between the ego vehicle (CAV_EGO) and another vehicle (CAV_1). The bottom half showcases planning results. Green lines depict ground truth future trajectories, while yellow and cyan curves show trajectories generated by V2V-LLM for CAV_EGO and CAV_1, respectively.</p><details><summary>read the caption</summary>Figure 6: V2V-LLM‚Äôs notable object identification and planning results on V2V-QA‚Äôs testing split. For notable object identification,¬†Magenta curve: planned future trajectories in questions. Green ‚àò\circ‚àò: ground-truth notable object locations. Yellow +++ and Cyan √ó\times√ó: model identification outputs corresponding to CAV_EGO and CAV_1, respectively. For planning, Green line: future trajectories in ground-truth answers. Yellow curve and Cyan curve: model planning outputs corresponding to CAV_EGO and CAV_1, respectively.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q1_dist.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of ground-truth answer locations relative to the CAV (Connected Autonomous Vehicle) in V2V-QA&rsquo;s Q1, which is the grounding task at a reference location. The figure consists of four subplots: (a) shows the distribution of x-coordinates (meters), (b) shows the distribution of y-coordinates (meters), (c) shows the distribution of distances (meters) between the ground-truth answers and the CAV, and (d) shows the distribution of angles (degrees) between the ground-truth answers and the CAV. The x-axis represents the coordinates and the distance, while the y-axis represents the probability density. The plots illustrate the spatial distribution of objects relevant to the grounding questions within the context of the autonomous driving scenario.</p><details><summary>read the caption</summary>(a) x (meters)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q1_angle.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of ground-truth answer locations&rsquo; y-coordinate relative to the CAV in V2V-QA&rsquo;s Q2, which is the question type of grounding behind a reference object at a location. The y-axis represents the probability density, and the x-axis represents the y-coordinate in meters. The distribution is shown in a histogram with different color representing different ranges of distances. The figure helps to understand the spatial distribution of the answers relative to the CAV for this specific question type.</p><details><summary>read the caption</summary>(b) y (meters)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q2_x.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of ground-truth answer locations relative to the CAV in V2V-QA&rsquo;s Q3: Grounding behind a reference object in a direction. The x-axis represents the x-coordinate in meters, where positive x is in the direction the CAV is facing, and the y-axis is the y-coordinate in meters, with positive y to the right of the CAV. The plot shows the probability density function of the distance (in meters) between the CAV&rsquo;s location and the locations of answers. The distribution of the angle (in degrees) between the CAV&rsquo;s front direction and the direction of answer locations is also displayed. The distributions are shown in the same order as in Fig. 8.</p><details><summary>read the caption</summary>(c) distance (meters)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q2_y.png alt></figure></p><blockquote><p>üîº This histogram shows the distribution of the angle (in degrees) of the ground truth answer locations relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA (Vehicle-to-Vehicle Question Answering) dataset for question type Q3: Grounding behind a reference object in a direction. The angle is calculated based on the relative position of the answer location with respect to the reference object&rsquo;s position from the perspective of the CAV.</p><details><summary>read the caption</summary>(d) angle (degrees)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q2_dist.png alt></figure></p><blockquote><p>üîº This figure visualizes the distribution of ground truth answer locations relative to the autonomous vehicle (CAV) in the V2V-QA dataset&rsquo;s Q1 task, which focuses on grounding at a reference location. The plots illustrate the distribution along the x-axis (front-facing direction of the CAV), the y-axis (right-facing direction of the CAV), the distance from the CAV, and the angle relative to the CAV. These distributions provide insights into the spatial characteristics of the ground truth answers within the dataset for this specific task, showing how they are distributed relative to the CAV&rsquo;s perspective.</p><details><summary>read the caption</summary>Figure 7: The distribution of ground-truth answer locations relative to CAV in V2V-QA‚Äôs Q1: Grounding at a reference location.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q2_angle.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of ground-truth answer locations relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA dataset&rsquo;s Q1 task (Grounding at a reference location). The x-axis represents the x-coordinate (meters) in the local coordinate system of the CAV, where x=0 is the CAV&rsquo;s front direction. Similarly, the y-axis represents the y-coordinate (meters), where y=0 is the CAV&rsquo;s right direction. The figure contains four subplots. (a) shows the distribution of x-coordinates, (b) shows the distribution of y-coordinates, (c) shows the distribution of distances (meters) between the CAV and the answer locations, and (d) shows the distribution of angles (degrees) of the answer locations relative to the CAV&rsquo;s front direction. This visualization helps to understand the spatial characteristics of the question-answer pairs in this specific grounding task.</p><details><summary>read the caption</summary>(a) x (meters)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q3_x.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of ground-truth answer locations&rsquo; y-coordinates relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA dataset. The y-axis represents the probability density, and the x-axis shows the y-coordinate in meters. The figure helps illustrate the spatial distribution of the answers in the dataset, indicating the range and frequency of y-coordinates of the answers relative to the CAVs. This is crucial for understanding the data distribution and evaluating the performance of models on various spatial scenarios.</p><details><summary>read the caption</summary>(b) y (meters)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q3_y.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of ground-truth answer locations relative to the CAV in V2V-QA&rsquo;s Q3: Grounding behind a reference object in a direction. The x-axis represents the distance (in meters) between the CAV and the ground-truth answer location. The histogram visually represents the frequency or probability density of different distances observed in the dataset.</p><details><summary>read the caption</summary>(c) distance (meters)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q3_dist.png alt></figure></p><blockquote><p>üîº This histogram shows the distribution of the angles of ground-truth answer locations relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA (Vehicle-to-Vehicle Question Answering) dataset for question type Q3: Grounding behind a reference object in a direction. The angle is calculated as the direction of the ground truth answer location with respect to the CAV&rsquo;s heading.</p><details><summary>read the caption</summary>(d) angle (degrees)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q3_angle.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of ground truth answer locations relative to the autonomous vehicle (CAV) in the V2V-QA dataset&rsquo;s Q2 task, which is &lsquo;Grounding behind a reference object at a location&rsquo;. The distributions are visualized in four subplots showing the x-coordinate, y-coordinate, distance, and angle of the ground truth answer locations relative to the CAV. This provides insights into the spatial characteristics of the answers provided to this specific type of question within the dataset. The distributions help illustrate the range and frequency of various locations of objects behind reference objects, useful for understanding the complexity and variability of the cooperative perception task.</p><details><summary>read the caption</summary>Figure 8: The distribution of ground-truth answer locations relative to CAV in V2V-QA‚Äôs Q2: Grounding behind a reference object at a location.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q4_x.png alt></figure></p><blockquote><p>üîº This figure visualizes the distribution of ground-truth answer locations relative to the CAV (connected autonomous vehicle) in V2V-QA&rsquo;s Q1 (Grounding at a reference location). The x-axis represents the x-coordinate (meters) and the y-axis represents the probability density. Subfigure (a) displays the distribution along the x-axis (front direction of the vehicle), (b) displays the distribution along the y-axis (right direction of the vehicle), (c) shows the distribution of distances (meters) between the ground truth locations and the CAV, and (d) presents the angular distribution (degrees) of the locations relative to the CAV. These distributions help characterize the spatial characteristics of the grounding task in the dataset.</p><details><summary>read the caption</summary>(a) x (meters)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q4_y.png alt></figure></p><blockquote><p>üîº This histogram shows the distribution of the y-coordinates of ground-truth answer locations relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA dataset&rsquo;s Q2 question type. The y-axis represents the probability density, and the x-axis represents the y-coordinate in meters. This visualization helps to understand the spatial distribution of objects relative to the CAV&rsquo;s position when answering grounding questions about objects located behind a reference object at a specific location. The distribution is centered around 0, reflecting the fact that most answers are near the reference object but spread across various Y locations.</p><details><summary>read the caption</summary>(b) y (meters)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q4_dist.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of the distances between ground truth answer locations and the locations of the asking CAVs in the V2V-QA&rsquo;s Q3 dataset. The x-axis represents the distance in meters, and the y-axis represents the probability density. The distribution is shown for different perspectives, including the x-coordinate, y-coordinate, distance, and angle. The x-coordinate and y-coordinate show the distribution along the x-axis and y-axis of the CAV&rsquo;s local coordinate system. The distance shows the distribution of the distances between the ground truth answer locations and the asking CAVs. The angle shows the distribution of the angles between the ground truth answer locations and the asking CAVs. The figure helps to understand the spatial distribution of the objects relative to the asking CAVs.</p><details><summary>read the caption</summary>(c) distance (meters)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q4_angle.png alt></figure></p><blockquote><p>üîº This histogram shows the distribution of the angle (in degrees) of ground-truth answer locations relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA dataset&rsquo;s Q4: Notable Object Identification task. The angle is measured as the direction from the CAV to the ground truth answer location.</p><details><summary>read the caption</summary>(d) angle (degrees)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q5_x.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of ground truth answer locations relative to the ego vehicle (CAV) for question type Q3 of the V2V-QA dataset. Q3 questions are of the form: &lsquo;Is there anything behind the [direction] object?&rsquo; The distributions are visualized for x and y coordinates (in meters), distance from the ego vehicle (in meters), and angle from the ego vehicle‚Äôs forward direction (in degrees). The distributions reveal the spatial spread of objects that satisfy Q3 queries in the dataset, which provides insight into the types of cooperative perception challenges addressed in the dataset and V2V-LLM model.</p><details><summary>read the caption</summary>Figure 9: The distribution of ground-truth answer locations relative to CAV in V2V-QA‚Äôs Q3: Grounding behind a reference object in a direction.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q5_y.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of ground-truth answer locations relative to the CAV (connected autonomous vehicle) in V2V-QA&rsquo;s Q1 (Grounding at a reference location). The figure is composed of four subplots: (a) shows the distribution of x-coordinates (in meters), (b) shows the distribution of y-coordinates (in meters), (c) shows the distribution of distances (in meters) between the CAV and the answer location, and (d) shows the distribution of angles (in degrees) between the CAV&rsquo;s forward direction and the answer location. Each subplot provides a histogram illustrating the frequency distribution of the respective metric. This visualization helps to understand the spatial characteristics and distribution of the answer locations relative to the CAV.</p><details><summary>read the caption</summary>(a) x (meters)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q5_dist.png alt></figure></p><blockquote><p>üîº This figure displays the distribution of ground-truth answer locations along the y-axis relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA dataset. The y-axis represents the lateral direction, with positive values indicating locations to the right of the CAV and negative values indicating locations to the left. The distribution is shown in a histogram, providing insights into the spatial distribution of objects relevant to the grounding questions in the dataset.</p><details><summary>read the caption</summary>(b) y (meters)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/extracted/6203597/figure/stats/q5_angle.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of the distance between the ground-truth answer locations and the asking CAV&rsquo;s location for question type Q3 (Grounding behind a reference object in a direction) in the V2V-QA dataset. The x-axis represents the distance in meters, and the y-axis represents the probability density. The distribution is shown as a histogram.</p><details><summary>read the caption</summary>(c) distance (meters)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x13.png alt></figure></p><blockquote><p>üîº This histogram shows the distribution of the angles of ground-truth answer locations relative to the CAV&rsquo;s coordinate system in the V2V-QA&rsquo;s Q3 dataset. The angle is measured in degrees and represents the direction of the answer location with respect to the CAV&rsquo;s forward direction. This visualization helps to understand the spatial distribution of answers relative to the CAV, providing insights into the dataset&rsquo;s characteristics and the challenges involved in cooperative perception tasks.</p><details><summary>read the caption</summary>(d) angle (degrees)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x14.png alt></figure></p><blockquote><p>üîº This figure visualizes the distribution of ground truth answer locations relative to the ego vehicle (CAV) for question type Q4 in the V2V-QA dataset. Specifically, it shows how the x and y coordinates (relative to the CAV), the distance from the CAV, and the angle relative to the CAV are distributed for the ground truth answers of question type Q4. This provides insights into the spatial characteristics of the notable objects that the model is expected to identify and helps to understand the difficulty and distribution of the data for this question type.</p><details><summary>read the caption</summary>Figure 10: The distribution of ground-truth answer locations relative to CAV in V2V-QA‚Äôs Q4: Notable object identification.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x15.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of ground-truth answer locations relative to the CAV (connected autonomous vehicle) in V2V-QA&rsquo;s Q1 (Grounding at a reference location). The figure consists of four subplots: (a) shows the x-coordinate distribution in meters, (b) shows the y-coordinate distribution in meters, (c) shows the distance distribution in meters, and (d) shows the angle distribution in degrees. Each subplot provides a histogram visualizing the frequency of different values for the corresponding metric. This helps understand the spatial distribution of the objects referenced in the dataset&rsquo;s grounding questions relative to the CAV&rsquo;s perspective.</p><details><summary>read the caption</summary>(a) x (meters)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x16.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of ground-truth answer locations relative to the CAV in V2V-QA&rsquo;s Q2. The y-axis represents the probability density, and the x-axis shows the y-coordinate of the ground truth answer location in meters. The figure provides four subplots: (a) x-coordinate distribution, (b) y-coordinate distribution, (c) distance distribution from the CAV to the answer location, and (d) angle distribution of the answer location relative to the CAV&rsquo;s heading. These distributions are important to understand the characteristics of the dataset and the difficulty of the grounding task, as they show how the relevant information is distributed in the dataset.</p><details><summary>read the caption</summary>(b) y (meters)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x17.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of the ground-truth answer distances to the asking CAV in V2V-QA&rsquo;s Q3. The distance is calculated from the location of the ground-truth answer to the location of the asking CAV. The distribution is shown as a histogram. The x-axis represents the distance in meters, and the y-axis represents the probability density.</p><details><summary>read the caption</summary>(c) distance (meters)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x18.png alt></figure></p><blockquote><p>üîº This histogram shows the distribution of the angle (in degrees) of ground-truth answer locations relative to the CAV (Connected Autonomous Vehicle) in the V2V-QA (Vehicle-to-Vehicle Question Answering) dataset. The angle is measured from the CAV&rsquo;s forward direction, indicating the orientation of the objects being located relative to the vehicle&rsquo;s heading. This distribution helps illustrate the range of object locations relative to the CAV that the models are tasked with identifying in the dataset and in the experiment.</p><details><summary>read the caption</summary>(d) angle (degrees)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x19.png alt></figure></p><blockquote><p>üîº This figure visualizes the distribution of ground truth answer locations relative to the ego vehicle (CAV) for the planning questions in the V2V-QA dataset. Specifically, it shows the distribution of the x-coordinate, y-coordinate, distance, and angle of the ending waypoints from the ground truth trajectories, providing insights into the spatial characteristics of the planning task within the dataset.</p><details><summary>read the caption</summary>Figure 11: The distribution of ground-truth answer locations relative to CAV in V2V-QA‚Äôs Q5: Planning.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x20.png alt></figure></p><blockquote><p>üîº This figure compares the performance of V2V-LLM and several baseline methods (No Fusion, Early Fusion, AttFuse, V2X-VIT, COBEVT) on the grounding task of the V2V-QA dataset. The grounding task involves identifying objects at specific locations. Each row presents a different example. The image shows the point cloud data. Magenta circles indicate the reference location provided in the question. Yellow plus signs (+) show the location predicted by the model. Green circles show the ground truth location of the object.</p><details><summary>read the caption</summary>Figure 12: V2V-LLM¬†and baseline methods‚Äô grounding results on V2V-QA‚Äôs testing split.¬†Magenta ‚àò\circ‚àò: reference locations in questions. Yellow +++: model output locations. Green ‚àò\circ‚àò: ground-truth answers.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09980/x21.png alt></figure></p><blockquote><p>üîº Figure 13 presents a comparison of grounding results between the proposed V2V-LLM model and several baseline methods. The task is to identify objects at specified locations within a driving scene, using data from the V2V-QA testing split. The figure visualizes the results by showing LiDAR point cloud data for a section of the driving scene and highlighting relevant objects. Magenta circles denote the reference locations specified in the questions. Yellow plus symbols represent the locations predicted by each method. Ground truth object locations are shown as green circles. This comparison allows for an evaluation of the accuracy and effectiveness of each method in cooperative grounding tasks. In particular, it can assess how well each approach fuses information from multiple vehicles to accurately and reliably identify objects in the driving scene.</p><details><summary>read the caption</summary>Figure 13: V2V-LLM¬†and baseline methods‚Äô grounding results on V2V-QA‚Äôs testing split.¬†Magenta ‚àò\circ‚àò: reference locations in questions. Yellow +++: model output locations. Green ‚àò\circ‚àò: ground-truth answers.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S3.T2.2><thead class=ltx_thead><tr class=ltx_tr id=S3.T2.2.1.1><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id=S3.T2.2.1.1.1><span class=ltx_text id=S3.T2.2.1.1.1.1 style=font-size:90%>QA type</span></th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id=S3.T2.2.1.1.2><span class=ltx_text id=S3.T2.2.1.1.2.1 style=font-size:90%>Training</span></th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" id=S3.T2.2.1.1.3><span class=ltx_text id=S3.T2.2.1.1.3.1 style=font-size:90%>Testing</span></th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id=S3.T2.2.1.1.4><span class=ltx_text id=S3.T2.2.1.1.4.1 style=font-size:90%>Total</span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S3.T2.2.2.1><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id=S3.T2.2.2.1.1><span class=ltx_text id=S3.T2.2.2.1.1.1 style=font-size:90%>Q1</span></th><td class="ltx_td ltx_align_right ltx_border_tt" id=S3.T2.2.2.1.2><span class=ltx_text id=S3.T2.2.2.1.2.1 style=font-size:90%>354820</span></td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id=S3.T2.2.2.1.3><span class=ltx_text id=S3.T2.2.2.1.3.1 style=font-size:90%>121383</span></td><td class="ltx_td ltx_align_right ltx_border_tt" id=S3.T2.2.2.1.4><span class=ltx_text id=S3.T2.2.2.1.4.1 style=font-size:90%>476203</span></td></tr><tr class=ltx_tr id=S3.T2.2.3.2><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id=S3.T2.2.3.2.1><span class=ltx_text id=S3.T2.2.3.2.1.1 style=font-size:90%>Q2</span></th><td class="ltx_td ltx_align_right" id=S3.T2.2.3.2.2><span class=ltx_text id=S3.T2.2.3.2.2.1 style=font-size:90%>35700</span></td><td class="ltx_td ltx_align_right ltx_border_r" id=S3.T2.2.3.2.3><span class=ltx_text id=S3.T2.2.3.2.3.1 style=font-size:90%>13882</span></td><td class="ltx_td ltx_align_right" id=S3.T2.2.3.2.4><span class=ltx_text id=S3.T2.2.3.2.4.1 style=font-size:90%>49582</span></td></tr><tr class=ltx_tr id=S3.T2.2.4.3><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id=S3.T2.2.4.3.1><span class=ltx_text id=S3.T2.2.4.3.1.1 style=font-size:90%>Q3</span></th><td class="ltx_td ltx_align_right" id=S3.T2.2.4.3.2><span class=ltx_text id=S3.T2.2.4.3.2.1 style=font-size:90%>14339</span></td><td class="ltx_td ltx_align_right ltx_border_r" id=S3.T2.2.4.3.3><span class=ltx_text id=S3.T2.2.4.3.3.1 style=font-size:90%>5097</span></td><td class="ltx_td ltx_align_right" id=S3.T2.2.4.3.4><span class=ltx_text id=S3.T2.2.4.3.4.1 style=font-size:90%>19436</span></td></tr><tr class=ltx_tr id=S3.T2.2.5.4><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id=S3.T2.2.5.4.1><span class=ltx_text id=S3.T2.2.5.4.1.1 style=font-size:90%>Q4</span></th><td class="ltx_td ltx_align_right" id=S3.T2.2.5.4.2><span class=ltx_text id=S3.T2.2.5.4.2.1 style=font-size:90%>12290</span></td><td class="ltx_td ltx_align_right ltx_border_r" id=S3.T2.2.5.4.3><span class=ltx_text id=S3.T2.2.5.4.3.1 style=font-size:90%>3446</span></td><td class="ltx_td ltx_align_right" id=S3.T2.2.5.4.4><span class=ltx_text id=S3.T2.2.5.4.4.1 style=font-size:90%>15736</span></td></tr><tr class=ltx_tr id=S3.T2.2.6.5><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id=S3.T2.2.6.5.1><span class=ltx_text id=S3.T2.2.6.5.1.1 style=font-size:90%>Q5</span></th><td class="ltx_td ltx_align_right" id=S3.T2.2.6.5.2><span class=ltx_text id=S3.T2.2.6.5.2.1 style=font-size:90%>12290</span></td><td class="ltx_td ltx_align_right ltx_border_r" id=S3.T2.2.6.5.3><span class=ltx_text id=S3.T2.2.6.5.3.1 style=font-size:90%>3446</span></td><td class="ltx_td ltx_align_right" id=S3.T2.2.6.5.4><span class=ltx_text id=S3.T2.2.6.5.4.1 style=font-size:90%>15736</span></td></tr><tr class=ltx_tr id=S3.T2.2.7.6><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id=S3.T2.2.7.6.1><span class=ltx_text id=S3.T2.2.7.6.1.1 style=font-size:90%>Total</span></th><td class="ltx_td ltx_align_right ltx_border_t" id=S3.T2.2.7.6.2><span class=ltx_text id=S3.T2.2.7.6.2.1 style=font-size:90%>429439</span></td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id=S3.T2.2.7.6.3><span class=ltx_text id=S3.T2.2.7.6.3.1 style=font-size:90%>147254</span></td><td class="ltx_td ltx_align_right ltx_border_t" id=S3.T2.2.7.6.4><span class=ltx_text id=S3.T2.2.7.6.4.1 style=font-size:90%>576693</span></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 2 presents a detailed breakdown of the V2V-QA dataset&rsquo;s composition. It shows the number of question-answer pairs for each of the five question types. These types are categorized as: Q1 (Grounding at a reference location), Q2 (Grounding behind a reference object at a location), Q3 (Grounding behind a reference object in a direction), Q4 (Notable object identification), and Q5 (Planning). The table displays the quantity of training and testing data for each question type and the overall total, offering insights into the dataset&rsquo;s size and balance across different tasks.</p><details><summary>read the caption</summary>Table 2: Dataset statistics of our V2V-QA. Q1: Grounding at a reference location. Q2: Grounding behind a reference object at a location. Q3: Grounding behind a reference object in a direction. Q4: Notable object identification. Q5: Planning.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S5.T3.18.18><thead class=ltx_thead><tr class=ltx_tr id=S5.T3.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id=S5.T3.1.1.1.2 rowspan=2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.1.1.1.2.1 style=font-size:90%>Method</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan=3 id=S5.T3.1.1.1.3 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.1.1.1.3.1 style=font-size:90%>Q1</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan=3 id=S5.T3.1.1.1.4 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.1.1.1.4.1 style=font-size:90%>Q2</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan=3 id=S5.T3.1.1.1.5 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.1.1.1.5.1 style=font-size:90%>Q3</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T3.1.1.1.6 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.1.1.1.6.1 style=font-size:90%>Q</span><sub class=ltx_sub id=S5.T3.1.1.1.6.2><span class=ltx_text id=S5.T3.1.1.1.6.2.1 style=font-size:90%>Gr</span></sub></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan=3 id=S5.T3.1.1.1.7 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.1.1.1.7.1 style=font-size:90%>Q4</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan=2 id=S5.T3.1.1.1.8 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.1.1.1.8.1 style=font-size:90%>Q5</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T3.1.1.1.1 rowspan=2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.1.1.1.1.1 style=font-size:90%>Comm(MB)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T3.1.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.1.m1.1d">‚Üì</annotation></semantics></math></span></th></tr><tr class=ltx_tr id=S5.T3.18.18.18><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.2.2.2.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.2.2.2.1.1 style=font-size:90%>F1</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.2.2.2.1.m1.1"><semantics id="S5.T3.2.2.2.1.m1.1a"><mo id="S5.T3.2.2.2.1.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T3.2.2.2.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.1.m1.1b"><ci id="S5.T3.2.2.2.1.m1.1.1.cmml" xref="S5.T3.2.2.2.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.2.2.1.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.3.3.3.2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.3.3.3.2.1 style=font-size:90%>P</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.3.3.3.2.m1.1"><semantics id="S5.T3.3.3.3.2.m1.1a"><mo id="S5.T3.3.3.3.2.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T3.3.3.3.2.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.2.m1.1b"><ci id="S5.T3.3.3.3.2.m1.1.1.cmml" xref="S5.T3.3.3.3.2.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.3.2.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.4.4.4.3 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.4.4.4.3.1 style=font-size:90%>R</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.4.4.4.3.m1.1"><semantics id="S5.T3.4.4.4.3.m1.1a"><mo id="S5.T3.4.4.4.3.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T3.4.4.4.3.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.3.m1.1b"><ci id="S5.T3.4.4.4.3.m1.1.1.cmml" xref="S5.T3.4.4.4.3.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.4.4.3.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.5.5.5.4 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.5.5.5.4.1 style=font-size:90%>F1</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.5.5.5.4.m1.1"><semantics id="S5.T3.5.5.5.4.m1.1a"><mo id="S5.T3.5.5.5.4.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T3.5.5.5.4.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.5.4.m1.1b"><ci id="S5.T3.5.5.5.4.m1.1.1.cmml" xref="S5.T3.5.5.5.4.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.5.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.5.5.5.4.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.6.6.6.5 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.6.6.6.5.1 style=font-size:90%>P</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.6.6.6.5.m1.1"><semantics id="S5.T3.6.6.6.5.m1.1a"><mo id="S5.T3.6.6.6.5.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T3.6.6.6.5.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.6.5.m1.1b"><ci id="S5.T3.6.6.6.5.m1.1.1.cmml" xref="S5.T3.6.6.6.5.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.6.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.6.6.6.5.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.7.7.7.6 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.7.7.7.6.1 style=font-size:90%>R</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.7.7.7.6.m1.1"><semantics id="S5.T3.7.7.7.6.m1.1a"><mo id="S5.T3.7.7.7.6.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T3.7.7.7.6.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T3.7.7.7.6.m1.1b"><ci id="S5.T3.7.7.7.6.m1.1.1.cmml" xref="S5.T3.7.7.7.6.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.7.7.7.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.7.7.7.6.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.8.8.8.7 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.8.8.8.7.1 style=font-size:90%>F1</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.8.8.8.7.m1.1"><semantics id="S5.T3.8.8.8.7.m1.1a"><mo id="S5.T3.8.8.8.7.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T3.8.8.8.7.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T3.8.8.8.7.m1.1b"><ci id="S5.T3.8.8.8.7.m1.1.1.cmml" xref="S5.T3.8.8.8.7.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.8.8.7.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.8.8.8.7.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.9.9.9.8 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.9.9.9.8.1 style=font-size:90%>P</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.9.9.9.8.m1.1"><semantics id="S5.T3.9.9.9.8.m1.1a"><mo id="S5.T3.9.9.9.8.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T3.9.9.9.8.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T3.9.9.9.8.m1.1b"><ci id="S5.T3.9.9.9.8.m1.1.1.cmml" xref="S5.T3.9.9.9.8.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.9.9.9.8.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.9.9.9.8.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.10.10.10.9 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.10.10.10.9.1 style=font-size:90%>R</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.10.10.10.9.m1.1"><semantics id="S5.T3.10.10.10.9.m1.1a"><mo id="S5.T3.10.10.10.9.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T3.10.10.10.9.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T3.10.10.10.9.m1.1b"><ci id="S5.T3.10.10.10.9.m1.1.1.cmml" xref="S5.T3.10.10.10.9.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.10.10.10.9.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.10.10.10.9.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.11.11.11.10 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.11.11.11.10.1 style=font-size:90%>F1</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.11.11.11.10.m1.1"><semantics id="S5.T3.11.11.11.10.m1.1a"><mo id="S5.T3.11.11.11.10.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T3.11.11.11.10.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T3.11.11.11.10.m1.1b"><ci id="S5.T3.11.11.11.10.m1.1.1.cmml" xref="S5.T3.11.11.11.10.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.11.11.11.10.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.11.11.11.10.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.12.12.12.11 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.12.12.12.11.1 style=font-size:90%>F1</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.12.12.12.11.m1.1"><semantics id="S5.T3.12.12.12.11.m1.1a"><mo id="S5.T3.12.12.12.11.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T3.12.12.12.11.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T3.12.12.12.11.m1.1b"><ci id="S5.T3.12.12.12.11.m1.1.1.cmml" xref="S5.T3.12.12.12.11.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.12.12.12.11.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.12.12.12.11.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.13.13.13.12 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.13.13.13.12.1 style=font-size:90%>P</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.13.13.13.12.m1.1"><semantics id="S5.T3.13.13.13.12.m1.1a"><mo id="S5.T3.13.13.13.12.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T3.13.13.13.12.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T3.13.13.13.12.m1.1b"><ci id="S5.T3.13.13.13.12.m1.1.1.cmml" xref="S5.T3.13.13.13.12.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.13.13.13.12.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.13.13.13.12.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.14.14.14.13 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.14.14.14.13.1 style=font-size:90%>R</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.14.14.14.13.m1.1"><semantics id="S5.T3.14.14.14.13.m1.1a"><mo id="S5.T3.14.14.14.13.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T3.14.14.14.13.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T3.14.14.14.13.m1.1b"><ci id="S5.T3.14.14.14.13.m1.1.1.cmml" xref="S5.T3.14.14.14.13.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.14.14.14.13.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.14.14.14.13.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.16.16.16.15 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.16.16.16.15.1 style=font-size:90%>L2</span><sub class=ltx_sub id=S5.T3.16.16.16.15.2><span class="ltx_text ltx_font_italic" id=S5.T3.16.16.16.15.2.1 style=font-size:90%>avg</span></sub><span class=ltx_text id=S5.T3.16.16.16.15.3 style=font-size:90%> (m)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.16.16.16.15.m2.1"><semantics id="S5.T3.16.16.16.15.m2.1a"><mo id="S5.T3.16.16.16.15.m2.1.1" mathsize="90%" stretchy="false" xref="S5.T3.16.16.16.15.m2.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T3.16.16.16.15.m2.1b"><ci id="S5.T3.16.16.16.15.m2.1.1.cmml" xref="S5.T3.16.16.16.15.m2.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.16.16.16.15.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.16.16.16.15.m2.1d">‚Üì</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.18.18.18.17 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.18.17.1 style=font-size:90%>CR</span><sub class=ltx_sub id=S5.T3.18.18.18.17.2><span class="ltx_text ltx_font_italic" id=S5.T3.18.18.18.17.2.1 style=font-size:90%>avg</span></sub><span class=ltx_text id=S5.T3.18.18.18.17.3 style=font-size:90%> (%)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.18.18.18.17.m2.1"><semantics id="S5.T3.18.18.18.17.m2.1a"><mo id="S5.T3.18.18.18.17.m2.1.1" mathsize="90%" stretchy="false" xref="S5.T3.18.18.18.17.m2.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T3.18.18.18.17.m2.1b"><ci id="S5.T3.18.18.18.17.m2.1.1.cmml" xref="S5.T3.18.18.18.17.m2.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.18.18.18.17.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.18.18.18.17.m2.1d">‚Üì</annotation></semantics></math></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T3.18.18.19.1><td class="ltx_td ltx_align_left ltx_border_tt" id=S5.T3.18.18.19.1.1 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_italic" id=S5.T3.18.18.19.1.1.1 style=font-size:90%>No Fusion</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.19.1.2.1 style=font-size:90%>66.6</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.3 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.19.1.3.1 style=font-size:90%>77.9</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.4 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.19.1.4.1 style=font-size:90%>58.2</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.5 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.19.1.5.1 style=font-size:90%>22.6</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.6 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.19.1.6.1 style=font-size:90%>29.4</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.7 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.19.1.7.1 style=font-size:90%>18.4</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.8 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.19.1.8.1 style=font-size:90%>17.2</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.9 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.19.1.9.1 style=font-size:90%>17.4</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.10 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.19.1.10.1 style=font-size:90%>16.9</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.11 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.19.1.11.1 style=font-size:90%>35.5</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.12 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.19.1.12.1 style=font-size:90%>47.3</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.13 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.19.1.13.1 style=font-size:90%>49.2</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.14 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.19.1.14.1 style=font-size:90%>45.6</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.15 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.19.1.15.1 style=font-size:90%>6.55</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.16 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.19.1.16.1 style=font-size:90%>4.57</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.18.18.19.1.17 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.19.1.17.1 style=font-size:90%>0</span></td></tr><tr class=ltx_tr id=S5.T3.18.18.20.2><td class="ltx_td ltx_align_left" id=S5.T3.18.18.20.2.1 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_italic" id=S5.T3.18.18.20.2.1.1 style=font-size:90%>Early Fusion</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.2 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.20.2.2.1 style=font-size:90%>73.5</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.3 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.20.2.3.1 style=font-size:90%>82.2</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.4 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.20.2.4.1 style=font-size:90%>66.5</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.5 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.20.2.5.1 style=font-size:90%>23.3</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.6 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.20.2.6.1 style=font-size:90%>29.1</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.7 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.20.2.7.1 style=font-size:90%>19.5</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.8 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.20.2.8.1 style=font-size:90%>20.8</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.9 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.20.2.9.1 style=font-size:90%>22.7</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.10 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.20.2.10.1 style=font-size:90%>19.3</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.11 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.20.2.11.1 style=font-size:90%>39.2</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.12 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.20.2.12.1 style=font-size:90%>53.9</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.13 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.20.2.13.1 style=font-size:90%>55.4</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.14 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.20.2.14.1 style=font-size:90%>52.6</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.15 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.20.2.15.1 style=font-size:90%>6.20</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.16 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.20.2.16.1 style=font-size:90%>3.55</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.20.2.17 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.20.2.17.1 style=font-size:90%>0.96</span></td></tr><tr class=ltx_tr id=S5.T3.18.18.21.3><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T3.18.18.21.3.1 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_italic" id=S5.T3.18.18.21.3.1.1 style=font-size:70%>Intermediate Fusion</span></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.2 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.3 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.4 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.5 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.6 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.7 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.8 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.9 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.10 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.11 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.12 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.13 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.14 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.15 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.16 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.21.3.17 style=padding-left:2pt;padding-right:2pt></td></tr><tr class=ltx_tr id=S5.T3.18.18.22.4><td class="ltx_td ltx_align_left" id=S5.T3.18.18.22.4.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.22.4.1.1 style=font-size:90%>AttFuse¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class=ltx_text id=S5.T3.18.18.22.4.1.2.1 style=font-size:90%>[</span><a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib52 title><span class=ltx_text style=font-size:90%>52</span></a><span class=ltx_text id=S5.T3.18.18.22.4.1.3.2 style=font-size:90%>]</span></cite></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.22.4.2.1 style=font-size:90%>70.7</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.3 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.22.4.3.1 style=font-size:90%>79.6</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.4 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.22.4.4.1 style=font-size:90%>63.6</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.5 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.22.4.5.1 style=font-size:90%>26.4</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.6 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.22.4.6.1 style=font-size:90%>31.6</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.7 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.22.4.7.1 style=font-size:90%>22.7</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.8 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.22.4.8.1 style=font-size:90%>18.4</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.9 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.22.4.9.1 style=font-size:90%>19.6</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.10 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.22.4.10.1 style=font-size:90%>17.4</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.11 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.22.4.11.1 style=font-size:90%>38.5</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.12 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.22.4.12.1 style=font-size:90%>56.9</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.13 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.22.4.13.1 style=font-size:90%>57.2</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.14 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.22.4.14.1 style=font-size:90%>56.6</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.15 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.22.4.15.1 style=font-size:90%>6.83</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.16 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.22.4.16.1 style=font-size:90%>4.12</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.22.4.17 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.22.4.17.1 style=font-size:90%>0.20</span></td></tr><tr class=ltx_tr id=S5.T3.18.18.23.5><td class="ltx_td ltx_align_left" id=S5.T3.18.18.23.5.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.23.5.1.1 style=font-size:90%>V2X-ViT¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class=ltx_text id=S5.T3.18.18.23.5.1.2.1 style=font-size:90%>[</span><a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib51 title><span class=ltx_text style=font-size:90%>51</span></a><span class=ltx_text id=S5.T3.18.18.23.5.1.3.2 style=font-size:90%>]</span></cite></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.23.5.2.1 style=font-size:90%>70.8</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.3 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.23.5.3.1 style=font-size:90%>81.1</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.4 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.23.5.4.1 style=font-size:90%>62.8</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.5 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.23.5.5.1 style=font-size:90%>28.0</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.6 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.23.5.6.1 style=font-size:90%>33.9</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.7 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.23.5.7.1 style=font-size:90%>23.9</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.8 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.23.5.8.1 style=font-size:90%>22.6</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.9 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.23.5.9.1 style=font-size:90%>25.2</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.10 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.23.5.10.1 style=font-size:90%>20.5</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.11 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.23.5.11.1 style=font-size:90%>40.5</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.12 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.23.5.12.1 style=font-size:90%>57.6</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.13 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.23.5.13.1 style=font-size:90%>57.0</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.14 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.23.5.14.1 style=font-size:90%>58.2</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.15 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.23.5.15.1 style=font-size:90%>7.08</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.16 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.23.5.16.1 style=font-size:90%>4.33</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.23.5.17 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.23.5.17.1 style=font-size:90%>0.20</span></td></tr><tr class=ltx_tr id=S5.T3.18.18.24.6><td class="ltx_td ltx_align_left" id=S5.T3.18.18.24.6.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.24.6.1.1 style=font-size:90%>CoBEVT¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class=ltx_text id=S5.T3.18.18.24.6.1.2.1 style=font-size:90%>[</span><a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib50 title><span class=ltx_text style=font-size:90%>50</span></a><span class=ltx_text id=S5.T3.18.18.24.6.1.3.2 style=font-size:90%>]</span></cite></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.2 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.24.6.2.1 style=font-size:90%>72.2</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.3 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.24.6.3.1 style=font-size:90%>76.8</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.4 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.24.6.4.1 style=font-size:90%>68.1</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.5 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.24.6.5.1 style=font-size:90%>29.3</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.6 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.24.6.6.1 style=font-size:90%>34.7</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.7 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.24.6.7.1 style=font-size:90%>25.3</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.8 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.24.6.8.1 style=font-size:90%>21.3</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.9 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.24.6.9.1 style=font-size:90%>22.1</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.10 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.24.6.10.1 style=font-size:90%>20.6</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.11 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.24.6.11.1 style=font-size:90%>40.9</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.12 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.24.6.12.1 style=font-size:90%>57.6</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.13 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.24.6.13.1 style=font-size:90%>57.2</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.14 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.24.6.14.1 style=font-size:90%>58.1</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.15 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.24.6.15.1 style=font-size:90%>6.72</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.16 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.24.6.16.1 style=font-size:90%>3.88</span></td><td class="ltx_td ltx_align_center" id=S5.T3.18.18.24.6.17 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.24.6.17.1 style=font-size:90%>0.20</span></td></tr><tr class=ltx_tr id=S5.T3.18.18.25.7><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T3.18.18.25.7.1 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_italic" id=S5.T3.18.18.25.7.1.1 style=font-size:70%>LLM Fusion</span></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.2 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.3 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.4 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.5 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.6 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.7 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.8 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.9 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.10 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.11 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.12 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.13 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.14 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.15 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.16 style=padding-left:2pt;padding-right:2pt></td><td class="ltx_td ltx_border_t" id=S5.T3.18.18.25.7.17 style=padding-left:2pt;padding-right:2pt></td></tr><tr class=ltx_tr id=S5.T3.18.18.26.8><td class="ltx_td ltx_align_left ltx_border_b" id=S5.T3.18.18.26.8.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.26.8.1.1 style=font-size:90%>V2V-LLM¬†(Ours)</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.26.8.2.1 style=font-size:90%>70.0</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.3 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.26.8.3.1 style=font-size:90%>80.1</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.4 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.26.8.4.1 style=font-size:90%>62.2</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.5 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.26.8.5.1 style=font-size:90%>30.8</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.6 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.26.8.6.1 style=font-size:90%>36.3</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.7 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.26.8.7.1 style=font-size:90%>26.7</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.8 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.26.8.8.1 style=font-size:90%>21.2</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.9 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.26.8.9.1 style=font-size:90%>21.5</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.10 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.26.8.10.1 style=font-size:90%>20.8</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.11 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.18.18.26.8.11.1 style=font-size:90%>40.7</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.12 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.26.8.12.1 style=font-size:90%>59.7</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.13 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.26.8.13.1 style=font-size:90%>61.9</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.14 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.26.8.14.1 style=font-size:90%>57.6</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.15 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.26.8.15.1 style=font-size:90%>4.99</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.16 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T3.18.18.26.8.16.1 style=font-size:90%>3.00</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T3.18.18.26.8.17 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T3.18.18.26.8.17.1 style=font-size:90%>0.203</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of the performance of various methods on the V2V-QA dataset. The methods include different fusion approaches: no fusion, early fusion, intermediate fusion, and the proposed V2V-LLM model. The table shows the performance metrics (F1 score, precision, recall, L2 distance error, and collision rate) for five different question types in the V2V-QA dataset: grounding at a reference location (Q1), grounding behind a reference object at a location (Q2), grounding behind a reference object in a direction (Q3), notable object identification (Q4), and planning (Q5). The average performance across the three grounding questions is also provided (QGr). The communication cost (Comm) for each method is also included. The best results for each metric are shown in bold, and the second-best results are underlined.</p><details><summary>read the caption</summary>Table 3: V2V-LLM‚Äôs performance in V2V-QA‚Äôs testing split in comparison with baseline methods. Q1: Grounding at a reference location. Q2: Grounding behind a reference object at a location. Q3: Grounding behind a reference object in a direction. QGr: Average of grounding (Q1, Q2, and Q3). Q4: Notable object identification. Q5: Planning. P: Precision. R: Recall. L2: L2 distance error. CR: Collision Rate. Comm: Communication cost. In each column, the best results are in boldface, and the second-best results are in underline.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S5.T4.9.9><thead class=ltx_thead><tr class=ltx_tr id=S5.T4.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id=S5.T4.1.1.1.2 rowspan=2><span class=ltx_text id=S5.T4.1.1.1.2.1 style=font-size:90%>Method</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan=4 id=S5.T4.1.1.1.3><span class=ltx_text id=S5.T4.1.1.1.3.1 style=font-size:90%>L2 (m)</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan=4 id=S5.T4.1.1.1.4><span class=ltx_text id=S5.T4.1.1.1.4.1 style=font-size:90%>CR (%)</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T4.1.1.1.1 rowspan=2><span class=ltx_text id=S5.T4.1.1.1.1.1 style=font-size:90%>Comm (MB)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.1.1.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T4.1.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.1.1.m1.1d">‚Üì</annotation></semantics></math></span></th></tr><tr class=ltx_tr id=S5.T4.9.9.9><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T4.2.2.2.1><span class=ltx_text id=S5.T4.2.2.2.1.1 style=font-size:90%>1s</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.2.2.2.1.m1.1"><semantics id="S5.T4.2.2.2.1.m1.1a"><mo id="S5.T4.2.2.2.1.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T4.2.2.2.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.1.m1.1b"><ci id="S5.T4.2.2.2.1.m1.1.1.cmml" xref="S5.T4.2.2.2.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.2.1.m1.1d">‚Üì</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T4.3.3.3.2><span class=ltx_text id=S5.T4.3.3.3.2.1 style=font-size:90%>2s</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.3.3.3.2.m1.1"><semantics id="S5.T4.3.3.3.2.m1.1a"><mo id="S5.T4.3.3.3.2.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T4.3.3.3.2.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.2.m1.1b"><ci id="S5.T4.3.3.3.2.m1.1.1.cmml" xref="S5.T4.3.3.3.2.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.3.3.3.2.m1.1d">‚Üì</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T4.4.4.4.3><span class=ltx_text id=S5.T4.4.4.4.3.1 style=font-size:90%>3s</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.4.4.4.3.m1.1"><semantics id="S5.T4.4.4.4.3.m1.1a"><mo id="S5.T4.4.4.4.3.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T4.4.4.4.3.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.4.3.m1.1b"><ci id="S5.T4.4.4.4.3.m1.1.1.cmml" xref="S5.T4.4.4.4.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.4.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.4.4.4.3.m1.1d">‚Üì</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T4.5.5.5.4><span class=ltx_text id=S5.T4.5.5.5.4.1 style=font-size:90%>average</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.5.5.5.4.m1.1"><semantics id="S5.T4.5.5.5.4.m1.1a"><mo id="S5.T4.5.5.5.4.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T4.5.5.5.4.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T4.5.5.5.4.m1.1b"><ci id="S5.T4.5.5.5.4.m1.1.1.cmml" xref="S5.T4.5.5.5.4.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.5.5.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.5.5.5.4.m1.1d">‚Üì</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T4.6.6.6.5><span class=ltx_text id=S5.T4.6.6.6.5.1 style=font-size:90%>1s</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.6.6.6.5.m1.1"><semantics id="S5.T4.6.6.6.5.m1.1a"><mo id="S5.T4.6.6.6.5.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T4.6.6.6.5.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T4.6.6.6.5.m1.1b"><ci id="S5.T4.6.6.6.5.m1.1.1.cmml" xref="S5.T4.6.6.6.5.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.6.6.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.6.6.6.5.m1.1d">‚Üì</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T4.7.7.7.6><span class=ltx_text id=S5.T4.7.7.7.6.1 style=font-size:90%>2s</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.7.7.7.6.m1.1"><semantics id="S5.T4.7.7.7.6.m1.1a"><mo id="S5.T4.7.7.7.6.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T4.7.7.7.6.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T4.7.7.7.6.m1.1b"><ci id="S5.T4.7.7.7.6.m1.1.1.cmml" xref="S5.T4.7.7.7.6.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.7.7.7.6.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.7.7.7.6.m1.1d">‚Üì</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T4.8.8.8.7><span class=ltx_text id=S5.T4.8.8.8.7.1 style=font-size:90%>3s</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.8.8.8.7.m1.1"><semantics id="S5.T4.8.8.8.7.m1.1a"><mo id="S5.T4.8.8.8.7.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T4.8.8.8.7.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T4.8.8.8.7.m1.1b"><ci id="S5.T4.8.8.8.7.m1.1.1.cmml" xref="S5.T4.8.8.8.7.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.8.8.8.7.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.8.8.8.7.m1.1d">‚Üì</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T4.9.9.9.8><span class=ltx_text id=S5.T4.9.9.9.8.1 style=font-size:90%>average</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.9.9.9.8.m1.1"><semantics id="S5.T4.9.9.9.8.m1.1a"><mo id="S5.T4.9.9.9.8.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T4.9.9.9.8.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T4.9.9.9.8.m1.1b"><ci id="S5.T4.9.9.9.8.m1.1.1.cmml" xref="S5.T4.9.9.9.8.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.9.9.9.8.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.9.9.9.8.m1.1d">‚Üì</annotation></semantics></math></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T4.9.9.10.1><td class="ltx_td ltx_align_left ltx_border_tt" id=S5.T4.9.9.10.1.1><span class="ltx_text ltx_font_italic" id=S5.T4.9.9.10.1.1.1 style=font-size:90%>No Fusion</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.9.9.10.1.2><span class=ltx_text id=S5.T4.9.9.10.1.2.1 style=font-size:90%>3.84</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.9.9.10.1.3><span class=ltx_text id=S5.T4.9.9.10.1.3.1 style=font-size:90%>6.52</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.9.9.10.1.4><span class=ltx_text id=S5.T4.9.9.10.1.4.1 style=font-size:90%>9.30</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.9.9.10.1.5><span class=ltx_text id=S5.T4.9.9.10.1.5.1 style=font-size:90%>6.55</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.9.9.10.1.6><span class=ltx_text id=S5.T4.9.9.10.1.6.1 style=font-size:90%>1.31</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.9.9.10.1.7><span class=ltx_text id=S5.T4.9.9.10.1.7.1 style=font-size:90%>4.76</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.9.9.10.1.8><span class=ltx_text id=S5.T4.9.9.10.1.8.1 style=font-size:90%>7.63</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.9.9.10.1.9><span class=ltx_text id=S5.T4.9.9.10.1.9.1 style=font-size:90%>4.57</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T4.9.9.10.1.10><span class="ltx_text ltx_font_bold" id=S5.T4.9.9.10.1.10.1 style=font-size:90%>0</span></td></tr><tr class=ltx_tr id=S5.T4.9.9.11.2><td class="ltx_td ltx_align_left" id=S5.T4.9.9.11.2.1><span class="ltx_text ltx_font_italic" id=S5.T4.9.9.11.2.1.1 style=font-size:90%>Early Fusion</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.11.2.2><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.9.9.11.2.2.1 style=font-size:90%>3.68</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.11.2.3><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.9.9.11.2.3.1 style=font-size:90%>6.19</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.11.2.4><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.9.9.11.2.4.1 style=font-size:90%>8.74</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.11.2.5><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.9.9.11.2.5.1 style=font-size:90%>6.20</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.11.2.6><span class=ltx_text id=S5.T4.9.9.11.2.6.1 style=font-size:90%>0.96</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.11.2.7><span class=ltx_text id=S5.T4.9.9.11.2.7.1 style=font-size:90%>3.86</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.11.2.8><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.9.9.11.2.8.1 style=font-size:90%>5.83</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.11.2.9><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.9.9.11.2.9.1 style=font-size:90%>3.55</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.11.2.10><span class=ltx_text id=S5.T4.9.9.11.2.10.1 style=font-size:90%>0.96</span></td></tr><tr class=ltx_tr id=S5.T4.9.9.12.3><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T4.9.9.12.3.1><span class="ltx_text ltx_font_italic" id=S5.T4.9.9.12.3.1.1 style=font-size:70%>Intermediate Fusion</span></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.12.3.2></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.12.3.3></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.12.3.4></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.12.3.5></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.12.3.6></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.12.3.7></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.12.3.8></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.12.3.9></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.12.3.10></td></tr><tr class=ltx_tr id=S5.T4.9.9.13.4><td class="ltx_td ltx_align_left" id=S5.T4.9.9.13.4.1><span class=ltx_text id=S5.T4.9.9.13.4.1.1 style=font-size:90%>AttFuse¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class=ltx_text id=S5.T4.9.9.13.4.1.2.1 style=font-size:90%>[</span><a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib52 title><span class=ltx_text style=font-size:90%>52</span></a><span class=ltx_text id=S5.T4.9.9.13.4.1.3.2 style=font-size:90%>]</span></cite></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.13.4.2><span class=ltx_text id=S5.T4.9.9.13.4.2.1 style=font-size:90%>4.06</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.13.4.3><span class=ltx_text id=S5.T4.9.9.13.4.3.1 style=font-size:90%>6.78</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.13.4.4><span class=ltx_text id=S5.T4.9.9.13.4.4.1 style=font-size:90%>9.64</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.13.4.5><span class=ltx_text id=S5.T4.9.9.13.4.5.1 style=font-size:90%>6.83</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.13.4.6><span class=ltx_text id=S5.T4.9.9.13.4.6.1 style=font-size:90%>1.42</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.13.4.7><span class=ltx_text id=S5.T4.9.9.13.4.7.1 style=font-size:90%>4.41</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.13.4.8><span class=ltx_text id=S5.T4.9.9.13.4.8.1 style=font-size:90%>6.53</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.13.4.9><span class=ltx_text id=S5.T4.9.9.13.4.9.1 style=font-size:90%>4.12</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.13.4.10><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.9.9.13.4.10.1 style=font-size:90%>0.20</span></td></tr><tr class=ltx_tr id=S5.T4.9.9.14.5><td class="ltx_td ltx_align_left" id=S5.T4.9.9.14.5.1><span class=ltx_text id=S5.T4.9.9.14.5.1.1 style=font-size:90%>V2X-ViT¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class=ltx_text id=S5.T4.9.9.14.5.1.2.1 style=font-size:90%>[</span><a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib51 title><span class=ltx_text style=font-size:90%>51</span></a><span class=ltx_text id=S5.T4.9.9.14.5.1.3.2 style=font-size:90%>]</span></cite></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.14.5.2><span class=ltx_text id=S5.T4.9.9.14.5.2.1 style=font-size:90%>4.21</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.14.5.3><span class=ltx_text id=S5.T4.9.9.14.5.3.1 style=font-size:90%>7.05</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.14.5.4><span class=ltx_text id=S5.T4.9.9.14.5.4.1 style=font-size:90%>9.99</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.14.5.5><span class=ltx_text id=S5.T4.9.9.14.5.5.1 style=font-size:90%>7.08</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.14.5.6><span class=ltx_text id=S5.T4.9.9.14.5.6.1 style=font-size:90%>1.33</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.14.5.7><span class=ltx_text id=S5.T4.9.9.14.5.7.1 style=font-size:90%>4.82</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.14.5.8><span class=ltx_text id=S5.T4.9.9.14.5.8.1 style=font-size:90%>6.85</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.14.5.9><span class=ltx_text id=S5.T4.9.9.14.5.9.1 style=font-size:90%>4.33</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.14.5.10><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.9.9.14.5.10.1 style=font-size:90%>0.20</span></td></tr><tr class=ltx_tr id=S5.T4.9.9.15.6><td class="ltx_td ltx_align_left" id=S5.T4.9.9.15.6.1><span class=ltx_text id=S5.T4.9.9.15.6.1.1 style=font-size:90%>CoBEVT¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class=ltx_text id=S5.T4.9.9.15.6.1.2.1 style=font-size:90%>[</span><a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib50 title><span class=ltx_text style=font-size:90%>50</span></a><span class=ltx_text id=S5.T4.9.9.15.6.1.3.2 style=font-size:90%>]</span></cite></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.15.6.2><span class=ltx_text id=S5.T4.9.9.15.6.2.1 style=font-size:90%>3.97</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.15.6.3><span class=ltx_text id=S5.T4.9.9.15.6.3.1 style=font-size:90%>6.71</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.15.6.4><span class=ltx_text id=S5.T4.9.9.15.6.4.1 style=font-size:90%>9.47</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.15.6.5><span class=ltx_text id=S5.T4.9.9.15.6.5.1 style=font-size:90%>6.72</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.15.6.6><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.9.9.15.6.6.1 style=font-size:90%>0.93</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.15.6.7><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.9.9.15.6.7.1 style=font-size:90%>3.74</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.15.6.8><span class=ltx_text id=S5.T4.9.9.15.6.8.1 style=font-size:90%>6.96</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.15.6.9><span class=ltx_text id=S5.T4.9.9.15.6.9.1 style=font-size:90%>3.88</span></td><td class="ltx_td ltx_align_center" id=S5.T4.9.9.15.6.10><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T4.9.9.15.6.10.1 style=font-size:90%>0.20</span></td></tr><tr class=ltx_tr id=S5.T4.9.9.16.7><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T4.9.9.16.7.1><span class="ltx_text ltx_font_italic" id=S5.T4.9.9.16.7.1.1 style=font-size:70%>LLM Fusion</span></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.16.7.2></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.16.7.3></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.16.7.4></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.16.7.5></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.16.7.6></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.16.7.7></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.16.7.8></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.16.7.9></td><td class="ltx_td ltx_border_t" id=S5.T4.9.9.16.7.10></td></tr><tr class=ltx_tr id=S5.T4.9.9.17.8><td class="ltx_td ltx_align_left ltx_border_b" id=S5.T4.9.9.17.8.1><span class=ltx_text id=S5.T4.9.9.17.8.1.1 style=font-size:90%>V2V-LLM¬†(ours)</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T4.9.9.17.8.2><span class="ltx_text ltx_font_bold" id=S5.T4.9.9.17.8.2.1 style=font-size:90%>2.96</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T4.9.9.17.8.3><span class="ltx_text ltx_font_bold" id=S5.T4.9.9.17.8.3.1 style=font-size:90%>4.97</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T4.9.9.17.8.4><span class="ltx_text ltx_font_bold" id=S5.T4.9.9.17.8.4.1 style=font-size:90%>7.05</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T4.9.9.17.8.5><span class="ltx_text ltx_font_bold" id=S5.T4.9.9.17.8.5.1 style=font-size:90%>4.99</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T4.9.9.17.8.6><span class="ltx_text ltx_font_bold" id=S5.T4.9.9.17.8.6.1 style=font-size:90%>0.55</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T4.9.9.17.8.7><span class="ltx_text ltx_font_bold" id=S5.T4.9.9.17.8.7.1 style=font-size:90%>3.19</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T4.9.9.17.8.8><span class="ltx_text ltx_font_bold" id=S5.T4.9.9.17.8.8.1 style=font-size:90%>5.25</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T4.9.9.17.8.9><span class="ltx_text ltx_font_bold" id=S5.T4.9.9.17.8.9.1 style=font-size:90%>3.00</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T4.9.9.17.8.10><span class=ltx_text id=S5.T4.9.9.17.8.10.1 style=font-size:90%>0.203</span></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 4 presents a comparison of the planning performance of V2V-LLM against several baseline methods using the V2V-QA testing dataset. The evaluation metrics used are the average L2 distance error (in meters) between planned and actual trajectories, and the average collision rate (percentage of instances where collisions occur). For better readability, the best results for each metric are highlighted in bold, while the second-best results are underlined. This allows for a direct comparison of V2V-LLM&rsquo;s planning capabilities against methods utilizing different fusion approaches (no fusion, early fusion, and intermediate fusion).</p><details><summary>read the caption</summary>Table 4: V2V-LLM‚Äôs planning performance in V2V-QA‚Äôs testing split in comparison with baseline methods. L2: L2 distance error. CR: Collision Rate. In each column, the best results are in boldface. and the second-best results are in underline.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S5.T5.18.18><thead class=ltx_thead><tr class=ltx_tr id=S5.T5.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=S5.T5.1.1.1.2 rowspan=2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.1.1.1.2.1 style=font-size:90%>Method</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan=3 id=S5.T5.1.1.1.3 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.1.1.1.3.1 style=font-size:90%>Q1</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan=3 id=S5.T5.1.1.1.4 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.1.1.1.4.1 style=font-size:90%>Q2</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan=3 id=S5.T5.1.1.1.5 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.1.1.1.5.1 style=font-size:90%>Q3</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T5.1.1.1.6 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.1.1.1.6.1 style=font-size:90%>Q</span><sub class=ltx_sub id=S5.T5.1.1.1.6.2><span class=ltx_text id=S5.T5.1.1.1.6.2.1 style=font-size:90%>Gr</span></sub></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan=3 id=S5.T5.1.1.1.7 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.1.1.1.7.1 style=font-size:90%>Q4</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan=2 id=S5.T5.1.1.1.8 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.1.1.1.8.1 style=font-size:90%>Q5</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T5.1.1.1.1 rowspan=2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.1.1.1.1.1 style=font-size:90%>Comm (MB)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T5.1.1.1.1.1.m1.1"><semantics id="S5.T5.1.1.1.1.1.m1.1a"><mo id="S5.T5.1.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T5.1.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.1.m1.1b"><ci id="S5.T5.1.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.1.1.1.1.1.m1.1d">‚Üì</annotation></semantics></math></span></th></tr><tr class=ltx_tr id=S5.T5.18.18.18><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T5.2.2.2.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.2.2.2.1.1 style=font-size:90%>F1</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.2.2.2.1.m1.1"><semantics id="S5.T5.2.2.2.1.m1.1a"><mo id="S5.T5.2.2.2.1.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T5.2.2.2.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.1.m1.1b"><ci id="S5.T5.2.2.2.1.m1.1.1.cmml" xref="S5.T5.2.2.2.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.2.2.2.1.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T5.3.3.3.2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.3.3.3.2.1 style=font-size:90%>P</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.3.3.3.2.m1.1"><semantics id="S5.T5.3.3.3.2.m1.1a"><mo id="S5.T5.3.3.3.2.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T5.3.3.3.2.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.3.2.m1.1b"><ci id="S5.T5.3.3.3.2.m1.1.1.cmml" xref="S5.T5.3.3.3.2.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.3.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.3.3.3.2.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T5.4.4.4.3 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.4.4.4.3.1 style=font-size:90%>R</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.4.4.4.3.m1.1"><semantics id="S5.T5.4.4.4.3.m1.1a"><mo id="S5.T5.4.4.4.3.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T5.4.4.4.3.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T5.4.4.4.3.m1.1b"><ci id="S5.T5.4.4.4.3.m1.1.1.cmml" xref="S5.T5.4.4.4.3.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.4.4.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.4.4.4.3.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T5.5.5.5.4 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.5.5.5.4.1 style=font-size:90%>F1</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.5.5.5.4.m1.1"><semantics id="S5.T5.5.5.5.4.m1.1a"><mo id="S5.T5.5.5.5.4.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T5.5.5.5.4.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T5.5.5.5.4.m1.1b"><ci id="S5.T5.5.5.5.4.m1.1.1.cmml" xref="S5.T5.5.5.5.4.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.5.5.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.5.5.5.4.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T5.6.6.6.5 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.6.6.6.5.1 style=font-size:90%>P</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.6.6.6.5.m1.1"><semantics id="S5.T5.6.6.6.5.m1.1a"><mo id="S5.T5.6.6.6.5.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T5.6.6.6.5.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T5.6.6.6.5.m1.1b"><ci id="S5.T5.6.6.6.5.m1.1.1.cmml" xref="S5.T5.6.6.6.5.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.6.6.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.6.6.6.5.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T5.7.7.7.6 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.7.7.7.6.1 style=font-size:90%>R</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.7.7.7.6.m1.1"><semantics id="S5.T5.7.7.7.6.m1.1a"><mo id="S5.T5.7.7.7.6.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T5.7.7.7.6.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T5.7.7.7.6.m1.1b"><ci id="S5.T5.7.7.7.6.m1.1.1.cmml" xref="S5.T5.7.7.7.6.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.7.7.7.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.7.7.7.6.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T5.8.8.8.7 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.8.8.8.7.1 style=font-size:90%>F1</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.8.8.8.7.m1.1"><semantics id="S5.T5.8.8.8.7.m1.1a"><mo id="S5.T5.8.8.8.7.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T5.8.8.8.7.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T5.8.8.8.7.m1.1b"><ci id="S5.T5.8.8.8.7.m1.1.1.cmml" xref="S5.T5.8.8.8.7.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.8.8.8.7.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.8.8.8.7.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T5.9.9.9.8 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.9.9.9.8.1 style=font-size:90%>P</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.9.9.9.8.m1.1"><semantics id="S5.T5.9.9.9.8.m1.1a"><mo id="S5.T5.9.9.9.8.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T5.9.9.9.8.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T5.9.9.9.8.m1.1b"><ci id="S5.T5.9.9.9.8.m1.1.1.cmml" xref="S5.T5.9.9.9.8.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.9.9.9.8.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.9.9.9.8.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T5.10.10.10.9 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.10.10.10.9.1 style=font-size:90%>R</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.10.10.10.9.m1.1"><semantics id="S5.T5.10.10.10.9.m1.1a"><mo id="S5.T5.10.10.10.9.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T5.10.10.10.9.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T5.10.10.10.9.m1.1b"><ci id="S5.T5.10.10.10.9.m1.1.1.cmml" xref="S5.T5.10.10.10.9.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.10.10.10.9.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.10.10.10.9.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T5.11.11.11.10 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.11.11.11.10.1 style=font-size:90%>F1</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.11.11.11.10.m1.1"><semantics id="S5.T5.11.11.11.10.m1.1a"><mo id="S5.T5.11.11.11.10.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T5.11.11.11.10.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T5.11.11.11.10.m1.1b"><ci id="S5.T5.11.11.11.10.m1.1.1.cmml" xref="S5.T5.11.11.11.10.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.11.11.11.10.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.11.11.11.10.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T5.12.12.12.11 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.12.12.12.11.1 style=font-size:90%>F1</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.12.12.12.11.m1.1"><semantics id="S5.T5.12.12.12.11.m1.1a"><mo id="S5.T5.12.12.12.11.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T5.12.12.12.11.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T5.12.12.12.11.m1.1b"><ci id="S5.T5.12.12.12.11.m1.1.1.cmml" xref="S5.T5.12.12.12.11.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.12.12.12.11.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.12.12.12.11.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T5.13.13.13.12 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.13.13.13.12.1 style=font-size:90%>P</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.13.13.13.12.m1.1"><semantics id="S5.T5.13.13.13.12.m1.1a"><mo id="S5.T5.13.13.13.12.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T5.13.13.13.12.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T5.13.13.13.12.m1.1b"><ci id="S5.T5.13.13.13.12.m1.1.1.cmml" xref="S5.T5.13.13.13.12.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.13.13.13.12.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.13.13.13.12.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T5.14.14.14.13 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.14.14.14.13.1 style=font-size:90%>R</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.14.14.14.13.m1.1"><semantics id="S5.T5.14.14.14.13.m1.1a"><mo id="S5.T5.14.14.14.13.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T5.14.14.14.13.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T5.14.14.14.13.m1.1b"><ci id="S5.T5.14.14.14.13.m1.1.1.cmml" xref="S5.T5.14.14.14.13.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.14.14.14.13.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.14.14.14.13.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T5.16.16.16.15 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.16.16.16.15.1 style=font-size:90%>L2</span><sub class=ltx_sub id=S5.T5.16.16.16.15.2><span class="ltx_text ltx_font_italic" id=S5.T5.16.16.16.15.2.1 style=font-size:90%>avg</span></sub><span class=ltx_text id=S5.T5.16.16.16.15.3 style=font-size:90%> (m)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T5.16.16.16.15.m2.1"><semantics id="S5.T5.16.16.16.15.m2.1a"><mo id="S5.T5.16.16.16.15.m2.1.1" mathsize="90%" stretchy="false" xref="S5.T5.16.16.16.15.m2.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T5.16.16.16.15.m2.1b"><ci id="S5.T5.16.16.16.15.m2.1.1.cmml" xref="S5.T5.16.16.16.15.m2.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.16.16.16.15.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.16.16.16.15.m2.1d">‚Üì</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T5.18.18.18.17 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.18.17.1 style=font-size:90%>CR</span><sub class=ltx_sub id=S5.T5.18.18.18.17.2><span class="ltx_text ltx_font_italic" id=S5.T5.18.18.18.17.2.1 style=font-size:90%>avg</span></sub><span class=ltx_text id=S5.T5.18.18.18.17.3 style=font-size:90%> (%)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T5.18.18.18.17.m2.1"><semantics id="S5.T5.18.18.18.17.m2.1a"><mo id="S5.T5.18.18.18.17.m2.1.1" mathsize="90%" stretchy="false" xref="S5.T5.18.18.18.17.m2.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T5.18.18.18.17.m2.1b"><ci id="S5.T5.18.18.18.17.m2.1.1.cmml" xref="S5.T5.18.18.18.17.m2.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.18.18.18.17.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.18.18.18.17.m2.1d">‚Üì</annotation></semantics></math></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T5.18.18.19.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=S5.T5.18.18.19.1.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.19.1.1.1 style=font-size:90%>Scene-level only</span></th><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.19.1.2.1 style=font-size:90%>69.9</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.3 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.19.1.3.1 style=font-size:90%>74.9</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.4 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.19.1.4.1 style=font-size:90%>65.5</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.5 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.19.1.5.1 style=font-size:90%>15.4</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.6 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.19.1.6.1 style=font-size:90%>19.9</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.7 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.19.1.7.1 style=font-size:90%>12.6</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.8 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.19.1.8.1 style=font-size:90%>17.9</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.9 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.19.1.9.1 style=font-size:90%>26.9</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.10 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.19.1.10.1 style=font-size:90%>13.5</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.11 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.19.1.11.1 style=font-size:90%>34.4</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.12 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.19.1.12.1 style=font-size:90%>43.2</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.13 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.19.1.13.1 style=font-size:90%>40.2</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.14 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.19.1.14.1 style=font-size:90%>46.7</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.15 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.19.1.15.1 style=font-size:90%>7.21</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.16 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.19.1.16.1 style=font-size:90%>15.55</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T5.18.18.19.1.17 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.19.1.17.1 style=font-size:90%>0.20</span></td></tr><tr class=ltx_tr id=S5.T5.18.18.20.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S5.T5.18.18.20.2.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.20.2.1.1 style=font-size:90%>Object-level only</span></th><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.2 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.20.2.2.1 style=font-size:90%>69.0</span></td><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.3 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.20.2.3.1 style=font-size:90%>80.9</span></td><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.4 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.20.2.4.1 style=font-size:90%>60.1</span></td><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.5 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.20.2.5.1 style=font-size:90%>26.9</span></td><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.6 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.20.2.6.1 style=font-size:90%>34.7</span></td><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.7 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.20.2.7.1 style=font-size:90%>21.9</span></td><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.8 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.20.2.8.1 style=font-size:90%>17.6</span></td><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.9 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.20.2.9.1 style=font-size:90%>18.3</span></td><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.10 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.20.2.10.1 style=font-size:90%>16.9</span></td><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.11 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.20.2.11.1 style=font-size:90%>37.8</span></td><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.12 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.20.2.12.1 style=font-size:90%>52.6</span></td><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.13 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.20.2.13.1 style=font-size:90%>57.3</span></td><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.14 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.20.2.14.1 style=font-size:90%>48.6</span></td><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.15 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.20.2.15.1 style=font-size:90%>5.24</span></td><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.16 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.20.2.16.1 style=font-size:90%>7.78</span></td><td class="ltx_td ltx_align_center" id=S5.T5.18.18.20.2.17 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.20.2.17.1 style=font-size:90%>0.003</span></td></tr><tr class=ltx_tr id=S5.T5.18.18.21.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.1 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.21.3.1.1 style=font-size:90%>V2V-LLM¬†(ours)</span></th><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.2 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.21.3.2.1 style=font-size:90%>70.0</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.3 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.21.3.3.1 style=font-size:90%>80.1</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.4 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.21.3.4.1 style=font-size:90%>62.2</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.5 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.21.3.5.1 style=font-size:90%>30.8</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.6 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.21.3.6.1 style=font-size:90%>36.3</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.7 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.21.3.7.1 style=font-size:90%>26.7</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.8 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.21.3.8.1 style=font-size:90%>21.2</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.9 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.21.3.9.1 style=font-size:90%>21.5</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.10 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.21.3.10.1 style=font-size:90%>20.8</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.11 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.21.3.11.1 style=font-size:90%>40.7</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.12 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.21.3.12.1 style=font-size:90%>59.7</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.13 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.21.3.13.1 style=font-size:90%>61.9</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.14 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.21.3.14.1 style=font-size:90%>57.6</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.15 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.21.3.15.1 style=font-size:90%>4.99</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.16 style=padding-left:2pt;padding-right:2pt><span class="ltx_text ltx_font_bold" id=S5.T5.18.18.21.3.16.1 style=font-size:90%>3.00</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=S5.T5.18.18.21.3.17 style=padding-left:2pt;padding-right:2pt><span class=ltx_text id=S5.T5.18.18.21.3.17.1 style=font-size:90%>0.203</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of an ablation study conducted on the V2V-QA dataset&rsquo;s testing split. The study evaluates the performance of the V2V-LLM model under various configurations. Specifically, it examines the impact of using only scene-level features or only object-level features, as opposed to using both, on the model&rsquo;s ability to perform different tasks. These tasks include grounding (Q1-Q3, an average of these three is QGr), notable object identification (Q4), and planning (Q5). The evaluation metrics used are F1 score, precision (P), recall (R), L2 distance error (L2), collision rate (CR), and communication cost (Comm). The results provide insights into the relative contributions of scene and object features to the overall performance of the V2V-LLM model across different tasks.</p><details><summary>read the caption</summary>Table 5: Ablation study in V2V-QA‚Äôs testing split. Q1: Grounding at a reference location. Q2: Grounding behind a reference object at a location. Q3: Grounding behind a reference object in a direction. QGr: Average of grounding (Q1, Q2, and Q3). Q4: Notable object identification. Q5: Planning. P: Precision. R: Recall. L2: L2 distance error. CR: Collision Rate. Comm: Communication cost.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S8.T6.4.4><thead class=ltx_thead><tr class=ltx_tr id=S8.T6.4.4.5.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=S8.T6.4.4.5.1.1 rowspan=2 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.5.1.1.1 style=font-size:90%>Method</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan=2 id=S8.T6.4.4.5.1.2 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.5.1.2.1 style=font-size:90%>1 input frame</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan=2 id=S8.T6.4.4.5.1.3 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.5.1.3.1 style=font-size:90%>3 input frames</span></th></tr><tr class=ltx_tr id=S8.T6.4.4.4><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S8.T6.1.1.1.1 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.1.1.1.1.1 style=font-size:90%>L2 (m)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S8.T6.1.1.1.1.m1.1"><semantics id="S8.T6.1.1.1.1.m1.1a"><mo id="S8.T6.1.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S8.T6.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S8.T6.1.1.1.1.m1.1b"><ci id="S8.T6.1.1.1.1.m1.1.1.cmml" xref="S8.T6.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.T6.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S8.T6.1.1.1.1.m1.1d">‚Üì</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S8.T6.2.2.2.2 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.2.2.2.2.1 style=font-size:90%>CR (%)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S8.T6.2.2.2.2.m1.1"><semantics id="S8.T6.2.2.2.2.m1.1a"><mo id="S8.T6.2.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="S8.T6.2.2.2.2.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S8.T6.2.2.2.2.m1.1b"><ci id="S8.T6.2.2.2.2.m1.1.1.cmml" xref="S8.T6.2.2.2.2.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.T6.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S8.T6.2.2.2.2.m1.1d">‚Üì</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S8.T6.3.3.3.3 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.3.3.3.3.1 style=font-size:90%>L2 (m)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S8.T6.3.3.3.3.m1.1"><semantics id="S8.T6.3.3.3.3.m1.1a"><mo id="S8.T6.3.3.3.3.m1.1.1" mathsize="90%" stretchy="false" xref="S8.T6.3.3.3.3.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S8.T6.3.3.3.3.m1.1b"><ci id="S8.T6.3.3.3.3.m1.1.1.cmml" xref="S8.T6.3.3.3.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.T6.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S8.T6.3.3.3.3.m1.1d">‚Üì</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S8.T6.4.4.4.4 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.4.4.1 style=font-size:90%>CR (%)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S8.T6.4.4.4.4.m1.1"><semantics id="S8.T6.4.4.4.4.m1.1a"><mo id="S8.T6.4.4.4.4.m1.1.1" mathsize="90%" stretchy="false" xref="S8.T6.4.4.4.4.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S8.T6.4.4.4.4.m1.1b"><ci id="S8.T6.4.4.4.4.m1.1.1.cmml" xref="S8.T6.4.4.4.4.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.T6.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S8.T6.4.4.4.4.m1.1d">‚Üì</annotation></semantics></math></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S8.T6.4.4.6.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=S8.T6.4.4.6.1.1 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_italic" id=S8.T6.4.4.6.1.1.1 style=font-size:90%>No Fusion</span></th><td class="ltx_td ltx_align_center ltx_border_tt" id=S8.T6.4.4.6.1.2 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.6.1.2.1 style=font-size:90%>6.55</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S8.T6.4.4.6.1.3 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.6.1.3.1 style=font-size:90%>4.57</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S8.T6.4.4.6.1.4 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.6.1.4.1 style=font-size:90%>5.94</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S8.T6.4.4.6.1.5 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.6.1.5.1 style=font-size:90%>3.77</span></td></tr><tr class=ltx_tr id=S8.T6.4.4.7.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S8.T6.4.4.7.2.1 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_italic" id=S8.T6.4.4.7.2.1.1 style=font-size:90%>Early Fusion</span></th><td class="ltx_td ltx_align_center" id=S8.T6.4.4.7.2.2 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S8.T6.4.4.7.2.2.1 style=font-size:90%>6.20</span></td><td class="ltx_td ltx_align_center" id=S8.T6.4.4.7.2.3 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S8.T6.4.4.7.2.3.1 style=font-size:90%>3.55</span></td><td class="ltx_td ltx_align_center" id=S8.T6.4.4.7.2.4 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S8.T6.4.4.7.2.4.1 style=font-size:90%>5.13</span></td><td class="ltx_td ltx_align_center" id=S8.T6.4.4.7.2.5 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S8.T6.4.4.7.2.5.1 style=font-size:90%>3.04</span></td></tr><tr class=ltx_tr id=S8.T6.4.4.8.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S8.T6.4.4.8.3.1 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_italic" id=S8.T6.4.4.8.3.1.1 style=font-size:70%>Intermediate Fusion</span></th><td class="ltx_td ltx_border_t" id=S8.T6.4.4.8.3.2 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S8.T6.4.4.8.3.3 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S8.T6.4.4.8.3.4 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S8.T6.4.4.8.3.5 style=padding-left:4pt;padding-right:4pt></td></tr><tr class=ltx_tr id=S8.T6.4.4.9.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S8.T6.4.4.9.4.1 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.9.4.1.1 style=font-size:90%>AttFuse¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class=ltx_text id=S8.T6.4.4.9.4.1.2.1 style=font-size:90%>[</span><a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib52 title><span class=ltx_text style=font-size:90%>52</span></a><span class=ltx_text id=S8.T6.4.4.9.4.1.3.2 style=font-size:90%>]</span></cite></th><td class="ltx_td ltx_align_center" id=S8.T6.4.4.9.4.2 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.9.4.2.1 style=font-size:90%>6.83</span></td><td class="ltx_td ltx_align_center" id=S8.T6.4.4.9.4.3 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.9.4.3.1 style=font-size:90%>4.12</span></td><td class="ltx_td ltx_align_center" id=S8.T6.4.4.9.4.4 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.9.4.4.1 style=font-size:90%>6.46</span></td><td class="ltx_td ltx_align_center" id=S8.T6.4.4.9.4.5 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.9.4.5.1 style=font-size:90%>3.50</span></td></tr><tr class=ltx_tr id=S8.T6.4.4.10.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S8.T6.4.4.10.5.1 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.10.5.1.1 style=font-size:90%>V2X-ViT¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class=ltx_text id=S8.T6.4.4.10.5.1.2.1 style=font-size:90%>[</span><a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib51 title><span class=ltx_text style=font-size:90%>51</span></a><span class=ltx_text id=S8.T6.4.4.10.5.1.3.2 style=font-size:90%>]</span></cite></th><td class="ltx_td ltx_align_center" id=S8.T6.4.4.10.5.2 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.10.5.2.1 style=font-size:90%>7.08</span></td><td class="ltx_td ltx_align_center" id=S8.T6.4.4.10.5.3 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.10.5.3.1 style=font-size:90%>4.33</span></td><td class="ltx_td ltx_align_center" id=S8.T6.4.4.10.5.4 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.10.5.4.1 style=font-size:90%>5.52</span></td><td class="ltx_td ltx_align_center" id=S8.T6.4.4.10.5.5 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.10.5.5.1 style=font-size:90%>3.84</span></td></tr><tr class=ltx_tr id=S8.T6.4.4.11.6><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S8.T6.4.4.11.6.1 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.11.6.1.1 style=font-size:90%>CoBEVT¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class=ltx_text id=S8.T6.4.4.11.6.1.2.1 style=font-size:90%>[</span><a class=ltx_ref href=https://arxiv.org/html/2502.09980v1#bib.bib50 title><span class=ltx_text style=font-size:90%>50</span></a><span class=ltx_text id=S8.T6.4.4.11.6.1.3.2 style=font-size:90%>]</span></cite></th><td class="ltx_td ltx_align_center" id=S8.T6.4.4.11.6.2 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.11.6.2.1 style=font-size:90%>6.72</span></td><td class="ltx_td ltx_align_center" id=S8.T6.4.4.11.6.3 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.11.6.3.1 style=font-size:90%>3.88</span></td><td class="ltx_td ltx_align_center" id=S8.T6.4.4.11.6.4 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.11.6.4.1 style=font-size:90%>6.02</span></td><td class="ltx_td ltx_align_center" id=S8.T6.4.4.11.6.5 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.11.6.5.1 style=font-size:90%>3.40</span></td></tr><tr class=ltx_tr id=S8.T6.4.4.12.7><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S8.T6.4.4.12.7.1 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_italic" id=S8.T6.4.4.12.7.1.1 style=font-size:70%>LLM Fusion</span></th><td class="ltx_td ltx_border_t" id=S8.T6.4.4.12.7.2 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S8.T6.4.4.12.7.3 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S8.T6.4.4.12.7.4 style=padding-left:4pt;padding-right:4pt></td><td class="ltx_td ltx_border_t" id=S8.T6.4.4.12.7.5 style=padding-left:4pt;padding-right:4pt></td></tr><tr class=ltx_tr id=S8.T6.4.4.13.8><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id=S8.T6.4.4.13.8.1 style=padding-left:4pt;padding-right:4pt><span class=ltx_text id=S8.T6.4.4.13.8.1.1 style=font-size:90%>V2V-LLM¬†(ours)</span></th><td class="ltx_td ltx_align_center ltx_border_b" id=S8.T6.4.4.13.8.2 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_bold" id=S8.T6.4.4.13.8.2.1 style=font-size:90%>4.99</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S8.T6.4.4.13.8.3 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_bold" id=S8.T6.4.4.13.8.3.1 style=font-size:90%>3.00</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S8.T6.4.4.13.8.4 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_bold" id=S8.T6.4.4.13.8.4.1 style=font-size:90%>4.82</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S8.T6.4.4.13.8.5 style=padding-left:4pt;padding-right:4pt><span class="ltx_text ltx_font_bold" id=S8.T6.4.4.13.8.5.1 style=font-size:90%>2.93</span></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 6 presents a comparison of the planning performance of the proposed V2V-LLM model against several baseline methods on the V2V-QA testing dataset. The performance is evaluated using two metrics: the average L2 distance error (L2) and the average collision rate (CR). Lower L2 error indicates better trajectory planning accuracy, and a lower collision rate signifies improved safety. The table highlights the best-performing model for each metric in boldface and the second-best performing model with an underline, indicating V2V-LLM&rsquo;s relative strengths and weaknesses in this specific task.</p><details><summary>read the caption</summary>Table 6: V2V-LLM‚Äôs planning performance in V2V-QA‚Äôs testing split in comparison with baseline methods. L2: L2 distance error. CR: Collision Rate. In each column, the best results are in boldface. and the second-best results are in underline.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S11.T7.2><thead class=ltx_thead><tr class=ltx_tr id=S11.T7.2.1.1><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id=S11.T7.2.1.1.1 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.1.1.1.1 style=font-size:90%>QA type</span></th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id=S11.T7.2.1.1.2 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.1.1.2.1 style=font-size:90%>Train-Pos</span></th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" id=S11.T7.2.1.1.3 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.1.1.3.1 style=font-size:90%>Train-Neg</span></th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id=S11.T7.2.1.1.4 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.1.1.4.1 style=font-size:90%>Test-Pos</span></th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" id=S11.T7.2.1.1.5 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.1.1.5.1 style=font-size:90%>Test-Neg</span></th><th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id=S11.T7.2.1.1.6 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.1.1.6.1 style=font-size:90%>Total</span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S11.T7.2.2.1><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S11.T7.2.2.1.1 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.2.1.1.1 style=font-size:90%>Q1</span></td><td class="ltx_td ltx_align_right ltx_border_tt" id=S11.T7.2.2.1.2 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.2.1.2.1 style=font-size:90%>217403</span></td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id=S11.T7.2.2.1.3 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.2.1.3.1 style=font-size:90%>137417</span></td><td class="ltx_td ltx_align_right ltx_border_tt" id=S11.T7.2.2.1.4 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.2.1.4.1 style=font-size:90%>76522</span></td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id=S11.T7.2.2.1.5 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.2.1.5.1 style=font-size:90%>44861</span></td><td class="ltx_td ltx_align_right ltx_border_tt" id=S11.T7.2.2.1.6 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.2.1.6.1 style=font-size:90%>476203</span></td></tr><tr class=ltx_tr id=S11.T7.2.3.2><td class="ltx_td ltx_align_center ltx_border_r" id=S11.T7.2.3.2.1 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.3.2.1.1 style=font-size:90%>Q2</span></td><td class="ltx_td ltx_align_right" id=S11.T7.2.3.2.2 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.3.2.2.1 style=font-size:90%>17859</span></td><td class="ltx_td ltx_align_right ltx_border_r" id=S11.T7.2.3.2.3 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.3.2.3.1 style=font-size:90%>17841</span></td><td class="ltx_td ltx_align_right" id=S11.T7.2.3.2.4 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.3.2.4.1 style=font-size:90%>8391</span></td><td class="ltx_td ltx_align_right ltx_border_r" id=S11.T7.2.3.2.5 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.3.2.5.1 style=font-size:90%>5491</span></td><td class="ltx_td ltx_align_right" id=S11.T7.2.3.2.6 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.3.2.6.1 style=font-size:90%>49582</span></td></tr><tr class=ltx_tr id=S11.T7.2.4.3><td class="ltx_td ltx_align_center ltx_border_r" id=S11.T7.2.4.3.1 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.4.3.1.1 style=font-size:90%>Q3</span></td><td class="ltx_td ltx_align_right" id=S11.T7.2.4.3.2 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.4.3.2.1 style=font-size:90%>7197</span></td><td class="ltx_td ltx_align_right ltx_border_r" id=S11.T7.2.4.3.3 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.4.3.3.1 style=font-size:90%>7142</span></td><td class="ltx_td ltx_align_right" id=S11.T7.2.4.3.4 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.4.3.4.1 style=font-size:90%>3082</span></td><td class="ltx_td ltx_align_right ltx_border_r" id=S11.T7.2.4.3.5 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.4.3.5.1 style=font-size:90%>2015</span></td><td class="ltx_td ltx_align_right" id=S11.T7.2.4.3.6 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.4.3.6.1 style=font-size:90%>19436</span></td></tr><tr class=ltx_tr id=S11.T7.2.5.4><td class="ltx_td ltx_align_center ltx_border_r" id=S11.T7.2.5.4.1 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.5.4.1.1 style=font-size:90%>Q4</span></td><td class="ltx_td ltx_align_right" id=S11.T7.2.5.4.2 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.5.4.2.1 style=font-size:90%>9911</span></td><td class="ltx_td ltx_align_right ltx_border_r" id=S11.T7.2.5.4.3 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.5.4.3.1 style=font-size:90%>2379</span></td><td class="ltx_td ltx_align_right" id=S11.T7.2.5.4.4 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.5.4.4.1 style=font-size:90%>2517</span></td><td class="ltx_td ltx_align_right ltx_border_r" id=S11.T7.2.5.4.5 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.5.4.5.1 style=font-size:90%>929</span></td><td class="ltx_td ltx_align_right" id=S11.T7.2.5.4.6 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.5.4.6.1 style=font-size:90%>15736</span></td></tr><tr class=ltx_tr id=S11.T7.2.6.5><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S11.T7.2.6.5.1 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.6.5.1.1 style=font-size:90%>Total</span></td><td class="ltx_td ltx_align_right ltx_border_t" id=S11.T7.2.6.5.2 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.6.5.2.1 style=font-size:90%>252370</span></td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id=S11.T7.2.6.5.3 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.6.5.3.1 style=font-size:90%>164779</span></td><td class="ltx_td ltx_align_right ltx_border_t" id=S11.T7.2.6.5.4 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.6.5.4.1 style=font-size:90%>90512</span></td><td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id=S11.T7.2.6.5.5 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.6.5.5.1 style=font-size:90%>53296</span></td><td class="ltx_td ltx_align_right ltx_border_t" id=S11.T7.2.6.5.6 style=padding-left:3.5pt;padding-right:3.5pt><span class=ltx_text id=S11.T7.2.6.5.6.1 style=font-size:90%>560957</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a breakdown of the V2V-QA dataset, categorized by question type and the positive/negative class of each data point. The dataset is used to evaluate models on cooperative autonomous driving tasks. It shows the number of positive (at least one object satisfies the question&rsquo;s condition) and negative (no object satisfies the condition) examples in the training and testing sets for four question types: Q1 (Grounding at a reference location), Q2 (Grounding behind a reference object at a location), Q3 (Grounding behind a reference object in a direction), and Q4 (Notable object identification). This breakdown helps understand the class distribution within the dataset, informing the evaluation of model performance.</p><details><summary>read the caption</summary>Table 7: Dataset statistics of our V2V-QA‚Äôs positive and negative cases in the training and testing splits. Q1: Grounding at a reference location. Q2: Grounding behind a reference object at a location. Q3: Grounding behind a reference object in a direction. Q4: Notable object identification.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-92ae0a0fbdcb7bcdd4b92da25bff3606 class=gallery><img src=https://ai-paper-reviewer.com/2502.09980/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09980/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09980/&amp;title=V2V-LLM:%20Vehicle-to-Vehicle%20Cooperative%20Autonomous%20Driving%20with%20Multi-Modal%20Large%20Language%20Models" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09980/&amp;text=V2V-LLM:%20Vehicle-to-Vehicle%20Cooperative%20Autonomous%20Driving%20with%20Multi-Modal%20Large%20Language%20Models" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09980/&amp;subject=V2V-LLM:%20Vehicle-to-Vehicle%20Cooperative%20Autonomous%20Driving%20with%20Multi-Modal%20Large%20Language%20Models" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2502.09980/index.md",oid_likes="likes_paper-reviews/2502.09980/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2502.09056/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-13T00:00:00+00:00>13 February 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2502.10248/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-14T00:00:00+00:00>14 February 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>