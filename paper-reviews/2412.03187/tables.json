[{"content": "| Source LLMs | Percentage |\n|---|---| \n| Mistral-Large-Instruct-2407 (Jiang et al., 2023a) | 28.24% |\n| Gemma2-27B-IT (Riviere et al., 2024) | 15.45% |\n| Qwen2-72B-Instruct (Yang et al., 2024) | 12.38% |\n| LLaMA3-70B-Instruct (Dubey et al., 2024) | 9.92% |\n| Gemma2-9B-IT (Riviere et al., 2024) | 9.91% |\n| InternLM2.5-20B-Chat (Cai et al., 2024) | 7.54% |\n| DeepSeek-V2-Chat (DeepSeek-AI et al., 2024) | 6.20% |\n| DeepSeek-Coder-V2-Instruct (Shao et al., 2024) | 4.01% |\n| Yi-1.5-34B-Chat (Young et al., 2024) | 3.86% |\n| Phi-3-medium-4k-instruct (Abdin et al., 2024) | 2.50% |", "caption": "Table 1: Details of the source LLMs used in our experiments along with the percentage of the highest-scoring responses from each source LLM.", "description": "This table lists the ten open-source Large Language Models (LLMs) used as source models in the experiments.  For each LLM, its name, parameter size (in billions), and the percentage of times it produced the highest-scoring response (as determined by the reward model) across all source LLMs are provided. This shows the relative contribution of each LLM to the overall fusion process.", "section": "4.1 Experimental Setup"}, {"content": "| Model | Size | AlpacaEval-2 (GPT-4-1106-Preview) |  | Arena-Hard (GPT-4-1106-Preview) | MT-Bench (GPT-4-0125-Preview) |  |  |\n|---|---|---|---|---|---|---|---| \n|  |  | LC(%) | WR(%) | WR(%) | T1 | T2 | Overall |\n|---|---|---|---|---|---|---|---| \n| **Source&Target LLMs** |  |  |  |  |  |  |  |\n| Target | 8B | 26.0 | 25.3 | 20.6 | 7.41 | 7.04 | 7.23 |\n| Mistral-Large-Instruct-2407 | 123B | 54.3 | 46.8 | 70.4 | 8.83 | 8.31 | 8.57 |\n| Gemma2-27B-IT | 27B | 55.5 | 41.0 | 57.5 | 8.34 | 8.03 | 8.19 |\n| Qwen2-72B-Instruct | 72B | 38.1 | 29.9 | 46.9 | 8.44 | 7.84 | 8.15 |\n| LLaMA3-70B-Instruct | 70B | 34.4 | 33.2 | 46.6 | 8.61 | 7.77 | 8.19 |\n| Gemma2-9B-IT | 9B | 51.1 | 38.1 | 40.8 | 8.27 | 7.44 | 7.86 |\n| InternLM2.5-20B-Chat | 20B | 37.4 | 45.3 | 31.2 | 8.03 | 7.23 | 7.64 |\n| DeepSeek-V2-Chat | 236B | 51.4 | 51.3 | 68.3 | 8.65 | 7.96 | 8.31 |\n| DeepSeek-Coder-V2-Instruct | 236B | 50.7 | 54.0 | 66.3 | 8.80 | 7.42 | 8.13 |\n| Yi-1.5-34B-Chat | 34B | 37.5 | 44.5 | 42.6 | 7.99 | 7.64 | 7.81 |\n| Phi-3-Medium-4K-Instruct | 14B | 29.8 | 24.2 | 33.4 | 8.63 | 7.46 | 8.04 |\n| **Collective LLMs** |  |  |  |  |  |  |  |\n| PackLLM-Top1-PPL | 849B | 49.1 | 48.0 | 64.8 | 8.29 | 8.20 | 8.25 |\n| LLM-Blender-Top1 | 849B | 46.2 | 44.3 | 58.2 | 8.69 | 8.06 | 8.38 |\n| MOA | 849B | 61.3 | 77.2 | 83.1 | 9.04 | 8.03 | 8.54 |\n| Target-FuseLLM | 8B | 36.0 | 33.8 | 32.1 | 7.53 | 7.13 | 7.33 |\n| Target-FuseChat | 8B | 38.1 | 35.2 | 32.7 | 7.68 | 7.07 | 7.38 |\n| **Preference Optimization Methods** |  |  |  |  |  |  |  |\n| Target-DPO | 8B | 48.2 | 47.5 | 35.2 | 7.68 | 7.23 | 7.46 |\n| Target-SimPO | 8B | 53.7 | 47.5 | 36.5 | 7.73 | 7.00 | 7.38 |\n| Target-IPO | 8B | 46.8 | 42.4 | 36.6 | 7.89 | 7.19 | 7.54 |\n| **Our Methods** |  |  |  |  |  |  |  |\n| Target-SFT | 8B | 27.2 | 26.0 | 24.7 | 7.69 | 7.03 | 7.36 |\n| Target-SFT-DPO | 8B | 50.7 | 53.1 | 40.2 | 7.98 | 7.23 | 7.61 |\n| Target-SFT-WRPO-Medium | 8B | 53.5 | 53.8 | 41.6 | 7.80 | 7.03 | 7.42 |\n| Target-SFT-WRPO | 8B | **55.9** | **57.6** | **46.2** | 7.95 | **7.31** | **7.63** |", "caption": "Table 2: \nOverall results of our proposed WRPO method with LLaMA3-8B-Instruct as the target model, compared against various baseline categories on AlpacaEval-2, Arena-Hard, and MT-Bench. \u201cT1\u201d and \u201cT2\u201d represent the average scores for the first and second turns, respectively. Bold indicates the best performance in 8B models.", "description": "This table presents a comprehensive comparison of different Large Language Models (LLMs) on three benchmark datasets: AlpacaEval-2, Arena-Hard, and MT-Bench.  The models are evaluated based on their performance in instruction-following tasks.  The key model being evaluated is LLaMA3-8B-Instruct, which has been enhanced using the Weighted-Reward Preference Optimization (WRPO) method.  The table includes various baseline models for comparison, categorized as source & target LLMs (individual models and their performance), collective LLMs (ensemble methods), and preference optimization methods.  The results are presented as win rates (AlpacaEval-2 and Arena-Hard) and average scores (MT-Bench, across two turns of dialogue, T1 and T2).  The best performing 8B parameter model is highlighted in bold.", "section": "4 Experiments"}, {"content": "| Method | AlpacaEval-2 LC(%) | AlpacaEval-2 WR(%) | MT-Bench Overall | \n|---|---|---|---| \n| SimPO | 53.9 | 49.9 | 7.39 | \n| IPO | 51.1 | 52.4 | 7.67 | \n| WRPO<sub>SimPO</sub> | 55.8 | 51.8 | 7.42 | \n| WRPO<sub>IPO</sub> | 53.3 | 57.7 | 7.72 | ", "caption": "Table 3: Results of WRPO combined with different preference optimization objectives.", "description": "This table presents the results of applying the Weighted-Reward Preference Optimization (WRPO) method in combination with three different preference optimization objectives: Direct Preference Optimization (DPO), Inverse Preference Optimization (IPO), and Simple Preference Optimization (SimPO).  It shows how WRPO's performance varies when integrated with these different optimization strategies. The metrics used to evaluate performance are likely related to the instruction-following capabilities of the language model.", "section": "4 Experiments"}, {"content": "| Num | AlpacaEval-2 |  | MT-Bench |\n|---|---|---|---|\n| | LC(%) | WR(%) | Overall |\n| 1 | 48.9 | 50.3 | 7.29 |\n| 2 | 52.3 | 50.4 | 7.54 |\n| 5 | 53.5 | 53.8 | 7.42 |\n| 10 | 55.9 | 58.0 | 7.63 |", "caption": "Table 4: Results of our WRPO implemented with varying numbers of source LLMs on AlpacaEval-2 and MT-Bench.", "description": "This table presents the performance of the Weighted-Reward Preference Optimization (WRPO) model on two benchmark datasets, AlpacaEval-2 and MT-Bench, using different numbers of source Large Language Models (LLMs).  It demonstrates how the model's performance changes as the number of source LLMs increases, showing the impact of adding more diverse knowledge sources on the model's ability to generate high-quality responses.  The results are likely presented as metrics such as accuracy or win rate, indicating the effectiveness of the model under varying conditions.", "section": "4 EXPERIMENTS"}, {"content": "Method|Objective|Hyperparameter\n---|---|---\nDPO [2023]|\u2212log\u03c3(\u03b2log\u03c0\u03b8(yw|x)\u03c0ref(yw|x)\u2212\u03b2log\u03c0\u03b8(yl|x)\u03c0ref(yl|x))|\u03b2\u2208[0.01,0.05,0.1]\nIPO [2023]|(log\u03c0\u03b8(yw|x)\u03c0ref(yw|x)\u2212log\u03c0\u03b8(yl|x)\u03c0ref(yl|x)\u221212\u03c4)2|\u03c4\u2208[0.01,0.1,1.0]\nSimPO [2024]|\u2212log\u03c3(\u03b2|yw|log\u03c0\u03b8(yw|x)\u2212\u03b2|yl|log\u03c0\u03b8(yl|x)\u2212\u03b3)|\u03b2\u2208[5.0,10.0]\n| |\u03b3\u2208[0,1.0,2.0]\nWRPO_DPO|\u2212log\u03c3(\u03b1\u22c5\u03b2log\u03c0\u03b8(yws\u2223x)\u03c0ref(yws\u2223x)+(1\u2212\u03b1)\u22c5\u03b2log\u03c0\u03b8(jwt\u2223x)\u03c0ref(jwt\u2223x)\u2212\u03b2log\u03c0\u03b8(yl\u2223x)\u03c0ref(yl\u2223x))|\u03b2=0.01\n| |\u03b1\u2208[0.1,0.3,0.5,0.7,0.9]\nWRPO_SimPO|\u2212log\u03c3(\u03b1\u22c5\u03b2|yws|log\u03c0\u03b8(yws\u2223x)+(1\u2212\u03b1)\u22c5\u03b2|jwt\u2223log\u03c0\u03b8(jwt\u2223x)\u2212\u03b2|yl|log\u03c0\u03b8(yl\u2223x)\u2212\u03b3)|\u03b2=10.0,\u03b3=0\n| |\u03b1\u2208[0.1,0.3,0.5]\nWRPO_IPO|(\u03b1\u22c5log\u03c0\u03b8(yws\u2223x)\u03c0ref(yws\u2223x)+(1\u2212\u03b1)\u22c5log\u03c0\u03b8(jwt\u2223x)\u03c0ref(jwt\u2223x)\u2212log\u03c0\u03b8(yl\u2223x)\u03c0ref(yl\u2223x)\u221212\u03c4)2|\u03c4\u2208[0.01,0.1]\n| |\u03b1\u2208[0.1,0.3,0.5]", "caption": "Table 5: Various preference optimization objectives and hyperparameter search range.", "description": "This table presents a comparison of different preference optimization objectives and their associated hyperparameter ranges.  It lists the objective function for each method (DPO, IPO, SimPO, and WRPO variants) along with the hyperparameters used and their respective search ranges during the optimization process. This information is crucial for understanding the experimental setup and for reproducibility.", "section": "3 Method"}, {"content": "| Method | \u03b2 | \u03b3 | \u03b1 | LR |\n|---|---|---|---|---|\n| DPO | 0.01 | - | - | 3e-7 |\n| IPO | - | - | 0.01 | 1e-6 |\n| SimPO | 10 | 1.0 | - | 6e-7 |\n| WRPO<SUB>DPO</SUB> | 0.01 | - | 0.1 | 3e-7 |\n| WRPO<SUB>IPO</SUB> | - | 0.01 | 0.1 | 1e-6 |\n| WRPO<SUB>SimPO</SUB> | 10 | 0 | 0.5 | 6e-7 |", "caption": "Table 6: Hyperparameter settings for preference optimization methods using Target-SFT as the policy model. \u201cLR\u201d denotes the learning rate.", "description": "This table presents the hyperparameter settings used for various preference optimization methods. The methods include Direct Preference Optimization (DPO), Implicit Preference Optimization (IPO), and Simple Preference Optimization (SimPO).  The Target-SFT model serves as the policy model for all these methods. The table shows the hyperparameters \u03b2, \u03c4, \u03b3, and the learning rate (LR) used for each method and highlights the specific values selected after tuning for optimal performance.", "section": "A.2 HYPERPARAMETER TUNING"}, {"content": "| Model | ARC | HellaSwag | MMLU | TruthfulQA | Winogrande | GSM8K | Avg. |\n|---|---|---|---|---|---|---|---| \n| Target | 61.43 | 78.48 | 65.71 | 51.64 | 75.61 | 75.21 | 68.01 |\n| Target-SFT | 51.19 | 79.83 | 64.56 | 45.93 | 76.87 | 62.77 | 63.53 |\n| Target-SFT-DPO | 60.67 | 81.7 | 64.98 | 50.3 | 76.95 | 68.76 | 67.23 |\n| Target-SFT-IPO | 60.58 | 81.68 | 65.5 | 53.93 | 77.9 | 69.67 | 68.21 |\n| Target-SFT-SimPO | 61.77 | 82.23 | 65.13 | 54.76 | 78.45 | 69.6 | 68.66 |\n| Target-SFT-WRPO | 62.63 | 82.38 | 64.91 | 54.72 | 78.53 | 71.57 | 69.12 |\n| Target-SFT-WRPO<sub>IPO</sub> | 59.98 | 81.53 | 65.35 | 53.48 | 78.14 | 69.83 | 68.05 |\n| Target-SFT-WRPO<sub>SimPO</sub> | 61.69 | 81.95 | 65.08 | 57.11 | 78.69 | 68.69 | 68.87 |", "caption": "Table 7: Results of evaluations on Huggingface Open LLM Leaderboard. \u201cTarget\u201d denotes LLaMA3-8B-Instruct.", "description": "This table presents the results of evaluating different language models on the HuggingFace Open LLM Leaderboard.  The models evaluated include the base LLaMA3-8B-Instruct model (denoted as 'Target'), a supervised fine-tuned version of this model ('Target-SFT'), and several variations incorporating different preference optimization techniques (DPO, IPO, SimPO, and WRPO). The evaluation covers seven diverse benchmarks assessing various aspects of language model capabilities, including commonsense reasoning, mathematical reasoning, and instruction following. Each model's performance is quantified by its score on each benchmark.", "section": "4.2 Overall Results"}, {"content": "| Num. | Runtime of DPO (min) | Runtime of WRPO (min) | Increase (%) |\n|---|---|---|---|\n| 1 | 183 | 212 | 15.85% |\n| 2 | 185 | 215 | 16.22% |\n| 5 | 186 | 216 | 16.13% |\n| 10 | 185 | 215 | 16.22% |", "caption": "Table 8: Runtime comparisons for DPO and WRPO across different numbers of source LLMs.", "description": "This table presents a comparison of the training times for Direct Preference Optimization (DPO) and Weighted-Reward Preference Optimization (WRPO), highlighting the impact of varying the number of source Large Language Models (LLMs) used in the WRPO method.  It shows the runtime in minutes for DPO and WRPO with 1, 2, 5, and 10 source LLMs and calculates the percentage increase in WRPO runtime compared to DPO for each scenario. This helps assess the computational efficiency and scalability of WRPO, especially when using multiple source LLMs.", "section": "A IMPLEMENTATION DETAILS"}, {"content": "| Method | AlpacaEval-2 |  |  | \n|---|---|---|---| \n|  | LC(%) | WR(%) | Length | \n|---|---|---|---| \n| Rank1 | 55.9 | 57.6 | 2159 | \n| Rank2 | 53.7 | 55.4 | 2143 | \n| Group1 | 53.5 | 53.8 | 2098 | \n| Group2 | 53.7 | 60.7 | 2440 | ", "caption": "Table 9: Results of our WRPO implemented with varying combinations of source LLMs on AlpacaEval-2.", "description": "This table presents the results of the Weighted-Reward Preference Optimization (WRPO) method on the AlpacaEval-2 benchmark using different combinations of source Large Language Models (LLMs).  It shows how the performance of the WRPO method changes when using different sets of source LLMs.  Specifically, it compares the length-controlled win rate (LC), raw win rate (WR), and the average length of generated responses for various scenarios. The scenarios include using only the highest-ranked response from source LLMs, only the second-highest, and using groups of five LLMs (Group1 and Group2) with different characteristics.", "section": "4.3 ADAPTABILITY OF WRPO TO VARIED OBJECTIVES AND SOURCE LLM SCALING"}, {"content": "| Method | AlpacaEval-2 LC(%) | AlpacaEval-2 WR(%) | MT-Bench Overall | \n|---|---|---|---| \n| WRPO | 55.9 | 57.6 | 7.63 | \n| WRPO<sub>w/y<sub>l<sub>s</sub></sub></sub> | 54.0 | 56.0 | 7.52 |", "caption": "Table 10: Results of WRPO combined with additional dispreferred responses from source models.", "description": "This table presents the results of an experiment evaluating the impact of incorporating additional dispreferred responses from source models into the WRPO framework.  It shows the performance metrics (length-controlled win rate (LC) and raw win rate (WR) on AlpacaEval-2, and the overall score on MT-Bench) for different model configurations.  Specifically, it compares the standard WRPO method against a modified version that includes dispreferred responses from both the target and source LLMs. This allows for an assessment of whether adding these extra dispreferred responses improves or harms the overall model performance.", "section": "F Including Dispreferred Responses from Source Models"}, {"content": "| Model | Huggingface ID |\n|---|---| \n| **Target** | <a href=\"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\">meta-llama/Meta-Llama-3-8B-Instruct</a> |\n| Mistral-Large-Instruct-2407 | <a href=\"https://huggingface.co/mistralai/Mistral-Large-Instruct-2407\">Mistral-Large-Instruct-2407</a> |\n| Gemma2-27B-IT | <a href=\"https://huggingface.co/google/gemma-2-27b-it\">google/gemma-2-27b-it</a> |\n| Qwen2-72B-Instruct | <a href=\"https://huggingface.co/Qwen/Qwen2-72B-Instruct\">Qwen/Qwen2-72B-Instruct</a> |\n| LLaMA3-70B-Instruct | <a href=\"https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\">meta-llama/Meta-Llama-3-70B-Instruct</a> |\n| Gemma2-9B-IT | <a href=\"https://huggingface.co/google/gemma-2-9b-it\">google/gemma-2-9b-it</a> |\n| Internlm2.5-20B-Chat | <a href=\"https://huggingface.co/internlm/internlm2_5-20b-chat\">internlm/internlm2_5-20b-chat</a> |\n| DeepSeek-V2-Chat | <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat-0628\">deepseek-ai/DeepSeek-V2-Chat-0628</a> |\n| DeepSeek-Coder-V2-Instruct | <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct-0724\">deepseek-ai/DeepSeek-Coder-V2-Instruct-0724</a> |\n| Yi-1.5-34B-Chat | <a href=\"https://huggingface.co/01-ai/Yi-1.5-34B-Chat\">01-ai/Yi-1.5-34B-Chat</a> |\n| Phi-3-medium-4k-instruct | <a href=\"https://huggingface.co/microsoft/Phi-3-medium-4k-instruct\">microsoft/Phi-3-medium-4k-instruct</a> |\n| ArmoRM-LLaMA3-8B-v0.1 | <a href=\"https://huggingface.co/RLHFlow/ArmoRM-Llama3-8B-v0.1\">RLHFlow/ArmoRM-Llama3-8B-v0.1</a> |\n| Target-DPO | <a href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-DPO-v0.2\">princeton-nlp/Llama-3-Instruct-8B-DPO-v0.2</a> |\n| Target-SimPO | <a href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-SimPO-v0.2\">princeton-nlp/Llama-3-Instruct-8B-SimPO-v0.2</a> |\n| Target-IPO | <a href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-IPO-v0.2\">princeton-nlp/Llama-3-Instruct-8B-IPO-v0.2</a> |", "caption": "Table 11: Details of open-source models in our experiments. \u201cTarget\u201d denotes LLaMA3-8B-Instruct.", "description": "This table lists the details of the open-source large language models (LLMs) used in the experiments described in the paper.  It includes the name of each model, its size (in billions of parameters), and its HuggingFace ID (a unique identifier for models hosted on the HuggingFace platform). A row is also dedicated to the target model used in the fusion process: LLaMA3-8B-Instruct. The table also specifies the HuggingFace IDs for the reward model and preference optimization baseline checkpoints.", "section": "4.1 Experimental Setup"}, {"content": "| Topic | Instruction | GPT-4-0314 Reference Answer | Target-SFT Answer | Target-SFT-DPO Answer | Target-SFT-WRPO Answer |\n|---|---|---|---|---|---| \n| Situation Puzzle Challenges | Help me find out how to solve a math puzzle. It\u2019s: \"How many dots you can joint down on a page while drawing lines between them, without three of them ever forming a straight line?\" | This math puzzle is related to the concept of \"no three points being collinear.\" In this case, you want to find the maximum number of points you can place on a page such that no three of them are in a straight line. The answer to this puzzle is that you can place an infinite number of points without having three of them form a straight line. One way to achieve this is by placing the points in a non-linear pattern, such as arranging them in a spiral or a circle. However, if you are looking for a more specific answer, you can consider placing the points in a triangular grid pattern. In this arrangement, you can place as many points as you want without having three of them form a straight line. Each new point added will be connected to the existing points without creating a straight line with any other two points. | This is a classic puzzle known as the \"Three Dots Puzzle\" or \"No Straight Lines Puzzle.\" The answer is 3. Here\u2019s why: If you draw lines between 3 dots, you can\u2019t avoid creating at least one straight line. If you draw lines between 4 dots, you can create a triangle with two straight sides, which means three dots are forming a straight line. If you draw lines between 5 dots, you can create a pentagon with three straight sides, which means three dots are forming a straight line. Therefore, the maximum number of dots you can connect without three of them forming a straight line is 3. | This puzzle is a classic example of a graph theory problem, specifically a planar graph challenge. Here\u2019s a breakdown and solution: Understanding the Problem: Graph: Think of each dot as a node, and the lines connecting them as edges. Planarity: The key constraint is that no three nodes (dots) can be connected by straight lines, forming a triangle. This ensures no three edges intersect at a single point. Goal: Find the maximum number of dots you can connect without violating this planarity condition. Solution: The maximum number of dots you can connect while maintaining planarity is 5. Why 5? Proof: Imagine arranging the dots in a pentagon (a five-sided shape). Each edge of the pentagon connects two dots, and no three dots form a straight line within the pentagon. Generalization: For any number of dots greater than 5, it\u2019s impossible to draw a planar graph without three dots forming a straight line. This is because as you add more dots, the likelihood of forming triangles increases, eventually leading to intersections. Key Takeaway: For any planar graph, the maximum number of nodes (dots) without three forming a straight line is 5. This fundamental limit arises from the inherent geometry of planar graphs. I hope this explanation helps you solve your math puzzle! | Unraveling the Puzzle: This problem is a classic example of \"Non-Collinear Point Arrangement\" with a twist. Let\u2019s break it down step by step: Key Insights: 1. No Straight Triangles: The core constraint is that no three dots (points) should ever form a straight line. 2. Geometric Considerations: - A straight line requires three points in a specific arrangement (collinear). - We need to avoid this configuration. Strategies for Solving: 1. The Fundamental Approach: Inductive Reasoning: Start with two dots. Adding a third dot that doesn\u2019t create a straight line is always possible (e.g., forming a triangle with slightly obtuse or acute angles). Generalization: If you can place three dots without a straight line, you can always add more dots, ensuring each new addition maintains this property. 2. Mathematical Proof (for the curious): Proof by Contradiction: Assume, for the sake of contradiction, that you\u2019ve found a set of dots where three form a straight line. Analysis: This implies a triangle with all angles equal (180\u00b0), which is impossible in a non-degenerate triangle (no sides are zero-length). Conclusion: Therefore, it\u2019s impossible to construct a set of dots where three always form a straight line. Practical Implications: Infinite Solutions: There are infinitely many ways to arrange dots without creating straight lines between any three. Page Limitation: While theoretically infinite, the practical limitation is your page size. You can draw as many dots as your paper allows, ensuring no three forms a straight line. Solving on Paper: Start with two dots: This sets the foundation. Add dots gradually: Choose each new dot carefully to avoid creating a straight line with the existing ones. Visual Inspection: Continuously check that no three dots form a straight line as you add more. Remember: The beauty of this puzzle lies in its simplicity and the creative process of finding arrangements that satisfy the constraint. Happy dot-connecting!|", "caption": "Table 12: Case study of a math puzzle on Situation Puzzle Challenges from Arena Hard. Our Target-SFT-WRPO model delivers a mathematically sound and user-oriented response, highlighting the efficacy of WRPO in enhancing model performance in challenging problem-solving scenarios.", "description": "This table presents a case study demonstrating the capabilities of the Weighted-Reward Preference Optimization (WRPO) model in solving a complex mathematical puzzle from the Arena Hard benchmark. It compares the performance of three different models: the base model (Target-SFT), a model enhanced with direct preference optimization (Target-SFT-DPO), and the WRPO model (Target-SFT-WRPO). The responses from each model are shown, along with an analysis of their respective approaches and outcomes. This case study highlights WRPO's ability to produce a mathematically sound and user-friendly solution by combining logical reasoning, detailed explanations, and insights not captured by the base model or the model using only direct preference optimization.  The inclusion of the GPT-4 response offers a comparison to a state-of-the-art model.", "section": "4. EXPERIMENTS"}]