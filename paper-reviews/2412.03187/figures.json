[{"figure_path": "https://arxiv.org/html/2412.03187/x1.png", "caption": "Figure 1: \nDistribution deviations between responses from heterogeneous source LLMs and the LLaMA3-8B-Instruct target LLM before (a) and after (b) DPO fine-tuning, with the prompts from Ultrafeedback\u00a0(Cui et\u00a0al., 2024) as input. Subfigure (c) shows the results (\u03c0DPO-offsubscript\ud835\udf0bDPO-off\\pi_{\\text{DPO-off}}italic_\u03c0 start_POSTSUBSCRIPT DPO-off end_POSTSUBSCRIPT) of preference optimization with this deviated preference dataset, compared to the results (\u03c0\u03b8subscript\ud835\udf0b\ud835\udf03\\pi_{\\theta}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT) from directly applying the target model and those (\u03c0DPO-onsubscript\ud835\udf0bDPO-on\\pi_{\\text{DPO-on}}italic_\u03c0 start_POSTSUBSCRIPT DPO-on end_POSTSUBSCRIPT) from DPO fine-tuning on un-deviated preference data sampled from the target model.", "description": "Figure 1 illustrates the distribution differences between responses generated by source LLMs and the target LLM (LLaMA3-8B-Instruct) before and after DPO fine-tuning.  Subfigure (a) shows the distributions before fine-tuning, highlighting the significant differences. Subfigure (b) demonstrates how DPO fine-tuning reduces these differences.  Finally, subfigure (c) compares the performance of preference optimization using the deviated preference dataset (\u03c0DPO-off) against using only the target model (\u03c0\u03b8) and using DPO fine-tuned on a non-deviated dataset (\u03c0DPO-on).  This comparison highlights the impact of the distribution shift on the effectiveness of DPO.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2412.03187/x2.png", "caption": "Figure 2: Overview of our proposed WRPO for implicit model fusion.", "description": "This figure illustrates the architecture of the Weighted-Reward Preference Optimization (WRPO) method for implicit model fusion.  It shows how multiple source LLMs provide responses to a given prompt.  A reward model ranks these responses, and the highest-ranked response is selected as the preferred response. The target LLM also generates responses, and the best and worst are identified. A progressive adaptation strategy adjusts the weighting of the preferred responses from the source LLMs and the target LLM during training to minimize distribution discrepancies.  The training objective aims to maximize the reward margin between preferred and dispreferred responses, leading to the implicit fusion of knowledge from the source LLMs into the target LLM.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2412.03187/x3.png", "caption": "Figure 3: Internal reward dynamics on Target-SFT model under different preference optimization setups. (a) DPO-on: DPO training on on-policy preference pairs (x,ywt,yl)\ud835\udc65subscript\ud835\udc66subscript\ud835\udc64\ud835\udc61subscript\ud835\udc66\ud835\udc59(x,y_{w_{t}},y_{l})( italic_x , italic_y start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ). (b) DPO-hybrid: DPO training on hybrid-policy preference pairs (x,yws,yl)\ud835\udc65subscript\ud835\udc66subscript\ud835\udc64\ud835\udc60subscript\ud835\udc66\ud835\udc59(x,y_{w_{s}},y_{l})( italic_x , italic_y start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ). (c) WRPO \u03b1=0.5\ud835\udefc0.5\\alpha=0.5italic_\u03b1 = 0.5: WRPO training with \u03b1\ud835\udefc\\alphaitalic_\u03b1 increasing from 0 to 0.5.", "description": "Figure 3 visualizes the changes in internal reward dynamics observed during the training of the Target-SFT model under three different preference optimization methods.  Each method uses different combinations of preferred and dispreferred response pairs. Panel (a) shows DPO training using only preferred and dispreferred responses generated from the target LLM (on-policy data).  Panel (b) demonstrates DPO training with preferred responses sourced from both target and source models (hybrid-policy data). Finally, Panel (c) showcases the WRPO method, where the influence of source model responses is progressively increased during training via a dynamic fusion coefficient (\u03b1) that starts at 0 and increases to 0.5. The figure illustrates how the different methods affect the reward dynamics over time, showing the internal rewards for both the chosen and rejected responses, as well as the resulting reward margin.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2412.03187/x4.png", "caption": "Figure 4: Results of ablation studies for our WRPO method on AlpacaEval-2, utilizing the length-controlled win rate metric.", "description": "This figure displays the results of ablation studies conducted to evaluate the effectiveness of different components within the Weighted-Reward Preference Optimization (WRPO) method.  The study specifically focuses on the AlpacaEval-2 benchmark, using the length-controlled win rate as the evaluation metric.  It likely shows the performance when certain aspects of the WRPO are removed or modified (ablated), such as removing the influence of the source LLMs or the target LLMs, demonstrating the contribution of each part of the overall WRPO model to its final performance.", "section": "4 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2412.03187/x5.png", "caption": "Figure 5: AlpacaEval-2 length-controlled win rate and hybrid-policy internal reward accuracy under different fusion coefficient \u03b1\ud835\udefc\\alphaitalic_\u03b1 settings.", "description": "This figure shows the results of an experiment on the AlpacaEval-2 benchmark, where the fusion coefficient \u03b1 (alpha) was varied.  The experiment tested a model's ability to implicitly learn from multiple language models (LLMs), using a technique that progressively incorporates LLM responses. The graph displays two key metrics: the length-controlled win rate (a measure of the model's performance against a baseline model) and the hybrid-policy internal reward accuracy (reflecting the model's confidence in its own predictions when combining its own generated responses with those from other LLMs).  The x-axis represents the different values of \u03b1, showing how the balance between the model's own responses and the other LLMs' responses influences the performance. The plot allows for assessing the optimal \u03b1 setting that maximizes both the win rate and the hybrid-policy accuracy.", "section": "4.4 Analysis of the Weighted-Reward Mechanism in WRPO"}, {"figure_path": "https://arxiv.org/html/2412.03187/x6.png", "caption": "Figure 6: Comparisons of dynamic and static tuning strategies for the fusion coefficient on AlpacaEval-2, utilizing the length-controlled win rate metric.", "description": "Figure 6 shows the results of an ablation study comparing two strategies for adjusting the fusion coefficient (\u03b1) in the Weighted-Reward Preference Optimization (WRPO) method. The fusion coefficient controls the balance between using preferred responses from the source LLMs and the target LLM during training.  The x-axis represents different values of \u03b1, ranging from 0.1 to 0.9. The y-axis shows the length-controlled win rate on the AlpacaEval-2 benchmark.  The figure compares a dynamic strategy (where \u03b1 increases linearly during training) and a static strategy (where \u03b1 is fixed at a particular value throughout training). The results indicate that the dynamic strategy generally performs better than the static strategy across various \u03b1 values, highlighting the benefits of progressively adapting the model's reliance on preferred responses from source LLMs during training.  Error bars might be present but are not described in the caption.", "section": "E TUNING STRATEGIES FOR FUSION COEFFICIENT"}]