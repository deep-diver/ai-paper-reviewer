[{"figure_path": "https://arxiv.org/html/2411.16819/x1.png", "caption": "Figure 1: Visualization of Frame2Frame\u2019s editing process.\nTemporal progression of our video-based approach.\nStarting from the source image (leftmost), frames illustrate the natural evolution toward the target edit (rightmost).\nOur method produces temporally coherent intermediate states while preserving fidelity to both the source content and the editing intent.", "description": "This figure demonstrates the Frame2Frame method's image editing process. It shows a sequence of images transitioning smoothly from a source image to a target image, representing the temporal evolution of the edit.  The intermediate frames showcase the method's ability to maintain consistency and preserve the fidelity of the original image while incorporating the desired edits.  Each image represents a step in the transformation from the source to the target, illustrating a natural and temporally coherent process.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.16819/x2.png", "caption": "Figure 2: Editing Manifold Pathway.\nGiven an input image and target caption \u201dA happy person making a heart shape with their hands\u201d, our method generates a continuous path on the natural image manifold.\nEach generated frame (indicated by black arrows) represents a plausible intermediate state between the source and target, maintaining temporal consistency throughout the transformation.\nAs a result, in contrast to the competing approach, F2F achieves the desired edit while preserving the \u201dAI\u201d text on the person\u2019s shirt.", "description": "This figure visualizes the image editing process using the Frame2Frame (F2F) method.  Starting with an input image of a person wearing a shirt with 'AI' written on it, and a target caption instructing the model to edit the image to show the person making a heart shape with their hands, the F2F method generates a series of intermediate images, effectively creating a smooth path across the image manifold.  Each image represents a plausible and consistent transition between the source image and the target edit.  The black arrows highlight the temporal progression.  Importantly, the F2F method maintains the 'AI' text on the shirt throughout the transformation, unlike competing approaches which might lose such detail. The figure demonstrates the temporal coherence and fidelity of the proposed method, highlighting its ability to preserve key elements of the original image while successfully implementing complex edits.", "section": "3. Frame2Frame"}, {"figure_path": "https://arxiv.org/html/2411.16819/x3.png", "caption": "Figure 3: Frame2Frame Overview. Given a source image and editing prompt, our pipeline proceeds in three steps. First, a Vision-Language Model generates a temporal caption describing the transformation. Next, this caption guides a video generator to create a natural progression of the edit. Finally, our frame selection strategy identifies the optimal frame that best realizes the desired edit, producing the final image of the cat mid-leap.", "description": "Frame2Frame is a three-step image editing process.  It begins with a source image and an editing prompt.  First, a Vision-Language Model (VLM) creates a temporal caption describing the desired transformation from the source to target image as a video. This temporal caption is then fed into a video generator which produces a video showing a smooth and realistic transition from the source image to the desired edit. Finally, a frame selection strategy, also guided by the VLM, identifies the optimal frame from the generated video that best achieves the edit, thus generating the final edited image.", "section": "3. Frame2Frame"}, {"figure_path": "https://arxiv.org/html/2411.16819/x4.png", "caption": "Figure 4: \nQualitative Results on TEdBench. Comparison with other methods across various editing tasks. Our approach consistently produces edits that better align with the target prompt while preserving the source image\u2019s content and structure. For instance, in the teddy bear example, our method uniquely achieves complex structural modifications while maintaining high visual quality.", "description": "Figure 4 presents a qualitative comparison of different image editing methods on the TEdBench benchmark. It showcases the results of several editing tasks, highlighting the superior performance of the proposed Frame2Frame (F2F) method. The F2F approach consistently generates edits that closely match the target descriptions while effectively preserving the original image's content and structural integrity.  A key example is the teddy bear edit, where F2F uniquely handles complex structural changes, resulting in high-quality edits.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16819/x5.png", "caption": "Figure 5: Qualitative Results on PosEdit. Comparison between our Frame2Frame method and LEDITS++ on human motion editing tasks. For each example, we show the source image, edited results from both methods, and the ground-truth target image. Our method better preserves subject identity while achieving more natural pose transitions.", "description": "Figure 5 presents a qualitative comparison of image editing results between the Frame2Frame method and LEDITS++, focusing on human pose editing tasks from the PosEdit benchmark dataset. Each row showcases a triplet of images: the original source image, the edited image produced using Frame2Frame, and the edited image produced using LEDITS++.  A ground truth image from the PosEdit dataset, which depicts the target pose, is also included in each row for reference. The figure highlights that Frame2Frame generally better preserves the subject's identity and produces more realistic-looking and natural transitions between poses when compared to LEDITS++.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16819/x6.png", "caption": "Figure 6: \nAdditional Vision Tasks. \nQualitative results of our image-to-video-to-image editing approach on selected traditional tasks.", "description": "This figure displays the results of applying the image-to-video-to-image editing approach to four tasks commonly addressed in computer vision: image denoising, image deblurring, image outpainting, and image relighting.  For each task, the original image is shown alongside the result obtained using the method. This demonstrates the versatility of the proposed approach to handle tasks beyond typical image editing.", "section": "Additional Vision Tasks"}, {"figure_path": "https://arxiv.org/html/2411.16819/x7.png", "caption": "Figure S7: TEdBench Editing Examples.", "description": "This figure displays several examples of image editing results obtained using the Frame2Frame method on the TEdBench benchmark. Each example shows the original image, the target edit description, the generated temporal editing caption (describing the transformation over time), and the final edited image produced by the model.  These examples highlight the model's ability to achieve complex edits while preserving the essential characteristics of the original image.", "section": "A. Additional Editing Examples"}, {"figure_path": "https://arxiv.org/html/2411.16819/x8.png", "caption": "Figure S8: TEdBench Editing Examples.", "description": "This figure displays several examples of image editing results generated using the Frame2Frame method on the TEdBench dataset. Each example includes the source image, the target edit caption describing the desired modification, the generated temporal editing caption guiding the video generation process, and the final edited image.  The examples showcase the method's ability to perform complex and nuanced edits while preserving the overall content and structure of the source image.", "section": "A. Additional Editing Examples"}, {"figure_path": "https://arxiv.org/html/2411.16819/x9.png", "caption": "Figure S9: In Context Learning Examples.", "description": "This figure shows example pairs used for in-context learning in the Frame2Frame model.  Each example consists of a source image, a target caption describing the desired edit, and a corresponding temporal caption generated by a Vision-Language Model (VLM). The temporal captions describe the natural evolution of the edit over time, providing instructions for the video generation process.  The examples illustrate how the VLM translates concise instructions into detailed step-by-step transformations for generating the video.", "section": "B. Temporal Editing Captions"}, {"figure_path": "https://arxiv.org/html/2411.16819/x10.png", "caption": "Figure S10: Frame Selection Collage. The target editing caption for this example is: \u201cA photo of a cat yawning.\u201d.", "description": "This figure shows a collage of images generated by the Frame2Frame model.  The model started with a single image of a cat and generated a sequence of images aiming to create a final image matching the caption \"A photo of a cat yawning.\"  The collage displays several frames from that video sequence, allowing one to visually observe the gradual transformation toward the target. The selection process chooses the frame that best represents the target image while maintaining fidelity to the original source image.", "section": "3. Frame2Frame"}, {"figure_path": "https://arxiv.org/html/2411.16819/x11.png", "caption": "Figure S11: Flux.1-dev Generations", "description": "This figure shows examples of images generated using the Flux.1-dev text-to-image model.  The images illustrate the diverse range of visual outputs the model can produce and represent various categories such as people with AI shirts, those making a heart shape, and a combination of both. This helps visualize how the natural image manifold is structured and how different edits can be achieved through transitions.", "section": "Editing Manifold Pathway"}, {"figure_path": "https://arxiv.org/html/2411.16819/x12.png", "caption": "Figure S12: PosEdit Examples.", "description": "Figure S12 provides several examples from the PosEdit dataset, showcasing various human pose editing tasks. Each example shows the source image (person standing neutrally), target image (person performing a specific action), the temporal editing caption describing the transition, and the ground truth target image for comparison.  The examples illustrate the range of poses and the level of detail involved in the dataset, highlighting the complexity of the pose editing task.", "section": "5.3 PosEdit Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.16819/x13.png", "caption": "Figure S13: PosEdit Examples.", "description": "Figure S13 shows example results of the PosEdit benchmark. Each example includes the source image, the target image (ground truth), the generated temporal editing caption used for the editing process, and the final edited image produced by the Frame2Frame method. This figure visually demonstrates the effectiveness of the proposed approach in accurately performing human pose editing tasks while maintaining the identity and key attributes of the subject.", "section": "5.3 PosEdit Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.16819/x14.png", "caption": "Figure S14: Survey Example", "description": "This figure displays a sample survey question used in the human evaluation study. The top shows the original image and the edited versions from two different methods, and the bottom shows the questions asked to assess the edit accuracy and edit quality. Edit accuracy is evaluated in terms of how well the final image corresponds to the requested edit in the caption. Edit quality is assessed based on the preservation of image content from the source image and the overall natural appearance of the changes.  This figure exemplifies the process of human evaluation used in the paper to assess the user's preference in the edited result.", "section": "5.4. Human Evaluation Survey"}, {"figure_path": "https://arxiv.org/html/2411.16819/x15.png", "caption": "Figure S15: Survey Example", "description": "This figure displays an example from the human evaluation survey conducted in Section 5.4.  It shows a source image, a target edit, and two edited images produced by different methods (likely Frame2Frame and LEDITS++). Participants in the survey were asked to compare the images based on edit accuracy (how well the edit matches the target description) and edit quality (image fidelity, naturalness, and seamless integration of edits).  The figure likely includes multiple-choice questions related to selecting the better-edited image according to each metric, reflecting the human preference-based evaluation.", "section": "5.4 Human Evaluation Survey"}]