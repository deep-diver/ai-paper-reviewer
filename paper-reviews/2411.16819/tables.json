[{"content": "| Model | LPIPS\u2193 | CLIP-I\u2191 | CLIP\u2191 |\n|---|---|---|---| \n| SDEdit | 0.30 | 0.85 | 0.60 |\n| Pix2Pix-ZERO | 0.29 | 0.84 | 0.62 |\n| LEDITS++ | 0.23 | 0.87 | **0.63** |\n| Imagic | 0.52 | 0.86 | **0.63** |\n| F2F | **0.22** | **0.89** | **0.63** |", "caption": "Table 1: \nTEdBench Results. Quantitative comparison on TEdBench benchmark. Source metrics (LPIPS and CLIP-I) measure content preservation, while Target metric (CLIP) measures edit accuracy.\nOur Frame2Frame (F2F) method achieves better or comparable performance across all metrics.", "description": "This table presents a quantitative comparison of the Frame2Frame (F2F) image editing method against several state-of-the-art methods on the TEdBench benchmark.  The evaluation uses three metrics: LPIPS and CLIP-I (to assess how well the edited image preserves the content of the original image), and CLIP (to evaluate how accurately the edit matches the target description). Lower LPIPS scores and higher CLIP-I scores indicate better content preservation, while higher CLIP scores signify better edit accuracy. The results show that F2F either outperforms or achieves comparable results to existing methods across all metrics.", "section": "5. Experiments"}, {"content": "| Model | Source LPIPS\u2193 | Source CLIP-I\u2191 | Target LPIPS\u2193 | Target CLIP-I\u2191 | Target CLIP\u2191 |\n|---|---|---|---|---|---| \n| SDEdit | 0.39 | 0.61 | 0.39 | 0.64 | 0.57 |\n| Pix2Pix-ZERO | 0.39 | 0.57 | 0.40 | 0.60 | 0.56 |\n| LEDITS++ | 0.26 | 0.65 | 0.28 | 0.69 | **0.64** |\n| F2F | **0.14** | **0.82** | **0.15** | **0.84** | **0.64** |", "caption": "Table 2: \nPosEdit Results.\nQuantitive evaluation on PosEdit.\nSource metrics assess similarity to the original image, while Target metrics now include both LPIPS and CLIP-I comparisons to the ground-truth target image, along with the CLIP score for edit accuracy.", "description": "This table presents a quantitative analysis of different image editing methods on the PosEdit benchmark dataset.  It evaluates how well each method preserves the source image (Source metrics: LPIPS and CLIP-I scores) and how accurately it produces the desired edit (Target metrics: LPIPS, CLIP-I, and CLIP scores compared to the ground truth target image). Lower LPIPS scores indicate better preservation of the source image, while higher CLIP-I scores suggest better semantic similarity between the source and edited images. The CLIP score measures the alignment between the edited image and the target edit description, reflecting the edit's accuracy. ", "section": "5. Experiments"}, {"content": "| Method | Edit Accuracy |  | Edit Quality |  |\n|---|---|---|---|---|\n|  | Overall | Per-Image | Overall | Per-Image |\n| F2F | **54.1%** | **53.0%** | **65.6%** | **67.0%** |\n| LEDITS++ | 45.9% | 47.0% | 34.4% | 33.0% |", "caption": "Table 3: \nHuman Survey Results.\nHuman evaluation on TEdBench shows that F2F surpasses LEDITS++ in edit accuracy while demonstrating a significant advantage in preserving the original image content.", "description": "A human evaluation comparing Frame2Frame (F2F) and LEDITS++ on the TEdBench dataset.  Participants assessed both methods based on edit accuracy (how well the edit matched the instructions) and edit quality (how well the original image content was preserved).  The results show that F2F significantly outperforms LEDITS++ in both areas.", "section": "5. Experiments"}, {"content": "| Model | LPIPS\u2193 | CLIP-I\u2191 | CLIP\u2191 |\n|---|---|---|---|\n| Original Captions | **0.21** | **0.89** | 0.60 |\n| Temporal Captions | 0.22 | **0.89** | **0.63** |", "caption": "Table S4: \nTemporal Editing Captions Ablation.", "description": "This table presents the ablation study results comparing the performance of using automatically generated temporal editing captions against directly using the target captions from the TEdBench benchmark.  It shows that while preserving source image content, using the automatically generated temporal captions leads to superior performance compared to using original captions directly.", "section": "B.2. Ablation"}, {"content": "| Model | LPIPS\u2193 | CLIP-I\u2191 | CLIP\u2191 |\n|---|---|---|---| \n| Last Frame | 0.24 | **0.9** | 0.61 |\n| Selected Frame | **0.22** | 0.89 | **0.63** |", "caption": "Table S5: \nFrame Selection Ablation.", "description": "This table presents an ablation study comparing two approaches for selecting the optimal frame from a generated video sequence in the image editing process. The first method uses the last frame of the video as the final edited image, while the second uses a Vision-Language Model (VLM) to select the best frame that fulfills the edit request. The table shows the quantitative evaluation results of both methods on the TEdBench benchmark, using LPIPS, CLIP-I\u2191 (for source preservation), and CLIP\u2191 (for edit accuracy) metrics. This ablation study demonstrates the effectiveness of the VLM-based frame selection method in improving the overall edit quality.", "section": "C. Frame Selection"}]