{"references": [{"fullname_first_author": "Albert Gu", "paper_title": "Hippo: Recurrent memory with optimal polynomial projections", "publication_date": "2020-12-01", "reason": "This paper introduces the HiPPO theory, which is the theoretical foundation for structured state space models (SSMs) and is central to the paper's analysis of SSM limitations."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-01", "reason": "This paper introduces the Mamba model, a key SSM architecture analyzed in this paper, and its selection mechanism which is critically examined in the context of recency bias."}, {"fullname_first_author": "Tri Dao", "paper_title": "Transformers are ssms: Generalized models and efficient algorithms through structured state space duality", "publication_date": "2024-05-21", "reason": "This paper establishes the theoretical link between transformers and SSMs, providing a broader context and justifying the comparison and contrast between the two model types."}, {"fullname_first_author": "Simran Arora", "paper_title": "Zoology: Measuring and improving recall in efficient language models", "publication_date": "2023-12-01", "reason": "This paper introduces the \"Needle in a Haystack\" benchmark used in the experiments, directly relevant to the evaluation of long-range dependency handling in SSMs and provides a comparison with transformers."}, {"fullname_first_author": "Hoang NT", "paper_title": "Revisiting graph neural networks: All we have is low-pass filters", "publication_date": "2021-08-01", "reason": "This paper introduces the concept of over-smoothing in graph neural networks (GNNs), which is used to explain the observed over-smoothing phenomenon in deep SSMs and to provide a theoretical analysis of the limitation."}]}