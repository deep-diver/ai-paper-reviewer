{"references": [{"fullname_first_author": "Yupeng Chang", "paper_title": "A survey on evaluation of large language models", "publication_date": "2024-03-01", "reason": "This survey provides a broad overview of LLM evaluation methods, crucial for understanding the context of StructFlowBench."}, {"fullname_first_author": "Lianmin Zheng", "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena", "publication_date": "2023-01-01", "reason": "This paper introduces MT-Bench, a key benchmark for multi-turn dialogue evaluation, providing a methodology upon which StructFlowBench builds."}, {"fullname_first_author": "Yixiao Fang", "paper_title": "BotChat: Evaluating LLMs' capabilities of having multi-turn dialogues", "publication_date": "2024-01-01", "reason": "This research offers insights into designing more robust multi-turn conversations that can be used in structFlowBench to generate more complex dialogue scenarios."}, {"fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "publication_date": "2024-04-01", "reason": "This paper provides the details for the Phi-3 model which is referenced as one of the state-of-the-art LLMs used to compare against structFlowBench."}, {"fullname_first_author": "Tao Zhang", "paper_title": "Cfbench: A comprehensive constraints-following benchmark for llms", "publication_date": "2024-08-01", "reason": "CFBench provides insights into constraint satisfaction, which is a focus of comparison in structFlowBench for evaluating its dual-constraint evaluation system."}]}