[{"figure_path": "2410.18505/tables/table_4_0.md", "caption": "Table 1: Pre-training Model Configuration Parameters", "description": "This table lists the hyperparameters used during the pre-training phase of the Qwen2-0.5B model.  These parameters encompass various aspects of the model's architecture and training process, including attention dropout rate, beginning and end of sequence token IDs, the activation function used, the hidden layer size, intermediate size, maximum sequence length, number of attention heads and hidden layers, number of key-value heads, padding token ID, epsilon value for RMS normalization, the theta parameter for RoPE (Rotary Position Embedding), whether word embeddings are tied, the data type used for PyTorch tensors, and the vocabulary size.", "section": "3.1 Experimental Setting"}, {"figure_path": "2410.18505/tables/table_6_0.md", "caption": "Table 2: Comparison of Dataset Impacts on Model Performance in Mixed and Chinese Dataset Experiments", "description": "This table presents the results of two experiments evaluating the impact of different datasets on model performance.  The \"Mixed Dataset Experiment\" compares the performance of SkyPile, Wanjuan-v1, CCI3.0, and CCI3.0-HQ across various metrics (ARC-C, ARC-E, HellaSwag, Winograd, MMLU, OpenbookQA, PIQA, SIQA, CEval, and CMMLU) using a dataset comprising 60% English, 10% code, and 30% Chinese content. The \"Chinese Dataset Experiment\" uses the same metrics but with a dataset consisting of 100% Chinese content, including Wanjuan-v1, SkyPile, CCI3.0, and CCI3.0-HQ.  For each dataset and metric, the average score is reported, along with sub-averages for English and Chinese metrics.  The table allows for a comparison of the datasets' performance across various tasks and languages.", "section": "3.2 Impacts of CCI3.0-HQ on Model Training"}, {"figure_path": "2410.18505/tables/table_7_0.md", "caption": "Table 3: Comparison of Two Quality Annotation Methods", "description": "This table compares the performance of two quality annotation methods, DCLM and FineWeb-edu, in evaluating the quality of Chinese web data.  The comparison is based on several metrics including ARC-C, ARC-E, HellaSwag, Winograd, MMLU, OpenbookQA, PIQA, SIQA, CEval, and CMMLU.  For each metric, the table shows the scores obtained by each method, along with average scores for English-specific metrics, Chinese-specific metrics, and an overall average.  The results indicate that FineWeb-edu generally outperforms DCLM, particularly in Chinese-specific metrics.", "section": "3.3 Assessment of Quality Annotation Techniques"}, {"figure_path": "2410.18505/tables/table_8_0.md", "caption": "Table 4: Evaluation of Different Quality Classifiers", "description": "Table 4 presents a comparison of four different quality classifiers: classifierFineWeb-edu, classifierChineseWebText, classifierIndustryCorpus2, and classifierCC13.0-HQ.  The performance of each classifier is evaluated based on precision, recall, and F1-score for both positive and negative sample classifications.  The results are shown separately for positive and negative classes, along with a macro-average F1-score, which provides an overall measure of classifier performance across both classes.  The classifierCC13.0-HQ demonstrates the best performance, achieving the highest macro-average F1-score of 0.73.", "section": "3.4 Evaluation of Quality Classifiers for Chinese Web data"}]