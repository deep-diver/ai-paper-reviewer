[{"page_end_idx": 1, "page_start_idx": 1, "section_number": 1, "section_title": "Introduction", "details": {"details": "The success of Large Language Models (LLMs) is largely due to high-quality, extensive pre-training data.  While open-source datasets like The Pile (825GB) and Common Crawl have been crucial in LLM development, the demand for pre-training data has surged to over 10 trillion tokens, highlighting the need for both larger and higher-quality datasets.  Current open-source Chinese datasets like WuDao, SkyPile150B, and WanjuanV1 are limited in scale and quality due to a scarcity of readily available high-quality Chinese data and a lack of effective data filtering methodologies.  This scarcity of high-quality data presents a significant barrier to the development of high-performing Chinese LLMs, underscoring the urgent need for improved data filtering and quality classification techniques.", "first_cons": "The introduction focuses heavily on the limitations of existing Chinese language datasets without offering concrete solutions or a clear roadmap to address those challenges within the section itself.", "first_pros": "The introduction effectively establishes the context and importance of high-quality data in the development of LLMs, particularly highlighting the significant gap in the availability of such data for Chinese language models.", "keypoints": ["The success of LLMs is primarily attributed to the availability of extensive, high-quality pre-training corpora.", "The demand for pre-training data has exceeded 10 trillion tokens.", "Existing open-source Chinese datasets are limited in scale (e.g.,  WuDao, SkyPile150B, and WanjuanV1) and quality.", "There is a lack of research focused on improving quality classification for Chinese web data.", "The scarcity of high-quality data presents a substantial barrier to the development of high-performance Chinese language models."], "second_cons": "While the problem is clearly stated, the section lacks specific examples of the types of issues with existing data or how these issues manifest in the performance of models trained on them.", "second_pros": "The introduction successfully emphasizes the urgency of the issue by highlighting the significant challenges posed by the limited availability of high-quality data for Chinese language models, effectively motivating the need for the presented work.", "summary": "This section introduces the critical role of high-quality, extensive pre-training data in the success of Large Language Models (LLMs).  It highlights the significant gap in the availability of such data for Chinese, with existing datasets limited in both scale and quality due to a scarcity of suitable sources and the lack of effective data filtering techniques.  This limitation poses a major hurdle to developing high-performing Chinese LLMs, emphasizing the need for improved data filtering and quality assessment methodologies."}}, {"page_end_idx": 3, "page_start_idx": 2, "section_number": 2, "section_title": "Pipeline", "details": {"details": "The data processing pipeline consists of two main stages: Fundamental Processing and High-Quality Processing.  Fundamental Processing involves four steps: safety filtering (removing unsafe content), text extraction and cleaning, document-level deduplication (using global MinHash to remove near-duplicates), and heuristic and basic quality filtering (using a classifier on ChineseWebText to identify low-quality documents).  This stage results in the CCI3.0 dataset. High-Quality Processing further refines the dataset using the Qwen2-72B-Instruct model to identify high-quality samples. This involves a two-step approach:  first annotating 140k samples (with a 80% agreement rate with GPT-4 annotations) using Qwen2-72B-Instruct model and then training a 0.5B quality classifier on those samples to efficiently filter the CCI3.0 dataset, producing the high-quality CCI3.0-HQ dataset.  This classifier is then used to process approximately 1.5 billion samples, resulting in the final CCI3.0-HQ dataset, a 500GB subset of CCI3.0.", "first_cons": "The description of the heuristic and basic quality filtering in the Fundamental Processing stage lacks detail on the specific heuristics used.  This makes it difficult to fully understand and replicate the process.", "first_pros": "The two-stage pipeline, combining rule-based and model-based filtering methods, is a sophisticated approach to enhance data quality. It demonstrates a pragmatic strategy for combining different filtering mechanisms to achieve better results than a single approach would.", "keypoints": ["Two-stage pipeline (Fundamental and High-Quality Processing)", "Fundamental Processing involves safety filtering, text extraction, deduplication, and basic quality filtering", "High-Quality Processing uses Qwen2-72B-Instruct for high-quality sample annotation and trains a 0.5B quality classifier", "140k samples annotated in the High-Quality stage, 80% agreement with GPT-4", "0.5B parameter quality classifier trained on 140k samples to filter 1.5 billion samples", "Results in the CCI3.0-HQ dataset, a 500GB subset of CCI3.0"], "second_cons": "The paper lacks specifics on the computational resources used for training the 0.5B quality classifier and processing 1.5 billion samples. This lack of detail makes the reproducibility and scalability assessment of this pipeline difficult.", "second_pros": "The use of a large language model (Qwen2-72B-Instruct) in the High-Quality Processing stage significantly enhances the ability to identify high-quality data, improving the overall quality of the final dataset.", "summary": "The paper describes a two-stage data processing pipeline for creating a high-quality Chinese pre-training dataset. The first stage, Fundamental Processing, uses traditional methods to clean and filter the data. The second stage, High-Quality Processing, leverages a large language model to identify high-quality samples and train a smaller classifier to further filter the data. This results in the CCI3.0-HQ dataset, which is a 500GB subset of the CCI3.0 dataset and contains approximately 140,000 high-quality samples, achieving an 80% agreement rate with GPT-4 annotations. The classifier is then open-sourced."}}, {"page_end_idx": 6, "page_start_idx": 3, "section_number": 3, "section_title": "Experiments", "details": {"details": "The experiment section evaluates the effectiveness of the CCI3.0-HQ dataset in pre-training large language models (LLMs).  Two main experiments were conducted: a mixed dataset experiment (60% English, 10% code, 30% Chinese) and a Chinese-only dataset experiment.  The results demonstrate that CCI3.0-HQ significantly outperforms existing Chinese datasets (SkyPile, Wanjuan-v1, CCI3.0) across various benchmarks and metrics (ARC-C, ARC-E, HellaSwag, Winograd, MMLU, OpenbookQA, PIQA, SIQA, CEval, CMMLU).  In the mixed dataset experiment, CCI3.0-HQ achieves the highest scores in several benchmarks, indicating its strong performance in both English and Chinese tasks. In the Chinese-only experiment, CCI3.0-HQ showcases a clear lead, confirming its superiority in Chinese language tasks.  The section also compares two methods for high-quality annotation (FineWeb-edu and DataComp-LM), concluding that FineWeb-edu is more effective for Chinese corpora. Finally, the performance of different quality classifiers (including one trained on CCI3.0-HQ) is evaluated, revealing that the CCI3.0-HQ classifier significantly outperforms others.", "first_cons": "The experiment section primarily focuses on comparing CCI3.0-HQ against other datasets, with less emphasis on detailed analysis of the internal workings or the methodology of the experiments themselves.", "first_pros": "The experiments rigorously compare CCI3.0-HQ with other datasets using a broad range of benchmarks and metrics, providing strong quantitative evidence supporting the dataset's superior performance.", "keypoints": ["CCI3.0-HQ significantly outperforms existing datasets like SkyPile and Wanjuan-v1 across multiple benchmarks and metrics.", "In the mixed dataset experiment, CCI3.0-HQ achieves the highest scores in several key benchmarks (e.g., ARC-E: 0.542, Winograd: 0.523).", "In the Chinese-only dataset experiment, CCI3.0-HQ significantly outperforms other datasets, with a notable lead in several key metrics.", "The FineWeb-edu annotation method proves more effective than DataComp-LM for defining high-quality samples in Chinese corpora.", "The CCI3.0-HQ quality classifier demonstrates superior performance compared to existing classifiers, particularly for positive samples (precision: 0.86, F1-score: 0.53)."], "second_cons": "While the experimental setup is described, specific details about hyperparameters, training procedures, and model architecture are relatively limited, which makes reproducibility somewhat difficult.", "second_pros": "The study uses a 0.5B parameter model trained on 100 billion tokens, representing a realistic and relevant scale for LLM pre-training. This adds to the credibility and significance of the results.", "summary": "This experiment section evaluates the CCI3.0-HQ dataset's performance in pre-training LLMs by comparing it to existing datasets (SkyPile, Wanjuan-v1, CCI3.0) across multiple benchmarks.  Two experiments are performed: a mixed language and a Chinese-only experiment, both showing CCI3.0-HQ's superior performance.  Furthermore, the section compares high-quality sample annotation methods (FineWeb-edu vs. DataComp-LM) and quality classifiers,  demonstrating the superiority of FineWeb-edu and the CCI3.0-HQ classifier."}}]