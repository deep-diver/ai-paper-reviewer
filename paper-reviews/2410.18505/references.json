{"references": [{" publication_date": "2020", "fullname_first_author": "Leo Gao", "paper_title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling", "reason": "This paper is foundational to the field of large language models by introducing The Pile, one of the first large-scale, diverse text datasets for pre-training.  The Pile served as a benchmark dataset and spurred the development of many subsequent large language models, making it a highly influential contribution to the field. The work highlighted the importance of large and diverse datasets for improving language model performance. This paper directly informed the current paper's motivation to create a similar dataset for the Chinese language.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "An Yang", "paper_title": "Qwen2 technical report", "reason": "This is a crucial reference because the paper leverages the Qwen2-72B-Instruct model for high-quality sample annotation in its dataset creation process.  The paper heavily relies on this model's capabilities to enhance the quality of its data.  The details of the Qwen2-72B-Instruct model are important for understanding the methodology employed.  The performance of the Qwen2 model is central to evaluating the effectiveness of their method.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The Llama 3 herd of models", "reason": "This paper is highly relevant because it discusses large language models, a topic directly related to the current paper's focus on creating high-quality data for training such models. The advancements and scaling trends in LLMs described likely influenced the motivations and methods used in creating the new dataset. The paper provides context for the large-scale nature of the current study's goals.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Common Crawl", "paper_title": "Common Crawl Corpus", "reason": "Common Crawl is a crucial data source for many large language models. Its mention in the paper highlights the importance and scale of data required for training state-of-the-art LLMs.  The challenges in filtering and processing this massive dataset are addressed by the new methods presented in the paper. This paper's inclusion establishes the existing methodologies that have been previously used and the current paper's work expands on this.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Sha Yuan", "paper_title": "Wudaocorpora: A super large-scale chinese corpora for pre-training language models", "reason": "This paper describes WuDao, a significant existing large-scale Chinese dataset.  Its inclusion allows for a direct comparison to the current paper's dataset, providing a benchmark for evaluating the size and quality of the newly proposed dataset.  It emphasizes the lack of high-quality, sufficiently large Chinese datasets, thus making the new work more relevant and impactful in this specific space.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Tianwen Wei", "paper_title": "Skywork: A more open bilingual foundation model", "reason": "This paper introduces the SkyPile dataset, a relevant benchmark for comparing the performance of the proposed dataset. The paper highlights that, though a large dataset, it still has shortcomings in size and quality. Therefore, it is crucial to compare their proposed methodology against this prior work.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Conghui He", "paper_title": "Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models", "reason": "This paper describes WanjuanV1, another significant existing Chinese dataset, forming a critical point of comparison for evaluating the new dataset's performance.   By including this citation, the authors highlight existing shortcomings in size and quality which their work aims to overcome.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Guilherme Penedo", "paper_title": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale", "reason": "This paper introduces FineWeb, a large-scale dataset with an emphasis on quality filtering that is used for comparison in the current paper. The approach utilized in the paper is highly relevant due to its model-driven filtering techniques, allowing for a detailed comparison of methods. The comparison provides insight into the advanced methods used in the current study's work.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Together Computer", "paper_title": "Redpajama: An open source recipe to reproduce llama training dataset", "reason": "This paper describes an earlier open-source approach to creating large language model datasets that utilized rule-based filtering. Its inclusion in the current study provides context for the evolution of dataset creation methodologies.  It highlights that rule-based methods alone are insufficient for the current demands of high-quality LLMs.", "section_number": 1}, {" publication_date": "1997", "fullname_first_author": "A. Broder", "paper_title": "On the resemblance and containment of documents", "reason": "This paper introduces the MinHash algorithm, a crucial component of the document-level deduplication step in the data processing pipeline.  The efficiency and effectiveness of MinHash are central to managing the scale of data processed. Its direct application justifies the citation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Jianghao Chen", "paper_title": "ChineseWebText: Large-scale high-quality chinese web text extracted with effective evaluation model", "reason": "This paper introduces ChineseWebText, a dataset used as a baseline for comparing the quality of the proposed dataset and classifier. The classifier's inclusion is important for comparing performance and establishing the advancement made by the new model.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Woosuk Kwon", "paper_title": "Efficient memory management for large language model serving with pagedattention", "reason": "This paper is highly relevant because it describes efficient memory management techniques, which are crucial for handling the massive datasets used in training LLMs. The focus on efficiency is important because the current work also emphasizes efficient processing of a massive dataset, and the techniques discussed here could have relevance to the process.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jianlv Chen", "paper_title": "Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation", "reason": "This paper introduces the BGE-M3 model, the foundation for the 0.5B parameter quality classifier used in the high-quality processing stage. This model's architecture and training methods are integral to understanding the classifier's performance and efficiency. Its use is a major part of the methodological contributions.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Raymond Li", "paper_title": "Starcoder: may the source be with you!", "reason": "This paper introduces the StarCoder dataset, a portion of which is used in the mixed dataset experiment.  The inclusion of this dataset is important for benchmarking the performance of the new dataset across multiple language types. The comparison highlights the breadth of applicability of the methodology.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Cl\u00e9mentine Fourrier", "paper_title": "Lighteval: A lightweight framework for llm evaluation", "reason": "This paper introduces the Lighteval library, which is used for model evaluation in the experiments.  The use of a standardized evaluation framework is crucial for ensuring fairness and consistency in comparing the performance of various datasets. The library allows for a direct comparison of the results to other studies.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Yuzhen Huang", "paper_title": "C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models", "reason": "This paper introduces the C-eval metric, a crucial component of the evaluation methodology for assessing the performance of Chinese language models. The direct use of this metric justifies its importance as a key component of the experimental setup.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Haonan Li", "paper_title": "CMMLU: Measuring massive multitask language understanding in chinese", "reason": "This paper introduces the CMMLU metric, which is used to evaluate the performance of LLMs on Chinese language understanding tasks.  Its inclusion demonstrates the comprehensive nature of the evaluation process, encompassing diverse metrics.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Peter Clark", "paper_title": "Think you have solved question answering? try arc, the ai2 reasoning challenge", "reason": "This paper describes the ARC challenge, one of the benchmarks used for evaluating the performance of the LLMs trained on the CCI3.0-HQ dataset. The benchmark's inclusion provides a measure of model capabilities across various reasoning tasks, thus further establishing the quality of the dataset.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Rowan Zellers", "paper_title": "HellaSwag: Can a machine really finish your sentence?", "reason": "This paper introduces the HellaSwag benchmark, one of the datasets used for evaluating the performance of LLMs trained on the CCI3.0-HQ dataset. Its inclusion shows the breadth of benchmarks used to test the model's performance across diverse question types and reasoning tasks.", "section_number": 3}]}