[{"figure_path": "https://arxiv.org/html/2502.04896/x1.png", "caption": "(a) Text-to-Image Samples", "description": "This figure displays a collection of images generated from text prompts using the Goku model.  The samples highlight the model's ability to generate diverse and detailed images from a wide range of text descriptions. These examples showcase its capacity for generating high-quality and visually appealing images.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.04896/x2.png", "caption": "(b) Text-to-Video Samples", "description": "The figure displays several video clips generated by the Goku model in response to text prompts. Each video showcases the model's ability to generate coherent and visually appealing videos based on the given text descriptions. The videos demonstrate a wide range of scenarios and visual styles.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.04896/x3.png", "caption": "Figure 1: Generated samples from Goku. Key components are highlighted in RED.", "description": "Figure 1 showcases examples of images and videos generated by the Goku model.  The text-to-image samples demonstrate Goku's ability to generate diverse and high-quality images based on textual descriptions. Similarly, the text-to-video samples highlight Goku's capability to create coherent and detailed videos from textual prompts. Key components of the pipeline responsible for generating these outputs are highlighted in red for emphasis.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.04896/extracted/6186370/figures/label_distribution_bar.png", "caption": "Figure 2: The data curation pipeline in Goku. Given a large volume of video/image data collected from Internet, we generate high-quality video/image-text pairs through a series of data filtering, captioning and balancing steps.", "description": "This figure illustrates the data processing pipeline used to create the dataset for training the Goku model.  The process begins with collecting large volumes of video and image data from the internet.  Subsequently, these raw data undergo several filtering stages to ensure high quality. This involves video and image filtering to remove low-quality or inappropriate content (e.g., NSFW content), using techniques such as aesthetic score filtering, OCR to eliminate images with excessive text, and motion analysis.  Following this, the pipeline generates captions for both images and videos utilizing a combination of Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs). These captions ensure that the text descriptions are both accurate and descriptive. The final step involves balancing the distribution of the data to correct any biases.", "section": "4. Data Curation Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.04896/x4.png", "caption": "(a) Semantic distribution of video clips.", "description": "The figure shows the distribution of video clips across different semantic categories in the training dataset.  The primary categories are displayed with their respective percentages, illustrating the relative abundance of each category in the dataset. This visualization helps in understanding the dataset's composition and the balance of various themes represented.", "section": "4. Data Curation Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.04896/x5.png", "caption": "(b) The balanced semantic distribution of subcategories.", "description": "This figure is a pie chart that visualizes the balanced distribution of subcategories within the video dataset used for training the Goku model.  The dataset was initially imbalanced, with some categories over-represented and others under-represented.  To address this, the authors implemented data balancing techniques.  This chart shows the resulting balanced distribution after applying these techniques, indicating a more even representation of diverse video content across various subcategories. Each slice of the pie chart represents a subcategory, and the size of the slice corresponds to the proportion of videos belonging to that subcategory in the balanced dataset.", "section": "4. Data Curation Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.04896/x6.png", "caption": "Figure 3: Training data distributions. The balanced semantic distribution of primary categories and subcategories are shown in (a) and (b), respectively.", "description": "Figure 3 presents the balanced distribution of video clips across different semantic categories.  Subfigure (a) shows the distribution of primary categories (e.g., human, scenery, animals, food), revealing the relative frequency of each category in the training dataset.  Subfigure (b) breaks down the data further, illustrating the distribution of subcategories within each primary category (e.g., different types of human activities, specific animal breeds, various food items). This balanced distribution is achieved through data curation techniques such as downsampling overrepresented categories and upsampling underrepresented ones.", "section": "4. Data Curation Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.04896/x7.png", "caption": "Figure 4: Samples of Goku-I2V. Reference images are presented in the leftmost columns. We omitted redundant information from the long prompts, displaying only the key details in each one. Key words are highlighted in RED.", "description": "Figure 4 showcases examples of Goku-I2V (Goku Image-to-Video), a model that generates videos conditioned on a given image and text prompt.  The figure's leftmost column displays the reference images used as input. Each row presents the corresponding text prompt (with key words highlighted in red) and the resulting video frames.  The prompts have been shortened from their original lengths to highlight only essential details, illustrating how Goku-I2V generates videos based on a combination of image and text. The videos effectively combine the reference image with the visual elements described in the text prompt.", "section": "5. Image-to-Video"}, {"figure_path": "https://arxiv.org/html/2502.04896/x8.png", "caption": "(a) Model Scaling", "description": "This figure presents ablation study results on the impact of model scaling on Goku-T2V.  The left-hand side shows video results from the Goku-T2V(2B) model, which has 2 billion parameters. The right-hand side shows video results from the Goku-T2V(8B) model, which has 8 billion parameters. The goal is to demonstrate that increasing model size (and thus increasing parameters) improves the quality of the generated videos, specifically reducing the occurrence of artifacts such as distorted objects.", "section": "5.5. Ablation Studies"}]