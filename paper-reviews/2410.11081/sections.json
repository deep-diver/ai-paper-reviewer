[{"page_end_idx": 1, "page_start_idx": 1, "section_number": 1, "section_title": "INTRODUCTION", "details": {"details": "Diffusion models have revolutionized generative AI, achieving remarkable results in various domains like image, 3D, audio, and video generation. However, their slow sampling speed, often requiring dozens to hundreds of steps, presents a significant drawback.  Existing diffusion distillation techniques, such as direct distillation, adversarial distillation, progressive distillation, and variational score distillation (VSD), aim to address this speed issue but face their own challenges.  Direct distillation is computationally expensive, adversarial distillation is complex, progressive distillation is less effective for few-step generation, and VSD can result in overly smooth samples lacking diversity at higher guidance levels.  Consistency models (CMs) offer a promising alternative, eliminating the need for supervision from diffusion model samples, bypassing the computational cost and complexities of other methods.  The introduction highlights the effectiveness of CMs in few-step generation, setting the stage for the paper's focus on simplifying, stabilizing, and scaling up the training of continuous-time CMs.", "first_cons": "Existing diffusion distillation techniques suffer from various limitations, including high computational cost (direct distillation), complexity (adversarial distillation), reduced effectiveness in few-step generation (progressive distillation), and limited diversity and struggles at high guidance levels (VSD).", "first_pros": "Diffusion models have achieved remarkable results in various generative AI tasks, revolutionizing the field.", "keypoints": ["Slow sampling speed of diffusion models (dozens to hundreds of steps)", "Challenges of existing diffusion distillation techniques (high cost, complexity, limited effectiveness, low diversity)", "Consistency models (CMs) offer significant advantages: no supervision from diffusion model samples, avoidance of computational costs, and no adversarial training", "Effectiveness of CMs in few-step generation (one or two steps)"], "second_cons": "Continuous-time CMs have faced challenges with training instability, limiting their success.", "second_pros": "Consistency models eliminate the need for supervision from diffusion model samples, avoiding computational costs and the complexities of other techniques.", "summary": "Diffusion models, while highly effective for generative AI, are hampered by slow sampling speeds.  Current distillation methods aim to improve speed but have limitations in computational cost, complexity, and sample quality.  Consistency models (CMs) offer an attractive solution by removing the need for diffusion model samples and avoiding adversarial training; however, their continuous-time versions have suffered from training instability.  This paper aims to address this instability and scale continuous-time CMs to larger model sizes."}}, {"page_end_idx": 4, "page_start_idx": 3, "section_number": 2, "section_title": "PRELIMINARIES", "details": {"details": "This section lays the groundwork for understanding consistency models by reviewing diffusion models.  It introduces two prominent formulations of diffusion models: the Energy-based Diffusion Model (EDM) and Flow Matching.  EDM simplifies the noising process by setting  \u03b1t = 1 and \u03c3t = t, leading to a straightforward training objective. Flow Matching uses differentiable coefficients \u03b1t and \u03c3t, employing time derivatives to define the training objective. Both methods ultimately aim to learn the reverse of a noising process which adds progressively more Gaussian noise to data samples. The section then introduces consistency models (CMs), which directly map noisy inputs to clean data samples in a single step, sidestepping the need for numerous sampling steps like standard diffusion models.  Two types of CMs are discussed: discrete-time CMs which utilize a fixed time step \u0394t, introducing potential discretization errors, and continuous-time CMs which approach the continuous limit (\u0394t \u2192 0), promising better accuracy but facing challenges in training stability. The core idea of CMs lies in enforcing consistency between outputs at adjacent time steps, with training objectives based on the distance (e.g., L2 loss, LPIPS) between the model's predictions at these points. The section concludes by highlighting the key advantages of continuous-time CMs over their discrete-time counterparts in terms of avoiding discretization errors. ", "first_cons": "Discrete-time consistency models introduce discretization errors due to the fixed time step \u0394t, potentially leading to suboptimal sample quality.", "first_pros": "Consistency models offer significant advantages over diffusion models and other distillation methods. They eliminate the need for supervision from diffusion model samples, avoiding the computational cost of generating synthetic datasets.  CMs bypass adversarial training, thus sidestepping its difficulties.  Continuous-time CMs, in theory, offer more accurate supervision and avoid discretization issues.", "keypoints": ["Two formulations of diffusion models are presented: EDM (\u03b1t = 1, \u03c3t = t) and Flow Matching (differentiable \u03b1t and \u03c3t).", "Consistency models (CMs) directly map noisy input to clean data, significantly speeding up sampling.", "Discrete-time CMs use a fixed time step \u0394t, while continuous-time CMs approach \u0394t \u2192 0 for better accuracy, but face stability challenges during training."], "second_cons": "Continuous-time CMs have faced challenges with training instability, limiting their effectiveness.", "second_pros": "Continuous-time CMs, in theory, avoid discretization errors by taking the limit as the time step approaches zero.", "summary": "This section provides a concise overview of diffusion models and introduces consistency models (CMs) as a faster alternative for generating samples. It details two formulations of diffusion models (EDM and Flow Matching) and two types of CMs (discrete-time and continuous-time), highlighting the advantages and challenges of each approach.  The core concept of CMs, training using consistency between predictions at adjacent time steps, is explained, and the benefits of using continuous-time CMs are emphasized."}}, {"page_end_idx": 6, "page_start_idx": 4, "section_number": 3, "section_title": "SIMPLIFYING CONTINUOUS-TIME CONSISTENCY MODELS", "details": {"details": "This section focuses on simplifying the training of continuous-time consistency models (CMs).  The authors introduce TrigFlow, a new formulation unifying existing diffusion models and flow matching methods. This simplification reveals the root causes of instability in CM training and allows for improved time conditioning, network architecture (adaptive group normalization), and training objectives (adaptive weighting and progressive annealing).  The improvements enable training CMs at an unprecedented scale, achieving 1.5 billion parameters on ImageNet 512x512.  The TrigFlow formulation uses trigonometric functions (cos(t) and sin(t)), simplifying previous formulations with complex arithmetic relationships to t and \u03c3a (standard deviation of the data distribution).  Analyzing the instability, they highlight the importance of stable tangent functions in the training objective and propose using positional time embeddings instead of Fourier embeddings to mitigate instability.  Adaptive double normalization is introduced to address issues with AdaGN layers negatively impacting CM training, offering a more stable training approach.  Further improvements to the training objective include adaptive weighting of key terms and progressive annealing to improve stability and scalability.  These combined improvements drastically elevate the performance and scalability of continuous-time CMs.", "first_cons": "The complex arithmetic relationships in previous CM formulations made theoretical analysis difficult and contributed to training instability.  TrigFlow addresses this simplification but needs further validation on various tasks and datasets beyond the results presented.", "first_pros": "TrigFlow significantly simplifies the formulation of diffusion models and CMs, leading to improved training stability and scalability. It unifies previous EDM and Flow Matching methods, creating a more comprehensive framework.", "keypoints": ["TrigFlow simplifies diffusion model and CM parameterization using trigonometric functions (cos(t), sin(t)).", "Improved time conditioning using positional time embeddings instead of Fourier embeddings.", "Adaptive group normalization in the network architecture enhances training stability.", "Adaptive weighting and progressive annealing in the training objective improve stability and scalability.", "Achieved 1.5 billion parameters on ImageNet 512x512, a significant scale increase for CMs."], "second_cons": "While the proposed improvements significantly improve training stability, the authors acknowledge that training continuous-time CMs remains a challenging task.  Further research is necessary to ensure robustness across different datasets and model architectures.", "second_pros": "The improvements enable training of continuous-time CMs at an unprecedented scale (1.5 billion parameters) and achieve comparable results to discrete-time methods with significantly less computational cost during sampling.  The approach is theoretically motivated and rigorously justified.", "summary": "This section details significant improvements to the training of continuous-time consistency models (CMs) by introducing TrigFlow, a simplified framework unifying previous methods.  This leads to key advancements in diffusion process parameterization, network architecture, and training objectives, resulting in more stable and scalable CM training, ultimately enabling models with 1.5 billion parameters on ImageNet 512x512 and significantly narrowing the FID gap compared to state-of-the-art diffusion models.  Key improvements include using positional time embeddings, adaptive group normalization, adaptive weighting, and progressive annealing."}}, {"page_end_idx": 7, "page_start_idx": 6, "section_number": 4, "section_title": "STABILIZING CONTINUOUS-TIME CONSISTENCY MODELS", "details": {"details": "Training continuous-time consistency models (CMs) has historically been plagued by instability, resulting in inferior performance compared to their discrete-time counterparts.  This section introduces several improvements to address this instability.  The core idea is to stabilize the computation of the tangent function, a key component in the continuous-time CM training objective. This is achieved through improvements in parameterization using TrigFlow, a novel formulation that simplifies the model representation, and network architecture changes.  TrigFlow unifies previous approaches like EDM and Flow Matching, simplifying the mathematical relationships and making the training process more stable.  To further stabilize training, adaptive group normalization, re-formulation of the training objective with adaptive weighting and normalization, and progressive annealing of key terms are introduced. These techniques collectively enhance the stability and scalability of the training process, enabling the training of larger CMs (reaching 1.5 billion parameters on ImageNet 512x512).", "first_cons": "While the section introduces several improvements, it doesn't quantify the computational cost associated with these changes. The improvements might introduce additional computational overhead that could offset some of the benefits.", "first_pros": "The section provides a clear and systematic approach to stabilize the training of continuous-time CMs. The proposed modifications, backed by theoretical justifications, lead to significant improvements in training stability and scalability, which are crucial for training large-scale generative models.", "keypoints": ["TrigFlow simplifies the model parameterization, leading to a more stable training process.", "Adaptive group normalization enhances the stability of the network architecture.", "Re-formulated training objective with adaptive weighting improves training stability and scalability.", "Progressive annealing further stabilizes the training process.", "The improvements allow for training of 1.5B parameter models on ImageNet 512x512."], "second_cons": "The effectiveness of the proposed techniques is primarily demonstrated empirically.  A more rigorous theoretical analysis of the impact of each improvement on stability and convergence would strengthen the claims.", "second_pros": "The section successfully addresses the major challenge of instability in training continuous-time CMs, making them a viable alternative to discrete-time models for large-scale generative AI.", "summary": "This section tackles the instability issues in training continuous-time consistency models by introducing TrigFlow, a unified framework simplifying model parameterization and improving training stability.  Further improvements include adaptive group normalization, a re-formulated training objective with adaptive weighting, and progressive annealing, which collectively enable training of significantly larger models (up to 1.5 billion parameters) while achieving state-of-the-art results on ImageNet 512x512."}}, {"page_end_idx": 10, "page_start_idx": 8, "section_number": 5, "section_title": "SCALING UP CONTINUOUS-TIME CONSISTENCY MODELS", "details": {"details": "This section details experiments on scaling up continuous-time consistency models (sCMs) to larger scales and datasets.  The authors train sCMs on CIFAR-10, ImageNet 64x64, and ImageNet 512x512, reaching an unprecedented scale of 1.5 billion parameters. They compare two training approaches: consistency training (sCT) and consistency distillation (sCD).  sCD, which leverages a pre-trained teacher model, demonstrates superior scalability, achieving results comparable to the teacher model with less than 10% of the computational cost.  Analysis shows that sample quality improves predictably with increased compute, and sCD's FID score narrows the gap with state-of-the-art diffusion models to within 10%.  The authors also compare sCMs to variational score distillation (VSD), highlighting sCMs' superior diversity and compatibility with guidance at higher levels.  They address the challenges of large-scale training by incorporating techniques like half-precision (FP16) computation, Flash Attention (for memory efficiency), and a JVP rearrangement for improved numerical stability in calculating the tangent function, which is crucial for continuous-time CM training.  The experiments show that sCMs achieve remarkable sample quality with fewer sampling steps, significantly reducing computational needs.", "first_cons": "The consistency training (sCT) approach is less efficient than consistency distillation (sCD) at larger scales and higher resolutions.  While sCT shows promising results at smaller scales, its performance degrades with increased model size, suggesting that a pre-trained model (as used in sCD) might be essential for efficient large-scale training.", "first_pros": "The study demonstrates significant improvements in sample quality with increased compute for continuous-time consistency models.  Scaling to 1.5 billion parameters, while achieving comparable quality to state-of-the-art diffusion models with substantially lower computational requirements is a substantial achievement.", "keypoints": ["sCMs achieve FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64x64, and 1.88 on ImageNet 512x512, narrowing the gap with the best existing diffusion models to within 10%.", "sCD (consistency distillation) is shown to scale more effectively than sCT (consistency training) with increased compute and model size.", "sCMs using 2-step sampling achieve sample quality comparable to state-of-the-art models using significantly less compute.", "sCMs use 1.5 billion parameters, representing a substantial scale-up in model size for consistency models.", "Techniques like FP16, Flash Attention, and JVP rearrangement were crucial to enable stable training at this scale."], "second_cons": "The reliance on a pre-trained teacher model in the sCD approach introduces a dependence on external resources. The performance gain might be sensitive to the quality of the teacher model and its hyperparameters.", "second_pros": "The paper rigorously analyzes the advantages of continuous-time CMs over discrete-time variants, showcasing a continuous improvement in sample quality as the distance between adjacent time steps decreases.  Furthermore, sCMs are demonstrated to be more diverse and robust to high guidance levels compared to VSD, highlighting their superiority in certain applications.", "summary": "This section presents a comprehensive evaluation of scaling up continuous-time consistency models (sCMs), focusing on two training methods: consistency training (sCT) and consistency distillation (sCD).  The results show that sCMs scale effectively with increased compute, achieving state-of-the-art performance on various datasets with up to 1.5 billion parameters, significantly reducing sampling steps compared to existing diffusion models.  Consistency distillation (sCD) exhibits superior scalability to consistency training (sCT), especially at larger scales and higher resolutions, highlighting the benefits of leveraging pre-trained models for improved efficiency.   The study also shows superior performance to VSD and addresses the challenges of large-scale training with innovative techniques."}}]