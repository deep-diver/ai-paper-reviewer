{"references": [{" publication_date": "2021", "fullname_first_author": "Yang Song", "paper_title": "Score-based generative modeling through stochastic differential equations", "reason": "This paper is foundational for score-based diffusion models and provides the theoretical background for many of the techniques used in the current work. It introduces the concept of solving stochastic differential equations to generate samples, which is crucial for understanding the continuous-time consistency models discussed in the paper.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Yang Song", "paper_title": "Maximum likelihood training of score-based diffusion models", "reason": "This paper introduces a maximum likelihood approach to training score-based diffusion models, providing a more efficient and stable training method than previous techniques. This is highly relevant to the current work, which aims to improve the training of continuous-time consistency models.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yang Song", "paper_title": "Consistency models", "reason": "This paper introduces consistency models (CMs), which are the focus of the current work. CMs offer a faster and more efficient alternative to diffusion models for generating samples, and this paper provides the foundational theoretical framework for CMs.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yang Song", "paper_title": "Improved techniques for training consistency models", "reason": "This paper proposes several techniques to improve the training of consistency models, which are directly relevant to the current work.  The authors address the challenges of training instability and propose solutions that are implemented and further developed in the current work.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This is a seminal work on diffusion models that established the theoretical foundation for many subsequent developments. The current paper builds upon this foundation by introducing a new formulation that unifies previous approaches and improves the training of continuous-time consistency models.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Tero Karras", "paper_title": "Analyzing and improving the training dynamics of diffusion models", "reason": "This paper provides a detailed analysis of the training dynamics of diffusion models and proposes techniques to improve their stability and efficiency. This is highly relevant to the current work, which focuses on improving the stability and scalability of training continuous-time consistency models.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Tero Karras", "paper_title": "Elucidating the design space of diffusion-based generative models", "reason": "This paper provides a comprehensive analysis of the design space of diffusion-based generative models, which is highly relevant to the current work because it provides insights into the choices of hyperparameters and architectures for diffusion models. The current paper uses many of the insights from this work to improve its own model.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Yaron Lipman", "paper_title": "Flow matching for generative modeling", "reason": "This paper introduces a new approach to generative modeling based on flow matching, which provides an alternative formulation to energy-based models. This approach is unified with the EDM approach in the current work, leading to a more general and efficient framework for training continuous-time consistency models.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Tim Salimans", "paper_title": "Progressive distillation for fast sampling of diffusion models", "reason": "This paper introduces progressive distillation as a technique for fast sampling of diffusion models.  The current work builds upon this idea by using a similar approach for training continuous-time consistency models, showing a similar improvement in sampling speed and quality.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Prafulla Dhariwal", "paper_title": "Diffusion models beat GANs on image synthesis", "reason": "This paper demonstrates the superiority of diffusion models over GANs in image synthesis, highlighting the potential of diffusion models for high-quality image generation. The current work builds upon this foundation by improving the efficiency and stability of training continuous-time consistency models.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Zhengcong Geng", "paper_title": "Consistency models made easy", "reason": "This paper provides a simplified approach to training consistency models. The current work extends upon this work by using a new formulation that further simplifies training and improves stability, leading to larger model sizes.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Michael S Albergo", "paper_title": "Stochastic interpolants: A unifying framework for flows and diffusions", "reason": "This paper provides a theoretical framework for understanding the relationship between flows and diffusions, which is used in the current work to simplify the formulation of diffusion models. This simplification leads to improved training stability and scalability.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Michael Samuel Albergo", "paper_title": "Building normalizing flows with stochastic interpolants", "reason": "This paper introduces a novel method for building normalizing flows using stochastic interpolants, which is a key component of the TrigFlow framework introduced in the current work.  This method contributes to improved training stability and scalability.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Tri Dao", "paper_title": "Flashattention: Fast and memory-efficient exact attention with io-awareness", "reason": "Flash Attention is crucial for the efficient processing of large-scale data in the training process. This paper presents the fundamental Flash Attention algorithm and its memory efficient implementation.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Tri Dao", "paper_title": "Flashattention-2: Faster attention with better parallelism and work partitioning", "reason": "This paper is an improvement on the Flash Attention algorithm.  Flash Attention is used in the current work for memory-efficient training of large models, thus this paper is highly relevant.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Patrick Esser", "paper_title": "Scaling rectified flow transformers for high-resolution image synthesis", "reason": "This paper explores scaling rectified flow transformers for high-resolution image synthesis. The current work uses a similar approach, scaling up continuous-time consistency models to unprecedented sizes and achieving state-of-the-art performance.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Yao Teng", "paper_title": "Diffusion mamba for efficient high-resolution image synthesis", "reason": "This paper proposes a novel architecture for efficient high-resolution image synthesis. The current paper uses similar ideas in designing its own model, which leads to improvements in efficiency and scalability.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Zhengyi Wang", "paper_title": "Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation", "reason": "This paper demonstrates the success of variational score distillation for 3D generation. The current paper compares the performance of its method to VSD, highlighting the advantages of the proposed approach.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Kaiwen Zheng", "paper_title": "DPM-solver-v3: Improved diffusion ode solver with empirical model statistics", "reason": "This paper presents an improved version of the DPM-Solver algorithm for sampling diffusion models.  The current paper uses DPM-Solver for sampling its consistency models, thus this paper is highly relevant for understanding the sampling process.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Hongkai Zheng", "paper_title": "Fast training of diffusion models with masked transformers", "reason": "This paper focuses on accelerating the training of diffusion models using masked transformers. This is relevant to the current paper because it explores efficient training techniques for large-scale models, a key concern in the current work.", "section_number": 5}]}