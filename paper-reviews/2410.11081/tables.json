[{"figure_path": "2410.11081/tables/table_9_0.html", "caption": "Table 1: Sample quality on unconditional CIFAR-10 and class-conditional ImageNet 64\u00d764.", "description": "Table 1 compares the FID scores and number of function evaluations (NFEs) for various generative models on unconditional CIFAR-10 and class-conditional ImageNet 64x64 datasets.", "section": "5 Experiments"}, {"figure_path": "2410.11081/tables/table_10_0.html", "caption": "Table 2: Sample quality on class-conditional ImageNet 512\u00d7512. Our reimplemented teacher diffusion model based on EDM2 (Karras et al., 2024) but with modifications in Sec. 4.1.", "description": "Table 2 presents the sample quality results (FID scores and number of function evaluations (NFEs)) of different generative models, including diffusion models, GANs, masked models, and consistency models (both training and distillation), on the class-conditional ImageNet 512x512 dataset.", "section": "5 Experiments"}, {"figure_path": "2410.11081/tables/table_17_0.html", "caption": "Table 1: Sample quality on unconditional CIFAR-10 and class-conditional ImageNet 64\u00d764.", "description": "Table 1 compares the sample quality of various generative models on CIFAR-10 and ImageNet 64x64 datasets, measured by FID and number of function evaluations.", "section": "5 Experiments"}, {"figure_path": "2410.11081/tables/table_29_0.html", "caption": "Table 3: Training settings of all models and training algorithms on ImageNet 64\u00d764 dataset.", "description": "Table 3 shows the training hyperparameters and settings for diffusion models, consistency models (both training and distillation), and variational score distillation models on ImageNet 64x64.", "section": "5.2 Experiments"}, {"figure_path": "2410.11081/tables/table_30_0.html", "caption": "Table 4: Training settings of all models and training algorithms on ImageNet 512x512 dataset.", "description": "Table 4 presents the training settings of all models and training algorithms used in the experiments on the ImageNet 512x512 dataset, detailing various hyperparameters and training configurations for different model sizes.", "section": "5 SCALING UP CONTINUOUS-TIME CONSISTENCY MODELS"}, {"figure_path": "2410.11081/tables/table_31_0.html", "caption": "Table 5: Evaluation of sample quality of different models on ImageNet 512x512 dataset. Results of EDM2 (Karras et al., 2024) are with EDM parameterization and the original AdaGN layer. The FDDINOv2 in EDM2 are obtained by tuned EMA rate, which is different from our EMA rates that are tuned for FID scores.", "description": "Table 5 presents a comparison of the sample quality, measured by FID and FDDINOv2, of various models (diffusion models, consistency models trained with SCT and sCD, and consistency models trained with sCD and adaptive VSD) on the ImageNet 512x512 dataset, showing the impact of model size and sampling techniques.", "section": "5.2 Experiments"}, {"figure_path": "2410.11081/tables/table_32_0.html", "caption": "Table 6: Ablation of adaptive VSD and sCD on ImageNet 512\u00d7512 dataset with model size M.", "description": "Table 6 shows the ablation study comparing the performance of VSD, sCD, and a combination of both methods on ImageNet 512x512 dataset using model size M, evaluating FID and FDDINOV2 scores for both 1-step and 2-step sampling.", "section": "5.2 Experiments"}, {"figure_path": "2410.11081/tables/table_32_1.html", "caption": "Table 7: Evaluation of sample quality of different models on ImageNet 64x64 dataset.", "description": "Table 7 presents a comparison of sample quality, measured by FID scores, for various models (diffusion models and consistency models trained with both sCT and sCD) on the ImageNet 64x64 dataset, showcasing the impact of different training methods and model sizes on sample quality.", "section": "5 EXPERIMENTS"}]