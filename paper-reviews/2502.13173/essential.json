{"importance": "This paper is vital for enhancing LLM reasoning. **ThinkPO's efficiency in leveraging existing data** offers a cost-effective solution, aligning with the trend towards resource-efficient AI. It inspires further research into preference optimization and continual learning strategies, **overcoming performance bottlenecks and improving real-world applicability**.", "summary": "ThinkPO improves LLM reasoning by preferring longer CoT, boosting performance without new data.", "takeaways": ["ThinkPO enhances long CoT reasoning in LLMs using preference optimization.", "The method boosts reasoning performance by favoring longer, detailed solutions.", "It effectively leverages existing SFT data, improving models like DeepSeek-R1-Distill-Qwen-7B."], "tldr": "Supervised Fine-Tuning (SFT) enhances LLM reasoning, but faces data costs and performance plateaus. To address these challenges, the paper introduces **Thinking Preference Optimization (ThinkPO)**, a post-SFT method. **ThinkPO enhances long chain-of-thought reasoning without needing new CoT responses.** It uses easily available short CoT responses as rejected answers and long CoT responses as chosen ones, encouraging the model to favor longer outputs. \n\nThinkPO significantly improves SFT-ed model\u2019s reasoning performance. **It increases math reasoning accuracy and output length.** The method continually boosts the performance of distilled SFT models. For example, the method increased DeepSeek-R1-Distill-Qwen-7B's performance on MATH500 from 87.4% to 91.2%. **ThinkPO maximizes the value of existing long reasoning data and enhances reasoning performance, requiring no additional CoT responses.**", "affiliation": "case.edu", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "2502.13173/podcast.wav"}