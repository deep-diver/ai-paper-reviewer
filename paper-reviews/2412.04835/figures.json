[{"figure_path": "https://arxiv.org/html/2412.04835/extracted/6047317/figs/front_fig_2.png", "caption": "Figure 1: Representation-Aligned Preference-based Learning (RAPL), is an observation-only method for learning visual robot rewards from significantly less human preference feedback.\n(center) Unlike traditional reinforcement learning from human feedback, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-user\u2019s visual representation.\nThe aligned representation is used to construct an optimal transport-based visual reward for aligning the robot\u2019s visuomotor policy. (left) Before alignment, the robot frequently picks up a bag of chips by squeezing the middle, risking damage to the contents. (right) After alignment with our RAPL reward, the robot adheres to the end-user\u2019s preference and picks up the bag by its edges.", "description": "Figure 1 illustrates the Representation-Aligned Preference-based Learning (RAPL) method.  The core idea is to efficiently learn visual rewards for robots using minimal human feedback.  Instead of directly providing feedback on robot actions, RAPL leverages human preferences to fine-tune a pre-trained vision encoder. This alignment ensures the robot's visual representation better matches the user's perspective.  An optimal transport-based visual reward is then created using this aligned representation, enabling more effective policy alignment.  The figure uses the example of a robot picking up a bag of chips to highlight the difference: before alignment (left), the robot grasps the bag improperly, risking damage; after alignment with RAPL (right), the robot grasps the bag correctly, demonstrating successful alignment with the user's preferences.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.04835/extracted/6047317/figs/sim_env_all.png", "caption": "Figure 2: X-Magical & IsaacGym tasks.\nTop row are high-reward behaviors and bottom row are low-reward behaviors according to the human\u2019s preferences.", "description": "This figure shows example images from the X-Magical and IsaacGym simulated environments used in the paper's experiments.  The top row displays example robot behaviors that are considered high-reward by human participants, while the bottom row illustrates low-reward behaviors. This highlights the difference in quality or desirability of the robot's actions from the human's perspective. These examples are used to train and evaluate the reward function in the proposed Representation-Aligned Preference-based Learning (RAPL) method.", "section": "5.2 Results: From Representation to Policy Alignment"}, {"figure_path": "https://arxiv.org/html/2412.04835/x1.png", "caption": "Figure 3: X-Magical. (left & right) examples of preferred and disliked videos for each task. (center) reward associated with each video under each method. RAPL\u2019s predicted reward follows the GT pattern: low reward when the behavior are disliked and high reward when the behavior are preferred. RLHF and TCC assign high reward to disliked behavior (e.g., (D)).", "description": "Figure 3 presents a comparison of reward prediction methods in the X-Magical simulation environment.  The left and right panels showcase example videos deemed preferred and disliked, respectively, by a human evaluator for two distinct tasks ('Avoiding' and 'Grouping').  The center panel displays the reward assigned to each video by four different methods: Ground Truth (GT), Representation-Aligned Preference-based Learning (RAPL), Reinforcement Learning from Human Feedback (RLHF), and Temporal Cycle Consistency (TCC).  The key finding is that RAPL's reward predictions closely match the human preferences (high rewards for liked videos, low for disliked), unlike RLHF and TCC which sometimes assign high rewards to disliked behaviors.", "section": "5.2 Results: From Representation to Policy Alignment"}, {"figure_path": "https://arxiv.org/html/2412.04835/x2.png", "caption": "Figure 4: X-Magical. Policy evaluation success rate during policy learning. Colored lines are the mean and variance of the evaluation success rate. RAPL can match GT in the avoiding task and outperforms baseline visual rewards in grouping task.", "description": "This figure displays the results of policy evaluation success rates during the training process of different reward learning methods in the X-Magical simulation environment.  The plot shows the mean and standard deviation of success rates across multiple trials. The goal is to compare RAPL to a ground truth reward (GT), RLHF (Reinforcement Learning from Human Feedback), and TCC (Temporal Cycle Consistency) methods. The results demonstrate that RAPL achieves comparable performance to the ground truth in the 'avoiding' task and outperforms the baseline methods (RLHF and TCC) in the 'grouping' task.  This highlights RAPL's effectiveness in learning visual rewards for robotic policy alignment.", "section": "5 Reinforcement Learning in Simulation with RAPL"}, {"figure_path": "https://arxiv.org/html/2412.04835/x3.png", "caption": "Figure 5: Manipulation: Reward Prediction. \n(center) Expert, preferred, and disliked video demo.\n(left) Reward of each video under each method. RAPL\u2019s predicted reward follows the GT pattern. RLHF assigns high reward to disliked behavior.\n(right) OT coupling matrix for each visual representation. Columns are embedded frames of expert demo. Rows of top matrices are embedded frames of preferred demo; rows of bottom matrices are embedded frames of disliked demo. Peaks exactly along the diagonal indicate that the frames of the two videos are aligned in the latent space; uniform values in the matrix indicate that the two videos cannot be aligned (i.e., all frames are equally \u201csimilar\u201d to the next). RAPL exhibits the diagonal peaks for expert-and-preferred and uniform for expert-and-disliked, while baselines show diffused values no matter the videos being compared.", "description": "Figure 5 demonstrates the effectiveness of RAPL in learning a visual reward function that aligns with human preferences.  The figure shows a comparison of RAPL against several baseline methods (GT, RLHF, MVP-OT, TCC-OT).  Three video demonstrations are shown: an expert demonstration, a preferred demonstration, and a disliked demonstration. The left panel shows the reward assigned to each video by each method.  RAPL correctly assigns higher rewards to preferred videos and lower rewards to disliked videos, unlike RLHF which assigns high rewards to disliked videos. The right panel displays optimal transport (OT) coupling matrices for each method.  The OT matrices visualize the similarity between the expert video and the preferred/disliked videos in the learned visual representation space.  RAPL shows clear diagonal peaks in the OT matrix for the expert-preferred comparison (high similarity), while showing uniform values for expert-disliked (low similarity). In contrast, the baseline methods show diffuse patterns, indicating poor alignment and inability to distinguish preferred from disliked videos.", "section": "5.2 Results: From Representation to Policy Alignment"}, {"figure_path": "https://arxiv.org/html/2412.04835/x4.png", "caption": "Figure 6: (Left) Manipulation: Qualitative RLHF Comparison. We visualize the attention map for RLHF-150 demos, RLHF-300 demos, and RAPL with 150 demos for both Franka and Kuka (cross-embodiment). Each entry shows two observations from respective demonstration set with the attention map overlaid. Bright yellow areas indicate image patches that contribute most to the final embedding; darker purple patches indicate less contribution. \u03d5R\u2062L\u2062H\u2062F\u2212150subscriptitalic-\u03d5\ud835\udc45\ud835\udc3f\ud835\udc3b\ud835\udc39150\\phi_{RLHF-150}italic_\u03d5 start_POSTSUBSCRIPT italic_R italic_L italic_H italic_F - 150 end_POSTSUBSCRIPT is biased towards paying attention to irrelevant areas that can induce spurious correlations; in contrast RAPL learns to focus on the task-relevant objects and the goal region. \u03d5R\u2062L\u2062H\u2062F\u2212300subscriptitalic-\u03d5\ud835\udc45\ud835\udc3f\ud835\udc3b\ud835\udc39300\\phi_{RLHF-300}italic_\u03d5 start_POSTSUBSCRIPT italic_R italic_L italic_H italic_F - 300 end_POSTSUBSCRIPT\u2019s attention is slightly shifted to objects but still pays high attention to the robot embodiment. (Right) Manipulation: Quantitative RLHF Comparison. RAPL outperforms RLHF by 75%percent7575\\%75 % with 50%percent5050\\%50 % less human preference data.", "description": "Figure 6 presents a comparison of RAPL with RLHF for robot manipulation tasks. The left part visualizes attention maps for different models (RLHF-150, RLHF-300, and RAPL) across two robot embodiments (Franka and Kuka), highlighting the focus areas in image observations.  It demonstrates that RLHF pays attention to irrelevant areas, while RAPL focuses on task-relevant objects and goals. The right part shows a quantitative comparison, indicating that RAPL outperforms RLHF in terms of success rate while needing significantly less human preference data.", "section": "5.2 Results: From Representation to Policy Alignment"}, {"figure_path": "https://arxiv.org/html/2412.04835/x5.png", "caption": "Figure 7: Manipulation: Policy Learning. Success rate during robot policy learning under each visual reward.", "description": "This figure displays the success rate of robot policy learning across different visual reward methods.  It shows how the success rate changes over time (epochs) during training for different methods: GT (ground truth), RAPL, RLHF, MVP-OT, TCC-OT, Fine-Tuned-MVP-OT, and R3M-OT. The x-axis represents the number of training epochs, and the y-axis shows the success rate in achieving the task.  This allows for a comparison of the learning efficiency and overall performance of each reward method in a robot manipulation task.", "section": "5.2 Results: From Representation to Policy Alignment"}, {"figure_path": "https://arxiv.org/html/2412.04835/x6.png", "caption": "Figure 8: Manipulation: Cross-Embodiment Reward Transfer.\n(center) Expert video of Franka robot, preferred video of Kuka, and disliked Kuka video.\n(left) Predicted reward under each method trained only on Franka video preferences. RAPL\u2019s reward generalizes to the Kuka robot and follows the GT pattern.\n(right) OT plan for each visual representation shown in the same style as in Figure\u00a05.\nRAPL\u2019s representation shows a diagonal OT plan for expert-and-preferred demos vs. a uniform for expert-and-disliked, while baselines show inconsistent plan patterns.", "description": "Figure 8 demonstrates the cross-embodiment generalization capability of RAPL.  It shows three videos: an expert demonstration (Franka robot), a preferred Kuka robot demonstration, and a disliked Kuka robot demonstration. The left panel displays the reward prediction for each method (trained only on Franka data), revealing that RAPL accurately predicts preferences even for the different Kuka robot. The right panel visualizes optimal transport (OT) plans for various visual representations, highlighting RAPL's ability to consistently show diagonal patterns (for expert/preferred video pairs) and uniform patterns (for expert/disliked video pairs). In contrast, other methods exhibit inconsistent patterns.", "section": "5.2 Results: From Representation to Policy Alignment"}, {"figure_path": "https://arxiv.org/html/2412.04835/x7.png", "caption": "Figure 9: X-Magical: Cross-Embodiment Reward Transfer. RAPL discriminates preferred and disliked videos of novel robots.", "description": "Figure 9 demonstrates the zero-shot generalization capability of the proposed RAPL method across different robot embodiments in the X-Magical environment.  It showcases RAPL's ability to maintain its ability to discriminate between preferred and disliked robot behaviors even when the robot used to generate the preference data differs from the robot being evaluated.  This highlights RAPL's robustness and generalizability beyond the specific robot used during the training phase of the visual reward model.", "section": "5.3 Results: Zero-Shot Generalization Across Robot Embodiments"}, {"figure_path": "https://arxiv.org/html/2412.04835/x8.png", "caption": "Figure 10: Cross-Embodiment Policy Learning. Policy evaluation success rate during policy learning. Colored lines are the mean and variance of the evaluation success rate. RAPL achieves a comparable success rate compared to GT with high learning efficiency, and outperforms all baselines.", "description": "Figure 10 presents the results of a policy learning experiment designed to evaluate the effectiveness of different visual reward methods in cross-embodiment scenarios.  The experiment measured the success rate of each policy during training.  The success rate is defined as the percentage of times the robot successfully completed the task based on the evaluation criterion. The x-axis shows the number of training epochs, while the y-axis represents the success rate. The plot shows that the RAPL (Representation-Aligned Preference-based Learning) method achieves a success rate comparable to the ground truth (GT), demonstrating high learning efficiency.  Moreover, RAPL significantly outperforms all the baseline methods (RLHF, MVP-OT, TCC-OT, Fine-tuned MVP-OT, R3M-OT, and ImageNet-OT), indicating that RAPL is superior in learning effective visual rewards in cross-embodiment situations.", "section": "5.3 Results: Zero-Shot Generalization Across Robot Embodiments"}, {"figure_path": "https://arxiv.org/html/2412.04835/x9.png", "caption": "Figure 11: Diffusion Policy Alignment Results. (Top) The pre-trained visuomotor policy exhibits undesired behaviors: grasping the interior of the cup (left), crushing the chips (middle), and making contact with the tines of the fork and dropping it out of the bowl (right). (Bottom) After alignment using RAPL rewards, the robot\u2019s behaviors are aligned with the end-user\u2019s preferences.", "description": "This figure shows the results of aligning a pre-trained visuomotor policy using the RAPL method. The top row displays the robot's undesired behaviors before alignment: grasping the inside of a cup, crushing a bag of chips, and dropping a fork. The bottom row shows the improved behavior after applying RAPL rewards, demonstrating alignment with user preferences by correctly grasping the cup's handle, bag's edge and the fork's handle.", "section": "6.2 Results"}]