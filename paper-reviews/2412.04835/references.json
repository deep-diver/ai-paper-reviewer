{"references": [{"fullname_first_author": "Christiano P", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-XX-XX", "reason": "This paper introduces reinforcement learning from human feedback (RLHF), a foundational concept for aligning AI systems with human preferences, which is directly addressed and extended upon in the current work."}, {"fullname_first_author": "Abbeel P", "paper_title": "Apprenticeship learning via inverse reinforcement learning", "publication_date": "2004-XX-XX", "reason": "This seminal work on inverse reinforcement learning (IRL) lays the groundwork for inferring reward functions from expert demonstrations, a crucial element in the proposed RAPL method."}, {"fullname_first_author": "Ziebart BD", "paper_title": "Maximum entropy inverse reinforcement learning", "publication_date": "2008-XX-XX", "reason": "This paper presents a maximum entropy approach to IRL, which addresses issues of reward ambiguity and provides a theoretical basis for many IRL algorithms, including those used in the context of visual rewards."}, {"fullname_first_author": "Dadashi R", "paper_title": "Primal wasserstein imitation learning", "publication_date": "2021-XX-XX", "reason": "This work utilizes optimal transport for imitation learning, which is directly applied in RAPL for visual reward construction and provides a framework to efficiently measure similarity between distributions in high dimensional spaces."}, {"fullname_first_author": "Rafailov R", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-XX-XX", "reason": "This recent work addresses the challenge of expensive human feedback in RLHF by proposing direct preference optimization (DPO), which is relevant as RAPL also seeks to minimize human feedback requirements."}]}