[{"content": "| Method | Franka Group | Franka Clutter | Kuka Group |\n|---|---|---|---| \n| **RAPL** <span style=\"color:#FF8000;\">&#x27A1;</span> | **0.59** | **0.61** | **0.47** |\n| **RLHF** <span style=\"color:#BDA6FF;\">&#x27A1;</span> | 0.38 | 0.26 | 0.31 |\n| **MVP-OT** <span style=\"color:#193D85;\">&#x27A1;</span> | -0.1 | 0.08 | 0.02 |\n| **FT-MVP-OT** <span style=\"color:#54CCFF;\">&#x27A1;</span> | 0.19 | 0.11 | 0.02 |\n| **ImNet-OT** <span style=\"color:#530100;\">&#x27A1;</span> | -0.09 | -0.02 | 0.12 |\n| **R3M-OT** <span style=\"color:#7A287C;\">&#x27A1;</span> | 0.03 | -0.17 | -0.14 |", "caption": "Table 1:  Spearman\u2019s rank correlation coefficient between the GT reward and each learned visual reward.", "description": "This table presents the Spearman's rank correlation coefficients, which measure the monotonic relationship between the ground truth reward and the rewards predicted by different methods.  Higher coefficients indicate stronger agreement between the predicted and actual rewards.  The methods compared include RAPL (the proposed method), RLHF (Reinforcement Learning from Human Feedback), several variations employing optimal transport (OT) with different visual representation backbones, such as MVP-OT and Fine-Tuned-MVP-OT,  as well as ImageNet-OT and R3M-OT.  The correlation is calculated separately for three tasks: Franka Group, Franka Clutter, and Kuka Group, reflecting different robotic manipulation scenarios.", "section": "5.2 Results: From Representation to Policy Alignment"}, {"content": "|                | Ref. | GT   | RAPL | RLHF | RLHF-L | TCC-OT | R3M-OT | MVP-OT |\n|----------------|------|------|------|------|--------|--------|--------|--------|\n| **Behavior Alignment Score (\u2191)** |      |      |      |      |        |        |        |        |\n| **Cup**         | 0.2  | 0.7  | **0.8** | 0.3  | 0.7    | 0.4    | 0.4    | 0.5    |\n| **Fork**        | 0.2  | 0.7  | **0.5** | 0.1  | 0.4    | 0.1    | 0.0    | 0.2    |\n| **Bag**         | 0.4  | 0.6  | **0.7** | 0.4  | 0.4    | 0.6    | 0.1    | 0.4    |", "caption": "Table 2: Hardware Experiments: Behavior Alignment Score. Robot visuomotor policies aligned using RAPL outperform all baselines and demonstrate performance comparable to GT but with 5x less human annotations.", "description": "This table presents the results of hardware experiments evaluating the alignment of robot visuomotor policies.  The \"Behavior Alignment Score\" measures how well the robot's actions match human preferences.  The table compares the performance of RAPL (Representation-Aligned Preference-based Learning) against several baseline methods.  Crucially, it highlights that RAPL achieves comparable performance to the ground truth (GT) using only 20% of the human annotations required by the best-performing baseline (RLHF-L). This demonstrates RAPL's significantly improved efficiency in aligning robot policies with human preferences.", "section": "6.2 Results"}]