[{"figure_path": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/BOLT.jpg", "caption": "Figure 1: Illustration of bootstrapping long chain-of-thought in large language models (BOLT). BOLT comprises three stages: 1) LongCoT Bootstrapping which involves synthesizing LongCoT data, 2) LongCoT Supervised Finetuning where we train a ShortCoT model to adapt to the LongCoT format, incorporating reasoning elements and practicing extended chains of thought before arriving at an external solution, 3) LongCoT Online Training where the LongCoT SFT model is further improved through online exploration and refinement. Bootstrapping LLM is a ShortCoT LLM that is used to generate LongCoT data via in-context learning. ORM is an outcome reward model which scores the external solution in the model response.", "description": "The figure illustrates the BOLT (Bootstrap Long Chain-of-Thought) framework, a three-stage process for training language models to generate long chains of reasoning.  Stage 1, LongCoT Bootstrapping, uses a ShortCoT (short chain-of-thought) language model and a small number of in-context examples to generate LongCoT data. This data is then used in Stage 2, LongCoT Supervised Finetuning, to train a ShortCoT model on the LongCoT format, encouraging it to generate richer reasoning before providing an answer. Finally, Stage 3, LongCoT Online Training, refines the model's LongCoT capabilities through online learning and feedback using an Outcome Reward Model (ORM) to evaluate the quality of answers. The ORM assesses the quality of the final solution generated by the model.", "section": "3. BOLT"}, {"figure_path": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/bolt_output_example.jpg", "caption": "Figure 2: An illustration of long chain-of-thought as internal thoughts. Portions of the external solution are omitted for brevity.", "description": "This figure illustrates the three stages of the BOLT method.  The first stage is LongCoT bootstrapping, which uses in-context learning on a standard instruct model to generate LongCoT data.  The second stage is LongCoT supervised finetuning, where a ShortCoT model is fine-tuned on the bootstrapped data.  The third stage is LongCoT online training, which further refines the model's LongCoT capabilities using online learning.  The figure visually represents this process, showing how the model generates internal thoughts and external solutions.", "section": "3. BOLT"}, {"figure_path": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/topic_dist.png", "caption": "Figure 3: Topic distribution of query data in LongCoT Bootstrapping, \ud835\udc9fb-querysubscript\ud835\udc9fb-query\\mathcal{D}_{\\text{b-query}}caligraphic_D start_POSTSUBSCRIPT b-query end_POSTSUBSCRIPT.", "description": "Figure 3 is a bar chart that visualizes the distribution of topics within the query data used in the LongCoT Bootstrapping stage of the BOLT method.  The x-axis represents various topic categories, such as \"Mathematics,\" \"Computer Science and Coding,\" \"Creative Writing,\" etc., and the y-axis represents the percentage of queries falling under each topic. The chart shows the relative frequency of different query types in the dataset, indicating the diversity of tasks the model is trained on. This is useful in understanding the breadth of reasoning capabilities the model is expected to handle.", "section": "3.1 LongCoT Bootstrapping"}, {"figure_path": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/icl_instructions.jpg", "caption": "Figure 4: An illustration of the prompt used in LongCoT Bootstrapping.", "description": "This figure shows the prompt template used in the LongCoT bootstrapping stage of the BOLT method.  The prompt presents several in-context examples of Long chain-of-thought reasoning. Each example consists of a query, followed by the model's internal thought process (reasoning steps), and finally, the model's external solution.  This carefully constructed prompt guides the ShortCoT model to generate similar LongCoT responses during the bootstrapping phase.", "section": "3.1 LongCoT Bootstrapping"}, {"figure_path": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/mistral_7b_plot.png", "caption": "(a) Mistral-7B", "description": "Figure 5 presents a performance comparison of three different language models across five distinct benchmarks.  The models are Mistral-7B, Llama-3.1-8B, and Llama-3.1-70B. The benchmarks are MT-Bench, Arena-Hard (style-controlled version), WildBench, ZebraLogic, and MATH500. The figure visualizes the performance scores, allowing for a direct comparison of each model's capabilities across diverse tasks that evaluate reasoning, problem-solving, and general language understanding abilities.  Each bar graph section corresponds to a specific benchmark.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/llama_8b_plot.png", "caption": "(b) Llama-3.1-8B", "description": "The figure shows the performance of BOLT on the Llama-3.1-8B model across five benchmarks: MT-Bench, Arena-Hard-SC, WildBench, ZebraLogic, and MATH500.  It compares the initial performance of the Llama-3.1-8B Instruct model against its performance after undergoing the three stages of BOLT (Bootstrapping, SFT, and Online Training).  The bar chart illustrates the improvement in scores achieved by applying BOLT for each benchmark.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/llama_70b_plot.png", "caption": "(c) Llama-3.1-70B", "description": "Figure 5(c) presents the performance comparison results for the Llama-3.1-70B model across various benchmarks.  These benchmarks cover a wide range of tasks, testing mathematical problem-solving abilities, coding skills, logical reasoning, and general reasoning capabilities.  The figure visually compares the performance of the original Llama-3.1-70B model (the 'Init' or initial model) to the performance of the model after undergoing the three stages of the BOLT method ('BOLT-Llama-3.1-70B'). The improvement in performance after applying the BOLT method is clearly shown.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/performance_trajectory_subfigures.png", "caption": "Figure 5: Performance of BOLT on Mistral-7B, Llama-3.1-8B, and Llama-3.1-70B across benchmarks. These benchmarks consist of challenging real user queries and test models\u2019 math, coding, logical reasoning and general capacity. Note: ArenaHard-SC means the style controlled version of ArenaHard which controls for the effect of length and markdown. The metric for ZebraLogic is the cell-level accuracy.", "description": "This figure displays the performance comparison of three different language models (Mistral-7B, Llama-3.1-8B, and Llama-3.1-70B) after applying the BOLT method.  The models are evaluated on five benchmarks: MT-Bench, Arena-Hard-SC (a style-controlled version of Arena-Hard focusing on content rather than formatting), WildBench, ZebraLogic, and MATH500.  Each benchmark tests a different aspect of the models' capabilities, ranging from multi-turn question answering and creative writing to logical reasoning and mathematical problem-solving.  The results illustrate the improvements achieved in each benchmark after applying the BOLT method across different model sizes.  The style-controlled version of Arena-Hard was used to isolate the impact of reasoning quality from stylistic differences in response formatting. For ZebraLogic, cell-level accuracy is reported.", "section": "4. Experiments"}]