{"importance": "This paper is crucial because it presents **a novel approach to enable LLMs to generate long chain-of-thought reasoning without relying on distillation or expensive human annotations.** This addresses a major limitation in current research, which often relies on these methods, opening new avenues for developing LongCoT reasoning capabilities in LLMs more efficiently and systematically.  The findings could significantly impact the development of more sophisticated and generalizable reasoning abilities in LLMs.", "summary": "BOLT bootstraps Long Chain-of-Thought reasoning in LLMs without distillation, achieving impressive results across various benchmarks.", "takeaways": ["BOLT effectively enables LLMs to generate Long Chain-of-Thought reasoning without distillation.", "The method uses a three-stage process involving bootstrapping, finetuning, and online training.", "BOLT demonstrates impressive performance on various benchmarks, showing its effectiveness across different model scales."], "tldr": "Current methods for enabling Large Language Models (LLMs) to perform complex reasoning using \"Long Chain of Thought\" (LongCoT) heavily rely on knowledge distillation from existing, powerful models or expensive human annotations. This creates bottlenecks in the systematic development and scalability of LongCoT abilities.  Furthermore, many existing approaches narrowly focus on math or coding tasks, limiting their generalizability. \nThis paper introduces BOLT (Bootstrap Long Chain-of-Thought), a novel approach that enables LLMs to exhibit LongCoT reasoning without distillation or extensive human annotation.  BOLT involves a three-stage process: 1) bootstrapping LongCoT data using in-context learning on a standard instruct model; 2) supervised fine-tuning on the generated data to adapt the model; 3) online training to refine LongCoT capabilities further. The experiments demonstrate the efficiency of this approach with only 10 examples used in the bootstrapping stage and achieves remarkable results across diverse benchmarks, surpassing previous methods and showcasing scalability across model sizes (7B, 8B, 70B).", "affiliation": "Salesforce AI Research", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2502.03860/podcast.wav"}