[{"content": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle\" id=\"S4.T1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S4.T1.1.1.1\">Arena-Hard-SC</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.2.1\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.2.2\">Score</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.2.3\">Length</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.3.1\">ArmoRM-Llama3-8B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.3.2\">44.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.3.3\">674</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T1.1.4.1\">Skywork-Reward-Llama-3.1-8B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.4.2\">51.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.4.3\">890</td>\n</tr>\n</table>", "caption": "Table 1: Ablation on reward model in online DPO training.", "description": "This table presents an ablation study on the impact of different reward models used in the online Direct Preference Optimization (DPO) training phase of the BOLT method.  It compares the performance of the method when using different reward models (ArmoRM-Llama3-8B and Skywork-Reward-Llama-3.1-8B), evaluating their influence on the final model's performance across different metrics (Arena-Hard-SC, Wildbench). The table displays the scores and lengths of responses generated by the models, demonstrating the effect of different reward mechanisms on the length and accuracy of the generated text.", "section": "4.4 Ablation on Reward Models"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle\" id=\"S4.T1.2\">\n<tr class=\"ltx_tr\" id=\"S4.T1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S4.T1.2.1.1\">WildBench</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.2.2.1\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.2\">Score</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.3\">Length</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.2.3.1\">ArmoRM-Llama3-8B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.3.2\">43.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.3.3\">3354.51</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T1.2.4.1\">Skywork-Reward-Llama-3.1-8B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.2.4.2\">49.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.2.4.3\">4588.15</td>\n</tr>\n</table>", "caption": "Table 2: Ablation on the initial model to which BOLT is applied.", "description": "This table presents the results of an ablation study on the initial model used in the BOLT method. It compares the performance of BOLT when applied to different initial models, showing the impact of the model's starting capabilities on the final LongCoT performance.  Specifically, it shows the performance (scores on Arena-Hard and Wildbench benchmarks) for BOLT when initialized with a standard instruct model and when initialized with a base model without instruction tuning.", "section": "4.5 Ablation on Initial Models"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T2.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.1\" style=\"font-size:90%;\">Models</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.2.1\" style=\"font-size:90%;\">Arena-Hard-SC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.3\"><span class=\"ltx_text\" id=\"S4.T2.1.1.3.1\" style=\"font-size:90%;\">WildBench</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.2.1\"><span class=\"ltx_text\" id=\"S4.T2.1.2.1.1\" style=\"font-size:80%;\">Meta-Llama-3.1-8B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.2\"><span class=\"ltx_text\" id=\"S4.T2.1.2.2.1\" style=\"font-size:90%;\">18.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.3\"><span class=\"ltx_text\" id=\"S4.T2.1.2.3.1\" style=\"font-size:90%;\">32.08</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.3.1\"><span class=\"ltx_text\" id=\"S4.T2.1.3.1.1\" style=\"font-size:80%;\">BOLT-Llama-3.1-8B-Base</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.2\"><span class=\"ltx_text\" id=\"S4.T2.1.3.2.1\" style=\"font-size:90%;\">41.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.3\"><span class=\"ltx_text\" id=\"S4.T2.1.3.3.1\" style=\"font-size:90%;\">39.79</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T2.1.4.1\"><span class=\"ltx_text\" id=\"S4.T2.1.4.1.1\" style=\"font-size:80%;\">BOLT-Llama-3.1-8B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.4.2\"><span class=\"ltx_text\" id=\"S4.T2.1.4.2.1\" style=\"font-size:90%;\">44.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.4.3\"><span class=\"ltx_text\" id=\"S4.T2.1.4.3.1\" style=\"font-size:90%;\">42.96</span></td>\n</tr>\n</table>", "caption": "Table 3: Ablation on the learning algorithm for LongCoT online training.", "description": "This table presents the results of an ablation study comparing four different online training algorithms used in the Long Chain-of-Thought (LongCoT) online training stage of the BOLT model.  The algorithms compared are Direct Preference Optimization (DPO), REINFORCE, Reverse Reinforcement Learning with Online Optimization (RLOO), and Proximal Policy Optimization (PPO). The table shows the performance of each algorithm on two benchmarks: Arena-Hard-SC and WildBench, reporting the score and sequence length for each. This ablation study aims to determine which algorithm is most effective for enhancing LongCoT reasoning capabilities in the BOLT framework.", "section": "3.3 LongCoT Online Training"}]