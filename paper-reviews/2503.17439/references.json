{"references": [{"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the MATH dataset", "publication_date": "2021-01-01", "reason": "This paper introduces the MATH dataset, a key benchmark used to evaluate the mathematical reasoning abilities of language models, making it crucial for assessing progress in this area."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-01-01", "reason": "This paper introduces the GSM8K dataset, and it provides a methodology for training verifiers, which are essential for improving the accuracy of solutions to mathematical word problems."}, {"fullname_first_author": "Yu et al.", "paper_title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models", "publication_date": "2024-01-01", "reason": "Yu et al. introduces MetaMath that presents a data augmentation technique, combining answer and question augmentation, along with backward reasoning methods, to enhance the performance of language models in mathematical problem-solving."}, {"fullname_first_author": "Zhang et al.", "paper_title": "Learn beyond the answer: Training language models with reflection for mathematical reasoning", "publication_date": "2024-01-01", "reason": "Zhang et al. focuses on training language models with reflection by appending a 'reflection' part to the original solution, prompting the model to propose alternative solutions and solve similar problems."}, {"fullname_first_author": "Li et al.", "paper_title": "MathChat: Benchmarking mathematical reasoning and instruction following in multi-turn interactions", "publication_date": "2024-01-01", "reason": "Li et al. introduces MathChat, which requires the model to reflect on previous generations and perform further reasoning. "}]}