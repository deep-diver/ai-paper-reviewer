[{"figure_path": "https://arxiv.org/html/2503.20220/extracted/6310656/figures/teaser_dino.jpg", "caption": "Figure 1: Overview of DINeMo, a novel neural mesh model trained on pseudo-correspondence obtained from large visual foundation models.", "description": "This figure illustrates the architecture of DINeMo, a novel neural mesh model.  It highlights the model's training process, which leverages pseudo-correspondence generated from large visual foundation models instead of relying on traditional 3D annotations.  The diagram shows how pseudo-correspondences (positive and negative) are derived from the foundation models and used to train the neural mesh.  The use of pseudo-correspondence allows the model to learn from large unlabeled datasets, enabling efficient scaling and improved robustness.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2503.20220/extracted/6310656/figures/bidirectional.jpg", "caption": "Figure 2: Bidirectional pseudo-correspondence generation. See Sec.\u00a03.2.", "description": "Figure 2 illustrates the two-step bidirectional pseudo-correspondence generation process. The first step (Local-to-Global) generates raw pseudo-correspondences using features from SD-DINO and determines the 3D object orientation through majority voting. The second step (Global-to-Local) refines these correspondences by down-weighting matches of vertices invisible from the estimated 3D orientation. This process combines low-level local appearance features with high-level global context information for improved consistency and accuracy in keypoint correspondence.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2503.20220/extracted/6310656/figures/inferred_pose_labeled.jpg", "caption": "Figure 3: Qualitative comparisons with and without our bidirectional pseudo-correspondence generation. See Sec.\u00a03.2.", "description": "This figure shows a qualitative comparison of keypoint pseudo-correspondence generation with and without the authors' proposed bidirectional method.  The bidirectional method uses both local appearance features and global context (3D object orientation) to improve the accuracy of the correspondence. The comparison highlights how the bidirectional approach significantly reduces mismatches and improves the overall quality of the generated correspondence, which is crucial for accurate 3D pose estimation.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2503.20220/extracted/6310656/figures/scaling.png", "caption": "Figure 4: Scaling properties of DINeMo. See Sec.\u00a04.2.", "description": "Figure 4 illustrates how the performance of the DINeMo model scales with the amount of unlabeled data used during training.  The x-axis represents the number of training images, ranging from 2048 to 15000. The y-axis displays two key performance metrics: Pose Accuracy at \u03c0/6 (measuring the accuracy of pose estimation) and Point PCK@0.1 (measuring the accuracy of keypoint correspondence). The graph shows that as the number of training images increases, both metrics also improve, demonstrating DINeMo's ability to efficiently leverage large-scale unlabeled data for improved performance.", "section": "4.2 Scaling Properties"}, {"figure_path": "https://arxiv.org/html/2503.20220/extracted/6310656/figures/Semantic_corr_qualitative.jpg", "caption": "Figure 5: Qualitative comparisons between DINOv2 (left) and our DINeMo (right) on the SPair71k[16] dataset.", "description": "Figure 5 presents a qualitative comparison of the performance of DINOv2 and DINeMo on the SPair71k dataset.  The images show examples of semantic correspondence generated by each method. Each row shows a pair of images, with the left image displaying the output from the DINOv2 model, and the right image showing the corresponding results from the DINeMo model. This visual comparison allows for a direct assessment of the relative strengths and weaknesses of the two models in terms of accuracy and robustness of semantic correspondence in images of cars with varying degrees of occlusion, viewpoint, and lighting conditions.", "section": "8. Qualitative Examples"}, {"figure_path": "https://arxiv.org/html/2503.20220/extracted/6310656/figures/pose_qualitative.png", "caption": "Figure 6: Qualitative pose estimation results on the Pascal3D+[32] dataset.", "description": "Figure 6 presents a qualitative assessment of 3D pose estimation results obtained using the proposed DINeMo model.  It showcases several examples of cars from the PASCAL3D+ dataset [32], with each example displaying both the input image and the estimated 3D mesh.  The color-coded mesh allows for visualization of the estimated pose and shape of the vehicle, providing a visual representation of the model's performance on diverse car instances within the dataset.", "section": "8. Qualitative Examples"}]