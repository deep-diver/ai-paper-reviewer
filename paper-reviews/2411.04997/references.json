{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational multimodal model that aligns vision and language, which is the central focus of the current research."}, {"fullname_first_author": "Christoph Schuhmann", "paper_title": "Laion-5b: An open large-scale dataset for training next generation image-text models", "publication_date": "2022-12-01", "reason": "This paper introduces a large-scale image-text dataset, Laion-5B, which is crucial for training and evaluating multimodal models like CLIP and is a primary dataset used in the current study."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "publication_date": "2024-07-01", "reason": "This paper introduces Llama 3, a large language model (LLM) that is central to the proposed LLM2CLIP approach and enhances the capabilities of the CLIP model."}, {"fullname_first_author": "Yuxin Fang", "paper_title": "EVA-02: A visual representation for neon genesis", "publication_date": "2023-03-01", "reason": "This paper introduces EVA-02, a state-of-the-art CLIP model that serves as a baseline and is significantly improved upon by the proposed LLM2CLIP method."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-07-01", "reason": "This paper introduces LLaVA, a multimodal model that integrates CLIP and LLMs, providing a strong related work and context for the proposed LLM2CLIP method."}]}