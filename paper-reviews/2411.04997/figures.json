[{"figure_path": "https://arxiv.org/html/2411.04997/x1.png", "caption": "Figure 1: LLM2CLIP Overview. After applying caption contrastive fine-tuning to the LLM, the increased textual discriminability enables more effective CLIP training. We leverage the open-world knowledge and general capabilities of the LLM to better process dense captions, addressing the previous limitations of the pretrained CLIP visual encoder and providing richer, higher-dimensional textual supervision. Experimental results demonstrate that LLM2CLIP can make any SOTA CLIP model even more SOTA ever.", "description": "LLM2CLIP uses a large language model (LLM) to improve CLIP's ability to learn from image captions.  First, the LLM undergoes contrastive fine-tuning to enhance its ability to distinguish between similar captions. This improved discriminability is crucial for effective CLIP training. Then, the fine-tuned LLM, with its open-world knowledge, processes dense image captions.  This addresses the limited context window and understanding of the original CLIP text encoder. Finally, the improved textual supervision guides CLIP's visual encoder, resulting in a richer, higher-dimensional multimodal representation.  Experimental results show that LLM2CLIP significantly boosts the performance of state-of-the-art (SOTA) CLIP models.", "section": "3 Methods"}, {"figure_path": "https://arxiv.org/html/2411.04997/x2.png", "caption": "Table 1: Comparison of top-1 Caption Retrieval Accuracy (CRA) for various language models in MS COCO validation set.", "description": "This table presents a comparison of the top-1 Caption Retrieval Accuracy (CRA) achieved by several language models when tested on the MS COCO validation set.  The CRA metric assesses the ability of a language model to correctly retrieve the most relevant caption from a pool of candidates given a query caption. The models compared include various versions of the Llama language model and the CLIP-L/14 model.  The table shows that the CLIP model significantly outperforms the Llama models, highlighting the challenge of using LLMs directly as text encoders for visual tasks like caption retrieval due to their lack of discriminative power in the output space.  Fine-tuning the LLM's output using contrastive learning is shown to improve the accuracy significantly, suggesting that adjusting the LLM's output feature space is crucial for effective integration with CLIP.", "section": "3.1 NATIVE LLMS ARE INEFFECTIVE TEXT ENCODERS FOR CLIP"}]