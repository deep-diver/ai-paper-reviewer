{"references": [{" publication_date": "2024", "fullname_first_author": "OpenAI", "paper_title": "GPT-4 Technical Report", "reason": "This paper is foundational as it introduces GPT-4, a state-of-the-art large language model that the current research uses as a benchmark.  The paper's insights into model architecture and performance are crucial for understanding the context of efficiency improvements presented in the current research.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Gemini", "paper_title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context", "reason": "This paper introduces Gemini 1.5, another state-of-the-art large language model, which provides a comparison point for the proposed method's efficiency and performance improvements.  The model's capabilities serve as a reference for the advancement in computational efficiency introduced in the research.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Sun", "paper_title": "A simple and effective pruning approach for large language models", "reason": "This paper is highly relevant as it explores a different technique, pruning, for improving the efficiency of LLMs.  This provides a comparative context, allowing for a better understanding of the strengths and weaknesses of the proposed Router-Tuning method.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Lin", "paper_title": "Awq: Activation-aware weight quantization for llm compression and acceleration", "reason": "This paper focuses on model compression through weight quantization. This is directly related to the current research as both approaches aim to enhance efficiency, but through different mechanisms, allowing for a comparative analysis of their effectiveness.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Raposo", "paper_title": "Mixture-of-depths: Dynamically allocating compute in transformer-based language models", "reason": "This is a core reference as it introduces the Mixture of Depths (MoD) concept which is directly addressed in the current research.  The paper's explanation of MoD and its challenges forms the basis for the current work's innovations in addressing the limitations of MoD.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Tan", "paper_title": "DLO: Dynamic layer operation for efficient vertical scaling of LLMs", "reason": "This paper is crucial as it presents a related approach, DLO, that also seeks to improve the efficiency of LLMs.  Comparing the proposed method's performance against DLO demonstrates the advancements and improvements made in the current research.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "He", "paper_title": "What matters in transformers? Not all attention is needed", "reason": "This paper is highly significant as it explores the importance of different layers within transformer models, directly influencing the design choices in the proposed method, particularly MindSkip.  It justifies the focus on attention layers for dynamic depth.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "He", "paper_title": "Demystifying the compression of mixture-of-experts through a unified framework", "reason": "This paper delves into mixture-of-experts (MoE) models, which are closely related to the Mixture-of-Depths (MoD) approach explored in the current research.  Understanding the compression techniques of MoE models is important for comparing the efficiency of the proposed method.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "reason": "This paper is a key reference as it introduces Llama 2, one of the language models used in the experiments.  Its characteristics and performance are essential for assessing the efficacy of the proposed method.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Jiang", "paper_title": "Mistral 7B", "reason": "This paper introduces the Mistral 7B language model which is used in the experimental evaluation of the proposed methodology.  The model's architecture and performance serve as a comparison point for assessing the effectiveness of Router-Tuning and MindSkip.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Bai", "paper_title": "Qwen Technical Report", "reason": "This paper is important as it details the Qwen language model, another model used in the experiments.  Understanding Qwen's architecture and performance is necessary for a comprehensive evaluation of the proposed approach.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Houlsby", "paper_title": "Parameter-efficient transfer learning for NLP", "reason": "This is a relevant work because it introduces the concept of parameter-efficient transfer learning, which is directly related to the core idea of Router-Tuning and justifies the focus on fine-tuning only the router network.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "reason": "This paper is important as it introduces Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method.  Understanding LoRA provides context for the comparison of Router-Tuning's efficiency and effectiveness.", "section_number": 2}, {" publication_date": "2013", "fullname_first_author": "Bengio", "paper_title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "reason": "This paper is highly relevant as it introduces the straight-through estimator, a crucial technique used in the proposed MindSkip method to handle the non-differentiability of the binary mask in the dynamic depth mechanism.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Cobbe", "paper_title": "Training verifiers to solve math word problems", "reason": "This paper introduces the GSM8K benchmark, a dataset used in the experimental evaluation of the proposed method.  The characteristics of GSM8K, particularly its complexity, are essential for understanding the performance gains achieved by Router-Tuning and MindSkip.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Gromov", "paper_title": "The unreasonable ineffectiveness of the deeper layers", "reason": "This paper is important because it supports the observation made in the research that deeper layers are more redundant. This finding supports the design decision to focus MindSkip's dynamic depth application on the deeper layers.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Taori", "paper_title": "Stanford alpaca: An instruction-following llama model", "reason": "This paper introduces the Alpaca dataset, which is used in the ablation study to assess the impact of different training datasets on the proposed method's performance. The use of Alpaca provides insights into the robustness and generalizability of the methodology.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Xu", "paper_title": "WizardLM: Empowering large language models to follow complex instructions", "reason": "This paper introduces the Evol-Instruct dataset, which is part of the ablation study on different training datasets.  Understanding how the training data influences the performance further supports the proposed method's robustness.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Zheng", "paper_title": "Judging LLMs-as-a-judge with MT-Bench and Chatbot Arena", "reason": "This paper introduces the ShareGPT dataset used in the ablation study. It allows for analysis of the effect of different training data on the performance of Router-Tuning and highlights the generalizability and robustness of the proposed method.", "section_number": 3}]}