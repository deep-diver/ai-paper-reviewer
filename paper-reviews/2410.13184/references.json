{"references": [{" publication_date": "2024", "fullname_first_author": "OpenAI", "paper_title": "GPT-4 Technical Report", "reason": "This paper is foundational for the field of large language models, providing a benchmark for performance and efficiency comparisons. The computational cost of LLMs is a key concern addressed in this paper, and this work establishes the context for the current study's focus on improving efficiency.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Gemini Team", "paper_title": "Gemini 1.5: Unlocking Multimodal Understanding Across Millions of Tokens of Context", "reason": "This paper is highly relevant due to its focus on multimodal understanding, a rapidly expanding area that frequently employs large language models. Its focus on improving efficiency and reducing computational costs aligns perfectly with the goals of the current study.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Sun", "paper_title": "A Simple and Effective Pruning Approach for Large Language Models", "reason": "This work directly addresses the challenge of computational efficiency in large language models (LLMs) which is the central motivation of this paper. The proposed pruning method is relevant as an alternative approach to achieve efficiency, and a comparison could be made between the two approaches.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Lin", "paper_title": "Awq: Activation-Aware Weight Quantization for LLM Compression and Acceleration", "reason": "This work is significant as it contributes to the efficiency of LLMs through a different method. It explores a different approach to address the challenge of computational cost in large language models which is the primary concern of this paper. Comparing both approaches could potentially give more insight into the problem.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Raposo", "paper_title": "Mixture-of-Depths: Dynamically Allocating Compute in Transformer-Based Language Models", "reason": "This is the most directly relevant paper because it introduces the Mixture of Depths (MoD) approach, which is the central theme of this research. The current study addresses the limitations and challenges of MoD highlighted in this prior work.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Tan", "paper_title": "DLO: Dynamic Layer Operation for Efficient Vertical Scaling of LLMs", "reason": "This paper also focuses on dynamic depth mechanisms in LLMs which is the core topic of this paper. Its exploration of different methods to achieve efficiency provides a context for the current study and allows for potential comparisons and discussions of advantages and disadvantages.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "He", "paper_title": "What Matters in Transformers? Not All Attention Is Needed", "reason": "This paper's findings are highly relevant to the current work, as it investigates the redundancy in attention layers within transformers.  This is a key aspect of the MindSkip method, making this study critical for providing foundational support and justification for the approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "He", "paper_title": "Demystifying the Compression of Mixture-of-Experts Through a Unified Framework", "reason": "This study offers a broader perspective on model compression techniques and their efficiency implications in LLMs. The investigation of mixture-of-experts models is indirectly relevant as it helps to inform the choices and considerations in model optimization.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Gromov", "paper_title": "The Unreasonable Ineffectiveness of the Deeper Layers", "reason": "This study adds support to the findings of this paper by offering different evidence to show that deeper layers might not be as useful or necessary as shallower ones, especially when considering efficiency. This indirectly supports the claims and method proposed by the current work.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Touvron", "paper_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "reason": "This is a seminal paper in the field of LLMs and is essential for understanding the context and background of the research in this study. Llama 2 serves as the basis for several experiments, highlighting the significance of this model and its impact on the research presented.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Jiang", "paper_title": "Mistral 7B", "reason": "This paper introduces a novel large language model, Mistral 7B, and its architecture.  The paper serves as a crucial baseline for comparing and validating the results presented in this study, as it is used as one of the models for experiments.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Bai", "paper_title": "Qwen Technical Report", "reason": "This paper details the Qwen large language model, another model used in the experiments of this paper.  This model's architecture and properties are important to understanding and interpreting the results and contributions of the study.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Houlsby", "paper_title": "Parameter-Efficient Transfer Learning for NLP", "reason": "This paper is foundational in the area of efficient fine-tuning for NLP models, particularly relevant to the Router-Tuning method in this paper, which uses efficient fine-tuning of the router network to improve performance without retraining the entire model.  The techniques and insights from this paper inform the current work.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Hu", "paper_title": "LoRA: Low-Rank Adaptation of Large Language Models", "reason": "LoRA is a significant parameter-efficient finetuning technique, forming the basis of comparison for the efficiency claims of Router-Tuning in this paper. This paper serves as an important benchmark to highlight the unique aspects of Router-Tuning.", "section_number": 2}, {" publication_date": "2013", "fullname_first_author": "Bengio", "paper_title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation", "reason": "This work introduces the Straight-Through Estimator (STE), a key component of MindSkip. STE enables training of the router network, allowing for differentiable control of the binary decision-making process in the MindSkip framework. Understanding STE is essential for interpreting the methodology presented.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Cobbe", "paper_title": "Training Verifiers to Solve Math Word Problems", "reason": "This paper introduces a benchmark dataset (GSM8K) which is directly used for evaluating the models in the experimental results.  The performance on this dataset is a key factor in assessing the effectiveness and improvements of the proposed method.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Taori", "paper_title": "Stanford Alpaca: An Instruction-Following LLaMA Model", "reason": "This paper introduces the Alpaca dataset used in experiments. Understanding the nature and characteristics of this dataset are vital in analyzing the experimental results and contributions.  The dataset is an important component in the model training and evaluation.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Xu", "paper_title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions", "reason": "This paper details the Evol-Instruct dataset, another dataset used in the training process, and its impact on the performance of MindSkip is evaluated in the experiments. This provides insights into the data diversity and its effect on model training and performance.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Zheng", "paper_title": "Judging LLMs-as-a-Judge with MT-Bench and Chatbot Arena", "reason": "This paper introduces the ShareGPT dataset used in the training phase. Understanding the composition and nature of this dataset is crucial for evaluating the impact of data on the model's performance and generalizability. The results using this dataset are important for determining the robustness of the approach.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Wu", "paper_title": "Llama Pro: Progressive LLaMA with Block Expansion", "reason": "This paper introduces the Llama-Pro dataset, used in the experiments for evaluating the impact of training data size on the performance of MindSkip.  This study highlights the effect of data size on the effectiveness of Router-Tuning and the robustness of the proposed approach.", "section_number": 3}]}