[{"figure_path": "2410.13184/tables/table_3_0.html", "caption": "Table 1: Experimental results of MindSkip deployed at different granularities. While MindSkip is primarily applied to Attention layers, we also evaluate its performance on Block and MLP layers for comparison. The number of skippable layers is constrained to 16, and the overall capacity of MindSkip is 50%.", "description": "Table 1 presents the experimental results of MindSkip applied to different granularities (Attention, Block, and MLP layers) across two versions of Llama-3-8B, comparing speedup, and performance on various tasks.", "section": "3.1 Main Results"}, {"figure_path": "2410.13184/tables/table_5_0.html", "caption": "Table 1: Experimental results of MindSkip deployed at different granularities. While MindSkip is primarily applied to Attention layers, we also evaluate its performance on Block and MLP layers for comparison. The number of skippable layers is constrained to 16, and the overall capacity of MindSkip is 50%.", "description": "Table 1 shows the experimental results of applying MindSkip at different granularities (Block, MLP, and Attention layers) on Llama-3-8B and Llama-3-8B-Instruct models, comparing speedup and performance metrics.", "section": "3.1 Main Results"}, {"figure_path": "2410.13184/tables/table_8_0.html", "caption": "Table 1: Experimental results of MindSkip deployed at different granularities. While MindSkip is primarily applied to Attention layers, we also evaluate its performance on Block and MLP layers for comparison. The number of skippable layers is constrained to 16, and the overall capacity of MindSkip is 50%.", "description": "Table 1 presents the experimental results of MindSkip applied to different layers (Attention, Block, MLP) of two Llama models, showing speedup and performance across various tasks.", "section": "3.1 Main Results"}]