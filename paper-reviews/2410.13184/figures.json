[{"figure_path": "2410.13184/figures/figures_2_0.png", "caption": "Figure 1: Overview of MindSkip. For simplicity, LayerNorm before Attention is omitted. Unlike traditional Attention, MindSkip processes the input only when the routing score R(x) \u2265 \u03c4. During Router-Tuning, only the Router is trainable to enable dynamic depth.", "description": "The figure illustrates the MindSkip mechanism, showing how it selectively applies attention layers based on a routing score, improving efficiency without sacrificing accuracy.", "section": "Methodology"}]