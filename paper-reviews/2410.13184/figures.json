[{"figure_path": "2410.13184/figures/figures_2_0.png", "caption": "Figure 1: Overview of MindSkip. For simplicity, LayerNorm before Attention is omitted. Unlike traditional Attention, MindSkip processes the input only when the routing score R(x) \u2265 \u03c4. During Router-Tuning, only the Router is trainable to enable dynamic depth.", "description": "Figure 1 illustrates the MindSkip mechanism, showing how it selectively processes input tokens based on a routing score to achieve dynamic depth in the transformer network.", "section": "Methodology"}]