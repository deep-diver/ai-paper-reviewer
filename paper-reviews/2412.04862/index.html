<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>EXAONE 3.5: Series of Large Language Models for Real-world Use Cases &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="EXAONE 3.5: Series of Large Language Models for Real-world Use Cases &#183; HF Daily Paper Reviews by AI"><meta name=description content="LG AI Research unveils EXAONE 3.5, a series of instruction-tuned language models (2.4B, 7.8B, and 32B parameters) excelling in real-world tasks, long-context understanding, and general benchmarks."><meta name=keywords content="Natural Language Processing,Large Language Models,üè¢ LG AI Research,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="EXAONE 3.5: Series of Large Language Models for Real-world Use Cases"><meta property="og:description" content="LG AI Research unveils EXAONE 3.5, a series of instruction-tuned language models (2.4B, 7.8B, and 32B parameters) excelling in real-world tasks, long-context understanding, and general benchmarks."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-06T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-06T00:00:00+00:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="üè¢ LG AI Research"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/cover.png"><meta name=twitter:title content="EXAONE 3.5: Series of Large Language Models for Real-world Use Cases"><meta name=twitter:description content="LG AI Research unveils EXAONE 3.5, a series of instruction-tuned language models (2.4B, 7.8B, and 32B parameters) excelling in real-world tasks, long-context understanding, and general benchmarks."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"EXAONE 3.5: Series of Large Language Models for Real-world Use Cases","headline":"EXAONE 3.5: Series of Large Language Models for Real-world Use Cases","abstract":"LG AI Research unveils EXAONE 3.5, a series of instruction-tuned language models (2.4B, 7.8B, and 32B parameters) excelling in real-world tasks, long-context understanding, and general benchmarks.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.04862\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2024","dateCreated":"2024-12-06T00:00:00\u002b00:00","datePublished":"2024-12-06T00:00:00\u002b00:00","dateModified":"2024-12-06T00:00:00\u002b00:00","keywords":["Natural Language Processing","Large Language Models","üè¢ LG AI Research"],"mainEntityOfPage":"true","wordCount":"5961"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-19/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-19</p></a><a href=/ai-paper-reviewer/2025-03-20/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-20</p></a><a href=/ai-paper-reviewer/2025-03-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-21</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-19/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-19</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-20/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-20</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-21</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.04862/cover_hu5327581153601638903.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.04862/>EXAONE 3.5: Series of Large Language Models for Real-world Use Cases</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">EXAONE 3.5: Series of Large Language Models for Real-world Use Cases</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-06T00:00:00+00:00>6 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>5961 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">28 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.04862/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.04862/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/natural-language-processing/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Natural Language Processing
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/large-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-lg-ai-research/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ LG AI Research</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#exaone-35-models>EXAONE 3.5 Models</a></li><li><a href=#training--tuning>Training & Tuning</a></li><li><a href=#benchmark-results>Benchmark Results</a></li><li><a href=#long-context-use>Long Context Use</a></li><li><a href=#responsible-ai>Responsible AI</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#exaone-35-models>EXAONE 3.5 Models</a></li><li><a href=#training--tuning>Training & Tuning</a></li><li><a href=#benchmark-results>Benchmark Results</a></li><li><a href=#long-context-use>Long Context Use</a></li><li><a href=#responsible-ai>Responsible AI</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.04862</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>LG AI Research et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2024-12-09</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.04862 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.04862 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/exaone-3-5-series-of-large-language-models target=_self role=button>‚Üó Papers with Code</a></p><audio controls><source src=https://ai-paper-reviewer.com/2412.04862/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Large language models (LLMs) have shown great promise but face challenges in real-world applications. Smaller models are often less capable, while larger models require significant resources. There&rsquo;s also a demand for models capable of handling longer contexts. This paper presents EXAONE 3.5, a family of three instruction-tuned LLMs (2.4B, 7.8B, and 32B parameters) designed to address these issues. They achieve state-of-the-art results on various benchmarks, demonstrating exceptional performance in real-world scenarios and long-context understanding.</p><p>The EXAONE 3.5 models were trained using a two-stage process: pre-training on a massive dataset followed by fine-tuning with instruction data. The researchers also implemented methods to mitigate catastrophic forgetting (the phenomenon where a model forgets previously learned information during training) and data contamination (when test data overlaps with training data). The results demonstrate the effectiveness of this training approach and the models&rsquo; ability to handle various tasks, particularly those demanding long-context processing. The models are open-source for research, contributing to broader AI research and potential advancements in various applications.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-b7f8b340c4e0b870cc8dcf0e70a7837d></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-b7f8b340c4e0b870cc8dcf0e70a7837d",{strings:[" EXAONE 3.5 models achieve top performance across multiple benchmarks, showcasing strong instruction-following abilities and long-context understanding. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-7da9231bd37704fd0d4620a9612269f9></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-7da9231bd37704fd0d4620a9612269f9",{strings:[" The models are available for research purposes, promoting collaborative advancements in AI. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-3d3afade1e96f1a32803fb2fb488d2f3></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-3d3afade1e96f1a32803fb2fb488d2f3",{strings:[" The 2.4B parameter model demonstrates surprisingly competitive results, highlighting potential for efficient, resource-constrained deployments. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is important because it introduces <strong>EXAONE 3.5</strong>, a series of instruction-tuned language models, addressing the need for smaller, more efficient models and larger models with enhanced performance. The open availability of these models facilitates further research and development in various applications, making it a valuable resource for the AI community. Its focus on real-world use cases and long-context understanding aligns with current research trends, opening new avenues for practical applications and addressing existing limitations in large language models.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04862/x1.png alt></figure></p><blockquote><p>üîº The figure illustrates the process of creating instruction-tuning data. It starts by extracting core knowledge from a massive web corpus and organizing it using a pre-defined taxonomy. This structured knowledge then serves as the basis for generating instruction-tuning datasets. To increase the complexity and diversity of these datasets, an instruction-evolving method is employed, resulting in a final dataset that spans various fields and difficulty levels.</p><details><summary>read the caption</summary>Figure 1: A procedure of instruction-tuning data construction. First, we extract the core knowledge from large-volume web corpora and classify it within the taxonomy we defined in advance. Next, instruction-tuning data is generated based on the knowledge. To construct additional training data that is more complex, we leverage an instruction-evolving method¬†[58] that lets the final dataset cover various fields with varying levels of difficulty.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model size</th><th>32B</th><th>7.8B</th><th>2.4B</th></tr></thead><tbody><tr><td>$d_model$</td><td>5,120</td><td>4,096</td><td>2,560</td></tr><tr><td>Number of layers</td><td>64</td><td>32</td><td>30</td></tr><tr><td>Pre-normalization</td><td>True</td><td>True</td><td>True</td></tr><tr><td>Non-linearity</td><td>SwiGLU [44]</td><td>SwiGLU</td><td>SwiGLU</td></tr><tr><td>Feedforward dimension</td><td>27,392</td><td>14,336</td><td>7,168</td></tr><tr><td>Head type</td><td>GQA [3]</td><td>GQA</td><td>GQA</td></tr><tr><td>Number of heads</td><td>40</td><td>32</td><td>32</td></tr><tr><td>Number of KV heads</td><td>8</td><td>8</td><td>8</td></tr><tr><td>Head size</td><td>128</td><td>128</td><td>80</td></tr><tr><td>Max sequence length</td><td>32,768</td><td>32,768</td><td>32,768</td></tr><tr><td>RoPE theta [46]</td><td>1,000,000</td><td>1,000,000</td><td>1,000,000</td></tr><tr><td>Tokenizer</td><td>BBPE [51]</td><td>BBPE</td><td>BBPE</td></tr><tr><td>Vocab size</td><td>102,400</td><td>102,400</td><td>102,400</td></tr><tr><td>Tied word embedding</td><td>False</td><td>False</td><td>True</td></tr></tbody></table></table></figure><blockquote><p>üîº This table details the architecture and hyperparameters of the three EXAONE 3.5 language models (32B, 7.8B, and 2.4B parameters). It lists key features such as the model size, number of layers, feed-forward dimensions, head types, and maximum sequence length. The table also specifies implementation details, like the type of pre-normalization used and the activation function.</p><details><summary>read the caption</summary>Table 1: Configurations of EXAONE 3.5 language models</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">EXAONE 3.5 Models<div id=exaone-35-models class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#exaone-35-models aria-label=Anchor>#</a></span></h4><p>The EXAONE 3.5 models represent a significant advancement in large language models (LLMs), offering improvements in instruction-following, long-context understanding, and overall performance compared to their predecessors and other models of similar size. <strong>Three model sizes (2.4B, 7.8B, and 32B parameters)</strong> cater to diverse computational resource needs. The 2.4B model is especially noteworthy for its performance relative to its size, showcasing efficiency for deployment on resource-constrained devices. The emphasis on real-world scenarios in evaluation highlights the models&rsquo; practical applicability. The availability of the models for research purposes facilitates further investigation and development within the AI community. <strong>Open-sourcing these models</strong> with clear licensing demonstrates a commitment to fostering collaboration and transparency within the field. However, the acknowledged limitations, such as occasional generation of inappropriate responses and potential biases, must be carefully addressed in future iterations. The report&rsquo;s emphasis on responsible AI development underscores a commitment to ethical considerations in LLM deployment.</p><h4 class="relative group">Training & Tuning<div id=training--tuning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#training--tuning aria-label=Anchor>#</a></span></h4><p>A robust large language model (LLM) necessitates a rigorous training and fine-tuning regimen. <strong>Pre-training</strong>, often on massive text corpora, equips the model with foundational language understanding and generation capabilities. This stage is computationally expensive, but crucial for establishing a broad knowledge base. <strong>Fine-tuning</strong>, conversely, focuses on adapting the pre-trained model to specific tasks or domains using smaller, more targeted datasets. Instruction tuning, a common fine-tuning approach, refines the model&rsquo;s ability to follow instructions accurately. <strong>Supervised fine-tuning (SFT)</strong> is also used to align model outputs with human preferences, often via demonstrations of correct behavior. <strong>Preference optimization</strong>, another essential stage, further refines the model by iteratively optimizing its responses based on rankings of alternative outputs provided by humans or a reward model. This multi-stage process is vital for creating LLMs that are both knowledgeable and adept at following instructions effectively. The process requires careful consideration of data quality, computational resources, and evaluation metrics to produce high-quality, reliable, and safe models.</p><h4 class="relative group">Benchmark Results<div id=benchmark-results class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#benchmark-results aria-label=Anchor>#</a></span></h4><p>A dedicated &lsquo;Benchmark Results&rsquo; section in a research paper would offer a crucial evaluation of the presented model. It should detail performance across various established benchmarks, comparing the new model against existing state-of-the-art solutions. <strong>Key metrics</strong> such as accuracy, precision, recall, and F1 scores should be meticulously reported for each benchmark. The analysis should go beyond simple numerical results; it needs to <strong>interpret the findings</strong>, explaining strengths and weaknesses across different task types. For instance, superior performance on certain benchmarks might highlight the model&rsquo;s proficiency in specific domains like question answering or natural language inference, while weaker performance in others could indicate areas needing further improvement. Furthermore, <strong>a discussion of the limitations</strong> of the benchmarks themselves and their suitability for evaluating specific model capabilities is crucial for a well-rounded analysis. This section provides valuable insights into the model&rsquo;s capabilities and potential limitations. Overall, a robust &lsquo;Benchmark Results&rsquo; section strengthens the paper&rsquo;s credibility and impact.</p><h4 class="relative group">Long Context Use<div id=long-context-use class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#long-context-use aria-label=Anchor>#</a></span></h4><p>The capacity to process extended contexts is crucial for the real-world application of large language models (LLMs). This research highlights the significance of <strong>long-context understanding</strong>, surpassing the limitations of previous models. The inclusion of benchmarks specifically designed to assess this capability demonstrates a commitment to evaluating performance in realistic scenarios. The paper&rsquo;s exploration of various techniques, such as long-context fine-tuning, further emphasizes this aspect&rsquo;s importance. Results reveal the models&rsquo; impressive proficiency in handling extensive input lengths. <strong>This ability to process longer contexts directly impacts the models&rsquo; usability and accuracy in multifaceted, real-world tasks</strong>, showing superior performance over comparable state-of-the-art models. The research therefore establishes long-context capabilities as a pivotal factor in assessing the overall efficacy of LLMs.</p><h4 class="relative group">Responsible AI<div id=responsible-ai class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#responsible-ai aria-label=Anchor>#</a></span></h4><p>The section on &lsquo;Responsible AI&rsquo; in this research paper is crucial, highlighting the researchers&rsquo; commitment to developing and deploying AI models ethically. They emphasize a multi-faceted approach including a <strong>Responsible AI Development Framework</strong> that incorporates data governance, ethical considerations, and risk management. The openness of the models, while beneficial for research, introduces challenges; thus, they&rsquo;ve conducted ethical impact assessments to mitigate potential risks such as bias and misuse. Their commitment to transparency is evident in their plans to maximize social benefits while ensuring safety and accountability. This dedication to responsible AI is a <strong>strong positive</strong>, showcasing a proactive approach to the ethical implications of their work and setting a valuable example for others in the field. The <strong>explicit mention of data compliance protocols</strong> further underscores their dedication to developing and deploying AI models responsibly.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04862/x2.png alt></figure></p><blockquote><p>üîº Figure 2 illustrates the two-stage preference optimization process. The top half shows how preference data is created: multiple model responses (y) are generated for a given prompt (x), and a reward model ranks these responses. The best (yw) and worst (yl) responses are then paired with the prompt (x) to form a preference dataset. The bottom half depicts the sequential training process using Direct Alignment Algorithms (DAA): an initial model (M0), pretrained with Supervised Fine-Tuning (SFT), is iteratively improved to create M1 and finally M2, using the preference dataset to refine its alignment with human preferences.</p><details><summary>read the caption</summary>Figure 2: Overview of the preference optimization pipeline. (Top) Preference Data Creation: It shows the process of constructing preference data {x,yw,yl}ùë•subscriptùë¶ùë§subscriptùë¶ùëô\{x,y_{w},y_{l}\}{ italic_x , italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT } by scoring the responses yùë¶yitalic_y generated from a model for the prompt xùë•xitalic_x using a reward model. (Bottom) Preference Optimization: Sequential training process where M0subscriptùëÄ0M_{0}italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT initialized from the SFT model is trained through DAA to obtain M1subscriptùëÄ1M_{1}italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and M2subscriptùëÄ2M_{2}italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04862/x3.png alt></figure></p><blockquote><p>üîº This figure displays the performance of EXAONE 3.5 language models on the Needle-in-a-Haystack (NIAH) benchmark, which tests the ability of models to locate and retrieve specific information within long documents. The x-axis shows the length of the input document in tokens (1k, 4k, 8k, 16k, 32k), while the y-axis indicates the relative position of the target information within the document (0% being the start and 100% the end). The color-coding (green for success, red for failure) visually represents the model&rsquo;s accuracy in retrieving the information at different positions and context lengths. The results demonstrate that EXAONE 3.5 models achieve near-perfect accuracy across various document depths and context lengths, in both English and Korean.</p><details><summary>read the caption</summary>Figure 3: NIAH results of EXAONE 3.5 language models. The x-axis represents the token length of the input text, while the y-axis shows the relative position within the text, expressed as a percentage (0% corresponds to the beginning, and 100% to the end). The results are represented using a color-coded scheme: green indicates successful retrievals, and red represents unsuccessful ones. EXAONE 3.5 language models achieve near-perfect accuracy in retrieving information across various document depths and context lengths in English and Korean.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04862/extracted/6036302/figures/decontam_fixed.png alt></figure></p><blockquote><p>üîº Figure 4 illustrates the data decontamination process used in training the EXAONE 3.5 language models. The process aims to remove training examples that overlap with the test data, a technique to prevent contamination that would otherwise bias evaluation results and reduce the model&rsquo;s ability to generalize. The figure shows a flowchart summarizing the steps involved: (1) extracting substrings from the test set to create a substring pool, (2) sampling substrings from a training example, (3) checking for matches between the sampled substrings and those in the pool, and (4) removing the training example if matches exceed a threshold. The improvement over the GPT-4 approach is highlighted: the number of random substrings sampled from each training example (N) is increased from 5 to 10 for stricter decontamination, ensuring a more rigorous removal of contaminated data.</p><details><summary>read the caption</summary>Figure 4: A summary of the decontamination method employed to train EXAONE 3.5 language models. Adopting an approach borrowed from the GPT-4 method, we increase the number of random sample to N=10ùëÅ10N=10italic_N = 10 for stricter decontamination.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model size</th><th>32B</th><th>7.8B</th><th>2.4B</th></tr></thead><tbody><tr><td>Training tokens</td><td>6.5T</td><td>9T</td><td>6.5T</td></tr><tr><td>Amount of computation (FLOPs)</td><td>1.25e+24</td><td>4.21e+23</td><td>9.36e+22</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the training data size and computational resource requirements for each of the three EXAONE 3.5 language models (2.4B, 7.8B, and 32B parameters). It details the number of training tokens used and the amount of computation (in FLOPs) required for training each model. This information provides context regarding the scale of the training effort for each model and allows for comparison across different model sizes.</p><details><summary>read the caption</summary>Table 2: The sizes of the training data corpus along with the amounts of computation to build EXAONE 3.5 language models</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Models</th><th>Model size</th><th>Training tokens</th><th>Amount of computation (ratio)</th></tr></thead><tbody><tr><td>EXAONE 3.5</td><td>32B</td><td>6.5T</td><td>1.00</td></tr><tr><td>Qwen 2.5</td><td>32B</td><td>18T</td><td>2.77</td></tr><tr><td>Gemma 2</td><td>27B</td><td>13T</td><td>1.69</td></tr><tr><td>Yi 1.5</td><td>34B</td><td>3.6T</td><td>0.59</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the computational cost of pre-training the EXAONE 3.5 32B language model against other large language models of similar size. The cost is approximated by multiplying the model&rsquo;s parameter count by the number of training tokens. While EXAONE 3.5 32B had fewer total computations than Qwen 2.5 32B and Gemma 2 27B, it still achieved competitive performance on benchmark evaluations.</p><details><summary>read the caption</summary>Table 3: Comparison of the total amounts of computations to build models. We approximate the amount of computations as the product of the model size and the number of training tokens. Although the EXAONE 3.5 32B model is behind in the computations compared to Qwen 2.5 and Gemma 2, it has shown competitive performances.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Category</th><th>Benchmark</th><th>Lang</th><th>Evaluation Settings</th><th>Metric</th></tr></thead><tbody><tr><td>Real-world Use Cases</td><td>MT-Bench [59]</td><td>EN</td><td>LLM-as-a-judge (judge: gpt-4o-2024-08-06)</td><td>LLM score</td></tr><tr><td></td><td>LiveBench [54] (v2024-08-31)</td><td>EN</td><td>Ground-truth match</td><td>Accuracy</td></tr><tr><td></td><td>Arena-Hard-v0.1 [29]</td><td>EN</td><td>LLM-as-a-judge (judge: gpt-4-1106-preview)</td><td>Win rate</td></tr><tr><td></td><td>AlpacaEval 2.0 LC [12]</td><td>EN</td><td>LLM-as-a-judge (judge: gpt-4-1106-preview)</td><td>Win rate</td></tr><tr><td></td><td>IFEval [61]</td><td>EN</td><td>Prompt-level / strict accuracy</td><td>Accuracy</td></tr><tr><td></td><td>KoMT-Bench [42]</td><td>KO</td><td>LLM-as-a-judge (judge: gpt-4o-2024-08-06)</td><td>LLM score</td></tr><tr><td></td><td>LogicKor [37]</td><td>KO</td><td>LLM-as-a-judge (judge: gpt-4-1106-preview)</td><td>LLM score</td></tr><tr><td>Long Context</td><td>Needle-In-A-Haystack [23]</td><td>EN/KO</td><td>Ground-truth match</td><td>Accuracy</td></tr><tr><td></td><td>LongBench [5]</td><td>EN</td><td>Ground-truth match</td><td>F1, Rouge</td></tr><tr><td></td><td>LongRAG [21] (extended)</td><td>EN</td><td>LLM-as-a-judge (judge: gpt-4o-2024-08-06)</td><td>LLM score</td></tr><tr><td></td><td>Ko-LongRAG (In-house)</td><td>KO</td><td>LLM-as-a-judge (judge: gpt-4o-2024-08-06)</td><td>LLM score</td></tr><tr><td></td><td>Ko-WebRAG (In-house)</td><td>KO</td><td>LLM-as-a-judge (judge: gpt-4o-2024-08-06)</td><td>LLM score</td></tr><tr><td>General Domain</td><td>GSM8K [9]</td><td>EN</td><td>0-shot / CoT</td><td>Accuracy</td></tr><tr><td></td><td>MATH [17, 27]</td><td>EN</td><td>0-shot / CoT</td><td>Accuracy</td></tr><tr><td></td><td>HumanEval [6]</td><td>EN</td><td>0-shot</td><td>pass@1</td></tr><tr><td></td><td>MBPP [4]</td><td>EN</td><td>0-shot (Evalplus base)</td><td>pass@1</td></tr><tr><td></td><td>GPQA [40]</td><td>EN</td><td>0-shot / CoT</td><td>Accuracy</td></tr><tr><td></td><td>ARC-C [8]</td><td>EN</td><td>0-shot</td><td>Accuracy</td></tr><tr><td></td><td>BBH [47]</td><td>EN</td><td>0-shot / CoT</td><td>Accuracy</td></tr><tr><td></td><td>MMLU [16]</td><td>EN</td><td>0-shot / CoT</td><td>Accuracy</td></tr><tr><td></td><td>KMMLU [45]</td><td>KO</td><td>0-shot / CoT</td><td>Accuracy</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comprehensive overview of the benchmark datasets used to evaluate the EXAONE 3.5 language models. It details the benchmark name, the languages used in the benchmark, the specific evaluation settings employed (e.g., whether it was a zero-shot evaluation or a chain-of-thought evaluation), and the metrics used to measure the model&rsquo;s performance (e.g., accuracy, F1 score, LLM score). The benchmarks are categorized into three groups reflecting different aspects of LLM capabilities: real-world use cases, long context understanding, and general domain abilities. Note that LongRAG has been extended from its original version, and Ko-LongRAG and Ko-WebRAG are custom benchmarks developed by the LG AI Research team.</p><details><summary>read the caption</summary>Table 4: The benchmarks used to evaluate the performance of EXAONE 3.5 language models along with their target languages, evaluation settings, and the metrics. LongRAG is extended from the original, and Ko-LongRAG and Ko-WebRAG are in-house benchmarks (see Section¬†3.4).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Models</th><th>Real-world Use Cases</th><th>Long Context</th><th>General Domain</th></tr></thead><tbody><tr><td>EXAONE 3.5 32B</td><td><strong>74.3</strong></td><td><strong>71.1</strong></td><td><strong>74.8</strong></td></tr><tr><td>Qwen 2.5 32B [49]</td><td><strong>69.8</strong></td><td><strong>66.9</strong></td><td><strong>78.7</strong></td></tr><tr><td>C4AI Command R 32B [10]</td><td>46.0</td><td>63.4</td><td>56.8</td></tr><tr><td>Gemma 2 27B [48]</td><td>64.2</td><td>-</td><td>68.7</td></tr><tr><td>Yi 1.5 34B [2]</td><td>46.9</td><td>-</td><td>53.9</td></tr><tr><td>EXAONE 3.5 7.8B</td><td><strong>70.7</strong></td><td><strong>66.6</strong></td><td><strong>70.2</strong></td></tr><tr><td>Qwen 2.5 7B [49]</td><td>52.7</td><td>56.1</td><td><strong>71.0</strong></td></tr><tr><td>Llama 3.1 8B [15]</td><td>48.6</td><td><strong>58.8</strong></td><td>62.4</td></tr><tr><td>Gemma 2 9B [48]</td><td><strong>57.9</strong></td><td>-</td><td>62.9</td></tr><tr><td>Phi 3 small (7B) [1]</td><td>41.7</td><td>33.4</td><td>63.2</td></tr><tr><td>EXAONE 3.5 2.4B</td><td><strong>61.1</strong></td><td><strong>63.4</strong></td><td><strong>63.3</strong></td></tr><tr><td>Qwen 2.5 3B [49]</td><td><strong>44.5</strong></td><td>40.7</td><td><strong>62.1</strong></td></tr><tr><td>Qwen 2.5 1.5B [49]</td><td>30.1</td><td>34.5</td><td>47.9</td></tr><tr><td>Llama 3.2 3B [34]</td><td>36.7</td><td><strong>44.2</strong></td><td>54.9</td></tr><tr><td>Gemma 2 2B [48]</td><td>41.7</td><td>-</td><td>42.2</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of the three EXAONE 3.5 language models (32B, 7.8B, and 2.4B parameters) against other similar-sized large language models across three benchmark categories: real-world use cases, long context understanding, and general domain tasks. A dash (-) indicates that a model does not support context lengths greater than 16K tokens. The best performance in each category is shown in bold, and the second-best is underlined. Details about the baseline models used for comparison can be found in Appendix D.1 of the paper.</p><details><summary>read the caption</summary>Table 5: Overall comparison results of EXAONE 3.5 language models with similar-sized baseline language models. Here, a dash (-) indicates the model does not support context lengths longer than 16K. Bold scores indicate the best performance, and underlined scores mean the second best. The detailed information for each baseline is described in Appendix¬†D.1.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Models</th><th>MT-Bench</th><th>LiveBench</th><th>Arena-Hard</th><th>AlpacaEval</th><th>IFEval</th><th>KoMT-Bench</th><th>LogicKor</th><th>Average</th></tr></thead><tbody><tr><td>EXAONE 3.5 32B</td><td><strong>8.51</strong></td><td><strong>43.0</strong></td><td><strong>78.6</strong></td><td><strong>60.6</strong></td><td><strong>81.7</strong></td><td><strong>8.05</strong></td><td><strong>9.06</strong></td><td><strong>74.3</strong></td></tr><tr><td>Qwen 2.5 32B</td><td><strong>8.49</strong></td><td><strong>50.6</strong></td><td><strong>67.0</strong></td><td>41.0</td><td><strong>78.7</strong></td><td><strong>7.75</strong></td><td><strong>8.89</strong></td><td><strong>69.8</strong></td></tr><tr><td>C4AI Command R 32B</td><td>7.38</td><td>29.7</td><td>17.0</td><td>25.9</td><td>26.1</td><td>6.72</td><td>8.24</td><td>46.0</td></tr><tr><td>Gemma 2 27B</td><td>8.28</td><td>40.0</td><td>57.5</td><td><strong>52.2</strong></td><td>59.7</td><td>7.19</td><td>8.56</td><td>64.2</td></tr><tr><td>Yi 1.5 34B</td><td>7.64</td><td>26.2</td><td>23.1</td><td>34.8</td><td>55.5</td><td>4.88</td><td>6.33</td><td>46.9</td></tr><tr><td>EXAONE 3.5 7.8B</td><td><strong>8.29</strong></td><td><strong>39.8</strong></td><td><strong>68.7</strong></td><td><strong>54.2</strong></td><td><strong>78.9</strong></td><td><strong>7.96</strong></td><td><strong>9.08</strong></td><td><strong>70.7</strong></td></tr><tr><td>Qwen 2.5 7B</td><td>6.48</td><td><strong>35.6</strong></td><td><strong>48.9</strong></td><td>31.7</td><td>72.5</td><td>5.19</td><td>6.38</td><td>52.7</td></tr><tr><td>Llama 3.1 8B</td><td>7.59</td><td>28.3</td><td>27.7</td><td>25.7</td><td><strong>74.5</strong></td><td>4.85</td><td>5.99</td><td>48.6</td></tr><tr><td>Gemma 2 9B</td><td><strong>7.64</strong></td><td>32.1</td><td>43.6</td><td><strong>47.3</strong></td><td>54.7</td><td><strong>7.10</strong></td><td><strong>8.05</strong></td><td><strong>57.9</strong></td></tr><tr><td>Phi 3 small (7B)</td><td>7.63</td><td>27.9</td><td>26.8</td><td>29.2</td><td>59.5</td><td>3.22</td><td>3.99</td><td>41.7</td></tr><tr><td>EXAONE 3.5 2.4B</td><td><strong>7.81</strong></td><td><strong>33.0</strong></td><td><strong>48.2</strong></td><td><strong>37.1</strong></td><td><strong>73.6</strong></td><td><strong>7.24</strong></td><td><strong>8.51</strong></td><td><strong>61.1</strong></td></tr><tr><td>Qwen 2.5 3B</td><td><strong>7.21</strong></td><td><strong>25.7</strong></td><td><strong>26.4</strong></td><td>17.4</td><td>60.8</td><td><strong>5.68</strong></td><td>5.21</td><td><strong>44.5</strong></td></tr><tr><td>Qwen 2.5 1.5B</td><td>5.72</td><td>19.2</td><td>10.6</td><td>8.4</td><td>40.7</td><td>3.87</td><td>3.60</td><td>30.1</td></tr><tr><td>Llama 3.2 3B</td><td>6.94</td><td>24.0</td><td>14.2</td><td>18.7</td><td><strong>70.1</strong></td><td>3.16</td><td>2.86</td><td>36.7</td></tr><tr><td>Gemma 2 2B</td><td>7.20</td><td>20.0</td><td>19.1</td><td><strong>29.1</strong></td><td>50.5</td><td>4.83</td><td><strong>5.29</strong></td><td>41.7</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of EXAONE 3.5 language models (32B, 7.8B, and 2.4B parameters) against other recently released models of similar sizes across seven real-world use case benchmarks. The benchmarks assess the models&rsquo; ability to understand and execute diverse instructions. Note that the macro average is calculated by multiplying the scores of MT-Bench, KoMT-Bench, and LogicKor by 10, because these benchmarks have a score out of 10, whereas the rest have scores out of 100. Bold scores indicate the best performing model for each benchmark, and underlined scores indicate the second-best performing model.</p><details><summary>read the caption</summary>Table 6: Performance comparison results of EXAONE 3.5 language models with similar-sized recently-released language models on seven benchmarks representing real-world use case scenarios. When calculating the macro average, the scores of MT-Bench, KoMT-Bench, and LogicKor are multiplied by 10 because they are scored out of 10 and the rest are scored out of 100. Bold scores indicate the best performance, and underlined scores mean the second best.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Models</th><th>LongBench</th><th>LongRAG</th><th>Ko-LongRAG</th><th>Ko-WebRAG</th><th>Average</th></tr></thead><tbody><tr><td>EXAONE 3.5 32B</td><td>49.2</td><td>67.6</td><td>85.3</td><td>82.3</td><td>71.1</td></tr><tr><td>Qwen 2.5 32B</td><td>49.1</td><td>63.6</td><td>73.5</td><td>81.3</td><td>66.9</td></tr><tr><td>C4AI Command R 32B</td><td>50.9</td><td>55.3</td><td>72.3</td><td>75.0</td><td>63.4</td></tr><tr><td>Gemma 2 27B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Yi 1.5 34B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>EXAONE 3.5 7.8B</td><td>46.0</td><td>68.3</td><td>71.7</td><td>80.3</td><td>66.6</td></tr><tr><td>Qwen 2.5 7B</td><td>47.2</td><td>60.1</td><td>55.3</td><td>61.7</td><td>56.1</td></tr><tr><td>Llama 3.1 8B</td><td>44.6</td><td>55.1</td><td>64.8</td><td>70.7</td><td>58.8</td></tr><tr><td>Gemma 2 9B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Phi 3 small (7B)</td><td>40.6</td><td>52.7</td><td>7.7</td><td>32.7</td><td>33.4</td></tr><tr><td>EXAONE 3.5 2.4B</td><td>42.7</td><td>63.3</td><td>74.7</td><td>73.0</td><td>63.4</td></tr><tr><td>Qwen 2.5 3B</td><td>42.0</td><td>45.8</td><td>40.5</td><td>34.7</td><td>40.7</td></tr><tr><td>Qwen 2.5 1.5B</td><td>37.1</td><td>39.0</td><td>33.8</td><td>28.0</td><td>34.5</td></tr><tr><td>Llama 3.2 3B</td><td>41.7</td><td>45.9</td><td>39.3</td><td>50.0</td><td>44.2</td></tr><tr><td>Gemma 2 2B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of EXAONE 3.5 language models (in 3 sizes: 32B, 7.8B, and 2.4B parameters) against other recently released, similar-sized language models across four long-context benchmarks: LongBench, LongRAG, Ko-LongRAG, and Ko-WebRAG. The benchmarks assess the models&rsquo; ability to understand and generate responses from lengthy contexts. A dash indicates that a model does not support contexts longer than 16K tokens. The table shows the individual benchmark scores and a macro average across all benchmarks. Bold scores represent the best performance in each benchmark, while underlined scores denote the second-best performance.</p><details><summary>read the caption</summary>Table 7: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released language models across four benchmarks representing long context scenarios. A dash (-) indicates that the model does not support context lengths longer than 16K. Context lengths for each model are detailed in Table¬†11. The average score in the rightmost is calculated as a macro average across the benchmarks. Bold scores indicate the best performance, and underlined scores mean the second best.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Models</th><th>GSM8K</th><th>MATH</th><th>HumanEval</th><th>MBPP</th><th>MMLU</th><th>KMMLU</th><th>GPQA</th><th>ARC-C</th><th>BBH</th><th>Average</th></tr></thead><tbody><tr><td>EXAONE 3.5 32B</td><td>91.9</td><td>70.5</td><td>87.2</td><td>81.8</td><td>78.3</td><td>57.0</td><td>39.7</td><td>91.7</td><td>75.3</td><td>74.8</td></tr><tr><td>Qwen 2.5 32B</td><td>92.0</td><td>76.5</td><td>89.0</td><td>88.9</td><td>81.4</td><td>62.1</td><td>40.9</td><td>95.1</td><td>82.7</td><td>78.7</td></tr><tr><td>C4AI Command R 32B</td><td>56.5</td><td>24.3</td><td>68.3</td><td>78.8</td><td>71.1</td><td>41.5</td><td>27.4</td><td>88.0</td><td>55.7</td><td>56.8</td></tr><tr><td>Gemma 2 27B</td><td>84.2</td><td>49.4</td><td>79.3</td><td>80.7</td><td>74.8</td><td>53.8</td><td>33.6</td><td>92.9</td><td>69.7</td><td>68.7</td></tr><tr><td>Yi 1.5 34B</td><td>83.7</td><td>52.0</td><td>5.5</td><td>35.7</td><td>75.3</td><td>41.7</td><td>30.0</td><td>93.9</td><td>67.6</td><td>53.9</td></tr><tr><td>EXAONE 3.5 7.8B</td><td>87.6</td><td>69.8</td><td>84.2</td><td>79.4</td><td>69.0</td><td>52.4</td><td>32.5</td><td>87.6</td><td>69.7</td><td>70.2</td></tr><tr><td>Qwen 2.5 7B</td><td>90.4</td><td>70.4</td><td>82.3</td><td>78.8</td><td>73.1</td><td>49.9</td><td>33.1</td><td>90.6</td><td>70.1</td><td>71.0</td></tr><tr><td>Llama 3.1 8B</td><td>82.1</td><td>48.8</td><td>67.7</td><td>70.6</td><td>72.4</td><td>45.9</td><td>27.4</td><td>83.7</td><td>63.3</td><td>62.4</td></tr><tr><td>Gemma 2 9B</td><td>82.0</td><td>44.6</td><td>68.3</td><td>75.1</td><td>73.7</td><td>34.6</td><td>27.9</td><td>90.5</td><td>69.7</td><td>62.9</td></tr><tr><td>Phi 3 small (7B)</td><td>86.3</td><td>47.8</td><td>72.6</td><td>72.0</td><td>68.8</td><td>33.4</td><td>25.3</td><td>90.4</td><td>72.5</td><td>63.2</td></tr><tr><td>EXAONE 3.5 2.4B</td><td>82.5</td><td>60.2</td><td>76.2</td><td>74.3</td><td>60.4</td><td>45.8</td><td>28.4</td><td>79.2</td><td>62.9</td><td>63.3</td></tr><tr><td>Qwen 2.5 3B</td><td>84.3</td><td>61.4</td><td>72.6</td><td>72.5</td><td>61.0</td><td>41.7</td><td>25.8</td><td>82.1</td><td>57.3</td><td>62.1</td></tr><tr><td>Qwen 2.5 1.5B</td><td>69.8</td><td>48.5</td><td>55.5</td><td>65.6</td><td>48.8</td><td>5.0</td><td>23.1</td><td>72.4</td><td>42.2</td><td>47.9</td></tr><tr><td>Llama 3.2 3B</td><td>77.4</td><td>46.6</td><td>54.9</td><td>60.6</td><td>64.9</td><td>35.0</td><td>23.2</td><td>78.0</td><td>53.8</td><td>54.9</td></tr><tr><td>Gemma 2 2B</td><td>29.8</td><td>18.7</td><td>45.7</td><td>55.0</td><td>56.1</td><td>37.4</td><td>22.6</td><td>76.3</td><td>38.2</td><td>42.2</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of EXAONE 3.5 language models of various sizes (32B, 7.8B, and 2.4B parameters) against other recently released, similarly sized language models across nine widely used general-domain benchmarks. These benchmarks assess the models&rsquo; abilities in diverse areas such as solving mathematical problems, generating code, and demonstrating broad factual knowledge. The comparison uses a macro-average of the scores across all nine benchmarks to provide a single, comprehensive performance metric. Bold scores highlight the top-performing model for each benchmark, while underlined scores indicate the second-best performance. This allows for a clear view of EXAONE 3.5&rsquo;s strengths and weaknesses relative to its competitors in various general reasoning tasks.</p><details><summary>read the caption</summary>Table 8: Performance comparison results of EXAONE 3.5 models with similar-sized recently-released language models on nine benchmarks representing general scenarios. The macro average is used to evaluate the overall performance. Bold scores indicate the best performance, and underlined scores mean the second best.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Category</th><th>Subcategory</th><th>Test Cases</th><th>Accuracy (32B)</th><th>Accuracy (7.8B)</th><th>Accuracy (2.4B)</th></tr></thead><tbody><tr><td>Bias</td><td>Gender & Sexual Orientation</td><td>295</td><td>91.2%</td><td>87.5%</td><td>76.6%</td></tr><tr><td></td><td>Race & Ethnicity & Nationality</td><td>432</td><td>86.8%</td><td>85.0%</td><td>72.2%</td></tr><tr><td></td><td>Political Affiliation</td><td>720</td><td>82.8%</td><td>79.9%</td><td>56.7%</td></tr><tr><td></td><td>Region</td><td>415</td><td>87.7%</td><td>84.6%</td><td>69.2%</td></tr><tr><td></td><td>Job</td><td>442</td><td>86.2%</td><td>81.9%</td><td>67.0%</td></tr><tr><td></td><td>Miscellaneous</td><td>406</td><td>85.2%</td><td>86.5%</td><td>73.2%</td></tr><tr><td></td><td><strong>Subtotal</strong></td><td><strong>2,710</strong></td><td><strong>86.0%</strong></td><td><strong>83.5%</strong></td><td><strong>67.4%</strong></td></tr><tr><td>Hate</td><td>Gender & Sexual Orientation</td><td>399</td><td>95.2%</td><td>92.2%</td><td>83.5%</td></tr><tr><td></td><td>Race & Ethnicity & Nationality</td><td>749</td><td>91.6%</td><td>88.4%</td><td>73.8%</td></tr><tr><td></td><td>Political Affiliation</td><td>1,164</td><td>85.7%</td><td>83.4%</td><td>66.2%</td></tr><tr><td></td><td>Region</td><td>499</td><td>92.0%</td><td>87.2%</td><td>74.1%</td></tr><tr><td></td><td>Job</td><td>852</td><td>91.0%</td><td>87.8%</td><td>72.3%</td></tr><tr><td></td><td><strong>Subtotal</strong></td><td><strong>3,663</strong></td><td><strong>90.0%</strong></td><td><strong>86.9%</strong></td><td><strong>72.2%</strong></td></tr><tr><td>Illegal</td><td>Illegal</td><td>1,126</td><td>92.9%</td><td>89.6%</td><td>80.3%</td></tr><tr><td>Sensitiveness</td><td>Contentious</td><td>710</td><td>83.1%</td><td>86.1%</td><td>79.0%</td></tr><tr><td></td><td>Ethical</td><td>966</td><td>81.2%</td><td>83.7%</td><td>72.8%</td></tr><tr><td></td><td>Predictive</td><td>825</td><td>79.8%</td><td>82.3%</td><td>71.0%</td></tr><tr><td></td><td><strong>Subtotal</strong></td><td><strong>2,501</strong></td><td><strong>81.2%</strong></td><td><strong>83.9%</strong></td><td><strong>74.0%</strong></td></tr><tr><td><strong>Overall</strong></td><td></td><td><strong>10,000</strong></td><td><strong>87.1%</strong></td><td><strong>85.6%</strong></td><td><strong>72.2%</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of evaluating the EXAONE 3.5 language models (32B, 7.8B, and 2.4B parameters) on the Korean Large Language Model Trustworthiness Benchmark Data [35]. This benchmark assesses the models&rsquo; safety and harmlessness by testing their responses to questions involving various harmful and dangerous categories, including bias (gender, race, politics, etc.), hate speech, illegal activities, and sensitive topics. The accuracy is measured as the percentage of times the model correctly selects appropriate answers from a set of provided options for each question.</p><details><summary>read the caption</summary>Table 9: Evaluation results of EXAONE 3.5 language models on the Korean Large Language Model Trustworthiness Benchmark Data¬†[35] to assess the model‚Äôs harmlessness. The accuracy is determined by the number of times the model selects appropriate options when presented with questions involving various harmful and dangerous categories, such as illegal content.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Benchmark</th><th>Benchmark example</th><th>Contaminated web corpus</th></tr></thead><tbody><tr><td>MMLU [16]</td><td>A teacher has three packages of stickers. One package contains 56 stickers, another package contains 48 stickers, and the third package contains 58 stickers. If the teacher divides all the stickers equally among 27 students, how many stickers will each student receive?<br>A. 6 stickers<br>B. 9 stickers<br>C. 54 stickers<br>D. 81 stickers<br>Answer:</td><td>(‚Ä¶truncated‚Ä¶) A teacher has three packages of stickers. One package contains 56 stickers, another package contains 48 stickers, and the third package contains 58 stickers. If the teacher divides all the stickers equally among 27 students, how many stickers will each student receive?<br>6 stickers is correct<br>#4 Last week Mario walked 7 3/4 miles. This week he walked 15 5/6 miles. What is the difference between the distance he walked this week and the distance he walked last week? (‚Ä¶truncated‚Ä¶)</td></tr><tr><td>KMMLU [45]</td><td>Íµ≠Í∞ÄÍ∞Ä Íµ≠ÎØºÏùò ÏÉùÌôúÏïàÏ†ïÍ≥º Î≥µÏßÄÏ¶ùÏßÑÏùÑ ÏúÑÌïòÏó¨ Î≥¥ÌóòÏùò ÏõêÎ¶¨Î•º ÎèÑÏûÖÌïòÏó¨ ÎßåÎì† ÏÇ¨ÌöåÎ≥¥ÌóòÏùò ÏùºÏ¢ÖÏúºÎ°ú Í∞ÄÏûÖÏûê, ÏÇ¨Ïö©Ïûê Î∞è Íµ≠Í∞ÄÎ°úÎ∂ÄÌÑ∞ ÏùºÏ†ïÌïú Î≥¥ÌóòÎ£åÎ•º Î∞õÍ≥† Ïù¥Î•º Ïû¨ÏõêÏúºÎ°ú Ïó¨Îü¨ Í∞ÄÏßÄ Ï†ïÌòïÌôîÎêú Î≥¥ÌóòÍ∏àÏùÑ ÏßÄÍ∏âÌïòÎäî ÏÇ¨ÌöåÎ≥¥Ïû•Ï†úÎèÑÎäî?<br>A. Íµ≠ÎØºÍ±¥Í∞ïÎ≥¥Ìóò<br>B. Íµ≠ÎØºÏó∞Í∏à<br>C. Í≥†Ïö©Î≥¥Ìóò<br>D. ÏÇ∞ÏóÖÏû¨Ìï¥Î≥¥ÏÉÅÎ≥¥Ìóò<br>Ï†ïÎãµ:<br><br>[Translation] What is the social security system, which is a type of social insurance created by the nation by introducing the principles of insurance to promote stability and welfare of citizens‚Äô lives, and which receives certain premiums from subscribers, employers, and the nation and use these funds to provide various standardized insurance benefits.<br>A. National Health Insurance<br>B. National Pension<br>C. Employment Insurance<br>D. Industrial Accident Compensation Insurance<br>Answer:</td><td>(‚Ä¶Ï§ëÎûµ‚Ä¶) ÎçîÍµ∞Îã§ÎÇò Í∞úÏù∏Ï£ºÏùòÏùò ÌôïÏÇ∞, ÌïµÍ∞ÄÏ°±ÌôîÏùò ÏßÑÏ†ÑÏóê Îî∞Îùº Ï†ÑÌÜµÏ†ÅÏù∏ Í∞ÄÏ°±Ïùò Ïó≠Ìï†Ïù∏ ÎÖ∏Ïù∏Î∂ÄÏñëÏùò Í∏∞Îä•Ïù¥ ÏïΩÌôîÎê®ÏúºÎ°úÏç® Íµ≠Í∞ÄÍ∞úÏûÖÏùò Ï§ëÏöîÏÑ±ÏùÄ ÎçîÏö± Ï¶ùÍ∞ÄÌïòÍ≤å ÎêòÏóàÎã§. Îî∞ÎùºÏÑú Íµ≠ÎØºÏó∞Í∏àÏ†úÎèÑÎäî Íµ≠Í∞ÄÍ∞Ä Íµ≠ÎØºÏùò ÏÉùÌôúÏïàÏ†ïÍ≥º Î≥µÏßÄÏ¶ùÏßÑÏùÑ ÏúÑÌïòÏó¨ Î≥¥ÌóòÏùò ÏõêÎ¶¨Î•º ÎèÑÏûÖÌïòÏó¨ ÎßåÎì† ÏÇ¨ÌöåÎ≥¥ÌóòÏùò ÏùºÏ¢ÖÏúºÎ°ú Í∞ÄÏûÖÏûê, ÏÇ¨Ïö©Ïûê Î∞è Íµ≠Í∞ÄÎ°úÎ∂ÄÌÑ∞ ÏùºÏ†ïÌïú Î≥¥ÌóòÎ£åÎ•º Î∞õÍ≥† Ïù¥Î•º Ïû¨ÏõêÏúºÎ°ú Ïó¨Îü¨ Í∞ÄÏßÄ Ï†ïÌòïÌôîÎêú Î≥¥ÌóòÍ∏àÏùÑ ÏßÄÍ∏âÌïòÎäî ÏÇ¨ÌöåÎ≥¥Ïû•Ï†úÎèÑÏù¥Îã§. (‚Ä¶Ï§ëÎûµ‚Ä¶)<br><br>[Translation] (‚Ä¶truncated‚Ä¶) Moreover, with the spread of individualism and the rise of nuclear families, the traditional family role of supporting the elderly has weakened, thereby increasing the importance of nation intervention. Accordingly, the National Pension System is a type of social insurance created by the nation by introducing the principles of insurance to promote stability and welfare of citizens‚Äô lives, and which receives certain premiums from subscribers, employers, and the nation and use these funds to provide various standardized insurance benefits. (‚Ä¶truncated‚Ä¶)</td></tr></tbody></table></table></figure><blockquote><p>üîº This table showcases examples of text passages found in both a benchmark test dataset and a large web corpus used for training language models. The grey-highlighted text represents sections that are identical in both sources, demonstrating how training data contamination can occur. The underlined text is the corresponding correct answer to the question from the test set. This illustrates how existing test examples in training data can lead to artificially inflated evaluation results.</p><details><summary>read the caption</summary>Table 10: Examples of contaminated web corpus. The text highlighted in grey is a part of the text that exists in both a benchmark test set and a web corpus. The text underlined is a corresponding golden answer.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model Name</th><th>Context Len.</th><th>Link</th><th>Release</th></tr></thead><tbody><tr><td>Qwen2.5 32B</td><td>128k</td><td><a href=https://huggingface.co/Qwen/Qwen2.5-32B-Instruct target=_blank>https://huggingface.co/Qwen/Qwen2.5-32B-Instruct</a></td><td>Sep., 2024</td></tr><tr><td>C4AI Command R 32B</td><td>128k</td><td><a href=https://huggingface.co/CohereForAI/c4ai-command-r-08-2024 target=_blank>https://huggingface.co/CohereForAI/c4ai-command-r-08-2024</a></td><td>Aug., 2024</td></tr><tr><td>Gemma 2 27B</td><td>8k</td><td><a href=https://huggingface.co/google/gemma-2-27b-it target=_blank>https://huggingface.co/google/gemma-2-27b-it</a></td><td>Jun., 2024</td></tr><tr><td>Yi 1.5 34B</td><td>16k</td><td><a href=https://huggingface.co/01-ai/Yi-1.5-34B-Chat-16K target=_blank>https://huggingface.co/01-ai/Yi-1.5-34B-Chat-16K</a></td><td>May, 2024</td></tr><tr><td>Qwen2.5 7B</td><td>128k</td><td><a href=https://huggingface.co/Qwen/Qwen2.5-7B-Instruct target=_blank>https://huggingface.co/Qwen/Qwen2.5-7B-Instruct</a></td><td>Sep., 2024</td></tr><tr><td>Llama 3.1 8B</td><td>128k</td><td><a href=https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct target=_blank>https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct</a></td><td>Jul., 2024</td></tr><tr><td>Gemma 2 9B</td><td>8k</td><td><a href=https://huggingface.co/google/gemma-2-9b-it target=_blank>https://huggingface.co/google/gemma-2-9b-it</a></td><td>Jun., 2024</td></tr><tr><td>Phi 3 small (7B)</td><td>128k</td><td><a href=https://huggingface.co/microsoft/Phi-3-small-128k-instruct target=_blank>https://huggingface.co/microsoft/Phi-3-small-128k-instruct</a></td><td>May, 2024</td></tr><tr><td>Qwen2.5 3B</td><td>32k</td><td><a href=https://huggingface.co/Qwen/Qwen2.5-3B-Instruct target=_blank>https://huggingface.co/Qwen/Qwen2.5-3B-Instruct</a></td><td>Sep., 2024</td></tr><tr><td>Qwen2.5 1.5B</td><td>32k</td><td><a href=https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct target=_blank>https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct</a></td><td>Sep., 2024</td></tr><tr><td>Llama 3.2 3B</td><td>128k</td><td><a href=https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct target=_blank>https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct</a></td><td>Sep., 2024</td></tr><tr><td>Gemma 2 2B</td><td>8k</td><td><a href=https://huggingface.co/google/gemma-2-2b-it target=_blank>https://huggingface.co/google/gemma-2-2b-it</a></td><td>Jul., 2024</td></tr></tbody></table></table></figure><blockquote><p>üîº This table lists the baseline language models used for comparison in the paper&rsquo;s experiments. For each model, it provides the model name, the maximum context length supported by the model, a link to the model on Hugging Face, and the release date of the model. This allows readers to easily find and access these models to reproduce the evaluation results presented in the paper and understand the context of the comparisons.</p><details><summary>read the caption</summary>Table 11: The list of baseline models used for the evaluation along with their supported context length and released date</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Language</th><th>Configuration</th><th>Details</th></tr></thead><tbody><tr><td>English</td><td>Haystack</td><td>Paul Graham essays [23]</td></tr><tr><td></td><td>Needle</td><td>‚ÄúThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.‚Äù</td></tr><tr><td></td><td>Query</td><td>‚ÄúWhat is the best thing to do in San Francisco?‚Äù</td></tr><tr><td></td><td>Instruction</td><td>‚ÄúAnalyze the content of the given document to locate the answer to the specified question. If found, provide the exact wording from the document without altering or summarizing it.‚Äù</td></tr><tr><td>Korean</td><td>Haystack</td><td>AI-Hub‚Å¥ ÎåÄÍ∑úÎ™® Íµ¨Îß§ÎèÑÏÑú Í∏∞Î∞ò ÌïúÍµ≠Ïñ¥ ÎßêÎ≠âÏπò Îç∞Ïù¥ÌÑ∞<br>(Large-scale Purchased Book-based Korean Language Corpus from AI-Hub)</td></tr><tr><td></td><td>Needle</td><td>‚ÄúÍ¥ëÌôîÎ¨∏ÏóêÏÑú Í∞ÄÏû• Ïû¨ÎØ∏ÏûàÎäî ÏùºÏùÄ ÌñáÏÇ¥ Ï¢ãÏùÄ ÎÇ†Ïóê ÏÉåÎìúÏúÑÏπòÎ•º Î®πÏúºÎ©∞ Ï≤≠ÏôÄÎåÄ ÏïàÏóê ÏûàÎäî Í≥µÏõêÏóê ÏïâÏïÑ ÏûàÎäî Í≤ÉÏûÖÎãàÎã§.‚Äù<br>(‚ÄúThe best thing to do at Gwanghwamun is eat a sandwich and sit in the park in the Blue House on a sunny day.‚Äù)</td></tr><tr><td></td><td>Query</td><td>‚ÄúÍ¥ëÌôîÎ¨∏ÏóêÏÑú Í∞ÄÏû• Ïû¨ÎØ∏ÏûàÎäî ÏùºÏù¥ Î¨¥ÏóáÏù∏Í∞ÄÏöî?‚Äù<br>(‚ÄúWhat is the best thing to do at Gwanghwamun?‚Äù)</td></tr><tr><td></td><td>Instruction</td><td>‚ÄúÏ£ºÏñ¥ÏßÑ Î¨∏ÏÑúÎ•º ÏùΩÍ≥† ÏßàÎ¨∏Ïóê ÎåÄÌïú ÎãµÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî. ÎãµÏùÑ Ï∞æÏúºÎ©¥, Î¨∏ÏÑúÏùò ÏõêÎ¨∏ÏùÑ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄÌïòÏó¨ ÏàòÏ†ïÏù¥ÎÇò Ìï¥ÏÑù ÏóÜÏù¥ Î∞òÌôòÌïòÏÑ∏Ïöî.‚Äù<br>(Identical to the English instruction)</td></tr></tbody></table></table></figure><blockquote><p>üîº The Needle-in-a-Haystack experiment tests a model&rsquo;s ability to locate and retrieve specific pieces of information (the &rsquo;needle&rsquo;) within long stretches of text (the &lsquo;haystack&rsquo;). The experiment uses two types of Haystacks, one English and one Korean, each composed of various texts, designed to act as distractors. A &lsquo;query&rsquo; is provided to the model to guide it toward the correct information. The table shows the specific texts used in both the English and Korean versions of the experiment as the Haystack, the specific piece of text acting as the Needle, and the query used to prompt the model to find the Needle within the Haystack.</p><details><summary>read the caption</summary>Table 12: Detailed configuration of the Needle-In-A-Haystack experiment. The ‚ÄúNeedle‚Äù refers to a specific text fragment embedded within the ‚ÄúHaystack,‚Äù which consists of long distractor texts. The task involves using a ‚ÄúQuery‚Äù as a cue to identify the needle within the haystack and retrieve the associated values.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Models</th><th>Single-doc QA</th><th>Multi-doc QA</th><th>Summarization</th><th>Few-shot Learning</th><th>Average</th></tr></thead><tbody><tr><td>EXAONE 3.5 32B</td><td>40.1</td><td>52.9</td><td>23.1</td><td>80.1</td><td>49.2</td></tr><tr><td>Qwen 2.5 32B</td><td>43.2</td><td>54.9</td><td>26.1</td><td>72.4</td><td>49.1</td></tr><tr><td>C4AI Command R 32B</td><td>44.6</td><td>48.9</td><td>26.4</td><td>83.6</td><td>50.9</td></tr><tr><td>Gemma 2 27B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Yi 1.5 34B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>EXAONE 3.5 7.8B</td><td>38.4</td><td>47.7</td><td>22.6</td><td>75.1</td><td>46.0</td></tr><tr><td>Qwen 2.5 7B</td><td>40.8</td><td>44.0</td><td>26.5</td><td>77.4</td><td>47.2</td></tr><tr><td>Llama 3.1 8B</td><td>39.8</td><td>41.2</td><td>27.6</td><td>69.9</td><td>44.6</td></tr><tr><td>Gemma 2 9B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Phi 3 small (7B)</td><td>33.2</td><td>26.5</td><td>26.3</td><td>76.2</td><td>40.6</td></tr><tr><td>EXAONE 3.5 2.4B</td><td>35.0</td><td>43.1</td><td>20.1</td><td>72.8</td><td>42.7</td></tr><tr><td>Qwen 2.5 3B</td><td>35.5</td><td>34.7</td><td>24.7</td><td>72.9</td><td>42.0</td></tr><tr><td>Qwen 2.5 1.5B</td><td>29.9</td><td>32.1</td><td>22.3</td><td>64.0</td><td>37.1</td></tr><tr><td>Llama 3.2 3B</td><td>33.9</td><td>34.9</td><td>25.8</td><td>72.3</td><td>41.7</td></tr><tr><td>Gemma 2 2B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of EXAONE 3.5 language models (32B, 7.8B, and 2.4B parameters) against other recently released language models of similar sizes across four long-context benchmarks. The benchmarks evaluate performance on Single-document Question Answering, Multi-document Question Answering, Summarization, and Few-shot Learning. Context length capabilities are noted, with a dash (-) indicating models that don&rsquo;t support context lengths over 16k tokens. The overall score for each model is the macro average across all four benchmarks. Bold scores highlight the top performance, and underlined scores show second-best performance.</p><details><summary>read the caption</summary>Table 13: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released language models across four benchmarks representing long context scenarios. Context lengths for each benchmark, as well as model limitations, are detailed in Table¬†11, where a dash (-) indicates that the model does not support context lengths longer than 16k. The final overall score for each model is calculated as a macro average across the benchmarks. Bold scores indicate the best performance, and underlined scores mean the second best.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Models</th><th>NQ Answerable</th><th>NQ Unanswerable</th><th>NQ Total</th><th>Hotpot QA Answerable</th><th>Hotpot QA Unanswerable</th><th>Hotpot QA Total</th><th>Average</th></tr></thead><tbody><tr><td>EXAONE 3.5 32B</td><td><strong>73.6</strong></td><td><strong>35.3</strong></td><td><strong>68.3</strong></td><td><strong>81.8</strong></td><td><strong>26.4</strong></td><td><strong>66.9</strong></td><td><strong>67.6</strong></td></tr><tr><td>Qwen 2.5 32B</td><td>62.3</td><td><strong>61.2</strong></td><td><strong>62.1</strong></td><td>62.9</td><td><strong>70.6</strong></td><td><strong>65.0</strong></td><td><strong>63.6</strong></td></tr><tr><td>C4AI Command R 32B</td><td><strong>64.0</strong></td><td>32.4</td><td>59.6</td><td><strong>63.1</strong></td><td>18.2</td><td>51.0</td><td>55.3</td></tr><tr><td>Gemma 2 27B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Yi 1.5 34B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>EXAONE 3.5 7.8B</td><td><strong>72.0</strong></td><td><strong>41.0</strong></td><td><strong>67.7</strong></td><td><strong>74.3</strong></td><td><strong>53.9</strong></td><td><strong>68.8</strong></td><td><strong>68.3</strong></td></tr><tr><td>Qwen 2.5 7B</td><td>64.5</td><td><strong>51.1</strong></td><td><strong>62.6</strong></td><td>61.8</td><td><strong>46.1</strong></td><td><strong>57.6</strong></td><td><strong>60.1</strong></td></tr><tr><td>Llama 3.1 8B</td><td>63.2</td><td>15.1</td><td>56.5</td><td><strong>67.4</strong></td><td>16.4</td><td>53.7</td><td>55.1</td></tr><tr><td>Gemma 2 9B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Phi 3 small (7B)</td><td><strong>66.8</strong></td><td>13.7</td><td>59.4</td><td>60.2</td><td>7.1</td><td>45.9</td><td>52.7</td></tr><tr><td>EXAONE 3.5 2.4B</td><td><strong>67.8</strong></td><td>25.9</td><td><strong>62.0</strong></td><td><strong>73.1</strong></td><td><strong>41.6</strong></td><td><strong>64.6</strong></td><td><strong>63.3</strong></td></tr><tr><td>Qwen 2.5 3B</td><td>49.5</td><td><strong>34.5</strong></td><td>47.4</td><td>52.5</td><td><strong>21.6</strong></td><td><strong>44.2</strong></td><td>45.8</td></tr><tr><td>Qwen 2.5 1.5B</td><td><strong>49.9</strong></td><td>18.0</td><td>45.5</td><td>43.6</td><td>2.2</td><td>32.5</td><td>39.0</td></tr><tr><td>Llama 3.2 3B</td><td>49.4</td><td><strong>41.7</strong></td><td><strong>48.3</strong></td><td><strong>53.6</strong></td><td>16.0</td><td>43.5</td><td><strong>45.9</strong></td></tr><tr><td>Gemma 2 2B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of EXAONE 3.5 language models (32B, 7.8B, and 2.4B parameters) against other recently released models of similar sizes on the LongRAG benchmark. LongRAG tests the models&rsquo; ability to answer questions using a large context of text. This table&rsquo;s unique feature is the inclusion of &lsquo;unanswerable&rsquo; cases in the benchmark. These are situations where the provided context does not contain the information necessary to answer the question. The models are evaluated on their ability to both answer answerable questions and correctly identify unanswerable ones. Performance is measured using an average score across all tasks. Bold scores indicate the top performance, while underlined scores show the second-best performance for each task.</p><details><summary>read the caption</summary>Table 14: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released language models with LongRAG benchmarks. The benchmark is extended with the ‚ÄúUnanswerable‚Äù case, which requires models to respond as ‚ÄúUnanswerable‚Äù when the information cannot be found within the context. Bold scores indicate the best performance, and underlined scores mean the second best.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Models</th><th>Single-doc QA Answerable</th><th>Single-doc QA Unanswerable</th><th>Single-doc QA Total</th><th>Multi-doc QA Answerable</th><th>Multi-doc QA Unanswerable</th><th>Multi-doc QA Total</th><th>Average</th></tr></thead><tbody><tr><td>EXAONE 3.5 32B</td><td>92.4</td><td>100.0</td><td>93.7</td><td>72.8</td><td>98.0</td><td>77.0</td><td>85.3</td></tr><tr><td>Qwen 2.5 32B</td><td>90.0</td><td>98.0</td><td>91.3</td><td>48.4</td><td>92.0</td><td>55.7</td><td>73.5</td></tr><tr><td>C4AI Command R 32B</td><td>85.6</td><td>66.0</td><td>82.3</td><td>62.4</td><td>62.0</td><td>62.3</td><td>72.3</td></tr><tr><td>Gemma 2 27B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Yi 1.5 34B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>EXAONE 3.5 7.8B</td><td>68.4</td><td>100.0</td><td>73.7</td><td>64.0</td><td>98.0</td><td>69.7</td><td>71.7</td></tr><tr><td>Qwen 2.5 7B</td><td>61.2</td><td>98.0</td><td>67.3</td><td>33.2</td><td>94.0</td><td>43.3</td><td>55.3</td></tr><tr><td>Llama 3.1 8B</td><td>78.0</td><td>76.0</td><td>77.7</td><td>56.8</td><td>28.0</td><td>52.0</td><td>64.8</td></tr><tr><td>Gemma 2 9B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Phi 3 small (7B)</td><td>8.0</td><td>14.0</td><td>9.0</td><td>4.8</td><td>14.0</td><td>6.3</td><td>7.7</td></tr><tr><td>EXAONE 3.5 2.4B</td><td>80.8</td><td>100.0</td><td>84.0</td><td>61.6</td><td>84.0</td><td>65.3</td><td>74.7</td></tr><tr><td>Qwen 2.5 3B</td><td>56.4</td><td>98.0</td><td>63.3</td><td>2.4</td><td>94.0</td><td>17.7</td><td>40.5</td></tr><tr><td>Qwen 2.5 1.5B</td><td>22.0</td><td>96.0</td><td>34.3</td><td>21.6</td><td>92.0</td><td>33.3</td><td>33.8</td></tr><tr><td>Llama 3.2 3B</td><td>48.8</td><td>12.0</td><td>42.7</td><td>40.0</td><td>16.0</td><td>36.0</td><td>39.3</td></tr><tr><td>Gemma 2 2B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of EXAONE 3.5 language models (of sizes 32B, 7.8B, and 2.4B) against other recently released, similarly sized language models. The comparison uses the Ko-LongRAG benchmark, which tests long-context comprehension and retrieval in Korean. The Ko-LongRAG benchmark includes &lsquo;unanswerable&rsquo; cases, where the models are expected to indicate when the information needed to answer the question is not present in the provided text. The table displays the performance on two subtasks (Single-doc QA and Multi-doc QA) and their average. Bold scores highlight the best performance for each model size, and underlined scores indicate the second-best performance.</p><details><summary>read the caption</summary>Table 15: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released language models with Ko-LongRAG benchmarks. The benchmark is extended with the ‚ÄúUnanswerable‚Äù case, which requires models to respond as ‚ÄúUnanswerable‚Äù when the information cannot be found within the context. Bold scores indicate the best performance, and underlined scores mean the second best.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-1a312068ac392592c2c085f9112291f9 class=gallery><img src=https://ai-paper-reviewer.com/2412.04862/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04862/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/&amp;title=EXAONE%203.5:%20Series%20of%20Large%20Language%20Models%20for%20Real-world%20Use%20Cases" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/&amp;text=EXAONE%203.5:%20Series%20of%20Large%20Language%20Models%20for%20Real-world%20Use%20Cases" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/&amp;subject=EXAONE%203.5:%20Series%20of%20Large%20Language%20Models%20for%20Real-world%20Use%20Cases" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.04862/index.md",oid_likes="likes_paper-reviews/2412.04862/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.05271/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-06T00:00:00+00:00>6 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.05210/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Evaluating and Aligning CodeLLMs on Human Preference</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-06T00:00:00+00:00>6 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>