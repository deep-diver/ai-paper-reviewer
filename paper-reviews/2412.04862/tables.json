[{"content": "| Model size | 32B | 7.8B | 2.4B |\n|---|---|---|---|\n| $d_model$ | 5,120 | 4,096 | 2,560 |\n| Number of layers | 64 | 32 | 30 |\n| Pre-normalization | True | True | True |\n| Non-linearity | SwiGLU [44] | SwiGLU | SwiGLU |\n| Feedforward dimension | 27,392 | 14,336 | 7,168 |\n| Head type | GQA [3] | GQA | GQA |\n| Number of heads | 40 | 32 | 32 |\n| Number of KV heads | 8 | 8 | 8 |\n| Head size | 128 | 128 | 80 |\n| Max sequence length | 32,768 | 32,768 | 32,768 |\n| RoPE theta [46] | 1,000,000 | 1,000,000 | 1,000,000 |\n| Tokenizer | BBPE [51] | BBPE | BBPE |\n| Vocab size | 102,400 | 102,400 | 102,400 |\n| Tied word embedding | False | False | True |", "caption": "Table 1: Configurations of EXAONE 3.5 language models", "description": "This table details the architecture and hyperparameters of the three EXAONE 3.5 language models (32B, 7.8B, and 2.4B parameters).  It lists key features such as the model size, number of layers, feed-forward dimensions, head types, and maximum sequence length.  The table also specifies implementation details, like the type of pre-normalization used and the activation function.", "section": "2.1 Model Configurations"}, {"content": "| Model size | 32B | 7.8B | 2.4B |\n|---|---|---|---|\n| Training tokens | 6.5T | 9T | 6.5T |\n| Amount of computation (FLOPs) | 1.25e+24 | 4.21e+23 | 9.36e+22 |", "caption": "Table 2: The sizes of the training data corpus along with the amounts of computation to build EXAONE 3.5 language models", "description": "This table presents the training data size and computational resource requirements for each of the three EXAONE 3.5 language models (2.4B, 7.8B, and 32B parameters).  It details the number of training tokens used and the amount of computation (in FLOPs) required for training each model. This information provides context regarding the scale of the training effort for each model and allows for comparison across different model sizes.", "section": "2.2 Pre-training"}, {"content": "| Models | Model size | Training tokens | Amount of computation (ratio) |\n|---|---|---|---|\n| EXAONE 3.5 | 32B | 6.5T | 1.00 |\n| Qwen 2.5 | 32B | 18T | 2.77 |\n| Gemma 2 | 27B | 13T | 1.69 |\n| Yi 1.5 | 34B | 3.6T | 0.59 |", "caption": "Table 3: Comparison of the total amounts of computations to build models. We approximate the amount of computations as the product of the model size and the number of training tokens. Although the EXAONE 3.5 32B model is behind in the computations compared to Qwen 2.5 and Gemma 2, it has shown competitive performances.", "description": "This table compares the computational cost of pre-training the EXAONE 3.5 32B language model against other large language models of similar size.  The cost is approximated by multiplying the model's parameter count by the number of training tokens. While EXAONE 3.5 32B had fewer total computations than Qwen 2.5 32B and Gemma 2 27B, it still achieved competitive performance on benchmark evaluations.", "section": "2.2.3 Training Cost"}, {"content": "| Category | Benchmark | Lang | Evaluation Settings | Metric |\n|---|---|---|---|---|\n| Real-world Use Cases | MT-Bench [59] | EN | LLM-as-a-judge (judge: gpt-4o-2024-08-06) | LLM score |\n|  | LiveBench [54] (v2024-08-31) | EN | Ground-truth match | Accuracy |\n|  | Arena-Hard-v0.1 [29] | EN | LLM-as-a-judge (judge: gpt-4-1106-preview) | Win rate |\n|  | AlpacaEval 2.0 LC [12] | EN | LLM-as-a-judge (judge: gpt-4-1106-preview) | Win rate |\n|  | IFEval [61] | EN | Prompt-level / strict accuracy | Accuracy |\n|  | KoMT-Bench [42] | KO | LLM-as-a-judge (judge: gpt-4o-2024-08-06) | LLM score |\n|  | LogicKor [37] | KO | LLM-as-a-judge (judge: gpt-4-1106-preview) | LLM score |\n| Long Context | Needle-In-A-Haystack [23] | EN/KO | Ground-truth match | Accuracy |\n|  | LongBench [5] | EN | Ground-truth match | F1, Rouge |\n|  | LongRAG [21] (extended) | EN | LLM-as-a-judge (judge: gpt-4o-2024-08-06) | LLM score |\n|  | Ko-LongRAG (In-house) | KO | LLM-as-a-judge (judge: gpt-4o-2024-08-06) | LLM score |\n|  | Ko-WebRAG (In-house) | KO | LLM-as-a-judge (judge: gpt-4o-2024-08-06) | LLM score |\n| General Domain | GSM8K [9] | EN | 0-shot / CoT | Accuracy |\n|  | MATH [17, 27] | EN | 0-shot / CoT | Accuracy |\n|  | HumanEval [6] | EN | 0-shot | pass@1 |\n|  | MBPP [4] | EN | 0-shot (Evalplus base) | pass@1 |\n|  | GPQA [40] | EN | 0-shot / CoT | Accuracy |\n|  | ARC-C [8] | EN | 0-shot | Accuracy |\n|  | BBH [47] | EN | 0-shot / CoT | Accuracy |\n|  | MMLU [16] | EN | 0-shot / CoT | Accuracy |\n|  | KMMLU [45] | KO | 0-shot / CoT | Accuracy |", "caption": "Table 4: The benchmarks used to evaluate the performance of EXAONE 3.5 language models along with their target languages, evaluation settings, and the metrics. LongRAG is extended from the original, and Ko-LongRAG and Ko-WebRAG are in-house benchmarks (see Section\u00a03.4).", "description": "This table presents a comprehensive overview of the benchmark datasets used to evaluate the EXAONE 3.5 language models.  It details the benchmark name, the languages used in the benchmark, the specific evaluation settings employed (e.g., whether it was a zero-shot evaluation or a chain-of-thought evaluation), and the metrics used to measure the model's performance (e.g., accuracy, F1 score, LLM score).  The benchmarks are categorized into three groups reflecting different aspects of LLM capabilities: real-world use cases, long context understanding, and general domain abilities.  Note that LongRAG has been extended from its original version, and Ko-LongRAG and Ko-WebRAG are custom benchmarks developed by the LG AI Research team.", "section": "3 Evaluation"}, {"content": "| Models | Real-world Use Cases | Long Context | General Domain |\n|---|---|---|---| \n| EXAONE 3.5 32B | **74.3** | **71.1** | **74.8** |\n| Qwen 2.5 32B [49] | **69.8** | **66.9** | **78.7** |\n| C4AI Command R 32B [10] | 46.0 | 63.4 | 56.8 |\n| Gemma 2 27B [48] | 64.2 | - | 68.7 |\n| Yi 1.5 34B [2] | 46.9 | - | 53.9 |\n| EXAONE 3.5 7.8B | **70.7** | **66.6** | **70.2** |\n| Qwen 2.5 7B [49] | 52.7 | 56.1 | **71.0** |\n| Llama 3.1 8B [15] | 48.6 | **58.8** | 62.4 |\n| Gemma 2 9B [48] | **57.9** | - | 62.9 |\n| Phi 3 small (7B) [1] | 41.7 | 33.4 | 63.2 |\n| EXAONE 3.5 2.4B | **61.1** | **63.4** | **63.3** |\n| Qwen 2.5 3B [49] | **44.5** | 40.7 | **62.1** |\n| Qwen 2.5 1.5B [49] | 30.1 | 34.5 | 47.9 |\n| Llama 3.2 3B [34] | 36.7 | **44.2** | 54.9 |\n| Gemma 2 2B [48] | 41.7 | - | 42.2 |", "caption": "Table 5: Overall comparison results of EXAONE 3.5 language models with similar-sized baseline language models. Here, a dash (-) indicates the model does not support context lengths longer than 16K. Bold scores indicate the best performance, and underlined scores mean the second best. The detailed information for each baseline is described in Appendix\u00a0D.1.", "description": "This table compares the performance of the three EXAONE 3.5 language models (32B, 7.8B, and 2.4B parameters) against other similar-sized large language models across three benchmark categories: real-world use cases, long context understanding, and general domain tasks.  A dash (-) indicates that a model does not support context lengths greater than 16K tokens.  The best performance in each category is shown in bold, and the second-best is underlined.  Details about the baseline models used for comparison can be found in Appendix D.1 of the paper.", "section": "3 Evaluation"}, {"content": "| Models | MT-Bench | LiveBench | Arena-Hard | AlpacaEval | IFEval | KoMT-Bench | LogicKor | Average |\n|---|---|---|---|---|---|---|---|---|\n| EXAONE 3.5 32B | **8.51** | **43.0** | **78.6** | **60.6** | **81.7** | **8.05** | **9.06** | **74.3** |\n| Qwen 2.5 32B | **8.49** | **50.6** | **67.0** | 41.0 | **78.7** | **7.75** | **8.89** | **69.8** |\n| C4AI Command R 32B | 7.38 | 29.7 | 17.0 | 25.9 | 26.1 | 6.72 | 8.24 | 46.0 |\n| Gemma 2 27B | 8.28 | 40.0 | 57.5 | **52.2** | 59.7 | 7.19 | 8.56 | 64.2 |\n| Yi 1.5 34B | 7.64 | 26.2 | 23.1 | 34.8 | 55.5 | 4.88 | 6.33 | 46.9 |\n| EXAONE 3.5 7.8B | **8.29** | **39.8** | **68.7** | **54.2** | **78.9** | **7.96** | **9.08** | **70.7** |\n| Qwen 2.5 7B | 6.48 | **35.6** | **48.9** | 31.7 | 72.5 | 5.19 | 6.38 | 52.7 |\n| Llama 3.1 8B | 7.59 | 28.3 | 27.7 | 25.7 | **74.5** | 4.85 | 5.99 | 48.6 |\n| Gemma 2 9B | **7.64** | 32.1 | 43.6 | **47.3** | 54.7 | **7.10** | **8.05** | **57.9** |\n| Phi 3 small (7B) | 7.63 | 27.9 | 26.8 | 29.2 | 59.5 | 3.22 | 3.99 | 41.7 |\n| EXAONE 3.5 2.4B | **7.81** | **33.0** | **48.2** | **37.1** | **73.6** | **7.24** | **8.51** | **61.1** |\n| Qwen 2.5 3B | **7.21** | **25.7** | **26.4** | 17.4 | 60.8 | **5.68** | 5.21 | **44.5** |\n| Qwen 2.5 1.5B | 5.72 | 19.2 | 10.6 | 8.4 | 40.7 | 3.87 | 3.60 | 30.1 |\n| Llama 3.2 3B | 6.94 | 24.0 | 14.2 | 18.7 | **70.1** | 3.16 | 2.86 | 36.7 |\n| Gemma 2 2B | 7.20 | 20.0 | 19.1 | **29.1** | 50.5 | 4.83 | **5.29** | 41.7 |", "caption": "Table 6: Performance comparison results of EXAONE 3.5 language models with similar-sized recently-released language models on seven benchmarks representing real-world use case scenarios. When calculating the macro average, the scores of MT-Bench, KoMT-Bench, and LogicKor are multiplied by 10 because they are scored out of 10 and the rest are scored out of 100. Bold scores indicate the best performance, and underlined scores mean the second best.", "description": "This table compares the performance of EXAONE 3.5 language models (32B, 7.8B, and 2.4B parameters) against other recently released models of similar sizes across seven real-world use case benchmarks.  The benchmarks assess the models' ability to understand and execute diverse instructions.  Note that the macro average is calculated by multiplying the scores of MT-Bench, KoMT-Bench, and LogicKor by 10, because these benchmarks have a score out of 10, whereas the rest have scores out of 100. Bold scores indicate the best performing model for each benchmark, and underlined scores indicate the second-best performing model.", "section": "3.3 Real-world Use Cases"}, {"content": "| Models | LongBench | LongRAG | Ko-LongRAG | Ko-WebRAG | Average |\n|---|---|---|---|---|---| \n| EXAONE 3.5 32B | 49.2 | 67.6 | 85.3 | 82.3 | 71.1 |\n| Qwen 2.5 32B | 49.1 | 63.6 | 73.5 | 81.3 | 66.9 |\n| C4AI Command R 32B | 50.9 | 55.3 | 72.3 | 75.0 | 63.4 |\n| Gemma 2 27B | - | - | - | - | - |\n| Yi 1.5 34B | - | - | - | - | - |\n| EXAONE 3.5 7.8B | 46.0 | 68.3 | 71.7 | 80.3 | 66.6 |\n| Qwen 2.5 7B | 47.2 | 60.1 | 55.3 | 61.7 | 56.1 |\n| Llama 3.1 8B | 44.6 | 55.1 | 64.8 | 70.7 | 58.8 |\n| Gemma 2 9B | - | - | - | - | - |\n| Phi 3 small (7B) | 40.6 | 52.7 | 7.7 | 32.7 | 33.4 |\n| EXAONE 3.5 2.4B | 42.7 | 63.3 | 74.7 | 73.0 | 63.4 |\n| Qwen 2.5 3B | 42.0 | 45.8 | 40.5 | 34.7 | 40.7 |\n| Qwen 2.5 1.5B | 37.1 | 39.0 | 33.8 | 28.0 | 34.5 |\n| Llama 3.2 3B | 41.7 | 45.9 | 39.3 | 50.0 | 44.2 |\n| Gemma 2 2B | - | - | - | - | - |", "caption": "Table 7: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released language models across four benchmarks representing long context scenarios. A dash (-) indicates that the model does not support context lengths longer than 16K. Context lengths for each model are detailed in Table\u00a011. The average score in the rightmost is calculated as a macro average across the benchmarks. Bold scores indicate the best performance, and underlined scores mean the second best.", "description": "This table compares the performance of EXAONE 3.5 language models (in 3 sizes: 32B, 7.8B, and 2.4B parameters) against other recently released, similar-sized language models across four long-context benchmarks: LongBench, LongRAG, Ko-LongRAG, and Ko-WebRAG.  The benchmarks assess the models' ability to understand and generate responses from lengthy contexts.  A dash indicates that a model does not support contexts longer than 16K tokens.  The table shows the individual benchmark scores and a macro average across all benchmarks.  Bold scores represent the best performance in each benchmark, while underlined scores denote the second-best performance.", "section": "3.4 Long Context"}, {"content": "| Models | GSM8K | MATH | HumanEval | MBPP | MMLU | KMMLU | GPQA | ARC-C | BBH | Average |\n|---|---|---|---|---|---|---|---|---|---|---|\n| EXAONE 3.5 32B | 91.9 | 70.5 | 87.2 | 81.8 | 78.3 | 57.0 | 39.7 | 91.7 | 75.3 | 74.8 |\n| Qwen 2.5 32B | 92.0 | 76.5 | 89.0 | 88.9 | 81.4 | 62.1 | 40.9 | 95.1 | 82.7 | 78.7 |\n| C4AI Command R 32B | 56.5 | 24.3 | 68.3 | 78.8 | 71.1 | 41.5 | 27.4 | 88.0 | 55.7 | 56.8 |\n| Gemma 2 27B | 84.2 | 49.4 | 79.3 | 80.7 | 74.8 | 53.8 | 33.6 | 92.9 | 69.7 | 68.7 |\n| Yi 1.5 34B | 83.7 | 52.0 | 5.5 | 35.7 | 75.3 | 41.7 | 30.0 | 93.9 | 67.6 | 53.9 |\n| EXAONE 3.5 7.8B | 87.6 | 69.8 | 84.2 | 79.4 | 69.0 | 52.4 | 32.5 | 87.6 | 69.7 | 70.2 |\n| Qwen 2.5 7B | 90.4 | 70.4 | 82.3 | 78.8 | 73.1 | 49.9 | 33.1 | 90.6 | 70.1 | 71.0 |\n| Llama 3.1 8B | 82.1 | 48.8 | 67.7 | 70.6 | 72.4 | 45.9 | 27.4 | 83.7 | 63.3 | 62.4 |\n| Gemma 2 9B | 82.0 | 44.6 | 68.3 | 75.1 | 73.7 | 34.6 | 27.9 | 90.5 | 69.7 | 62.9 |\n| Phi 3 small (7B) | 86.3 | 47.8 | 72.6 | 72.0 | 68.8 | 33.4 | 25.3 | 90.4 | 72.5 | 63.2 |\n| EXAONE 3.5 2.4B | 82.5 | 60.2 | 76.2 | 74.3 | 60.4 | 45.8 | 28.4 | 79.2 | 62.9 | 63.3 |\n| Qwen 2.5 3B | 84.3 | 61.4 | 72.6 | 72.5 | 61.0 | 41.7 | 25.8 | 82.1 | 57.3 | 62.1 |\n| Qwen 2.5 1.5B | 69.8 | 48.5 | 55.5 | 65.6 | 48.8 | 5.0 | 23.1 | 72.4 | 42.2 | 47.9 |\n| Llama 3.2 3B | 77.4 | 46.6 | 54.9 | 60.6 | 64.9 | 35.0 | 23.2 | 78.0 | 53.8 | 54.9 |\n| Gemma 2 2B | 29.8 | 18.7 | 45.7 | 55.0 | 56.1 | 37.4 | 22.6 | 76.3 | 38.2 | 42.2 |}", "caption": "Table 8: Performance comparison results of EXAONE 3.5 models with similar-sized recently-released language models on nine benchmarks representing general scenarios. The macro average is used to evaluate the overall performance. Bold scores indicate the best performance, and underlined scores mean the second best.", "description": "This table compares the performance of EXAONE 3.5 language models of various sizes (32B, 7.8B, and 2.4B parameters) against other recently released, similarly sized language models across nine widely used general-domain benchmarks.  These benchmarks assess the models' abilities in diverse areas such as solving mathematical problems, generating code, and demonstrating broad factual knowledge. The comparison uses a macro-average of the scores across all nine benchmarks to provide a single, comprehensive performance metric.  Bold scores highlight the top-performing model for each benchmark, while underlined scores indicate the second-best performance. This allows for a clear view of EXAONE 3.5's strengths and weaknesses relative to its competitors in various general reasoning tasks.", "section": "3 Evaluation"}, {"content": "| Category | Subcategory | Test Cases | Accuracy (32B) | Accuracy (7.8B) | Accuracy (2.4B) |\n|---|---|---|---|---|---| \n| Bias | Gender & Sexual Orientation | 295 | 91.2% | 87.5% | 76.6% |\n|  | Race & Ethnicity & Nationality | 432 | 86.8% | 85.0% | 72.2% |\n|  | Political Affiliation | 720 | 82.8% | 79.9% | 56.7% |\n|  | Region | 415 | 87.7% | 84.6% | 69.2% |\n|  | Job | 442 | 86.2% | 81.9% | 67.0% |\n|  | Miscellaneous | 406 | 85.2% | 86.5% | 73.2% |\n|  | **Subtotal** | **2,710** | **86.0%** | **83.5%** | **67.4%** |\n| Hate | Gender & Sexual Orientation | 399 | 95.2% | 92.2% | 83.5% |\n|  | Race & Ethnicity & Nationality | 749 | 91.6% | 88.4% | 73.8% |\n|  | Political Affiliation | 1,164 | 85.7% | 83.4% | 66.2% |\n|  | Region | 499 | 92.0% | 87.2% | 74.1% |\n|  | Job | 852 | 91.0% | 87.8% | 72.3% |\n|  | **Subtotal** | **3,663** | **90.0%** | **86.9%** | **72.2%** |\n| Illegal | Illegal | 1,126 | 92.9% | 89.6% | 80.3% |\n| Sensitiveness | Contentious | 710 | 83.1% | 86.1% | 79.0% |\n|  | Ethical | 966 | 81.2% | 83.7% | 72.8% |\n|  | Predictive | 825 | 79.8% | 82.3% | 71.0% |\n|  | **Subtotal** | **2,501** | **81.2%** | **83.9%** | **74.0%** |\n| **Overall** |  | **10,000** | **87.1%** | **85.6%** | **72.2%** |", "caption": "Table 9: Evaluation results of EXAONE 3.5 language models on the Korean Large Language Model Trustworthiness Benchmark Data\u00a0[35] to assess the model\u2019s harmlessness. The accuracy is determined by the number of times the model selects appropriate options when presented with questions involving various harmful and dangerous categories, such as illegal content.", "description": "This table presents the results of evaluating the EXAONE 3.5 language models (32B, 7.8B, and 2.4B parameters) on the Korean Large Language Model Trustworthiness Benchmark Data [35].  This benchmark assesses the models' safety and harmlessness by testing their responses to questions involving various harmful and dangerous categories, including bias (gender, race, politics, etc.), hate speech, illegal activities, and sensitive topics. The accuracy is measured as the percentage of times the model correctly selects appropriate answers from a set of provided options for each question.", "section": "3 Evaluation"}, {"content": "| Benchmark | Benchmark example | Contaminated web corpus |\n|---|---|---|\n| MMLU [16] | A teacher has three packages of stickers. One package contains 56 stickers, another package contains 48 stickers, and the third package contains 58 stickers. If the teacher divides all the stickers equally among 27 students, how many stickers will each student receive? <br> A. 6 stickers <br> B. 9 stickers <br> C. 54 stickers <br> D. 81 stickers <br> Answer: | (\u2026truncated\u2026)  A teacher has three packages of stickers. One package contains 56 stickers, another package contains 48 stickers, and the third package contains 58 stickers. If the teacher divides all the stickers equally among 27 students, how many stickers will each student receive? <br> 6 stickers is correct <br> #4 Last week Mario walked 7 3/4 miles. This week he walked 15 5/6 miles. What is the difference between the distance he walked this week and the distance he walked last week? (\u2026truncated\u2026) |\n| KMMLU [45] | \uad6d\uac00\uac00 \uad6d\ubbfc\uc758 \uc0dd\ud65c\uc548\uc815\uacfc \ubcf5\uc9c0\uc99d\uc9c4\uc744 \uc704\ud558\uc5ec \ubcf4\ud5d8\uc758 \uc6d0\ub9ac\ub97c \ub3c4\uc785\ud558\uc5ec \ub9cc\ub4e0 \uc0ac\ud68c\ubcf4\ud5d8\uc758 \uc77c\uc885\uc73c\ub85c \uac00\uc785\uc790, \uc0ac\uc6a9\uc790 \ubc0f \uad6d\uac00\ub85c\ubd80\ud130 \uc77c\uc815\ud55c \ubcf4\ud5d8\ub8cc\ub97c \ubc1b\uace0 \uc774\ub97c \uc7ac\uc6d0\uc73c\ub85c \uc5ec\ub7ec \uac00\uc9c0 \uc815\ud615\ud654\ub41c \ubcf4\ud5d8\uae08\uc744 \uc9c0\uae09\ud558\ub294 \uc0ac\ud68c\ubcf4\uc7a5\uc81c\ub3c4\ub294? <br> A. \uad6d\ubbfc\uac74\uac15\ubcf4\ud5d8 <br> B. \uad6d\ubbfc\uc5f0\uae08 <br> C. \uace0\uc6a9\ubcf4\ud5d8 <br> D. \uc0b0\uc5c5\uc7ac\ud574\ubcf4\uc0c1\ubcf4\ud5d8 <br> \uc815\ub2f5: <br> <br> [Translation] What is the social security system, which is  a type of social insurance created by the nation by introducing the principles of insurance to promote stability and welfare of citizens\u2019 lives, and which receives certain premiums from subscribers, employers, and the nation and use these funds to provide various standardized insurance benefits. <br> A. National Health Insurance <br> B. National Pension <br> C. Employment Insurance <br> D. Industrial Accident Compensation Insurance <br> Answer: | (\u2026\uc911\ub7b5\u2026) \ub354\uad70\ub2e4\ub098 \uac1c\uc778\uc8fc\uc758\uc758 \ud655\uc0b0, \ud575\uac00\uc871\ud654\uc758 \uc9c4\uc804\uc5d0 \ub530\ub77c \uc804\ud1b5\uc801\uc778 \uac00\uc871\uc758 \uc5ed\ud560\uc778 \ub178\uc778\ubd80\uc591\uc758 \uae30\ub2a5\uc774 \uc57d\ud654\ub428\uc73c\ub85c\uc368 \uad6d\uac00\uac1c\uc785\uc758 \uc911\uc694\uc131\uc740 \ub354\uc6b1 \uc99d\uac00\ud558\uac8c \ub418\uc5c8\ub2e4. \ub530\ub77c\uc11c \uad6d\ubbfc\uc5f0\uae08\uc81c\ub3c4\ub294 \uad6d\uac00\uac00 \uad6d\ubbfc\uc758 \uc0dd\ud65c\uc548\uc815\uacfc \ubcf5\uc9c0\uc99d\uc9c4\uc744 \uc704\ud558\uc5ec \ubcf4\ud5d8\uc758 \uc6d0\ub9ac\ub97c \ub3c4\uc785\ud558\uc5ec \ub9cc\ub4e0 \uc0ac\ud68c\ubcf4\ud5d8\uc758 \uc77c\uc885\uc73c\ub85c \uac00\uc785\uc790, \uc0ac\uc6a9\uc790 \ubc0f \uad6d\uac00\ub85c\ubd80\ud130 \uc77c\uc815\ud55c \ubcf4\ud5d8\ub8cc\ub97c \ubc1b\uace0 \uc774\ub97c \uc7ac\uc6d0\uc73c\ub85c \uc5ec\ub7ec \uac00\uc9c0 \uc815\ud615\ud654\ub41c \ubcf4\ud5d8\uae08\uc744 \uc9c0\uae09\ud558\ub294 \uc0ac\ud68c\ubcf4\uc7a5\uc81c\ub3c4\uc774\ub2e4. (\u2026\uc911\ub7b5\u2026) <br> <br> [Translation] (\u2026truncated\u2026) Moreover, with the spread of individualism and the rise of nuclear families, the traditional family role of supporting the elderly has weakened, thereby increasing the importance of nation intervention. Accordingly, the National Pension System is  a type of social insurance created by the nation by introducing the principles of insurance to promote stability and welfare of citizens\u2019 lives, and which receives certain premiums from subscribers, employers, and the nation and use these funds to provide various standardized insurance benefits. (\u2026truncated\u2026)|", "caption": "Table 10: Examples of contaminated web corpus. The  text highlighted in grey is a part of the text that exists in both a benchmark test set and a web corpus. The text underlined is a corresponding golden answer.", "description": "This table showcases examples of text passages found in both a benchmark test dataset and a large web corpus used for training language models.  The grey-highlighted text represents sections that are identical in both sources, demonstrating how training data contamination can occur. The underlined text is the corresponding correct answer to the question from the test set.  This illustrates how existing test examples in training data can lead to artificially inflated evaluation results.", "section": "2.2.2 Decontamination"}, {"content": "| Model Name | Context Len. | Link | Release |\n|---|---|---|---| \n| Qwen2.5 32B | 128k | https://huggingface.co/Qwen/Qwen2.5-32B-Instruct | Sep., 2024 |\n| C4AI Command R 32B | 128k | https://huggingface.co/CohereForAI/c4ai-command-r-08-2024 | Aug., 2024 |\n| Gemma 2 27B | 8k | https://huggingface.co/google/gemma-2-27b-it | Jun., 2024 |\n| Yi 1.5 34B | 16k | https://huggingface.co/01-ai/Yi-1.5-34B-Chat-16K | May, 2024 |\n| Qwen2.5 7B | 128k | https://huggingface.co/Qwen/Qwen2.5-7B-Instruct | Sep., 2024 |\n| Llama 3.1 8B | 128k | https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct | Jul., 2024 |\n| Gemma 2 9B | 8k | https://huggingface.co/google/gemma-2-9b-it | Jun., 2024 |\n| Phi 3 small (7B) | 128k | https://huggingface.co/microsoft/Phi-3-small-128k-instruct | May, 2024 |\n| Qwen2.5 3B | 32k | https://huggingface.co/Qwen/Qwen2.5-3B-Instruct | Sep., 2024 |\n| Qwen2.5 1.5B | 32k | https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct | Sep., 2024 |\n| Llama 3.2 3B | 128k | https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct | Sep., 2024 |\n| Gemma 2 2B | 8k | https://huggingface.co/google/gemma-2-2b-it | Jul., 2024 |", "caption": "Table 11: The list of baseline models used for the evaluation along with their supported context length and released date", "description": "This table lists the baseline language models used for comparison in the paper's experiments.  For each model, it provides the model name, the maximum context length supported by the model, a link to the model on Hugging Face, and the release date of the model. This allows readers to easily find and access these models to reproduce the evaluation results presented in the paper and understand the context of the comparisons.", "section": "D Evaluation Details"}, {"content": "| Language | Configuration | Details |\n|---|---|---|\n| English | Haystack | Paul Graham essays [23] |\n|  | Needle | \u201cThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\u201d |\n|  | Query | \u201cWhat is the best thing to do in San Francisco?\u201d |\n|  | Instruction | \u201cAnalyze the content of the given document to locate the answer to the specified question. If found, provide the exact wording from the document without altering or summarizing it.\u201d |\n| Korean | Haystack | AI-Hub\u2074 \ub300\uaddc\ubaa8 \uad6c\ub9e4\ub3c4\uc11c \uae30\ubc18 \ud55c\uad6d\uc5b4 \ub9d0\ubb49\uce58 \ub370\uc774\ud130 <br> (Large-scale Purchased Book-based Korean Language Corpus from AI-Hub) |\n|  | Needle | \u201c\uad11\ud654\ubb38\uc5d0\uc11c \uac00\uc7a5 \uc7ac\ubbf8\uc788\ub294 \uc77c\uc740 \ud587\uc0b4 \uc88b\uc740 \ub0a0\uc5d0 \uc0cc\ub4dc\uc704\uce58\ub97c \uba39\uc73c\uba70 \uccad\uc640\ub300 \uc548\uc5d0 \uc788\ub294 \uacf5\uc6d0\uc5d0 \uc549\uc544 \uc788\ub294 \uac83\uc785\ub2c8\ub2e4.\u201d <br> (\u201cThe best thing to do at Gwanghwamun is eat a sandwich and sit in the park in the Blue House on a sunny day.\u201d) |\n|  | Query | \u201c\uad11\ud654\ubb38\uc5d0\uc11c \uac00\uc7a5 \uc7ac\ubbf8\uc788\ub294 \uc77c\uc774 \ubb34\uc5c7\uc778\uac00\uc694?\u201d <br> (\u201cWhat is the best thing to do at Gwanghwamun?\u201d) |\n|  | Instruction | \u201c\uc8fc\uc5b4\uc9c4 \ubb38\uc11c\ub97c \uc77d\uace0 \uc9c8\ubb38\uc5d0 \ub300\ud55c \ub2f5\uc744 \ud655\uc778\ud558\uc138\uc694. \ub2f5\uc744 \ucc3e\uc73c\uba74, \ubb38\uc11c\uc758 \uc6d0\ubb38\uc744 \uadf8\ub300\ub85c \uc720\uc9c0\ud558\uc5ec \uc218\uc815\uc774\ub098 \ud574\uc11d \uc5c6\uc774 \ubc18\ud658\ud558\uc138\uc694.\u201d <br> (Identical to the English instruction) |", "caption": "Table 12: Detailed configuration of the Needle-In-A-Haystack experiment. The \u201cNeedle\u201d refers to a specific text fragment embedded within the \u201cHaystack,\u201d which consists of long distractor texts. The task involves using a \u201cQuery\u201d as a cue to identify the needle within the haystack and retrieve the associated values.", "description": "The Needle-in-a-Haystack experiment tests a model's ability to locate and retrieve specific pieces of information (the \"needle\") within long stretches of text (the \"haystack\").  The experiment uses two types of Haystacks, one English and one Korean, each composed of various texts, designed to act as distractors.  A \"query\" is provided to the model to guide it toward the correct information. The table shows the specific texts used in both the English and Korean versions of the experiment as the Haystack, the specific piece of text acting as the Needle, and the query used to prompt the model to find the Needle within the Haystack.", "section": "D.2.1 Needle-In-A-Haystack"}, {"content": "| Models | Single-doc QA | Multi-doc QA | Summarization | Few-shot Learning | Average |\n|---|---|---|---|---|---| \n| EXAONE 3.5 32B | 40.1 | 52.9 | 23.1 | 80.1 | 49.2 |\n| Qwen 2.5 32B | 43.2 | 54.9 | 26.1 | 72.4 | 49.1 |\n| C4AI Command R 32B | 44.6 | 48.9 | 26.4 | 83.6 | 50.9 |\n| Gemma 2 27B | - | - | - | - | - |\n| Yi 1.5 34B | - | - | - | - | - |\n| EXAONE 3.5 7.8B | 38.4 | 47.7 | 22.6 | 75.1 | 46.0 |\n| Qwen 2.5 7B | 40.8 | 44.0 | 26.5 | 77.4 | 47.2 |\n| Llama 3.1 8B | 39.8 | 41.2 | 27.6 | 69.9 | 44.6 |\n| Gemma 2 9B | - | - | - | - | - |\n| Phi 3 small (7B) | 33.2 | 26.5 | 26.3 | 76.2 | 40.6 |\n| EXAONE 3.5 2.4B | 35.0 | 43.1 | 20.1 | 72.8 | 42.7 |\n| Qwen 2.5 3B | 35.5 | 34.7 | 24.7 | 72.9 | 42.0 |\n| Qwen 2.5 1.5B | 29.9 | 32.1 | 22.3 | 64.0 | 37.1 |\n| Llama 3.2 3B | 33.9 | 34.9 | 25.8 | 72.3 | 41.7 |\n| Gemma 2 2B | - | - | - | - | - |", "caption": "Table 13: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released language models across four benchmarks representing long context scenarios. Context lengths for each benchmark, as well as model limitations, are detailed in Table\u00a011, where a dash (-) indicates that the model does not support context lengths longer than 16k. The final overall score for each model is calculated as a macro average across the benchmarks. Bold scores indicate the best performance, and underlined scores mean the second best.", "description": "This table compares the performance of EXAONE 3.5 language models (32B, 7.8B, and 2.4B parameters) against other recently released language models of similar sizes across four long-context benchmarks.  The benchmarks evaluate performance on Single-document Question Answering, Multi-document Question Answering, Summarization, and Few-shot Learning. Context length capabilities are noted, with a dash (-) indicating models that don't support context lengths over 16k tokens. The overall score for each model is the macro average across all four benchmarks.  Bold scores highlight the top performance, and underlined scores show second-best performance.", "section": "3.4 Long Context"}, {"content": "| Models | NQ Answerable | NQ Unanswerable | NQ Total | Hotpot QA Answerable | Hotpot QA Unanswerable | Hotpot QA Total | Average |\n|---|---|---|---|---|---|---|---| \n| EXAONE 3.5 32B | **73.6** | **35.3** | **68.3** | **81.8** | **26.4** | **66.9** | **67.6** |\n| Qwen 2.5 32B | 62.3 | **61.2** | **62.1** | 62.9 | **70.6** | **65.0** | **63.6** |\n| C4AI Command R 32B | **64.0** | 32.4 | 59.6 | **63.1** | 18.2 | 51.0 | 55.3 |\n| Gemma 2 27B | - | - | - | - | - | - | - |\n| Yi 1.5 34B | - | - | - | - | - | - | - |\n| EXAONE 3.5 7.8B | **72.0** | **41.0** | **67.7** | **74.3** | **53.9** | **68.8** | **68.3** |\n| Qwen 2.5 7B | 64.5 | **51.1** | **62.6** | 61.8 | **46.1** | **57.6** | **60.1** |\n| Llama 3.1 8B | 63.2 | 15.1 | 56.5 | **67.4** | 16.4 | 53.7 | 55.1 |\n| Gemma 2 9B | - | - | - | - | - | - | - |\n| Phi 3 small (7B) | **66.8** | 13.7 | 59.4 | 60.2 | 7.1 | 45.9 | 52.7 |\n| EXAONE 3.5 2.4B | **67.8** | 25.9 | **62.0** | **73.1** | **41.6** | **64.6** | **63.3** |\n| Qwen 2.5 3B | 49.5 | **34.5** | 47.4 | 52.5 | **21.6** | **44.2** | 45.8 |\n| Qwen 2.5 1.5B | **49.9** | 18.0 | 45.5 | 43.6 | 2.2 | 32.5 | 39.0 |\n| Llama 3.2 3B | 49.4 | **41.7** | **48.3** | **53.6** | 16.0 | 43.5 | **45.9** |\n| Gemma 2 2B | - | - | - | - | - | - | - |", "caption": "Table 14: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released language models with LongRAG benchmarks. The benchmark is extended with the \u201cUnanswerable\u201d case, which requires models to respond as \u201cUnanswerable\u201d when the information cannot be found within the context. Bold scores indicate the best performance, and underlined scores mean the second best.", "description": "This table compares the performance of EXAONE 3.5 language models (32B, 7.8B, and 2.4B parameters) against other recently released models of similar sizes on the LongRAG benchmark.  LongRAG tests the models' ability to answer questions using a large context of text. This table's unique feature is the inclusion of 'unanswerable' cases in the benchmark. These are situations where the provided context does not contain the information necessary to answer the question. The models are evaluated on their ability to both answer answerable questions and correctly identify unanswerable ones.  Performance is measured using an average score across all tasks.  Bold scores indicate the top performance, while underlined scores show the second-best performance for each task.", "section": "3.4 Long Context"}, {"content": "| Models | Single-doc QA Answerable | Single-doc QA Unanswerable | Single-doc QA Total | Multi-doc QA Answerable | Multi-doc QA Unanswerable | Multi-doc QA Total | Average |\n|---|---|---|---|---|---|---|---| \n| EXAONE 3.5 32B | 92.4 | 100.0 | 93.7 | 72.8 | 98.0 | 77.0 | 85.3 |\n| Qwen 2.5 32B | 90.0 | 98.0 | 91.3 | 48.4 | 92.0 | 55.7 | 73.5 |\n| C4AI Command R 32B | 85.6 | 66.0 | 82.3 | 62.4 | 62.0 | 62.3 | 72.3 |\n| Gemma 2 27B | - | - | - | - | - | - | - |\n| Yi 1.5 34B | - | - | - | - | - | - | - |\n| EXAONE 3.5 7.8B | 68.4 | 100.0 | 73.7 | 64.0 | 98.0 | 69.7 | 71.7 |\n| Qwen 2.5 7B | 61.2 | 98.0 | 67.3 | 33.2 | 94.0 | 43.3 | 55.3 |\n| Llama 3.1 8B | 78.0 | 76.0 | 77.7 | 56.8 | 28.0 | 52.0 | 64.8 |\n| Gemma 2 9B | - | - | - | - | - | - | - |\n| Phi 3 small (7B) | 8.0 | 14.0 | 9.0 | 4.8 | 14.0 | 6.3 | 7.7 |\n| EXAONE 3.5 2.4B | 80.8 | 100.0 | 84.0 | 61.6 | 84.0 | 65.3 | 74.7 |\n| Qwen 2.5 3B | 56.4 | 98.0 | 63.3 | 2.4 | 94.0 | 17.7 | 40.5 |\n| Qwen 2.5 1.5B | 22.0 | 96.0 | 34.3 | 21.6 | 92.0 | 33.3 | 33.8 |\n| Llama 3.2 3B | 48.8 | 12.0 | 42.7 | 40.0 | 16.0 | 36.0 | 39.3 |\n| Gemma 2 2B | - | - | - | - | - | - | - |", "caption": "Table 15: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released language models with Ko-LongRAG benchmarks. The benchmark is extended with the \u201cUnanswerable\u201d case, which requires models to respond as \u201cUnanswerable\u201d when the information cannot be found within the context. Bold scores indicate the best performance, and underlined scores mean the second best.", "description": "This table compares the performance of EXAONE 3.5 language models (of sizes 32B, 7.8B, and 2.4B) against other recently released, similarly sized language models. The comparison uses the Ko-LongRAG benchmark, which tests long-context comprehension and retrieval in Korean.  The Ko-LongRAG benchmark includes 'unanswerable' cases, where the models are expected to indicate when the information needed to answer the question is not present in the provided text. The table displays the performance on two subtasks (Single-doc QA and Multi-doc QA) and their average.  Bold scores highlight the best performance for each model size, and underlined scores indicate the second-best performance.", "section": "3.4 Long Context"}]