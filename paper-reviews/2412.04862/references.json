{"references": [{"fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "publication_date": "2024-04-14", "reason": "This paper is a technical report introducing a highly capable language model, serving as a significant baseline for comparison with the EXAONE 3.5 models."}, {"fullname_first_author": "Joshua Ainslie", "paper_title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints", "publication_date": "2023-12-01", "reason": "This paper introduces a novel training method for Transformer models (GQA) which is used in the EXAONE 3.5 models, improving efficiency and performance."}, {"fullname_first_author": "Yushi Bai", "paper_title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding", "publication_date": "2024-07-01", "reason": "LongBench is a key benchmark used to evaluate the long-context understanding capabilities of EXAONE 3.5, providing crucial comparative performance data."}, {"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-03", "reason": "This paper presents a benchmark (HumanEval) for evaluating code generation capabilities of LLMs, a key aspect of EXAONE 3.5's functionality."}, {"fullname_first_author": "Shouyuan Chen", "paper_title": "Extending context window of large language models via positional interpolation", "publication_date": "2023-06-15", "reason": "This paper details a technique used in EXAONE 3.5 to extend context window length, improving the model's ability to handle longer inputs and generate more coherent responses."}]}