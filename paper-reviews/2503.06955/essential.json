{"importance": "This work facilitates controllable multimodal motion generation, with a new dataset. This enables more versatile & precise motion generation. Future research can explore & refine this approach for virtual characters, human-computer interaction, & robotics, advancing the field.", "summary": "Motion Anything: control human motion generation with multimodal conditions like text and music.", "takeaways": ["Introduces 'Motion Anything,' a framework for multimodal motion generation that uses attention-based mask modeling.", "Presents a new dataset, 'Text-Music-Dance (TMD),' to support multimodal research.", "Achieves state-of-the-art results on multiple benchmarks, demonstrating improved control and coherence in motion generation."], "tldr": "Conditional motion generation struggles with prioritizing dynamic frames based on conditions and effectively integrating multiple modalities. Existing masking models need improvement for different conditions. To solve this, the paper introduces an 'Attention-based Mask Modeling' method for spatial and temporal control over key frames and actions. This will enable the model to focus on key actions during motion generation. In addition, this model adaptively encodes text and music to improve controllability.\n\nTo help this research, the paper presents 'Text-Music-Dance (TMD),' which has paired music and text. Experimental results show that this framework surpasses current state-of-the-art methods on multiple benchmarks, with significant improvement of about 15% in FID on HumanML3D.  It demonstrates consistent gains on the AIST++ and TMD datasets. This new method will push the boundary of conditional motion generation.", "affiliation": "ANU", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Generation"}, "podcast_path": "2503.06955/podcast.wav"}