[{"figure_path": "2410.20474/tables/table_8_0.html", "caption": "Table 1: Quantitative comparisons of grounding accuracy on HRS [3] and DrawBench [43] benchmarks. Bold represents the best, and underline represents the second best method.", "description": "Table 1 quantitatively compares the spatial grounding accuracy of different methods on the HRS and DrawBench datasets, highlighting GROUNDIT's superior performance.", "section": "6.2 Grounding Accuracy"}, {"figure_path": "2410.20474/tables/table_10_0.html", "caption": "Table 2: Quantitative comparisons on prompt fidelity on HRS benchmark [3]. Bold represents the best method.", "description": "Table 2 quantitatively compares the prompt fidelity of images generated by PixArt-R&B and GROUNDIT using three metrics: CLIP score, ImageReward, and PickScore.", "section": "6.3 Prompt Fidelity"}, {"figure_path": "2410.20474/tables/table_12_0.html", "caption": "Table 1: Quantitative comparisons of grounding accuracy on HRS [3] and DrawBench [43] benchmarks. Bold represents the best, and underline represents the second best method.", "description": "Table 1 quantitatively compares the spatial grounding performance of GROUNDIT against several baselines across two benchmark datasets, HRS and DrawBench, using three evaluation criteria: spatial, size, and color.", "section": "6.2 Grounding Accuracy"}, {"figure_path": "2410.20474/tables/table_15_0.html", "caption": "Table 1: Quantitative comparisons of grounding accuracy on HRS [3] and DrawBench [43] benchmarks. Bold represents the best, and underline represents the second best method.", "description": "Table 1 quantitatively compares the grounding accuracy (spatial, size, and color) of different methods on the HRS and DrawBench datasets, highlighting the superior performance of GROUNDIT.", "section": "6.2 Grounding Accuracy"}, {"figure_path": "2410.20474/tables/table_16_0.html", "caption": "Table 4: Quantitative comparisons of mIoU (\u2191) on a subset of MS-COCO-2014 [33], HRS-Spatial [3], and our custom dataset. Bold represents the best, and underline represents the second best method.", "description": "Table 4 quantitatively compares the mean Intersection over Union (mIoU) scores achieved by GROUNDIT and baseline methods across three datasets with varying numbers of bounding boxes, demonstrating GROUNDIT's superior performance in spatial grounding.", "section": "6.2 Grounding Accuracy"}, {"figure_path": "2410.20474/tables/table_16_1.html", "caption": "Table 2: Quantitative comparisons on prompt fidelity on HRS benchmark [3]. Bold represents the best method.", "description": "Table 2 quantitatively compares the prompt fidelity of images generated by GROUNDIT and PixArt-R&B using three metrics: CLIP score, ImageReward, and PickScore.", "section": "6.3 Prompt Fidelity"}, {"figure_path": "2410.20474/tables/table_18_0.html", "caption": "Table 1: Quantitative comparisons of grounding accuracy on HRS [3] and DrawBench [43] benchmarks. Bold represents the best, and underline represents the second best method.", "description": "Table 1 quantitatively compares the grounding accuracy (spatial, size, and color) of different methods on the HRS and DrawBench datasets, showing GROUNDIT's superior performance.", "section": "6 Results"}, {"figure_path": "2410.20474/tables/table_18_1.html", "caption": "Table 6: Comparison of average execution time based on the number of bounding boxes. Values in the table are given in seconds", "description": "This table shows the average execution time of different models (R&B, PixArt-R&B, and GROUNDIT) for varying numbers of bounding boxes in an image, demonstrating the computational cost increase with more bounding boxes.", "section": "A.6 Analysis on Computation Time"}]