[{"figure_path": "https://arxiv.org/html/2501.00599/x3.png", "caption": "Figure 1: Comparisons with previous general and specialized MLLMs. Our VideoRefer excels in multiple fine-grained regional and temporal video understanding tasks, including basic video object referring, complex video relationship analysis, and video object retrieval.", "description": "Figure 1 compares VideoRefer to other general and specialized Multimodal Large Language Models (MLLMs) for video understanding.  It highlights VideoRefer's superiority in handling fine-grained spatial and temporal details within videos, outperforming other methods on three key tasks:  video object referring (identifying and describing specific objects), complex video relationship analysis (understanding interactions between multiple objects), and video object retrieval (locating objects within the video based on textual descriptions). The figure visually demonstrates these capabilities, showing how VideoRefer excels compared to previous models which primarily focused on holistic video understanding.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.00599/x4.png", "caption": "Figure 2: A multi-agent data engine for the construction of our VideoRefer-700K.", "description": "This figure illustrates the multi-agent data engine used to create the VideoRefer-700K dataset.  The engine leverages several expert models working collaboratively to generate high-quality object-level video instruction data. The steps involved include analyzing the video for nouns, generating object-level captions, creating pixel-level masks for objects, verifying the accuracy of the annotations, and refining the final captions. This process results in a dataset with detailed captions, short captions, and multi-round QA pairs for each object across multiple video frames.", "section": "3.1 VideoRefer-700K Dataset"}, {"figure_path": "https://arxiv.org/html/2501.00599/x5.png", "caption": "Figure 3: Model architecture of our VideoRefer for spatial-temporal video object understanding.", "description": "This figure details the architecture of the VideoRefer model, designed for comprehensive spatial-temporal understanding in videos.  It illustrates how the model processes both single-frame and multi-frame video data to generate object-level representations. The model incorporates a shared visual encoder, a versatile spatial-temporal object encoder (consisting of a Spatial Token Extractor and a Temporal Token Merge Module), and an instruction-following large language model (LLM). The object encoder handles free-form input regions and merges temporal information from multiple frames. The final output is generated by the LLM, which integrates image-level, object-level, and linguistic embeddings.", "section": "3.2 VideoRefer Model"}, {"figure_path": "https://arxiv.org/html/2501.00599/x6.png", "caption": "Figure 4: Exemplar visual illustration of VideoRefer-Bench.", "description": "Figure 4 is a visual representation of the VideoRefer-Bench benchmark. It shows example queries and their corresponding ground truth descriptions for different aspects of video understanding, such as single-object descriptions, complex reasoning, and future prediction. The figure highlights the multidimensional nature of the benchmark, illustrating how it evaluates different aspects of video comprehension.", "section": "3.3 VideoRefer-Bench"}, {"figure_path": "https://arxiv.org/html/2501.00599/x7.png", "caption": "Figure 5: Data characteristics of VideoRefer-Bench.", "description": "This figure presents a detailed breakdown of the data characteristics used in the VideoRefer-Bench benchmark.  Specifically, it shows the distribution of categories within the benchmark (e.g., people, animals, transportation, etc.) and illustrates the proportion of different question types, providing insights into the diversity and complexity of questions included within the benchmark.  The distribution of question types are shown to provide a balance between different types of reasoning abilities.", "section": "3.3 VideoRefer-Bench"}, {"figure_path": "https://arxiv.org/html/2501.00599/x8.png", "caption": "Figure 6: Visual comparisons between our VideoRefer with general GPT-4o and regional video-level Elysium and Artemis. Here we provide detailed illustrations on VideoRefer-BenchDD{}^{\\text{D}}start_FLOATSUPERSCRIPT D end_FLOATSUPERSCRIPT.", "description": "Figure 6 presents a qualitative comparison of video descriptions generated by VideoRefer, GPT-40, Elysium, and Artemis on the VideoRefer-BenchD benchmark.  The figure directly compares the output of each model for the same video clip, demonstrating VideoRefer's superior ability to capture fine-grained details, temporal aspects, and relationships within the video.  Each model's description showcases its strengths and weaknesses regarding accuracy, completeness, and overall quality of the video understanding.", "section": "3.3 VideoRefer-Bench"}, {"figure_path": "https://arxiv.org/html/2501.00599/x9.png", "caption": "Figure 7: Visualizations of similarity among adjacent object-level token pairs across the temporal dimension. Here, we use cosine similarity as the measurement.", "description": "This figure displays the cosine similarity between consecutive object-level tokens across temporal sequences within a video.  Each row represents a different video segment showing how the similarity between object tokens changes over time. The heatmap shows that for most videos the similarity between consecutive tokens is high, indicating that the object tokens capture the temporal relationships between adjacent frames. The x-axis represents the frame number, and the y-axis represents video samples. The color intensity of each cell indicates the cosine similarity between the respective tokens.", "section": "3.2.2 A Versatile Spatial-Temporal Object Encoder"}, {"figure_path": "https://arxiv.org/html/2501.00599/x10.png", "caption": "Figure 8: Visual illustrations of the data distribution for each training stage.", "description": "This figure shows the data distribution for each of the four training stages of the VideoRefer model. Stage 1 focuses on Image-Text Alignment Pre-training using 10M samples. Stage 2 performs Region-Text Alignment Pre-training with 511K samples. Stage 2.5 involves High-Quality Knowledge Learning using 118K image-caption, 30K video-caption, 79K image-level region-caption and 125K video-level region-caption pairs. Finally, Stage 3 conducts Visual Instruction Tuning with 478K samples and additional 115K video-level region-caption pairs.  The number of samples in each stage reflects the increasing complexity and specificity of the training data.", "section": "3.1 VideoRefer-700K Dataset"}, {"figure_path": "https://arxiv.org/html/2501.00599/x11.png", "caption": "Figure 9: Data distributions of our VideoRefer-700K dataset, encompassing five different data types.", "description": "Figure 9 shows the distribution of data types within the VideoRefer-700K dataset.  The dataset is composed of five main categories: short captions, detailed captions, basic questions, reasoning questions, and future predictions.  Each bar in the chart represents the number of samples in that category, illustrating the relative proportion of each data type within the overall dataset. This visualization helps understand the dataset's composition and balance across different task types.", "section": "3.1 VideoRefer-700K Dataset"}, {"figure_path": "https://arxiv.org/html/2501.00599/x12.png", "caption": "Figure 10: Visual illustrations of human check process. TP, TN, FP and FN are introduced for the assessment on Reviewer.", "description": "This figure visually demonstrates the human evaluation process used to assess the performance of the 'Reviewer' component within a multi-agent data engine.  The Reviewer's task is to verify the accuracy of object-level annotations generated automatically.  The image displays several examples of video clips and their corresponding annotations. Each example is categorized into one of four groups based on the Reviewer's assessment and the ground truth: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).  True Positives represent correctly identified accurate annotations. True Negatives are correctly identified inaccurate annotations. False Positives are inaccurate annotations incorrectly identified as accurate.  False Negatives are accurate annotations incorrectly identified as inaccurate. This process is essential in ensuring the quality and reliability of the dataset.", "section": "3.1 VideoRefer-700K Dataset"}, {"figure_path": "https://arxiv.org/html/2501.00599/x13.png", "caption": "Figure 11: A detailed illustrative example of the construction pipeline in our multi-agent data engine.", "description": "This figure illustrates the multi-agent data engine used to create the VideoRefer-700K dataset.  The process begins with noun extraction from video captions using an analyzer. Then, an annotator generates detailed descriptions of these objects, focusing on both appearance and motion. A segmentor creates pixel-level masks for each object in a randomly selected frame, which are then propagated to all other frames using SAM 2 [34]. A reviewer verifies the accuracy of the descriptions, discarding inaccurate ones. Finally, a refiner summarizes and refines the captions to ensure coherence and accuracy, resulting in a high-quality dataset with detailed object descriptions and corresponding masks.", "section": "3.1 VideoRefer-700K Dataset"}, {"figure_path": "https://arxiv.org/html/2501.00599/x14.png", "caption": "Figure 12: Visualization results of VideoRefer across various tasks, including single-object referring, video relationship analysis, complex reasoning, future prediction, video object retrieval, as well as general video understanding and image object understanding.", "description": "Figure 12 showcases VideoRefer's capabilities on various video understanding tasks.  Each row presents a different task: single-object referring (describing a specific object), video relationship analysis (explaining the interaction between objects), complex reasoning (inferring information about an object's role), future prediction (predicting an object's future action), video object retrieval (identifying an object based on a textual query), and general video understanding (providing a holistic summary of the video). For each task, example video clips and VideoRefer's corresponding response are displayed, highlighting the model's ability to understand various levels of video comprehension.", "section": "3.3 VideoRefer-Bench"}, {"figure_path": "https://arxiv.org/html/2501.00599/x15.png", "caption": "Figure 13: Visual samples from our VideoRefer-700 dataset, typical including short descriptions, detailed descriptions, and QA pairs.", "description": "Figure 13 shows example data from the VideoRefer-700K dataset.  It demonstrates the three types of annotations included: short descriptions (concise summaries of the video's content), detailed descriptions (more thorough and comprehensive explanations), and question-answer (QA) pairs (questions about the video and their corresponding answers). This variety of annotations helps to ensure that the VideoRefer model can accurately understand and respond to various queries about the video.", "section": "3.1 VideoRefer-700K Dataset"}, {"figure_path": "https://arxiv.org/html/2501.00599/x16.png", "caption": "Figure 14: Visual examples of our VideoRefer-Bench, including VideoRefer-BenchDD{}^{\\text{D}}start_FLOATSUPERSCRIPT D end_FLOATSUPERSCRIPT and VideoRefer-BenchQQ{}^{\\text{Q}}start_FLOATSUPERSCRIPT Q end_FLOATSUPERSCRIPT.", "description": "Figure 14 presents visual examples from the VideoRefer-Bench benchmark, which is designed to comprehensively evaluate the capabilities of Video LLMs in video understanding.  The benchmark consists of two sub-benchmarks: VideoRefer-BenchD (for Description Generation) and VideoRefer-BenchQ (for Multiple-choice Question Answering). The figure showcases diverse examples illustrating the various task types within each sub-benchmark, demonstrating the complexity and variety of questions that VideoRefer-Bench assesses.  This includes questions about basic object identification and descriptions, sequential actions, relationships between objects, reasoning, and future predictions.", "section": "3.3 VideoRefer-Bench"}]