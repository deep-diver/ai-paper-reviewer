[{"content": "| Benchmark | Data Size | Language | Data Source | Domain | Reasoning | Metric |\n|---|---|---|---|---|---|---|\n| WebQA (Li et al., 2016) | 42187 | Chinese | Real World | Knowledge | \u2713 | Accuracy |\n| MMLU (Hendrycks et al., 2021) | 15,908 | English | Exams & Textbooks | Knowledge | \u2713 | Accuracy |\n| CMMLU (Li et al., 2023a) | 11,528 | Chinese | Exams | Knowledge | \u2713 | Accuracy |\n| GSM8K (Cobbe et al., 2021) | 8,792 | English | Human Writers | Math | \u2713 | Accuracy |\n| AlpacaEval (Li et al., 2023d) | 805 | English | Alpaca Data | General | \u2713 | LLM-as-a-Judge |\n| MT-Bench (Zheng et al., 2023) | 80 | English | Self-constructed | General | \u2713 | LLM-as-a-Judge |\n| Arena-Hard (Li et al., 2024) | 500 | English | Human Writers | General | \u2713 | LLM-as-a-Judge |\n| C-Eval (Huang et al., 2023) | 13,948 | Chinese | Exams | Knowledge | \u2713 | Accuracy |\n| SimpleQA (Wei et al., 2024) | 4,326 | English | Human Writers | Knowledge | \u00d7 | LLM-as-a-Judge |\n| **Chinese SimpleQA (Ours)** | 3000 | Chinese | Self-constructed & Human Writers | Knowledge | \u00d7 | LLM-as-a-Judge |", "caption": "Table 1: Comparisons between our Chinese SimpleQA and other benchmarks.", "description": "This table compares the characteristics of Chinese SimpleQA with those of other prominent large language model (LLM) evaluation benchmarks.  The comparison includes data size, language, data sources, domains covered, reasoning types required, and the evaluation metrics used. This allows for a clear understanding of how Chinese SimpleQA differs from and builds upon existing benchmarks.", "section": "2 CHINESE SIMPLEQA"}, {"content": "| Self-constructed & Human Writers |\n|---|---|", "caption": "Table 2: Dataset statistics of Chinese SimpleQA.", "description": "This table presents a statistical overview of the Chinese SimpleQA dataset, including the total number of problems, their distribution across six primary topics and their respective subtopics, and the length characteristics of both questions and answers.", "section": "2 CHINESE SIMPLEQA"}, {"content": "| Statistics | Number | Statistics | Number |\n|---|---|---|---| \n| **#Problems** | 3000 | **Length** |  |\n| **Primary Topics** |  | **Question Length** |  |\n| - Chinese Culture | 323 | - *maximum length* | 81 |\n| - Humanities | 623 | - *minimum length* | 8 |\n| - Engineering, Technology | 473 | - *avg length* | 23.6 |\n| and Applied Sciences |  | **Reference Answer Length** |  |\n| - Life, Art and Culture | 602 | - *maximum length* | 47 |\n| - Society | 450 | - *minimum length* | 1 |\n| - Natural Science | 529 | - *avg length* | 6.1 |", "caption": "Table 3: Results of different models on Chinese SimpleQA. For metrics,\nCO, NA, IN, and CGA denote \u201cCorrect\u201d, \u201cNot attempted\u201d, \u201cIncorrect\u201d, and \u201cCorrect given attempted\u201d, respectively.\nFor subtopics, CC, HU, ETAS, LAC, SO and NS represent \u201cChinese Culture\u201d, \u201cHumanities\u201d, \u201cEngineering, Technology, and Applied Sciences\u201d, \u201cLife, Art, and Culture\u201d, \u201cSociety\u201d, and \u201cNatural Science\u201d, respectively.", "description": "This table presents the performance of various large language models (LLMs) on the Chinese SimpleQA benchmark.  The benchmark evaluates the models' ability to answer short, factual questions in Chinese across six primary topic areas: Chinese Culture (CC), Humanities (HU), Engineering, Technology, and Applied Sciences (ETAS), Life, Art, and Culture (LAC), Society (SO), and Natural Science (NS). The results are presented using five metrics: Correct (CO), Not Attempted (NA), Incorrect (IN), Correct Given Attempted (CGA), and F-score.  Each metric assesses a different aspect of the model's factuality performance. The table allows for a detailed comparison of LLMs, revealing their strengths and weaknesses within each topic and overall.", "section": "3.2 Main Results"}]