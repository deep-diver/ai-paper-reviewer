{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational in establishing the capabilities of large language models as few-shot learners, which is highly relevant to the field of LLM evaluation."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2021-01-01", "reason": "The paper introduces the MMLU benchmark, a significant contribution to evaluating the broad capabilities of LLMs, and this work heavily influenced the development of similar benchmarks."}, {"fullname_first_author": "Jason Wei", "paper_title": "Measuring short-form factuality in large language models", "publication_date": "2024-01-01", "reason": "This is a very recent and relevant paper, as it introduces the SimpleQA benchmark that directly inspires the creation of Chinese SimpleQA, and it serves as a direct comparison dataset."}, {"fullname_first_author": "Yuzhen Huang", "paper_title": "C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models", "publication_date": "2023-01-01", "reason": "This paper is crucial as it introduces C-Eval, a well-regarded Chinese LLM benchmark that the authors of this paper look to for inspiration and comparison."}, {"fullname_first_author": "Haonan Li", "paper_title": "CMMLU: Measuring massive multitask language understanding in Chinese", "publication_date": "2023-01-01", "reason": "This is another key reference as CMMLU is a significant existing benchmark, similar to C-Eval, used for comparison and inspiration for Chinese SimpleQA."}]}