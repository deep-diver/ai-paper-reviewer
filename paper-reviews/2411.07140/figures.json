[{"figure_path": "https://arxiv.org/html/2411.07140/x1.png", "caption": "Figure 1: Overview of Chinese SimpleQA. \u201cChinese Cul.\u201d and \u201cETAS\u201d represent \u201cChinese Culture\u201d and \u201cEngineering, Technology, and Applied Sciences\u201d, respectively.", "description": "This figure is a sunburst chart visualizing the distribution of questions across different categories in the Chinese SimpleQA benchmark. The outermost ring displays the six main topics: Chinese Culture, Humanities, Engineering, Technology, and Applied Sciences (ETAS), Life, Art, and Culture, Society, and Natural Science. Each main topic is further broken down into multiple subtopics in the subsequent inner rings, showing the hierarchical structure of the dataset. The size of each segment is proportional to the number of questions within that category, providing a visual representation of the dataset's composition across various subject areas.", "section": "2 CHINESE SIMPLEQA"}, {"figure_path": "https://arxiv.org/html/2411.07140/x2.png", "caption": "Figure 2: An overview of the data construction process of Chinese SimpleQA.", "description": "This figure details the creation of the Chinese SimpleQA dataset. It begins with extracting and filtering relevant content from sources like Wikipedia.  Next, question-answer pairs are automatically generated using an LLM and then undergo quality control steps. These include verifying the pairs against predefined criteria, using a retrieval augmented generation (RAG) approach with search engine data to validate answers, and finally, human review and filtering for difficulty.  The entire process aims to create high-quality, objective, and time-invariant question-answer pairs that effectively test the factuality of LLMs.", "section": "2 CHINESE SIMPLEQA"}, {"figure_path": "https://arxiv.org/html/2411.07140/x3.png", "caption": "Figure 3: Results (CO and CGA metrics) of different models for six topics.", "description": "This figure presents a comparison of the performance of various large language models (LLMs) across six primary topics, as measured by two metrics: Correct (CO) and Correct Given Attempted (CGA).  The six topics represent broad subject categories from the Chinese SimpleQA benchmark dataset, allowing for an assessment of the models' factual accuracy and knowledge breadth across different domains. Each model's performance is visually displayed for each topic, allowing for a direct comparison between the models and across topic areas.", "section": "3.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2411.07140/extracted/5992628/figures/calibration_and_inference.png", "caption": "Figure 4: Left: Calibration of LLMs based on their stated confidence. Right: Improvement in accuracy with increased test-time compute using Best-of-N.", "description": "This figure shows two plots. The left plot displays the calibration of various Large Language Models (LLMs) based on their stated confidence levels.  It assesses how well the models' confidence scores match their actual accuracy in answering questions from the Chinese SimpleQA dataset.  A perfectly calibrated model would have confidence scores that precisely reflect its accuracy rate. The right plot illustrates how the accuracy of LLMs improves as the number of inferences (test-time compute) increases using a Best-of-N strategy. Best-of-N involves running the model multiple times for each question and selecting the answer with the highest confidence. The improvement in accuracy demonstrates the effectiveness of this strategy.", "section": "3.3 Further Analysis"}, {"figure_path": "https://arxiv.org/html/2411.07140/x4.png", "caption": "Figure 5: The effect of RAG strategy.", "description": "The figure illustrates the impact of employing a Retrieval-Augmented Generation (RAG) strategy on the performance of various large language models (LLMs) when evaluated using the Chinese SimpleQA benchmark.  The chart compares the F1-scores achieved by different LLMs with and without RAG. It demonstrates that incorporating RAG substantially enhances the accuracy of most LLMs, especially smaller models, and significantly reduces performance gaps between different LLMs.", "section": "3.3 FURTHER ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2411.07140/x5.png", "caption": "Figure 6: The effect of alignment in post-training.", "description": "This figure shows the impact of alignment techniques (post-training) on the factuality of various LLMs.  It compares the performance of pre-trained models versus their aligned counterparts across several models (Qwen2.5 series, DeepSeek, GPT-40).  The bars represent the F1 score, a metric combining precision and recall, and visually demonstrate whether alignment improved or hurt the model's factuality.  The results indicate that alignment does not always improve factuality, highlighting what is known as the \"alignment tax.\"", "section": "3.3 FURTHER ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2411.07140/x6.png", "caption": "Figure 7: Detailed results on some selected subtopics.", "description": "This figure presents a detailed breakdown of the performance of various LLMs across six selected subtopics within the Chinese SimpleQA benchmark.  The subtopics shown are Education, Entertainment, Mathematics, Medicine, Law, and Computer Science.  Each model's performance is visualized using a radar chart, comparing their correct answer rates (CO) across these diverse subject areas. This granular level of analysis allows for a deeper understanding of each model's strengths and weaknesses in specific knowledge domains, moving beyond the overall scores.", "section": "3.3 Further Analysis"}, {"figure_path": "https://arxiv.org/html/2411.07140/x7.png", "caption": "Figure 8: The rankings of different LLMs on SimpleQA and Chinese SimpleQA.", "description": "This figure compares the performance rankings of various large language models (LLMs) on two different question-answering benchmarks: SimpleQA (English) and Chinese SimpleQA (Chinese).  It highlights the differences in model rankings between the two benchmarks, showing that the relative strengths of different models can vary significantly depending on the language and dataset used for evaluation.  This underscores the importance of evaluating LLMs across diverse datasets to get a more comprehensive understanding of their capabilities and limitations.", "section": "3.3.6 Comparison Between Chinese SimpleQA and SimpleQA"}]