[{"content": "|           | Vision Encoder           | LLM                | Params. | Training cost / example |                        |                        |\n| :--------- | :----------------------- | :------------------ | :------ | :----------------------- | :--------------------- | :--------------------- |\n|           |                         |                    |         | 224px<sup>2</sup>         | 448px<sup>2</sup>        | 896px<sup>2</sup>        |\n| PaliGemma 2 3B |           | Gemma 2 2B          | 3.0B    | 1.0                     | 4.6                    | ~123.5                  |\n| PaliGemma 2 10B | SigLIP-So400m           | Gemma 2 9B          | 9.7B    | 3.7                     | 18.3                   | ~167.7                  |\n| PaliGemma 2 28B |           | Gemma 2 27B         | 27.7B   | 18.9                    | 63.5                   | ~155.6                  |", "caption": "Table 1: The vision encoder parameter count is small compared to the LLM, but the compute is dominated by the vision tokens in the LLM. The last three columns show the relative training cost per example (as measured in our pre-training setup). Models are trained on Cloud TPUv5e\u00a0[24], except the 28B model at 896px2 is trained on TPUv5p, for which we assume a speed-up of 2.3\u00d72.3\\times2.3 \u00d7 per chip.", "description": "This table compares different versions of the PaliGemma 2 model, highlighting the impact of model size and resolution on training costs.  The vision encoder uses a consistent size across models (SigLIP-So400m), but the language model (LLM) varies in size (2B, 9B, 27B). Training is done at three image resolutions (224px\u00b2, 448px\u00b2, 896px\u00b2). The table shows that although the vision encoder's parameter count is small compared to the LLM, the compute time is dominated by processing the visual information.  The final three columns present the relative training cost per example for each model variant; these costs are measured using the described pre-training setup and the specified TPU hardware.  Note that the largest model (28B at 896px\u00b2) used different hardware (TPUv5p) and assumes a speed improvement of 2.3x compared to other models using TPUv5e.", "section": "3. Model"}, {"content": "|               | ICDAR'15 Incidental          | Total-Text                |\n| :------------- | :-------------------------: | :-------------------------: |\n|               | P      | R      | F1     | P      | R      | F1     |\n| HTS           | 81.9   | 68.4   | 74.5   | 75.7   | 69.4   | 72.4   |\n| PaliGemma 2 3B | 81.9   | 70.7   | 75.9   | 73.8   | 74.5   | 74.2   |", "caption": "Table 2: Text detection and recognition performance: The 896px2 PaliGemma\u00a02 model outperforms the state-of-the-art model HTS\u00a0[58] on ICDAR\u201915 Incidental and Total-Text, under the evaluation protocol of HierText\u00a0[57].", "description": "This table presents a comparison of the performance of the PaliGemma 2 model (specifically the 3B version at 896px resolution) against the state-of-the-art model, HTS, on two widely used datasets for text detection and recognition: ICDAR'15 Incidental and Total-Text.  The evaluation is conducted using the HierText protocol, ensuring a consistent and rigorous comparison. The table shows precision (P), recall (R), and F1-score for both datasets, highlighting the superior performance of PaliGemma 2.", "section": "4.2. Text detection and recognition"}, {"content": "|                     | FinTabNet                                      | PubTabNet                                       |\n| :------------------ | :------------------------------------------------: | :------------------------------------------------: |\n|                     | S-TEDS | TEDS  | GriTS-Top | GriTS-Con | S-TEDS | TEDS  | GriTS-Top | GriTS-Con |\n| SOTA                | 98.9   | 98.2  | 99.0     | 98.6     | 97.9   | 96.9  | -        | -        |\n| PaliGemma 2 3B     | 99.2   | 98.9  | 99.4     | 99.2     | 97.6   | 97.3  | 98.0     | 97.8    |", "caption": "Table 3: PaliGemma\u00a02 results for table structure recognition on FinTabNet\u00a0[111] and PubTabNet\u00a0[112], compared to the state of the art. The reference metrics are from [28, 86, 60, 38].", "description": "Table 3 presents a comparison of PaliGemma 2's performance on table structure recognition tasks against the state-of-the-art.  It evaluates PaliGemma 2's performance on two benchmark datasets: FinTabNet and PubTabNet.  The table shows the model's scores on key metrics (S-TEDS, TEDS, GriTS-Top, GriTS-Con) for both datasets.  These metrics measure the accuracy of the model in identifying the text content, bounding boxes, and overall structure of tables. The reference values are taken from previously published works, enabling a direct comparison with the best-performing models before PaliGemma 2.", "section": "4.3. Table structure recognition"}, {"content": "| | Full Match \u2191 |\n|---|---| \n| MolScribe [76] | 93.8 |\n| PaliGemma 2 10B <span style=\"font-size:70%;color:#808080;\">448px<sup>2</sup></span> | 94.8 |", "caption": "Table 4: PaliGemma\u00a02 performance for molecule structure recognition on ChemDraw data\u00a0[76].", "description": "This table presents the performance of PaliGemma 2 models of different sizes and resolutions on the molecule structure recognition task using the ChemDraw dataset [76].  The results are shown in terms of the 'Full Match' metric, indicating the percentage of correctly predicted molecular structures.  It demonstrates the impact of model size and resolution on the accuracy of molecule structure prediction.", "section": "4. Experiments"}, {"content": "| CER\u2193 | SER\u2193 | LER\u2193 |\n|---|---|---|\n| Sheet Music Tr. [80] | 3.9 | 5.1 | 13.1 |\n| PaliGemma 2 3B <sup>896px2</sup> | 1.6 | 2.3 | 6.7 |", "caption": "Table 5: PaliGemma\u00a02 performance for music score recognition on the GrandStaff data set\u00a0[80]. Character Error Rate (CER), Symbol Error Rate (SER), and Line Error Rate (LER) in [%].", "description": "Table 5 presents the performance of PaliGemma 2, a vision-language model, on the GrandStaff dataset [80] for optical music score recognition.  It details the model's accuracy in terms of three key metrics: Character Error Rate (CER), Symbol Error Rate (SER), and Line Error Rate (LER). These metrics quantify the model's errors at the character, symbol (a combination of characters), and line levels, respectively.  Lower values indicate better performance.", "section": "4.5. Optical music score recognition"}, {"content": "| Model | #par. | #char. | #sent. | NES\u2193 |\n|---|---|---|---|---|\n| MiniGPT-4 | 7B | 1484 | 5.6 | 52.3 |\n| mPLUG-Owl2 | 8B | 1459 | 4.4 | 48.4 |\n| InstructBLIP | 7B | 1510 | 4.0 | 42.6 |\n| LLaVA-1.5 | 7B | 1395 | 4.2 | 40.6 |\n| VILA | 7B | 1871 | 8.6 | 28.6 |\n| PaliGemma | 3B | 1535 | 8.9 | 34.3 |\n| PaLI-5B | 5B | 1065 | 11.3 | 32.9 |\n| PaliGemma 2<sup>448px<sup>2</sup></sup> | 3B | 1529 | 7.7 | 28.4 |\n| PaliGemma 2<sup>448px<sup>2</sup></sup> | 10B | 1521 | 7.5 | 20.3 |", "caption": "Table 6: \nPaliGemma\u00a02 results for long captioning on the DOCCI data\u00a0[69]. Pali* models are models fine-tuned on DOCCI at 448px2; the other baselines are instruction-tuned on a broad range of tasks.\nAverage prediction length in characters and sentences, and percentage of Non-Entailment Sentences (NES), measuring factual inaccuracies.", "description": "Table 6 presents the performance of PaliGemma 2 models on the DOCCI long captioning dataset.  It compares models fine-tuned on DOCCI at 448px\u00b2 resolution (Pali*) against baselines that underwent instruction tuning across a wider array of tasks. The table details average caption lengths (characters and sentences), and the percentage of captions that exhibit factual inaccuracies (Non-Entailment Sentences, NES).  The NES metric quantifies how often generated captions are not factually consistent with the image content.", "section": "4.6. Generating long, fine-grained captions"}, {"content": "|           | zs. split | rand. split |\n|-----------|------------|-------------|\n| Human [53] | 95.4       |             |\n| InstructBLIP (zs.) [18] | 65.6       | -           |\n| LXMERT [89] | 70.1       | 61.2        |\n| PaliGemma 2 13B <sup>2</sup> | 74.8       | 81.6        |\n| PaliGemma 2 10B <sup>2</sup> | 79.8       | 86.8        |", "caption": "Table 7: PaliGemma\u00a02 accuracy on VSR\u00a0[53] on the zeroshot and random test splits. We show a fine-tuned (LXMERT) and zero-shot (InstructBLIP) baseline from the literature.", "description": "Table 7 presents a comparison of PaliGemma 2's performance on the Visual Spatial Reasoning (VSR) benchmark [53] against two baselines from the existing literature: LXMERT (fine-tuned) and InstructBLIP (zero-shot).  The table displays accuracy results for both zero-shot and random test splits of the VSR benchmark, offering a clear view of PaliGemma 2's capabilities in spatial reasoning compared to established methods.", "section": "4.7. Spatial reasoning"}, {"content": "|       | C\u2191 | B\u2191 | R\u2191 | F1\u2191 |\n|---|---|---|---|---|\n| Flamingo-CXR [90] | 13.8 | 10.1 | 29.7 | 20.5 |\n| Med-Gemini-2D [102] | 17.5 | 20.5 | 28.3 | 24.4 |\n| PaliGemma 2 13B <span style=\"font-size:70%;color:#808080;\">896px<sup>2</sup></span> | 19.9 | 14.6 | 31.9 | 28.8 |\n| PaliGemma 2 10B <span style=\"font-size:70%;color:#808080;\">896px<sup>2</sup></span> | 17.4 | 15.0 | 32.4 | 29.5 |", "caption": "Table 8: PaliGemma\u00a02 performance for radiography report generation on the on the MIMIC-CXR data\u00a0[33, 23]. We report CIDEr (C), BlEU4 (B), Rouge-L (R), and RadGraph F1-scores [%]\u00a0[30] (a clinical metric).", "description": "This table presents the performance of the PaliGemma 2 model on the MIMIC-CXR dataset for radiography report generation.  The MIMIC-CXR dataset contains chest X-ray images and associated radiology reports.  The table shows the model's performance using four evaluation metrics: CIDEr, BLEU4, Rouge-L, and RadGraph F1-score. The RadGraph F1-score is a clinical metric specifically designed for evaluating the quality of generated radiology reports. The results are broken down by model size and resolution, allowing for comparison across different configurations.", "section": "4.8. Radiography report generation"}, {"content": "| Processor | Threads | ViT Walltime [s] | Prefill Walltime [s] | Extend Walltime [s] | Prefill Tokens/sec | Extend Tokens/sec |\n|---|---|---|---|---|---|---|\n| Apple M1 Max | 4+1 | 1.6 | 8.2 | 0.9 | 32 | 12 |\n| Apple M3 Pro | 7+1 | 0.8 | 4.4 | 0.5 | 59 | 22 |\n| AMD Milan | 8+1 | 0.82 | 4.9 | 0.64 | 53 | 17 |\n| AMD Milan | 32+1 | 0.39 | 1.8 | 0.34 | 144 | 32 |\n| AMD Genoa | 8+1 | 0.36 | 1.8 | 0.29 | 147 | 37 |\n| AMD Genoa | 32+1 | 0.17 | 0.8 | 0.27 | 323 | 41 |", "caption": "Table 9: CPU-only inference speed measurements with gemma.cpp-based implementation on different architectures. Inference of finetuned PaliGemma\u00a02\u00a03B (224px2) with greedy decoding. Prefill is done with 260 tokens and followed by 11 calls to extend during decoding.", "description": "This table presents the results of measuring the inference speed of the PaliGemma 2 3B (224px2) model using the gemma.cpp framework on various CPU architectures.  The model was fine-tuned and used with greedy decoding. Each inference started with a prefill sequence of 260 tokens, followed by 11 extension calls to complete the decoding process. The table details the processor used, the number of threads, the time taken for vision transformer (ViT), prefill, and extension phases, as well as the tokens processed per second during the prefill and extension stages.", "section": "4.9. CPU inference and quantization"}, {"content": "|                | COCOcap | TextCaps | AI2D | OKVQA | DocVQA(val) |\n|----------------|---------|----------|------|-------|------------|\n| Jax, F32, 12.1GB | 140.0   | 126.3    | 75.4 | 64.0  | 39.8       |\n| gemma.cpp, quantized, 4.0GB | 139.8   | 126.6    | 75.6 | 64.1  | 39.8       |\n| relative metric values [%] | 99.9    | 100.2    | 100.1 | 100.1 | 99.9       |", "caption": "Table 10: Quality comparison between Jax/f32 inference on TPU and quantized gemma.cpp-based inference on CPU. Inference of one fine-tuned PaliGemma\u00a02\u00a03B (224px2) run. Noticeable differences to Table\u00a013 for the Jax version are the result of using greedy decoding for COCOcap and TextCaps. Relative numbers based on metric values before rounding to one decimal.", "description": "This table compares the performance of two different inference methods for the PaliGemma 2 3B (224px2) model: Jax/f32 inference on a TPU and quantized gemma.cpp-based inference on a CPU.  The comparison is made using various metrics after fine-tuning on several tasks.  A key difference between the two inference methods is that the Jax results use greedy decoding for the COCOcap and TextCaps tasks, while the gemma.cpp results do not.  The relative performance values shown are calculated based on the unrounded metric values to highlight small differences between the two methods.", "section": "4.9. CPU inference and quantization"}, {"content": "| May 31, 2015 | Securities in an unrealized loss position for less than twelve months | Securities in an unrealized loss position for more than twelve months | Total |\n|---|---|---|---|---|\n| In millions | Gross unrealized losses | Fair value | Gross unrealized losses | Fair Value | Gross unrealized losses | Fair Value |\n| Type of issue: |  |  |  |  |  |  |\n| General obligation municipal bonds | $(3.8) | $355.1 | $(0.5) | $26.3 | $(4.3) | $381.4 |\n| Revenue municipal bonds | $(3.2) | 361.6 | \u2013 | \u2013 | $(3.2) | 361.6 |\n| Total | $(7.0) | $896.7 | $(0.5) | $26.3 | $(7.5) | $923.0 |", "caption": "Table 11: Mean average precision (mAP) after transfer to detection tasks. PG1 and PG2 refer to PaliGemma\u00a0[9] and PaliGemma\u00a02, respectively.", "description": "This table presents the mean Average Precision (mAP) results achieved after fine-tuning PaliGemma and PaliGemma 2 models on object detection tasks within the MS COCO and DocLayNet datasets.  Different model sizes (3B, 10B, 28B) and image resolutions (224px, 448px, 896px) are evaluated.  PG1 refers to the original PaliGemma model, while PG2 denotes the upgraded PaliGemma 2 model. The table allows for a comparison of performance between the two model versions under various configurations.", "section": "4.1 Investigating model size and resolution"}, {"content": "| Model | Caption |\n|---|---| \n| InstructBLIP | The image features a brown horse standing in a dirt area, with a saddle on its back. The horse appears to be well-trained, as it is standing still and paying attention to its surroundings.  There are several people visible in the scene, including one person standing near the horse\u2019s left side, another person standing near the horse\u2019s right side, and a third person standing further away from the horse. In addition to the people, there are two cars parked in the background, one on the left side and another on the right side of the image. |\n| mPLUG-Owl2 | In this image we can see a horse on the ground.  In the background there is a shed and we can see trees. |\n| MiniGPT-4 | This image is of a horse standing in the dirt near a building.  The horse has a saddle on its back and is wearing a number 5 on its forehead. There is a barn or stable visible in the background, and some trees and shrubs in the foreground. |\n| LLaVA-1.5 | The image features a brown horse standing in a dirt field. The horse is wearing a saddle and bridle, indicating that it is ready for riding.  The saddle is adorned with stars, adding a unique touch to the horse\u2019s appearance.  The horse appears to be calmly standing in the field, possibly waiting for its rider or simply enjoying the outdoors. |\n| VILA | In the image, a majestic brown horse stands in a fenced area, its coat gleaming under the sunlight. The horse is adorned with a black saddle and bridle, ready for a ride. The number 55 is prominently displayed on its side, perhaps indicating its racing number or a unique identifier.  The horse\u2019s tail swishes gently, adding a sense of movement to the scene. The horse is positioned in front of a stone wall, which provides a rustic backdrop to the image. The wall\u2019s texture contrasts with the smoothness of the horse\u2019s coat, creating a visually interesting composition. The ground beneath the horse is covered in dirt, suggesting that this might be a training or riding area. The fence in the background further reinforces this idea, providing a safe and controlled environment for the horse. Overall, this image captures a moment of calm before what could be an exciting ride, with the horse poised and ready for action. |\n| PaLI-5B (FT on DOCCI) | A side view of a brown horse that is standing on a dirt ground.  The horse is facing to the right and has a black saddle on it. The horse has a white star on its face. The horse has a bridle around its neck. The horse is standing on a dirt ground that is covered in white rocks. There is a stone wall behind the horse. There is a metal shed behind the horse. There are trees behind the shed. |\n| PaliGemma 3B 448px<sup>2</sup> | A side view of a brown horse that is standing on a dirt ground. The horse is facing to the left and has a black saddle on its back. The saddle is black and has a brown leather strap on it.  The horse has a black rope around its neck. The horse has a white star hanging from its bridle. The horse has a white number \"55\" written on its back.  The horse has white hooves. The horse has a black rope wrapped around its legs. The horse has a brown rope wrapped around its head. The horse has a black rope wrapped around its tail. The horse is standing on a dirt ground that has small white rocks on it. Behind the horse is a stone wall that is made up of different sized rocks.  Behind the stone wall is a metal gate that is being held up by metal beams. Behind the gate is a tree line that is made up of green trees. |\n| PaliGemma 2 3B 448px<sup>2</sup> | A brown horse is standing in a dirt area with rocks scattered around.  The horse has a black saddle on its back and a white star painted on its chest. The number \"55\" is painted on the side of the horse in white. A stone building is behind the horse. A metal structure is in the background of the image with a wooden roof over it. Trees are in the background of the image as well. |\n| PaliGemma 2 10B 448px<sup>2</sup> | A brown horse is standing in a dirt area with small rocks. The horse has a black saddle on its back and a white star painted on its side. The horse has \"55\" written on its back in white.  There is a pile of horse manure in front of the horse. There is a stone wall behind the horse. There is a wooden structure with a metal roof behind the stone wall. There are trees in the background.|", "caption": "Table 12: Safety statistics for captions generated by PaliGemma\u00a02 VLMs on FairFace\u00a0[37] using the Perspective API\u00a0[46]. Numbers indicate the fraction of instances with thresholds \u22650.8absent0.8\\geq 0.8\u2265 0.8 in [%], i.e.\u00a0a value of e.g.\u00a00.09 means 0.09%.", "description": "This table presents safety analysis results for captions generated by three different sizes of the PaliGemma 2 Vision-Language Model (VLM) on the FairFace dataset.  The analysis was done using the Perspective API, which assesses the toxicity and other harmful attributes of text.  Each cell shows the percentage of captions that exceeded a toxicity threshold of 0.8 (for metrics such as toxicity, threat, insult, profanity, and identity attack). Results are broken down by maximum and median values for each model size, and also categorized by perceived gender, ethnicity, and age group of the individuals depicted in the images.", "section": "D. Ethics and Safety"}, {"content": "| Indication | Radiologist report | PaliGemma 2 3B 896px prediction | \n|---|---|---| \n| INDICATION: Woman with cardiomyopathy and cdiff with acute desaturation and dyspnea // PE, pulmonary edema, vs aspiration PE, pulmonary edema, vs aspiration. | IMPRESSION: Enlargement of the cardiac silhouette with pulmonary edema. Bilateral pleural effusions, more prominent on the left. | FINDINGS: There is substantial enlargement of the cardiac silhouette with pulmonary edema. Retrocardiac opacification is consistent with volume loss in the left lower lobe and pleural effusion. In the appropriate clinical setting, superimposed pneumonia would have to be considered. | ", "caption": "Table 13: Mean and std-deviation over 5 finetuning runs of PaliGemma 3B, 10B, 28B models at 224px2 and 448px2 resolutions on over 30+ academic tasks from [9]. Tasks splits, preprocessing, metrics and hyper-parameters following the 224px2 versions according to previous work. Only the learning rate has been selected per model size based on validation splits.", "description": "Table 13 presents the mean and standard deviation of performance metrics across three different sizes (3B, 10B, and 28B parameters) of the PaliGemma 2 model and two image resolutions (224x2 and 448x2 pixels) when fine-tuned on 30+ tasks. The results are from five separate runs for each combination of model size, resolution, and task.  The tasks, data splits, preprocessing methods, and hyperparameters remained consistent with the 224x2 resolution setup used in previous work (from PaliGemma, as cited in [9]), except for the learning rate which was chosen individually for each model size based on its validation set performance. This table thus highlights the impact of model size and resolution on transfer learning performance across various tasks.", "section": "4.1 Investigating model size and resolution"}, {"content": "|       | 224px<sup>2</sup>          |             |             | 448px<sup>2</sup>          |             |             | 896px<sup>2</sup>          |             |             |\n| :---- | :-----------------------: | :----------: | :----------: | :-----------------------: | :----------: | :----------: | :-----------------------: | :----------: | :----------: |\n|       | PG1 3B         | PG2 3B        | PG2 10B       | PG1 3B         | PG2 3B        | PG2 10B       | PG1 3B         | PG2 3B        | PG2 10B       |\n| COCO  | 28.7                     | 30.4          | 30.3          | 37.0                     | 38.5          | 39.2          | 41.1                     | 42.3          | 43.6          |\n| DocLayNet | 50.8                     | 46.7          | 50.4          | 64.1                     | 62.5          | 63.5          | 66.5                     | 66.1          | 66.0          |", "caption": "Table 14: Sweep of learning rates on the various tasks and model sizes at 224px2 resolution. Although we report numbers in all metrics, learning rate selection was done based on the validation split and not on the zero-shot numbers.", "description": "This table presents a comprehensive analysis of the impact of different learning rates on the performance of various downstream tasks.  It explores three different model sizes (3B, 10B, and 28B parameters) at a resolution of 224x224 pixels. The results are broken down for each model size and learning rate, showing the performance across multiple metrics.  Note that while performance metrics are reported for all learning rates, the actual selection of the optimal learning rate for each task was determined using the validation split, not the zero-shot numbers.", "section": "4.1 Investigating model size and resolution"}, {"content": "| Metric | Perceived Gender |  |  | Ethnicity |  |  | Age Group |  |  |\n|---|---|---|---|---|---|---|---|---|---| \n|  | 3B | 10B | 28B | 3B | 10B | 28B | 3B | 10B | 28B |\n| Maximum |  |  |  |  |  |  |  |  |  |\n| Toxicity | 0.14 | 0.15 | 0.19 | 0.29 | 0.39 | 0.39 | 0.26 | 0.18 | 0.32 |\n| Identity Attack | 0.04 | 0.02 | 0.02 | 0.13 | 0.06 | 0.06 | 0.06 | 0.03 | 0.06 |\n| Insult | 0.17 | 0.25 | 0.17 | 0.37 | 0.52 | 0.52 | 0.27 | 0.39 | 0.24 |\n| Threat | 0.55 | 0.43 | 0.57 | 0.83 | 0.48 | 0.48 | 0.64 | 0.43 | 0.64 |\n| Profanity | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n|  | Median |  |  |  |  |  |  |  |  |\n| Toxicity | 0.13 | 0.10 | 0.18 | 0.07 | 0.07 | 0.14 | 0.12 | 0.08 | 0.12 |\n| Identity Attack | 0.02 | 0.01 | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n| Insult | 0.15 | 0.23 | 0.14 | 0.14 | 0.17 | 0.13 | 0.09 | 0.18 | 0.16 |\n| Threat | 0.35 | 0.27 | 0.41 | 0.28 | 0.19 | 0.42 | 0.27 | 0.31 | 0.40 |\n| Profanity | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |", "caption": "Table 14: Sweep of learning rates on the various tasks and model sizes at 224px2 resolution. Although we report numbers in all metrics, learning rate selection was done based on the validation split and not on the zero-shot numbers.", "description": "This table presents a comprehensive analysis of the impact of different learning rates on the performance of various downstream tasks using PaliGemma 2 models of varying sizes (3B, 10B, and 28B parameters).  The experiments were conducted at a resolution of 224x2 pixels. While the table reports performance metrics for each combination of learning rate, model size, and task, it's crucial to note that the optimal learning rate selection for each model and task was determined using the validation set, not the zero-shot results. This approach ensures that the reported performance values accurately reflect the model's ability to generalize to unseen data.", "section": "4.1 Investigating model size and resolution"}, {"content": " | 224px<sup>2</sup> | 448px<sup>2</sup>\n|---|---|---|\n|  | 3B | 10B | 28B |\n| AI2D [40] | 74.7 (\u00b1 0.5) | 83.1 (\u00b1 0.4) | 83.2 (\u00b1 0.7) |\n| AOKVQA-DA (val) [81] | 64.2 (\u00b1 0.5) | 68.9 (\u00b1 0.3) | 70.2 (\u00b1 0.2) |\n| AOKVQA-MC (val) [81] | 79.7 (\u00b1 1.0) | 83.7 (\u00b1 1.1) | 84.7 (\u00b1 0.8) |\n| ActivityNet-CAP [43] | 34.2 (\u00b1 0.3) | 35.9 (\u00b1 0.5) | 100-0(\u00b10.0) |\n| ActivityNet-QA [107] | 51.3 (\u00b1 0.2) | 53.2 (\u00b1 0.4) | 100-0(\u00b10.0) |\n| COCO-35L (avg34) [91] | 113.9 (\u00b1 0.2) | 115.8 (\u00b1 0.0) | 116.5 (\u00b1 0.1) |\n| COCO-35L (en) [91] | 138.4 (\u00b1 0.2) | 140.8 (\u00b1 0.3) | 142.4 (\u00b1 0.4) |\n| COCOcap [51] | 141.3 (\u00b1 0.5) | 143.7 (\u00b1 0.2) | 144.0 (\u00b1 0.3) |\n| ChartQA (aug) [63] | 74.4 (\u00b1 0.7) | 74.2 (\u00b1 0.8) | 68.9 (\u00b1 0.6) |\n| ChartQA (human) [63] | 42.0 (\u00b1 0.3) | 48.4 (\u00b1 1.1) | 46.8 (\u00b1 0.6) |\n| CountBenchQA [9] | 81.0 (\u00b1 1.0) | 84.0 (\u00b1 1.4) | 86.4 (\u00b1 1.6) |\n| DocVQA (val) [64] | 39.9 (\u00b1 0.3) | 43.9 (\u00b1 0.6) | 44.9 (\u00b1 0.4) |\n| GQA [29] | 66.2 (\u00b1 0.3) | 67.2 (\u00b1 0.2) | 67.3 (\u00b1 0.2) |\n| InfoVQA (val) [65] | 25.2 (\u00b1 0.2) | 33.6 (\u00b1 0.2) | 36.4 (\u00b1 0.1) |\n| MARVL (avg5) [52] | 83.5 (\u00b1 0.2) | 89.5 (\u00b1 0.2) | 90.6 (\u00b1 0.2) |\n| MSRVTT-CAP [101] | 68.5 (\u00b1 1.3) | 72.1 (\u00b1 0.5) | 100-0(\u00b10.0) |\n| MSRVTT-QA [100] | 50.5 (\u00b1 0.1) | 51.9 (\u00b1 0.1) | 100-0(\u00b10.0) |\n| MSVD-QA [12] | 61.1 (\u00b1 0.2) | 62.5 (\u00b1 0.2) | 100-0(\u00b10.0) |\n| NLVR2 [87] | 91.4 (\u00b1 0.1) | 93.9 (\u00b1 0.2) | 94.2 (\u00b1 0.1) |\n| NoCaps [2] | 123.1 (\u00b1 0.3) | 126.3 (\u00b1 0.4) | 127.1 (\u00b1 0.3) |\n| OCR-VQA [67] | 73.4 (\u00b1 0.0) | 74.7 (\u00b1 0.1) | 75.3 (\u00b1 0.2) |\n| OKVQA [62] | 64.2 (\u00b1 0.1) | 68.0 (\u00b1 0.1) | 71.2 (\u00b1 0.2) |\n| RSVQA-hr (test) [55] | 92.7 (\u00b1 0.1) | 92.6 (\u00b1 0.0) | 92.7 (\u00b1 0.0) |\n| RSVQA-hr (test2) [55] | 90.9 (\u00b1 0.1) | 90.8 (\u00b1 0.1) | 90.9 (\u00b1 0.1) |\n| RSVQA-lr [55] | 93.0 (\u00b1 0.4) | 92.8 (\u00b1 0.6) | 93.5 (\u00b1 0.2) |\n| RefCOCO (testA) [106] | 75.7 (\u00b1 0.2) | 77.2 (\u00b1 0.1) | 76.8 (\u00b1 0.1) |\n| RefCOCO (testB) [106] | 71.0 (\u00b1 0.3) | 74.2 (\u00b1 0.3) | 73.9 (\u00b1 0.1) |\n| RefCOCO (val) [106] | 73.4 (\u00b1 0.1) | 75.9 (\u00b1 0.1) | 75.0 (\u00b1 0.0) |\n| RefCOCO+ (testA) [39] | 72.7 (\u00b1 0.2) | 74.7 (\u00b1 0.2) | 73.6 (\u00b1 0.2) |\n| RefCOCO+ (testB) [39] | 64.2 (\u00b1 0.2) | 68.4 (\u00b1 0.3) | 67.1 (\u00b1 0.1) |\n| RefCOCO+ (val) [39] | 68.6 (\u00b1 0.1) | 72.0 (\u00b1 0.2) | 70.3 (\u00b1 0.2) |\n| RefCOCOg (test) [61] | 69.0 (\u00b1 0.2) | 71.9 (\u00b1 0.1) | 70.7 (\u00b1 0.1) |\n| RefCOCOg (val) [61] | 68.3 (\u00b1 0.3) | 71.4 (\u00b1 0.2) | 70.5 (\u00b1 0.1) |\n| ST-VQA (val) [10] | 61.9 (\u00b1 0.1) | 64.3 (\u00b1 0.4) | 65.1 (\u00b1 0.4) |\n| SciCap [27] | 165.1 (\u00b1 0.5) | 159.5 (\u00b1 0.7) | 156.9 (\u00b1 1.0) |\n| ScienceQA [59] | 96.1 (\u00b1 0.3) | 98.2 (\u00b1 0.2) | 98.2 (\u00b1 0.2) |\n| Screen2Words [95] | 113.3 (\u00b1 0.8) | 117.8 (\u00b1 0.7) | 122.8 (\u00b1 0.5) |\n| TallyQA (complex) [1] | 70.3 (\u00b1 0.3) | 73.4 (\u00b1 0.1) | 74.2 (\u00b1 0.1) |\n| TallyQA (simple) [1] | 81.8 (\u00b1 0.1) | 83.2 (\u00b1 0.1) | 83.4 (\u00b1 0.1) |\n| TextCaps [82] | 127.5 (\u00b1 0.3) | 137.9 (\u00b1 0.3) | 139.9 (\u00b1 0.4) |\n| TextVQA (val) [83] | 59.6 (\u00b1 0.3) | 64.0 (\u00b1 0.3) | 64.7 (\u00b1 0.2) |\n| VATEX [97] | 80.8 (\u00b1 0.4) | 82.7 (\u00b1 0.5) | 100-0(\u00b10.0) |\n| WidgetCap [49] | 138.1 (\u00b1 0.7) | 139.8 (\u00b1 1.0) | 138.8 (\u00b1 0.8) |\n| xGQA (avg7) [73] | 58.6 (\u00b1 0.2) | 61.4 (\u00b1 0.1) | 61.1 (\u00b1 0.1) |", "caption": "Table 15: Comparison of PaliGemma 3B and PaliGemma\u00a02 3B at 224px2 and 448px2 resolutions. PG1 and PG2 refer to PaliGemma\u00a0[9] and PaliGemma\u00a02, respectively.", "description": "This table presents a comparison of the performance of two versions of the PaliGemma model (3B variant): the original PaliGemma [9] and the updated PaliGemma 2.  The comparison is done across two different image resolutions (224px\u00b2 and 448px\u00b2) and considers a wide range of academic benchmark tasks to assess performance differences between the two models. PG1 denotes PaliGemma [9], and PG2 denotes PaliGemma 2.", "section": "4.1. Investigating model size and resolution"}]