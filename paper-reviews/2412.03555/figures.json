[{"figure_path": "https://arxiv.org/html/2412.03555/x1.png", "caption": "Figure 1: PaliGemma\u00a02 processes a 224px2/ 448px2/896px2 image with a SigLIP-400m encoder with patch size 14px2, yielding 256/1024/ 4096 tokens. After a linear projection, the image tokens are concatenated with the input text tokens and Gemma\u00a02 autoregressively completes this prefix with an answer.", "description": "PaliGemma 2 processes an image (224x224, 448x448, or 896x896 pixels) using a SigLIP-400m encoder.  The encoder divides the image into patches (14x14 pixels each), resulting in 256, 1024, or 4096 image tokens depending on the image resolution. These image tokens are then linearly projected into a format compatible with the Gemma 2 language model.  The image tokens are combined with any input text tokens, and the Gemma 2 model autoregressively generates a text response as output.", "section": "3. Model"}, {"figure_path": "https://arxiv.org/html/2412.03555/x3.png", "caption": "Figure 2: Referring segmentation example from our PaliGemma demoa.\nThe model is pretrained with a vocabulary that includes localization tokens (for detection) and segmentation tokens (to define a binary mask inside a bounding box).", "description": "This figure showcases an example of referring segmentation from the PaliGemma model's interactive demo.  The model's training incorporates a vocabulary of localization tokens (for identifying objects) and segmentation tokens (used to create a binary mask representing the precise area of the object within a bounding box). The example visually demonstrates the model's ability to not only locate a specified object but also delineate its exact boundaries within the image.", "section": "2. Related work"}, {"figure_path": "https://arxiv.org/html/2412.03555/x4.png", "caption": "Figure 3: Relative improvements of metrics after transfer, when choosing a pre-trained checkpoint with a larger LM, or with a higher resolution. The tasks are grouped into tasks sensitive to both model size and resolution (\u00a0), sensitive to model size (\u00a0), and sensitive to resolution (\u00a0).\nNote that some benchmarks are quite saturated (e.g. ScienceQA\u2019s relative improvement of 2.2% corresponds to an error reduction of 53.8% \u2013 see Figure\u00a013).\nData used to create this plot available in Table\u00a013.", "description": "This figure shows the relative improvement in performance on various downstream tasks when using either a larger language model (LM) or a higher image resolution during the pre-training phase.  The tasks are categorized into three groups based on their sensitivity to changes in LM size and resolution: those sensitive to both, those primarily sensitive to LM size, and those primarily sensitive to resolution.  It's important to note that some tasks show minimal improvement (due to being already near peak performance) even with significant increases in LM size or resolution, highlighting the complexity of transfer learning and the interaction between model capacity and data characteristics.  A specific example is provided (ScienceQA) to illustrate how small percentage improvements in performance can represent substantial reductions in error, emphasizing the need for nuanced interpretations of evaluation metrics in this context.  The underlying data for the plot is provided in Table 13.", "section": "4.1. Investigating model size and resolution"}, {"figure_path": "https://arxiv.org/html/2412.03555/x5.png", "caption": "Figure 4: Transfer performance as a function of model size and resolution (median over 5 transfer runs). The shaded area marks standard deviation to reported value. Lighter lines correspond to higher resolution (448px2). The tasks are grouped into tasks sensitive to both model size and resolution (\u00a0), sensitive to model size (\u00a0), and sensitive to resolution (\u00a0). Data for this plot is available in Table\u00a013.", "description": "This figure displays the impact of model size and resolution on the performance of various downstream tasks in the PaliGemma 2 model.  The x-axis represents the relative improvement when moving from a 3B parameter model to a 10B parameter model.  The y-axis shows the relative improvement achieved by using a higher resolution (448px2) compared to the lower resolution (224px2). Each point represents a different task, grouped according to its sensitivity to model size and resolution.  Points in green indicate tasks benefiting from both larger models and higher resolution, yellow points show tasks that benefit more from higher resolution, and blue points show tasks benefiting primarily from larger models. The shaded area around each point depicts the standard deviation of the median performance across five runs. The complete dataset used to generate this figure is provided in Table 13 of the paper.", "section": "4.1. Investigating model size and resolution"}, {"figure_path": "https://arxiv.org/html/2412.03555/extracted/6045874/figures/totaltext.jpg", "caption": "Figure 5: Per-task performance as a function of model size and learning rate for several of the downstream tasks. Values are normalized for each task and model size, with darker color indicating better task performance. Larger models tend to have a lower optimal transfer learning rate. Zero-shot tasks not shown as their values were not used to select learning rates. The data used for this plot is provided in Table\u00a0LABEL:tab:app:pg_lr_sweep.", "description": "This figure displays the performance of various downstream tasks using different model sizes (3B, 10B, and 28B) and a range of learning rates. The performance is normalized for each task and model size. Darker colors represent better performance. The results reveal a trend: larger models tend to have lower optimal learning rates for transfer learning. Zero-shot tasks are excluded because their results were not used to select the learning rates.", "section": "4.1.3. Using Gemma 2 instead of Gemma 1"}, {"figure_path": "https://arxiv.org/html/2412.03555/extracted/6045874/figures/molecule.png", "caption": "Figure 6: Test set example from Total-Text\u00a0[17] with PaliGemma\u00a02 3B 896px2 predictions.", "description": "This figure shows a sample image from the Total-Text dataset [17], a benchmark dataset for scene text detection and recognition.  The image displays a storefront sign with text.  The caption highlights that the image was processed using the PaliGemma 2 model, specifically the 3B parameter version at 896x896 pixel resolution. The model's predictions for the text in the image are overlaid on the image itself, demonstrating the model's ability to recognize and transcribe the text in the image.", "section": "A. Tasks"}, {"figure_path": "https://arxiv.org/html/2412.03555/extracted/6045874/figures/kern_example.png", "caption": "Figure 7: Original image from FinTabNet\u00a0[111] with predicted cell content boxes (green), and resulting PaliGemma\u00a02 model prediction.", "description": "Figure 7 displays an image from the FinTabNet dataset [111] that contains a table. The image is pre-processed and fed into the PaliGemma 2 model for table structure recognition. The model successfully identifies the table's structure and extracts the content of each cell with high accuracy, as demonstrated by the green boxes correctly outlining the cell boundaries. The figure also shows the model's prediction for the table's content, which matches the actual content almost perfectly.  This showcases the model's ability to handle complex table structures and extract accurate content from visual input.", "section": "4.3. Table structure recognition"}, {"figure_path": "https://arxiv.org/html/2412.03555/x6.png", "caption": "Figure 8: Example of a rendered molecule with the corresponding SMILES string CC1([C@@H]([C@@H](C2=C(O1)C=CC(=C2)C(C(F)(F)F)(F)F)N3CCCCC3=O)O)C.", "description": "Figure 8 displays a 2D representation of a molecule, specifically its chemical structure. The image shows the molecule's atoms (carbon, oxygen, fluorine, etc.) connected by bonds.  The SMILES string (Simplified Molecular Input Line Entry System) provides a textual representation of this molecular structure which is a standardized and concise way to encode molecules using text.  This string CC1([C@@H]([C@@H](C2=C(O1)C=CC(=C2)C(C(F)(F)F)(F)F)N3CCCCC3=O)O)C allows computers to interpret and process the molecular structure. The figure illustrates the close relationship between the visual and textual representation of the molecule.", "section": "4.4. Molecular structure recognition"}, {"figure_path": "https://arxiv.org/html/2412.03555/x7.png", "caption": "Figure 9: Example of a pianoform sheet with its **kern transcription (source https://www.humdrum.org/guide/ch02/).", "description": "Figure 9 displays an example of a single-staff musical score written in pianoform notation.  The image shows a portion of a musical piece, likely a melody or simple harmonic progression. Below the musical score is its corresponding transcription in the **kern format. This is a symbolic representation used to store musical information digitally; it is not easily readable by humans without specialized software.  The link provided points to additional documentation about the **kern format, offering further insight into its structure and encoding methodology.", "section": "4.5. Optical music score recognition"}]