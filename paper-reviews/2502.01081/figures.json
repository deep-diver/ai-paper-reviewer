[{"figure_path": "https://arxiv.org/html/2502.01081/extracted/6172587/images/curve.png", "caption": "Figure 1: The performance of GPT-[n] and o-[n] series models on PuzzleVQA and AlgoPuzzleVQA, illustrating how multimodal reasoning evolves over time with model releases and inference cost. The size of each circle roughly represents the inference cost per puzzle.", "description": "This figure displays the performance of various GPT models (GPT-[n] and o-[n]) on two multimodal reasoning tasks: PuzzleVQA and AlgoPuzzleVQA.  The x-axis represents the release date of each model, and the y-axis represents the accuracy achieved on each task.  The size of each data point (circle) is proportional to the inference cost (in terms of computational resources) required for the model to solve a single puzzle.  The figure visually demonstrates the progression of multimodal reasoning capabilities over time, showing how accuracy improves with newer model releases.  It also highlights the trade-off between performance gains and increased computational cost.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2502.01081/x1.png", "caption": "Figure 2: ARC-AGI semi-private scores of the OpenAI models over time.", "description": "This figure displays the progression of OpenAI's large language models' scores on the ARC-AGI (Abstraction and Reasoning Corpus for Artificial General Intelligence) benchmark over time. It shows how different models, such as GPT-2, GPT-3, GPT-4, and the newer o-series models, perform on this benchmark and how their performance has evolved.  The scores represent the models' ability to solve abstract reasoning problems, illustrating advancements in AI capabilities. Notably, it highlights the substantial improvement achieved by the o-series models compared to previous GPT models.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2502.01081/extracted/6172587/images/intro_case.png", "caption": "Figure 3: Case study on an abstract puzzle from the Colors & Shapes (left) category and Colors & Numbers (right) category in PuzzleVQA.", "description": "This figure showcases two examples of abstract puzzles from the PUZZLEVQA dataset. The left panel displays a puzzle from the 'Colors & Shapes' category, requiring visual reasoning about colors and shapes to identify the missing element. The right panel presents a puzzle from the 'Colors & Numbers' category, demanding a combination of visual and numerical reasoning to solve it.  The figure highlights the differences in complexity and the type of reasoning involved in solving multimodal puzzles, which involves visual and textual information processing.", "section": "2 PUZZLEVQA & ALGOPUZZLEVQA"}, {"figure_path": "https://arxiv.org/html/2502.01081/x2.png", "caption": "Figure 4: Example single-concept and dual-concept abstract puzzles in PuzzleVQA, designed around fundamental concepts such as numbers, colors, size, and shapes.", "description": "Figure 4 showcases examples of puzzles from the PUZZLEVQA dataset.  These puzzles test visual reasoning abilities by presenting abstract patterns involving numbers, colors, sizes, and shapes. The figure displays both single-concept puzzles (e.g., identifying a missing shape) and dual-concept puzzles (e.g., identifying a missing number based on both color and shape patterns).  These puzzles illustrate the variety and complexity of problems within the dataset used to evaluate multimodal reasoning capabilities of large language models.", "section": "2 PUZZLEVQA & ALGOPUZZLEVQA"}, {"figure_path": "https://arxiv.org/html/2502.01081/extracted/6172587/images/clock_case.png", "caption": "Figure 5: Example of puzzles from AlgoPuzzleVQA with visual features represented in the top row and algorithmic features in the bottom two rows. For each feature, at least one puzzle instance from each category is presented. Note that the header categories are not exhaustive, as some puzzles may belong to additional categories not listed in the headers. The complete categorization can be found in Section\u00a0B.1.", "description": "Figure 5 showcases examples of puzzles from the AlgoPuzzleVQA dataset. The top row highlights puzzles emphasizing visual features such as color, position, shape, or size.  The bottom two rows illustrate puzzles focused on algorithmic reasoning, including categories like arithmetic, Boolean logic, combinatorics, graph theory, optimization, search, and set theory.  Each visual and algorithmic category contains at least one example puzzle.  It's important to note that some puzzles may belong to multiple categories, although only the primary categories are listed in the figure's headers. For a complete breakdown of puzzle categorization, refer to Section B.1 of the paper.", "section": "2 PUZZLEVQA & ALGOPUZZLEVQA"}, {"figure_path": "https://arxiv.org/html/2502.01081/extracted/6172587/images/number_slide_case.png", "caption": "Figure 6: Case study on Clock in AlgoPuzzleVQA on multiple-choice and open-ended setting.", "description": "This figure shows a case study of the 'Clock' puzzle from the ALGOPUZZLEVQA dataset, comparing the performance of a model in multiple-choice and open-ended question answering settings.  It highlights the differences in model reasoning and accuracy when presented with structured choices versus the need for generating a free-form response.", "section": "4.3 DISCUSSIONS"}, {"figure_path": "https://arxiv.org/html/2502.01081/extracted/6172587/images/circle_number_size_case.png", "caption": "Figure 7: Case study on Chain Link in AlgoPuzzleVQA on multiple-choice and open-ended setting.", "description": "This figure shows a case study of the Chain Link puzzle from the ALGOPUZZLEVQA dataset. It compares the performance of the model in two different settings: multiple-choice and open-ended.  The multiple-choice setting provides the model with answer options, while the open-ended setting requires the model to generate its own answer. The figure showcases the model's responses (ground truth and model output) in each setting, highlighting the differences in its reasoning process and accuracy.  It aims to illustrate how the availability of answer choices affects the model's ability to solve the puzzle correctly and the underlying challenges related to reasoning and visual understanding in multimodal reasoning tasks.", "section": "4.3 DISCUSSIONS"}, {"figure_path": "https://arxiv.org/html/2502.01081/x5.png", "caption": "Figure 8: Case study on Number Slide in AlgoPuzzleVQA across GPT-[n] and o-[n] models.", "description": "This figure shows a case study of the Number Slide puzzle from the AlgoPuzzleVQA dataset, comparing the performance of GPT-[n] and o-[n] models.  It highlights the differences in how these models approach solving this puzzle, which involves manipulating numbered tiles on a grid to reach a target configuration. The figure demonstrates the models' ability to perceive the puzzle's state and reason through the necessary steps. Specifically, it showcases the superior performance of the o-[n] model compared to the GPT-[n] models, emphasizing the role of visual perception in successful puzzle solving. The example emphasizes that the o-[n] models correctly identify the location of the blank space, leading to a correct solution, whereas GPT-[n] models struggle with correct perception.", "section": "4.2 Expanded Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.01081/extracted/6172587/images/bottleneck.png", "caption": "Figure 9: Case study on an abstract puzzle from the Numbers & Size category in PuzzleVQA.", "description": "Figure 9 presents a case study of a specific puzzle from the Numbers & Size category within the PUZZLEVQA dataset.  It showcases how the model's performance changes depending on the level of information provided.  Three versions of the prompt are shown: the original prompt (only the visual puzzle is given), a prompt with additional perception details (visual puzzle + description of the puzzle elements), and a prompt with both perception and inductive reasoning details (visual puzzle + description of puzzle elements + explanation of the pattern).  This figure illustrates the reasoning bottlenecks faced by the models and highlights the impact of providing additional guidance on the model's ability to solve the puzzle correctly.", "section": "4.2 Expanded Evaluation"}]