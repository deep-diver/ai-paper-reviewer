{"references": [{"fullname_first_author": "Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the field of large language models (LLMs), introducing the concept of few-shot learning in LLMs which this paper builds upon."}, {"fullname_first_author": "Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This paper introduced the Vision Transformer (ViT), a crucial component in ARM4R's architecture for processing image data."}, {"fullname_first_author": "Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper introduced the CLIP model, which ARM4R leverages for language encoding, demonstrating the power of multi-modal learning which is relevant to ARM4R."}, {"fullname_first_author": "Collaboration", "paper_title": "OpenX-Embodiment: Robotic learning datasets and RT-X models", "publication_date": "2023-10-26", "reason": "This paper introduces the OpenX dataset, a large-scale dataset used in pre-training and benchmarking robotic models. ARM4R uses it for fine-tuning and comparison."}, {"fullname_first_author": "Brohan", "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control", "publication_date": "2023-07-18", "reason": "This paper details RT-2, a vision-language-action model, which is directly compared to ARM4R in the paper's experimental evaluation."}]}