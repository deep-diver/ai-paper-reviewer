{"references": [{"fullname_first_author": "Ralph Allan Bradley", "paper_title": "Rank analysis of incomplete block designs: I. the method of paired comparisons", "publication_date": "1952-00-00", "reason": "This paper introduces the Bradley-Terry model, a fundamental statistical model used in the paper's reward model training."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-00", "reason": "This paper establishes scaling laws for neural language models, providing crucial context for understanding the computational requirements of large language models for code generation."}, {"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-00-00", "reason": "This paper introduces HumanEval, a benchmark used to evaluate the code generation capabilities of large language models, which the authors use for their experiments."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This paper details Reinforcement Learning from Human Feedback (RLHF), a technique that the authors adapt and extend for training their reward model and improving code generation."}, {"fullname_first_author": "Daya Guo", "paper_title": "DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning", "publication_date": "2025-00-00", "reason": "This paper proposes DeepSeek-R1, a method of reinforcement learning that the authors draw inspiration from for their own RL training approach"}]}