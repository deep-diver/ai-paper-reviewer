{"importance": "This paper introduces **a novel image editing approach, Edit Transfer, that leverages visual in-context learning to enable complex, non-rigid transformations** from minimal examples. The research addresses limitations of text and reference-based methods, opening avenues for few-shot visual relation learning in image manipulation.", "summary": "Edit Transfer: Learns image edits from a single example and applies it to new images, surpassing text/reference-based methods!", "takeaways": ["Introduces Edit Transfer, a new task for learning image editing transformations from a single example.", "Proposes a visual relation in-context learning paradigm using a DiT-based model and LoRA fine-tuning.", "Demonstrates superior performance on non-rigid and compositional edits with minimal training data."], "tldr": "Text and reference-based image editing methods often fall short in complex spatial transformations due to limitations in expressing geometric details or transferring non-rigid poses. To address these limitations, this paper introduces a novel task, **Edit Transfer**, where the model learns a transformation from a single source-target editing example and applies it to a new query image. This approach aims to mitigate the shortcomings of existing methods by explicitly learning the editing transformation. \n\nThe paper proposes **a visual relation in-context learning paradigm**, inspired by in-context learning in large language models, to capture complex spatial transformations from minimal examples. By arranging the edited example and the query image into a unified four-panel composite and applying lightweight LoRA fine-tuning to a DiT-based text-to-image model, the method effectively transfers both single and compositional non-rigid edits, even with limited training data.", "affiliation": "Communication University of China", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}, "podcast_path": "2503.13327/podcast.wav"}