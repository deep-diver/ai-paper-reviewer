[{"figure_path": "https://arxiv.org/html/2410.21666/x1.png", "caption": "Figure 1: An example for Theorem\u00a03.", "description": "This figure illustrates Theorem 3, which describes how to find optimal couplings in the neighborhood of a deterministic mapping. It shows how, starting from a deterministic mapping represented by the matrix  `p<sub>XT</sub>`, one can obtain optimal solutions for slightly higher (`R<sub>g</sub> + \u03b5`) and lower (`R<sub>g</sub> - \u03b5`) entropy rates by carefully adjusting the probabilities in the matrix.  Specifically, it demonstrates the two probability mass transformations described in Theorem 3 for increasing and decreasing the rate. The transformations involve shifting a small amount of probability mass to a column that either has zero probability (`R<sub>g</sub> + \u03b5`) or to a column with the highest sum (`R<sub>g</sub> - \u03b5`). The resulting changes in mutual information (`I(X;T)`) are also depicted.", "section": "Optimal Coupling Around Deterministic Mappings"}, {"figure_path": "https://arxiv.org/html/2410.21666/x2.png", "caption": "Figure 2: \nSolutions to the EBIM problem for pX=[0.7,0.2,0.1]subscript\ud835\udc5d\ud835\udc4b0.70.20.1p_{X}=[0.7,0.2,0.1]italic_p start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT = [ 0.7 , 0.2 , 0.1 ]. Left: brute force solution. Right: application of the transformations from Theorem\u00a03 to each deterministic mapping (dashed lines) and selection of solutions with maximal mutual information for each R\ud835\udc45Ritalic_R value (thick solid line). This strategy effectively recovers optimal solutions, aligning with those found by brute force in this case.", "description": "Figure 2 illustrates the effectiveness of the proposed method for solving the Entropy-Bounded Information Maximization (EBIM) problem. The left panel shows the optimal solutions obtained via brute-force search for the input distribution pX = [0.7, 0.2, 0.1]. The right panel demonstrates the proposed two-step approach, where deterministic mappings are first identified using Algorithm 1, and then the optimal couplings near these mappings are found using Theorem 3. The dashed lines represent the couplings obtained from applying Theorem 3 to each deterministic mapping, while the thick solid line highlights the optimal couplings selected from among those solutions. This figure highlights the efficacy of the proposed algorithm in closely approximating the optimal solutions obtained by exhaustive search.", "section": "Entropy-Bounded Information Maximization"}, {"figure_path": "https://arxiv.org/html/2410.21666/x3.png", "caption": "Figure 3: The structure of a Markov Coding Game with Rate Limit.", "description": "In a rate-limited Markov Coding Game, a source transmits a message to a receiver via an agent. The agent participates in a Markov Decision Process (MDP) where actions indirectly convey information about the message.  The source compresses the message (signal T) before transmission to the agent, who then uses this information to guide its actions in the MDP.  Finally, the receiver attempts to decode the original message from the agent's observed MDP trajectory. The communication channel between the source and the agent has a rate constraint, limiting the amount of information that can be transmitted.", "section": "Application: Markov Coding Game with Rate Limit"}, {"figure_path": "https://arxiv.org/html/2410.21666/x4.png", "caption": "Figure 4: \nThe trade-off between average MDP reward vs. receiver\u2019s accuracy, navigated by varying the value of \u03b2\ud835\udefd\\betaitalic_\u03b2. Left: using our search algorithm for compression (Algorithm\u00a01), Right: using uniform quantization in Algorithm\u00a05. The message size is 512 with a uniform prior, and each data point is averaged over 200 episodes.", "description": "This figure illustrates the trade-off between the average reward obtained in a Markov Decision Process (MDP) and the accuracy with which a receiver decodes a message, controlled by a parameter \u03b2 (beta).  The left panel shows results using a novel deterministic search algorithm for message compression (Algorithm 1), while the right panel presents a baseline approach using uniform quantization (Algorithm 5).  Both approaches are tested with messages of size 512, uniformly distributed a priori. Each data point plotted represents the average outcome over 200 MDP episodes.", "section": "Application: Markov Coding Game with Rate Limit"}, {"figure_path": "https://arxiv.org/html/2410.21666/x5.png", "caption": "Figure 5: Evolution of message belief over time, for various values of \u03b2\ud835\udefd\\betaitalic_\u03b2 and rate budget, using our search algorithm for compression in Algorithm\u00a01 vs. uniform quantization in Algorithm\u00a05.", "description": "This figure visualizes the evolution of message belief (probability distribution over messages) across different time steps (agent actions) in a Markov Coding Game. It compares two compression methods: the authors' proposed deterministic EBIM solver (Algorithm 1) and a uniform quantization method (Algorithm 5).  Different lines represent different values of the temperature parameter (\u03b2) which controls the stochasticity of the agent's policy. Each plot shows a different compression rate (the ratio of message entropy to code budget). The figure demonstrates how the message belief converges toward the true message over time, illustrating the impact of both the compression method and the temperature parameter on decoding accuracy.", "section": "Application: Markov Coding Game with Rate Limit"}, {"figure_path": "https://arxiv.org/html/2410.21666/x6.png", "caption": "Figure 6: Optimal solutions in the neighborhood of a deterministic mapping.", "description": "This figure illustrates the optimal solutions for the Entropy-Bounded Information Maximization (EBIM) problem in the vicinity of a deterministic mapping.  It shows how the optimal solution changes as the entropy constraint (R) varies slightly above and below the entropy of the deterministic mapping (Rg).  The figure helps to visualize the impact of small changes to the entropy constraint on the optimal coupling between the input and output variables (X and T).  Specifically, it demonstrates the methods described in Theorem 3 for finding optimal couplings near a deterministic mapping by transferring infinitesimal probability mass between cells of the joint distribution matrix.", "section": "Optimal Coupling Around Deterministic Mappings"}, {"figure_path": "https://arxiv.org/html/2410.21666/x7.png", "caption": "Figure 7: The Grid World Setup used in the experiments. The starting cell is depicted by a red circle, while the goal, trap, and obstacle cells are colored green, red, and grey, respectively. Additionally, a non-deterministic policy is demonstrated through the probabilities of actions in each direction within each cell. The path taken by the agent is traced in black. Note that due to the noisy environment, the agent may move in directions not explicitly suggested by the policy.", "description": "The figure shows a grid world environment used in Markov Coding Game experiments.  The agent starts in a red circle and must navigate to a green goal circle, avoiding a red trap and grey obstacles.  Crucially, the agent's policy is non-deterministic, with probabilities for moving in each direction shown in each cell.  The black path illustrates one possible trajectory of the agent, demonstrating how the noisy environment can cause deviations from the intended actions.", "section": "Application: Markov Coding Game with Rate Limit"}, {"figure_path": "https://arxiv.org/html/2410.21666/x8.png", "caption": "Figure 8: The Maximum Entropy policy learned through Soft Q-Value iteration of Algorithm\u00a08, for log\u2061\u03b2=\u22126\ud835\udefd6\\log\\beta=-6roman_log italic_\u03b2 = - 6 (left) and log\u2061\u03b2=\u22123\ud835\udefd3\\log\\beta=-3roman_log italic_\u03b2 = - 3 (right).", "description": "This figure visualizes the Maximum Entropy policies obtained through Soft Q-value iteration (Algorithm 8) for two different values of the beta parameter (\u03b2). The left panel displays the policy when log(\u03b2) = -6, indicating a preference for high randomness in actions. Conversely, the right panel shows the policy when log(\u03b2) = -3, demonstrating a lower level of randomness in actions. The policies are represented as matrices, mapping states to action probabilities, and are learned within the Markov Coding Game environment described in the paper. These policies highlight the trade-off between the level of randomness in actions and their contribution to the overall reward within the game.", "section": "Application: Markov Coding Game with Rate Limit"}, {"figure_path": "https://arxiv.org/html/2410.21666/x9.png", "caption": "Figure 9: \nObtained I\u2062(X;T)\ud835\udc3c\ud835\udc4b\ud835\udc47I(X;T)italic_I ( italic_X ; italic_T ) vs. maximum allowed H\u2062(T)\ud835\udc3b\ud835\udc47H(T)italic_H ( italic_T ) for Binomial (left) and Truncated Geometric (right) input distributions.", "description": "This figure compares the mutual information achieved by our proposed deterministic EBIM solver against the encoder proposed by Shkel et al. [3], for different maximum allowed code entropies. The left panel shows results for a Binomial distribution, while the right panel presents results for a Truncated Geometric distribution. The comparison highlights the superior performance of our proposed approach, especially in lower rate regimes.", "section": "D.1 Deterministic EBIM Solver vs. Shkel et al. (2017)"}, {"figure_path": "https://arxiv.org/html/2410.21666/x10.png", "caption": "Figure 10: \nGenerated couplings in MEC-B formulation (2), for uniform input and output distributions. The compression rate is defined as H\u2062(X)/R\ud835\udc3b\ud835\udc4b\ud835\udc45H(X)/Ritalic_H ( italic_X ) / italic_R. Higher compression rates lead to more stochastic couplings with increased entropy.", "description": "Figure 10 illustrates the impact of compression rate on the resulting coupling between the input (X) and output (Y) distributions in the Minimum Entropy Coupling with Bottleneck (MEC-B) framework.  The input and output distributions are uniform.  The compression rate is calculated as the ratio of the input entropy H(X) to the allowed code rate R. The figure shows that at lower compression rates (H(X)/R closer to 1), couplings tend to be deterministic, with little stochasticity. As the compression rate increases (H(X)/R becomes larger),  the couplings become increasingly stochastic, characterized by higher entropy and less predictability in mapping from X to Y.", "section": "Application: Markov Coding Game with Rate Limit"}, {"figure_path": "https://arxiv.org/html/2410.21666/x11.png", "caption": "Figure 11: Block diagram of the unsupervised image restoration framework.", "description": "This block diagram illustrates the architecture of the unsupervised image restoration framework.  It shows the data flow from a low-resolution input image (X) through an encoder (f_\u03b8) that produces a compressed representation (T).  This compressed representation is then passed to a generator (g_\u03c6), which adds noise (z) to produce an upscaled, potentially noisy image (\u0176).  A discriminator (d_\u03c8) is used to enforce the desired output distribution (p_Y) by comparing the generated upscaled image to high-resolution images in the target domain (Y). Finally, a reconstructor network (\u03b1_\u03b3) refines the image based on \u0176 and the compressed representation T.", "section": "E Unsupervised Image Restoration"}, {"figure_path": "https://arxiv.org/html/2410.21666/x12.png", "caption": "Figure 12: Output samples from the MNIST dataset, for different number of code dimensions and the number of bits per dimension of the code.", "description": "This figure visualizes the results of unsupervised image restoration on the MNIST dataset. It showcases the reconstructed images from compressed representations, varying the number of code dimensions and bits per dimension. Each image grid represents a set of reconstructed images, demonstrating the impact of compression parameters on the quality of the restored images.", "section": "E Unsupervised Image Restoration"}, {"figure_path": "https://arxiv.org/html/2410.21666/x13.png", "caption": "Figure 13: Input and output samples from the SVHN dataset.", "description": "This figure displays a comparison of input and output images from the Street View House Numbers (SVHN) dataset after applying an unsupervised image restoration technique.  The input images are low-resolution, and the outputs show the corresponding upscaled versions. This illustrates the model's ability to reconstruct higher-resolution images from lower-resolution input without direct paired training data, which is a key characteristic of unsupervised learning.", "section": "E Unsupervised Image Restoration"}]