{"references": [{"fullname_first_author": "Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduced the Transformer architecture, which is fundamental to many modern LLMs."}, {"fullname_first_author": "Dao", "paper_title": "FlashAttention: Fast and memory-efficient exact attention with IO-awareness", "publication_date": "2022-05-14", "reason": "FlashAttention optimizes memory access patterns for faster attention computation, a crucial element in Transformer efficiency."}, {"fullname_first_author": "Zaheer", "paper_title": "Big bird: Transformers for longer sequences", "publication_date": "2020-01-01", "reason": "BigBird introduced sparse attention mechanisms to reduce the computational complexity of Transformers, enabling longer sequence processing."}, {"fullname_first_author": "Child", "paper_title": "Generating long sequences with sparse transformers", "publication_date": "2019-01-01", "reason": "This work explores sparse transformers, highlighting the potential for reducing computation in long sequence generation."}, {"fullname_first_author": "Peng", "paper_title": "Yarn: Efficient context window extension of large language models", "publication_date": "2023-01-01", "reason": "This paper describes a method to extend the context window of LLMs, improving performance in processing longer sequences."}]}