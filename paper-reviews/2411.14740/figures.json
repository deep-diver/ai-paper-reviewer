[{"figure_path": "https://arxiv.org/html/2411.14740/x1.png", "caption": "Figure 1. \n3D meshes with textures generated by our method.\nWe show a gallery of 3D meshes with textures generated by our method (left) and the texture map and multi-view renderings of the bird model (right).\nOur approach models the distribution of mesh textures at high resolution, generating high-quality textures from text and image prompts, more multi-view renderings are shown in fig.\u00a05.", "description": "This figure displays the results of the TEXGen method for generating textures on 3D meshes. The left side shows a variety of 3D models with high-resolution textures created using text and image prompts as input. The right side focuses on a bird model, showing its texture map and multiple views, highlighting the quality and detail of the generated textures.  More views of the bird model are available in Figure 5.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2411.14740/x2.png", "caption": "Figure 2. \nAn illustration of (a) a mesh with its (b) UV map. Three islands S1subscript\ud835\udc461S_{1}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, S2subscript\ud835\udc462S_{2}italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and S3subscript\ud835\udc463S_{3}italic_S start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT are shown both on the mesh surface and its flattened UV map, where continuous islands S1subscript\ud835\udc461S_{1}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and S2subscript\ud835\udc462S_{2}italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are positioned far apart on the UV map while disconnected islands S1subscript\ud835\udc461S_{1}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and S3subscript\ud835\udc463S_{3}italic_S start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT show closer distance on the UV map.", "description": "This figure illustrates the concept of UV mapping in 3D mesh processing. It shows a 3D mesh and its corresponding 2D UV map.  The key point highlighted is that the spatial relationships between different parts of the mesh (called 'islands' in the caption) are not always preserved in the UV map. For instance, two continuous parts of the mesh might end up far apart in the UV map, while two disconnected parts could appear closer together. This difference is important because many 2D image-processing techniques are used on UV maps.  Understanding the mapping's limitations is crucial for successful texture generation and other mesh-processing tasks.", "section": "Representation for Texture Synthesis"}, {"figure_path": "https://arxiv.org/html/2411.14740/x3.png", "caption": "Figure 3. An overview of TEXGen. (a). An overview of our training pipeline. We train a diffusion model to\ngenerate high-resolution texture maps for a given mesh S\ud835\udc46Sitalic_S based on a single-view image I\ud835\udc3cIitalic_I and text descriptions by learning to denoise from a noise texture map xtsubscript\ud835\udc65\ud835\udc61x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. The core of our denoising network is our proposed hybrid 2D-3D block. (b). The structure of a single hybrid block. (c)-(d). The detailed designs of our UV head block and point block.", "description": "Figure 3 provides a comprehensive overview of the TEXGen model, illustrating its training pipeline and architecture. Panel (a) shows the overall training process, where a diffusion model learns to generate high-resolution texture maps from a noisy input texture map, guided by a single-view image and text descriptions.  The model iteratively refines the noisy texture to reach a high-quality output. The core of the denoising network is a novel hybrid 2D-3D block, explained further in the other panels. Panel (b) details the structure of this hybrid 2D-3D block.  Finally, Panels (c) and (d) provide detailed diagrams showcasing the specific design of the UV head block (processing 2D texture information) and the point block (incorporating 3D spatial information) respectively.  These blocks work together to effectively process both 2D texture features and 3D spatial relationships for accurate and coherent texture generation.", "section": "4 METHOD"}, {"figure_path": "https://arxiv.org/html/2411.14740/x4.png", "caption": "Figure 4. An illustration of the feature learning procedure in 3D space. In panel (a), we start with rasterized dense point features, which we sparsify using grid-pooling to create sparse point features shown in (b). Different pools are indicated by various colors in (a). These points are then serialized to determine their order for subsequent group-based self-attention, as part of the learning process shown in (d). In (c), we visualize different groups formed based on Hilbert serialization, where each color signifies a distinct group. Finally, the processed features are scattered back to their original coordinates, providing the output dense point features.", "description": "This figure details the feature learning process within the 3D space of the TEXGen model.  Panel (a) shows the initial dense point cloud features.  These are then sparsified using grid-pooling, as shown in (b), where different colored regions represent different pools of points. Panel (c) visualizes the groups created using Hilbert serialization, with each color representing a distinct group.  Finally, panel (d) illustrates the overall learning process, showing how the serialized, grouped points are processed through self-attention, and the results are then scattered back to their original dense format, producing the final output features.", "section": "4.2 Model Construction"}, {"figure_path": "https://arxiv.org/html/2411.14740/x5.png", "caption": "Figure 5. Texture generation results. For given meshes, our method can synthesize highly detailed textures conditioned on guided single-view images and text prompts. We show three novel view images from our textured results and representative zoom-in regions from the textured mesh. The generated full texture maps are also shown.", "description": "This figure showcases the results of the TEXGen model in generating high-resolution textures for various 3D meshes. The model's ability to synthesize highly detailed textures using both single-view images and text prompts is demonstrated.  For each mesh, the figure displays three novel views rendered with the generated texture, several zoomed-in sections highlighting the level of detail achieved, and the complete UV texture map itself.  This provides a comprehensive visualization of the model's output, showcasing its effectiveness across different object types and the high fidelity of the generated textures.", "section": "4.4 Texture Generation"}, {"figure_path": "https://arxiv.org/html/2411.14740/x6.png", "caption": "Figure 6. Comparison with state-of-the-art methods. We compare our method with four representative state-of-the-art methods. Our model can synthesize more detailed and coherent textures compared to these methods which rely on test-time optimization using a 2D pretrained text-to-image diffusion model. Also, our method trained on the 3D dataset and 3D representation avoids the Janus problem that commonly occurs in other methods.", "description": "Figure 6 presents a comparison of texture synthesis results between the proposed TEXGen model and four other state-of-the-art methods: TEXTure, Text2Tex, Paint3D.  Each method is applied to four 3D models (tote bag, house, wooden barrel, frog), and the resulting textures are shown alongside the original models. This visual comparison highlights TEXGen's ability to generate more detailed and coherent textures compared to methods relying on 2D diffusion models for test-time optimization.  The figure also demonstrates that TEXGen, trained with 3D data and using a 3D representation, successfully avoids the \"Janus problem\", a common artifact in other methods where features like eyes or mouths are unnaturally duplicated on opposite sides of a 3D object.", "section": "5.1 Main Results and Comparisons"}, {"figure_path": "https://arxiv.org/html/2411.14740/x7.png", "caption": "Figure 7. An indoor scene with all meshes textured by TEXGen. We generate a single view using text-conditioned ControlNet with depth control for each mesh and paint them with both the text and single view prompt with TEXGen.", "description": "This figure showcases an indoor scene where multiple 3D objects have been textured using the TEXGen model.  Each object's texture was generated using a combination of text prompts describing the desired appearance and a single-view image generated by ControlNet, which incorporates depth information for enhanced realism. The resulting textures are detailed and realistic, demonstrating the model's ability to create high-quality textures for diverse 3D models within a coherent scene.", "section": "5.2 Applications"}, {"figure_path": "https://arxiv.org/html/2411.14740/x8.png", "caption": "Figure 8. TEXGen as a texture inpainter. We demonstrate the potential of TEXGen as a texture inpainter. We showcase here (a) randomly masked texture maps and (b) the inpainted texture maps, with unknown regions rendered as black.", "description": "This figure demonstrates the use of TEXGen as a texture inpainting model.  Part (a) shows examples of texture maps with randomly masked regions, simulating missing or incomplete texture data.  Part (b) displays the same texture maps after TEXGen has been used to inpaint the masked regions, filling in the missing texture information. The inpainted areas are clearly distinguishable from the original texture, and the unknown regions are represented as black for visualization. This illustrates the model's capacity to effectively reconstruct and complete incomplete texture data.", "section": "5.2 Applications"}, {"figure_path": "https://arxiv.org/html/2411.14740/x9.png", "caption": "Figure 9. Texture completion from sparse views. With sparse views of objects provided (front and back views as shown in (a)), unprojected textures retain many unseen regions (b). TEXGen effectively fills these unseen regions with harmonious textures (c).", "description": "Figure 9 demonstrates TEXGen's ability to complete textures from sparse views.  Panel (a) shows the limited input views (front and back) available to the model. In (b), the texture is shown with the unseen areas unprojected (and therefore black/missing).  Panel (c) displays the completed texture generated by TEXGen, showing how the model seamlessly fills in the missing portions in a visually consistent manner, resulting in harmonious textures across the entire 3D model.", "section": "5.2 Applications"}, {"figure_path": "https://arxiv.org/html/2411.14740/x10.png", "caption": "Figure 10. Qualitative ablation results on the hybrid design. Compared to the full model A (a), the model B (b) with only UV blocks can not easily capture overall semantic and 3D consistency while that the model C (c) with only point blocks struggles with producing high-frequency patterns.", "description": "Figure 10 shows an ablation study comparing three model variations: the full model (A), a model using only UV blocks (B), and a model using only point blocks (C).  Model A, the full model, serves as the baseline, effectively capturing both high-level semantic understanding of the scene and maintaining 3D consistency across the mesh.  In contrast, model B, lacking the 3D point cloud processing, fails to capture the overall scene semantics and struggles with maintaining consistent textures across the 3D surface. Finally, model C, which omits UV-based feature extraction, significantly struggles to reproduce high-frequency details and fine textures within the mesh, resulting in a loss of visual fidelity. This comparison highlights the importance of the hybrid 2D-3D architecture in achieving high-quality and consistent texture synthesis.", "section": "5.3 Model Analysis"}, {"figure_path": "https://arxiv.org/html/2411.14740/x11.png", "caption": "Figure 11. Results on 3D avatars. Our model, trained on 3D data, adeptly avoids the Janus problem.", "description": "Figure 11 showcases the results of applying the TEXGen model to generate textures for various 3D avatars.  The images demonstrate the model's ability to produce high-quality, detailed textures while avoiding the \"Janus problem.\" The Janus problem is a common artifact in texture generation where features like eyes or mouths are duplicated on opposite sides of a 3D model due to limitations in how the model processes the texture across different views.  The figure visually emphasizes that TEXGen, trained with a focus on 3D data and representations, successfully generates coherent and realistic textures without such artifacts.", "section": "5.1 Main Results and Comparisons"}, {"figure_path": "https://arxiv.org/html/2411.14740/x12.png", "caption": "Figure 12. Results on real-scan models. Our method is robust to real-scan models with non-smooth surfaces and fragmented UV maps.", "description": "Figure 12 showcases the effectiveness of the TEXGen model on real-world, scanned 3D models.  Real-scan models often present challenges due to irregularities like non-smooth surfaces and fragmented UV maps (the 2D representation of the 3D mesh's texture). The figure demonstrates that TEXGen successfully generates high-quality textures even on these complex models, highlighting its robustness and generalizability.", "section": "5 Experiments"}]