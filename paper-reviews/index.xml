<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>About on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/</link><description>Recent content in About on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Mon, 13 Jun 2022 20:55:37 +0100</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/index.xml" rel="self" type="application/rss+xml"/><item><title>Counting Ability of Large Language Models and Impact of Tokenization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19730/</link><pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19730/</guid><description>LLM counting abilities are surprisingly sensitive to tokenization; carefully crafted tokenization strategies significantly improve accuracy, bridging the gap between theory and practice.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19730/cover.png"/></item><item><title>FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19355/</link><pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19355/</guid><description>FasterCache: a training-free strategy boosts video diffusion model inference speed by 1.67x without sacrificing video quality, using dynamic feature reuse and CFG-Cache.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19355/cover.png"/></item><item><title>Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19290/</link><pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19290/</guid><description>PREREQ-TUNE, a novel LLM fine-tuning strategy, disentangles skill and knowledge learning to significantly reduce hallucinations by mitigating knowledge inconsistency between pre-training and fine-tuni&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19290/cover.png"/></item><item><title>Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18889/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18889/</guid><description>LLMs can detect and correct substantial label errors in NLP datasets, significantly improving model performance and highlighting the importance of data quality in NLP.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18889/cover.png"/></item><item><title>CAMEL-Bench: A Comprehensive Arabic LMM Benchmark</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18976/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18976/</guid><description>CAMEL-Bench: a new open-source benchmark rigorously evaluates Arabic LMMs across 8 diverse domains and 38 sub-domains, revealing significant room for improvement even in top models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18976/cover.png"/></item><item><title>CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18505/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18505/</guid><description>CCI3.0-HQ: A new 500GB high-quality Chinese dataset significantly boosts large language model performance, surpassing existing datasets on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18505/cover.png"/></item><item><title>Data Scaling Laws in Imitation Learning for Robotic Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18647/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18647/</guid><description>Robotic manipulation policies achieve near 90% success in novel environments and with unseen objects using a data-driven approach that leverages power-law scaling relationships.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18647/cover.png"/></item><item><title>DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18860/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18860/</guid><description>DeCoRe, a novel training-free decoding strategy, significantly reduces LLM hallucinations by contrasting outputs from masked and unmasked retrieval heads, improving accuracy on various tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18860/cover.png"/></item><item><title>Distill Visual Chart Reasoning Ability from LLMs to MLLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18798/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18798/</guid><description>Researchers synthesize a new multimodal dataset, REACHQA, using code as an intermediary to efficiently distill visual chart reasoning abilities from LLMs to MLLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18798/cover.png"/></item><item><title>Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18912/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18912/</guid><description>This work introduces a new framework that learns object dynamics directly from multi-view videos by explicitly considering robot actions, achieving accurate 3D action-conditioned video prediction and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18912/cover.png"/></item><item><title>Framer: Interactive Frame Interpolation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18978/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18978/</guid><description>Framer: an interactive frame interpolation tool lets users customize video transitions by adjusting keypoints, yielding smooth, creative results—even handling complex scenarios with an &amp;lsquo;autopilot&amp;rsquo; mod&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18978/cover.png"/></item><item><title>Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19133/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19133/</guid><description>Researchers developed a hybrid approach to collecting preference data for AI alignment, cleverly routing instances to either human or AI annotators based on a predictive model, resulting in improved m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19133/cover.png"/></item><item><title>Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18558/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18558/</guid><description>Infinity-MM, a 40-million-sample multimodal instruction dataset, boosts open-source VLM performance to state-of-the-art levels by combining real-world and synthetic data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18558/cover.png"/></item><item><title>LOGO -- Long cOntext aliGnment via efficient preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18533/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18533/</guid><description>LOGO, a novel training strategy, significantly boosts long-context model performance by efficiently optimizing preference alignment, achieving comparable results to GPT-4 with minimal data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18533/cover.png"/></item><item><title>MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19168/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19168/</guid><description>MMAU benchmark challenges multimodal LLMs with diverse audio tasks, revealing significant gaps in current audio understanding capabilities and driving future advancements.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19168/cover.png"/></item><item><title>MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18977/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18977/</guid><description>MotionCLR: Training-free, interactive human motion editing via attention mechanism manipulation. Versatile editing, good generation quality, and strong explainability achieved.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18977/cover.png"/></item><item><title>Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18775/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18775/</guid><description>VINE, a novel watermarking method, significantly improves robustness against advanced image editing using generative priors, outperforming existing methods in both image quality and robustness, as val&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18775/cover.png"/></item><item><title>Should We Really Edit Language Models? On the Evaluation of Edited Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18785/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18785/</guid><description>Contrary to popular belief, current language model editing techniques cause inevitable performance decline and safety issues when scaling edits, urging the need for more practical methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18785/cover.png"/></item><item><title>Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18451/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18451/</guid><description>Skywork-Reward achieves state-of-the-art results on RewardBench using a novel data-centric approach, developing high-performing reward models with a significantly smaller dataset (80K pairs) than exis&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18451/cover.png"/></item><item><title>SMITE: Segment Me In TimE</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18538/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18538/</guid><description>SMITE: a new video segmentation method achieving temporally consistent, fine-grained segmentations using only a few reference images, outperforming state-of-the-art alternatives.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18538/cover.png"/></item><item><title>Stable Consistency Tuning: Understanding and Improving Consistency Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18958/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18958/</guid><description>Stable Consistency Tuning (SCT) significantly boosts consistency model training, achieving state-of-the-art results by reducing variance and improving sampling efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18958/cover.png"/></item><item><title>Taipan: Efficient and Expressive State Space Language Models with Selective Attention</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18572/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18572/</guid><description>Taipan, a novel hybrid language model, achieves superior performance and efficiency in handling extremely long text sequences by selectively applying attention, combining the strengths of State Space &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18572/cover.png"/></item><item><title>The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18441/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18441/</guid><description>This paper enhances generative AI Transformer models by introducing probabilistic optimization solutions for subword encoding, hyperparameter tuning, attention mechanisms, and quantization, resulting &amp;hellip;</description></item><item><title>Unbounded: A Generative Infinite Game of Character Life Simulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18975/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18975/</guid><description>UNBOUNDED, a generative infinite game, uses AI to create a continuously evolving character life simulation with open-ended interactions and real-time visual generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18975/cover.png"/></item><item><title>Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18693/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18693/</guid><description>ScaleQuest: a novel data synthesis method unleashes LLMs&amp;rsquo; reasoning power by generating a massive, high-quality mathematical reasoning dataset from scratch using efficient, open-source models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18693/cover.png"/></item><item><title>WAFFLE: Multi-Modal Model for Automated Front-End Development</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18362/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18362/</guid><description>WAFFLE: a new fine-tuning method dramatically improves UI design-to-HTML code generation by using structure-aware attention and contrastive learning, outperforming current state-of-the-art models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18362/cover.png"/></item><item><title>Why Does the Effective Context Length of LLMs Fall Short?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18745/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18745/</guid><description>Researchers unveil STRING, a training-free method that boosts large language models&amp;rsquo; long-context performance by cleverly shifting position embeddings, achieving state-of-the-art results on open-sourc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18745/cover.png"/></item><item><title>ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17779/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17779/</guid><description>ADEM-VL boosts vision-language model efficiency by using a parameter-free cross-attention mechanism and an adaptive fusion scheme, achieving state-of-the-art accuracy with reduced computational demand&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17779/cover.png"/></item><item><title>Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18252/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18252/</guid><description>Asynchronous off-policy RLHF accelerates LLM training by 40% without sacrificing performance, achieving compute-optimal scaling by decoupling generation and learning phases.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18252/cover.png"/></item><item><title>DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18084/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18084/</guid><description>DynamicCity generates large-scale, high-quality 4D LiDAR scenes capturing dynamic environments, surpassing existing methods in efficiency and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18084/cover.png"/></item><item><title>Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18076/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18076/</guid><description>SUPE leverages unlabeled prior data to pre-train skills and pseudo-label trajectories for efficient online RL exploration, significantly outperforming existing methods on challenging tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18076/cover.png"/></item><item><title>Lightweight Neural App Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17883/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17883/</guid><description>LiMAC, a novel lightweight architecture, enables efficient mobile app control by combining a small action transformer with a fine-tuned vision-language model, significantly improving accuracy and spee&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17883/cover.png"/></item><item><title>MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17637/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17637/</guid><description>MIA-DPO boosts large vision-language models&amp;rsquo; multi-image understanding by cleverly augmenting single-image data and using attention mechanisms to improve preference alignment, significantly reducing a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17637/cover.png"/></item><item><title>Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18234/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18234/</guid><description>Researchers boost large language model inference speed by 10x using a novel multi-draft speculative sampling method with theoretical performance guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18234/cover.png"/></item><item><title>ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17856/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17856/</guid><description>ROCKET-1 masters open-world Minecraft interaction by using visual-temporal context prompting, enabling VLMs to effectively guide low-level policies for complex tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17856/cover.png"/></item><item><title>Scalable Ranked Preference Optimization for Text-to-Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18013/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18013/</guid><description>Researchers created a scalable training method for text-to-image models using synthetic, ranked preference data, significantly improving both visual quality and prompt-following.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18013/cover.png"/></item><item><title>Scaling Diffusion Language Models via Adaptation from Autoregressive Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17891/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17891/</guid><description>Researchers efficiently adapt existing large autoregressive language models into competitive diffusion language models, achieving scalability and outperforming prior diffusion models on various benchm&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17891/cover.png"/></item><item><title>TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18071/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18071/</guid><description>TP-Eval, a novel framework, tackles MLLM evaluation bias by customizing prompts for each model, revealing true capabilities and improving benchmark reliability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18071/cover.png"/></item><item><title>Value Residual Learning For Alleviating Attention Concentration In Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17897/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17897/</guid><description>ResFormer and SVFormer alleviate Transformer attention concentration, boosting training speed and accuracy by introducing residual value connections and single-layer value sharing, respectively.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17897/cover.png"/></item><item><title>WorldSimBench: Towards Video Generation Models as World Simulators</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18072/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18072/</guid><description>WorldSimBench: a new benchmark rigorously evaluates video generation models as embodied AI agents, using dual evaluation (perceptual and manipulative) and the novel HF-Embodied Dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18072/cover.png"/></item><item><title>ZIP-FIT: Embedding-Free Data Selection via Compression-Based Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18194/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18194/</guid><description>ZIP-FIT uses gzip compression to efficiently select task-relevant training data for language models, drastically improving fine-tuning speed and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18194/cover.png"/></item><item><title>Aligning Large Language Models via Self-Steering Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17131/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17131/</guid><description>Self-Steering Optimization (SSO) autonomously generates high-quality preference signals for aligning large language models, significantly improving performance across various benchmarks without manual&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17131/cover.png"/></item><item><title>Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17243/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17243/</guid><description>Inf-CL shatters memory limits in contrastive learning, enabling training with massive batch sizes (millions) using a novel tile-based computation strategy for unprecedented accuracy and speed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17243/cover.png"/></item><item><title>Frontiers in Intelligent Colonoscopy</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17241/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17241/</guid><description>This study advances intelligent colonoscopy by creating ColonINST, a large-scale multimodal dataset, and ColonGPT, a multimodal language model, to improve colonoscopic scene perception.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17241/cover.png"/></item><item><title>JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17250/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17250/</guid><description>JMMMU, a new benchmark, rigorously evaluates large multimodal models&amp;rsquo; Japanese language and cultural understanding, revealing significant performance gaps and highlighting the need for culturally awar&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17250/cover.png"/></item><item><title>LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17434/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17434/</guid><description>LongVU, a novel spatiotemporal compression mechanism, enables efficient processing of long videos by LLMs, improving video understanding performance significantly.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17434/cover.png"/></item><item><title>LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17242/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17242/</guid><description>LVSM, a novel transformer-based model, achieves state-of-the-art novel view synthesis by eliminating 3D inductive biases, enabling superior quality, scalability, and zero-shot generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17242/cover.png"/></item><item><title>Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16930/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16930/</guid><description>Math Neurosurgery precisely isolates math reasoning parameters within LLMs using only forward passes, boosting performance without affecting non-math skills.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16930/cover.png"/></item><item><title>MiniPLM: Knowledge Distillation for Pre-Training Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17215/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17215/</guid><description>MINIPLM: A novel knowledge distillation framework boosts pre-trained language models&amp;rsquo; performance by efficiently refining the training data distribution using teacher LM knowledge, achieving significa&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17215/cover.png"/></item><item><title>PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17247/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17247/</guid><description>PyramidDrop boosts Large Vision-Language Model efficiency by 40% during training and 55% during inference, achieving comparable performance by progressively reducing image token redundancy in deeper l&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17247/cover.png"/></item><item><title>SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17249/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17249/</guid><description>SpectroMotion: a novel approach to reconstruct dynamic specular scenes by combining 3D Gaussian Splatting with physically-based rendering and deformation fields, outperforming existing methods in view&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17249/cover.png"/></item><item><title>3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16266/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16266/</guid><description>3DGS-Enhancer boosts 3D scene rendering from sparse views by cleverly using video diffusion priors to improve view consistency, resulting in superior quality and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16266/cover.png"/></item><item><title>Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16259/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16259/</guid><description>Agent-to-Sim (ATS) learns realistic 3D agent behavior models from casual, longitudinal videos by reconstructing a persistent 4D representation and training a generative model, enabling real-to-sim tra&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16259/cover.png"/></item><item><title>Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15748/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15748/</guid><description>Alchemy: Amplifying Theorem-Proving with Symbolic Mutation synthesizes formal mathematical theorems, boosting neural theorem-proving performance by up to 5%.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15748/cover.png"/></item><item><title>AutoTrain: No-code training for state-of-the-art models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15735/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15735/</guid><description>AutoTrain: a no-code, open-source library simplifies training state-of-the-art models on custom datasets for various tasks, democratizing access to advanced AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15735/cover.png"/></item><item><title>Can Knowledge Editing Really Correct Hallucinations?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16251/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16251/</guid><description>HalluEditBench: A new benchmark reveals knowledge editing&amp;rsquo;s limitations in truly fixing LLM hallucinations, offering valuable insights for future improvements.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16251/cover.png"/></item><item><title>CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16256/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16256/</guid><description>CompassJudger-1: An open-source, all-in-one judge LLM offering unitary scoring, model comparison, critique generation, and diverse task execution, significantly advancing LLM evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16256/cover.png"/></item><item><title>Continuous Speech Synthesis using per-token Latent Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16048/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16048/</guid><description>SALAD, a novel per-token latent diffusion model, achieves superior zero-shot speech synthesis, surpassing discrete methods in intelligibility while maintaining speech quality and speaker similarity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16048/cover.png"/></item><item><title>FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16271/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16271/</guid><description>FrugalNeRF: achieving high-fidelity 3D scene reconstruction from minimal data with unprecedented speed, eliminating the need for pre-trained models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16271/cover.png"/></item><item><title>Improve Vision Language Model Chain-of-thought Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16198/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16198/</guid><description>Researchers enhance vision-language model reasoning by distilling rationales from GPT-4, fine-tuning with a new dataset, and applying reinforcement learning, achieving significant performance gains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16198/cover.png"/></item><item><title>Language Models are Symbolic Learners in Arithmetic</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15580/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15580/</guid><description>LLMs don&amp;rsquo;t calculate; they&amp;rsquo;re symbolic learners in arithmetic, mastering tasks through subgroup pattern recognition, prioritizing easy-to-hard pattern selection.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15580/cover.png"/></item><item><title>LLM-based Optimization of Compound AI Systems: A Survey</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16392/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16392/</guid><description>This survey reveals how Large Language Models (LLMs) efficiently optimize complex AI systems by acting as end-to-end optimizers, bypassing gradient calculations and generating intricate instructions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16392/cover.png"/></item><item><title>Mitigating Object Hallucination via Concentric Causal Attention</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15926/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15926/</guid><description>Concentric Causal Attention (CCA) significantly reduces object hallucination in large vision-language models by mitigating the negative effects of long-term decay in Rotary Position Encoding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15926/cover.png"/></item><item><title>Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16153/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16153/</guid><description>PANGEA: A fully open multilingual, multimodal LLM trained on a diverse 6M instruction dataset, significantly outperforming existing models in multilingual and culturally diverse scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16153/cover.png"/></item><item><title>Pantograph: A Machine-to-Machine Interaction Interface for Advanced Theorem Proving, High Level Reasoning, and Data Extraction in Lean 4</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16429/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16429/</guid><description>Pantograph: a new Lean 4 interface boosts machine-assisted theorem proving by enabling efficient proof search and high-level reasoning via novel features, including draft-sketch-proof (DSP) support.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16429/cover.png"/></item><item><title>Pre-training Distillation for Large Language Models: A Design Space Exploration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16215/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16215/</guid><description>Boosting large language model pre-training: This research explores pre-training distillation, systematically optimizing its design to significantly improve student LLM performance.</description></item><item><title>Reflection-Bench: probing AI intelligence with reflection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16270/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16270/</guid><description>Reflection-Bench, a new benchmark, reveals current LLMs lack true reflection abilities, highlighting a critical gap in achieving human-level AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16270/cover.png"/></item><item><title>RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16184/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16184/</guid><description>RM-BENCH, a novel benchmark, rigorously evaluates reward models&amp;rsquo; sensitivity to subtle content and style biases, showing a strong correlation with policy model performance and revealing significant ro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16184/cover.png"/></item><item><title>SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16268/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16268/</guid><description>SAM2Long enhances video object segmentation by adding a training-free memory tree, significantly improving accuracy and robustness in complex, long-term videos without needing further training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16268/cover.png"/></item><item><title>Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15633/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15633/</guid><description>GATEAU, a novel framework, efficiently selects high-quality long-context samples for LLM training by using Homologous Models&amp;rsquo; Guidance and Contextual Awareness Measurement, significantly boosting perf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15633/cover.png"/></item><item><title>Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15999/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15999/</guid><description>SPARE, a training-free method, uses sparse autoencoders to precisely steer LLMs&amp;rsquo; knowledge selection, resolving context-memory conflicts and significantly improving accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15999/cover.png"/></item><item><title>Teach Multimodal LLMs to Comprehend Electrocardiographic Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19008/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19008/</guid><description>PULSE, a new MLLM, achieves state-of-the-art accuracy in ECG image interpretation, exceeding existing models by 15-30%, thanks to a novel million-sample instruction tuning dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19008/cover.png"/></item><item><title>xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16267/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16267/</guid><description>xGen-MM-Vid efficiently captures temporal information in videos using only 32 tokens, achieving state-of-the-art accuracy with significantly reduced computational cost.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16267/cover.png"/></item><item><title>Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15460/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15460/</guid><description>New training method, Sensitive Neuron Dropout (SeND), reduces large language model hallucinations by up to 40% while improving efficiency.</description></item><item><title>Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15316/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15316/</guid><description>Ichigo, a new real-time voice assistant, leverages a novel mixed-modal early-fusion approach for superior speed and accuracy in speech-based tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15316/cover.png"/></item><item><title>M-RewardBench: Evaluating Reward Models in Multilingual Settings</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15522/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15522/</guid><description>M-REWARDBENCH: A new multilingual benchmark reveals significant performance gaps in reward models across languages, highlighting the need for improved multilingual LLM development.</description></item><item><title>Baichuan Alignment Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14940/</link><pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14940/</guid><description>Baichuan Alignment unveils cutting-edge techniques for aligning large language models, resulting in significant performance improvements and valuable insights for advancing AI research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14940/cover.png"/></item><item><title>DM-Codec: Distilling Multimodal Representations for Speech Tokenization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15017/</link><pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15017/</guid><description>DM-Codec, a novel speech tokenizer, leverages combined language and speech model distillation to achieve state-of-the-art performance in speech tokenization, reducing error rates significantly.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15017/cover.png"/></item><item><title>How Many Van Goghs Does It Take to Van Gogh? Finding the Imitation Threshold</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15002/</link><pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15002/</guid><description>Researchers discover the &amp;lsquo;imitation threshold&amp;rsquo; in text-to-image models: around 200-600 training examples of a concept are needed before reliable imitation occurs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15002/cover.png"/></item><item><title>Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14677/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14677/</guid><description>AI-generated text detection is flawed; this paper reveals dataset quality issues, proposes evaluation methods, and shows how high-quality generated data can improve detection model accuracy.</description></item><item><title>BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14672/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14672/</guid><description>BiGR: A novel image generation model using compact binary codes, unifying generation and discrimination for superior performance and zero-shot generalization across various vision tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14672/cover.png"/></item><item><title>EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14649/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14649/</guid><description>EvoPress: A new evolutionary search method achieves optimal dynamic LLM compression, surpassing current techniques in accuracy and efficiency across various compression methods.</description></item><item><title>How Do Training Methods Influence the Utilization of Vision Models?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14470/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14470/</guid><description>Training methods profoundly alter how neural networks utilize their layers, revealing that efficient training prioritizes early layers while adversarial training emphasizes deeper ones.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14470/cover.png"/></item><item><title>Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14208/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14208/</guid><description>Montessori-Instruct optimizes synthetic training data for LLMs by aligning it with student learning preferences, significantly boosting student model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14208/cover.png"/></item><item><title>NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14669/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14669/</guid><description>NaturalBench: a new benchmark exposes VLMs&amp;rsquo; vulnerabilities to natural adversarial samples, highlighting compositionality challenges &amp;amp; bias issues, and promoting dynamic VLM evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14669/cover.png"/></item><item><title>Teaching Models to Balance Resisting and Accepting Persuasion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14596/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14596/</guid><description>LLMs are taught to both resist harmful and accept helpful persuasion using Persuasion-Balanced Training, resulting in more reliable and collaborative AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14596/cover.png"/></item><item><title>ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13924/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13924/</guid><description>ARKit LabelMaker creates the largest real-world 3D dataset with dense semantic annotations, boosting performance of 3D semantic segmentation models and accelerating progress in indoor scene understand&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13924/cover.png"/></item><item><title>CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13218/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13218/</guid><description>CBT-BENCH: a new benchmark reveals LLMs&amp;rsquo; potential and limitations in assisting Cognitive Behavioral Therapy, highlighting the need for further research in AI-driven mental healthcare.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13218/cover.png"/></item><item><title>Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13394/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13394/</guid><description>New framework, CIA Suite, enables accurate, automated cross-lingual evaluation of multilingual LLMs using a novel test set and evaluator LLMs, bridging the gap in multilingual NLP assessment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13394/cover.png"/></item><item><title>DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13726/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13726/</guid><description>DAWN: a new framework for generating realistic talking head videos from a single image and audio, using a fast non-autoregressive diffusion model to overcome limitations of previous approaches.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13726/cover.png"/></item><item><title>Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13674/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13674/</guid><description>Boosting AI&amp;rsquo;s learning from limited or poor-quality data, this paper introduces DisCL, a novel curriculum learning method using image-guided diffusion models to generate diverse synthetic training dat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13674/cover.png"/></item><item><title>DPLM-2: A Multimodal Diffusion Protein Language Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13782/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13782/</guid><description>DPLM-2: A multimodal diffusion model revolutionizes protein structure &amp;amp; sequence generation, achieving superior accuracy and diversity via efficient training and structure tokenization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13782/cover.png"/></item><item><title>FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13925/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13925/</guid><description>FiTv2, an enhanced flexible vision transformer, achieves state-of-the-art image generation by dynamically processing images as sequences of tokens, overcoming resolution limitations of prior models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13925/cover.png"/></item><item><title>In-context learning and Occam's razor</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14086/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14086/</guid><description>In-context learning&amp;rsquo;s success is explained by its implicit minimization of both training error and model complexity, akin to Occam&amp;rsquo;s Razor, achieved through a data compression lens.</description></item><item><title>Looking Inward: Language Models Can Learn About Themselves by Introspection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13787/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13787/</guid><description>Language Models can learn about themselves through introspection, outperforming other models in self-prediction tasks, suggesting a form of internal self-awareness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13787/cover.png"/></item><item><title>MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13370/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13370/</guid><description>MagicTailor empowers text-to-image models with component-level control over personalized concepts, enabling fine-grained customization and high-quality image generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13370/cover.png"/></item><item><title>MedINST: Meta Dataset of Biomedical Instructions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13458/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13458/</guid><description>MEDINST, a novel biomedical instruction meta-dataset with 133 tasks and 7M samples, significantly improves LLMs&amp;rsquo; cross-task generalization in medical analysis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13458/cover.png"/></item><item><title>PUMA: Empowering Unified MLLM with Multi-granular Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13861/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13861/</guid><description>PUMA: a unified multi-granular MLLM excels at diverse visual tasks by seamlessly integrating image generation and understanding, addressing varying granularity demands.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13861/cover.png"/></item><item><title>Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13184/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13184/</guid><description>Router-Tuning and MindSkip boost Transformer efficiency by dynamically adjusting computation depth, achieving 21% speedup with minimal performance loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13184/cover.png"/></item><item><title>SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13276/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13276/</guid><description>SeerAttention learns intrinsic attention sparsity, achieving significant speedups in LLMs without sacrificing accuracy, via a novel learnable gating mechanism and customized FlashAttention.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13276/cover.png"/></item><item><title>SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14745/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14745/</guid><description>SEMIEVOL, a novel semi-supervised framework, significantly improves large language model adaptation by effectively leveraging both limited labeled and abundant unlabeled data, achieving superior perfo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14745/cover.png"/></item><item><title>Steering Your Generalists: Improving Robotic Foundation Models via Value Guidance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13816/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13816/</guid><description>Boosting robot performance at deployment time, Value-Guided Policy Steering (V-GPS) re-ranks actions from existing policies using a value function learned via offline RL, consistently improving perfor&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13816/cover.png"/></item><item><title>UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14059/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14059/</guid><description>UCFE benchmark realistically evaluates LLMs&amp;rsquo; financial expertise via user-centric design and dynamic interactions, revealing performance gaps and highlighting human-preference alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14059/cover.png"/></item><item><title>Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13232/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13232/</guid><description>Boosting LLM-based web agents: This work introduces world models, improving efficiency and cost in web navigation by simulating action outcomes before execution.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13232/cover.png"/></item><item><title>Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/</guid><description>KeyNMF, a novel topic modeling approach, effectively analyzes information dynamics in Chinese diaspora media, revealing the PRC&amp;rsquo;s potential influence on European elections.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/cover.png"/></item><item><title>Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12788/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12788/</guid><description>Meta-Chunking boosts RAG performance by intelligently segmenting text into logically coherent chunks, improving knowledge retrieval and question answering.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12788/cover.png"/></item><item><title>Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11190/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11190/</guid><description>Mini-Omni2 is an open-source, multi-modal language model closely replicating GPT-40&amp;rsquo;s vision, speech, and duplex capabilities, trained efficiently on a limited dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11190/cover.png"/></item><item><title>SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11331/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11331/</guid><description>Shakti, a 2.5B parameter language model, achieves high performance on edge devices using innovative techniques like VGQA and SwiGLU, outperforming larger models in several benchmarks.</description></item><item><title>HART: Efficient Visual Generation with Hybrid Autoregressive Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10812/</link><pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10812/</guid><description>HART: A hybrid autoregressive transformer achieves state-of-the-art image generation quality at significantly higher speeds than diffusion models, thanks to its innovative hybrid tokenizer and residua&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10812/cover.png"/></item></channel></rss>