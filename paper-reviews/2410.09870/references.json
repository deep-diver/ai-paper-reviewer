{"references": [{" publication_date": "2024", "fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "reason": "This paper introduces a highly capable language model, which is relevant to the overall discussion on advancements in LLMs and their potential to handle chronological knowledge.  Understanding its capabilities helps contextualize the challenges and opportunities in improving temporal reasoning in LLMs.", "section_number": 4}, {" publication_date": "2004", "fullname_first_author": "Oliver Bodenreider", "paper_title": "The unified medical language system (umls): integrating biomedical terminology", "reason": "This paper describes the Unified Medical Language System (UMLS), a crucial resource used in the biomedical domain of the paper's benchmark dataset.  Understanding the UMLS is essential for evaluating the study's findings on biomedical temporal knowledge representation in LLMs.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Hoyeon Chang", "paper_title": "How do large language models acquire factual knowledge during pretraining?", "reason": "This paper explores how LLMs acquire factual knowledge during pretraining, which directly relates to the central theme of evaluating and improving their chronological knowledge. Its findings are relevant to understanding the origins of temporal biases in LLMs and informing strategies for enhancement.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Wenhu Chen", "paper_title": "A dataset for answering time-sensitive questions", "reason": "The paper's introduction of a dataset for answering time-sensitive questions is directly relevant to the research problem of this paper, which focuses on evaluating and improving the ability of LLMs to handle chronological knowledge. Understanding this dataset helps contextualize the paper's benchmark dataset.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Damai Dai", "paper_title": "Knowledge neurons in pretrained transformers", "reason": "This work investigates knowledge neurons in pretrained transformers, providing valuable insights into the internal representations of knowledge within LLMs. This understanding is crucial for evaluating and improving LLMs' chronological knowledge capabilities.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Tim Dettmers", "paper_title": "Qlora: Efficient finetuning of quantized Ilms", "reason": "The paper's focus on efficient fine-tuning of quantized LLMs is relevant to the broader context of updating and enhancing LLMs' knowledge, especially regarding their temporal capabilities.  Understanding efficient fine-tuning techniques is crucial for improving the chronological accuracy of LLMs.", "section_number": 8}, {" publication_date": "2022", "fullname_first_author": "Bhuwan Dhingra", "paper_title": "Time-aware language models as temporal knowledge bases", "reason": "This paper directly addresses the topic of time-aware language models and their ability to function as temporal knowledge bases.  It provides a relevant benchmark for evaluating the performance of the proposed LLM evaluation framework.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces a new set of LLMs, which are directly used for comparison in the experimental setup of this paper.  Understanding the characteristics of these models, including their training data and architecture, is essential for interpreting and evaluating the results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Bahare Fatemi", "paper_title": "Test of time: A benchmark for evaluating Ilms on temporal reasoning", "reason": "This paper introduces a benchmark for evaluating LLMs' temporal reasoning capabilities. This benchmark offers a valuable comparison for evaluating the performance of the proposed framework, allowing for a relative assessment of its effectiveness.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Xiou Ge", "paper_title": "Time sensitive knowledge editing through efficient finetuning", "reason": "This paper directly tackles the problem of updating LLMs' knowledge, particularly focusing on time-sensitive knowledge. This is highly relevant to the paper's focus on improving the chronological accuracy of LLMs and provides a valuable point of comparison for evaluating the proposed method.", "section_number": 8}, {" publication_date": "2024", "fullname_first_author": "Zorik Gekhman", "paper_title": "Does fine-tuning llms on new knowledge encourage hallucinations?", "reason": "This paper explores the impact of fine-tuning LLMs on new knowledge, a process that is directly relevant to the proposed method of enhancing LLMs' chronological knowledge. It helps contextualize the challenges and potential pitfalls of knowledge update strategies.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Mor Geva", "paper_title": "Dissecting recall of factual associations in auto-regressive language models", "reason": "This paper investigates the recall of factual associations in auto-regressive language models, which is directly relevant to the assessment of LLMs' ability to recall and represent chronological information. Understanding the mechanisms of factual recall is essential for developing effective strategies for improving the temporal accuracy of LLMs.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Gaurav Rohit Ghosal", "paper_title": "Understanding finetuning for factual knowledge extraction", "reason": "This paper investigates fine-tuning for factual knowledge extraction, which is a relevant technique for updating LLMs' knowledge.  Understanding fine-tuning techniques helps to contextualize the paper's non-parametric approach, particularly in the context of addressing the limitations of parametric knowledge update strategies.", "section_number": 8}, {" publication_date": "2024", "fullname_first_author": "Daniela Gottesman", "paper_title": "Estimating knowledge in large language models without generating a single token", "reason": "This paper presents a novel method for estimating knowledge in large language models without generating text.  The method's focus on efficient knowledge assessment is relevant to the paper's aim of developing an efficient and effective framework for evaluating and updating LLMs' chronological knowledge.", "section_number": 8}, {" publication_date": "2017", "fullname_first_author": "Gus Hahn-Powell", "paper_title": "Swanson linking revisited: Accelerating literature-based discovery across domains using a conceptual influence graph", "reason": "This paper introduces a method for accelerating literature-based discovery, which is particularly relevant to the biomedical domain of the study's benchmark dataset.  Understanding the methods for integrating and analyzing biomedical literature is crucial for interpreting the results of the LLM evaluation.", "section_number": 8}, {" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "reason": "This work establishes a benchmark for measuring massive multitask language understanding, which is highly relevant to the evaluation of LLMs' abilities to process and reason about temporal information.  It provides a framework for evaluating the model's performance in relation to its ability to handle various tasks related to time.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "John Hewitt", "paper_title": "A structural probe for finding syntax in word representations", "reason": "This paper introduces a structural probe for finding syntax in word representations, a technique that is relevant to the overall discussion on how LLMs process and represent information.  Understanding this technique helps to contextualize the paper's approach to evaluating and improving LLMs' chronological knowledge.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Filip Ilievski", "paper_title": "Cskg: The commonsense knowledge graph", "reason": "This paper introduces the Commonsense Knowledge Graph (CSKG), a key resource used in the paper's benchmark dataset.  Understanding the CSKG is crucial for interpreting the results of the LLM evaluation, particularly in the commonsense domain.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Joel Jang", "paper_title": "Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models", "reason": "This paper introduces a benchmark dataset focusing on evaluating ever-evolving language models.  This is directly relevant to the paper's focus on assessing and improving LLMs' chronological knowledge, providing a valuable comparison point for evaluating the proposed framework.", "section_number": 1}]}