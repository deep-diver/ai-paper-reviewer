[{"figure_path": "https://arxiv.org/html/2412.02114/x2.png", "caption": "Figure 1: \nOmniCreator enables universal text-guided video editing across four distinct editing types (top) and eight different scenarios (bottom). For comprehensive definitions of the editing types and scenarios, please refer to App.\u00a0A.", "description": "Figure 1 shows the capabilities of OmniCreator in performing universal text-guided video editing. The top row displays four main editing types: foreground editing, background editing, composite editing, and overall/style editing.  The bottom part of the figure illustrates eight diverse editing scenarios that OmniCreator can handle. These scenarios are categorized into four main groups: object, environment, human/animal, and composite.  Each scenario contains examples of video edits guided by specific text prompts. Appendix A provides a detailed explanation of the editing types and scenarios.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.02114/x3.png", "caption": "Figure 2: \nOmniCreator samples for text-to-video generation. OmniCreator not only enables universal video editing but also generates high-quality videos from text prompts.", "description": "Figure 2 showcases example videos generated by OmniCreator using only text prompts as input, demonstrating its capacity for high-quality text-to-video generation.  This highlights OmniCreator's ability to function not just as a video editor, but also as a generative model for creating novel video content.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.02114/x4.png", "caption": "Figure 3: \nOverview of OmniCreator. During training, the original video also serves as a condition of the denoising process. To enable temporal information understanding, we incorporate an adapter. Additionally, we utilize a query transformer to effectively align video and text embeddings, which aids in the denoising process. For computational efficiency, LoRAs are integrated into the denoising U-Net. During inference, OmniCreator enables universal video editing by adopting a reference video alongside an editing text prompt.", "description": "OmniCreator uses a self-supervised training approach where the original video is part of the denoising process.  An adapter module helps the model understand temporal information in the video, and a query transformer aligns video and text embeddings for better denoising.  Low-rank adaptations (LoRAs) are used to make the U-Net more efficient. During inference, the model takes a reference video and an editing text prompt to perform universal video editing.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.02114/x5.png", "caption": "Figure 4: \nIllustration of the alignment between video and text embeddings. We utilize Euclidean Distance (left, blue) and Cosine Similarity (right, red) to evaluate the impact of the Adapter (Ada.) and Query Transformer (Query) on the embedding alignment.", "description": "Figure 4 demonstrates the effectiveness of the adapter and query transformer modules in aligning video and text embeddings.  The left panel shows Euclidean distance, while the right panel shows cosine similarity.  Each bar represents a different experimental condition: with adapter and query transformer, with adapter only, with query transformer only, and without either. The results illustrate the improved alignment achieved when both modules are used, demonstrating their contribution to effective multimodal fusion.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.02114/x6.png", "caption": "Figure 5: \nIllustration of semantic correspondence. Top: Reference video and its original caption. Middle: Results using only one condition. Bottom: Effects of full sentence vs. delta prompt.", "description": "Figure 5 illustrates the concept of semantic correspondence in the OmniCreator framework. The top row displays a reference video and its original caption, establishing the ground truth.  The middle row shows the results of using only one condition (text or video) for generation, demonstrating the limitations of relying on a single input modality.  The bottom row compares the results of using a full-sentence prompt versus a delta prompt (only the editing instruction) for generation, highlighting the impact of specifying editing changes precisely.  This comparison reveals the ability of delta prompts to effectively combine textual and video semantics for better control during the video editing process.", "section": "3.2 OmniCreator"}, {"figure_path": "https://arxiv.org/html/2412.02114/x7.png", "caption": "Figure 6: \nStatistics of OmniBench-99.", "description": "The figure shows a statistical overview of the OmniBench-99 dataset.  It presents the distribution of videos across three categories (Human/Animal, Environment, Object) and the number of videos associated with each category.  Additionally, it illustrates the distribution of editing prompts (Full sentence prompt and Delta prompt), and shows how many videos have been annotated with each type of prompt.", "section": "4. OmniBench-99"}, {"figure_path": "https://arxiv.org/html/2412.02114/x8.png", "caption": "Figure 7: \nVideo editing comparison with baselines. We follow the baseline\u2019s prompt setting but only show delta prompts here. Due to space constraints, we only compare editing scenarios with TokenFlow and CCEdit, for complete comparisons, please refer to App.\u00a0G.2.", "description": "Figure 7 shows a comparison of video editing results between OmniCreator and two baseline models, TokenFlow and CCEdit.  The comparison focuses on various editing scenarios, using delta prompts (short, focused prompts specifying only the desired changes). Due to space limitations in the main paper, the figure only presents a subset of the editing scenarios.  A complete comparison across all scenarios is available in Appendix G.2 of the paper.", "section": "4. OmniBench-99"}, {"figure_path": "https://arxiv.org/html/2412.02114/x9.png", "caption": "Figure 8: \nImage editing comparison with baselines.", "description": "Figure 8 shows a qualitative comparison of image editing results between OmniCreator and several baseline methods.  Each row represents a different editing task (Addition, Replacement, Removal, Background, Style, Texture, Action).  Within each row, the leftmost column shows the original reference image. Subsequent columns display the results from the different models, illustrating their respective performance in terms of editing precision, visual consistency, and adherence to the provided instructions. This allows for a direct visual assessment of OmniCreator's capabilities in comparison to existing image editing techniques.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x10.png", "caption": "Figure 9: \nQualitative results of OmniCreator T2V samples.", "description": "Figure 9 presents several examples of high-quality videos generated by OmniCreator using only text prompts.  These examples showcase the model's ability to generate diverse and complex video scenes from various textual descriptions. The images illustrate the versatility and quality of the generated videos, highlighting the model's success in text-to-video generation.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x11.png", "caption": "Figure 10: \nQualitative results of OmniCreator T2I samples.", "description": "This figure showcases several examples of images generated by the OmniCreator model from text prompts only.  It demonstrates the model's ability to create high-quality and diverse images from a wide range of text descriptions, highlighting its capacity for detailed and photorealistic generation. The diverse styles and subject matters across the images demonstrate the versatility of OmniCreator in text-to-image generation.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x12.png", "caption": "Figure 11: \nAblation on video condition modeling. Ada. indicates the adapter, and Query is the query transformer.", "description": "This ablation study investigates the impact of the adapter and query transformer on video condition modeling within the OmniCreator framework.  The figure showcases how different components of the model affect the ability to accurately reconstruct the reference video, particularly focusing on the background elements.  The results demonstrate the significant contributions of both the adapter (which enables the model to understand temporal dynamics) and the query transformer (which facilitates alignment between text and video embeddings). The absence of either component impairs the quality of the reconstructed video, while including both leads to superior performance.", "section": "B. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.02114/x13.png", "caption": "Figure 12: \nAblation on LoRA ranks. LoRA with different ranks exhibits different learning comprehension abilities", "description": "This ablation study investigates the impact of different LoRA (Low-Rank Adaptation) ranks on the model's ability to learn and understand semantic relationships between text and video.  The results demonstrate that the performance of the model is affected by the choice of LoRA rank, suggesting there is an optimal rank for balancing performance and computational efficiency.  Using excessively low or high ranks hinders the model's ability to capture semantic correlations adequately.", "section": "B. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.02114/x14.png", "caption": "Figure 13: \nAblation on multimodal guidance scales. wtxtsubscript\ud835\udc64txtw_{\\mathrm{txt}}italic_w start_POSTSUBSCRIPT roman_txt end_POSTSUBSCRIPT controls consistency with the edit instruction, while wvidsubscript\ud835\udc64vidw_{\\mathrm{vid}}italic_w start_POSTSUBSCRIPT roman_vid end_POSTSUBSCRIPT controls the similarity with reference video.", "description": "This ablation study investigates the effects of two scaling factors,  \ud835\udc64\ud835\udc61\ud835\udc65\ud835\udc61<binary data, 1 bytes><binary data, 1 bytes><binary data, 1 bytes> and \ud835\udc64\ud835\udc63\ud835\udc56\ud835\udc51<binary data, 1 bytes><binary data, 1 bytes><binary data, 1 bytes>, on the performance of the OmniCreator model. The scaling factor \ud835\udc64\ud835\udc61\ud835\udc65\ud835\udc61<binary data, 1 bytes><binary data, 1 bytes><binary data, 1 bytes> controls how closely the generated output adheres to the edit instruction provided in the text prompt, while \ud835\udc64\ud835\udc63\ud835\udc56\ud835\udc51<binary data, 1 bytes><binary data, 1 bytes><binary data, 1 bytes> controls how similar the output is to the reference video. The figure shows that appropriate adjustment of these scaling factors is crucial for achieving optimal results in various editing scenarios.", "section": "B. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.02114/x15.png", "caption": "Figure 14: \nText Prompt Demonstration.", "description": "This figure demonstrates the process of generating text prompts for the OmniBench-99 dataset.  It shows an example of how a center frame from a video is used, along with the video's category (Human/Animal, Object, or Environment), to generate a main description of the frame and then prompts designed for different editing types (Foreground, Background, Style, Composite) and editing scenarios specific to the category. It also shows how delta prompts are created, specifying only the changes to be made.", "section": "D. OmniBench-99"}, {"figure_path": "https://arxiv.org/html/2412.02114/x16.png", "caption": "Figure 15: \nImage editing comparison.", "description": "Figure 15 presents a qualitative comparison of image editing results achieved using various methods.  It showcases the effects of different approaches on several editing scenarios, such as adding, replacing, and removing objects; altering the background and textures; and changing the overall style or even actions within the image. This visual comparison allows for a direct assessment of each method's ability to produce edits that are visually pleasing, contextually appropriate, and realistic in terms of maintaining coherence and integrity.", "section": "5.2. Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2412.02114/x17.png", "caption": "Figure 16: \nExamples of OmniBench-99 benchmark. The delta prompts are highlighted in yellow-brown.", "description": "Figure 16 shows examples from the OmniBench-99 benchmark dataset.  The dataset contains diverse videos categorized into three groups: Human/Animal, Object, and Environment. For each video, multiple prompts are provided to cover a range of editing types (Foreground, Background, Style, and Composite) and editing scenarios (Appearance, Motion/Pose, Addition, Removal, Replacement, Weather, Time, Background). The figure highlights the 'delta prompts'\u2014short, specific instructions focusing on localized edits\u2014in yellow-brown to differentiate them from the full-sentence prompts.", "section": "4. OmniBench-99"}, {"figure_path": "https://arxiv.org/html/2412.02114/x18.png", "caption": "Figure 17: \nDemonstration of our user study interface. Here we demonstrate one complete sample.", "description": "This figure shows the user interface used for the user study.  The interface is designed to allow users to evaluate video generation and editing results across various metrics such as Text Alignment, Temporal Consistency, Structure Alignment (for editing only), and Overall Quality. Each metric is rated on a 5-point scale, and users provide scores for several randomly selected videos for each prompt condition.  A complete example of the interface is displayed, showing a user's evaluation of video clips with a specific prompt.", "section": "E.2 User Study"}, {"figure_path": "https://arxiv.org/html/2412.02114/x19.png", "caption": "Figure 18: \nGallery of OmniCreator\u2019s text-to-image generation results.", "description": "This figure showcases a gallery of images generated by the OmniCreator model using only text prompts.  The results demonstrate the model's ability to generate diverse and visually compelling images based on varied textual descriptions, ranging from natural landscapes and underwater scenes to abstract styles and fantastical settings. The images highlight OmniCreator's capacity for detailed rendering, accurate object representation, and adherence to the specified prompt's semantics.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x20.png", "caption": "Figure 19: \nGallery of OmniCreator\u2019s text-to-video generation results.", "description": "This figure showcases various video clips generated by the OmniCreator model using only text prompts as input.  Each video clip demonstrates the model's ability to generate diverse and coherent video content from textual descriptions, highlighting its capabilities in text-to-video generation. The videos depict different scenes and styles, showcasing the range of content the model can produce.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x21.png", "caption": "Figure 20: \nGallery of OmniCreator\u2019s text-to-video generation results.", "description": "This figure showcases various video clips generated by the OmniCreator model using only text prompts.  Each row represents a different text prompt, demonstrating the model's ability to generate diverse and coherent videos based on textual descriptions. The videos illustrate different scenes and styles, highlighting OmniCreator's capacity for generating high-quality video content from text prompts alone.", "section": "5 Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x22.png", "caption": "Figure 21: \nVideo editing comparison: Editing-Type-Foreground.", "description": "This figure compares the results of different video editing models on the \"Foreground\" editing type.  The editing task focuses on modifying only the foreground of a video while leaving the background unchanged.  The reference video is shown at the top, followed by the output of various models, including Tune-A-Video, Pix2Video, ControlVideo, TokenFlow, InsV2V, Video-P2P, CCEdit, and OmniCreator (the authors' model).  The figure allows for a visual comparison of the effectiveness of each model in preserving the background and accurately editing only the foreground as instructed by the prompt.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x23.png", "caption": "Figure 22: \nVideo editing comparison: Editing-Type-Background.", "description": "This figure compares the performance of different video editing models in modifying the background of a video.  The models are given the same source video and a text prompt instructing a change to the background. The figure allows a visual comparison of the results from each model, showing how accurately and effectively each model changed the background while maintaining the rest of the video. This helps to evaluate the models' ability to perform text-guided background editing.", "section": "5 Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x24.png", "caption": "Figure 23: \nVideo editing comparison: Editing-Type-Style.", "description": "This figure displays a comparison of video editing results across different models for the 'Style' editing type.  The reference video is shown at the top, followed by the results from several other models, including Tune-A-Video, Pix2Video, ControlVideo, TokenFlow, InsV2V, Video-P2P, CCEdit, and finally OmniCreator (the authors' model). Each row shows a series of frames from the edited video, allowing for visual comparison of the different models' capabilities in modifying the style or overall aesthetic of the video according to the prompt, which in this case is \"Oil painting style.\"", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x25.png", "caption": "Figure 24: \nVideo editing comparison: Editing-Type-Composite.", "description": "This figure compares the performance of different video editing models on composite video editing tasks. Composite editing involves making multiple changes to a video, such as altering both the foreground and background simultaneously.  The figure shows the results of each model's attempt to edit a video according to a specific composite editing prompt. By visually comparing the results, one can assess the strengths and weaknesses of each model in terms of maintaining visual consistency, accurately implementing the changes specified in the prompt, and achieving a high-quality, coherent output.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x26.png", "caption": "Figure 25: \nVideo editing comparison: Editing-Scenario-Environment-Weather.", "description": "This figure compares the performance of different video editing models on the specific task of weather editing within the environment scenario.  The original video shows a particular weather condition. Each row in the figure represents a different video editing model, demonstrating their ability to modify the original video's weather based on a text prompt (e.g., changing clear skies to stormy skies with rain and mist). The comparison allows for a visual assessment of each model's capability to accurately and realistically alter the weather conditions in a video while maintaining consistency with the original visual context.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x27.png", "caption": "Figure 26: \nVideo editing comparison: Editing-Scenario-Environment-Time.", "description": "This figure shows a comparison of different video editing models' performance on the \"Environment-Time\" editing scenario.  The goal is to change the time of day depicted in a video.  The reference video is shown alongside the results from several models, including Tune-A-Video, Pix2Video, ControlVideo, TokenFlow, InsV2V, Video-P2P, CCEdit and OmniCreator. The comparison highlights the ability of each model to accurately reflect the specified time change (e.g., sunset) while preserving the overall visual style and integrity of the video.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x28.png", "caption": "Figure 27: \nVideo editing comparison: Editing-Scenario-Environment-Background.", "description": "This figure compares the performance of different video editing models on a specific editing scenario: changing the background of an environmental video.  The reference video shows an initial scene, and each row demonstrates how different models (Tune-A-Video, Pix2Video, ControlVideo, TokenFlow, InsV2V, Video-P2P, CCEdit) modify the background according to the same textual prompt, with the last row showing OmniCreator's results.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x29.png", "caption": "Figure 28: \nVideo editing comparison: Editing-Scenario-Object-Addition.", "description": "This figure compares the results of several video editing models on the task of adding objects to a video.  The models are presented with a video and a text prompt instructing them to add an object (in this case, multiple bees).  Each row shows the results from a different model, including the reference video and the outputs generated by Tune-A-Video, Pix2Video, ControlVideo, TokenFlow, InsV2V, Video-P2P, CCEdit, and OmniCreator. This allows for a visual comparison of each model's ability to accurately and realistically integrate the new objects into the scene while maintaining consistency with the overall video style and structure. The differences in the results highlight the varying capabilities of each model in terms of object placement, integration, and preserving context.", "section": "5 Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x30.png", "caption": "Figure 29: \nVideo editing comparison: Editing-Scenario-Object-Removal.", "description": "This figure compares the results of various video editing models on the task of object removal within a specific scenario. The reference video shows an object, and the goal is to edit the video to remove the object, leaving the rest of the video intact. Each row shows the results from a different video editing model, showcasing their ability to remove the target object cleanly and effectively while preserving the rest of the video's background and context.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x31.png", "caption": "Figure 30: \nVideo editing comparison: Editing-Scenario-Object-Replacement.", "description": "This figure shows a comparison of different video editing models' performance on the task of object replacement.  The models are given a video and a text prompt instructing them to replace a specific object with another. The figure visually demonstrates the differences in editing quality, accuracy, and visual coherence, enabling a qualitative assessment of the effectiveness of each model in executing this type of video manipulation.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.02114/x32.png", "caption": "Figure 31: \nVideo editing comparison: Editing-Scenario-Human-Appearance.", "description": "This figure shows a comparison of different video editing models' performance on the task of human appearance editing.  The models are given the same reference video and a text prompt instructing them to change the appearance of a person in the video (in this case, to make the person wear a blue coat). The figure displays frames from each model's output video, allowing for a visual comparison of the editing results. This allows for assessment of each model's ability to accurately reflect the prompt's instructions while maintaining overall video quality and coherence.", "section": "5. Experiment"}]