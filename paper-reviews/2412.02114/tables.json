[{"content": "| Method | Editing Type |  |  |  |  | Editing Scenario |  |  |  |  |  | \n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  | Automatic |  | User Study |  |  |  | Automatic |  | User Study |  |  |  | \n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  | CLIP Frame \u2191 | Pick Score \u2191 | Align. \u2191 | Temp. \u2191 | Stru. \u2191 | Overall \u2191 | CLIP Frame \u2191 | Pick Score \u2191 | Align. \u2191 | Temp. \u2191 | Stru. \u2191 | Overall \u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Tune-A-Video [114] _ICCV\u201923_ | 0.931 | 0.205 | 3.07 | 2.87 | 3.13 | 3.20 | 0.929 | 0.198 | 3.33 | 2.93 | 3.00 | 3.13 |\n| Pix2Video [8] _ICCV\u201923_ | 0.929 | 0.203 | 3.60 | 3.20 | 3.27 | 3.33 | 0.927 | 0.199 | 3.93 | 3.53 | 3.27 | 3.47 |\n| ControlVideo [132] _ICLR\u201924_ | 0.949 | 0.210 | 2.93 | 2.27 | 2.40 | 2.40 | 0.950 | 0.203 | 1.80 | 1.87 | 2.13 | 2.27 |\n| TokenFlow [30] _ICLR\u201924_ | 0.948 | 0.208 | 2.73 | 3.33 | 2.80 | 3.07 | 0.951 | 0.200 | 3.07 | 3.07 | 2.93 | 3.13 |\n| InsV2V [18] _ICLR\u201924_ | 0.914 | 0.208 | 2.13 | 2.20 | 2.33 | 2.47 | 0.911 | 0.198 | 1.73 | 1.93 | 1.87 | 2.00 |\n| Video-P2P [68] _CVPR\u201924_ | 0.930 | 0.198 | 3.13 | 3.27 | 3.20 | 3.00 | 0.928 | 0.189 | 3.13 | 3.20 | 3.13 | 3.07 |\n| CCEdit [26] _CVPR\u201924_ | 0.932 | 0.210 | 1.73 | 2.53 | 2.27 | 2.20 | 0.935 | 0.204 | 1.53 | 2.53 | 2.20 | 2.20 |\n| OmniCreator (Ours) | 0.962 | 0.212 | 4.47 | 4.33 | 4.07 | 4.33 | 0.966 | 0.216 | 4.07 | 4.13 | 4.20 | 4.00 |", "caption": "Table 1: Quantitative comparison with text-guided video editing methods on our OmniBench-99. User study includes text alignment (Align.), temporal consistency (Temp.), structure alignment (Stru.), and overall quality (Overall). We also conduct a quantitative evaluation on the BalanceCC\u00a0[26] and LOVEU-TGVE-2023 [115] benchmarks in App.\u00a0G.1.", "description": "Table 1 presents a quantitative comparison of various text-guided video editing methods, evaluated using the OmniBench-99 benchmark.  The evaluation includes both automated metrics (CLIP Frame and PickScore) and a user study assessing text alignment, temporal consistency, structural alignment (for video-specific methods only), and overall quality.  For a more comprehensive comparison, additional quantitative results using the BalanceCC [26] and LOVEU-TGVE-2023 [115] benchmarks are provided in Appendix G.1.", "section": "5. Experiment"}, {"content": "| Method | Addition |  | Replacement |  | Removal |  | Background |  | Style |  | Texture |  | Action |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Null-Text [74] _CVPR\u201923_ | - | - | 8.15 | 0.80 | - | - | 7.28 | 1.17 | 7.16 | 0.90 | 6.82 | 1.44 | - | - |\n| Disentangle [116] _CVPR\u201923_ | 6.14 | 1.74 | 7.66 | 1.41 | - | - | - | - | - | - | 6.78 | 1.07 | - | - |\n| InstructPix2Pix [7] _CVPR\u201923_ | 6.88 | 2.31 | 5.00 | 1.95 | - | - | 6.51 | 2.49 | 8.21 | 0.40 | 6.10 | 1.41 | - | - |\n| Imagic [58] _CVPR\u201923_ | 7.80 | 1.27 | 7.22 | 1.65 | - | - | - | - | 7.26 | 1.25 | 5.57 | 1.49 | 6.97 | 0.80 |\n| ProxEdit [32] _WACV\u201924_ | 7.06 | 1.53 | 7.53 | 1.63 | 7.75 | 1.26 | 6.35 | 0.78 | 6.80 | 1.07 | - | - | - | - |\n| LEDITS++ [6] _CVPR\u201924_ | 6.74 | 1.72 | 7.41 | 1.86 | 8.65 | 1.29 | 6.91 | 0.97 | 6.86 | 1.20 | - | - | - | - |\n| InstructDiffusion [29] _CVPR\u201924_ | 7.59 | 1.89 | 6.55 | 1.46 | 7.48 | 1.68 | - | - | 7.41 | 0.66 | 7.13 | 1.83 | - | - |\n| OmniCreator (Ours) | 7.63 | 1.79 | 8.49 | 0.96 | 8.33 | 1.01 | 7.40 | 0.81 | 8.22 | 0.45 | 6.99 | 1.03 | 7.53 | 1.11 |", "caption": "Table 2: Quantitative comparison with text-guided image editing methods. Details regarding metrics are available in App.\u00a0E.3.", "description": "Table 2 presents a quantitative comparison of OmniCreator against other state-of-the-art text-guided image editing methods.  The table evaluates performance across several image editing tasks, such as addition, removal, replacement, background change, style transfer, texture alteration, and action modification.  Specific metrics used in the evaluation are detailed in Appendix E.3 of the paper.", "section": "5. Experiment"}, {"content": "| Method | Automatic |  | User Study |  |  | \n|---|---|---|---|---|---| \n| **Method** | **FVD \u2193** | **CLIPSIM \u2191** | **Align. \u2191** | **Temp. \u2191** | **Overall \u2191** |  | \n| LaVie [111] _arXiv\u201923_ | 526.30 | 0.2949 | 3.93 | 3.20 | 3.33 |  | \n| CogVideo (En) [43] _ICLR\u201923_ | 701.59 | 0.2631 | 1.87 | 1.87 | 2.00 |  | \n| CogVideoX 5B [127] _arXiv\u201924_ | - | - | 4.07 | 4.07 | 4.13 |  | \n| HiGen [87] _CVPR\u201924_ | 497.21 | 0.2947 | 3.93 | 2.87 | 3.20 |  | \n| OmniCreator (Ours) | **332.42** | **0.3102** | **4.20** | **4.07** | **4.13** |  | ", "caption": "Table 3: Quantitative comparison with T2V generation models using FVD on UCF-101 and CLIPSIM on MSR-VTT.", "description": "This table presents a quantitative comparison of various text-to-video (T2V) generation models.  The comparison uses two metrics: Fr\u00e9chet Video Distance (FVD), calculated on the UCF-101 dataset, and CLIP Similarity (CLIPSIM), computed using the MSR-VTT dataset.  These metrics evaluate the quality and fidelity of the generated videos compared to ground truth.  The table helps assess the performance of different T2V models in generating high-quality videos faithful to the given text prompts.", "section": "5. Experiment"}, {"content": "| Method | Attribute Binding |  |  | Object Relationship |  | \n|---|---|---|---|---|---|---|\n|  | Color \u2191 | Shape \u2191 | Texture \u2191 | Spatial \u2191 | Non-Spatial \u2191 |  |\n| Stable v2 [91] _CVPR\u201922_ | 0.5065 | 0.4221 | 0.4922 | 0.1342 | 0.3096 |  |\n| GORS [47] _NeurIPS\u201923_ | 0.6603 | 0.4785 | 0.6287 | 0.1815 | 0.3193 |  |\n| SDXL [84] _ICLR\u201924_ | 0.6369 | 0.5408 | 0.5637 | 0.2032 | 0.3110 |  |\n| PixArt-\u03b1 [14] _CVPR\u201924_ | 0.6886 | 0.5582 | 0.7044 | 0.2082 | 0.3179 |  |\n| OmniCreator (Ours) | 0.6792 | 0.5621 | 0.7103 | 0.2039 | 0.3290 |  |", "caption": "Table 4: Quantitative comparison with T2I generation models on T2I-CompBench.", "description": "This table presents a quantitative comparison of OmniCreator's text-to-image (T2I) generation capabilities against other state-of-the-art models using the T2I-CompBench benchmark.  The benchmark assesses various aspects of image generation quality, focusing particularly on the model's ability to correctly represent different attributes of objects and their relationships. The table likely shows scores (such as precision, recall, or F1-scores) for each model across different attributes such as color, shape, texture, spatial relationships, and non-spatial relationships. This allows for a detailed comparison of how well different models perform on various aspects of image generation complexity.", "section": "5. Experiment"}, {"content": "Method|Tune|I2I|Additional Control|Editing Type|Editing Scenario\n---|---|---|---|---|---\n|Human/Animal|Object|Environment|\n---|---|---|---|---|\nCond.|Feat.|DDIM|Fore.|Back.|Comp.|Overall|App.|Mo./Pose|Add.|Rem.|Rep.|Wea.|Time|Back.|\n---|---|---|---|---|---|---|---|---|---|---|---|---|---|---\nVideo-Video Pair-based||||||||||||||||\nInstuctVid2Vid [86] _ICME\u201924_||||||\u2713|\u2713||||||||\u2713\nInsV2V [18] _ICLR\u201924_||||||\u2713|\u2713|\u2713|\u2713||||||||\u2713\nText-Video Pair-based||||||||||||||||\nOnly Style/Overall Editing||||||||||||||||\nControl-A-Video [17] _arXiv\u201923_||||C/H/D||||||||\u2713||||||||\nVideo ControlNet [19] _arXiv\u201923_||||O/D||||||||\u2713||||||||\nVideoControlNet [45] _arXiv\u201923_||||O+Ma/D/C||||||||\u2713||||||||\nDreamix [75] _arXiv\u201923_||\u2713||||||||\u2713||||||||\nVid2Vid-Zero [109] _arXiv\u201923_||||||\u2713|\u2713||||||||\u2713||||||||\nFate-Zero [85] _ICCV\u201923_||||||\u2713|\u2713||||||||\u2713||||||||\nPix2Video [8] _ICCV\u201923_||\u2713||||||||\u2713||||||||\nEI<sup>2</sup> [130] _NeurIPS\u201923_||\u2713||||||||\u2713||||||||\nRAV [125] _SIGGRAPH Asia\u201923_||||E+O||||||||\u2713||||||||\nMotionClone [67] _arXiv\u201924_||||||\u2713||||||||\u2713||||||||\nMake-Your-Video [120] _TVCG\u201924_||||D||||||||\u2713||||||||\nFLATTEN [20] _ICLR\u201924_||||O|\u2713|\u2713||||||||\u2713||||||||\nFollow-Your-Pose [72] _AAAI\u201924_||||P||||||||\u2713||||||||\nFreSCo [126] _CVPR\u201924_||||S||||||||\u2713||||||||\nFlowVid [65] _CVPR\u201924_||||O+D|\u2713||||||||\u2713||||||||\nRAVE [56] _CVPR\u201924_||||D|\u2713||||||||\u2713||||||||\nCoDeF [80] _CVPR\u201924_||\u2713||||||||\u2713||||||||\nVMC [54] _CVPR\u201924_||\u2713|Mo||||||||\u2713||||||||\nSimDA [121] _CVPR\u201924_||||||||\u2713||||||||\u2713||||||||\nLAMP [117] _CVPR\u201924_||\u2713|C||||||||\u2713||||||||\nCusAV [90] _ECCV\u201924_||\u2713||||||||\u2713||||||||\nMotionDirector [133] _ECCV\u201924_||\u2713||||||||\u2713||||||||\nNeRCan [16] _NeurIPS\u201924_||\u2713||||||||\u2713||||||||\nDiverse Editing||||||||||||||||\nText2LIVE [2] _ECCV\u201922_|||A||||||||\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nMoCA [123] _arXiv\u201923_|||O|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nDiffusionAtlas [10] _arXiv\u201923_||\u2713|A|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nMake-A-Prota. [134] _arXiv\u201923_||D+Ma|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nMagicEdit [66] _arXiv\u201923_|||D/P|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nVidEdit [22] _TMLR\u201923_|||A+H+Ma|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nSTL [63] _CVPR\u201923_|||A+Ma|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nT2V-Zero [59] _ICCV\u201923_|||Ma|\u2713|\u2713|\u2713||||||||\u2713\nTune-A-Video [114] _ICCV\u201923_||\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nGen-1 [23] _ICCV\u201923_|||D+Ma||||||||\u2713|\u2713\nStableVideo [9] _ICCV\u201923_|||A+D+C|\u2713|\u2713|\u2713||||||||\u2713|\u2713\nVideoComposer [110] _NeurIPS\u201923_|||D/S/Ma/Mo/SI|\u2713|\u2713||||||||\u2713|\u2713|\u2713\nUniEdit [1] _arXiv\u201924_||||||||\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nAnyV2V [61] _arXiv\u201924_||||||||\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nEdit-A-Video [97] _ACML\u201924_||\u2713||||||||\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nTokenFlow [30] _ICLR\u201924_||||||||\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nGround-A-Video [52] _ICLR\u201924_|||D+O+B|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nControlVideo [132] _ICLR\u201924_|||C/H/D/P|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nCCEdit [26] _CVPR\u201924_|||D/S|\u2713|\u2713|\u2713||||||||\u2713|\u2713\nFairy [113] _CVPR\u201924_||||||||\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nVideo-P2P [68] _CVPR\u201924_||||||||\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713\nMotionEditor [104] _CVPR\u201924_|||P+Ma|\u2713||||||||\u2713\nOmniCreator (Ours)||||||||\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713", "caption": "Table 5: Editing Capability Overview. \u201cTune\u201d: One-shot or few-shot tuning-based; \u201cI2I\u201d: Image editing model (e.g., InstructPix2Pix\u00a0[7]) assisted; \u201cAdditional Control\u201d: \u2018Cond.\u2019: Condition={Ma=Mask, Mo=Motion vector, E=Edge, O=Optical flow, C=Canny, H=HED boundary, D=Depth, P=Pose, S=Sketch, SI=Style Image, B=Bounding box, A=Atlas}, \u2018Feat.\u2019: Attention feature injection during inference, \u2018DDIM\u2019: DDIM inversion-assisted;\n\u201cEditing Type\u201d: \u2018Fore.\u2019: Foreground, \u2018Back.\u2019: Background, \u2018Comp.\u2019: Composite, \u2018Overall\u2019: only for overall editing, e.g., style;\n\u201cEditing Scenario\u201d: \u2018App.\u2019: Appearance, \u2018Mo.\u2019: Motion, \u2018Add.\u2019: Addition, \u2018Rem.\u2019: Removal, \u2018Rep.\u2019: Replacement, \u2018Wea.\u2019: Weather, \u2018Back.\u2019: Background. Note: since many methods are not open source, we only evaluate this type of model through the results shown in its paper/page.", "description": "This table provides a comprehensive overview of various text-guided video editing methods, categorized by their approaches and capabilities. It examines the techniques used (one-shot/few-shot tuning, image-to-image methods), additional control mechanisms (condition-based, attention features, DDIM inversion), editing types (foreground, background, composite, overall), and editing scenarios (appearance, motion, addition, removal, replacement, weather, background).  The table highlights the diversity of techniques, showing which methods employ each approach. It's important to note that due to limited open-source availability for some methods, evaluation was based on results reported in the respective papers.  The table is a valuable resource for understanding the capabilities and limitations of different video editing techniques.", "section": "A Editing Capabilities Overview"}, {"content": "| Method | LOVEU |  | BalanceCC |  |\n|---|---|---|---|---|\n| ControlVideo [132] _arXiv\u201923_ | 0.930 | 0.201 | 0.950 | 0.210 |\n| Tune-A-Video [114] _ICCV\u201923_ | 0.924 | 0.204 | 0.937 | 0.206 |\n| Pix2Video [8] _ICCV\u201923_ | 0.916 | 0.201 | 0.939 | 0.208 |\n| RAV [125] _SIGGRAPH Asia\u201924_ | 0.909 | 0.196 | 0.928 | 0.201 |\n| TokenFlow [30] _ICLR\u201924_ | 0.940 | 0.205 | 0.949 | 0.210 |\n| InsV2V [18] _ICLR\u201924_ | 0.911 | 0.208 | - | - |\n| Video-P2P [68] _CVPR\u201924_ | 0.935 | 0.201 | - | - |\n| CCEdit [26] _CVPR\u201924_ | - | - | 0.936 | 0.213 |\n| OmniCreator (Ours) | **0.958** | **0.209** | **0.963** | **0.214** |", "caption": "Table 6: Additional Quantitative Comparison on LOVEU-TGVE-2023 and BalanceCC benchmarks, which only focus on editing-type evaluations.", "description": "Table 6 presents a quantitative comparison of various text-guided video editing methods, focusing specifically on editing types.  The comparison uses two established benchmarks: LOVEU-TGVE-2023 and BalanceCC. These benchmarks primarily evaluate the performance of different methods based on four main editing types (Foreground, Background, Overall/Style, and Composite).  The table displays the performance of OmniCreator and several other state-of-the-art methods using two automatic metrics: CLIP Frame and PickScore.  The inclusion of this table highlights the effectiveness of the OmniCreator model in comparison to other existing methods within a standardized, established evaluation framework.", "section": "5. Experiment"}]