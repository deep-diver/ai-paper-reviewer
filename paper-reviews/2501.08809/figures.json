[{"figure_path": "https://arxiv.org/html/2501.08809/x1.png", "caption": "Figure 1: The architectural overview of our XMusic framework. It contains two essential components: XProjector and XComposer. XProjector parses various input prompts into specific symbolic music elements. These elements then serve as control signals, guiding the music generation process within the Generator of XComposer. Additionally, XComposer includes a Selector that evaluates and identifies high-quality generated music. The Generator is trained on our large-scale dataset, XMIDI, which includes precise emotion and genre labels.", "description": "XMusic is a symbolic music generation framework with two main components: XProjector and XComposer.  XProjector takes multi-modal inputs (images, videos, text, tags, humming) and translates them into symbolic musical elements like emotion, genre, rhythm, and notes. These elements act as control signals for XComposer. XComposer consists of a Generator, which produces music based on these control signals, and a Selector, which uses a multi-task learning approach (quality, emotion, and genre assessment) to select the highest-quality output. The Generator is trained on a large dataset, XMIDI, that includes emotion and genre labels.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.08809/x2.png", "caption": "Figure 2: Illustration of the proposed XMusic, which supports flexible (a) X-Prompts to guide the generation of high-quality symbolic music. The XProjector analyzes these prompts, mapping them to symbolic music elements within the (b) Projection Space. Subsequently, the (c) Generator of XComposer transforms these symbolic music elements into token sequences based on our enhanced representation. It employs a Transformer Decoder as the generative model to predict successive events iteratively, thereby creating complete musical compositions. Finally, the (d) Selector of XComposer utilizes a Transformer Encoder to encode the complete token sequences and employs a multi-task learning scheme to evaluate the quality of the generated music.", "description": "Figure 2 illustrates the XMusic framework, a system for generating high-quality symbolic music from various prompts.  Panel (a) shows the diverse input types: images, videos, text, tags, and humming.  The XProjector component (b) processes these prompts, translating them into symbolic music elements (emotions, genres, rhythms, and notes) within a common 'Projection Space'.  Next, the XComposer component's Generator (c) uses these elements to generate the actual music, employing a Transformer Decoder to create sequential musical representations. Finally, the Selector (d) assesses the generated music's quality via a multi-task learning process incorporating emotion recognition, genre recognition, and overall quality evaluation.", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x3.png", "caption": "Figure 3: Comparison between our representation and Compound Word (CP) \u00a0[10] representation. The dotted boxes represent our new tokens in comparison with those of the CP representation.", "description": "Figure 3 illustrates the differences between our proposed symbolic music representation and the Compound Word (CP) representation introduced in a previous work [10].  The core enhancement in our representation is the inclusion of new token types (shown in dotted boxes) which provide explicit control over musical elements, namely: genre, emotion, and instrument type. These additional tokens improve the model's ability to generate diverse and controlled symbolic music. The addition of the 'Tag' token, encapsulating 'Emotion' and 'Genre', allows for high-level semantic control over style and emotional expression. The 'Instrument' token, further divided into 'Program', facilitates the creation of multi-track compositions. The improvements to the 'Rhythm' token allows more precise control over rhythmic elements, namely 'density' and 'strength'.", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x4.png", "caption": "Figure 4: Data statistics of our XMIDI dataset.", "description": "Figure 4 presents a visual summary of the XMIDI dataset's key statistical properties. It consists of three subfigures: (a) illustrates the distribution of the 11 emotion classes, showing the relative frequency of each emotion in the dataset.  (b) shows the distribution of the 6 genre classes, similarly displaying their proportions.  (c) illustrates the distribution of music lengths across the dataset, indicating the frequency of different song durations. This figure provides a concise overview of the dataset's characteristics in terms of emotional variety, musical genres and song lengths.", "section": "IV. Experiments"}]