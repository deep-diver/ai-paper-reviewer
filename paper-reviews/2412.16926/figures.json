[{"figure_path": "https://arxiv.org/html/2412.16926/x1.png", "caption": "Figure 1: Results of various sample selection approaches in many-shot ICL with LCLMs. Approaches include Retrieval that selects examples similar to the target query, Diversity that aims for maximizing example variety, Curriculum that arranges examples in order from easiest to hardest, and Hard that uses only challenging examples, alongside Random that selects examples without any constraints. Results indicate that sample selection methods provide no significant improvement over the naive (random) approach and sometimes perform worse. Meanwhile, Augmentation refers to the approach that generates additional demonstrations and uses them along with original samples for ICL, for low-resource tasks (such as translation, reasoning, and classification) that do not contain enough samples to utilize the full capacity of LCLMs, showing substantial performance gains.", "description": "This figure compares the performance of various sample selection methods in many-shot in-context learning (ICL) using Long Context Language Models (LCLMs).  The methods tested include: Retrieval (selecting similar examples), Diversity (maximizing example variety), Curriculum (arranging examples from easiest to hardest), and Hard (using only challenging examples).  A baseline Random selection method is also included.  The results show that sophisticated sample selection techniques do not significantly improve performance over the simple random approach and sometimes perform worse.  A final method, Augmentation, generates additional examples to supplement existing ones, demonstrating substantial performance gains for low-resource tasks (translation, reasoning, classification) where available data is limited compared to the context window size of the LCLM.", "section": "2.2 Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2412.16926/x8.png", "caption": "Figure 2: Detailed results of various sample selection approaches on ICL with LCLMs, such as Gemini Pro (Top), Gemini Flash (Middle), and Llama 3.1 (Bottom), across four different tasks (translation, summarization, reasoning, and extreme classification) with 18 datasets. Each bar represents the averaged performance, with the upper and lower limits indicating standard deviation.", "description": "Figure 2 presents a comprehensive comparison of various sample selection methods within the in-context learning (ICL) paradigm using three different long-context language models (LCLMs): Gemini Pro, Gemini Flash, and Llama 3.1.  The results are shown across four distinct tasks\u2014translation, summarization, reasoning, and extreme classification\u2014and encompass 18 diverse datasets. Each bar in the chart represents the average performance of a specific sample selection method for a given task and dataset combination. Error bars denote the standard deviation, indicating performance variability. This visualization enables a detailed assessment of the effectiveness of different sample selection techniques in the context of many-shot ICL using LCLMs.", "section": "2.3 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2412.16926/x9.png", "caption": "Figure 3: Results with varying the number of examples for ICL with Gemini Pro, where we average the results for each task.", "description": "This figure displays the performance of In-context Learning (ICL) with Gemini Pro across four different tasks (summarization, translation, reasoning, and classification) as the number of ICL examples varies.  The results are averages across multiple runs for each task.  It shows how performance changes with the number of examples provided in the context window.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2412.16926/x10.png", "caption": "Figure 4: Results with varying the ratio of noisy examples within the context of LCLMs, where we report the relative performance over the ICL without noisy examples (i.e., the noise ratio of 0) and the results are averaged over multiple runs.", "description": "This figure displays the impact of noisy examples on the performance of Long Context Language Models (LCLMs) in In-Context Learning (ICL).  Different noise ratios (percentage of noisy examples) are tested across four tasks: summarization, translation, reasoning, and classification.  For each task, multiple datasets are used, and the relative performance is calculated compared to a baseline of zero noise. The y-axis shows the relative performance (percentage), and the x-axis shows the noise ratio (%). Averaged results across multiple runs are presented. The figure visually demonstrates how LCLM performance degrades as the percentage of noisy examples increases, with the degradation being more significant in certain tasks (e.g., low resource translation).", "section": "4.1 LCLM-Based ICL with Noisy Examples"}]