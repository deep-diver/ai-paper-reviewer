[{"content": "| **LCLMs** | **Methods** | **Tran.** | **Summ.** | **Reas.** | **Clas.** | **Total** |\n|---|---|---|---|---|---|---|\n| **Gemini Pro** | Relevance | 0 / 6 | 0 / 3 | 0 / 4 | 0 / 5 | 0 / 18 |\n|  | Diversity | 0 / 6 | 0 / 3 | 1 / 4 | 2 / 5 | 3 / 18 |\n|  | Curriculum | 1 / 6 | 0 / 3 | 0 / 4 | 1 / 5 | 2 / 18 |\n|  | Hard | 0 / 6 | 0 / 3 | 1 / 4 | 0 / 5 | 1 / 18 |\n| **Gemini Flash** | Relevance | 0 / 6 | 0 / 3 | 0 / 4 | 2 / 5 | 2 / 18 |\n|  | Diversity | 0 / 6 | 0 / 3 | 0 / 4 | 2 / 5 | 2 / 18 |\n|  | Curriculum | 0 / 6 | 0 / 3 | 0 / 4 | 0 / 5 | 0 / 18 |\n|  | Hard | 0 / 6 | 0 / 3 | 0 / 4 | 0 / 5 | 0 / 18 |\n| **Llama 3.1** | Relevance | 1 / 6 | 0 / 3 | 1 / 4 | 1 / 5 | 3 / 18 |\n|  | Diversity | 0 / 6 | 0 / 3 | 0 / 4 | 2 / 5 | 2 / 18 |\n|  | Curriculum | 0 / 6 | 0 / 3 | 0 / 4 | 1 / 5 | 1 / 18 |\n|  | Hard | 0 / 6 | 0 / 3 | 0 / 4 | 2 / 5 | 2 / 18 |\n| **Total** | Relevance | 1 / 18 | 0 / 9 | 1 / 12 | 3 / 15 | 5 / 54 |\n|  | Diversity | 0 / 18 | 0 / 9 | 1 / 12 | 6 / 15 | 7 / 54 |\n|  | Curriculum | 1 / 18 | 0 / 9 | 0 / 12 | 2 / 15 | 3 / 54 |\n|  | Hard | 0 / 18 | 0 / 9 | 1 / 12 | 2 / 15 | 3 / 54 |", "caption": "Table 1: Counting the statistical significance of sophisticated selection approaches over random selection on each experiment instance, by conducting the t-test with 95% confidence threshold. Tran., Summ., Reas, Clas, denote translation, summarization, reasoning, and classification tasks, respectively.", "description": "This table presents a statistical analysis comparing the performance of different sample selection methods against a simple random selection approach.  It shows the number of times (out of the total number of experiments conducted within each task) that sophisticated methods demonstrated statistically significant improvement over random selection, using a t-test with a 95% confidence level. The tasks considered are translation, summarization, reasoning, and classification.  This analysis helps determine if more complex sample selection strategies offer a substantial performance advantage over a simpler random approach, especially for LCLMs.", "section": "2.3 Experimental Results"}, {"content": "| Methods | Summarization | Translation | Reasoning | Classification |\n|---|---|---|---|---|\n| Random | 0.310 \u00b1 0.004 | 0.553 \u00b1 0.004 | 0.650 \u00b1 0.023 | 0.539 \u00b1 0.007 |\n| Ascending | 0.307 \u00b1 0.006 | 0.557 \u00b1 0.004 | 0.641 \u00b1 0.027 | 0.534 \u00b1 0.010 |\n| Descending | 0.309 \u00b1 0.003 | 0.552 \u00b1 0.007 | 0.648 \u00b1 0.021 | 0.539 \u00b1 0.005 |", "caption": "Table 2: Results with varying the order of ICL samples, where Ascending and Descending represent cases where examples closer to the query appear earlier and later in the LCLM context, respectively. In contrast, random denotes the case where examples are arranged randomly without a specific order.", "description": "This table presents the results of an experiment investigating the impact of the order of in-context learning (ICL) examples on the performance of long-context language models (LCLMs). Three ordering methods were compared: 'Ascending' (examples most similar to the query appear first), 'Descending' (most similar examples appear last), and 'Random' (examples are shuffled). The table shows the average performance and standard deviation for each ordering method across four different tasks: summarization, translation, reasoning, and classification. This helps determine whether LCLM performance is sensitive to ICL example order, particularly in many-shot scenarios.", "section": "Analysis on Example Order"}, {"content": "LCLMs|Methods|ENG to BEM|ENG to KMR|ENG to EWE|ENG to SPA|ENG to FRA|ENG to DEU|Date|Salient\n---|---|---|---|---|---|---|---|---|---\n**Gemini Pro**|Random|0.470 \u00b1 0.003|0.439 \u00b1 0.001|0.419 \u00b1 0.004|0.580 \u00b1 0.006|0.734 \u00b1 0.002|0.676 \u00b1 0.010|0.854 \u00b1 0.009|0.776 \u00b1 0.035|\n**Gemini Pro**|Best Selection|0.470 \u00b1 0.004|0.443 \u00b1 0.004|0.418 \u00b1 0.002|0.583 \u00b1 0.004|**0.745** \u00b1 0.005|0.676 \u00b1 0.004|**0.896** \u00b1 0.021|0.772 \u00b1 0.017|\n---|---|---|---|---|---|---|---|---|---\n**Gemini Pro**|Augmentation|**0.487** \u00b1 0.007|**0.469** \u00b1 0.003|**0.437** \u00b1 0.003|**0.595** \u00b1 0.005|0.748 \u00b1 0.007|0.694 \u00b1 0.005|**0.927** \u00b1 0.019|0.784 \u00b1 0.018|\nLCLMs|Methods|Tracking7|Web|Banking77|DialogRE|Discovery|FewNERD|GoEmotion|Average\n---|---|---|---|---|---|---|---|---|---\n**Gemini Pro**|Random|0.294 \u00b1 0.029|0.675 \u00b1 0.021|0.878 \u00b1 0.002|0.661 \u00b1 0.009|0.195 \u00b1 0.007|0.568 \u00b1 0.012|0.393 \u00b1 0.007|0.574 \u00b1 0.010|\n**Gemini Pro**|Best Selection|0.311 \u00b1 0.031|**0.700** \u00b1 0.028|**0.886** \u00b1 0.004|**0.709** \u00b1 0.014|0.204 \u00b1 0.011|0.569 \u00b1 0.006|**0.413** \u00b1 0.006|0.586 \u00b1 0.011|\n---|---|---|---|---|---|---|---|---|---\n**Gemini Pro**|Augmentation|0.307 \u00b1 0.031|**0.768** \u00b1 0.040|**0.889** \u00b1 0.004|**0.698** \u00b1 0.010|**0.209** \u00b1 0.009|0.574 \u00b1 0.008|**0.428** \u00b1 0.006|**0.601** \u00b1 0.012|\nLCLMs|Methods|ENG to BEM|ENG to KMR|ENG to EWE|ENG to SPA|ENG to FRA|ENG to DEU|Date|Salient\n---|---|---|---|---|---|---|---|---|---\n**Gemini Flash**|Random|0.419 \u00b1 0.006|0.427 \u00b1 0.004|0.363 \u00b1 0.002|0.573 \u00b1 0.004|0.726 \u00b1 0.004|0.666 \u00b1 0.005|0.754 \u00b1 0.022|0.682 \u00b1 0.019|\n**Gemini Flash**|Best Selection|0.421 \u00b1 0.002|0.434 \u00b1 0.002|0.360 \u00b1 0.003|0.575 \u00b1 0.002|0.732 \u00b1 0.003|0.673 \u00b1 0.001|0.777 \u00b1 0.030|0.687 \u00b1 0.015|\n---|---|---|---|---|---|---|---|---|---\n**Gemini Flash**|Augmentation|**0.436** \u00b1 0.006|**0.460** \u00b1 0.002|**0.378** \u00b1 0.004|**0.594** \u00b1 0.007|0.737 \u00b1 0.010|0.676 \u00b1 0.012|**0.804** \u00b1 0.037|**0.714** \u00b1 0.013|\nLCLMs|Methods|Tracking7|Web|Banking77|DialogRE|Discovery|FewNERD|GoEmotion|Average\n---|---|---|---|---|---|---|---|---|---\n**Gemini Flash**|Random|0.256 \u00b1 0.030|0.582 \u00b1 0.033|0.868 \u00b1 0.004|0.541 \u00b1 0.008|0.065 \u00b1 0.007|0.521 \u00b1 0.006|0.362 \u00b1 0.016|0.520 \u00b1 0.011|\n**Gemini Flash**|Best Selection|0.270 \u00b1 0.031|0.566 \u00b1 0.031|0.872 \u00b1 0.006|0.547 \u00b1 0.012|**0.083** \u00b1 0.007|**0.532** \u00b1 0.002|**0.385** \u00b1 0.006|0.528 \u00b1 0.010|\n---|---|---|---|---|---|---|---|---|---\n**Gemini Flash**|Augmentation|0.281 \u00b1 0.035|0.609 \u00b1 0.040|**0.880** \u00b1 0.006|**0.578** \u00b1 0.025|**0.090** \u00b1 0.005|**0.537** \u00b1 0.009|**0.392** \u00b1 0.015|**0.544** \u00b1 0.015", "caption": "Table 3: Results of LCLM-enabled ICL on four different tasks, where Random indicates the naive sample selection approach without selection criteria, Best Selection indicates the model that achieves the best performance among sophisticated sample selection methods for each experiment unit, and Augmentation indicates the proposed approach that generates demonstrations and uses them alongside original samples with random selection. We emphasize statistically significant results over Random in bold. We exclude Llama from the augmentation scenario as its context capacity is approximately ten times smaller than that of Gemini, allowing it to fully utilize its available context with the original examples alone, making augmentation unnecessary.", "description": "This table presents the results of In-Context Learning (ICL) experiments using Long Context Language Models (LCLMs) on four tasks: translation, reasoning, summarization, and classification.  It compares three approaches: 1) Random sampling (no sample selection), 2) Best Selection (the best performing sophisticated selection method for each dataset), and 3) Augmentation (a new method combining randomly selected samples with generated demonstrations).  The table shows the performance of each approach for several different datasets, highlighting statistically significant improvements (in bold) over the Random approach.  Llama is excluded from the Augmentation tests because its context window is much smaller than Gemini's, rendering augmentation unnecessary in that case.", "section": "Experimental Results"}, {"content": "| Methods | Translation | Reasoning | Classification |\n|---|---|---|---|\n| Augmentation | **0.571** \u00b1 0.005 | **0.696** \u00b1 0.027 | **0.560** \u00b1 0.008 |\n| w/o Filtering | 0.552 \u00b1 0.005 | 0.666 \u00b1 0.031 | 0.548 \u00b1 0.009 |\n| w/o Original | 0.544 \u00b1 0.002 | 0.611 \u00b1 0.025 | 0.531 \u00b1 0.007 |\n| Only Original | 0.553 \u00b1 0.004 | 0.650 \u00b1 0.023 | 0.539 \u00b1 0.007 |", "caption": "Table 4: Results on ablation study, where w/o Filtering and w/o Original denote the ICL results based on augmented samples without filtering and without original samples, respectively. Only Original is the performance without generated samples.", "description": "This ablation study investigates the individual and combined contributions of filtering and original samples to the performance of the augmentation approach.  'w/o Filtering' shows the results when augmented samples are used without the filtering process. 'w/o Original' presents the results using only the augmented samples, excluding the original samples.  'Only Original' shows the baseline performance, using only the original samples without any augmentation. This analysis reveals the impact of each component on the overall improvement achieved by the augmentation method.", "section": "3.3 Experimental Results"}, {"content": "| Types | Prompts |\n|---|---| \n| Translation | You are an expert translator. I am going to give you one or more example pairs of text snippets where the first is in {SOURCE_LANGUAGE} and the second is a translation of the first snippet into {TARGET_LANGUAGE}. The sentences will be written as the following format: {SOURCE_LANGUAGE}: &lt;first sentence&gt; {TARGET_LANGUAGE}: &lt;translated first sentence&gt; After the example pairs, I am going to provide another sentence in {SOURCE_LANGUAGE} and I want you to translate it into {TARGET_LANGUAGE}. Give only the translation, and no extra commentary, formatting, or chattiness. Translate the text from {SOURCE_LANGUAGE} to {TARGET_LANGUAGE}. {EXAMPLES} {TARGET_QUERY} |\n| Summarization | You are an expert in article summarization. I am going to give you one or more example pairs of article and its summary in fluent English. The pairs will be written as the following format: Article: &lt;article&gt; Summary: &lt;summary&gt; After the example pairs, I am going to provide another article and I want you to summarize it. Give only the summary, and no extra commentary, formatting, or chattiness. {EXAMPLES} {TARGET_QUERY} |\n| Reasoning | You are an expert in multiple-choice question answering tasks. I am going to give you one or more example pairs of question and its answer in a multiple-choice question answering format. The pairs will be written as the following format: Question: &lt;question&gt; Answer: &lt;answer&gt; After the example pairs, I am going to provide another question and I want you to predict its answer. Give only the answer that follows a consistent format as in the provided examples, and no extra commentary, formatting, or chattiness. {EXAMPLES} {TARGET_QUERY} |", "caption": "Table 5: A list of prompts that we use for many-shot ICL on translation, summarization, and reasoning tasks.", "description": "This table presents the prompts used in the many-shot in-context learning (ICL) experiments for three different tasks: translation, summarization, and reasoning.  Each prompt provides example input-output pairs followed by a target input for the model to predict the output. The examples aim to guide the model's behavior for the specific task. The table details the structure of the input to be given to the model in each case.", "section": "2.2 Experimental Setup"}, {"content": "| Types | Prompts |\n|---|---| \n| BANKING77 | I am going to give you one or more example pairs of customer service query and its intent.The pairs will be written as the following format:service query: &lt;query&gt;intent category: &lt;category&gt;After the example pairs, I am going to provide another customer service query and I want you to classify the label of it that must be one among the intent categories provided in the examples. Give only the category, and no extra commentary, formatting, or chattiness.{EXAMPLES}{TARGET_QUERY}| \n| DialogRE | I am going to give you one or more examples of the dialogue, the list of entity pairs within it, and their corresponding relation types.The examples will be written as the following format:Dialogue: &lt;dialogue&gt;The list of k entity pairs are (&lt;entity 1&gt;, &lt;entity 2&gt;), \u2026The k respective relations between each entity pair are: &lt;relation&gt;, \u2026After the examples, I am going to provide another dialogue along with its associated entity pairs, and I want you to classify their corresponding relation types that must be one among the relation types provided in the examples. Give only the relations, and no extra commentary, formatting, or chattiness.{EXAMPLES}{TARGET_QUERY}| \n| Discovery | I am going to give you one or more example pairs of two sentences and the conjunction word between them.The pairs will be written as the following format:&lt;sentence 1&gt; ( ) &lt;sentence 2&gt;the most suitable conjunction word in the previous ( ) is &lt;conjunction word&gt;After the example pairs, I am going to provide another two sentences and I want you to classify the conjunction word between them that must be one among the conjunction words provided in the examples. Give only the conjunction word, and no extra commentary, formatting, or chattiness.{EXAMPLES}{TARGET_QUERY}| \n| FewNERD | I am going to give you one or more examples of the sentence, the named entities within it, and their corresponding entity types.The examples will be written as the following format:Sentence: &lt;sentence&gt;&lt;named entity&gt;: &lt;entity type&gt;After the example pairs, I am going to provide another comment and I want you to classify the label of it that must be one among the emotion categories provided in the examples. Give only the category, and no extra commentary, formatting, or chattiness.{EXAMPLES}{TARGET_QUERY}| \n| GoEmotion | I am going to give you one or more example pairs of comment and its emotion category.The pairs will be written as the following format:comment: &lt;comment&gt;emotion category: &lt;category&gt;After the example pairs, I am going to provide another sentence, and I want you to classify the named entities within it and their corresponding entity types that must be one among the entity types provided in the examples. Give only the named entities and their corresponding entity types, and no extra commentary, formatting, or chattiness.{EXAMPLES}{TARGET_QUERY}|", "caption": "Table 6: A list of prompts that we use for many-shot ICL on five different extreme classification tasks.", "description": "This table details the prompts used in the many-shot in-context learning (ICL) experiments for five extreme classification tasks.  Each prompt provides a specific instruction format, example pairs for the model to learn from, and a final query for the model to classify. The five tasks covered include: BANKING77 (customer service queries), DialogRE (dialogue relation extraction), Discovery (conjunction word prediction), FewNERD (named entity recognition), and GoEmotion (emotion classification).", "section": "2.2 Experimental Setup"}, {"content": "| Types | Prompts |\n|---|---| \n| Generation | You are an expert in data augmentation. You will be provided with a series of demonstrations that show how a task is performed. Your objective is to generate a new example that closely follows the pattern, structure, and style of the demonstrations. Carefully analyze the key steps, transitions, and output style in the provided demonstrations. Then, create a new sample that maintains consistency in format and correctness while introducing variety in content. <br> Here are the demonstrations: <br> {EXAMPLES} <br> Now, as an expert, generate a new sample that aligns with the original demonstrations: | \n| Filtering | You are an expert in assessing data quality. Given the original set of samples, your task is to carefully evaluate the provided sample in comparison to the original samples. Based on your expertise, determine whether the provided sample is of high quality, meeting or exceeding the standards set by the original set. <br> Here are the original samples: <br> {EXAMPLES} <br> Now, as an expert, evaluate the provided sample: <br> {GENERATED_SAMPLE} <br> Please provide only a single numerical rating (1, 2, 3, 4, or 5) based on the quality of the sample, without any additional commentary, formatting, or chattiness. |", "caption": "Table 7: A list of prompts that we use for generating synthetic demonstrations and filtering them of low-quality.", "description": "Table 7 presents the prompts used in the data augmentation process.  The first prompt instructs the language model to generate new examples similar in style and content to a provided set of examples. The second prompt guides the model to evaluate the quality of a synthetically generated example by assigning a score based on its adherence to the style and quality of the original examples.", "section": "3 Augmenting ICL Demonstrations to Increase Context Capacity of LCLMs"}]