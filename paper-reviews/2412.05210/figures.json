[{"figure_path": "https://arxiv.org/html/2412.05210/x1.png", "caption": "Figure 1: A comparison between the GPT4o with better human preference and Qwen2.5-Coder-7B-Instruct. Qwen2.5-Coder-7B-Instruct solves the user question by simply replying with the code snippet without details.", "description": "Figure 1 compares the responses of two code large language models (LLMs) to a user query.  GPT-4 offers a response with a detailed explanation, algorithm breakdown, and instructions on how to use the code, demonstrating a strong alignment with human preferences. In contrast, Qwen2.5-Coder-7B-Instruct provides only the code snippet without any additional context or explanation.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.05210/x2.png", "caption": "Figure 2: Task types of CodeArena.", "description": "This figure shows a hierarchical breakdown of the task categories present in the CodeArena benchmark dataset.  The top level shows seven major categories of coding tasks. Each major category is further divided into multiple subcategories, offering a detailed view of the diverse coding problems included in the benchmark. This visualization helps to understand the breadth and depth of coding scenarios covered by CodeArena, ensuring comprehensive evaluation of code language models.", "section": "2 CodeArena"}, {"figure_path": "https://arxiv.org/html/2412.05210/x3.png", "caption": "Figure 3: Statistics of programming languages in CodeArena.", "description": "This figure shows the distribution of various programming languages used within the CodeArena dataset.  It provides a visual representation of the frequency or number of samples involving each language, illustrating the diversity of programming languages covered in the benchmark.", "section": "2 CodeArena"}, {"figure_path": "https://arxiv.org/html/2412.05210/x4.png", "caption": "Figure 4: Number of samples of different difficulties (Easy/Medium/Hard) across categories in CodeArena.", "description": "This bar chart visualizes the distribution of task difficulty levels (Easy, Medium, Hard) across various categories within the CodeArena benchmark dataset.  Each bar represents a category of coding tasks, and the height of each bar segment indicates the number of samples belonging to that category and difficulty level.  The chart provides insight into the balance of difficulty levels within CodeArena, allowing researchers to assess whether the benchmark adequately covers the range of skill levels that might be present in a real-world coding context.", "section": "2 CodeArena"}, {"figure_path": "https://arxiv.org/html/2412.05210/x5.png", "caption": "Figure 5: Overview of the CodeArena creation benchmark. We first collect the online code Q&A and code-related raw text from the website. We cluster the code-related data and classify them into different categories using LLM. We uniformly sample the samples from different subtasks as the seed data for manual annotation.", "description": "Figure 5 illustrates the process of creating the CodeArena benchmark dataset.  It starts by gathering code-related questions and answers from online Q&A websites.  These data are then processed using a large language model (LLM) to cluster similar questions and classify them into various categories.  Finally, a representative sample of questions from each category is selected for manual annotation to ensure high quality and relevance.", "section": "2 CodeArena"}, {"figure_path": "https://arxiv.org/html/2412.05210/x6.png", "caption": "Figure 6: Prompt of generating large-scale self-contained synthetic instruction data.", "description": "This figure displays the prompt template used to generate a large-scale, synthetic dataset of code instructions.  The prompt guides annotators to create self-contained programming questions based on given text excerpts, ensuring the questions are clear, solvable, and relevant to real-world scenarios.  It includes guidelines specifying the desired language, difficulty level, and overall quality of the generated question.", "section": "3 SynCode-Instruct"}, {"figure_path": "https://arxiv.org/html/2412.05210/x7.png", "caption": "Figure 7: Examples of CodeArena. The LLM judger decides which response is better.", "description": "Figure 7 showcases six diverse examples from the CodeArena benchmark, illustrating the range of coding tasks and programming languages involved.  Each example includes a user's query, two model-generated responses (one acting as a baseline, the other from a tested model), and the final judgment of a GPT-4 Large Language Model (LLM) acting as a 'judger', indicating which response was superior. This highlights CodeArena's ability to assess model performance across various aspects of code generation beyond simple execution correctness, encompassing factors like comprehensiveness, clarity, and practical utility.", "section": "Examples of CodeArena"}, {"figure_path": "https://arxiv.org/html/2412.05210/x8.png", "caption": "Figure 8: Comparison between MultiPL-E and CodeArena. LLMs in the blue circle present relatively mismatched performances on two benchmarks.", "description": "Figure 8 is a scatter plot visualizing the performance of various Large Language Models (LLMs) on two different code-related benchmarks: MultiPL-E and CodeArena.  Each point represents an LLM, with its x-coordinate showing its performance on MultiPL-E and its y-coordinate showing its performance on CodeArena. LLMs clustered closely together indicate similar performance on both benchmarks.  The points inside the blue circle highlight models that show a significant difference in performance between MultiPL-E and CodeArena, suggesting inconsistencies in their capabilities across different evaluation methods. This indicates a potential mismatch between the evaluation criteria of these two benchmarks and highlights the limitations of relying on a single benchmark for evaluating LLMs.", "section": "5 Results and Discussion"}, {"figure_path": "https://arxiv.org/html/2412.05210/x9.png", "caption": "Figure 9: Results of CodeArena with different data size on MultiPL-E and CodeArena.", "description": "Figure 9 illustrates the performance of the Qwen2.5-SynCoder model on both the MultiPL-E and CodeArena benchmarks when trained with varying amounts of synthetic instruction data.  The x-axis represents the size of the instruction tuning dataset (in billions of tokens), and the y-axis shows the model's performance, expressed as the win rate on CodeArena and Pass@1 score on MultiPL-E. The graph shows how the model's performance improves as the size of the training data increases, highlighting the impact of large-scale synthetic instruction data on model performance across different benchmarks.  Noteworthy is the two-stage fine-tuning strategy's superior performance compared to single-stage training. This comparison demonstrates the influence of high-quality, human-generated data on overall effectiveness.", "section": "5 Results and Discussion"}, {"figure_path": "https://arxiv.org/html/2412.05210/x10.png", "caption": "Figure 10: Distribution of CodeArena and MultiPL-E of different languages.", "description": "This figure visualizes the distribution of programming language queries in the CodeArena and MultiPL-E datasets using t-SNE dimensionality reduction.  CodeArena exhibits a broader, more dispersed distribution, reflecting its diverse range of real-world coding tasks and user queries. Conversely, MultiPL-E shows more clustered representations for each language, indicating a more focused scope. This visualization highlights CodeArena's greater coverage of diverse, practical coding scenarios compared to MultiPL-E.", "section": "5.2 Discussion"}]