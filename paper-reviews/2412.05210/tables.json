[{"content": "| Statistics | Number |\n|---|---| \n| **Problems** | 397 |\n| User Interface&Experience | 45 |\n| Development&Programming | 131 |\n| Specialized Computing | 91 |\n| Tools, Environments, and Application | 39 |\n| Miscellaneous and General Inquiry | 62 |\n| Databases&Data Handling | 22 |\n| Miscellaneous and General Inquiry | 7 |\n| **#Difficulty Level** |  |\n| - Easy/Medium/Hard | 97/173/132 |\n| **Length** |  |\n| Question |  |\n|  - _maximum length_ | 6736 tokens |\n|  - _minimum length_ | 5 tokens |\n|  - _avg length_ | 291 tokens |\n| Baseline Answer |  |\n|  - _maximum length_ | 5913 tokens |\n|  - _minimum length_ | 7 tokens |\n|  - _avg length_ | 4517 tokens |", "caption": "Table 1: CodeArena dataset statistics.", "description": "This table presents a statistical overview of the CodeArena dataset, detailing the number of problems, their distribution across various categories (e.g., User Interface & Experience, Development & Programming), the difficulty levels (Easy, Medium, Hard), and the length (in tokens) of both the questions and the corresponding baseline answers.", "section": "2 CodeArena"}, {"content": "| Benchmark | #Programming Languages | #Task | Source | #Languages | Evaluation | Human Annotation |\n|---|---|---|---|---|---|---|\n| HumanEval [Chen et al., 2021a] | 1 | 1 | Human Creation | 1 | Execution | \u2713 |\n| MBPP [Austin et al., 2021] | 1 | 1 | Human Creation | 1 | Execution | \u2713 |\n| LiveCodeBench [Jain et al., 2024] | 1 | 4 | Scraped from Code Contest Website | 1 | Execution | \u2713 |\n| MultiPl-E [Cassano et al., 2023] | 24 | 1 | Translated from HumanEval & MBPP | 1 | Execution | \u2717 |\n| McEval [Chai et al., 2024] | 40 | 3 | Human Creation | 1 | Execution | \u2713 |\n| MdEval [Liu et al., 2024b] | 18 | 3 | Human Creation | 1 | Execution | \u2713 |\n| CruxEval [Gu et al., 2024] | 1 | 2 | LLM Generation | 1 | Execution | \u2717 |\n| NaturalCodeBench [Zhang et al., 2024] | 2 | 6 | Scrape & LLM Generation & Human Filtered | 1 | Execution | \u2717 |\n| DebugBench [Tian et al., 2024] | 3 | 18 | Scrape & LLM Generation & Human Filtered | 1 | Execution | \u2717 |\n| CodeEditorBench [Guo et al., 2024b] | 3 | 4 | Scrape & LLM Generation & Human Filtered | 1 | Execution | \u2717 |\n| CodeArena (Ours) | 44 | 40 | Online Q&A | 2 | Human Preference | \u2713 |", "caption": "Table 2: Comparison between CodeArena and other benchmarks. CodeArena provides a comprehensive view by creating diverse user prompts to evaluation alignment between the model-generated response and human preference.", "description": "Table 2 compares CodeArena with other code-related benchmarks, highlighting its unique features. Unlike existing benchmarks that primarily focus on code correctness through limited test cases, CodeArena emphasizes human preference alignment. It achieves this by using diverse, real-world user prompts to evaluate code LLMs, providing a more comprehensive assessment that goes beyond simple code execution.", "section": "2 CodeArena"}, {"content": "| Model| Size| UI&UX| Development & Programming| Specialized Computing| Tools, Environs, & Practices| Emerging Techs & Apps| Miscellaneous & General Inquiry| Databases & Data Handling| Avg.| \n|---|---|---|---|---|---|---|---|---|---| \n| **Proprietary LLMs and 200B+ LLMs**| | | | | | | | | | \n| Claude-3.5-Sonnet-20240620| \"\"\"<span class=\"ltx_ERROR undefined\">\"\"\">| 88.9/2.2| 77.3/13.6| 74.2/18.0| 81.4/11.9| 78.9/10.5| 71.4/28.6| 63.6/4.5| 77.8/12.5| \n| Claude-3.5-Sonnet-20241022| \"\"\"<span class=\"ltx_ERROR undefined\">\"\"\">| 82.2/6.7| 75.8/12.9| 76.4/16.9| 84.7/10.2| 84.2/13.2| 57.1/28.6| 68.2/22.7| 78.1/13.5| \n| GPT-3.5-turbo-0125| \"\"\"<span class=\"ltx_ERROR undefined\">\"\"\">| 17.8/24.4| 11.4/20.5| 4.5/19.1| 11.9/18.6| 10.5/21.1| 13.6/9.1| 0.0/14.3| 10.5/19.6| \n| GPT-4o-mini-2024-07-18| \"\"\"<span class=\"ltx_ERROR undefined\">\"\"\">| 71.1/13.3| 62.1/17.4| 50.0/13.6| 65.2/14.6| 72.9/13.6| 71.1/18.4| 71.4/14.3| 65.8/15.6| \n| GPT-4o-2024-08-06| \"\"\"<span class=\"ltx_ERROR undefined\">\"\"\">| 66.7/17.8| 72.7/19.7| 62.9/19.1| 69.5/15.3| 76.3/13.2| 85.7/14.3| 59.1/22.7| 69.1/18.1| \n| o1-mini| \"\"\"<span class=\"ltx_ERROR undefined\">\"\"\">| <span class=\"ltx_text ltx_framed ltx_framed_underline\">93.3/4.4</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">94.7/2.6</span>| 84.1/7.6| <span class=\"ltx_text ltx_framed ltx_framed_underline\">91.0/5.6</span>| 88.1/3.4| <span class=\"ltx_text ltx_framed ltx_framed_underline\">95.5/0.0</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">100.0/0.0</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">89.3/5.1</span>| \n| o1-preview| \"\"\"<span class=\"ltx_ERROR undefined\">\"\"\">| 93.3/2.2| 81.8/7.6| <span class=\"ltx_text ltx_framed ltx_framed_underline\">85.4/7.9</span>| 78.0/6.8| <span class=\"ltx_text ltx_framed ltx_framed_underline\">92.1/2.6</span>| 77.3/4.5| 71.4/28.6| 83.9/6.6| \n| Yi-lightning| \"\"\"<span class=\"ltx_ERROR undefined\">\"\"\">| 62.2/15.6| 60.0/11.5| 57.9/5.3| 49.4/16.9| 71.2/11.9| 54.5/13.6| 85.7/0.0| 59.5/12.6| \n| Doubao-Pro| \"\"\"<span class=\"ltx_ERROR undefined\">\"\"\">| 51.1/20.0| 40.8/18.5| 55.3/26.3| 38.2/19.1| 47.5/22.0| 36.4/31.8| 42.9/57.1| 43.6/21.5| \n| Qwen-Max| \"\"\"<span class=\"ltx_ERROR undefined\">\"\"\">| 75.6/17.8| 74.2/13.6| 59.6/24.7| 78.0/6.8| 68.4/23.7| 100.0/0.0| 81.8/4.5| 71.9/15.8| \n| **0.5B+ Open-source LLMs**| | | | | | | | | | \n| Qwen2.5-0.5B-Instruct| 0.5B| <span class=\"ltx_text ltx_framed ltx_framed_underline\">2.2/4.4</span>| 4.6/4.6| <span class=\"ltx_text ltx_framed ltx_framed_underline\">5.3/10.5</span>| 2.2/4.5| <span class=\"ltx_text ltx_framed ltx_framed_underline\">3.4/5.1</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">4.5/9.1</span>| 0.0/14.3| 3.6/5.6| \n| Qwen2.5-Coder-0.5B-Instruct| 0.5B| 2.2/2.2| <span class=\"ltx_text ltx_framed ltx_framed_underline\">4.6/6.9</span>| 2.6/5.3| <span class=\"ltx_text ltx_framed ltx_framed_underline\">4.5/2.2</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">3.4/5.1</span>| 4.5/0.0| <span class=\"ltx_text ltx_framed ltx_framed_underline\">28.6/14.3</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">4.4/4.6</span>| \n| **1B+ Open-source LLMs**| | | | | | | | | | \n| DS-Coder-1.3B-Instruct| 1.3B| <span class=\"ltx_text ltx_framed ltx_framed_underline\">66.7/2.2</span>| 2.3/5.4| 2.6/10.5| 1.7/6.8| 0.0/9.1| 2.2/3.4| 0.0/14.3| 2.6/5.6| \n| Yi-Coder-1.5B-Chat| 1.5B| 11.1/2.2| 5.1/3.4| 5.4/4.6| 2.6/5.3| 2.2/5.6| 4.5/4.5| 14.3/14.3| 7.4/5.1| \n| Qwen2.5-Coder-1.5B-Instruct| 1.5B| 11.1/4.4| <span class=\"ltx_text ltx_framed ltx_framed_underline\">15.9/9.1</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">9.0/16.9</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">13.6/11.9</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">13.2/5.3</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">14.3/42.9</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">18.2/4.5</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">13.2/10.7</span>| \n| OpenCoder-1.5B-Instruct| 1.5B| 11.1/4.4| 3.8/5.4| 0.0/5.3| 2.2/4.5| 3.4/8.5| 4.5/9.1| 0.0/0.0| 6.7/3.8| \n| **3B+ Open-source LLMs**| | | | | | | | | | \n| Qwen2.5-Coder-3B-Instruct| 3B| <span class=\"ltx_text ltx_framed ltx_framed_underline\">35.6/11.1</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">29.5/10.6</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">27.0/15.7</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">20.3/18.6</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">28.9/10.5</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">42.9/14.3</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">27.3/13.6</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">28.3/13.3</span>| \n| **6B+ Open-source Models**| | | | | | | | | | \n| CodeLlama-7B-Instruct| 7B| 33.3/8.9| 28.8/18.6| 23.8/13.8| 18.2/9.1| 31.6/5.3| 29.2/14.6| <span class=\"ltx_text ltx_framed ltx_framed_underline\">71.4/0.0</span>| 28.2/12.8| \n| Llama3-8B-Instruct| 7B| 20.0/17.8| 14.6/11.5| 15.8/2.6| 13.5/9.0| 16.9/11.9| 22.7/0.0| 57.1/14.3| 16.7/10.3| \n| Llama3.1-8B-Instruct| 7B| 2.2/8.9| 4.5/10.1| 3.8/6.2| 3.4/6.8| 5.3/2.6| 9.1/9.1| 14.3/0.0| 7.9/4.4| \n| DS-Coder-6.7B-Instruct| 6.7B| 11.1/17.8| 13.1/13.8| 13.6/8.5| 13.2/7.9| 9.0/7.9| 13.6/4.5| 28.6/0.0| 12.3/10.8| \n| CodeQwen1.5-7B-Chat| 7B| 17.8/15.6| 13.8/12.3| 15.8/0.0| 15.7/9.0| 15.3/15.3| 18.2/13.6| 14.3/42.9| 15.4/11.8| \n| Yi-Coder-9B-Chat| 9B| 15.6/17.8| 15.4/9.2| 15.8/7.9| 13.5/13.5| 10.2/20.3| 18.2/13.6| 28.6/28.6| 14.6/13.3| \n| DS-Coder-V2-Lite-Instruct| 2.4/16B| <span class=\"ltx_text ltx_framed ltx_framed_underline\">42.2/20.0</span>| 33.3/17.4| 31.5/16.9| 35.6/20.3| <span class=\"ltx_text ltx_framed ltx_framed_underline\">39.5/21.1</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">71.4/14.3</span>| 31.8/22.7| 35.5/18.6| \n| Qwen2.5-Coder-7B-Instruct| 7B| 40.0/22.2| <span class=\"ltx_text ltx_framed ltx_framed_underline\">46.2/19.7</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">43.8/15.7</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">40.7/20.3</span>| 34.2/15.8| 71.4/0.0| 40.9/22.7| <span class=\"ltx_text ltx_framed ltx_framed_underline\">43.1/18.6</span>| \n| OpenCoder-8B-Instruct| 8B| 24.4/8.9| 14.6/8.5| 10.5/7.9| 9.0/4.5| 13.6/6.8| 18.2/9.1| 14.3/0.0| 14.1/7.1| \n| **13B+ Models**| | | | | | | | | | \n| CodeLlama-13B-Instruct| 13B| 13.3/4.4| 7.9/6.7| 6.8/8.5| 7.7/6.2| 4.5/4.5| 5.3/5.3| 14.3/14.3| 11.2/7.9| \n| Starcoder2-15B-Instruct-v0.1| 15B| 6.7/6.7| 6.8/12.9| 4.5/15.7| 6.8/6.8| 5.3/13.2| 13.6/13.6| 0.0/14.3| 6.4/12.0| \n| Qwen2.5-Coder-14B-Instruct| 14B| <span class=\"ltx_text ltx_framed ltx_framed_underline\">51.1/24.4</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">53.0/17.4</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">52.8/16.9</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">50.8/18.6</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">57.9/7.9</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">28.6/28.6</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">36.4/27.3</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">60.6/51.5</span>| \n| **20B+ Models**| | | | | | | | | | \n| CodeLlama-34B-Instruct| 34B| 11.1/6.7| 2.6/2.6| 6.9/2.3| 8.5/6.8| 7.9/10.1| 9.1/9.1| 14.3/0.0| 7.7/5.6| \n| CodeStral-22B-v0.1| 22B| 17.8/22.2| 27.3/13.6| 14.6/14.6| 25.4/10.2| 18.4/10.5| 14.3/42.9| 22.7/22.7| 21.7/15.8| \n| DS-Coder-33B-Instruct| 33B| 13.3/11.1| 22.0/9.8| 12.4/12.4| 13.6/6.8| 13.2/18.4| 28.6/42.9| 22.7/18.2| 16.8/12.0| \n| CodeLlama-70B-Instruct| 70B| 11.1/22.2| 9.2/10.0| 10.5/5.3| 9.0/6.7| 16.9/8.5| 9.1/13.6| 0.0/0.0| 15.5/10.5| \n| DS-Coder-V2-Instruct| 21/236B| 55.6/11.1| 62.1/18.2| 60.7/14.6| 50.8/18.6| 52.6/21.1| 71.4/14.3| 40.9/31.8| 57.4/17.6| \n| DS-V2.5| 21/236B| 77.8/11.1| <span class=\"ltx_text ltx_framed ltx_framed_underline\">72.0/12.9</span>| 71.9/13.5| 71.2/8.5| 73.7/10.5| 100.0/0.0| 68.2/13.6| 73.0/11.7| \n| Llama3-70B-Instruct| 7B| 35.6/20.0| 26.2/26.2| 25.4/22.0| 34.2/15.8| 23.6/14.6| 36.4/4.5| 14.3/57.1| 27.7/20.5| \n| Llama3.1-70B-Instruct| 7B| 48.9/24.4| 43.8/20.0| 34.2/26.3| 40.4/22.5| 54.2/20.3| 45.5/9.1| 71.4/14.3| 44.9/21.0| \n| Qwen2.5-Coder-32B-Instruct| 32B| 71.1/13.3| 66.7/15.9| 67.4/16.9| 74.6/13.6| 65.8/18.4| <span class=\"ltx_text ltx_framed ltx_framed_underline\">100.0/0.0</span>| 63.6/18.2| 68.9/15.6| \n| Qwen2.5-32B-Instruct| 32B| 62.2/15.6| 52.3/15.4| 57.9/18.4| 50.6/23.6| 54.2/13.6| 50.0/13.6| 71.4/14.3| 54.1/17.1| \n| QwQ-32B-Preview| 32B| 53.3/15.6| 56.8/16.7| 50.6/16.9| 64.4/5.1| 52.6/21.1| 85.7/0.0| 63.6/9.1| 56.6/14.5| \n| Qwen2.5-72B-Instruct| 72B| <span class=\"ltx_text ltx_framed ltx_framed_underline\">82.2/6.7</span>| 71.5/14.6| <span class=\"ltx_text ltx_framed ltx_framed_underline\">76.3/13.2</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">75.3/15.7</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">71.2/18.6</span>| 63.6/13.6| <span class=\"ltx_text ltx_framed ltx_framed_underline\">85.7/14.3</span>| <span class=\"ltx_text ltx_framed ltx_framed_underline\">73.8/14.4</span>| \n| <span style=\"background-color:#D9FFD9;\">Qwen2.5-SynCoder</span>| <span style=\"background-color:#D9FFD9;\">32B</span>| <span style=\"background-color:#D9FFD9;\">55.6/26.7</span>| <span style=\"background-color:#D9FFD9;\">49.2/20.8</span>| <span style=\"background-color:#D9FFD9;\">36.8/36.8</span>| <span style=\"background-color:#D9FFD9;\">50.6/20.2</span>| <span style=\"background-color:#D9FFD9;\">52.5/20.3</span>| <span style=\"background-color:#D9FFD9;\">40.9/18.2</span>| <span style=\"background-color:#D9FFD9;\">57.1/0.0</span>| <span style=\"background-color:#D9FFD9;\">49.2/22.3</span>| ", "caption": "Table 3: The win/tie rate of different instruction LLMs on CodeArena. The underlined numbers represent the best scores within the same model size range.", "description": "This table presents the win/tie rates achieved by various instruction-tuned large language models (LLMs) on the CodeArena benchmark.  The win/tie rate reflects the model's performance compared to a baseline model (gpt-4-turbo-2024-04-09) in generating responses preferred by human evaluators. Models are categorized by size (in terms of parameters), and the underlined numbers highlight the top-performing model in each size category.  The table provides a comparative analysis of different LLMs' abilities to generate human-preferred code, considering various model architectures and sizes.", "section": "5 Results and Discussion"}, {"content": "| Model | Size | HE | HE+ | MBPP | MBPP+ | Python | Java | C++ | C# | TS | JS | PHP | Bash | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Closed-APIs** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Claude-3.5-Sonnet-20240620 | \"\"\"\\faLock\"\"\" | 89.0 | 81.1 | 87.6 | 72.0 | 89.6 | 86.1 | 82.6 | 85.4 | 84.3 | 84.5 | 80.7 | 48.1 | 80.2 |\n| Claude-3.5-Sonnet-20241022 | \"\"\"\\faLock\"\"\" | 92.1 | 86.0 | 91.0 | 74.6 | 93.9 | 86.7 | 88.2 | 87.3 | 88.1 | 91.3 | 82.6 | 52.5 | 83.8 |\n| GPT-4o-mini-2024-07-18 | \"\"\"\\faLock\"\"\" | 87.8 | 84.8 | 86.0 | 72.2 | 87.2 | 75.9 | 77.6 | 79.7 | 79.2 | 81.4 | 75.2 | 43.7 | 79.1 |\n| GPT-4o-2024-08-06 | \"\"\"\\faLock\"\"\" | 92.1 | 86.0 | 86.8 | 72.5 | 90.9 | 83.5 | 76.4 | 81.0 | 83.6 | 90.1 | 78.9 | 48.1 | 79.1 |\n| o1-mini | \"\"\"\\faLock\"\"\" | 97.6 | 90.2 | 93.9 | 78.3 | 95.7 | 90.5 | 93.8 | 77.2 | 91.2 | 92.5 | 84.5 | 55.1 | 85.1 |\n| o1-preview | \"\"\"\\faLock\"\"\" | 95.1 | 88.4 | 93.4 | 77.8 | 96.3 | 88.0 | 91.9 | 84.2 | 90.6 | 93.8 | 90.1 | 47.5 | 85.3 |\n| **0.5B+ Models** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Qwen2.5-Coder-0.5B-Instruct | 0.5B | 61.6 | 57.3 | 52.4 | 43.7 | 61.6 | 57.3 | 52.4 | 43.7 | 50.3 | 50.3 | 52.8 | 27.8 | 49.6 |\n| **1B+ Models** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| DS-Coder-1.3B-Instruct | 1.3B | 65.9 | 60.4 | 65.3 | 54.8 | 65.2 | 51.9 | 45.3 | 55.1 | 59.7 | 52.2 | 45.3 | 12.7 | 48.4 |\n| Yi-Coder-1.5B-Chat | 1.5B | 69.5 | 64.0 | 65.9 | 57.7 | 67.7 | 51.9 | 49.1 | 57.6 | 57.9 | 59.6 | 52.2 | 19.0 | 51.9 |\n| Qwen2.5-Coder-1.5B-Instruct | 1.5B | 70.7 | 66.5 | 69.2 | 59.4 | 71.2 | 55.7 | 50.9 | 64.6 | 61.0 | 62.1 | 59.0 | 29.1 | 56.7 |\n| **3B+ Models** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Qwen2.5-Coder-3B-Instruct | 3B | 84.1 | 80.5 | 73.6 | 62.4 | 83.5 | 74.7 | 68.3 | 78.5 | 79.9 | 75.2 | 73.3 | 43.0 | 72.1 |\n| **6B+ Models** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| CodeLlama-7B-Instruct | 7B | 40.9 | 33.5 | 54.0 | 44.4 | 34.8 | 30.4 | 31.1 | 21.6 | 32.7 | - | 28.6 | 10.1 | - |\n| DS-Coder-6.7B-Instruct | 6.7B | 74.4 | 71.3 | 74.9 | 65.6 | 78.6 | 68.4 | 63.4 | 72.8 | 67.2 | 72.7 | 68.9 | 36.7 | 66.1 |\n| CodeQwen1.5-7B-Chat | 7B | 83.5 | 78.7 | 77.7 | 67.2 | 84.1 | 73.4 | 74.5 | 77.8 | 71.7 | 75.2 | 70.8 | 39.2 | 70.8 |\n| Yi-Coder-9B-Chat | 9B | 82.3 | 74.4 | 82.0 | 69.0 | 85.4 | 76.0 | 67.7 | 76.6 | 72.3 | 78.9 | 72.1 | 45.6 | 71.8 |\n| DS-Coder-V2-Lite-Instruct | 2.4/16B | 81.1 | 75.6 | 82.8 | 70.4 | 81.1 | 76.6 | 75.8 | 76.6 | 80.5 | 77.6 | 74.5 | 43.0 | 73.2 |\n| Qwen2.5-Coder-7B-Instruct | 7B | 88.4 | 84.1 | 83.5 | 71.7 | 87.8 | 76.5 | 75.6 | 80.3 | 81.8 | 83.2 | 78.3 | 48.7 | 76.5 |\n| OpenCoder-8B-Instruct | 8B | 83.5 | 78.7 | 79.1 | 69.0 | 83.5 | 72.2 | 61.5 | 75.9 | 78.0 | 79.5 | 73.3 | 44.3 | 71.0 |\n| **13B+ Models** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| CodeLlama-13B-Instruct | 13B | 40.2 | 32.3 | 60.3 | 51.1 | 42.7 | 40.5 | 42.2 | 24.0 | 39.0 | - | 32.3 | 13.9 | - |\n| Starcoder2-15B-Instruct-v0.1 | 15B | 67.7 | 60.4 | 78.0 | 65.1 | 68.9 | 53.8 | 50.9 | 62.7 | 57.9 | 59.6 | 53.4 | 24.7 | 54.0 |\n| Qwen2.5-Coder-14B-Instruct | 14B | 89.6 | 87.2 | 86.2 | 72.8 | 89.0 | 79.7 | 85.1 | 84.2 | 86.8 | 84.5 | 80.1 | 47.5 | 79.6 |\n| **20B+ Models** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| CodeLlama-34B-Instruct | 34B | 48.2 | 40.2 | 61.1 | 50.5 | 41.5 | 43.7 | 45.3 | 31.0 | 40.3 | - | 36.6 | 19.6 | - |\n| CodeStral-22B-v0.1 | 22B | 81.1 | 73.2 | 78.2 | 62.2 | 81.1 | 63.3 | 65.2 | 43.7 | 68.6 | - | 68.9 | 42.4 | - |\n| DS-Coder-33B-Instruct | 33B | 81.1 | 75.0 | 80.4 | 70.1 | 79.3 | 73.4 | 68.9 | 74.1 | 67.9 | 73.9 | 72.7 | 43.0 | 69.2 |\n| CodeLlama-70B-Instruct | 70B | 72.0 | 65.9 | 77.8 | 64.6 | 67.8 | 58.2 | 53.4 | 36.7 | 39.0 | - | 58.4 | 29.7 | - |\n| DS-Coder-V2-Instruct | 21/236B | 85.4 | 82.3 | 89.4 | 75.1 | 90.2 | 82.3 | 84.8 | 82.3 | 83.0 | 84.5 | 79.5 | 52.5 | 79.9 |\n| Qwen2.5-Coder-32B-Instruct | 32B | 92.7 | 87.2 | 90.2 | 75.1 | 92.7 | 80.4 | 79.5 | 82.9 | 86.8 | 85.7 | 78.9 | 48.1 | 79.4 |\n| Qwen2.5-32B-Instruct | 32B | 87.8 | 82.9 | 86.8 | 70.9 | 88.4 | 80.4 | 81.0 | 74.5 | 83.5 | 82.4 | 78.3 | 46.8 | 76.9 |\n| Qwen2.5-72B-Instruct | 32B | 85.4 | 79.3 | 90.5 | 77.0 | 82.9 | 81.0 | 80.7 | 81.6 | 81.1 | 82.0 | 77.0 | 48.7 | 75.1 |\n| Qwen2.5-SynCoder | 32B | 92.7 | 87.8 | 86.2 | 74.7 | 92.1 | 80.4 | 80.7 | 81.6 | 83.0 | 85.7 | 77.6 | 49.4 | 78.8 |", "caption": "Table 4: The performance of different instruction LLMs on EvalPlus and MultiPL-E. \u201cHE\u201d denotes the HumanEval, \u201cHE+\u201d denotes the plus version with more test cases, and \u201cMBPP+\u201d denotes the plus version with more test cases.", "description": "This table presents a comprehensive evaluation of various instruction-tuned Large Language Models (LLMs) on two established code generation benchmarks: EvalPlus and MultiPL-E.  EvalPlus is an enhanced version of HumanEval and MBPP, while MultiPL-E extends HumanEval to multiple programming languages.  The table details the performance of each LLM across different metrics, including HumanEval (HE), its extended version with more test cases (HE+), MBPP, its extended version (MBPP+), and individual scores for several programming languages (Python, Java, C++, C#, TypeScript, JavaScript, PHP, Bash). The results offer a comparative analysis of the LLMs' code generation capabilities across various tasks and languages, highlighting strengths and weaknesses of different models.", "section": "4.2 Evaluation Benchmark"}]