{"references": [{"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-03", "reason": "This paper introduces HumanEval, a benchmark for evaluating code generation capabilities of LLMs, which is foundational to many subsequent code LLM evaluation efforts."}, {"fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-07", "reason": "This paper is among the earliest works to demonstrate the capabilities of LLMs for program synthesis, establishing a key research direction in the field."}, {"fullname_first_author": "Federico Cassano", "paper_title": "MultiPL-E: A scalable and polyglot approach to benchmarking neural code generation", "publication_date": "2023-07-01", "reason": "MultiPL-E provides a benchmark for evaluating code generation across multiple programming languages, expanding the scope of earlier benchmarks and becoming a widely used evaluation suite."}, {"fullname_first_author": "Baptiste Rozi\u00e8re", "paper_title": "Code Llama: Open foundation models for code", "publication_date": "2023-08-12", "reason": "Code Llama is an open-source LLM specifically designed for code generation tasks, representing a significant advancement in readily-accessible code generation models."}, {"fullname_first_author": "Linzheng Chai", "paper_title": "McEval: Massively multilingual code evaluation", "publication_date": "2024-06-07", "reason": "McEval expands the evaluation of code generation capabilities to a wide range of programming languages, pushing beyond the limitations of earlier benchmarks focused on a small set of languages."}]}