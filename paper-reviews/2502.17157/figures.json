[{"figure_path": "https://arxiv.org/html/2502.17157/x1.png", "caption": "Figure 1: \nWith one single model, DiCeption\u00a0solves\nmultiple tasks\nwithout relying on any task-specific modules (rows 1 to 3). The red dots in the figure indicate the input points used for point-prompted segmentation. DiCeption\u00a0preserves fine details in segmentation, such as hair (row 4). DiCeption\u00a0supports both human pose estimation and semantic segmentation (row 5, 6). DiCeption\u00a0can quickly adapt to new tasks by fine-tuning less than 1% of its parameters on as few as 50 images (row 7). For additional visualizations, please refer to Figures\nS1,\nS2,\nS4,\nS6,\nS7,\nS8,\nS9,\nS10,\nS11,\nS12,\nS13,\nS14 in the Appendix.", "description": "Figure 1 showcases DiCeption's capabilities in handling various visual perception tasks.  The figure demonstrates that a single DiCeption model can perform multiple tasks (rows 1-3) without needing task-specific components. Point-prompted segmentation is highlighted with red dots as input (rows 1-3).  The model's precision is evident in preserving fine details like hair (row 4).  The versatility of the model extends to human pose estimation and semantic segmentation (rows 5-6). Finally, its adaptability is shown by its ability to learn new tasks with minimal fine-tuning (less than 1% of parameters on just 50 images, row 7).  Additional results are available in the appendix (Figures S1-S14).", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.17157/x2.png", "caption": "Figure 2: \nComparisons of mIoU with SAM-vit-h. We achieve results on par with SAM using only 0.06% of their data (600K vs.\u00a0 1B).", "description": "This figure presents a bar chart comparing the mean Intersection over Union (mIoU) scores achieved by the DICEPTION model and the SAM-vit-h model on various datasets.  The key finding is that DICEPTION achieves comparable performance to SAM-vit-h while using significantly less data \u2013 a mere 0.06% of the data used by SAM-vit-h (600,000 images versus 1 billion images).  The chart visually represents the difference in mIoU scores for each dataset, highlighting DICEPTION's efficiency in achieving similar results with drastically reduced data requirements.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.17157/x3.png", "caption": "Figure 3: \nComparisons between 1-point and 5-point segmentation of mIoU on all 23 validation datasets.", "description": "This figure compares the mean Intersection over Union (mIoU) scores achieved by a model using one versus five points in a point-prompted segmentation task.  The comparison is performed across all 23 validation datasets used in the study.  The goal is to demonstrate how increasing the number of input points can impact the performance and accuracy of the model in segmenting objects.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.17157/x4.png", "caption": "Figure S1: \nAdditional visualizations. Our one single model tackles multiple perception tasks.", "description": "This figure shows a variety of examples demonstrating the model's ability to perform multiple visual perception tasks using a single model.  Each row represents a different input image and shows the model's output for depth estimation, surface normal estimation, point-prompted segmentation, pose estimation, entity segmentation, and semantic segmentation. This showcases the versatility of the DICEPTION model in handling diverse visual perception tasks, even with minimal training data per task.", "section": "Additional Visualizations"}, {"figure_path": "https://arxiv.org/html/2502.17157/x5.png", "caption": "Figure S2: \nSegmentation results on furry objects.", "description": "This figure showcases the DICEPTION model's performance on images containing furry objects, a challenging scenario for segmentation. The results demonstrate the model's ability to accurately delineate the boundaries of furry animals, such as cats, dogs, and llamas, even in the presence of fine details and variations in fur texture.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.17157/x6.png", "caption": "Figure S3: \nWhen post-processing RGB masks, small regions and excessive numbers of objects lead to significant metric degradation.", "description": "During post-processing of RGB segmentation masks, the algorithm filters out small regions and those with excessive numbers of objects. While this improves the overall quality, it also removes some valid segments (like small birds or people in a crowd), leading to significant drops in metrics such as average precision (AP).  The figure likely shows examples where this filtering negatively impacts the results.", "section": "B.3. Performance Degradation of RGB Masks"}, {"figure_path": "https://arxiv.org/html/2502.17157/x7.png", "caption": "Figure S4: \nAdditional few-shot fine-tuning results on image highlighting.", "description": "This figure showcases the results of applying DICEPTION to image highlighting after few-shot fine-tuning.  It displays multiple example images where highlighting has been applied to specific regions or objects within the image, demonstrating the model's ability to adapt quickly to new image manipulation tasks with minimal training data.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.17157/x8.png", "caption": "Figure S5: \nOur segmentation not only separates semantically identical objects but also distinguishes different instances of the same category, achieving higher segmentation quality. Moreover, One Diffusion tends to generate an image similar to the input when performing image understanding tasks, as red-highlighted in the figure.", "description": "The figure showcases a comparison of DICEPTION's segmentation results versus One Diffusion's on various images.  DICEPTION effectively differentiates between semantically similar objects and successfully segments multiple instances of the same category, leading to improved segmentation accuracy and detail.  In contrast, One Diffusion, when tasked with image understanding, produces output images that closely resemble the input, failing to effectively perform the segmentation task, a shortcoming highlighted by the red boxes in the figure.", "section": "Additional Visualizations"}, {"figure_path": "https://arxiv.org/html/2502.17157/x9.png", "caption": "Figure S6: \nAdditional few-shot fine-tuning results on lung segmentation and tumor segmentation.", "description": "This figure displays the results of fine-tuning DICEPTION on lung and tumor segmentation tasks using a small amount of data.  It showcases the model's ability to adapt quickly to new medical imaging tasks and achieve high-quality segmentations, even with limited training samples.  Each image pair likely shows an input medical image and the corresponding segmentation mask produced by DICEPTION after few-shot fine-tuning. The accuracy and detail of the segmentations highlight DICEPTION's effectiveness.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.17157/x10.png", "caption": "Figure S7: \nAdditional depth estimation visualizations.", "description": "This figure displays a grid of images, each showing a different scene, along with their corresponding depth estimations generated by the DICEPTION model. The depth maps are presented in a heatmap format, where warmer colors represent closer distances and cooler colors represent further distances. This visualization showcases DICEPTION's capacity for accurate depth estimation across diverse real-world scenes and objects.", "section": "Additional Visualizations"}, {"figure_path": "https://arxiv.org/html/2502.17157/x11.png", "caption": "Figure S8: \nAdditional normal visualizations.", "description": "This figure displays a grid of images showcasing additional examples of surface normal estimations generated by the DICEPTION model.  Surface normals are crucial for representing the 3D shape and orientation of surfaces in images, and this visualization helps demonstrate the model's ability to accurately estimate these normals across a variety of scenes and objects, including people, cars, furniture, and everyday items.", "section": "Additional Visualizations"}, {"figure_path": "https://arxiv.org/html/2502.17157/x12.png", "caption": "Figure S9: \nAdditional entity segmentation visualizations.", "description": "This figure displays a grid of images showcasing the results of entity segmentation performed by the DICEPTION model.  Entity segmentation focuses on identifying individual instances within an image without classifying them into specific categories. Each identified instance is assigned a unique random color, facilitating visual distinction between different objects. The images demonstrate the model's ability to accurately segment diverse objects, ranging from everyday items (like bowls of fruit) to more complex scenes, showcasing the model's effectiveness across various object types and levels of visual complexity.", "section": "Additional Visualizations"}, {"figure_path": "https://arxiv.org/html/2502.17157/x13.png", "caption": "Figure S10: \nAdditional point-prompted segmentation visualizations.", "description": "This figure displays several examples of point-prompted image segmentation results.  Point-prompted segmentation uses a small number of points as input to specify the region of interest for segmentation.  The figure showcases the model's ability to accurately segment diverse objects and scenes, even with complex backgrounds or unusual viewpoints. Each image shows the input image alongside the model's predicted segmentation mask highlighting the object or area specified by the input points.", "section": "Additional Visualizations"}, {"figure_path": "https://arxiv.org/html/2502.17157/x14.png", "caption": "Figure S11: \nComparison of the segmentation results between DiCeption\u00a0and SAM-vit-h with 1-point input.", "description": "This figure displays a comparison of segmentation results between the DiCeption model and the SAM-vit-h model.  Both models were given a single point as input to guide the segmentation process.  The images show side-by-side comparisons of the input image, DiCeption's output segmentation mask, and SAM-vit-h's output segmentation mask. This allows for a visual assessment of the relative performance of the two models on various image types and segmentation challenges using minimal input information.", "section": "Additional Results"}, {"figure_path": "https://arxiv.org/html/2502.17157/x15.png", "caption": "Figure S12: \nComparison of the segmentation results between DiCeption\u00a0and SAM-vit-h with 5-point input.", "description": "This figure displays a comparison of segmentation results obtained using DiCeption and SAM-vit-h.  Both models were given the same input images and five points to guide their segmentation.  The comparison highlights the differences in segmentation accuracy and detail captured by each model, demonstrating the relative strengths and weaknesses of DiCeption and SAM-vit-h for point-prompted segmentation tasks.", "section": "Additional Results"}, {"figure_path": "https://arxiv.org/html/2502.17157/x16.png", "caption": "Figure S13: \nAdditional pose estimation visualizations.", "description": "This figure displays several example images and their corresponding human pose estimations generated by the DICEPTION model.  The images show diverse scenes and poses, demonstrating the model's ability to accurately estimate human poses in various contexts. Each image is paired with a visualization of the detected keypoints and their connections, illustrating the model's performance on different individuals, clothing styles, and activities.", "section": "Additional Visualizations"}, {"figure_path": "https://arxiv.org/html/2502.17157/x17.png", "caption": "Figure S14: \nAdditional semantic segmentation visualizations.", "description": "This figure shows various examples of semantic segmentation results produced by the DICEPTION model.  It demonstrates the model's ability to accurately segment various objects and scenes into their respective semantic classes, showcasing its performance on a range of complex and diverse visual inputs. The images depict a variety of scenes and objects, including food items, landscapes, and indoor settings.  Each image has its corresponding ground truth segmentation for comparison. The color-coded segmentation masks illustrate the model's classification of different semantic categories within the scene.", "section": "Appendix"}, {"figure_path": "https://arxiv.org/html/2502.17157/x18.png", "caption": "Figure S15: \nThe model tends to produce more failure cases in 1-step scenario.", "description": "The figure shows examples where using a one-step denoising process in the DICEPTION model, instead of the standard multi-step approach, leads to a significant increase in prediction errors or failures, particularly in more complex visual perception tasks.  The one-step method's inability to properly resolve ambiguity and handle intricate details across multiple tasks simultaneously is highlighted.", "section": "C.2. One-step Inference Does Not Work"}, {"figure_path": "https://arxiv.org/html/2502.17157/x19.png", "caption": "Figure S16: \nA\nUNet-based model fails to perform multi-task.", "description": "This figure demonstrates the failure of a UNet-based model to effectively handle multiple visual perception tasks simultaneously.  Unlike the DICEPTION model (which uses a Transformer-based architecture), the UNet architecture struggles to maintain comprehensive representations across various tasks, leading to significantly reduced performance and an inability to achieve good results on multiple tasks concurrently. This highlights the limitations of the UNet structure for multi-task learning in comparison to the more capable Transformer-based architecture.", "section": "Architecture of Diffusion Model"}]