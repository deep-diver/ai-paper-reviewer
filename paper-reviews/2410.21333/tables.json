[{"figure_path": "2410.21333/tables/table_6_0.html", "caption": "Table 1: Results contrasting zero-shot and CoT for artificial grammar learning.", "description": "The table presents the performance of various LLMs on an artificial grammar learning task using zero-shot and chain-of-thought prompting, showing the significant drop in performance with CoT.", "section": "4.1 Implicit Statistical Learning"}, {"figure_path": "2410.21333/tables/table_7_0.html", "caption": "Table 2: Comparison of zero-shot and CoT prompts for facial recognition.", "description": "The table shows the performance of various large multimodal models on a facial recognition task using zero-shot and chain-of-thought prompting, revealing consistent drops in accuracy for all models when using CoT.", "section": "4.2 FACIAL RECOGNITION"}, {"figure_path": "2410.21333/tables/table_8_0.html", "caption": "Table 3: Average number of rounds for models to learn labels using either direct or CoT prompting.", "description": "The table presents the average number of rounds needed for three large language models to correctly classify all vehicles in a dataset using direct prompting versus chain-of-thought prompting, showing a significant increase in rounds needed with chain-of-thought.", "section": "4.3 CLASSIFYING DATA WITH PATTERNS THAT CONTAIN EXCEPTIONS"}, {"figure_path": "2410.21333/tables/table_9_0.html", "caption": "Table 4: Results comparing zero-shot and CoT across the logical inconsistency task using stimuli from MNLI, SNLI, and synthetic LLM generation.", "description": "The table presents the zero-shot and chain-of-thought performance of several LLMs on a logical inconsistency task, using stimuli from three different datasets.", "section": "4.4 TASKS WITH A MISMATCH BETWEEN HUMAN AND MODEL ABILITIES"}, {"figure_path": "2410.21333/tables/table_9_1.html", "caption": "Table 5: Results comparing zero-shot and CoT on the spatial intuition task.", "description": "Table 5 shows the comparison of zero-shot and chain-of-thought prompting on the spatial intuition task, indicating performance change (absolute and relative) and p-values for various models.", "section": "4.4 TASKS WITH A MISMATCH BETWEEN HUMAN AND MODEL ABILITIES"}, {"figure_path": "2410.21333/tables/table_10_0.html", "caption": "Table 6: Results for apartment selection task across four models and three ranges of \u0394.", "description": "The table presents the zero-shot and chain-of-thought prompting performance of four LLMs on an apartment selection task, categorized by three ranges of apartment quality differences.", "section": "4.4 TASKS WITH A MISMATCH BETWEEN HUMAN AND MODEL ABILITIES"}, {"figure_path": "2410.21333/tables/table_17_0.html", "caption": "Table 1: Results contrasting zero-shot and CoT for artificial grammar learning.", "description": "The table presents a comparison of the performance of various large language models on an artificial grammar learning task using zero-shot prompting versus chain-of-thought prompting, showing a significant performance decrease with CoT prompting.", "section": "4.1 Implicit Statistical Learning"}, {"figure_path": "2410.21333/tables/table_17_1.html", "caption": "Table 8: Results comparing zero-shot, CoT, and ToT on a subset of the artificial grammar learning task.", "description": "The table presents the performance of GPT-40 model on artificial grammar learning task using zero-shot, chain-of-thought, and tree-of-thought prompting methods, showing the accuracy and p-values for each method.", "section": "A.3 TREE-OF-THOUGHT EXPERIMENTS"}, {"figure_path": "2410.21333/tables/table_20_0.html", "caption": "Table 1: Results contrasting zero-shot and CoT for artificial grammar learning.", "description": "The table presents the performance of nine different language models on an artificial grammar learning task, comparing their accuracy with zero-shot prompting versus chain-of-thought prompting, showing consistent performance decrease with CoT prompting.", "section": "4.1 Implicit Statistical Learning"}]