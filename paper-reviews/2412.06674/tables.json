[{"content": "| Method vs. Criterion | \u2780 | \u2781 | \u2782 | \u2783 |\n|---|---|---|---|---|\n| MobileNet Series [8, 9, 32] | \u2714 | \u2714 | \u271a | \u2718 |\n| MobileViT Series [17, 14, 32] | \u271a | \u271a | \u271a | \u2718 |\n| EdgeNeXt [2] | \u271a | \u2718 | \u2714 | \u2718 |\n| EdgeViT [55] | \u2714 | \u271a | \u271a | \u2718 |\n| RepViT [40] | \u2714 | \u2718 | \u2714 | \u2718 |\n| EfficientFormerV2 [1] | \u2714 | \u271a | \u2714 | \u2718 |\n| EfficientVMamba [65] | \u2718 | \u2718 | \u271a | \u2718 |\n| MogaNet [50] | \u2714 | \u2714 | \u2714 | \u2718 |\n| EMOv1 | \u2714 | \u2714 | \u2714 | \u2718 |\n| EMOv2 | \u2714 | \u2714 | \u2714 | \u2714 |", "caption": "TABLE I: Criterion comparison for current efficient models. \u2780: Usability; \u2781: Uniformity; \u2782: Efficiency and Effectiveness; \u2783: Generalization. \u2714: Satisfied. \u271a: Partially satisfied. \u2718: Unsatisfied.", "description": "This table compares several efficient models based on four criteria: Usability (ease of implementation and optimization), Uniformity (consistency and simplicity of model design), Efficiency and Effectiveness (balance between model parameters, computational cost, and accuracy), and Generalization (ability to adapt to various perception and generation tasks). Each criterion is marked as satisfied (\u2714), partially satisfied (\u271a), or unsatisfied (\u2718) for each model.", "section": "3 METHODOLOGY"}, {"content": "Module | #Params | FLOPs | MPL\n---|---|---|---\nMHSA | 4(C+1)C | 8C<sup>2</sup>L+4CL<sup>2</sup>+3L<sup>2</sup> | O(1)\nW-MHSA | 4(C+1)C | 8C<sup>2</sup>L+4CLl+3Ll | O(Inf)\nConv | (Ck<sup>2</sup>/G+1)C | (2Ck<sup>2</sup>/G)LC | O(2W/(k-1))\nDW-Conv | (k<sup>2</sup>+1)C | (2k<sup>2</sup>)LC | O(2W/(k-1))", "caption": "TABLE II: Complexity and Maximum Path Length analysis of modules. Input/output feature maps are in \u211dC\u00d7W\u00d7Wsuperscript\u211d\ud835\udc36\ud835\udc4a\ud835\udc4a\\mathbb{R}^{C\\times W\\times W}blackboard_R start_POSTSUPERSCRIPT italic_C \u00d7 italic_W \u00d7 italic_W end_POSTSUPERSCRIPT, L=W2\ud835\udc3fsuperscript\ud835\udc4a2L=W^{2}italic_L = italic_W start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, l=w2\ud835\udc59superscript\ud835\udc642l=w^{2}italic_l = italic_w start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, W\ud835\udc4aWitalic_W and w\ud835\udc64witalic_w are feature map size and window size, while k\ud835\udc58kitalic_k and G\ud835\udc3aGitalic_G are kernel size and group number.", "description": "This table details the computational complexity and maximum path length of different modules used in the paper, specifically focusing on the relationship between parameters, FLOPs (floating-point operations), and the dimensions of the input feature maps.  The variables defined (C, W, w, k, G, L, l) represent the number of channels, feature map width/height, window width/height, kernel size, number of groups in convolution, total number of pixels in feature map, and total number of pixels in window, respectively.  Understanding these relationships is crucial for evaluating the efficiency of different components in lightweight model design.", "section": "3.2.2 Micro Designs for Deducted iRMB"}, {"content": "| Model | #Params \u2193 | FLOPs \u2193 | Top-1 \u2191 |\n|---|---|---|---|\n| DeiT-Tiny [43] | 5.7M | 1.3G | 72.2 |\n| DeiT-Tiny w / iRMB | 4.9M | 1.1G | 74.3 +2.1%\u2191 |\n| DeiT-Tiny w / i\u00b2RMB | 5.0M | 1.3G | 75.0 +2.8%\u2191 |\n| PVT-Tiny [19] | 13.2M | 1.9G | 75.1 |\n| PVT-Tiny w / iRMB | 11.7M | 1.8G | 75.4 +0.3%\u2191 |\n| PVT-Tiny w / i\u00b2RMB | 11.9M | 1.9G | 76.1 +1.0%\u2191 |", "caption": "TABLE III: Toy experiments for assessing iRMB and i2RMB.", "description": "This table presents the results of toy experiments conducted to evaluate the performance of two types of Inverted Residual Mobile Blocks (iRMB and i2RMB).  The experiments involve replacing the transformer blocks in DeiT-Tiny and PVT-Tiny models with the iRMB and i2RMB blocks respectively. The table shows the number of parameters, FLOPs, and Top-1 accuracy achieved by each model configuration.", "section": "3.2.2 Micro Designs for Deducted iRMB"}, {"content": "| Manner | #Params. | FLOPs | Top1 | Throughput |\n|---|---|---|---|---|\n| Parallel | 5.1M | 964M | 78.1 | 1618.4 |\n| Cascaded (Ours) | 5.1M | 903M | 78.3 | 1731.7 |", "caption": "TABLE IV: Core configurations of EMOv2 variants.", "description": "This table details the core configurations used to create different versions of the EMOv2 model.  These configurations control aspects of the model's architecture, allowing for variations in the number of parameters and computational cost, thereby influencing the model's performance and suitability for different resource constraints.  The configurations specify the depth, embedding dimension, and expansion ratio of the model's components.", "section": "3.3 Parameter-Efficient Extension (EMOv2)"}, {"content": "| Items | EMoV2-1M | EMoV2-2M | EMoV2-5M |\n|---|---|---|---|\n| Depth | [ 2, 2, 8, 3 ] | [ 3, 3, 9, 3 ] | [ 3, 3, 9, 3 ] |\n| Emb. Dim. | [ 32, 48, 80, 180 ] | [ 32, 48, 120, 200 ] | [ 48, 72, 160, 288 ] |\n| Exp. Ratio | [ 2.0, 2.5, 3.0, 3.5 ] | [ 2.0, 2.5, 3.0, 3.5 ] | [ 2.0, 3.0, 4.0, 4.0 ] |", "caption": "TABLE V: Ablation study on components in iRMB/i2RMB.", "description": "This table presents an ablation study analyzing the impact of individual components within the Improved Inverted Residual Mobile Block (iRMB) and its enhanced version, the i2RMB.  It shows the performance (Top-1 accuracy) achieved by removing either the EW-MHSA (Expanded Window Multi-Head Self-Attention) or the DW-Conv (Depthwise Convolution) component, or both, from the baseline model. This allows for a quantitative assessment of the contribution of each component to the overall model accuracy. The experiment is conducted for both EMOv1 and EMOv2.", "section": "3.2.2 Micro Designs for Deducted iRMB"}, {"content": "| EMOV1 [13] |  |  | EMOV2 |  |  |\n|---|---|---|---|---|---|---|\n| EW-MHSA | DW-Conv | 73.5 | SEW-MHSA | DW-Conv | 73.5 |\n| \u2718 | \u2718 | 73.5 | \u2718 | \u2718 | 73.5 |\n| \u2714 | \u2718 | 76.6+3.1\u2191 | \u2714 | \u2718 | 77.7+4.2\u2191 |\n| \u2718 | \u2714 | 77.6+4.1\u2191 | \u2718 | \u2714 | 78.1+4.6\u2191 |\n| \u2714 | \u2714 | 78.4+4.9\u2191 | \u2714 | \u2714 | 79.4+5.9\u2191 |", "caption": "TABLE VI: Performance of our EMOv1/v2 with different lightweight model training recipes.", "description": "This table compares the performance of EMOv1 and EMOv2 models trained using various lightweight training recipes.  It highlights how different training methodologies impact the final accuracy of the models, demonstrating their robustness or sensitivity to different training strategies. The results are likely presented as top-1 accuracy on a benchmark dataset like ImageNet.", "section": "4.1 Image Classification"}, {"content": "| Recipe | MNetv3 [10] | DeiT [43] | EdgeNeXt [2] | Vim [64] | Ours |\n|---|---|---|---|---|---| \n| EMOV1 [13] | NaN | 78.1 | 78.3 | 77.9 | 78.4 |\n| EMOV2  | NaN | 78.8 | 79.1 | 78.5 | 79.4 |", "caption": "TABLE VII: Classification performance comparison among different kinds of backbones on ImageNet-1K dataset in terms of 5M-magnitude, as well as 1M-magnitude and 2M models. White, grey, orange, and blue backgrounds indicate CNN-based, Transformer-based, RNN-based, and our EMO series, respectively. This kind of display continues for all subsequent experiments. Gray indicates the results obtained from the original paper. Comprehensive suggested models are marked in bold. Unit: #Params with (M) and FLOPs with (M). Abbreviations: MNet \u2192\u2192\\rightarrow\u2192 MobileNet; MViT \u2192\u2192\\rightarrow\u2192 MobileViT; MFormer \u2192\u2192\\rightarrow\u2192 MobileFormer. \u2217\u2217\\ast\u2217: Neural Architecture Search (NAS) for elaborate structures. \u2020\u2020\\dagger\u2020: Using knowledge distillation. \u2021\u2021\\ddagger\u2021: Re-parameterization strategy.\n\u2217\u2217\\ast\u2217: Using stronger training strategy displayed in Tab.\u00a0XVII(e).", "description": "Table VII presents a comparison of classification performance on the ImageNet-1K dataset for various lightweight models, specifically focusing on those with parameter counts around 1M, 2M, and 5M.  The table categorizes models based on their architecture: CNN-based (white background), Transformer-based (gray background), RNN-based (orange background), and the authors' proposed EMO models (blue background).  Results from the original papers are shown in gray, while the authors highlight their recommended models in bold.  Key details, such as the number of parameters (#Params) and floating-point operations (FLOPS), are included, along with Top-1 accuracy.  Additional notes clarify the use of specialized training techniques like Neural Architecture Search (NAS), knowledge distillation (KD), and re-parameterization, allowing for better interpretation of the results.", "section": "4.1 Image Classification"}, {"content": " | Model | #Params \u2193 | FLOPs \u2193 | Reso. | Top-1 | Venue |\n|---|---|---|---|---|---|---|\n| <img src=\"https://arxiv.org/html/2412.06674/figure/1m_magnitude.png\" width=\"18.0pt\" height=\"84.4pt\" style=\"vertical-align:-0.0pt;\"> | MNetv1-0.50 [8] | 1.3 | 149 | 224<sup>2</sup> | 63.7 | arXiv\u20191704 |\n|  | MNetv3-L-0.50 [10] | 2.6 | 69 | 224<sup>2</sup> | 68.8 | ICCV\u201919 |\n|  | MViTv1-XXS [17] | 1.3 | 364 | 256<sup>2</sup> | 69.0 | ICLR\u201922 |\n|  | MViTv2-0.5 [14] | 1.4 | 466 | 256<sup>2</sup> | 70.2 | arXiv\u201922 |\n|  | EdgeNeXt-XXS [2] | 1.3 | 261 | 256<sup>2</sup> | 71.2 | ECCVW\u201922 |\n|  | EATFormer-Mobile [24] | 1.8 | 360 | 224<sup>2</sup> | 69.4 | IJCV\u201924 |\n|  | \u2729 EMOV1-1M [13] | 1.3 | 261 | 224<sup>2</sup> | 71.5 | ICCV\u201923 |\n|  | \u2605 **EMOv2-1M** | 1.4 | 285 | 224<sup>2</sup> | **72.3** | - |\n|  | \u2605 **EMOv2-1M\u2020** | 1.4 | 285 | 224<sup>2</sup> | **73.5** | - |\n| <img src=\"https://arxiv.org/html/2412.06674/figure/2m_magnitude.png\" width=\"18.0pt\" height=\"76.1pt\" style=\"vertical-align:-0.0pt;\"> | MNetv2-1.40 [9] | 6.9 | 585 | 224<sup>2</sup> | 74.7 | CVPR\u201918 |\n|  | MNetv3-L-0.75 [10] | 4.0 | 155 | 224<sup>2</sup> | 73.3 | ICCV\u201919 |\n|  | FasterNet-T0 [93] | 3.9 | 340 | 224<sup>2</sup> | 71.9 | CVPR\u201923 |\n|  | GhostNetV3-0.5x [41] \u2020,\u2021 | 2.7 | 48 | 224<sup>2</sup> | 69.4 | arXiv\u20192404 |\n|  | MNetv4-Conv-S [42] \u2217\u2020 | 3.8 | 200 | 224<sup>2</sup> | 73.8 | arXiv\u20192404 |\n|  | MoCoViT-1.0 [94] | 5.3 | 147 | 224<sup>2</sup> | 74.5 | arXiv\u201922 |\n|  | PVTv2-B0 [20] | 3.7 | 572 | 224<sup>2</sup> | 70.5 | CVM\u201922 |\n|  | MViTv1-XS [17] | 2.3 | 986 | 256<sup>2</sup> | 74.8 | ICLR\u201922 |\n|  | MFormer-96M [33] | 4.6 | 96 | 224<sup>2</sup> | 72.8 | CVPR\u201922 |\n|  | EdgeNeXt-XS [2] | 2.3 | 538 | 256<sup>2</sup> | 75.0 | ECCVW\u201922 |\n|  | EdgeViT-XXS [55] | 4.1 | 557 | 256<sup>2</sup> | 74.4 | ECCV\u201922 |\n|  | tiny-MOAT-0 [75] | 3.4 | 800 | 224<sup>2</sup> | 75.5 | ICLR\u201923 |\n|  | EfficientViT-M1 [95] | 3.0 | 167 | 224<sup>2</sup> | 68.4 | CVPR\u201923 |\n|  | EfficientFormerV2-S0 [1] \u2217\u2020 | 3.5 | 400 | 224<sup>2</sup> | 75.7 | ICCV\u201923 |\n|  | EATFormer-Lite [24] | 3.5 | 910 | 224<sup>2</sup> | 75.4 | IJCV\u201924 |\n|  | \u2729 EMOV1-2M [13] | 2.3 | 439 | 224<sup>2</sup> | 75.1 | ICCV\u201923 |\n|  | \u2605 **EMOv2-2M** | 2.3 | 487 | 224<sup>2</sup> | **75.8** | - |\n|  | \u2605 **EMOv2-2M\u2020** | 2.3 | 487 | 224<sup>2</sup> | **76.7** | - |\n|  | MNetv3-L-1.25 [10] | 7.5 | 356 | 224<sup>2</sup> | 76.6 | ICCV\u201919 |\n|  | EfficientNet-B0 [12] | 5.3 | 399 | 224<sup>2</sup> | 77.1 | ICML\u201919 |\n|  | FasterNet-T2 [93] | 15.0 | 1910 | 224<sup>2</sup> | 78.9 | CVPR\u201923 |\n|  | RepViT [40] \u2021 | 6.8 | 1100 | 224<sup>2</sup> | 78.6 | CVPR\u201924 |\n|  | RepViT [40] \u2020,\u2021 | 6.8 | 1100 | 224<sup>2</sup> | 80.0 | CVPR\u201924 |\n|  | GhostNetV3-1.3x [41] \u2020,\u2021 | 8.9 | 269 | 224<sup>2</sup> | 79.1 | arXiv\u20192404 |\n|  | MNetv4-Conv-M [42] \u2217\u2020 | 9.2 | 1000 | 224<sup>2</sup> | 79.9 | arXiv\u20192404 |\n|  | DeiT-Ti [43] | 5.7 | 1258 | 224<sup>2</sup> | 72.2 | ICML\u201921 |\n|  | XCiT-T12 [57] | 6.7 | 1254 | 224<sup>2</sup> | 77.1 | NeurIPS\u201921 |\n|  | LightViT-T [53] | 9.4 | 700 | 224<sup>2</sup> | 78.7 | arXiv\u201922 |\n|  | MViTv1-S [17] | 5.6 | 2009 | 256<sup>2</sup> | 78.4 | ICLR\u201922 |\n|  | MViTv2-1.0 [14] | 4.9 | 1851 | 256<sup>2</sup> | 78.1 | arXiv\u201922 |\n|  | EdgeNeXt-S [2] | 5.6 | 965 | 224<sup>2</sup> | 78.8 | ECCVW\u201922 |\n|  | PoolFormer-S12 [52] | 11.9 | 1823 | 224<sup>2</sup> | 77.2 | CVPR\u201922 |\n|  | MFormer-294M [33] | 11.4 | 294 | 224<sup>2</sup> | 77.9 | CVPR\u201922 |\n|  | MPViT-T [96] | 5.8 | 1654 | 224<sup>2</sup> | 78.2 | CVPR\u201922 |\n|  | EdgeViT-XS [55] | 6.7 | 1136 | 256<sup>2</sup> | 77.5 | ECCV\u201922 |\n|  | tiny-MOAT-1 [75] | 5.1 | 1200 | 224<sup>2</sup> | 78.3 | ICLR\u201923 |\n|  | EfficientViT-M5 [95] | 12.4 | 522 | 224<sup>2</sup> | 77.1 | CVPR\u201923 |\n|  | \u2729 EMOV1-5M [13] | 5.1 | 903 | 224<sup>2</sup> | 78.4 | ICCV\u201923 |\n|  | \u2605 **EMOv2-5M** | 5.1 | 1035 | 224<sup>2</sup> | **79.4** | - |\n|  | \u2605 **EMOv2-5M\u2217** | 5.1 | 5627 | 512<sup>2</sup> | **82.9** | - |\n|  | Vim-Ti [64] | 7.0 | 1500 | 224<sup>2</sup> | 76.1 | ICML\u201924 |\n|  | EfficientVMamba-T [65] | 6.0 | 800 | 224<sup>2</sup> | 76.5 | arXiv\u20192403 |\n|  | EfficientVMamba-S [65] | 11.0 | 1300 | 224<sup>2</sup> | 78.7 | arXiv\u20192403 |\n|  | VRWKV-T [60] | 6.2 | 1200 | 224<sup>2</sup> | 75.1 | arXiv\u20192403 |\n|  | MSVMamba-S [97] | 7.0 | 900 | 224<sup>2</sup> | 77.3 | arXiv\u20192405 |\n|  | MambaOut-Femto [98] | 7.0 | 1200 | 224<sup>2</sup> | 78.9 | arXiv\u20192405 |", "caption": "TABLE VIII: Object detection performance by SSDLite\u00a0[10] on MS-COCO 2017\u00a0[99] dataset at 320\u00d7\\times\u00d7320 resolution. Abbreviated MNet/MViT: MobileNet/MobileViT. \u2020\u2020\\dagger\u2020: 512 \u00d7\\times\u00d7 512 resolution.", "description": "This table presents the performance of different models on the object detection task using the SSDLite [10] framework.  The models are evaluated on the MS-COCO 2017 [99] dataset at a resolution of 320x320 pixels. The results are shown in terms of mean Average Precision (mAP). For easier reference, MobileNet and MobileViT models are abbreviated as MNet and MViT, respectively. Some models were additionally evaluated at a higher resolution of 512x512 pixels; these results are marked with a \u2020 symbol.", "section": "4.2 Downstream Applications"}, {"content": "| Backbone | #Params \u2193 | FLOPs \u2193 | mAP |\n|---|---|---|---|\n| MNetv1 [8] | 5.1 | 1.3G | 22.2 |\n| MNetv2 [9] | 4.3 | 0.8G | 22.1 |\n| MNetv3 [10] | 5.0 | 0.6G | 22.0 |\n| MViTv1-XXS [17] | 1.7 | 0.9G | 19.9 |\n| MViTv2-0.5 [14] | 2.0 | 0.9G | 21.2 |\n| \u2729 EMOV1-1M [13] | 2.3 | 0.6G | 22.0 |\n| \u2605 **EMOv2-1M** | 2.4 | 0.7G | 22.3 |\n| \u2605 **EMOv2-1M\u2020** | 2.4 | 2.3G | 26.6 |\n| MViTv2-0.75 [14] | 3.6 | 1.8G | 24.6 |\n| \u2729 EMOV1-2M [13] | 3.3 | 0.9G | 25.2 |\n| \u2605 **EMOv2-2M** | 3.3 | 1.2G | 26.0 |\n| \u2605 **EMOv2-2M\u2020** | 3.3 | 4.0G | 30.7 |\n| ResNet50 [44] | 26.6 | 8.8G | 25.2 |\n| MViTv1-S [17] | 5.7 | 3.4G | 27.7 |\n| MViTv2-1.25 [14] | 8.2 | 4.7G | 27.8 |\n| EdgeNeXt-S [2] | 6.2 | 2.1G | 27.9 |\n| \u2729 EMOV1-5M [13] | 6.0 | 1.8G | 27.9 |\n| \u2605 **EMOv2-5M** | 6.0 | 2.4G | 29.6 |\n| \u2605 **EMOv2-5M\u2020** | 6.0 | 8.0G | 34.8 |", "caption": "TABLE IX: Object detection results by RetinaNet\u00a0[36] on MS-COCO 2017\u00a0[99] dataset.", "description": "Table IX presents the performance comparison of different lightweight backbones on the MS COCO 2017 object detection dataset using the RetinaNet framework.  The table shows the mean Average Precision (mAP) results for various object sizes (small, medium, large) and overall mAP, along with the number of parameters and FLOPs (floating point operations) for each backbone model. This allows for a quantitative comparison of the trade-off between model efficiency and detection accuracy across different lightweight architectures.", "section": "4.2 Downstream Applications"}, {"content": "| Backbone | #Params | mAP<sup>b</sup> | mAP<sup>b</sup><sub>50</sub> | mAP<sup>b</sup><sub>75</sub> | mAP<sup>b</sup><sub>S</sub> | mAP<sup>b</sup><sub>M</sub> | mAP<sup>b</sup><sub>L</sub> |\n|---|---|---|---|---|---|---|---| \n| ResNet-50 [44] | 37.7 | 36.3 | 55.3 | 38.6 | 19.3 | 40.0 | 48.8 |\n| PVTv1-Tiny [19] | 23.0 | 36.7 | 56.9 | 38.9 | 22.6 | 38.8 | 50.0 |\n| PVTv2-B0 [20] | 13.0 | 37.2 | 57.2 | 39.5 | 23.1 | 40.4 | 49.7 |\n| EdgeViT-XXS [55] | 13.1 | 38.7 | 59.0 | 41.0 | 22.4 | 42.0 | 51.6 |\n| \u2729 EMOV1-5M | 14.4 | 38.9 | 59.8 | 41.0 | 23.8 | 42.2 | 51.7 |\n| \u2605 **EMOV2-5M** | 14.4 | 41.5 | 62.7 | 44.1 | 25.7 | 45.5 | 55.5 |", "caption": "TABLE X: Object detection results by Mask RCNN\u00a0[100] on MS-COCO 2017\u00a0[99] dataset.", "description": "Table A3 presents a comprehensive comparison of object detection performance achieved by the Mask R-CNN model [100] using different backbones on the MS-COCO 2017 dataset [99].  It details the performance metrics, specifically mean Average Precision (mAP) across various Intersection over Union (IoU) thresholds, for different model sizes (1M, 2M, 5M, and 20M parameters) of the EMOv2 architecture.  The table allows for a detailed analysis of how the EMOv2 backbone impacts object detection accuracy at different scales and under different model configurations (with and without the enhanced training strategy denoted by '+').", "section": "4.2 Downstream Applications"}, {"content": "| Backbone | #Params \u2193 | mAP<sup>b</sup> | mAP<sup>b</sup><sub>50</sub> | mAP<sup>b</sup><sub>75</sub> | mAP<sup>b</sup><sub>S</sub> | mAP<sup>b</sup><sub>M</sub> | mAP<sup>b</sup><sub>L</sub> | mAP<sup>m</sup> | mAP<sup>m</sup><sub>50</sub> | mAP<sup>m</sup><sub>75</sub> | mAP<sup>m</sup><sub>S</sub> | mAP<sup>m</sup><sub>M</sub> | mAP<sup>m</sup><sub>L</sub> |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| PVT-Tiny [19] | 33.0 | 36.7 | 59.2 | 39.3 | - | - | - | 35.1 | 56.7 | 37.3 | - | - | - |\n| PVTv2-B0 [20] | 23.0 | 38.2 | 60.5 | 40.7 | - | - | - | 36.2 | 57.8 | 38.6 | - | - | - |\n| PoolFormer-S12 [52] | 31.0 | 37.3 | 59.0 | 40.1 | - | - | - | 34.6 | 55.8 | 36.9 | - | - | - |\n| MPViT-T [96] | 28.0 | 42.2 | 64.2 | 45.8 | - | - | - | 39.0 | 61.4 | 41.8 | - | - | - |\n| EATFormer-Tiny [24] | 25.9 | 42.3 | 64.7 | 46.2 | 25.5 | 45.5 | 55.1 | 39.0 | 61.5 | 42.0 | 22.4 | 42.0 | 52.7 |\n| \u2729 EMOV1-5M | 24.8 | 39.3 | 61.7 | 42.4 | 23.5 | 42.3 | 51.1 | 36.4 | 58.4 | 38.7 | 18.2 | 39.0 | 52.6 |\n| \u2605 **EMOV2-5M** | 24.8 | 42.3 | 64.3 | 46.3 | 25.8 | 45.6 | 56.3 | 39.0 | 61.4 | 42.1 | 20.0 | 41.8 | 57.0 |", "caption": "TABLE XI: Semantic segmentation results by DeepLabv3\u00a0[102], Semantic FPN\u00a0[103], SegFormer\u00a0[104], and PSPNet\u00a0[105] on ADE20K\u00a0[106] dataset at 512\u00d7\\times\u00d7512 resolution.", "description": "Table XI presents a comparison of semantic segmentation performance achieved by four different models (DeepLabv3, Semantic FPN, SegFormer, and PSPNet) on the ADE20K dataset.  The comparison is made using the same resolution (512x512) for all models, allowing for a direct assessment of their performance under the same conditions.  The table likely shows metrics such as mean Intersection over Union (mIoU), accuracy, and other relevant performance measures for each model, providing a quantitative comparison of the relative strengths of the various approaches to semantic segmentation.", "section": "4.2 Downstream Applications"}, {"content": "| Backbone | #Params \u2193 | FLOPs \u2193 | mIoU |\n|---|---|---|---|\n| **DeepLabv3 [102]** |  |  |  |\n| MViTv2-0.5 | 6.3 | 26.1G | 31.9 |\n| MViTv3-0.5 | 6.3 | - | 33.5 |\n| \u2729 EMOv1-1M | 5.6 | 2.4G | 33.5 |\n| \u2605 **EMOv2-1M** | 5.6 | 3.3G | 34.6 |\n| MNetv2 | 18.7 | 75.4G | 34.1 |\n| MViTv2-0.75 | 9.6 | 40.0G | 34.7 |\n| MViTv3-0.75 | 9.7 | - | 36.4 |\n| \u2729 EMOv1-2M | 6.9 | 3.5G | 35.3 |\n| \u2605 **EMOv2-2M** | 6.6 | 5.0G | 36.8 |\n| MViTv2-1.0 | 13.4 | 56.4G | 37.0 |\n| MViTv3-1.0 | 13.6 | - | 39.1 |\n| \u2729 EMOv1-5M | 10.3 | 5.8G | 37.8 |\n| \u2605 **EMOv2-5M** | 9.9 | 9.1G | 39.8 |\n| **Semantic FPN [103]** |  |  |  |\n| ResNet-18 | 15.5 | 32.2G | 32.9 |\n| \u2729 EMOv1-1M | 5.2 | 22.5G | 34.2 |\n| \u2605 **EMOv2-1M** | 5.3 | 23.4G | 37.1 |\n| ResNet-50 | 28.5 | 45.6G | 36.7 |\n| PVTv1-Tiny | 17.0 | 33.2G | 35.7 |\n| PVTv2-B0 | 7.6 | 25.0G | 37.2 |\n| \u2729 EMOv1-2M | 6.2 | 23.5G | 37.3 |\n| \u2605 **EMOv2-2M** | 6.2 | 25.1G | 39.9 |\n| ResNet-101 | 47.5 | 65.1G | 38.8 |\n| ResNeXt-101 | 47.1 | 64.7G | 39.7 |\n| PVTv1-Small | 28.2 | 44.5G | 39.8 |\n| EdgeViT-XXS | 7.9 | 24.4G | 39.7 |\n| EdgeViT-XS | 10.6 | 27.7G | 41.4 |\n| PVTv2-B1 | 17.8 | 34.2G | 42.5 |\n| \u2729 EMOv1-5M | 8.9 | 25.8G | 40.4 |\n| \u2605 **EMOv2-5M** | 8.9 | 29.1G | 42.3 |\n| **SegFormer [104]** |  |  |  |\n| MiT-B0 | 3.8 | 8.4G | 37.4 |\n| \u2605 **EMOv2-2M** | 2.6 | 10.3G | 40.2 |\n| MiT-B1 | 13.7 | 15.9G | 42.2 |\n| \u2605 **EMOv2-5M** | 5.3 | 14.4G | 43.0 |\n| **PSPNet [105]** |  |  |  |\n| MNetv2 | 13.7 | 53.1G | 29.7 |\n| MViTv2-0.5 | 3.6 | 15.4G | 31.8 |\n| \u2729 EMOv1-1M | 4.3 | 2.1G | 33.2 |\n| \u2605 **EMOv2-1M** | 4.2 | 2.9G | 33.6 |\n| MViTv2-0.75 | 6.2 | 26.6G | 35.2 |\n| \u2729 EMOv1-2M | 5.5 | 3.1G | 34.5 |\n| \u2605 **EMOv2-2M** | 5.2 | 4.6G | 35.7 |\n| MViTv2-1.0 | 9.4 | 40.3G | 36.5 |\n| \u2729 EMOv1-5M | 8.5 | 5.3G | 38.2 |\n| \u2605 **EMOv2-5M** | 8.1 | 8.6G | 39.1 |", "caption": "TABLE XII: Semantic segmentation results by UNet\u00a0[108] on HRF\u00a0[109] dataset at 256\u00d7\\times\u00d7256 resolution.", "description": "This table presents a comparison of semantic segmentation performance achieved by different models on the HRF dataset.  Specifically, it shows the results obtained using the UNet architecture with various backbones on images with a resolution of 256x256 pixels. The metrics used to evaluate performance are likely mDice, average accuracy (aAcc), and mean accuracy (mAcc). The table aims to demonstrate how the choice of backbone network (and thus, the underlying model architecture) influences the overall performance of the UNet model for semantic segmentation.  The focus is likely on the performance trade-off between the model's size/complexity and its accuracy in the segmentation task.", "section": "4.4 Visual Analysis between EMOv1/V2"}, {"content": "| Backbone | #Params \u2193 | FLOPs \u2193 | mDice | aAcc | mAcc |\n|---|---|---|---|---|---| \n| UNet-S5-D16 | 29.0 | 204G | 88.9 | 97.0 | 86.2 |\n| EdgeNeXt-S [2] | 23.7 | 221G | 89.1 | 97.1 | 87.5 |\n| \u2605 U-EMOv2-5M | 21.3 | 228G | 89.5 | 97.1 | 88.3 |", "caption": "TABLE XIII: Comparison with the state-of-the-art on Kinetics-400\u00a0[110] dataset with four input frames.", "description": "This table compares the performance of EMOv2-5M against other state-of-the-art models on the Kinetics-400 dataset, a benchmark for video recognition.  The comparison focuses on top-1 accuracy, using four input frames for each video.  It highlights EMOv2-5M's performance relative to models with varying numbers of parameters and FLOPs (floating point operations). This helps illustrate the efficiency and accuracy of EMOv2-5M for video classification tasks.", "section": "4.1 Image Classification"}, {"content": "| Backbone | #Params \u2193 | FLOPs \u2193 | Top-1 |\n|---|---|---|---|\n| UniFormer-XXS | 9.8 | 1.0G | 63.2 |\n| EdgeNeXt-S [2] | 6.8 | 1.2G | 64.3 |\n| \u2605 V-EMOv2-5M | 5.9 | 1.3G | 65.2 |", "caption": "TABLE XIV: Comparison with DiT\u00a0[67] for 400K training steps in generating 256\u00d7\\times\u00d7256 ImageNet\u00a0[79] images.", "description": "This table presents a comparison of the Fr\u00e9chet Inception Distance (FID) scores achieved by different models when generating 256x256 ImageNet images after 400K training steps.  It compares the performance of the proposed D-EMOv2 model against the baseline DiT model and its variations, showcasing the FID scores and the number of parameters and FLOPs used by each model.", "section": "4.1 Image Classification"}, {"content": "| Model | #Params \u2193 | FLOPs \u2193 | FID |\n|---|---|---|---|\n| DiT-S-2 | 33.0 | 5.5G | 68.4 |\n| SiT-S-2 | 33.0 | 5.5G | 57.6 |\n| D-EMOv2-S-2 | 24.6 | 5.4G | 46.3 |\n| DiT-B-2 | 130.5 | 21.8G | 43.5 |\n| SiT-B-2 | 130.5 | 21.8G | 33.5 |\n| D-EMOv2-B-2 | 96.1 | 19.9G | 24.8 |\n| DiT-L-2 | 458.1 | 77.5G | 23.3 |\n| SiT-L-2 | 458.1 | 77.5G | 18.8 |\n| D-EMOv2-L-2 | 334.8 | 69.3G | 11.2 |\n| DiT-XL-2 | 675.1 | 114.5G | 19.5 |\n| SiT-XL-2 | 675.1 | 114.5G | 17.2 |\n| D-EMOv2-XL-2 | 492.7 | 101.5G | 9.6 |", "caption": "TABLE XV: Efficiency and performance comparison of different depth and channel configurations.", "description": "This table presents a comparison of different model configurations, varying the depth and number of channels, while keeping the number of parameters relatively constant. It shows how these variations impact model efficiency (FLOPs) and performance (Top-1 accuracy).  This helps to understand the optimal balance between depth, channel count, and overall model performance.", "section": "4.3 Structural Ablation and Analysis"}, {"content": "| Depth | Channels | #Params | FLOPs | Top-1 |\n|---|---|---|---|---|\n| [2, 2, 10, 3] | [48, 72, 160, 288] | 5.3M | 1038M | 79.1 |\n| [2, 2, 12, 2] | [48, 72, 160, 288] | 5.0M | 1127M | 78.9 |\n| [4, 4, 8, 3] | [48, 72, 160, 288] | 5.1M | 1132M | 79.4 |\n| [3, 3, 9, 3] | [48, 72, 160, 288] | 5.1M | 1035M | 79.4 |\n| [2, 2, 12, 3] | [48, 72, 160, 288] | 5.1M | 1136M | 79.1 |\n| [2, 2, 8, 2] | [48, 72, 224, 288] | 5.1M | 1117M | 79.0 |", "caption": "TABLE XVI: Comparisons of throughput on CPU/GPU and running speed on mobile iPhone15 (ms).", "description": "This table compares the processing throughput (in images per second) of various models on CPU, GPU, and iPhone 15 mobile devices.  It also shows the model's running speed (in milliseconds) on an iPhone 15 and its Top-1 accuracy on the ImageNet-1K dataset. The models are categorized by parameter count, allowing comparison of performance across different model sizes.", "section": "4.3 Structural Ablation and Analysis"}, {"content": "| Method | #Params \u2193 | FLOPs | CPU | GPU | iPhone15 | Top-1 |\n|---|---|---|---|---|---|---|\n| EdgeNeXt-XXS | 1.3M | 261M | 73.1 | 2860.6 | 10.2 | 71.2 |\n| \u2729 **EMOv1-1M** | 1.3M | 261M | 158.4 | 3414.6 | 3.0 | 71.5 |\n| \u2605 **EMOv2-1M** | 1.4M | 285M | 147.1 | 3182.2 | 3.6 | 72.3 |\n| EdgeNeXt-XS | 2.3M | 538M | 69.1 | 1855.2 | 17.6 | 75.0 |\n| \u2729 **EMOv1-2M** | 2.3M | 439M | 126.6 | 2509.8 | 3.7 | 75.1 |\n| \u2605 **EMOv2-2M** | 2.3M | 487M | 118.2 | 3312.4 | 4.3 | 75.8 |\n| EdgeNeXt-S | 5.6M | 965M | 54.2 | 1622.5 | 22.5 | 78.8 |\n| \u2729 **EMOv1-5M** | 5.1M | 903M | 106.5 | 1731.7 | 4.9 | 78.4 |\n| \u2605 **EMOv2-5M** | 5.1M | 1035M | 93.9 | 1607.8 | 5.9 | 79.4 |", "caption": "TABLE XVII: Ablation studies and comparison analysis on ImageNet\u00a0[79]. All the experiments use EMOv2-5M as default structure.", "description": "This table presents ablation study results and performance comparisons on the ImageNet dataset.  Using the EMOv2-5M model as a baseline, various modifications and hyperparameter changes were tested, and their impacts on Top-1 accuracy, mean Average Precision (mAP) for object detection, and mean Intersection over Union (mIoU) for semantic segmentation are shown.  The table allows for detailed analysis of the individual components' contributions within the EMOv2 architecture and helps to assess the overall model's robustness to different training strategies and design choices.", "section": "4.3 Structural Ablation and Analysis"}, {"content": "| Mode | #Params \u2193 | FLOPs \u2193 | Top-1 | mAP | mIoU |\n|---|---|---|---|---|---| \n| None | 4.3M | 802M | 77.9 | 39.3 | 37.2 |\n| None (Scaling to 5.1M) | 5.1M | 991M | 78.4 | 39.6 | 37.7 |\n| Neighborhood Attention | 5.1M | 967M | 78.8 | 40.4 | 39.0 |\n| Remote Attention | 5.1M | 967M | 79.0 | 39.9 | 38.6 |\n| Spanning Attention | 5.1M | 1035M | 79.4 | 41.5 | 39.8 |", "caption": "TABLE XVIII: Core configurations of scaled EMOv2 variants.", "description": "This table details the core architectural configurations for scaled-up versions of the EMOv2 model.  It shows how the depth, embedding dimension, and expansion ratio of the model's building blocks change as the model's size increases (from 5M to 20M and then 50M parameters). This allows for an analysis of the scalability and efficiency of the EMOv2 architecture.", "section": "4.3 Structural Ablation and Analysis"}, {"content": "| Stage | #Params \u2193 | FLOPs \u2193 | Top-1 |\n|---|---|---|---|\n| S-4 | 4.7M | 832M | 78.5 |\n| S-34 | 5.1M | 1035M | 79.4 |\n| S-234 | 5.1M | 1096M | 79.3 |\n| S-1234 | 5.2M | 1213M | 79.1 |", "caption": "TABLE XIX: Evaluation of scaling capabilities of EMOv2 at 20M/50M magnitudes on ImageNet-1K dataset.", "description": "This table presents a comparison of the performance of EMOv2 models with 20 million and 50 million parameters on the ImageNet-1K dataset. It shows the number of parameters, FLOPs (floating point operations), resolution, and Top-1 accuracy for each model, along with a comparison to several other state-of-the-art models with similar parameter counts.  This demonstrates the scalability of the EMOv2 architecture and its ability to maintain high accuracy even with a significant increase in model size.", "section": "4.1 Image Classification"}, {"content": "| DPR | Top-1 | BS | Top-1 |\n|---|---|---|---|\n| 0.00 | 79.1 | 256 | 78.9 |\n| 0.03 | 79.2 | 512 | 79.2 |\n| 0.05 | 79.4 | 1024 | 79.4 |\n| 0.10 | 79.3 | 2048 | 79.4 |\n| 0.20 | 79.1 | 4096 | 79.4 |", "caption": "TABLE A1: Comparison of training recipes among popular and contemporary methods and we employ the same setting in all experiments. Please zoom in for clearer comparisons. Abbreviations: MNet \u2192\u2192\\rightarrow\u2192 MobileNet; MViT \u2192\u2192\\rightarrow\u2192 MobileViT; EFormerv2 \u2192\u2192\\rightarrow\u2192 EfficientFormerv2; GNet \u2192\u2192\\rightarrow\u2192 GhostNet; NAS: Neural Architecture Search; KD: Knowledge Distillation; #Repre.: Re-parameterization strategy.", "description": "Table A1 compares the training hyperparameters used by various popular and contemporary lightweight vision models.  It highlights the differences in training strategies employed by different models, offering insights into the methodologies used to train efficient vision models. This comparison focuses on key parameters such as the number of epochs, batch size, optimizer, learning rate and its decay schedule, use of warmup epochs, label smoothing, dropout, drop path rate, RandAugment, Mixup and Cutmix techniques, the use of erasing probability, the presence of positional embeddings, multi-scale samplers, the use of neural architecture search (NAS), knowledge distillation (KD), and re-parameterization strategies.  The table's goal is to showcase the diverse training regimes used in the field and to clearly state that the authors used a consistent, less intensive training approach for their own models, enabling more fair comparisons.", "section": "Appendix A"}, {"content": "| Size | #Params \u2193 | FLOPs \u2193 | Top-1 |\n|---|---|---|---|\n| K-1 | 4.8M | 969M | 78.6 |\n| K-3 | 4.9M | 991M | 79.0 |\n| K-5 | 5.1M | 1035M | 79.4 |\n| K-7 | 5.3M | 1102M | 79.2 |\n| K-9 | 5.5M | 1184M | 79.3 |\n| K-5 + D-2 | 5.1M | 1035M | 79.3 |\n| K-5 + D-3 | 5.1M | 1035M | 79.1 |\n| K-5 + DCNv2 [113] | 6.7M | 1625M | 78.5 |", "caption": "TABLE A2: Detailed object detection performance using SSDLite\u00a0[10] and RetinaNet\u00a0[36] of our EMOv2 on MS-COCO 2017\u00a0[99] dataset. \u2020\u2020\\dagger\u2020: 512 \u00d7\\times\u00d7 512 resolution.", "description": "Table A2 presents a detailed comparison of object detection performance using two different models, SSDLite and RetinaNet,  with our EMOv2 model on the MS-COCO 2017 dataset.  The table shows the performance across different scales of the EMOv2 model (1M, 2M, 5M, 20M parameters), and includes results at both 320x320 and 512x512 image resolutions.  The metrics used to evaluate the performance are mean Average Precision (mAP) for different object sizes (small, medium, large), as well as overall mAP. This allows for a comprehensive analysis of EMOv2's effectiveness at different scales and resolutions in object detection tasks.", "section": "Appendix B: Detailed Object Detection Results"}, {"content": "| Resolution | KD | Long Training | #Params. | FLOPs | Top-1 |\n|---|---|---|---|---|---| \n| 224 | \u2718 | \u2718 | 1.0G | 5.1M | 79.4 |\n| 256 | \u2718 | \u2718 | 1.4G | 5.1M | 79.9 |\n| 224 | \u2714 | \u2718 | 1.0G | 5.1M | 80.8 |\n| 224 | \u2718 | \u2714 | 1.0G | 5.1M | 80.4 |\n| 512 | \u2718 | \u2718 | 5.6G | 5.1M | 81.5 |\n| 512 | \u2714 | \u2718 | 5.6G | 5.1M | 82.4 |\n| 512 | \u2714 | \u2714 | 5.6G | 5.1M | 82.9 |", "caption": "TABLE A3: Detailed object detection performance using Mask RCNN\u00a0[100] of our EMOv2 on MS-COCO 2017\u00a0[99] dataset.", "description": "Table A3 presents a detailed analysis of object detection performance using the Mask RCNN model. It showcases the results obtained by employing different versions of the EMOv2 model (with varying numbers of parameters) on the MS-COCO 2017 dataset. The table provides a comprehensive evaluation, breaking down the performance across different metrics, allowing for a thorough comparison of the EMOv2 model's effectiveness in object detection compared to other state-of-the-art models.", "section": "APPENDIX"}, {"content": "| Items | EMOV2-20M | EMOV2-50M |\n|---|---|---|\n| Depth | [ 3, 3, 13, 3 ] | [ 5, 8, 20, 7 ] |\n| Emb. Dim. | [ 64, 128, 320, 448 ] | [ 64, 128, 384, 512 ] |\n| Exp. Ratio | [ 2.0, 3.0, 4.0, 4.0 ] | [ 2.0, 3.0, 4.0, 4.0 ] |", "caption": "TABLE A4: Detailed semantic segmentation performance using DeepLabv3\u00a0[102], Semantic FPN\u00a0[103], SegFormer\u00a0[104], and PSPNet\u00a0[105] to adequately evaluate our EMOv2 on ADE20K\u00a0[106] dataset.", "description": "Table A4 presents a detailed comparison of semantic segmentation performance achieved by different models on the ADE20K dataset.  It assesses the models' effectiveness using four popular semantic segmentation architectures: DeepLabv3, Semantic FPN, SegFormer, and PSPNet. The table focuses on demonstrating the performance of various sizes of the EMOv2 model (1M, 2M, 5M, and 20M parameters), highlighting its effectiveness across different scales.  The results include mIoU, aAcc, and mAcc, offering a comprehensive evaluation of the EMOv2's capabilities in semantic segmentation.", "section": "4.2 Downstream Applications"}, {"content": "| Model | #Params \u2193 | FLOPs \u2193 | Reso. | Top-1 | Venue |\n|---|---|---|---|---|---| \n| ResNet-50 [44, 114] | 25.5M | 4.1G | 224<sup>2</sup> | 80.4 | CVPR\u201916 |\n| ConvNeXt-T [115] | 28.5M | 4.5G | 224<sup>2</sup> | 82.1 | CVPR\u201922 |\n| PVTv2-B2 [20] | 25.3M | 4.0G | 224<sup>2</sup> | 82.0 | ICCV\u201921 |\n| Swin-T [21] | 28.2M | 4.5G | 224<sup>2</sup> | 81.3 | ICCV\u201921 |\n| PoolFormer-S36 [52] | 30.8M | 5.0G | 224<sup>2</sup> | 81.4 | CVPR\u201922 |\n| ViTAEv2-S [116] | 19.3M | 5.7G | 224<sup>2</sup> | 82.6 | IJCV\u201923 |\n| EATFormer-Small [24] | 24.3M | 4.3G | 224<sup>2</sup> | 83.1 | IJCV\u201924 |\n| \u2729 EMOV1-20M [13] | 20.5M | 3.8G | 224<sup>2</sup> | 82.0 | ICCV\u201923 |\n| \u2605 EMOV2-20M | 20.1M | 4.0G | 224<sup>2</sup> | 83.3 | - |\n| ResNet-152 [44, 114] | 60.1M | 11.5G | 224<sup>2</sup> | 82.0 | CVPR\u201916 |\n| Swin-B [21] | 87.7M | 15.5G | 224<sup>2</sup> | 83.5 | ICCV\u201921 |\n| PoolFormer-M48 [52] | 73.4M | 11.6G | 224<sup>2</sup> | 82.5 | CVPR\u201922 |\n| ViTAEv2-48M [116] | 48.6M | 13.4G | 224<sup>2</sup> | 83.8 | IJCV\u201923 |\n| EATFormer-Base [24] | 49.0M | 8.9G | 224<sup>2</sup> | 83.9 | IJCV\u201924 |\n| \u2605 EMOV2-50M | 49.8M | 8.8G | 224<sup>2</sup> | 84.1 | - |", "caption": "TABLE A5: Detailed semantic segmentation performance by adapting UNet with i2RMB on ADE20K\u00a0[106] dataset.", "description": "Table A5 presents a detailed comparison of semantic segmentation performance achieved using different models on the ADE20K dataset.  The table specifically focuses on the results obtained by adapting the UNet architecture with the Improved Inverted Residual Mobile Block (i2RMB) introduced in the paper. It shows how incorporating i2RMB impacts the model's performance metrics like mIoU, average accuracy (aAcc), and mean accuracy (mAcc). The comparison includes a baseline UNet model and the modified UNet with the i2RMB, offering insights into the effectiveness of i2RMB for semantic segmentation tasks.", "section": "4.2 Downstream Applications"}]