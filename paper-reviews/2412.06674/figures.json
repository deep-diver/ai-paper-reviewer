[{"figure_path": "https://arxiv.org/html/2412.06674/extracted/6056458/figs/teaser.png", "caption": "Figure 1: \nTop: Performance vs. Parameters with concurrent methods. Our EMOv2 achieves significant accuracy with fewer parameters. Superscript \u2217\u2217\\ast\u2217: The comparison methods employ more robust training strategies described in their papers, while ours uses the strategy mentioned in Tab.\u00a0XVII(e).\nBottom: The range of token interactions varies with different window attention mechanisms. Our EMOv2, with parameter-shared spanning attention in Sec.\u00a03.3.1, has a larger and correspondingly stronger Effective Receptive Field (ERF).", "description": "Figure 1 demonstrates EMOv2's parameter efficiency and superior performance compared to other lightweight models.  The top panel shows a plot of Top-1 accuracy versus the number of model parameters.  It highlights that EMOv2 achieves higher accuracy with significantly fewer parameters than competing approaches, even when those approaches use more advanced training techniques (marked with an asterisk). The bottom panel illustrates the concept of effective receptive field (ERF). It compares the range of token interactions for various window attention mechanisms. EMOv2's use of parameter-shared spanning attention results in a substantially larger ERF, signifying a greater ability to capture contextual information.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2412.06674/extracted/6056458/figs/emo2.png", "caption": "Figure 2: \nLeft: Abstracted unified Meta-Mobile Block from Multi-Head Self-Attention, Feed-Forward Network\u00a0[35], and Inverted Residual Block\u00a0[9] (c.f. Sec\u00a03.2.1). The inductive block can be deduced into specific modules using different expansion ratio \u03bb\ud835\udf06\\lambdaitalic_\u03bb and efficient operator \u2131\u2131\\mathcal{F}caligraphic_F.\nMiddle: We construct a family of vision models based on our i2RMB module: 4-stage EMOv2, composed solely of the deduced i2RMB (c.f. Sec\u00a03.2.2), for various perception tasks (image classification, detection, and segmentation in Sec.\u00a04.2). Additionally, we introduce the temporally extended V-EMO for video classification, the U-EMO based on an encoder-decoder architecture, and D-EMO to replace the Transformer block in DiT\u00a0[67]. These downstream models are typically built based on the i2RMB.\nRight: Performance comparison with different SoTAs on various tasks.", "description": "Figure 2 illustrates the core components and applications of the proposed EMOv2 model. The left panel shows the unified Meta-Mobile Block (MMBlock), a generalized building block derived from Multi-Head Self-Attention, Feed-Forward Networks, and Inverted Residual Blocks.  This MMBlock can be instantiated into specific modules (like the Improved Inverted Residual Mobile Block or i2RMB) by adjusting parameters (expansion ratio \u03bb and operator \u2131). The middle panel depicts how the 4-stage EMOv2 model is constructed using only the i2RMB, along with variations for different tasks such as video classification (V-EMO), encoder-decoder based image segmentation (U-EMO), and transformer replacement in DiT (D-EMO). The right panel provides a performance comparison of EMOv2 against other state-of-the-art models on various vision tasks.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2412.06674/x1.png", "caption": "Figure 3: Meta-paradigm comparison between our MMBlock and MetaFormer\u00a0[52]. We integrate \ud835\udcd5\ud835\udcd5{\\color[rgb]{0.69140625,0.140625,0.09375}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.69140625,0.140625,0.09375}\\bm{\\mathcal{F}}}bold_caligraphic_F into expended FFN to construct a more streamlined and shallower single-module block.", "description": "Figure 3 compares the proposed Meta Mobile Block (MMBlock) with the MetaFormer [52] architecture.  The MMBlock simplifies the MetaFormer by integrating the efficient operator \ud835\udcd5 into the expanded feed-forward network (FFN), resulting in a single-module block that is both shallower and more streamlined than the two-module MetaFormer design. This streamlined design reduces computational complexity and improves efficiency.", "section": "3.2 Efficient Model (EMOv1)"}, {"figure_path": "https://arxiv.org/html/2412.06674/extracted/6056458/figs/i2rmb.png", "caption": "Figure 4: Detailed implementation comparison of the Inverted Residual Mobile Block (iRMB in Sec.\u00a03.2.2) and the improved version (i2RMB in Sec.\u00a03.3.1). i2RMB designs a parameter-sharing spanning window attention mechanism that simultaneously models the interaction of distant and close window information.", "description": "Figure 4 illustrates the architectural differences between the original Inverted Residual Mobile Block (iRMB) and its enhanced counterpart, the i2RMB.  The iRMB uses a standard windowed attention mechanism, processing only information within a limited spatial window.  The improved i2RMB introduces a parameter-sharing spanning window attention mechanism. This enhancement allows the i2RMB to simultaneously consider both local (nearby) and distant spatial relationships within the input feature map, leading to a more comprehensive and potentially more accurate understanding of the context.  This is achieved without a significant increase in model parameters.", "section": "3.2.2 Micro Designs for Deducted iRMB"}, {"figure_path": "https://arxiv.org/html/2412.06674/extracted/6056458/figs/v1v2.png", "caption": "Figure 5: Downstream gains of EMOv2-5M over EMOv1-5M.", "description": "This figure compares the performance of EMOv2-5M and EMOv1-5M on various downstream tasks, including object detection using SSDLite and RetinaNet, and semantic segmentation using DeepLabv3, Semantic FPN, and PSPNet.  It visually demonstrates the improvement in accuracy achieved by EMOv2-5M over EMOv1-5M in these tasks. The improvements are shown as bar chart showing the mAP for object detection and mIoU for semantic segmentation.", "section": "4.2 Downstream Applications"}, {"figure_path": "https://arxiv.org/html/2412.06674/extracted/6056458/figs/qualitative.png", "caption": "(a)", "description": "This figure shows a comparison of the Inverted Residual Mobile Block (iRMB) and its improved version (i2RMB).  The iRMB uses a cascaded design of Multi-Head Self-Attention (MHSA) and convolution operations. The i2RMB introduces a parameter-sharing spanning window attention mechanism which models both local and distant features. The figure details the architectural differences and highlights the efficiency and effectiveness of the i2RMB.", "section": "3.2.2 Micro Designs for Deducted iRMB"}, {"figure_path": "https://arxiv.org/html/2412.06674/extracted/6056458/figs/cam.png", "caption": "(b)", "description": "This figure displays a comparison of different attention mechanisms' implementations within the Inverted Residual Mobile Block (iRMB).  It shows the original Window MHSA, a modified version called Spanning Window MHSA (SEW-MHSA), and their respective reverse operations.  SEW-MHSA is highlighted as the improved method that simultaneously models both near and distant feature interactions, aiming to overcome limitations of only modeling local neighbor interactions within a smaller window.  The diagram visually depicts the data flow and window partitioning strategies for each approach.", "section": "3.2.2 Micro Designs for Deducted iRMB"}, {"figure_path": "https://arxiv.org/html/2412.06674/extracted/6056458/figs/ablation_trajectory.png", "caption": "(c)", "description": "This figure shows a comparison of the improved Inverted Residual Mobile Block (i2RMB) and the original iRMB.  The i2RMB introduces a parameter-sharing spanning window attention mechanism.  This mechanism efficiently models both local (neighbor) and long-range (distant) feature interactions simultaneously, unlike the original iRMB which focuses solely on local interactions within a window. This improvement is crucial for enhancing the model's effective receptive field, especially in high-resolution tasks.  The diagram visually details the architectural differences between the two blocks, illustrating the added component that makes the i2RMB more efficient and powerful.", "section": "3.3 Parameter-Efficient Extension (EMOv2)"}]