[{"figure_path": "https://arxiv.org/html/2412.07774/x2.png", "caption": "Figure 1: Demonstrations of UniReal\u2019s versatile capabilities.\nAs a universal framework, UniReal supports a broad spectrum of image generation and editing tasks within a single model, accommodating diverse input-output configurations and generating highly realistic results, which effectively handle challenging scenarios, e.g., shadows, reflections, lighting effects, object pose changes, etc.", "description": "UniReal, a universal image generation and editing framework, is shown in Figure 1 to be capable of handling a wide variety of tasks with diverse input-output configurations.  The model produces highly realistic results and adeptly manages challenging conditions such as shadows, reflections, lighting variations, and alterations in object positioning.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.07774/x3.png", "caption": "Figure 2: \nOverall pipeline of UniReal.\nWe formulate image generation and editing tasks as discontinuous frame generation.\nFirst, input images are encoded into latent space by VAE encoder.\nThen, we patchify the image latent and noise latent into visual tokens.\nAfterward, we add index embeddings and image prompt\u00a0(asset/canvas/control) to the visual tokens.\nAt the same time, the context prompt and base prompt are processed by the T5 encoder.\nWe concatenate all the latent patches and text embeddings as a long 1D tensor and send them to the transformer.\nFinally, we decode the denoised results to get the desired output images.", "description": "UniReal processes image generation and editing tasks as a discontinuous video generation process.  Input images are first encoded into a latent space using a Variational Autoencoder (VAE). These latent representations, along with noise latents, are then divided into patches and converted into visual tokens.  Index embeddings and image prompts (classifying each image as an 'asset', 'canvas', or 'control' image) are added to these visual tokens. Concurrently, context and base prompts (textual instructions) are processed by a T5 text encoder to generate text embeddings.  Visual and text embeddings are combined into a single, long 1D tensor, which is fed into a transformer network with full attention. Finally, a VAE decoder reconstructs the denoised latent representations to produce the desired output images.", "section": "3.1 Model Design"}, {"figure_path": "https://arxiv.org/html/2412.07774/x4.png", "caption": "Figure 3: Data construction pipeline.\nStarting from raw videos, we use off-the-shelf models to construct data for different kinds of tasks.\nTwo examples of instructive editing and image customization data\u00a0(we segment objects from one frame to generate another frame) are given at the bottom of the image.", "description": "This figure illustrates the process of creating a dataset for training the UniReal model. It starts with raw video data.  Off-the-shelf models automatically extract information like video captions and segment objects from video frames. This information is used to construct various types of training data, including instructive image editing and image customization.  The process is shown visually, with two example datasets at the bottom that show how object segmentation from one frame is used to generate a second, modified frame.", "section": "3.2. Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2412.07774/x5.png", "caption": "Figure 4: Comparison results for instructive image editing.\nWe compare with the state-of-the-art methods OmniGen\u00a0[60], UltraEdit\u00a0[72], MGIE\u00a0[13], InstructPix2Pix\u00a0[1], and CosXL\u00a0[52].\nOur UniReal shows significant advantages in the aspects of instruction-following and generation quality.\nWe generate multiple results for each model and pick the best ones for demonstration.", "description": "Figure 4 presents a comparison of UniReal's performance on instructive image editing against several state-of-the-art methods: OmniGen, UltraEdit, MGIE, InstructPix2Pix, and CosXL.  The results showcase UniReal's superior ability to follow instructions accurately and produce high-quality image edits. Multiple results were generated for each method, with only the best example shown for each.", "section": "4.1 Comparisons with Existing Works"}, {"figure_path": "https://arxiv.org/html/2412.07774/x6.png", "caption": "Figure 5: Qualitative comparison for image customization.\nFor single subject, we compare with OmniGen\u00a0[60], Emu2\u00a0[53], BLIP-Diffusion\u00a0[27], ELITE\u00a0[58], and IP-Adapter\u00a0[68] with Flux\u00a0[62] backbone.\nFor multiple subjects, we chose OmniGen and Emu2 as competitors.\nThe listed prompts are in the formats of UniReal, and they are formulated according to the requirements of each method.", "description": "Figure 5 presents a qualitative comparison of image customization results.  The top row demonstrates single-subject customization, where UniReal's performance is compared against OmniGen, Emu2, BLIP-Diffusion, ELITE, and IP-Adapter (using the Flux backbone). The bottom row shows multi-subject customization, with comparisons made against OmniGen and Emu2.  The prompts used in this experiment are formatted according to UniReal's method and tailored to each model's specific input requirements to ensure a fair comparison.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.07774/x7.png", "caption": "Figure 6: Our preference rates against other methods evaluated by user studies.\nWe compare SuTI\u00a0[8], OmniGen\u00a0[60], UltraEdit\u00a0[72] and AnyDoor\u00a0[10] for different tasks.", "description": "Figure 6 presents a detailed comparison of user preference rates for UniReal against several state-of-the-art methods across three distinct image manipulation tasks: image customization, instructive editing, and object insertion.  For each task, user preferences were collected and compared against the results produced by SuTI [8], OmniGen [60], UltraEdit [72], and AnyDoor [10]. The bar chart visually represents the percentage of users who preferred UniReal's output for each task, providing a clear and quantitative assessment of UniReal's performance relative to existing approaches. This figure offers crucial insights into the effectiveness of UniReal's methodology and its advantages over alternative techniques in diverse image manipulation scenarios.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.07774/x8.png", "caption": "Figure 7: Comparison results for object insertion.\nOur method could automatically adjust the status of the reference object according to the environment\nand strictly preserve the background.\nOur method does not require any mask as input.", "description": "Figure 7 showcases the effectiveness of the UniReal model in object insertion tasks.  Unlike other methods which often require manual segmentation masks, UniReal automatically integrates the reference object into the target image, seamlessly adjusting its pose and attributes to match the context. The background is consistently maintained, avoiding artifacts and preserving its integrity. This demonstrates UniReal's capacity to perform complex object insertion without explicit guidance on object placement or segmentation.", "section": "4.1. Comparisons with Existing Works"}, {"figure_path": "https://arxiv.org/html/2412.07774/x9.png", "caption": "Figure 8: Effects of hierarchical prompt.\nThe same input could correspond to various types of targets when given different image prompts\u00a0(row\u00a01) and context prompts\u00a0(row\u00a02).", "description": "This figure demonstrates the impact of UniReal's hierarchical prompting system on image generation and editing.  The same input images are used in each row, but different results are obtained by changing the image prompt (row 1) and the context prompt (row 2). This highlights the ability of the system to achieve diverse output by altering prompts, even with the same base input.  The results shown in the figure include examples of varied effects achieved, such as addition or removal of objects, alteration of attributes, and changes in scene backgrounds.  It shows how careful prompt engineering enables controlling the outcome of the image generation or editing process within UniReal.", "section": "3.1 Model Design"}, {"figure_path": "https://arxiv.org/html/2412.07774/x10.png", "caption": "Figure 9: Ablation study for the training data.\nWe visualize the results for models that are trained on Video Frame2Frame dataset, task-specific expert dataset, and our multi-task full dataset.\nIt is impressive that the model trained only on video data could master many editing tasks\u00a0(e.g., add, remove, attribute/pose changing), even for tasks with multiple input images.", "description": "This ablation study investigates the impact of different training data on UniReal's performance. Three models were trained: one using only the Video Frame2Frame dataset (pairs of video frames with captions describing the changes), another using task-specific expert datasets (curated datasets for specific image editing tasks), and a third using a combination of both (the full dataset).  The results demonstrate that even a model trained solely on the Video Frame2Frame dataset, a less curated and more general source of data, can successfully perform a variety of image editing tasks including adding or removing objects, changing attributes, or adjusting poses. This finding highlights the effectiveness of the video data as a scalable and universal training approach for generalizable image editing and generation.", "section": "4.2. Analysis for the Core Components"}]