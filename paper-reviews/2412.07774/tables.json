[{"content": "| No. | Dataset | # Samples | Supporting Tasks |\n|---|---|---|---| \n| 1 | InstructP2P [1] | 300K | Universal Instructive Editing |\n| 2 | UltraEdit [72] | 500K | Universal Instructive Editing |\n| 3 | VTON-HD [11] | 10K | Virtual Try-on |\n| 4 | RefCOCO Series [24, 33] | 150K | Referring Segmentation |\n| 5 | T2I Generation (in house) | 300M | Text to Image Generation |\n| 6 | Instruct Editing (in house) | 2M | Universal Instructive Editing |\n| 7 | Object Insertion (in house) | 100K | Object Insertion with Reference |\n| 8 | Video Frame2Frame | 8M | Universal Instructive Editing |\n| 9 | Video Multi-object | 5M | Multi-subject Customization |\n| 10 | Video Object Insertion | 1M | Object Insertion with Reference |\n| 11 | Video ObjectAdd | 1M | Object Insertion with Prompt |\n| 12 | Video SEG | 5M | Referring Segmentation |\n| 13 | Video Control | 3M | Perception, Controllable Generation |", "caption": "Table 1: Statistics of datasets used for training.\nWe mix the existing datasets\u00a0(the first block) with our newly constructed\nvideo-based datasets\u00a0(the second block).", "description": "This table presents a summary of the datasets used to train the UniReal model. It's divided into two sections: existing datasets used in prior research and newly created video-based datasets.  The table lists each dataset's name, the number of samples it contains, and the specific image generation or editing tasks it supports. The combination of these datasets provides comprehensive training data for the model, covering a wide range of image manipulation tasks.", "section": "3.2. Dataset Construction"}, {"content": "| Method | CLIP<sub>dir</sub>\u2191 | CLIP<sub>im</sub>\u2191 | CLIP<sub>out</sub>\u2191 | L1\u2193 | DINO\u2191 | CLIP<sub>dir</sub>\u2191 | CLIP<sub>im</sub>\u2191 | CLIP<sub>out</sub>\u2191 | L1\u2193 | DINO\u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|\n| InstructPix2Pix [1] | 0.078 | 0.834 | 0.219 | 0.121 | 0.762 | 0.115 | 0.837 | 0.245 | 0.093 | 0.767 |\n| MagicBrush [69] | 0.090 | 0.838 | 0.222 | 0.100 | 0.776 | 0.123 | 0.883 | 0.261 | 0.058 | 0.871 |\n| PnP [55] | 0.028 | 0.521 | 0.089 | 0.304 | 0.153 | 0.025 | 0.568 | 0.101 | 0.289 | 0.220 |\n| Null-Text Inv. [35] | 0.101 | 0.761 | 0.236 | 0.075 | 0.678 | 0.121 | 0.752 | 0.263 | 0.077 | 0.664 |\n| UltraEdit [72] | 0.107 | 0.793 | 0.283 | 0.071 | 0.844 | - | 0.868 | - | 0.088 | 0.792 |\n| EMU Edit [48] | 0.109 | 0.859 | 0.231 | 0.094 | 0.819 | 0.135 | 0.897 | 0.261 | **0.052** | **0.879** |\n| ACE [15] | 0.086 | **0.895** | 0.274 | 0.076 | **0.862** | - | - | 0.284 | - | - |\n| OmniGen [60] | - | 0.836 | 0.233 | - | 0.804 | - | - | - | - | - |\n| PixWizard [29] | 0.104 | 0.845 | 0.248 | **0.069** | 0.798 | 0.124 | 0.884 | 0.265 | 0.063 | 0.876 |\n| UniReal (ours) | **0.127** | 0.851 | **0.285** | 0.099 | 0.790 | **0.151** | **0.903** | **0.308** | 0.081 | 0.837 |", "caption": "Table 2: Comparison results for instructive image editing on EMU Edit\u00a0[48] and MagicBrush\u00a0[69] test sets.\nWe list the task-specific models in the first block and some concurrent universal models in the second block.", "description": "This table presents a quantitative comparison of various models' performance on two instructive image editing datasets: EMU Edit and MagicBrush.  The models are categorized into task-specific models (designed for a single editing task) and concurrent universal models (designed to handle various image editing tasks).  The comparison uses metrics such as CLIP direction (CLIP dir), CLIP image (CLIP im), CLIP output (CLIP out), L1 distance (L1), and DINO similarity (DINO) to assess the models' ability to accurately follow instructions, maintain image consistency, and produce high-quality results.  These metrics evaluate alignment with instruction, preservation of source image details, and the quality of the edited output.", "section": "4.1. Comparisons with Existing Works"}, {"content": "| Model | **CLIP-T**\u2191 | **CLIP-I**\u2191 | **DINO**\u2191 |\n|---|---|---|---|\n| Oracle (reference images) | - | 88.5 | 77.4 |\n| Textual Inversion [35] | 0.255 | 0.780 | 0.569 |\n| DreamBooth [47] | 0.305 | 0.803 | 0.668 |\n| BLIP-Diffusion [27] | 0.302 | 0.805 | 0.670 |\n| ELITE [58] | 0.296 | 0.772 | 0.647 |\n| Re-Imagen [7] | 0.270 | 0.740 | 0.600 |\n| BootPIG [42] | 0.311 | 0.797 | 0.674 |\n| SuTI [8] | 0.304 | **0.819** | **0.741** |\n| OmniGen [60] (our test) | 0.320 | 0.810 | 0.693 |\n| UniReal (ours) | **0.326** | 0.806 | 0.702 |", "caption": "Table 3: Quantitative results for customized generation on DreamBench\u00a0[47]. We report the oracle results in the first row and compare both tuning methods and zero-shot methods.", "description": "This table presents a quantitative comparison of different methods for image generation customization on the DreamBench benchmark [47].  The evaluation focuses on customized generation, comparing both fine-tuning approaches and zero-shot methods.  The 'Oracle' row indicates the best possible performance, acting as an upper bound. Results are reported using several metrics, including CLIP-T (measuring text-image agreement), CLIP-I (comparing generated and reference images), and DINO (another image similarity metric). This allows for a comprehensive assessment of the effectiveness of various techniques in achieving high-fidelity image generation that adheres to the specified customizations.", "section": "4. Experiments"}, {"content": "| Method | MagicBrush Test set |  |  | DreamBench |  |  |  |\n|---|---|---|---|---|---|---|---| \n|  | CLIP<sub>dir</sub>\u2191 | CLIP<sub>out</sub>\u2191 | DINO\u2191 | CLIP-T\u2191 | CLIP-I\u2191 | DINO\u2191 |  |\n| w/o Context Prompt | 0.144 | 0.294 | 0.769 | 0.315 | 0.781 | 0.683 |  |\n| w/o Image Prompt | 0.136 | 0.305 | 0.809 | 0.295 | 0.782 | 0.698 |  |\n| only Expert Data | 0.139 | **0.310** | 0.788 | 0.309 | 0.790 | **0.708** |  |\n| UniReal-full | **0.151** | 0.308 | **0.837** | **0.326** | **0.806** | 0.702 |  |", "caption": "Table 4: Quantitative studies for our basic components on MagicBrush\u00a0[69] test sets, and DreamBench\u00a0[47].", "description": "Table 4 presents a quantitative analysis of the core components of the UniReal model.  It assesses the model's performance on two distinct benchmark datasets: MagicBrush [69] and DreamBench [47].  The table shows the impact of key components, such as the context prompt and image prompt, on various metrics, providing a detailed comparison of the model's performance with and without these components.  The metrics likely include scores evaluating the quality of image generation and editing based on the datasets' specific evaluation criteria.", "section": "4.2. Analysis for the Core Components"}]