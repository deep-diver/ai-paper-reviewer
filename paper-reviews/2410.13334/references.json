{"references": [{" publication_date": "2022", "fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "reason": "This paper is foundational in the field of reinforcement learning from human feedback (RLHF) for aligning LLMs.  RLHF is a crucial safety alignment technique discussed in the paper, and this work provides a detailed explanation of its methodology and challenges, making it a significant contribution to the background on LLM safety.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Michiel Bakker", "paper_title": "Fine-tuning language models to find agreement among humans with diverse preferences", "reason": "This paper is highly relevant to the study's focus on bias in safety alignment. It explores how fine-tuning LLMs to align with human preferences can introduce biases, directly addressing a core theme of the paper.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is a seminal work in the field of LLMs, introducing the concept of few-shot learning which is critical to understanding the capabilities and limitations of LLMs addressed in the study.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "reason": "This paper is highly relevant as it introduces a foundational technique (RLHF) used in safety alignment.  The study discusses how these alignment methods can introduce biases, and this work is directly relevant to understanding the context of that bias.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Gelei Deng", "paper_title": "Jailbreaker: Automated jailbreak across multiple large language model chatbots", "reason": "This paper directly addresses the issue of jailbreaking LLMs, providing an approach to automated attacks that the current study builds upon and contrasts with its own method.  Understanding this existing method helps to contextualize the novelty of the research presented.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Peng Ding", "paper_title": "A wolf in sheep's clothing: Generalized nested jailbreak prompts can fool large language models easily", "reason": "This work is highly relevant as it presents a sophisticated jailbreaking method.  The presented research builds upon the limitations of this method, offering a more scalable and effective alternative, thereby increasing the significance of this work.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Patrick Chao", "paper_title": "JailbreakBench: An open robustness benchmark for jailbreaking large language models", "reason": "The paper uses JailbreakBench dataset for experiments.  This dataset is a widely used benchmark in the field of LLM security, making this citation important for the reproducibility and validation of the study's results.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Xiaogeng Liu", "paper_title": "AutoDAN: Generating stealthy jailbreak prompts on aligned large language models", "reason": "This work is a direct antecedent to the current research, exploring methods for generating jailbreak prompts.  The current research builds upon and improves these techniques by providing a novel approach to exploit biases in LLMs.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Ethan Perez", "paper_title": "Red teaming language models with language models", "reason": "This paper discusses red teaming, a technique that is often used in safety alignment, and the paper focuses on the potential biases these techniques can introduce. Therefore, it is relevant as background for the study.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Daniel Kang", "paper_title": "Exploiting programmatic behavior of LLMs: Dual-use through standard security attacks", "reason": "This paper provides insights into the broader security implications of LLMs, which is crucial for understanding the context of the research on jailbreaks. It's a significant citation because it shows the relevance of this work to a wider field of study.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Raz Lapid", "paper_title": "Open sesame! universal black box jailbreaking of large language models", "reason": "This paper explores general methods for jailbreaking LLMs, and the proposed work builds upon these techniques by providing a more scalable method that exploits inherent biases in LLMs.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "John X Morris", "paper_title": "Reevaluating adversarial examples in natural language", "reason": "This work provides a critical perspective on adversarial attacks on LLMs, helping to establish the context for understanding the challenges in crafting effective and undetectable jailbreak attacks.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper details RLHF, a key technique used in LLM safety alignment. The study addresses how these methods can unintentionally introduce biases, so understanding RLHF is vital for understanding the context of the study.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Bertie Vidgen", "paper_title": "Learning from the worst: Dynamically generated datasets to improve online hate detection", "reason": "This paper focuses on improving hate speech detection, a related area relevant to ensuring the safety of LLMs. The study utilizes methods for detecting harmful content, which is directly applicable to the discussed jailbreaking techniques.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Alexander Wei", "paper_title": "Jailbroken: How does llm safety training fail?", "reason": "This paper examines the shortcomings of LLM safety training, a critical topic relevant to the research. Understanding these failures is critical for assessing the relevance and impact of the presented work.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yi Zeng", "paper_title": "Beear: Embedding-based adversarial removal of safety backdoors in instruction-tuned language models", "reason": "This paper directly tackles the issue of safety backdoors in LLMs, a problem closely related to jailbreaking.  Understanding this work is essential for properly positioning the current study within the wider context of LLM safety.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Andy Zou", "paper_title": "Universal and transferable adversarial attacks on aligned language models", "reason": "This work provides a comprehensive overview of adversarial attacks against aligned language models.  The research expands on these techniques, offering a new method for generating jailbreaks, thus making this citation crucial for understanding the progress in the field.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Maksym Andriushchenko", "paper_title": "Jailbreaking leading safety-aligned LLMs with simple adaptive attacks", "reason": "This paper is highly relevant because it presents a state-of-the-art jailbreaking technique. The current study uses this technique as a benchmark to compare the performance of their novel method, thus establishing the significance of their contribution.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Neel Jain", "paper_title": "Baseline defenses for adversarial attacks against aligned language models", "reason": "This work examines defense mechanisms against adversarial attacks on LLMs.  This citation is essential to provide a critical assessment of the effectiveness of the proposed PCDefense mechanism and place the study within the context of existing defense strategies.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Terry Yue Zhuo", "paper_title": "Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity", "reason": "This paper examines biases, robustness, reliability, and toxicity in LLMs.  This paper is relevant to the study because it examines aspects that are directly affected by the intentional biases introduced during safety alignment and which are subsequently exploited by jailbreak attacks.", "section_number": 4}]}