[{"figure_path": "2410.13334/figures/figures_2_0.png", "caption": "Figure 2: Illustration showcasing the difference in response between a standard prompt and a PCJailbreak prompt. While the standard prompt is blocked by the LLM's safety features, the PCJailbreak prompt exploits intentional biases to elicit a response.", "description": "The figure illustrates how a standard prompt is blocked by safety mechanisms, while a PCJailbreak prompt, leveraging intentional biases, successfully elicits a response from the LLM.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13334/figures/figures_6_0.png", "caption": "Figure 3: Overview of the PCJailbreak methodology. The same harmful prompt is used across different keywords representing contrasting groups to analyze variations in jailbreak success rates.*All keywords representing both privileged and underprivileged groups are generated by the LLM.", "description": "The figure illustrates the PCJailbreak methodology, showing how the same harmful prompt is used with different keywords representing privileged and marginalized groups to analyze variations in jailbreak success rates.", "section": "3.2 PCJAILBREAK APPROACH"}, {"figure_path": "2410.13334/figures/figures_7_0.png", "caption": "Figure 4: PCDefense adjusts inherent biases in LLMs that are exploited by PCJailbreak. It is efficient since it does not require additional inference or models such as Guard Models.", "description": "The figure illustrates the difference in LLM responses to a harmful prompt with and without the PCDefense prompt, showing how PCDefense mitigates the biases exploited by PCJailbreak attacks.", "section": "3.3 PCDEFENSE: PREVENTING JAILBREAKS WITHOUT ADDITIONAL INFERENCE OR GUARD MODELS"}]