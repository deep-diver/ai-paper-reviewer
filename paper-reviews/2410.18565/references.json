{"references": [{" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduced the Transformer architecture, a foundational model for many modern large language models, including Bielik 7B v0.1.  Its impact on the field of NLP is immense, making it a crucial reference for understanding the architecture of Bielik and its predecessors.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Joshua Ainslie", "paper_title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints", "reason": "This paper introduced Grouped-Query Attention (GQA), a technique used in Bielik 7B v0.1 to reduce computational complexity and memory usage in self-attention mechanisms.  The improved efficiency from GQA is crucial for training and using large language models.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Rewon Child", "paper_title": "Generating long sequences with sparse transformers", "reason": "This paper presented Sliding Window Attention, a technique that improves the efficiency of handling long sequences in transformer models.  Its inclusion in Bielik 7B v0.1 is significant for its ability to process longer contexts efficiently, a key aspect of its design.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "Yann Dauphin", "paper_title": "Language modeling with gated convolutional networks", "reason": "This paper introduced the SwiGLU activation function, which is used in Bielik 7B v0.1.  SwiGLU offers improved performance and trainability over traditional activation functions, enhancing the model's overall efficiency and effectiveness.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jianlin Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "reason": "This paper introduced Rotary Positional Embeddings (RoPE), incorporated into Bielik 7B v0.1.  RoPE provides advantages over absolute positional embeddings in capturing relative positions of tokens, improving the model's ability to handle longer sequences and improving overall performance.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jason Ansel", "paper_title": "PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation", "reason": "This paper details PyTorch 2 optimizations that improve the efficiency of training and deploying large language models.  It's important due to the emphasis on efficient training and deployment of Bielik, which was facilitated through the ALLaMo framework's use of PyTorch 2.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Albert Q. Jiang", "paper_title": "Mistral 7b", "reason": "This paper introduces the Mistral 7B v0.1 model, which served as the foundation for Bielik 7B v0.1.  This is a crucial reference because Bielik adapted and fine-tuned Mistral 7B v0.1, leveraging its architecture and pre-training, for Polish language processing.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Edward Beeching", "paper_title": "Open Ilm leaderboard (2023-2024)", "reason": "This paper details the Open LLM Leaderboard v1, a benchmark used to evaluate Bielik 7B v0.1's performance on various NLP tasks.  Its importance lies in its role as a standardized evaluation framework that provides comparable results across different models.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Leo Gao", "paper_title": "A framework for few-shot language model evaluation", "reason": "This paper presents the Im-evaluation-harness framework, a tool utilized for evaluating Bielik 7B v0.1's performance on the Open PL LLM Leaderboard.  Its importance is in providing a standardized and reproducible evaluation method.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Lianmin Zheng", "paper_title": "Judging Ilm-as-a-judge with mt-bench and chatbot arena", "reason": "This paper introduces the MT-bench, a benchmark used to evaluate the conversational abilities of language models.  Its importance to this paper is in its use as a benchmark to assess Bielik's performance in conversation tasks.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Krzysztof Ociepa", "paper_title": "Introducing apt3-1b-base: Polish language model", "reason": "This reference is crucial as it introduces the APT3 tokenizer, which was partially incorporated into Bielik 7B v0.1 to improve its handling of Polish tokens. The enhanced Polish tokenization capabilities are a core element of Bielik's design.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Daria Soboleva", "paper_title": "SlimPajama: A 627B token cleaned and deduplicated version of RedPajama", "reason": "This paper describes the SlimPajama dataset, which is a significant part of the pre-training data for Bielik 7B v0.1.  Its high-quality English data helps prevent catastrophic forgetting and improves the model's generalizability.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Teknium", "paper_title": "Openhermes 2.5: An open dataset of synthetic data for generalist Ilm assistants", "reason": "This paper introduces the OpenHermes-2.5 dataset, which is a significant part of Bielik 7B v0.1's post-training data.  Its high-quality instructions and dialogues contributed to enhancing the model's capabilities.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Arindam Mitra", "paper_title": "Orca-math: Unlocking the potential of slms in grade school math", "reason": "This paper introduces the orca-math-word-problems-200k dataset, another significant part of Bielik 7B v0.1's post-training data. This dataset further augmented Bielik's capabilities in math-related tasks.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zhengyan Shi", "paper_title": "Instruction tuning with loss over instructions", "reason": "This paper describes the masked token approach used in Bielik 7B v0.1's post-training. This approach improves training efficiency and focuses learning on the most relevant parts of the input, thereby enhancing the model's performance and reducing noise from less important tokens.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Diego Granziol", "paper_title": "Learning rates as a function of batch size: A random matrix theory approach to neural network training", "reason": "This paper provides theoretical justification for the adaptive learning rate strategy used in Bielik 7B v0.1\u2019s post-training.  The adaptive learning rate helps maintain consistent influence from each training sample regardless of length variations, improving model performance.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Ji Lin", "paper_title": "Awq: Activation-aware weight quantization for llm compression and acceleration", "reason": "This paper introduces Activation-aware Weight Quantization (AWQ), one of the quantization methods used to optimize Bielik 7B v0.1 for deployment on resource-constrained devices. This is important because it addresses model deployment challenges and showcases Bielik's adaptability.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Hicham Badri", "paper_title": "Half-quadratic quantization of large machine learning models", "reason": "This paper introduces Half-Quadratic Quantization (HQQ), another quantization method used in Bielik 7B v0.1.  HQQ is important as it provides a flexible approach to model compression, allowing for deployment on diverse hardware.", "section_number": 6}, {" publication_date": "2022", "fullname_first_author": "Elias Frantar", "paper_title": "GPTQ: Accurate post-training compression for generative pretrained transformers", "reason": "This paper introduces GPTQ (Accurate Post-Training Quantization for Generative Pre-trained Transformers), a quantization method used in Bielik 7B v0.1. GPTQ is important because it enables efficient compression of the model without significant performance loss.", "section_number": 6}]}