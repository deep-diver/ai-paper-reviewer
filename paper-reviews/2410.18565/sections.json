[{"page_end_idx": 2, "page_start_idx": 1, "section_number": 1, "section_title": "Introduction", "details": {"details": "The rapid advancement in NLP has led to increasingly sophisticated language models capable of understanding and generating human-like text.  However, developing high-performing models for less-resourced languages like Polish remains challenging due to limited large, diverse datasets and computational resources. Existing Polish language models, such as TRURL 2 and Qra, have made progress but are limited by the size of their training data.  This paper introduces Bielik 7B v0.1, a new state-of-the-art Polish language model designed to address these challenges.", "first_cons": "Limited large, diverse datasets for training.", "first_pros": "Rapid advancement in NLP has led to sophisticated language models.", "keypoints": ["Challenges in Polish language model development due to limited resources", "Introduction of Bielik 7B v0.1, a new state-of-the-art Polish language model", "Existing Polish models (TRURL 2, Qra) have limitations in training data size", "Bielik addresses challenges through innovative techniques and curated Polish corpora"], "second_cons": "Computational resource constraints.", "second_pros": "Bielik 7B v0.1 offers a powerful tool for diverse linguistic applications.", "summary": "This paper introduces Bielik 7B v0.1, a new 7-billion parameter Polish language model addressing key challenges in language model development for low-resource languages."}}, {"page_end_idx": 3, "page_start_idx": 2, "section_number": 2, "section_title": "Model and Tokenizer", "details": {"details": "The Bielik 7B v0.1 model is based on the Transformer architecture and incorporates several advanced techniques to improve performance.  **Key architectural decisions** include using SwiGLU activation function, Rotary Positional Embeddings, and Root Mean Square Layer Normalization.  A **causal mask** is used in self-attention to maintain autoregressive properties.  **Grouped-query attention** reduces computational complexity.  **Sliding Window Attention** handles longer sequences more efficiently. The model was adapted from Mistral 7B v0.1 due to resource constraints and then further pre-trained. The tokenizer is based on the Mistral 7B tokenizer with additions from the APT3 tokenizer to improve handling of Polish tokens.  Experiments show that this approach is better at handling Polish than original multilingual tokenizers.", "first_cons": "The decision to adapt an existing model (Mistral 7B v0.1) rather than training from scratch might limit the model's ultimate potential and the choice may reflect resource limitations more than a deliberate design decision.", "first_pros": "Utilizing an existing, well-performing model as a base (Mistral 7B v0.1) saves significant time and resources compared to training a model from scratch. The Apache 2.0 license of Mistral allows for flexibility and ease of use.", "keypoints": ["Advanced techniques like SwiGLU, RoPE, RMSNorm enhance performance.", "Causal masking and grouped-query attention improve efficiency and quality.", "Sliding window attention enables handling of longer sequences.", "Tokenizer adapted from Mistral and APT3 for improved Polish handling.", "Model adapted from Mistral 7B v0.1 due to data and resource limitations."], "second_cons": "Ambiguity in merging tokens during the tokenization process, due to overlapping vocabularies between the base and added tokenizers, caused issues in text generation.", "second_pros": "The combined tokenizer demonstrates superior handling of Polish compared to using only multilingual models.  The lower token count compared to other models indicates more efficient processing.", "summary": "The Bielik 7B v0.1 model uses the Transformer architecture with advanced techniques like SwiGLU and RoPE, and a tokenizer adapted for Polish from Mistral 7B and APT3, to achieve efficient and high-quality Polish language processing."}}, {"page_end_idx": 4, "page_start_idx": 3, "section_number": 3, "section_title": "Pre-training", "details": {"details": "The pre-training phase aimed to enhance the model's Polish language capabilities using a diverse selection of high-quality Polish texts.  These texts underwent rigorous cleaning and quality evaluation to ensure high standards.  A novel, diverse, and high-quality dataset was created, primarily from Polish language texts.  The dataset was supplemented with English texts from the SlimPajama dataset to mitigate catastrophic forgetting and improve adaptation to the Polish language.  The final training dataset consisted of 36 billion tokens, representing a balance between Polish and English texts.  A crucial step was the quality evaluation of the data.  9000 documents were manually labeled with quality levels (HIGH, MEDIUM, LOW), and stylometric features were extracted and fed into an XGBoost classifier.  The classifier's output helped determine which data to include in the final pre-training set, resulting in a dataset of the highest possible quality for training.", "first_cons": "The need to manually curate and evaluate 9000 documents to ensure data quality is a time-consuming process.", "first_pros": "**High-quality Polish texts** were used for training, ensuring the model's ability to accurately represent the language.", "keypoints": ["A large, high-quality dataset of Polish texts was created, supplemented by English data to prevent catastrophic forgetting.", "Rigorous data cleaning and quality assessment, including manual annotation of 9000 documents and use of an XGBoost classifier, were performed.", "The final training dataset consisted of 36 billion tokens, balancing Polish and English texts for optimal model performance.", "The focus was on developing a model with superior capabilities in Polish, reflecting the importance of language specificity in NLP development."], "second_cons": "The reliance on manual data cleaning and quality evaluation is a major bottleneck, limiting scalability and potentially introducing human bias.", "second_pros": "The multilingual approach (using both Polish and English data) helps prevent issues related to catastrophic forgetting and enhances the model's generalizability.", "summary": "The pre-training of Bielik 7B v0.1 involved creating a large, high-quality dataset of Polish texts, supplemented by English data, and employing rigorous quality control measures to ensure optimal model performance."}}, {"page_end_idx": 6, "page_start_idx": 4, "section_number": 4, "section_title": "Post-training", "details": {"details": "The post-training phase focused on enhancing the model's abilities in various areas using a combination of manually annotated and automatically generated data.  To address the issue of varying data quality, **masked tokens** were employed, and an **adaptive learning rate** and **weighted instruction cross-entropy loss** were implemented to improve learning efficiency and prioritize high-quality instructions.  The model was fine-tuned on over 2.3 million instructions amounting to more than 700 million tokens, integrating both manually verified and automatically generated instruction-response pairs.  The training involved a cosine decay learning rate schedule, mixed precision with bfloat16, and a global batch size of 128.  The choice of existing models was driven by the need to manage computational constraints and the unavailability of sufficiently large high-quality Polish datasets.", "first_cons": "Varying quality of training instructions negatively impacts model performance.  The availability of sufficiently large and high-quality open datasets for Polish instruction and dialogue is limited.  .", "first_pros": "Adaptive learning rate and weighted cross-entropy loss functions improve model training efficiency by mitigating variations in the quality of training data.   Masked token approach filters out noise from user instruction and control tokens. Using both manually annotated and automatically generated data improves the robustness and coverage of the training dataset.", "keypoints": ["**Adaptive Learning Rate** and **Weighted Instruction Cross-Entropy Loss** improve training efficiency.", "**Masked Tokens** filter noise from user instructions.", "Combined manually annotated and automatically generated data.", "Over 2.3 million instructions used for fine-tuning.", "Addressed the challenge of data quality variation."], "second_cons": "Despite efforts to clean the data, the model might still produce inaccurate, biased, or offensive content.", "second_pros": "The post-training phase significantly improved the model's capabilities in various areas, including mathematics, logical reasoning, and instruction following.", "summary": "Post-training of the Bielik model focused on enhancing its performance across various tasks by employing advanced training techniques, including an adaptive learning rate and weighted loss function to address the issue of variable data quality, ultimately improving its capabilities."}}, {"page_end_idx": 7, "page_start_idx": 7, "section_number": 5, "section_title": "Evaluations", "details": {"details": "The Open PL LLM Leaderboard evaluates Polish language models across various NLP tasks like sentiment analysis, categorization, and text classification, but it does not assess conversational skills.  Bielik 7B v0.1 achieved a top score in the RAG Reader task, outperforming others by 9 percentage points.  However, in subjective conversational ability tests, it showed better performance than models with higher average scores. The Polish MT-Bench, designed for evaluating conversational abilities in two-step conversations, was also used for evaluation. Bielik 7B v0.1 demonstrated strong results in Reasoning (6.15/10) and Role-playing (7.83/10) categories. These results indicate that the model is superior in certain tasks despite its relatively smaller size when compared to other models. The Open PL LLM Leaderboard's limitations in conversational assessment and the strong showing in MT-Bench highlights a need for comprehensive evaluation metrics.", "first_cons": "The Open PL LLM Leaderboard lacks evaluation of conversational abilities.", "first_pros": "Bielik 7B v0.1 achieved top scores in RAG Reader, exceeding other models by 9 percentage points.", "keypoints": ["**Open PL LLM Leaderboard results:** Bielik 7B v0.1 performed exceptionally well in the RAG Reader task, showcasing strength in specific NLP tasks.", "**Polish MT-Bench results:** Bielik 7B v0.1 excelled in Reasoning and Role-playing, demonstrating strong conversational capabilities.", "**Subjective evaluation:**  Bielik 7B v0.1 outperformed some higher-scoring models in subjective conversational ability tests.", "**Limitations of Leaderboard:** The benchmark does not fully assess conversational skills, underscoring the need for comprehensive evaluation metrics."], "second_cons": "The subjective conversational evaluations are not as rigorous as the benchmark scores.", "second_pros": "The MT-Bench results confirm Bielik 7B v0.1\u2019s strong conversational performance.", "summary": "Bielik 7B v0.1 shows strong performance in specific NLP tasks on the Open PL LLM Leaderboard and the Polish MT-Bench, particularly excelling in reasoning and role-playing, despite limitations in the benchmark's evaluation of conversational abilities."}}]