[{"figure_path": "2410.18565/tables/table_2_0.html", "caption": "Table 1: Model architecture.", "description": "Table 1 presents the key parameters of the Bielik 7B v0.1 model architecture, detailing the values for various components such as layers, model dimensions, attention heads, and context length.", "section": "2.1 Model Architecture"}, {"figure_path": "2410.18565/tables/table_3_0.html", "caption": "Table 2: Comparison of token count, characters per token (CpT), and tokens per word (TpW) for the preamble of the Constitution of the Republic of Poland in Polish and English versions, processed by various tokenizers: APT3 (dedicated Polish language tokenizer), Llama2 and Mistral v0.1 (multilingual tokenizers with minimal Polish support), and merged tokenizers Llama2 + APT3 and Mistral v0.1 + APT3.", "description": "Table 2 compares the performance of different tokenizers (APT3, Llama2, Mistral v0.1, and their combinations) in terms of token count, characters per token, and tokens per word when processing the Polish and English versions of the preamble of the Constitution of the Republic of Poland.", "section": "2 Model and Tokenizer"}, {"figure_path": "2410.18565/tables/table_4_0.html", "caption": "Table 3: Validation results for the XGBoost classifier model.", "description": "The table presents the precision, recall, and F1-score of the XGBoost classifier model used for quality evaluation of the pre-training data.", "section": "3.1.2 Quality Evaluation"}, {"figure_path": "2410.18565/tables/table_8_0.html", "caption": "Table 4: A comparison of the training performance between the TinyLlama implementation and the ALLaMo framework.", "description": "Table 4 compares the training throughput performance of the TinyLlama implementation and the ALLaMo framework across different GPU configurations and batch sizes.", "section": "4.4 Efficient Implementation"}, {"figure_path": "2410.18565/tables/table_9_0.html", "caption": "Table 5: Detailed comparison among Bielik 7B v0.1 and other representative open-source models", "description": "Table 5 compares the performance of Bielik 7B v0.1 against other open-source language models across various NLP tasks, including overall performance, RAG reranking, RAG reader, and perplexity.", "section": "5 Evaluations"}, {"figure_path": "2410.18565/tables/table_10_0.html", "caption": "Table 6: Polish MT-Bench results for various language models", "description": "Table 6 presents the Polish MT-Bench evaluation results for various language models, showing the Polish score, proportion of Polish responses, and the average score across all categories.", "section": "5.2 Polish MT-Bench"}, {"figure_path": "2410.18565/tables/table_10_1.html", "caption": "Table 7: Polish MT-Bench results for various language models by category", "description": "Table 7 presents a detailed breakdown of the Polish MT-Bench results, showing scores across eight different categories for each model.", "section": "5.2 Polish MT-Bench"}, {"figure_path": "2410.18565/tables/table_11_0.html", "caption": "Table 8: Comparison of quantization results for the Bielik 7B v0.1 model using imatrix: PPL - Perplexity, \u25b3PPL - change in perplexity, KLD - Kullback-Leibler Divergence, Mean \u25b3p - mean change in correct token probability, RMS \u25b3p - root mean square of change in token probabilities, Same top p - the percentage of instances where the quantized model and the FP16 model assign the highest probability to the same token.", "description": "Table 8 compares the performance of different quantization schemes (with and without imatrix) for the Bielik 7B v0.1 model across various bit-widths, using perplexity, Kullback-Leibler divergence, and other metrics.", "section": "Calibration and Evaluation of Quantized Models"}]