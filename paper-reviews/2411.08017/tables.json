[{"content": "| Representation | IoU | Number of Input Variables |\n|---|---|---|\n| Ground-truth SDF (256<sup>3</sup>) | 1.0 | 16,777,216 (~64MB) |\n| Point Cloud (Nichol et al., 2022a) | 0.8642 | 12,288 (~0.05MB) |\n| Latent Vectors (Jun & Nichol, 2023a) | 0.8576 | 1,048,576 (~4MB) |\n| Coarse Component (Hui et al., 2022) | 0.9531 | 97,336 (~0.4MB) |\n| Wavelet tree (Hui et al., 2024) | 0.9956 | 1,129,528 (~4.3MB) |\n| **WaLa** | 0.9780 | 6,912 (~0.03MB) |", "caption": "Table 1: \n3D representations compared on GSO dataset\u00a0(Downs et\u00a0al., 2022): Intersection over Union (IoU) for accuracy & number of input variables for generative models to evaluate complexity.", "description": "This table compares different 3D shape representations used in generative models, focusing on their performance on the GSO dataset and their complexity.  It shows the Intersection over Union (IoU) score, which measures the accuracy of the representation, and the number of input variables required for the generative model, which indicates the complexity. By comparing these two metrics, the table helps to understand the trade-offs between accuracy and complexity of various 3D shape representations for large-scale generative modeling.", "section": "2 RELATED WORK"}, {"content": "| Method | GSO Dataset LFD \u2193 | GSO Dataset IoU \u2191 | GSO Dataset CD \u2193 | MAS Dataset LFD \u2193 | MAS Dataset IoU \u2191 | MAS Dataset CD \u2193 |\n|---|---|---|---|---|---|---|\n| Poisson surface reconstruction (Kazhdan et al., 2006) | 3306.66 | 0.3838 | 0.0055 | 4565.56 | 0.2258 | 0.0085 |\n| Point-E SDF model (Nichol et al., 2022c) | 2301.96 | 0.6006 | 0.0037 | 4378.51 | 0.4899 | 0.0158 |\n| MeshAnything (Chen et al., 2024b) | 2228.62 | 0.3731 | 0.0064 | 2892.13 | 0.3378 | 0.0091 |\n| Make-A-Shape (Hui et al., 2024) | 2274.92 | 0.7769 | 0.0019 | 1857.84 | 0.7595 | 0.0036 |\n| WaLa(Ours) | **1114.01** | **0.9389** | **0.0011** | **1467.55** | **0.8625** | **0.0014** |", "caption": "Table 2: \nQuantitative comparison between different methods of point cloud to mesh generation. We present LFD, IOU and CD metrics. Our method, WaLa, outperforms the other methods on both GSO and MAS Validation datasets.", "description": "Table 2 presents a quantitative comparison of various methods used for generating 3D meshes from point cloud data.  The comparison uses three key metrics: Light Field Distance (LFD), Intersection over Union (IoU), and Chamfer Distance (CD). Lower LFD and CD values indicate better mesh quality, while higher IoU values suggest more accurate reconstruction of the original shape. The results demonstrate that the Wavelet Latent Diffusion (WaLa) method significantly outperforms other existing techniques on both the Google Scanned Objects (GSO) and MAS validation datasets.", "section": "4.2.1 Point Cloud-to-Mesh"}, {"content": "| Method | GSO Dataset LFD \u2193 | GSO Dataset IoU \u2191 | GSO Dataset CD \u2193 | MAS Dataset LFD \u2193 | MAS Dataset IoU \u2191 | MAS Dataset CD \u2193 |\n|---|---|---|---|---|---|---|\n| Nearest Neighbour Interpolation | 5158.63 | 0.1773 | 0.0225 | 5401.12 | 0.1724 | 0.0217 |\n| Trilinear Interpolation | 4666.85 | 0.1902 | 0.0361 | 4599.97 | 0.1935 | 0.0371 |\n| Make-A-Shape (Hui et al., 2024) | 1913.69 | 0.7682 | 0.0029 | 2566.22 | 0.6631 | 0.0051 |\n| WaLa(Ours) | **1544.67** | **0.8285** | **0.0020** | **1874.41** | **0.75739** | **0.0020** |", "caption": "Table 3: \nQuantitative evaluation on lower resolution voxel data (163superscript16316^{3}16 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT resolution) to mesh generation task. Our method, WaLa, surpasses traditional Nearest neighbour and Trilinear upsampling as well as data-centric method like Make-a-Shape.", "description": "This table presents a quantitative comparison of different methods for generating 3D meshes from low-resolution (16^3) voxel data.  The methods compared include traditional upsampling techniques (nearest neighbor and trilinear interpolation) and a data-centric approach (Make-a-Shape). The evaluation metrics used are Light Field Distance (LFD), Intersection over Union (IoU), and Chamfer Distance (CD). The results demonstrate that the Wavelet Latent Diffusion (WaLa) method significantly outperforms the other approaches in terms of mesh quality, as measured by these metrics.", "section": "4.2 Experiments"}, {"content": "| Method | Inference Time | GSO Dataset LFD\u2193 | GSO Dataset IoU\u2191 | GSO Dataset CD\u2193 | MAS Val Dataset LFD\u2193 | MAS Val Dataset IoU\u2191 | MAS Val Dataset CD\u2193 |\n|---|---|---|---|---|---|---|---| \n| Point-E (Nichol et al., 2022a) | ~31 Sec | 5018.73 | 0.1948 | 0.02231 | 6181.97 | 0.2154 | 0.03536 |\n| Shap-E (Jun & Nichol, 2023a) | ~6 Sec | 3824.48 | 0.3488 | 0.01905 | 4858.92 | 0.2656 | 0.02480 |\n| Single-view One-2-3-45 (Liu et al., 2023a) | ~45 Sec | 4397.18 | 0.4159 | 0.04422 | 5094.11 | 0.2900 | 0.04036 |\n| OpenLRM (He & Wang, 2024) | ~5 Sec | 3198.28 | 0.5748 | 0.01303 | 4348.20 | 0.4091 | 0.01668 |\n| TripoSR (Tochilkin et al., 2024) | ~1 Sec | 3750.65 | 0.4524 | 0.01388 | 4551.29 | 0.3521 | 0.03339 |\n| InstantMesh (Xu et al., 2024) | ~10 Sec | 3833.20 | 0.4587 | 0.03275 | 5339.98 | 0.2809 | 0.05730 |\n| LGM (Tang et al., 2024) | ~37 Sec | 4391.68 | 0.3488 | 0.05483 | 5701.92 | 0.2368 | 0.07276 |\n| Make-A-Shape (Hui et al., 2024) | ~2 Sec | 3406.61 | 0.5004 | 0.01748 | 4071.33 | 0.4285 | 0.01851 |\n| WaLa (RGB) | ~2.5 Sec | 2509.20 | 0.6154 | 0.02150 | 2920.74 | 0.6056 | 0.01530 |\n| WaLa Large (RGB) | ~2.6 Sec | 2473.35 | 0.5984 | 0.02175 | 2562.70 | 0.6610 | **0.00575** |\n| WaLa (depth) | ~2.5 Sec | 2172.52 | 0.6927 | **0.01301** | 2544.56 | 0.6358 | 0.01213 |\n| WaLa Large (depth) | ~2.6 Sec | **2076.50** | **0.7043** | 0.01344 | **2322.75** | **0.6758** | 0.00756 |\n| InstantMesh (Xu et al., 2024) | ~1.5 Sec | 3009.19 | 0.5579 | 0.01560 | 4001.09 | 0.4074 | 0.02855 |\n| Multi-view LGM (Tang et al., 2024) | ~35 Sec | 1772.98 | 0.6842 | 0.00783 | 2712.30 | 0.5418 | 0.00867 |\n| Make-A-Shape (Hui et al., 2024) | ~2 Sec | 1890.85 | 0.7460 | 0.00337 | 2217.25 | 0.6707 | 0.00350 |\n| WaLa(RGB 4) | ~2.5 Sec | 1260.64 | 0.8500 | 0.00182 | 1540.22 | 0.8175 | 0.00208 |\n| WaLa(Depth 4) | ~2.5 Sec | 1185.39 | 0.87884 | 0.00164 | 1417.40 | 0.83313 | 0.00160 |\n| WaLa(Depth 6) | ~4 Sec | **1122.61** | **0.91245** | **0.00125** | **1358.82** | **0.85986** | **0.00129** |", "caption": "Table 4: \nComparison between different methods on Image-to-3D task (Top) and Multiview-to-3D task (Bottom).\nQuantitative evaluation shows that our single-view model excels the baselines, achieving the highest IoU and lowest LFD metrics. Our multi-view model further enhances performance by incorporating additional information. RGB 4, Depth 4, and Depth 6 represents conditioning using RGB images from 4 different views, and depth estimates from 4 and 6 views respectively. Inference time is measured on A100 GPU.", "description": "Table 4 presents a quantitative comparison of various methods for generating 3D models from images, specifically focusing on single-view and multi-view scenarios.  The key performance indicators are the Intersection over Union (IoU), measuring the overlap between the generated and ground truth 3D models, and the Light Field Distance (LFD), representing the dissimilarity in appearance from multiple viewpoints.  The table demonstrates that the proposed Wavelet Latent Diffusion (WaLa) model significantly outperforms existing methods in both single-view and multi-view settings.  The improvement in multi-view is attributed to the inclusion of additional information from multiple perspectives.  Different conditioning strategies are explored using RGB images and depth estimations from varying numbers of views.  Inference times are also provided, all measured using an A100 GPU.", "section": "4.2.3 Image-to-Mesh"}, {"content": "| Sampling Loss | Amount of finetune data | IOU \u2191 | MSE \u2193 | D-IOU \u2191 | D-MSE \u2193 |\n|---|---|---|---|---|---| \n| No<sup>1</sup> | - | 0.91597 | 0.00270 | 0.91597 | 0.00270 |\n| Yes<sup>1</sup> | - | **0.92619** | **0.00136** | **0.91754** | **0.00229** |\n| Yes | - | 0.95479 | 0.00090 | 0.94093 | 0.00169 |\n| Yes | 2500 | 0.95966 | 0.00078 | 0.94808 | 0.00149 |\n| Yes | 5000 | 0.95873 | 0.00078 | 0.94793 | 0.00149 |\n| Yes | 10000 | **0.95979** | **0.00078** | **0.94820** | **0.00148** |\n| Yes | 20000 | 0.95707 | 0.00079 | 0.94659 | 0.00150 |\n\n<sup>1</sup>Results for the first two rows are based on 200k iterations.", "caption": "Table 5: \nAblation study on adaptive sampling as well finetuning of the VQ-VAE model.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of adaptive sampling loss and VQ-VAE finetuning on the performance of the model. It shows how different combinations of these techniques affect the model's ability to reconstruct shapes accurately, as measured by Intersection over Union (IoU) and Mean Squared Error (MSE). The study also considers D-IoU and D-MSE metrics, which take data imbalance into account. The results demonstrate the effectiveness of adaptive sampling loss and balanced fine-tuning for improved accuracy.", "section": "C.1 VQ-VAE Adaptive Sampling Loss Analysis"}, {"content": "| Architecture | hidden dim | No. of layers | post or pre | LFD \u2193 | IoU \u2191 | CD \u2193 |\n|---|---|---|---|---|---|---|\n| U-VIT | 384 | 32 | pre | 1523.74 | 0.8211 | 0.001544 |\n| U-VIT | 768 | 32 | pre | 1618.73 | 0.7966 | 0.001540 |\n| U-VIT | 1152 | 8 | pre | 1596.88 | 0.8020 | 0.001561 |\n| U-VIT | 1152 | 16 | pre | 1521.81 | **0.8237** | 0.001573 |\n| U-VIT | 1152 | 32 | pre | **1507.43** | 0.8199 | **0.001482** |\n| DiT | 1152 | 32 | pre | 1527.16 | 0.8145 | 0.001602 |\n| U-VIT | 1152 | 32 | post | 1576.07 | 0.8176 | 0.001695 |", "caption": "Table 6: \nAblation study on the generative model design choices.", "description": "This ablation study investigates the impact of different design choices on the generative model's performance. It examines the effects of varying the hidden dimension and the number of layers in the U-ViT architecture, comparing the results with a DiT architecture.  It also explores the impact of applying the generative model before or after quantization and the effect of using a different number of layers in the attention block.", "section": "3.2 Stage 2: Latent Diffusion Model"}, {"content": "| Method | Number of Parameters |\n|---|---| \n| Autoencoder Model | 12.9 million |\n| Uncondition Model | 1.1 billion |\n| Single View Model | 956 million |\n| Single View Model Large | 1.4 billion |\n| Depth View Model | 956 million |\n| Depth View Model Large | 1.4 billion |\n| Pointcloud Model | 966.7 million |\n| Multi View Model (Depth and Image) | 956 million |\n| 6 view Depth Model | 898 million |\n| Voxel Model | 906.9 million |", "caption": "Table 7: Number of Parameters for Different Models", "description": "This table presents the number of parameters used in each of the models developed in the study.  It breaks down the model sizes for different model types including the autoencoder, various conditional models (single-view image, depth, multi-view), an unconditional model and the voxel model,  providing a clear view of the model complexity and scale for each task.", "section": "4 Results"}, {"content": "| Model | Scale | Timestep |\n|---|---|---|\n| Voxel | 1.5 | 5 |\n| Pointcloud | 1.3 | 8 |\n| Single-View RGB | 1.8 | 5 |\n| Single-View Depth | 1.8 | 5 |\n| Multi-View RGB | 1.3 | 5 |\n| Multi-View Depth | 1.3 | 5 |\n| 6 Multi-View Depth | 1.5 | 10 |\n| Unconditional | - | 1000 |", "caption": "Table 8: Classifier free scale and timestep used in the paper", "description": "This table lists the hyperparameters used for the classifier-free guidance in the diffusion model during inference.  Specifically, it shows the classifier-free guidance scale and the number of timesteps used for generating 3D shapes from different input modalities, including voxels, point clouds, single-view and multi-view RGB images, and multi-view depth maps, as well as for unconditional generation.", "section": "4.2 Experiments"}]