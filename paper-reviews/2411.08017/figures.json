[{"figure_path": "https://arxiv.org/html/2411.08017/x1.png", "caption": "Figure 1: We propose a new 3D generative model, called WaLa, that can generate shapes from conditions such as sketches, text, single-view images, low-resolution voxels, point clouds & depth-maps.", "description": "Figure 1 showcases the capabilities of the Wavelet Latent Diffusion (WaLa) model, a novel 3D generative model.  The figure displays example inputs (sketches, text descriptions, single-view images, low-resolution voxel grids, point clouds, and depth maps) and their corresponding generated 3D outputs. This demonstrates the model's ability to create diverse 3D shapes from various types of input conditions, highlighting its versatility and potential applications.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2411.08017/x2.png", "caption": "Figure 2: WaLa generates 3D shapes across various input modalities (see appendix for more).", "description": "Figure 2 showcases the versatility of the WaLa model by demonstrating its ability to generate a wide variety of 3D shapes from different input types.  These inputs include point clouds, voxels, single-view images, multi-view images, sketches, and text descriptions. The figure displays several example outputs for each input modality, highlighting the model's capacity to create high-quality, detailed, and diverse 3D shapes across multiple representations and conditions. More examples are available in the paper's appendix.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2411.08017/x3.png", "caption": "Figure 3: Overview of the WaLa\u00a0network architecture and 2-stage training process and inference method. Top Left: Stage 1 autoencoder training, compressing diffusible wavelet tree (W\ud835\udc4aWitalic_W) shape representation into a compact latent space.\nTop Right: Conditional/unconditional diffusion training.\nBottom: Inference pipeline, illustrating sampling from the trained diffusion model and decoding the sampled latent into a Wavelet Tree (W\ud835\udc4aWitalic_W), then into a mesh.", "description": "Figure 3 illustrates the WaLa model's architecture and workflow.  The top-left panel depicts Stage 1 training, where a VQ-VAE autoencoder compresses a high-resolution wavelet tree representation of a 3D shape (W) into a lower-dimensional latent space (Z). The top-right panel shows Stage 2, the conditional/unconditional diffusion training process on the latent representations to generate new shapes. The bottom panel details the inference process: starting with random noise, the diffusion model generates a latent code (Z), which is then decoded into a wavelet tree (W) and finally converted into a mesh representation of the 3D shape.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2411.08017/x4.png", "caption": "Figure 4: Qualitative comparison with other methods for single-view (top-left), multi-view (top-right), voxels (bottom-left), and point cloud (bottom-right) conditional input modalities. Hui et\u00a0al. (2024); He & Wang (2024); Tochilkin et\u00a0al. (2024); Xu et\u00a0al. (2024); Tang et\u00a0al. (2024); Chen et\u00a0al. (2024b); Nichol et\u00a0al. (2022c)", "description": "Figure 4 presents a qualitative comparison of 3D shape generation results using different methods and input modalities.  The top-left quadrant shows single-view image input results, comparing the authors' model (WaLa) against Make-A-Shape, OpenLRM, and TripoSR. The top-right quadrant displays multi-view image input results comparing WaLa to Make-A-Shape and InstantMesh. The bottom-left quadrant showcases voxel input results, comparing WaLa against Make-A-Shape, Nearest, and Trilinear. Finally, the bottom-right quadrant displays point cloud input results comparing WaLa against Make-A-Shape and MeshAnything. This figure visually demonstrates the performance of WaLa compared to other state-of-the-art 3D generative models across various input modalities, highlighting its ability to generate high-quality shapes.", "section": "4 Results"}, {"figure_path": "https://arxiv.org/html/2411.08017/extracted/5995222/figures/sketches.png", "caption": "Figure 5: The 6 different sketch types. From left to right: Grease Pencil, Canny, HED, HED+potrace, HED+scribble, CLIPaasso, and a depth map for reference. Mesh taken from \u00a0(Fu et\u00a0al., 2021).", "description": "Figure 5 showcases six distinct methods for generating sketches from a 3D model (mesh from Fu et al., 2021).  These methods are: Grease Pencil (a Blender tool creating artistic strokes), Canny edge detection (for outlining shapes), HED (Holistically-Nested Edge Detection, a deep learning technique to highlight edges), HED+potrace (HED output further processed using potrace to clean up the lines), HED+scribble (HED output with a scribble effect), and CLIPasso (a method generating sketches from a depth map, using strokes consistent with a given caption). A reference depth map is also included for comparison.", "section": "D SKETCH DATA GENERATION"}, {"figure_path": "https://arxiv.org/html/2411.08017/extracted/5995222/figures/sketch-views.png", "caption": "Figure 6: The 8 different views for which sketches were generated. Images created using the Grease Pencil technique on a mesh taken from Fu et\u00a0al. (2021). The CLIPasso technique was only used on the first, fifth, and sixth views from the left.", "description": "Figure 6 shows the eight different viewpoints from which sketches were generated for use as input to the 3D shape generation model.  The images were created using Blender's Grease Pencil tool, with a mesh from the Fu et al. (2021) paper as the base.  The CLIPasso technique, an alternative method for sketch generation, was only used for three of the eight views (the first, fifth, and sixth from the left).  These sketches represent a variety of perspectives of the same object used to train the model, which likely helps the model learn to generalize the object from various angles.", "section": "D SKETCH DATA GENERATION"}, {"figure_path": "https://arxiv.org/html/2411.08017/x5.png", "caption": "Figure 7: This figure presents more results from the text-to-3D generation task. Each row corresponds to a unique text prompt, with the resulting 3D renderings highlighting the model\u2019s capability to produce detailed and varied shapes from these inputs.", "description": "Figure 7 showcases the model's ability to generate detailed and diverse 3D shapes from text descriptions. Each row displays a unique text prompt and the corresponding 3D renderings produced by the model.  The variety of shapes demonstrates the model's capacity to handle diverse textual inputs and produce high-quality, detailed outputs.", "section": "4.2.3 Image-to-Mesh"}, {"figure_path": "https://arxiv.org/html/2411.08017/x6.png", "caption": "Figure 8: Here, for each caption, four different 3D variations are displayed. This figure emphasizes the model\u2019s flexibility in generating multiple distinct outputs for the same text description while maintaining thematic consistency.", "description": "This figure showcases the model's ability to generate diverse 3D models from the same text prompt.  For each of the listed text prompts, four different 3D variations are shown.  Despite the variations, all four models maintain a strong thematic resemblance to the prompt. This demonstrates the model's flexibility in producing multiple creative and distinct outputs while staying true to the core concept represented in the text.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2411.08017/x7.png", "caption": "Figure 9: Here, for each caption, four different 3D variations are displayed. This figure emphasizes the model\u2019s flexibility in generating multiple distinct outputs for the same text description while maintaining thematic consistency.", "description": "Figure 9 showcases the model's ability to generate diverse 3D models from the same text prompt.  For each of the nine text prompts shown, four distinct 3D variations are presented. This demonstrates the model's flexibility and capacity to produce multiple creative outputs while maintaining a consistent theme or concept for each prompt.  The variations are subtle yet noticeable, highlighting the model's ability to explore different interpretations of the same input instruction.", "section": "Results"}]