{"references": [{"fullname_first_author": "Jian Chen", "paper_title": "FinTextQA: A dataset for long-form financial question answering", "publication_date": "2024-08-11", "reason": "This paper introduces a new benchmark dataset specifically designed for evaluating long-form question answering in the financial domain, which is relevant to the focus of OmniEval on financial RAG evaluation."}, {"fullname_first_author": "Jianlv Chen", "paper_title": "BGE m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation", "publication_date": "2024-02-06", "reason": "This work presents BGE embeddings, a high-quality embedding model crucial for retrieval tasks in RAG systems, directly contributing to OmniEval's retriever evaluation component."}, {"fullname_first_author": "Jon Saad-Falcon", "paper_title": "ARES: an automated evaluation framework for retrieval-augmented generation systems", "publication_date": "2024-06-16", "reason": "This study offers ARES, an automated RAG evaluation framework, providing valuable insights for the development of OmniEval's automatic evaluation system."}, {"fullname_first_author": "Zehan Li", "paper_title": "Towards general text embeddings with multi-stage contrastive learning", "publication_date": "2023-08-08", "reason": "This paper introduces a method for generating general text embeddings, an essential component in the retrieval stage of RAG systems, thus supporting OmniEval's retrieval evaluation."}, {"fullname_first_author": "Jiawei Chen", "paper_title": "Benchmarking large language models in retrieval-augmented generation", "publication_date": "2024-02-20", "reason": "This work offers a benchmark for evaluating LLMs in RAG contexts, providing foundational knowledge and methodologies for developing a comprehensive RAG evaluation benchmark like OmniEval."}]}