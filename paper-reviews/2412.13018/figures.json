[{"figure_path": "https://arxiv.org/html/2412.13018/x3.png", "caption": "Figure 1: The visualization of OmniEval\u2019s generation pipeline of evaluation data.", "description": "This figure visualizes the four-step data generation pipeline of OmniEval to build evaluation data instances. Step 1 is topic classification, which receives a document from the knowledge corpus and utilizes a topic classification agent to automatically recognize and classify which domain topic the document belongs to. Step 2 is initial data generation. For a specified document and its related domain topic, a data generation agent is requested to generate a question-answer (QA) pair and its most relevant passage within the document, which corresponds to a specific RAG task. Step 3 is automatic data quality inspection, which utilizes a quality inspection agent to inspect the quality of the generated data instance from step 2 and only retains the high-quality instances. The last step is manual quality inspection and correction, which incorporates human experts to check the generated data instances and further correct some flawed instances to ensure the quality of the final evaluation dataset.", "section": "3. Construction Pipeline of OmniEval"}, {"figure_path": "https://arxiv.org/html/2412.13018/x4.png", "caption": "Figure 2: Topic & task systems used for building our benchmark.", "description": "This figure shows the two systems used for building the benchmark, OmniEval: a topic system and a task system.  The topic system consists of 16 financial topics, such as retail banking, commercial banking, and investment banking. The task system comprises 5 RAG tasks, including extractive QA, multi-hop reasoning QA, contrast QA, long-form QA, and conversational QA.", "section": "3. Construction Pipeline of OmniEval"}, {"figure_path": "https://arxiv.org/html/2412.13018/x5.png", "caption": "Figure 3: Statistical information of manual inspection.", "description": "This bar chart presents the distribution of manual inspection results on the quality of automatically generated data. The results are categorized into \"Bad Case (1-2)\", \"Middle Case (3)\", and \"Good Case (4-5)\" based on a 5-point scale assessment by human annotators. The \"Acceptable Case (3-5)\" represents the total percentage of instances deemed acceptable or better.  The high percentage of acceptable cases validates the automatic data generation approach.", "section": "3. Construction Pipeline of OmniEval"}, {"figure_path": "https://arxiv.org/html/2412.13018/x6.png", "caption": "Figure 4: Visualization of matrix-based evaluation of GTE-Qwen2.5-1.5B+Yi15-34B on Rouge-L.", "description": "This figure visualizes the Rouge-L scores of a Retrieval-Augmented Generation (RAG) system composed of the GTE-Qwen2.5-1.5B retriever and the Yi15-34B large language model (LLM) across various financial topics and question answering task types. The x-axis represents the financial topics, while the y-axis corresponds to the different RAG tasks. The color intensity of each cell in the matrix indicates the Rouge-L score, providing a comprehensive overview of the system's performance across diverse scenarios.", "section": "4 Experiment"}, {"figure_path": "https://arxiv.org/html/2412.13018/x7.png", "caption": "Figure 5: Visualization of matrix-based evaluation of GTE-Qwen2.5-1.5B+Deepseek-v2-chat on Rouge-L.", "description": "This figure visualizes the Rouge-L scores of a Retrieval-Augmented Generation (RAG) system across different financial topics and question-answering tasks.  The RAG system combines the GTE-Qwen2.5-1.5B retriever with the Deepseek-v2-chat large language model (LLM). The visualization uses a heatmap, where each cell represents the Rouge-L score for a specific combination of topic (e.g., Retail Banking, Stock Market) and task (e.g., Extractive QA, Multi-hop Reasoning QA). The color intensity of each cell corresponds to the Rouge-L score, with darker colors indicating higher scores.  This matrix-based visualization allows for a detailed analysis of the RAG system's performance across various scenarios, revealing strengths and weaknesses in different areas.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.13018/x8.png", "caption": "Figure 6: Visualization of matrix-based evaluation of GTE-Qwen2.5-1.5B+Qwen2.5-72B on Rouge-L.", "description": "This figure visualizes the Rouge-L scores of a Retrieval-Augmented Generation (RAG) system, combining the GTE-Qwen2.5-1.5B retriever and the Qwen2.5-72B generator.  The visualization is presented as a heatmap, where rows represent different question answering (QA) tasks (Extractive QA, Multi-hop Reasoning QA, Contrast QA, Long-form QA, and Conversational QA) and columns represent various financial topics (e.g., Retail Banking, Commercial Banking, Stock Market, etc.). Each cell in the heatmap represents the Rouge-L score of the RAG system on the corresponding topic and task combination, providing a fine-grained performance analysis across different scenarios. The color intensity of each cell indicates the magnitude of the Rouge-L score, with darker colors representing higher scores and thus better performance.", "section": "4 Experiment"}, {"figure_path": "https://arxiv.org/html/2412.13018/x9.png", "caption": "Figure 7: Visualization of matrix-based evaluation of GTE-Qwen2.5-1.5B+Llama3.1-70B on Rouge-L.", "description": "This figure visualizes the Rouge-L scores of a Retrieval-Augmented Generation (RAG) system comprising the GTE-Qwen2.5-1.5B retriever and Llama3.1-70B generator, evaluated across various financial topics (Retail Banking, Commercial Banking, etc.) and question answering tasks (Extractive QA, Multi-hop Reasoning QA, etc.).  The heatmap represents the Rouge-L score for each topic-task combination, providing a fine-grained performance analysis.", "section": "4 Experiment"}, {"figure_path": "https://arxiv.org/html/2412.13018/x10.png", "caption": "Figure 8: Statistical information of the automated-generated training set.", "description": "This heatmap figure visualizes the distribution of the training dataset across different RAG scenarios, defined by the combination of 5 financial tasks (Extractive QA, Multi-hop Reasoning QA, Contrast QA, Long-form QA, and Conversational QA) and 16 financial topics (Retail Banking, Commercial Banking, Investment Banking, Stock Market, Bond Market, Fund, Derivatives Markets, Life Insurance, Property Insurance, Health insurance, Blockchain, Artificial Intelligence, Big Data, Anti-Money Laundering (AML), Compliance Audit, and Regulatory Reports).  The color intensity represents the number of instances in each scenario, with darker shades indicating a higher number of samples.", "section": "4 Experiment"}, {"figure_path": "https://arxiv.org/html/2412.13018/x11.png", "caption": "Figure 9: Statistical information of the automated-generated test set.", "description": "This heatmap visualizes the distribution of the automated-generated test instances across different topics (Retail Banking, Commercial Banking, etc.) and RAG task types (Extractive QA, Multi-hop Reasoning QA, etc.). The color intensity represents the number of instances in each topic-task combination, with warmer colors indicating a higher number of instances. This allows for analyzing the dataset's composition and potential biases toward specific topics or tasks.", "section": "4 Experiment"}, {"figure_path": "https://arxiv.org/html/2412.13018/x12.png", "caption": "Figure 10: Statistical information of the human-annotated test set.", "description": "This table visualizes the number of instances included in each subset of the human-annotated test set. The rows represent the five financial QA tasks defined, namely \"Extractive QA,\" \"Multi-hop Reasoning QA,\" \"Constract QA,\" \"Long-form QA,\" and \"Conversational QA.\" The columns represent 16 financial topics, including \"Retail Banking,\" \"Commercial Banking,\" \"Investment Banking,\" \"Stock Market,\" \"Bond Market,\" \"Fund,\" \"Derivatives Markets,\" \"Life Insurance,\" \"Property Insurance,\" \"Health Insurance,\" \"Blockchain,\" \"Artificial Intelligence,\" \"Big Data,\" \"Anti-Money Laundering,\" \"Compliance Audit,\" and \"Regulatory Reports.\" Each cell in the heatmap represents the number of questions for a given task and topic, with cells coloured with higher numbers meaning more instances in that topical or task subsets.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.13018/x13.png", "caption": "Figure 11: The topic-specific evaluation results on the auto-generated set.", "description": "This figure presents a bar chart visualizing the topic-wise performance evaluation of different large language models (LLMs) on an automatically generated dataset.  The evaluation metrics likely include Rouge-L and other related metrics. The x-axis represents various topics in the financial domain such as \"Retail Banking,\" \"Commercial Banking,\" \"Investment Banking,\" etc., and the y-axis likely represents the performance scores. Different colors represent various LLMs being evaluated, providing a comparison across models. The figure aims to show how each LLM performs across different financial topics and to highlight the variation in performance due to topic specialization.", "section": "4.2 Experiments on Topic-specific Subsets"}, {"figure_path": "https://arxiv.org/html/2412.13018/x14.png", "caption": "Figure 12: The topic-specific evaluation results on the human-annotated set.", "description": "This figure visualizes the performance of different RAG models across various financial topics on a human-annotated dataset.  The x-axis represents the topics like retail banking, commercial banking, investment banking, etc. and the y-axis shows the Rouge-L scores, indicating the models' text generation quality. Different colors correspond to different large language models (LLMs) used for generation after retrieving context. The results demonstrate how performance can vary based on the specific financial topic, suggesting that certain topics might be more challenging for current RAG models than others due to nuances in domain-specific language or knowledge requirements.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.13018/x15.png", "caption": "Figure 13: The task-specific evaluation results on the auto-generated set.", "description": "This bar chart presents the evaluation results of different Large Language Models (LLMs) across five specific tasks within the financial domain, using the automated-generated dataset. The tasks include Extractive Question Answering, Multi-hop Reasoning Question Answering, Contrast Question Answering, Long-form Question Answering, and Conversational Question Answering. The LLMs evaluated are deepseek-v2-chat, llama3-70b-instruct, qwen2-72b, and yi15-34b.  The performance metric used is Rouge-L, which measures the longest common subsequence between the generated text and the reference text. Higher Rouge-L scores indicate better performance.", "section": "4.3 Experiments on Task-specific Subsets"}, {"figure_path": "https://arxiv.org/html/2412.13018/x16.png", "caption": "Figure 14: The task-specific evaluation results on the human-annotated set.", "description": "This figure presents a bar graph illustrating the performance of four large language models (LLMs) across five distinct financial question-answering tasks using the human-annotated evaluation dataset.  Rouge-L scores are used to measure performance.  The tasks include extractive question answering, multi-hop reasoning, contrast question answering, long-form question answering, and conversational question answering.  The LLMs evaluated are DeepSeek-v2-chat, Llama3.1-70b-Instruct, Qwen2.5-72b, and Yi15-34b. The results indicate performance variations among the models and across the tasks.", "section": "4. Experiment"}]