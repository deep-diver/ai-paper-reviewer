[{"content": "| Model | Param | FID\u2193 | CLIP\u2191 |\n|---|---|---|---| \n| UniDiffuser | 0.9B | 9.71 | 30.93 |\n| CoDi | 4.3B | 11.26 | 30.69 |\n| UIO-2XXL | 6.8B | 13.39 | - |\n| SDv1.5 | 0.9B | 11.12 | 30.63 |\n| SDXL* | 2.6B | 16.49 | 31.36 |\n| SD3-Medium* | 2B | 20.94 | 30.65 |\n| OmniFlow* | 3.4B | 13.40 | 31.54 |", "caption": "Table 1: Text-to-Image Generation on MSCOCO-30K Benchmark. *Indicates models pretraining data consists of high quality images and captions that do not follow the distribution of COCO dataset, which can negatively affect FID scores.", "description": "This table presents a comparison of various models' performance on the MSCOCO-30K benchmark for text-to-image generation.  The models are evaluated using two metrics: FID (Fr\u00e9chet Inception Distance), a measure of the generated images' visual similarity to real images from the COCO dataset, and CLIP (Contrastive Language\u2013Image Pre-training), assessing the alignment between the generated image and its corresponding text caption. Lower FID scores indicate better image quality, while higher CLIP scores reflect stronger alignment.  The asterisk (*) next to some model names signifies that their pretraining data included high-quality images and captions not representative of the COCO dataset's distribution. This deviation in training data can lead to lower FID scores, potentially misrepresenting the model's true performance.", "section": "5.2. Text-to-Image Generation"}, {"content": "| Model |  | Param | Images | Gen.\u2191 |\n|---|---|---|---|---|\n| *Text-to-Image Specialist* |  |  |  |  |\n| SD1.5 |  | 0.9B | 4.0B | .43 |\n| SDv2.1 |  | 0.9B | 2.3B | .50 |\n| SDXL |  | 2.6B | 1.6B | .55 |\n| DALL-E 2 |  | 4.2B | 2.6B | .52 |\n| SD3-Medium |  | 2B | 1B | .62 |\n| SD3-Large |  | 8B | 2.0B | .68 |\n| *Generalist* |  |  |  |  |\n| CoDi |  | 4.3B | 400M* | .38 |\n| UniDiff. |  | 0.9B | 2B | .43 |\n| OmniFlow |  | 3.4B | 30M* | .62 |\n| Chameleon |  | 7B | 3.5B | .39 |\n| Transfusion |  | 7B | 3.5B | .63 |", "caption": "Table 2: Text-to-Image Generation on GenEval Benchmark. We compare the model size, number of training images and GenEval benmark Score. * Indicates fine-tuning dataset. CoDi and MMDiT-O are both initialized with pretrained text-to-image diffusion models (SD and SD3).", "description": "Table 2 presents a comparison of various models' performance on the GenEval benchmark, a more comprehensive evaluation of text-to-image generation capabilities than simple metrics like FID and CLIP.  The table highlights key factors influencing model performance, including model size (number of parameters), the quantity of training images used, and the final GenEval score achieved.  It specifically compares models categorized as either 'Text-to-Image Specialists' or 'Generalists', illustrating the trade-off between specialization and generalization. The asterisk (*) indicates models that underwent additional fine-tuning, highlighting that the results may not be solely representative of the base models. Noteworthy is the inclusion of CoDi and MMDIT-O, which were both initialized with pre-trained text-to-image diffusion models (Stable Diffusion and Stable Diffusion 3 respectively), showing how prior training can impact performance.", "section": "5. Main Results"}, {"content": "| Model | Param | FAD\u2193 | CLAP\u2191 |\n|---|---|---|---| \n| *Text-to-Audio Specialist* |  |  |  |\n| AudioGen-L [24] | 1B | 1.82 | - |\n| Make-an-Audio [19] | 0.4B | 2.66 | - |\n| AudioLDM-L [32] | 0.7B | 1.96 | .141 |\n| Make-an-Audio 2 [18] | 0.9B | 2.05 | .173 |\n| AudioLDM 2-Full-L [33] | 0.7B | 1.86 | .182 |\n| *Generalist* |  |  |  |\n| CoDi | 3.4B | 1.80 | .053* |\n| OmniFlow | 3.4B | **1.75** | **.183** |\n| <span style=\"color:#808080;\">UIO-2XXL</span> | <span style=\"color:#808080;\">6.7B</span> | <span style=\"color:#808080;\">2.64</span> | <span style=\"color:#808080;\">-</span> |", "caption": "Table 3: Text-to-Audio Generation on AudioCaps Evaluation Set. Comparison of FAD and CLAP scores for various audio generators. *Reproduced from official checkpoint, see Appendix for details.", "description": "This table presents a quantitative comparison of different text-to-audio generation models on the AudioCaps evaluation set.  The comparison is based on two metrics: FAD (Fr\u00e9chet Audio Distance), which measures the similarity between the generated audio and the ground truth audio, and CLAP (Contrastive Language\u2013Audio Pre-training), which evaluates how well the generated audio aligns with the text prompt. Lower FAD scores indicate better audio quality and higher CLAP scores reflect a stronger alignment between audio and text. The table includes both specialized text-to-audio models and general-purpose models that can perform a wider range of tasks. One model's result is marked with an asterisk (*) indicating the data was obtained from an official checkpoint rather than the authors' own training, with details found in the appendix.", "section": "5.3. Text-to-Audio Generation"}, {"content": "|                | Audio Gen. | Text Gen. |\n|----------------|-------------|------------|\n|                | **FAD\u2193**     | **CLAP\u2191**  |\n| *Continuous Flow Matching* |             |            |\n| eps/linear      | 2.08        | .141       |\n| v/cos          | 2.01        | .203       |\n| v/linear       | 1.86        | .126       |\n| rf/uniform     | 1.82        | .227       |\n| rf/lognorm     | **1.79**    | **.254**   |\n| *Discrete Text Diffusion* |             |            |\n| SEDD[35]       | -           | .180       |\n| MDLM[45]       | -           | .163       |", "caption": "Table 4: Various Formulations for Audio and Text Generation. We report FAD for audio generation and CLAP for text generation on AudioCaps dataset.", "description": "This table presents a comparison of different mathematical formulations used for training audio and text diffusion models.  The performance of each formulation is evaluated using two metrics: FAD (Frequency-Aware Distance) for audio generation quality and CLAP (Contrastive Language\u2013Audio Pre-training) for text generation quality. The results are obtained from experiments conducted on the AudioCaps dataset, a dataset of audio clips paired with their corresponding text descriptions. The table aids in understanding the impact of different mathematical formulations on the quality of generated audio and text.", "section": "5.4 Recipes for Audio and Text Diffusions"}, {"content": "| Dataset | Size | Modality |\n|---|---|---|\n| LAION-Aesthetics-3M | 2M* | T,I |\n| CC12M | 12M | T,I |\n| COYO-700M(Subset) | 5M | T,I |\n| LAION-COCO | 7M | T,I |\n| SoundNet | 2M | T,A,I\u2020 |\n| VGGSound | 0.2M | T,A,I\u2020 |\n| T2I-2M | 2M | T,I |\n| AudioSet | 2M | T,A |\n| AudioCaps | 46K | T,A |\n| WavCaps | 0.4M | T,A |", "caption": "Table 5: List of all datasets used in training. *Some image URLs are no longer accessible. \u2020\u2020{\\dagger}\u2020 We generate synthetic captions using BLIP.", "description": "This table lists the datasets used to train the OmniFlow model.  It details the name of each dataset, the approximate number of samples (images, audio, or text), and the modalities (text, image, and/or audio) present in each dataset.  Note that some image URLs from certain datasets may no longer be accessible, and synthetic captions were generated for some datasets using the BLIP model.", "section": "4. Setup"}, {"content": "|           | Images | Parms. | CLAP\u2191 | CIDEr\u2191 | CLIP\u2191 | CIDEr\u2191 |\n|---|---|---|---|---|---|---|\n| **AudioCaps** |  |  | **CLAP\u2191** | **CIDEr\u2191** | **CLIP\u2191** | **CIDEr\u2191** |\n| **COCO-Karpathy** |  |  |  |  |  |  |\n| ***Specialist*** |  |  |  |  |  |  |\n| BLIP-2[29] | 129M | 2.7B | - | - | - | 145.8 \u2021 |\n| SLAM-AAC[8] | - | 7B | - | 84.1 \u2021 | - | - |\n| ***Generalist*** |  |  |  |  |  |  |\n| OmniFlow | 30M | 3.4B | 0.254 | 48.0 | 26.8 | 47.3 |\n| CoDi \u2020 | 400M | 4.3B | 0.206 | 7.9 | 25.9 | 17.2 |\n| Unidiffuser \u2020 | 2B | 0.9B | - | - | 29.3 | 20.5 |\n| UIO2-XXL | 1B* | 6.8B | - | 48.9 | - | 125.4* |\n| Transfusion | 3.5B | 7B | - | - | - | 35.2 |", "caption": "Table 6: X-to-Text Performance comparison on AudioCaps and COCO Captions. * UIO2\u2019s training data includes COCO. The fine-tuning dataset also includes 53M image understanding data, including 14 image captioning datasets. \u2020\u2020{\\dagger}\u2020 evaluated with official checkpoints. \u2021\u2021\\ddagger\u2021 fine-tuned on respective datasets (COCO and Audiocaps).", "description": "This table compares the performance of different models on two tasks: generating captions for audio clips (AudioCaps) and generating captions for images (COCO-Karpathy).  The models are categorized as either specialized (trained only on one dataset) or generalist (trained on multiple datasets). Performance is measured using three metrics: CLAP (a metric assessing the alignment of generated text with audio features), CIDEr (a metric evaluating the quality and relevance of image captions), and CLIP (a metric assessing the consistency between image features and the generated text). Note that UIO2 used COCO data in its training, and it was further fine-tuned with additional image understanding data. Some models' results were obtained using officially released checkpoints, while other models were fine-tuned on specific datasets to ensure a fair comparison.", "section": "5. Main Results"}, {"content": "| ID | CoDi | OmniFlow | GT |\n|---|---|---|---|\n| yVjivgsU2aA | Four car driver trying forcoming for a speeding car. | A race car engine revs and tires squeal. | An engine running followed by the engine revving and tires screeching. |\n| 8F-ndyrEWJ8 | Fire police cars stop and red traffic on different highway. | A fire siren goes off loudly as a man shouts and a low hum of an engine is running throughout the whole time. | A distant police siren, then racing car engine noise, and a man calling in police code over his radio. |\n| 350OCezayrk | Four motor car driving for completing an automobile service. | A vehicle engine is revving and idling. | A motor vehicle engine starter grinds, and a mid-size engine starts up and idles smoothly. |\n| LCwSUVuTyvg | Door, a blue hat and winter jacket. | A door is being slammed. | Glass doors slamming and sliding shut. |\n| 7XUt6sQS7nM | The sheep of the woman are the sheep of the sheep. | Multiple sheep bleat nearby. | A sheep is bleating and a crowd is murmuring. |\n| PVvi2SDOjVc | Car going for a car coming home. Three cars coming for a blue car coming down a road after the highway. | A car horn beeps. | A car engine idles and then the horn blows. |\n| Z_smJ66Tb3c | Men in the bird while the man in the boat. | Two men talk over blowing wind and bird chirps. | A man is speaking with bird sounds in the background followed by a whistling sound. |\n| CMNlIW6Lkwc | Two men in the fire and two men are coming towards the other man in the game. | A man speaks, followed by a loud bang and people laughing. | A man talking as a camera muffles followed by a loud explosion then a group of people laughing and talking. |\n| JQz40TkjymY | Writing computers for people in writing. | Typing on a computer keyboard. | Typing on a computer keyboard. |\n| U90e2P9jy30 | A man shouts the word to the person on the sidewalk to walk to get him to the door the hand to fall down on the sidewalk in. | Basketballs being dribbled and people talking. | Several basketballs bouncing and shoes squeaking on a hardwood surface as a man yells in the distance. |\n| 5I8lmN8rwDM | Stationary fire drill technician drilling down a hose pipe while wearing safety gear. Railroad safety drill for motorcycle with hose or oil checking equipment. | A drill runs continuously. | Drilling noise loud and continues. |\n| NlKlRKz8OKI | Birds on blue birds. | A woman talks and then an animal chewing. | A woman speaks with flapping wings and chirping birds. |", "caption": "Table 7: Qualitative comparisons of CoDi and OmniFlow\u00a0 on Audiocaps audio captioning task. Audios are randomly sampled. Audiocaps provide five ground truth captions per audio. For better presentation, we only list one in this table.", "description": "This table presents a qualitative comparison of the performance of two any-to-any generation models, CoDi and OmniFlow, on the AudioCaps audio captioning task.  Randomly selected audio clips from the AudioCaps dataset were used for evaluation.  AudioCaps provides five ground truth captions per audio clip, but only one ground truth caption is shown in the table for better readability and presentation. The table allows for a direct comparison of the generated captions for each audio clip between the two models, highlighting differences in accuracy and detail. This comparison provides insight into the relative strengths and weaknesses of CoDi and OmniFlow in terms of their ability to generate accurate and descriptive captions for unseen audio data.", "section": "5.4. Receipes for Audio and Text Diffusions"}, {"content": "| Reconstruction | GT |\n|---|---| \n| Crispy chicken tenders alongside a portion of a bbq sauce. | Crispy chicken tenders alongside a portion of bbq sauce. |\n| A well-furnished living room with a patterned curtain rod, a small white side table holding a vase of flowers, and a tufted gray sofa. | A well-decorated living room with a patterned curtain panel hanging from the window, a small white side table holding a vase of flowers, and a tufted gray sofa. |\n| A young man wearing a black shirt and holding an American flag. | A young man wearing a black shirt and holding an American flag. |\n| An artistic painting of a futuristic city by the water. | An artistic painting of a futuristic city by the water. |\n| Cozy and well-designed living room with a green velvet sofa, glass coffee table displaying potted plants, and a large skylight overhead. | Cozy and stylish living room with a green velvet sofa, glass coffee table displaying potted plants, and a large skylight overhead. |\n| A silver Audi Rs4 sedan driving on the passenger side near a mountainous coastline. | A silver Acura RLX sedan driving on the passenger side near a mountainous coastline. |", "caption": "Table 8: Text VAE reconstruction results. We show reconstruction results (Left) and the ground truth text (Right).The reconstruction mostly adheres to the semantics of the ground truth, with minor differences.", "description": "This table presents a qualitative evaluation of the Text Variational Autoencoder (VAE) used in the OmniFlow model.  For several input captions, it shows the text generated by the VAE after reconstruction (left column) and compares it to the original, ground truth caption (right column).  The purpose is to demonstrate that while there may be minor word choices differences between the reconstructed text and the original, the overall semantic meaning is preserved.", "section": "Supplementary Material"}]