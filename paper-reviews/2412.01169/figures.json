[{"figure_path": "https://arxiv.org/html/2412.01169/x2.png", "caption": "Figure 1: OmniFlow\u00a0is capable of a diverse range of any-to-any generation tasks. OmniFlow\u00a0supports generation of any output modalities given any input modality, such as text-to-image, text-to-audio, audio-to-image generations. It also supports tasks in multiple input modalities such as text+audio-to-image.", "description": "Figure 1 showcases OmniFlow's versatility in any-to-any multi-modal generation.  It demonstrates the model's ability to generate various output modalities (image, text, audio) from various input modalities (image, text, audio).  Examples include generating an image from text, generating audio from text, and generating an image from both text and audio. This highlights OmniFlow's capacity to seamlessly integrate and process information across different modalities, achieving coherent and diverse outputs.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.01169/x3.png", "caption": "Figure 2: Pipeline of OmniFlow. Previous any-to-any models such as CoDi [46] (Top) concatenate multiple modality-specific encoders and decoders, and naively average the embedding of multiple modalities to achieve joint conditioning. By contrast, OmniFlow\u00a0(Bottom) is a unified, modular multi-modal model, where features from different modalities directly interact with each other through joint attention layers. OmniFlow\u00a0is inspired by the modular design of Stable Diffusion 3 [11] (Middle), a text-to-image model.", "description": "Figure 2 illustrates the architectural differences between OmniFlow and previous any-to-any generation models.  Traditional approaches, exemplified by CoDi, use separate encoders and decoders for each modality (e.g., text, image, audio), combining their embeddings through simple averaging for joint conditioning.  This method lacks nuanced interaction between modalities. In contrast, OmniFlow presents a unified, modular architecture.  It directly interacts features across modalities using joint attention layers, allowing for more complex and coherent multi-modal generation.  OmniFlow's design is inspired by the modular text-to-image model Stable Diffusion 3.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.01169/x4.png", "caption": "(a) Overall Pipeline of OmniFlow", "description": "This figure shows a detailed breakdown of the OmniFlow model's architecture.  It illustrates the multi-modal nature of the model, demonstrating how multiple input modalities (text, image, audio) are processed through a series of encoding, joint attention, and decoding stages to generate outputs in any of those modalities or combinations thereof. The figure highlights the modular design of OmniFlow, where modality-specific blocks are used, allowing for independent pre-training and efficient integration.  This unified approach contrasts with prior methods that concatenate separate modality-specific models.", "section": "3. Model Architecture"}, {"figure_path": "https://arxiv.org/html/2412.01169/x5.png", "caption": "(b) Design of Omni-Transformer Block", "description": "The Omni-Transformer Block is a modular building block of the OmniFlow model. It takes as input the modality-specific latent representations (image, audio, text) and a unified timestep embedding.  The modality-specific inputs are processed through separate projection layers to obtain queries (Q), keys (K), and values (V). These are concatenated across modalities before being fed into a joint attention mechanism, which allows for interaction between different modalities. The output of the joint attention is then passed through a feed-forward network (FFN) and skip connections are added to improve information flow.  The unified timestep embedding modulates both the joint attention and FFN layers. The figure shows the architecture of this block in detail.", "section": "3. Model Architecture"}, {"figure_path": "https://arxiv.org/html/2412.01169/x6.png", "caption": "Figure 3: Architecture of OmniFlow. Left: We highlight the architecture of OmniFlow. Right: We show the design of an individual Omni-Transformer Block.", "description": "This figure details the architecture of the OmniFlow model, a multi-modal generative model. The left panel presents a high-level overview of the OmniFlow pipeline, illustrating how different modalities (image, text, and audio) are processed.  It shows the input streams, the modality-specific Variational Autoencoders (VAEs) used for latent representation, the Omni-Transformer blocks where multi-modal interaction happens, and the final output streams. The right panel zooms in on the structure of a single Omni-Transformer block, which is a key component of the model. It displays the internal operations within a block, including the use of joint attention mechanisms to allow information flow and interaction between different modalities. This detailed view highlights the modular and multi-modal design of the OmniFlow model.", "section": "3. Model Architecture"}, {"figure_path": "https://arxiv.org/html/2412.01169/x7.png", "caption": "(a) Text-to-Audio Generation.", "description": "This figure shows the impact of classifier-free guidance (CFG) and timestep shift on the quality of text-to-audio generation.  The x-axis represents the CFG scale, while the y-axis represents the FAD score (lower is better). Different lines represent different timestep shifts. The plot helps to determine the optimal settings for achieving the best audio generation quality.", "section": "5. Main Results"}, {"figure_path": "https://arxiv.org/html/2412.01169/x8.png", "caption": "(b) Audio-to-Text Generation.", "description": "This figure shows the results of experiments on audio-to-text generation. The graph displays the relationship between classifier-free guidance (CFG) and the timestep shift for audio-to-text generation. Different lines on the graph represent experiments with different timestep shifts. This allows for an understanding of how changes in the CFG value and the timestep shift affect the performance of audio-to-text generation. The x-axis shows the CFG value, and the y-axis shows the CLAP score, which measures the quality of the generated text captions.", "section": "5. Main Results"}, {"figure_path": "https://arxiv.org/html/2412.01169/x9.png", "caption": "Figure 4: Effect of CFG and Shift for audio and text generation. We evaluate the impact of guidance and timestep shift on text-to-audio and audio-to-text tasks.", "description": "This figure displays the impact of classifier-free guidance (CFG) and timestep shift on the quality of audio and text generation.  Two subfigures are presented, one for text-to-audio generation and another for audio-to-text generation.  Each subfigure shows curves plotting a performance metric (FAD for audio and CLIP for text) against the CFG scale for different values of timestep shift.  The curves illustrate how adjusting these two parameters influences the quality of the generated audio and text, allowing for fine-tuned control over the generation process.", "section": "5. Main Results"}, {"figure_path": "https://arxiv.org/html/2412.01169/x10.png", "caption": "Figure 5: Effect of Multi-Modal Guidance. In this example, the user can flexibly control the alignment between output text and input image, audio independently by varying \u03b1ausubscript\ud835\udefcau\\alpha_{\\text{au}}italic_\u03b1 start_POSTSUBSCRIPT au end_POSTSUBSCRIPT and \u03b1imsubscript\ud835\udefcim\\alpha_{\\text{im}}italic_\u03b1 start_POSTSUBSCRIPT im end_POSTSUBSCRIPT. Higher \u03b1imsubscript\ud835\udefcim\\alpha_{\\text{im}}italic_\u03b1 start_POSTSUBSCRIPT im end_POSTSUBSCRIPT will make the output texts resemble image captions, with visual descriptions such as lined up, driving down. Higher \u03b1ausubscript\ud835\udefcau\\alpha_{\\text{au}}italic_\u03b1 start_POSTSUBSCRIPT au end_POSTSUBSCRIPT will make the output texts resemble audio captions, with descriptions such as accelerating, revving.", "description": "Figure 5 demonstrates the control OmniFlow offers over the alignment between generated text and the input modalities (image and audio).  By adjusting two parameters, \u03b1im (alpha_im) and \u03b1au (alpha_au), users can independently influence whether the generated text prioritizes visual or auditory details. Increasing \u03b1im emphasizes aspects described in the input image (e.g., 'lined up', 'driving down'), while raising \u03b1au focuses the generated text on audio-related descriptors (e.g., 'accelerating', 'revving'). This showcases the model's ability to blend different input sources in flexible and nuanced ways.", "section": "3.2 Multi-Modal Guidance"}, {"figure_path": "https://arxiv.org/html/2412.01169/x11.png", "caption": "Figure 6: Qualitative Comparison with baselines on text-to-image generation. OmniFlow\u00a0achieves better image quality and prompt alignment when compared to previous generalist models.", "description": "Figure 6 presents a qualitative comparison of image generation results between OmniFlow and two other any-to-any generation models (CoDi and UniDiffuser) using the same text prompts.  The figure showcases how OmniFlow produces images with significantly better quality and a closer adherence to the details and style specified in the text prompts when compared to the baselines.  This visual comparison highlights OmniFlow's strengths in achieving superior alignment between the generated image and the input text description.", "section": "6. Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2412.01169/extracted/6038158/figs/Artboard_6.png", "caption": "Figure 7: Paths encoding different any-to-any generation tasks. (t1,t2,t3)subscript\ud835\udc611subscript\ud835\udc612subscript\ud835\udc613(t_{1},t_{2},t_{3})( italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) represents the \u201cnoise level\u201d of image, text and audio modalities. (0,0,0)000(0,0,0)( 0 , 0 , 0 ) represents clean (image, text, audio) triplets, and (1,1,1)111(1,1,1)( 1 , 1 , 1 ) represents pure Gaussian noise.", "description": "This figure illustrates how different any-to-any generation tasks can be represented using a three-dimensional space defined by the \"noise levels\" of image (t1), text (t2), and audio (t3) modalities. Each point in this space corresponds to a different combination of noise levels for each modality.  The origin (0, 0, 0) signifies clean data for all three modalities (a clean image, clean text, and clean audio). The point (1, 1, 1) represents pure Gaussian noise for all three modalities.  Different generation tasks are represented by paths connecting various points within this space. For example, text-to-image generation would be a path from a point representing clean text and pure noise for image and audio to a point representing a combination of clean text and image with pure noise in audio. This visualization helps to understand how OmniFlow handles the joint distribution of multiple modalities in a unified framework.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.01169/x12.png", "caption": "Figure 8: Training Pipeline of OmniFlow. We initialize our model with SD3 (Model 1). We then train the model on text-audio pairs to obtain Model 2. We merge Model 1 and Model 2 to obtain Model 3. The final model is obtained by further training Model 3 on any-to-any generation tasks.", "description": "This figure illustrates the training pipeline used to develop the OmniFlow model.  The process begins by initializing the model with the architecture and weights of Stable Diffusion 3 (referred to as Model 1). Next, Model 1 is trained on a dataset of text-audio pairs to create a specialized model capable of text-to-audio generation (Model 2).  Model 1 and Model 2 are then merged, combining their respective components to generate Model 3.  Finally, Model 3 is further fine-tuned on a more extensive dataset encompassing various any-to-any generation tasks (text-to-image, audio-to-image, etc.) to achieve the final OmniFlow model.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.01169/x13.png", "caption": "Figure 9: Architecture of Text VAE and Text Encoders in OmniFlow. SD3 (Top) uses three text encoders: CLIP-L, CLIP-G, and T5-XXL. OmniFlow\u00a0(Middile) replaces the 4.7B T5-XXL with a VAE encoder based on Flan-T5-L. CLIP encoders become optional and are not used for tasks without clean text inputs. The decoder of VAE (Bottom) is based on TinyLlama-1.1B. The VAE embedding is used as the prefix for decoding.", "description": "Figure 9 details the architecture of the text Variational Autoencoder (VAE) and text encoders used in the OmniFlow model.  The top portion shows the architecture of Stable Diffusion 3 (SD3), which uses three text encoders: CLIP-L, CLIP-G, and the large T5-XXL model.  The middle section illustrates how OmniFlow modifies this architecture.  It replaces the large and computationally expensive T5-XXL model with a more efficient VAE encoder built upon the Flan-T5-L model.  The CLIP encoders become optional in OmniFlow and are only used when clean text inputs are available; they are not necessary for tasks that don't involve text input.  Finally, the bottom section shows the VAE decoder, which is based on the TinyLlama-1.1B model.  The embedding generated by the VAE is used as a prefix in the decoding process.", "section": "3. Model Architecture"}, {"figure_path": "https://arxiv.org/html/2412.01169/x14.png", "caption": "Figure 10: Discrete Diffusion Variant of OmniFlow. In this setup, we remove the text VAE and directly pass token embedding to the Omni-Transformer layers. \u201c[m]\u201d indicates a mask token.", "description": "This figure illustrates a variant of the OmniFlow model architecture where discrete diffusion is used for text processing.  Unlike the standard OmniFlow, which uses a text Variational Autoencoder (VAE), this variant directly inputs token embeddings into the Omni-Transformer blocks.  The text is processed in a discrete token space, where tokens are progressively masked using \"[m]\" tokens, representing a masked token. This modification simplifies the text processing pathway and removes the need for the VAE.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.01169/x15.png", "caption": "Figure 11: Synthetic Experiments on three 1D-modalities. We consider the joint distribution of three toy modalities (x1,x2,x3subscript\ud835\udc651subscript\ud835\udc652subscript\ud835\udc653x_{1},x_{2},x_{3}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT), each represented by a vector of dimension 1. Hence, a triplet consisting of three modalities be represented by a point in \u211d3superscript\u211d3\\mathbb{R}^{3}blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT We assume the joint distribution is a uniform distribution in the neighborhood of tetrahedron (Left). We experiment with training OmniFlowusing triplets, pairs, and only individual modalities. Models trained with triplets of three modalities best represent the original distribution.", "description": "This figure displays synthetic experiments performed on three 1D modalities to evaluate the effectiveness of training data configurations on OmniFlow. Each modality (x1, x2, x3) is represented by a 1D vector, resulting in triplets represented as points in 3D space.  A uniform distribution within the neighborhood of a tetrahedron is assumed as the joint distribution. The experiment compares the performance of OmniFlow trained on different data configurations: triplets (all three modalities together), pairs (combinations of two modalities), and individual modalities. The results show that models trained on triplets best capture the original distribution.", "section": "B. Additional Discussions"}]