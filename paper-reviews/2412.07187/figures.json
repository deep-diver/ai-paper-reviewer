[{"figure_path": "https://arxiv.org/html/2412.07187/x1.png", "caption": "Figure 1: Left. Existing methods mainly explore defenses mechanisms on the shared gradients. Such mechanisms, including SMC, HE, and DP, inherently involve substantial privacy-utility trade-offs. Right. A novel FL framework that \u201cbreaks the direct connection\u201d between the shared parameters and the local private data is proposed to achieve a favorable privacy-utility trade-off.", "description": "The figure illustrates two approaches to securing Federated Learning (FL) against gradient inversion attacks. The left panel depicts traditional methods (SMC, HE, DP) which focus on enhancing the security of shared gradients, but often compromise model accuracy. The right panel presents a novel FL framework that decouples shared parameters from local private data, thus mitigating the privacy-utility trade-off.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.07187/x2.png", "caption": "Figure 2: The proposed HyperFL framework. HyperFL decouples each client\u2019s network into the former feature extractor f(;\u03b8i)f(;\\theta_{i})italic_f ( ; italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) and the latter classifier head g(;\u03d5i)g(;{\\phi_{i}})italic_g ( ; italic_\u03d5 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). An auxiliary hypernetwork h(;\u03c6i)h(;{\\varphi_{i}})italic_h ( ; italic_\u03c6 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) is introduced to generate local clients\u2019 feature extractor f(;\u03b8i)f(;\\theta_{i})italic_f ( ; italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) using the client\u2019s private embedding vector \ud835\udc2fisubscript\ud835\udc2f\ud835\udc56\\mathbf{v}_{i}bold_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, i.e., \u03b8i=h\u2062(\ud835\udc2fi;\u03c6i)subscript\ud835\udf03\ud835\udc56\u210esubscript\ud835\udc2f\ud835\udc56subscript\ud835\udf11\ud835\udc56{\\theta_{i}}=h({{\\bf{v}}_{i}};{\\varphi_{i}})italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_h ( bold_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; italic_\u03c6 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). These generated parameters are then used to extract features from the input xisubscript\ud835\udc65\ud835\udc56{x}_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which are subsequently fed into the classifier to obtain the output y^isubscript^\ud835\udc66\ud835\udc56\\hat{y}_{i}over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, expressed as y^i=g\u2062(f\u2062(xi;\u03b8i);\u03d5i)subscript^\ud835\udc66\ud835\udc56\ud835\udc54\ud835\udc53subscript\ud835\udc65\ud835\udc56subscript\ud835\udf03\ud835\udc56subscriptitalic-\u03d5\ud835\udc56\\hat{y}_{i}=g(f({x}_{i};\\theta_{i});\\phi_{i})over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_g ( italic_f ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ; italic_\u03d5 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). Throughout the FL training, only the hypernetwork \u03c6isubscript\ud835\udf11\ud835\udc56\\varphi_{i}italic_\u03c6 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is shared, while all other components are kept private, thus effectively mitigating potential privacy leakage concerns.", "description": "HyperFL framework protects data privacy by breaking the direct link between shared parameters and local data. It splits each client's network into a feature extractor and a classifier.  A hypernetwork generates the feature extractor parameters using a private client embedding vector. Only the hypernetwork parameters are shared during federated learning, keeping other components private to prevent privacy leakage from gradient inversion attacks.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2412.07187/x3.png", "caption": "Figure 3: The proposed HyperFL-LPM framework within each client. In this framework, the weights of the pre-trained model are fixed, while only the classifier, hypernetwork, and client embedding are trainable. Note that \u03b8\ud835\udf03\\thetaitalic_\u03b8 here represents the parameters of the adapters.", "description": "HyperFL-LPM is a variation of the HyperFL framework designed for use with large pre-trained models.  Instead of generating the entire feature extractor's parameters, the hypernetwork generates only the parameters for adapter modules. These adapters modify the pre-trained feature extractor, allowing for personalization without the computational cost of training a large model from scratch.  The pre-trained model's weights remain fixed; only the classifier, hypernetwork, and client embeddings are updated during training.  This approach enhances efficiency when working with complex models, and helps to maintain privacy.", "section": "3.3 HyperFL-LPM"}, {"figure_path": "https://arxiv.org/html/2412.07187/x4.png", "caption": "Figure 4: (a) Average training loss of different methods on the EMNIST dataset with 20 clients. (b) Parameter difference of the generated feature extractor of one client between adjacent training round on the EMNIST dataset with 20 clients.", "description": "Figure 4 presents a comparative analysis of the training performance and parameter stability of the proposed HyperFL framework against other established methods. Subfigure (a) illustrates the average training loss curves across multiple training rounds for different algorithms, revealing the convergence speed and overall training efficiency. Subfigure (b) focuses on the parameter changes in the generated feature extractor of a single client between consecutive training rounds. This visualization helps to assess the stability and convergence behavior of the model parameters during the training process. The EMNIST dataset is used with 20 participating clients for this comparison.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.07187/x5.png", "caption": "Figure 5: Reconstructed images of IG.", "description": "This figure visualizes the results of Gradient Inversion Attacks (GIA) on various federated learning (FL) methods.  The \"Ground Truth\" column shows the original images from the EMNIST and CIFAR-10 datasets used for training. Subsequent columns display the reconstructed images obtained by applying the IG (Geiping et al., 2020) attack to different FL approaches, namely FedAvg, pFedHN, pFedHN-PC, DP-FedAvg, CENTAUR, PPSGD, and the proposed HyperFL method.  The visual comparison highlights the effectiveness of the HyperFL framework in resisting GIA, as its reconstructed images are significantly less clear and closer to random noise than other methods.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.07187/x6.png", "caption": "Figure 6: Reconstructed images of ROG.", "description": "Figure 6 presents the results of applying the ROG (Reconstruction via Optimization of Gradients) attack method on the HyperFL framework. The figure visually compares the original images (ground truth) to the images reconstructed by the ROG attack after the HyperFL model has been trained.  This comparison allows for an assessment of the effectiveness of the HyperFL framework in preventing reconstruction attacks.  The quality of the reconstructed images is a direct indication of the degree to which the training data's privacy has been compromised by the attack.  Lower quality reconstructions, significantly different from the original images, demonstrate the stronger privacy-preserving capabilities of the HyperFL framework.", "section": "5.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2412.07187/x7.png", "caption": "Figure 7: Reconstructed images of the tailored attack method. The first row contains the original images, while the second row shows the reconstruction results.", "description": "This figure displays the results of a tailored attack method designed to test the privacy-preserving capabilities of the HyperFL framework.  The top row shows the original images from the EMNIST and CIFAR-10 datasets used in the experiment. The bottom row presents the reconstructed images produced by the attack method. The comparison allows for a visual assessment of the effectiveness of HyperFL in protecting the privacy of sensitive data by observing how closely the reconstructed images resemble the originals.", "section": "5.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2412.07187/x8.png", "caption": "Figure 8: (a) Label distribution of the EMNIST dataset with 20 clients. (b) The t-SNE visualization of the learned client embeddings of the EMNIST dataset with 20 clients.", "description": "Figure 8 presents a detailed visualization of the learned client embeddings in the EMNIST dataset using 20 clients.  Panel (a) shows the distribution of labels across the 20 clients, highlighting the non-IID nature of the data. Each client's data is not uniformly distributed across the 62 classes in EMNIST.  Panel (b) uses t-SNE to reduce the high-dimensional client embedding vectors to two dimensions, allowing for visualization.  The clustering of the points indicates meaningful relationships between clients based on their data characteristics.  Clients with similar data distributions cluster closely together. This visualization demonstrates the effectiveness of the learned client embeddings in capturing the underlying data heterogeneity and relationships among clients in a federated learning setting.", "section": "5 Experiments"}]