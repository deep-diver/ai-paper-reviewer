[{"figure_path": "https://arxiv.org/html/2501.01957/extracted/6109788/figures/vita_demo.jpg", "caption": "Figure 1: VITA-1.5 enables near real-time vision and speech interaction via an end-to-end framework. It allows you to turn on the camera and have a fluent speech conversation. Please see our demo video at this YouTube link.", "description": "Figure 1 showcases VITA-1.5's real-time vision and speech interaction capabilities.  The image displays a user interacting with the system using a mobile phone equipped with a camera. The model processes both visual input from the camera and audio from the user's speech, enabling a fluent conversational interaction. The image highlights the system's end-to-end nature, suggesting seamless integration of visual and audio processing within the language model.  A YouTube link is provided for a demonstration video, allowing readers to directly experience the model's functionality.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.01957/x1.png", "caption": "Figure 2: Overall Architecture of VITA-1.5. The input side consists of vision and audio encoders, along with their adapters connected to a LLM. The output side has an end-to-end speech generation module, rather than directly using an external TTS model as the initial VITA-1.0 version\u00a0[16].", "description": "VITA-1.5's architecture integrates vision and audio encoders with adapters, connecting them to a large language model (LLM).  Unlike its predecessor, VITA-1.0, it features an end-to-end speech generation module, eliminating the need for a separate text-to-speech (TTS) system. This design improves efficiency and coherence in speech output.", "section": "3 VITA-1.5"}, {"figure_path": "https://arxiv.org/html/2501.01957/x2.png", "caption": "Figure 3: Training Pipeline of VITA-1.5. The training process is divided into three stages to incrementally incorporate vision and audio into the LLM while relieving modality conflicts. Stage I focuses on Vision-Language Training, including vision alignment (Stage 1.1, using 20% caption data from Table\u00a01), vision understanding (Stage 1.2, using 100% caption data), and instruction tuning for visual QA (Stage 1.3, using 20% caption data and 100% QA data). Stage 2 introduces Audio Input Tuning, with audio alignment (Stage 2.1, utilizing 11,000 hours of speech-transcription pairs) and instruction tuning for speech QA (Stage 2.2, sampling 4% caption data and 20% QA data). Finally, Stage 3 focuses on Audio Output Tuning, including the training of the codec model (Stage 3.1, using 3,000 hours of text-speech data) and speech decoder training (Stage 3.2). The percentages shown in the image correspond to the data sampling ratios specified in Table\u00a01.", "description": "This figure illustrates the three-stage training pipeline of the VITA-1.5 model.  Each stage progressively integrates a new modality (vision and then audio) into the Language Learning Model (LLM), mitigating conflicts between modalities. Stage 1 focuses solely on vision and language, establishing the model's visual understanding. Stage 2 introduces audio input processing, enabling the model to understand and respond to audio. Finally, Stage 3 develops speech output capabilities, enabling end-to-end speech generation without external TTS modules. The figure details the specific data used at each stage and the proportions used, referring to Table 1 for exact figures.  It shows the model architecture and data flow at each training stage, clearly illustrating the incremental approach to multi-modal learning.", "section": "3 VITA-1.5"}]