[{"content": "| Data Scenario | QA Type | Dataset Name | Questions (K) | Language |\n|---|---|---|---|---|\n| General Image | Description | ShareGPT4V | 99.50 | Eng |\n|  |  | ALLaVA-Caption | 697.40 | Eng |\n|  |  | ShareGTP4o-Image | 55.50 | Eng |\n|  |  | Synthetic Data | 593.70 | CN |\n| General Image | QA | LLaVA-150K | 218.36 | CN |\n|  |  | LLaVA-Mixture-sample | 1872.10 | Eng |\n|  |  | LVIS-Instruct | 939.36 | Eng |\n|  |  | ScienceQA | 12.72 | Eng |\n|  |  | ChatQA | 7.39 | Eng |\n|  |  | LLaVA-OV General | 1754.65 | Eng |\n|  |  | LLaVA-OV Math Reasoning | 1140.92 | Eng |\n|  |  | Synthetic Data | 212.68 | CN |\n| OCR & Diagram | Description | Anyword-3M | 1709.30 | CN |\n|  |  | ICDAR2019-LSVT | 366.30 | CN |\n|  |  | UReader | 100.00 | Eng |\n|  |  | SynDOG-EN | 100.00 | Eng |\n|  |  | SynDOG-CN | 101.90 | CN |\n| OCR & Diagram | QA | ICDAR2019-LSVT-QA | 630.08 | CN |\n|  |  | LLaVA-OV Doc Chart Screen | 4431.50 | Eng |\n|  |  | LLaVA-OV General OCR | 404.20 | Eng |\n| General Video | Description | ShareGemini | 205.70 | CN |\n|  |  | Synthetic Data | 569.40 | CN & Eng |\n| General Video | QA | Synthetic Data | 4336.30 | CN & Eng |\n| Pure Text | QA | Synthetic Data | 1574.20 | CN & Eng |\n| Total |  |  | 22133.16 | CN & Eng |", "caption": "Table 1: Training data of multimodal instruction tuning. The images of the synthetic data come from open-source datasets like Wukong\u00a0[19], LAION\u00a0[46], and CC12M\u00a0[5].", "description": "Table 1 details the datasets used for training the VITA-1.5 multimodal large language model.  It's broken down by data type (image captioning, image QA, OCR & diagrams, video data, and pure text data), providing the dataset name, number of questions (in thousands), and language(s) for each.  Synthetic image data was generated using open-source datasets like Wukong, LAION, and CC12M.", "section": "3.2 Training Data"}, {"content": "| Questions (K) |\n|---|---|", "caption": "Table 2: Evaluation on Image Understanding Benchmarks. VITA-1.5 shows performance comparable to the leading open-source models and advanced closed-source counterparts. MMB refers to MMBench, MMS to MMStar, Hal to HallusionBench, MathV to MathVista, and OCR to OCRBench. Note that after the training of Stages 2 (Audio Input Tuning) and 3 (Audio Output Tuning), VITA-1.5 retains almost its original visual-language capabilities in Stage 1 (Vision-Language Training).", "description": "Table 2 presents a comprehensive evaluation of VITA-1.5's image understanding capabilities by comparing its performance against leading open-source and closed-source large language models (LLMs) across multiple benchmark datasets.  These benchmarks assess various aspects of image understanding, including general multimodal capabilities (MMBench, MMStar),  hallucination detection (HallusionBench), mathematical reasoning (MathVista), and optical character recognition (OCRBench).  The results show that VITA-1.5 achieves performance comparable to top-performing models, demonstrating its robust image understanding abilities.  Importantly, the table also highlights that the model retains its strong visual-language capabilities even after undergoing additional audio training stages (Stages 2 and 3).", "section": "4.1 Vision-Language Evaluation"}, {"content": "| Method | LLM | MMB | MMB | MMS | MMMU | MathV | Hal | AI2D | OCR | MMVet | MME | Avg |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| VILA-1.5 | Vicuna-v1.5-13B | 68.5 | 44.2 | 41.1 | 42.5 | 39.3 | 69.9 | 460.0 | 45.0 | 1718.2 | 52.1 |\n| LLaVA-Next | Yi-34b | 77.8 | 51.6 | 48.8 | 40.4 | 34.8 | 78.9 | 574.0 | 50.7 | 2006.5 | 58.3 |\n| CogVLM2 | Llama3-8B-Instruct | 70.7 | 50.5 | 42.6 | 38.6 | 41.3 | 73.4 | 757.0 | 57.8 | 1869.5 | 58.8 |\n| InternLM-Xcomposer2 | InternLM2-7B | 77.6 | 56.2 | 41.4 | 59.5 | 41.0 | 81.2 | 532.0 | 46.7 | 2220.4 | 61.2 |\n| Cambrian | Nous-Hermes-2-Yi-34B | 77.8 | 54.2 | 50.4 | 50.3 | 41.6 | 79.5 | 591.0 | 53.2 | 2049.9 | 61.4 |\n| InternVL-Chat-1.5 | InternLM2-20B | 79.7 | 57.1 | 46.8 | 54.7 | 47.4 | 80.6 | 720.0 | 55.4 | 2189.6 | 65.1 |\n| Ovis1.5 | Gemma2-9B-It | 77.3 | 58.1 | 49.7 | 65.6 | 48.2 | 84.5 | 752.0 | 53.8 | 2125.2 | 66.9 |\n| InternVL2 | InternLM2.5-7b | 79.4 | 61.5 | 51.2 | 58.3 | 45.0 | 83.6 | 794.0 | 54.3 | 2215.1 | 67.3 |\n| MiniCPM-V 2.6 | Qwen2-7B | 78.0 | 57.5 | 49.8 | 60.6 | 48.1 | 82.1 | 852.0 | 60.0 | 2268.7 | 68.5 |\n| Proprietary |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4V | - | 65.5 | 50.4 | 59.3 | 48.2 | 39.3 | 71.4 | 678.0 | 49.0 | 1790.3 | 58.5 |\n| GPT-4o mini | - | 76.0 | 54.8 | 60.0 | 52.4 | 46.1 | 77.8 | 785.0 | 66.9 | 2003.4 | 66.3 |\n| Gemini 1.5 Pro | - | 73.9 | 59.1 | 60.6 | 57.7 | 45.6 | 79.1 | 754.0 | 64.0 | 2110.6 | 67.2 |\n| GPT-4o | - | 82.8 | 61.6 | 62.8 | 56.5 | 51.7 | 77.4 | 663.0 | 66.5 | 2328.7 | 69.3 |\n| Claude3.5 Sonnet | - | 78.5 | 62.2 | 65.9 | 61.6 | 49.9 | 80.2 | 788.0 | 66.0 | 1920.0 | 69.3 |\n| Ours |  |  |  |  |  |  |  |  |  |  |  |\n| VITA-1.0 | Mixtral-8x7B | 71.8 | 46.4 | 47.3 | 44.9 | 39.7 | 73.1 | 678.0 | 41.6 | 2097.0 | 57.8 |\n| VITA-1.5 (Stage 1) | Qwen2-7B | 77.1 | 59.1 | 53.1 | 66.2 | 44.1 | 80.3 | 752.0 | 51.1 | 2311.0 | 67.1 |\n| VITA-1.5-Audio (Stage 3) | Qwen2-7B | 76.7 | 59.9 | 52.1 | 66.2 | 44.9 | 79.3 | 732.0 | 49.6 | 2352.0 | 66.8 |", "caption": "Table 3: Evaluation on Video Understanding Benchmarks. Although VITA-1.5 still lags behind models like GPT-4o and Gemini-1.5-Pro, it performs comparably to many open-source models. Note that after the training of Stages 2 (Audio Input Tuning) and 3 (Audio Output Tuning), VITA-1.5 retains almost its original visual-language capabilities in Stage 1 (Vision-Language Training).", "description": "Table 3 presents a comprehensive evaluation of VITA-1.5's video understanding capabilities, comparing its performance against several leading open-source and proprietary large language models (LLMs) across multiple benchmarks.  The benchmarks assess various aspects of video understanding, including video-text alignment, video question answering, and overall video comprehension.  While VITA-1.5 doesn't surpass the performance of top-tier proprietary models like GPT-40 and Gemini, it demonstrates performance on par with many other open-source models.  Importantly, the table notes that the audio training stages (2 and 3) do not significantly impact VITA-1.5's original visual-language abilities established in Stage 1.", "section": "4.1 Vision-Language Evaluation"}, {"content": "| Method | LLM | Video-MME w/o sub | Video-MME w/ sub | MVBench | TempCompass |\n|---|---|---|---|---|---| \n| Video-LLaVA | Vicuna-v1.5-13B | 39.9 | 41.6 |  | 49.8 |\n| SliME | Llama3-8B-Instruct | 45.3 | 47.2 | - | - |\n| LongVA | Qwen2-7B | 52.6 | 54.3 | - | 57.0 |\n| VILA-1.5 | Llama3-8B-Instruct | - | - | - | 58.8 |\n| InternLM-XComposer-2.5 | InternLM2-7B | - | - | - | 62.1 |\n| LLaVA-OneVision | Qwen2-7B | 58.2 | 61.5 | 56.7 | 64.2 |\n| InternVL-2 | InternLM2.5-7b | - | - | - | 66.0 |\n| MiniCPM-V-2.6 | Qwen2-7B | 60.9 | 63.7 | - | 66.3 |\n|  |  |  |  |  |  |\n| GPT-4o-mini | - | 64.8 | 68.9 | - |  |\n| Gemini-1.5-Pro | - | 75.0 | 81.3 | - | 67.1 |\n| GPT-4o | - | 71.9 | 77.2 | - | 73.8 |\n|  |  |  |  |  |  |\n| VITA-1.0 | Mixtral-8x7B | 55.8 | 59.2 | - | 62.3 |\n| VITA-1.5 (Stage 1) | Qwen2-7B | 56.8 | 59.5 | 56.8 | 65.5 |\n| VITA-1.5 (Stage 3) | Qwen2-7B | 56.1 | 58.7 | 55.4 | 66.7 |", "caption": "Table 4: Evaluation on ASR Benchmarks. VITA-1.5 has demonstrated strong performance in both Mandarin and English ASR tasks. It outperforms specialized speech models, achieving better results in both languages.", "description": "Table 4 presents a comprehensive evaluation of VITA-1.5's performance on Automatic Speech Recognition (ASR) tasks, using both Mandarin and English datasets.  The table compares VITA-1.5 against several baseline ASR models, showcasing its character error rate (CER) for Mandarin and word error rate (WER) for English.  The results highlight VITA-1.5's superior performance compared to specialized speech models, demonstrating its ability to achieve state-of-the-art results in both languages. This underscores VITA-1.5's robust speech capabilities.", "section": "4 Evaluation"}]