[{"content": "| | | |\n|---|---|---| \n| ![browser](https://arxiv.org/html/2412.14161/extracted/6080186/figs/icon/browser.png) | **Website** | https://the-agent-company.com |\n| ![Uncaptioned image](https://arxiv.org/html/2412.14161/x1.png) | **Code** | https://github.com/TheAgentCompany/TheAgentCompany |\n| ![desktop](https://arxiv.org/html/2412.14161/extracted/6080186/figs/icon/desktop.png) | **Evaluations** | https://github.com/TheAgentCompany/experiments |", "caption": "Table 1: Comparison of different AI agent benchmarks. Interface: the interface agent has access to; \u00a0is web browser, \u00a0is desktop, \u00a0is API usage, \u00a0is Python script, \u00a0is chat platform, \u00a0is bash terminal. Supported Tasks: tasks in the benchmark, \u2217*\u2217 indicate tasks with no association with real-world occupations; SE refers to software engineering, HR is human resources, PM is project manager. Checkpoint-based evaluation: if tasks are evaluated at intermediate checkpoints and assigned partial scores. Interact with NPC Agents: If the agent can interact with other NPC agents during task-solving.", "description": "This table compares existing AI agent benchmarks across various criteria, including the interfaces they use (web, desktop, API, etc.), the types of tasks they support (real-world work-related vs. not), whether they use checkpoint-based evaluation with partial scores, and whether agents can interact with simulated colleagues (NPC agents).  The table aims to highlight TheAgentCompany's unique features in comparison to these other benchmarks by checking of each benchmarks meet those features.", "section": "2 BENCHMARK DESIDERATA AND COMPARISON TO OTHER BENCHMARKS"}, {"content": "| Model | Success | Score | Steps | Costs |\n|---|---|---|---|---| \n| API-based Models | | | | |\n| Claude-3.5-Sonnet | 24.0% | 34.4% | 29.17 | $6.34 |\n| Gemini-2.0-Flash | 11.4% | 19.0% | 39.85 | $0.79 |\n| GPT-4o | 8.6% | 16.7% | 14.55 | $1.29 |\n| Gemini-1.5-Pro | 3.4% | 8.0% | 22.10 | $6.78 |\n| Amazon-Nova-Pro-v1 | 1.7% | 5.7% | 19.59 | $1.55 |\n| Open-weights Models | | | | |\n| Llama-3.1-405b | 7.4% | 14.1% | 22.95 | $3.21 |\n| Llama-3.3-70b | 6.9% | 12.8% | 20.93 | $0.93 |\n| Qwen-2.5-72b | 5.7% | 11.8% | 23.99 | $1.53 |\n| Llama-3.1-70b | 1.7% | 6.5% | 19.18 | $0.83 |\n| Qwen-2-72b | 1.1% | 4.2% | 23.70 | $0.28 |", "caption": "Table 2: Example task intents and checkpoints for three domains.", "description": "This table provides example task intents and checkpoints for three domains in TheAgentCompany, namely, SWE, Finance, and PM. Each domain includes a task intent, which is a brief description of the task, and several checkpoints that evaluate the agent's progress in completing the task. Checkpoints reflect intermediate steps and measure completion based on actions performed, accuracy, and collaboration elements. This table showcases the diversity and structure of the tasks designed in TheAgentCompany.", "section": "4 Task Structure"}, {"content": "| Model | *GitLab* (71 tasks) | *Plane* (17 tasks) | *RocketChat* (79 tasks) | *ownCloud* (70 tasks) |\n|---|---|---|---|---| \n|---| Success (%) | Score (%) | Success (%) | Score (%) | Success (%) | Score (%) | Success (%) | Score (%) |\n| **API-based Models** | | | | | | | | |\n| Claude-3.5-Sonnet | 30.99 | 40.25 | 41.18 | 50.37 | 21.52 | 34.68 | 10.00 | 21.81 |\n| Gemini-2.0-Flash | 11.27 | 18.21 | 17.65 | 29.84 | 13.92 | 23.34 | 2.86 | 8.52 |\n| GPT-4o | 11.27 | 19.46 | 23.53 | 33.68 | 5.06 | 16.08 | 1.43 | 7.76 |\n| Gemini-1.5-Pro | 2.82 | 3.88 | 5.88 | 14.05 | 3.80 | 10.97 | 0.00 | 4.22 |\n| Amazon-Nova-Pro-v1 | 2.82 | 7.22 | 5.88 | 16.67 | 1.27 | 5.36 | 0.00 | 2.43 |\n| **Open-weights Models** | | | | | | | | |\n| Llama-3.1-405b | 5.63 | 11.84 | 29.41 | 39.12 | 8.86 | 16.46 | 0.00 | 4.45 |\n| Llama-3.3-70b | 8.45 | 14.26 | 11.76 | 21.65 | 5.06 | 12.06 | 0.00 | 3.76 |\n| Qwen-2.5-72b | 5.63 | 11.33 | 11.76 | 23.56 | 5.06 | 12.60 | 0.00 | 4.14 |\n| Llama-3.1-70b | 1.41 | 6.09 | 5.88 | 15.35 | 2.53 | 8.23 | 0.00 | 3.32 |\n| Qwen-2-72b | 1.41 | 1.94 | 5.88 | 12.45 | 0.00 | 4.88 | 0.00 | 2.60 |", "caption": "Table 3: Performance comparison of various foundation models on TheAgentCompany.", "description": "This table compares the performance of different large language models (LLMs) on a set of real-world tasks as defined in TheAgentCompany benchmark. It includes both API-based models (like Claude, Gemini, GPT-40, etc.) and open-weight models (like Llama, Qwen, etc.). The metrics used for comparison include success rate, overall score (taking into account partial completions), number of steps taken per task, and cost per task.", "section": "7. EXPERIMENTAL RESULTS"}, {"content": "| Model | *SDE* (69 tasks) | *PM* (28 tasks) | *DS* (14 tasks) | *Admin* (15 tasks) | *HR* (29 tasks) | *Finance* (12 tasks) | *Other* (8 tasks) |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | Success | Score | Success | Score | Success | Score | Success | Score | Success | Score | Success | Score | Success | Score |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **API-based Models** | | | | | | | | | | | | | |\n| Claude-3.5-Sonnet | 30.43 | 38.02 | 35.71 | 51.31 | 14.29 | 21.70 | 0.00 | 11.59 | 24.14 | 34.49 | 8.33 | 25.17 | 12.50 | 22.40 |\n| Gemini-2.0-Flash | 13.04 | 18.99 | 17.86 | 31.71 | 0.00 | 6.49 | 6.67 | 15.20 | 17.24 | 23.08 | 0.00 | 4.31 | 0.00 | 10.05 |\n| GPT-4o | 13.04 | 19.18 | 17.86 | 32.27 | 0.00 | 4.70 | 6.67 | 13.89 | 0.00 | 8.28 | 0.00 | 7.36 | 0.00 | 10.78 |\n| Gemini-1.5-Pro | 4.35 | 5.64 | 3.57 | 13.19 | 0.00 | 4.82 | 6.67 | 9.92 | 3.45 | 11.42 | 0.00 | 2.78 | 0.00 | 8.07 |\n| Amazon-Nova-Pro-v1 | 2.90 | 6.07 | 3.57 | 12.54 | 0.00 | 3.27 | 0.00 | 0.00 | 0.00 | 4.27 | 0.00 | 2.78 | 0.00 | 2.86 |\n| **Open-weights Models** | | | | | | | | | | | | | |\n| Llama-3.1-405b | 5.80 | 11.33 | 21.43 | 35.62 | 0.00 | 5.42 | 0.00 | 3.33 | 6.90 | 12.56 | 0.00 | 5.00 | 12.50 | 17.45 |\n| Llama-3.3-70b | 11.59 | 16.49 | 7.14 | 19.83 | 0.00 | 4.70 | 0.00 | 1.67 | 6.90 | 11.38 | 0.00 | 5.69 | 0.00 | 7.03 |\n| Qwen-2.5-72b | 7.25 | 11.99 | 10.71 | 22.90 | 0.00 | 5.42 | 0.00 | 2.14 | 6.90 | 12.36 | 0.00 | 7.15 | 0.00 | 5.99 |\n| Llama-3.1-70b | 1.45 | 4.77 | 3.57 | 15.16 | 0.00 | 5.42 | 0.00 | 2.42 | 3.45 | 7.19 | 0.00 | 3.82 | 0.00 | 2.86 |\n| Qwen-2-72b | 2.90 | 3.68 | 0.00 | 7.44 | 0.00 | 4.70 | 0.00 | 0.56 | 0.00 | 4.14 | 0.00 | 3.61 | 0.00 | 4.95 |", "caption": "Table 4: Performance of the models in tasks that require different platforms in TheAgentCompany. All numbers are percentages (%).", "description": "This table presents a breakdown of the performance of different large language models on tasks that involve interactions with specific platforms within TheAgentCompany, such as GitLab, Plane, RocketChat, and ownCloud. The performance is measured in terms of success rate and overall score, both presented as percentages.", "section": "7. Experimental Results"}]