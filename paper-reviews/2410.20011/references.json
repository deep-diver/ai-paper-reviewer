{"references": [{" publication_date": "2020", "fullname_first_author": "Iz Beltagy", "paper_title": "Longformer: The long-document transformer", "reason": "This paper is highly significant due to its introduction of the Longformer architecture, a key advancement in handling long sequences within the context of small language models.  The Longformer architecture addresses the computational limitations of standard transformers when dealing with extensive text inputs, making it particularly relevant to the focus of this survey on resource-efficient models.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Nikita Kitaev", "paper_title": "Reformer: The efficient transformer", "reason": "The Reformer model proposed a novel approach to self-attention mechanisms, significantly improving the efficiency of processing long sequences. This work is crucial for creating small language models (SLMs) that retain high performance while reducing computational complexity, a key theme of this survey.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Yi Sun", "paper_title": "MobileBERT: a compact task-agnostic BERT for resource-limited devices", "reason": "MobileBERT is a seminal work that demonstrated the feasibility of building lightweight and highly efficient BERT-like models for resource-constrained settings. Its emphasis on reducing the size and speed of the model while retaining considerable performance makes it a critical contribution to the field of SLMs.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Xiaoqi Jiao", "paper_title": "TinyBERT: Distilling BERT for natural language understanding", "reason": "TinyBERT presented a method for effectively distilling knowledge from a larger BERT model to create a significantly smaller model that retained much of the original model's performance.  This is a seminal work in model compression for SLMs and is highly relevant to the discussion of efficient training techniques in this survey.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "reason": "While not directly a small language model, BERT's impact on the field of NLP and its subsequent influence on the development of numerous SLMs makes it a fundamental reference. Its architectural innovations served as a basis for several lightweight architectures discussed in the survey.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Paulius Micikevicius", "paper_title": "Mixed precision training", "reason": "This paper introduced the concept of mixed-precision training, a crucial technique for making deep learning models more efficient and easier to train. This is especially relevant to the training of small language models (SLMs) given their need for efficient use of computational resources.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Neil Burgess", "paper_title": "Bfloat16 processing for neural networks", "reason": "The introduction of the BFLOAT16 format is a significant development in the field of mixed-precision training. This paper provides valuable insights into how this format can be used to improve training efficiency and numerical stability, thereby directly addressing the resource constraints inherent in training small language models.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Jack W Rae", "paper_title": "Scaling language models: Methods, analysis & insights from training Gopher", "reason": "This paper provides insights into the training of large language models and how the results could translate to training small language models. The challenges and trade-offs involved in training large models can be directly relevant to understanding how to train small language models, making it a valuable reference in this survey.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Brian Lester", "paper_title": "The power of scale for parameter-efficient prompt tuning", "reason": "Prompt tuning is a highly effective technique for adapting pre-trained language models to specific tasks with minimal computational overhead. This makes it a key element in the efficient training of SLMs, and this seminal paper establishes its core principles.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "reason": "LoRA (Low-Rank Adaptation) is an innovative parameter-efficient fine-tuning technique that significantly reduces the computational cost of adapting large language models. This is a key method for making small language models (SLMs) more efficient and practical, hence its significance to this survey.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Elias Frantar", "paper_title": "GPTQ: Accurate post-training quantization for generative pre-trained transformers", "reason": "GPTQ is a highly efficient post-training quantization technique that has proven very effective for compressing large language models.  This method directly addresses the need for reducing model size and improving efficiency, which are core goals in building and deploying small language models.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Ji Lin", "paper_title": "AWQ: Activation-aware weight quantization for on-device LLM compression and acceleration", "reason": "AWQ (Activation-Aware Weight Quantization) is an advanced quantization technique that cleverly considers the importance of activations when quantizing weights, resulting in significant improvements in both accuracy and efficiency for on-device applications.  This makes it extremely relevant to the creation of small language models.", "section_number": 4}, {" publication_date": "2015", "fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "reason": "This paper introduced the groundbreaking concept of knowledge distillation, a powerful technique for compressing and transferring knowledge from a large model to a smaller one.  This concept is extensively used in creating efficient SLMs, and its foundational nature makes it essential to this survey.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Iz Beltagy", "paper_title": "Longformer: The long-document transformer", "reason": "This paper is highly relevant to the section on evaluation because its focus on efficient processing of long documents directly impacts the metrics and benchmarks used to assess the performance of small language models in specific applications. Longformer's ability to handle long sequences makes it a benchmark for the performance of SLMs.", "section_number": 5}, {" publication_date": "2019", "fullname_first_author": "Tom Kwiatkowski", "paper_title": "Natural Questions: a benchmark for question answering research", "reason": "Natural Questions is a highly influential benchmark dataset widely used in evaluating the performance of question-answering systems. Its relevance to this survey lies in its use for evaluating small language models (SLMs) and the importance of carefully considering datasets and evaluation metrics when assessing the success of such models.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Paul-Edouard Sarlin", "paper_title": "SuperGlue: Learning feature matching with graph neural networks", "reason": "SuperGlue is a significant benchmark dataset that enables comprehensive evaluation of various models for downstream tasks. Its application in evaluating small language models (SLMs) allows for a more robust assessment of their effectiveness and is critical for this survey.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Jovan Stojkovic", "paper_title": "Towards greener LLMs: Bringing energy-efficiency to the forefront of LLM inference", "reason": "This paper directly addresses the critical issue of energy efficiency in deploying LLMs, particularly relevant to SLMs which frequently target resource-constrained environments.  The focus on reducing energy consumption is crucial in assessing the practical utility of small language models, making it a vital reference in this survey.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Jovan Stojkovic", "paper_title": "DynamoLLM: Designing LLM inference clusters for performance and energy efficiency", "reason": "This paper focuses on the important problem of energy-efficient deployment of large language models. This is particularly relevant to this survey as small language models are often deployed in resource-constrained devices.  Understanding how to design energy-efficient inference clusters is important for making SLMs practical.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Fernanda De La Torre", "paper_title": "LLMR: Real-time prompting of interactive worlds using large language models", "reason": "This paper introduces the use of large language models in interactive environments. While not specifically focused on small models, the concepts it presents are relevant to understanding how to design efficient SLMs for interactive applications such as real-time interactions and content generation in mixed reality environments.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Edward J Hu", "paper_title": "Apple Intelligence foundation language models", "reason": "This work highlights the practical application of small language models on resource-constrained devices, thereby directly addressing a key concern in the survey. The discussion of deployment and performance on real-world devices offers valuable insights for researchers working on SLMs.", "section_number": 6}]}