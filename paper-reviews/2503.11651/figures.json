[{"figure_path": "https://arxiv.org/html/2503.11651/x2.png", "caption": "Figure 1: \nVGGT\nis a large feed-forward transformer with minimal 3D-inductive biases trained on a trove of 3D-annotated data.\nIt accepts up to hundreds of images and predicts cameras, point maps, depth maps, and point tracks for all images at once in less than a second, which often outperforms optimization-based alternatives without further processing.", "description": "VGGT, a large feed-forward transformer, processes up to hundreds of images simultaneously to predict 3D scene properties such as camera poses, point clouds, depth maps, and point tracks.  Its design minimizes reliance on explicit 3D inductive biases, relying instead on a large dataset of 3D-annotated data for training. The model's speed is a key feature, performing the complete 3D reconstruction in under one second, often exceeding the accuracy of optimization-based methods without requiring additional processing.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.11651/x3.png", "caption": "Figure 2: Architecture Overview. Our model first patchifies the input images into tokens by DINO, and appends camera tokens for camera prediction.\nIt then alternates between frame-wise and global self attention layers.\nA camera head makes the final prediction for camera extrinsics and intrinsics, and a DPT\u00a0[87] head for any dense output.", "description": "This figure illustrates the architecture of the Visual Geometry Grounded Transformer (VGGT) model.  The process begins with input images that are initially divided into patches using the DINO (DETR with Improved DeNoising) method. These image patches are then converted into tokens, which are numerical representations used by the transformer network. To predict camera parameters, the model adds special 'camera tokens' to the image tokens. The core of VGGT is a transformer network that uses alternating layers of frame-wise and global self-attention. Frame-wise attention processes information within each individual image, while global self-attention integrates information across all input images.  Finally, a dedicated 'camera head' in the network produces the final estimates for camera extrinsics (position and orientation) and intrinsics (focal length and sensor dimensions). A separate DPT (Depth Prediction Transformer) head generates the dense outputs such as depth maps and point clouds.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.11651/x4.png", "caption": "Figure 3: Qualitative comparison of our predicted 3D points to DUSt3R on in-the-wild images.\nAs shown in the top row, our method successfully predicts the geometric structure of an oil painting, while DUSt3R predicts a slightly distorted plane. In the second row, our method correctly recovers a 3D scene from two images with no overlap, while DUSt3R fails.\nThe third row provides a challenging example with repeated textures, while our prediction is still high-quality.\nWe do not include examples with more than 32 frames, as DUSt3R runs out of memory beyond this limit.", "description": "This figure qualitatively compares the 3D point cloud predictions of the proposed VGGT method and the DUSt3R method on real-world images.  It showcases VGGT's superior performance in three scenarios: reconstructing the geometry of an oil painting (where DUSt3R produces a distorted plane), recovering a 3D scene from only two images with no overlap (a task DUSt3R fails), and handling an image with repetitive textures (where VGGT maintains high quality).  The figure highlights VGGT's ability to produce accurate 3D reconstructions even in challenging situations, unlike DUSt3R, which is limited by memory constraints for scenes with more than 32 images.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2503.11651/x5.png", "caption": "Figure 4: Additional Visualizations of Point Map Estimation. Camera frustums illustrate the estimated camera poses.\nExplore our interactive demo for better visualization quality.", "description": "Figure 4 presents supplementary visualizations enhancing the understanding of point map estimation from the paper.  It showcases multiple examples of 3D point cloud reconstructions, with camera frustums overlaid to clearly illustrate the estimated camera poses and orientations for each scene.  The caption encourages users to explore an interactive demo for an improved and more detailed visualization experience, suggesting that the static figure is a limited representation of the full capabilities of the model.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.11651/x6.png", "caption": "Figure 5: Visualization of Rigid and Dynamic Point Tracking.\nTop: VGGT\u2019s tracking module \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T outputs keypoint tracks for an unordered set of input images depicting a static scene.\nBottom: We finetune the backbone of VGGT to enhance a dynamic point tracker CoTracker\u00a0[56], which processes sequential inputs.", "description": "Figure 5 demonstrates the application of VGGT's tracking module for both static and dynamic scenes. The top row showcases the tracking module's ability to generate keypoint tracks from an unordered set of images depicting a static scene.  The bottom row illustrates how finetuning VGGT's backbone improves the performance of a dynamic point tracker (CoTracker), which typically processes sequential inputs. This highlights VGGT's versatility and adaptability to various tracking scenarios.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.11651/x7.png", "caption": "Figure 6: Qualitative Examples of Novel View Synthesis. The top row shows the input images, the middle row displays the ground truth images from target viewpoints, and the bottom row presents our synthesized images.", "description": "This figure showcases qualitative examples of novel view synthesis.  The top row displays the input images used for synthesis. The middle row shows the corresponding ground truth images from the target viewpoints, providing a reference for comparison. The bottom row presents the novel views synthesized by the VGGT model, illustrating its ability to generate realistic and visually consistent images from unseen perspectives.", "section": "4.6. Finetuning for Downstream Tasks"}]