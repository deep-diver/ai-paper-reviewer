[{"figure_path": "https://arxiv.org/html/2411.19067/x1.png", "caption": "Figure 1: Conventional data augmentations (DA) in semantic segmentation are incompatible with referring image segmentation. Random crop and horizontal flip could change the referred object (e.g., \u201clady under the red umbrella on left\u201d) to another one, and color distortion could make the described object disappear.", "description": "This figure illustrates why standard data augmentation techniques used in semantic segmentation are not directly applicable to referring image segmentation.  The top row shows semantic segmentation, where augmentations like random cropping, flipping, and color distortion do not significantly affect the object of interest. However, in referring image segmentation (bottom row), these same augmentations can drastically change the location, orientation, or even visibility of the referred object, thereby making the text description inaccurate. For example, cropping might remove the object entirely, flipping could change its position relative to the rest of the image which might be specified in the text description (e.g., 'lady on the left'), and color distortion may make the object unrecognizable, rendering the text description incorrect. This highlights the incompatibility of general-purpose image augmentation with the specific demands of referring image segmentation.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.19067/x2.png", "caption": "(a) Performance with conventional augmentations", "description": "This figure compares the performance of various Referring Image Segmentation (RIS) models when using conventional data augmentation techniques, such as random cropping, horizontal flipping, and color distortion. The bar chart displays the Intersection over Union (IoU) metric for each model, illustrating the significant drop in performance caused by these augmentations. This highlights a key challenge in RIS: conventional augmentations, while beneficial in other image segmentation tasks, often disrupt the semantic relationship between the text description and the target object, leading to decreased accuracy.  This finding motivates the need for a novel augmentation strategy designed specifically for RIS.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.19067/x3.png", "caption": "(b) Performance with individual augmentation on CARIS\u00a0[33]", "description": "The figure shows the impact of individual data augmentation techniques on the performance of the CARIS model [33].  It compares the model's performance (measured by IoU) when applying random cropping, horizontal flipping, color distortion, image masking, and text masking individually. This allows for a detailed analysis of how each augmentation method affects the model's ability to accurately segment objects in referring image segmentation tasks. The baseline performance without any augmentation is also shown.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.19067/x4.png", "caption": "Figure 2: Existing RIS methods show a noticeable decline in their performance when applying conventional image augmentations (random cropping, color jittering, and horizontal flipping). In contrast, image masking (I-Mask) and text masking (T-Mask) improve model performance.", "description": "This figure displays a comparison of the performance of existing Referring Image Segmentation (RIS) methods when using conventional image augmentations versus image masking and text masking.  The left graph shows a substantial drop in performance across several state-of-the-art RIS models (CRIS, ETRIS, CARIS, ReMamber) when using random cropping, horizontal flipping, and color jittering. This indicates that conventional data augmentation techniques are not suitable for RIS. The right graph illustrates that utilizing image masking or text masking, on the other hand, significantly improves the performance of at least one RIS model (CARIS). This result highlights the effectiveness of masking techniques in enhancing the robustness and accuracy of RIS models by increasing data diversity without introducing semantic conflicts.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.19067/x5.png", "caption": "Figure 3: The existing RIS method tends to be inaccurate when faced with occluded context. CARIS\u00a0[33] represents the SoTA method in RIS. Words highlighted in red represent occluded objects in the image (left and center) and masked words in the text query (right).", "description": "This figure demonstrates the limitations of existing Referring Image Segmentation (RIS) models when dealing with occluded contexts. It shows examples where the state-of-the-art model, CARIS, fails to accurately segment the target object when parts of the image or the text query are missing. The image on the left demonstrates an intra-object occlusion, the middle image shows an inter-object occlusion, and the image on the right illustrates a text occlusion. Red highlights indicate the missing parts.", "section": "3.2. Masking Strategy for Image and Text"}, {"figure_path": "https://arxiv.org/html/2411.19067/x6.png", "caption": "Figure 4: The overall framework of MaskRIS. Both image and text masking are employed to generate diverse image-text training pairs (Sec.\u00a03.2). To maximize the benefits of the masking strategy, Distortion-aware Contextual Learning (DCL) is introduced (Sec.\u00a03.3).", "description": "This figure illustrates the MaskRIS framework, a novel data augmentation method for Referring Image Segmentation (RIS).  It shows how both image and text masking are used to create more diverse training data. The image masking randomly obscures parts of the input image, while the text masking similarly hides parts of the text description. A key component is Distortion-aware Contextual Learning (DCL), which processes both the original and masked input data through two separate paths.  These paths are then combined using a distillation loss to ensure the model learns robust features regardless of masking. The process improves performance in RIS by training the model to handle incomplete information, variations in visual input, and varied textual descriptions.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.19067/x7.png", "caption": "Figure 5: Qualitative examples of segmentation results on the RefCOCO dataset.", "description": "This figure showcases several examples of referring image segmentation results from the RefCOCO dataset, comparing the ground truth segmentations (GT) with those produced by the CARIS model and the MaskRIS model.  The examples highlight MaskRIS's improved ability to accurately segment objects in various situations, including those with occlusions,  complex visual scenes, and linguistic ambiguity. The improved precision and robustness of MaskRIS compared to CARIS are visually demonstrated through this comparison.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.19067/x8.png", "caption": "Figure 6: Robustness on various image corruptions provided by ImageNet-C\u00a0[12] and linguistically complex situations. The results (oIoU) are evaluated on the RefCOCO validation set.", "description": "This figure displays a robustness evaluation of the MaskRIS model against various image corruptions from the ImageNet-C dataset and linguistically complex situations in referring image segmentation.  The x-axis represents different types of image corruptions and linguistically complex situations. The y-axis shows the overall intersection over union (oIoU) score, a metric measuring the accuracy of the segmentation task.  The bars represent the oIoU achieved by CARIS (the previous state-of-the-art model) and MaskRIS.  The results are based on the RefCOCO validation set, demonstrating MaskRIS's superior performance and robustness across diverse challenges.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.19067/x9.png", "caption": "Figure 7: Qualitative examples of masking strategies on the RefCOCO dataset. I-Mask, T-Mask, and Both denote the results of applying image masking, text masking, and both, respectively.", "description": "This figure showcases the effectiveness of different masking strategies in the Masked Referring Image Segmentation (MaskRIS) framework.  It presents qualitative examples from the RefCOCO dataset, comparing the performance of the baseline model (CARIS) against the MaskRIS model with image masking only (I-Mask), text masking only (T-Mask), and both image and text masking (Both). The visual comparison demonstrates how each masking technique impacts the ability of the model to accurately segment the objects described in the text queries, particularly in challenging situations involving occlusions or incomplete textual information.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.19067/x10.png", "caption": "Figure A: Various types of mask sampling strategies. We use patch-wise sampling\u00a0[11] as our default. For block-wise sampling, we follow BEiT[2] to remove large random blocks. For Cutout\u00a0[8], we follow the implementation of timm\u00a0[51]. For all strategies except Cutout, we maintain a consistent masking ratio of 75%.", "description": "This figure illustrates different mask sampling strategies used for data augmentation in image processing.  The default approach is 'patch-wise sampling', which divides the image into patches and masks a percentage of them randomly.  Other methods are shown for comparison, including 'block-wise sampling' (removing larger random image blocks as done in BEiT), and 'Cutout' (removing a random rectangular area as done in timm).  The key difference between the methods is how image regions are selected for masking.  For all methods except 'Cutout', 75% of the image was consistently masked.  This figure helps visualize the differences in how these strategies mask information.", "section": "A Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.19067/x11.png", "caption": "Figure B: Training loss analysis of MaskRIS on RefCOCO. In the CARIS w/ Masking setting, we employ both image and text masking as data augmentation strategies within the CARIS\u00a0[33] framework. We visualize (a) the training loss for the original (i.e., unmasked) inputs, (b) the training loss for the masked inputs, and (c) the performance of the model on the RefCOCO validation set.", "description": "This figure displays a training loss analysis for the MaskRIS model and a comparison with the CARIS model.  It shows three subplots. (a) Training loss for original (unmasked) inputs for both MaskRIS and CARIS. (b) Training loss for masked inputs for both MaskRIS and CARIS.  (c) Model performance (measured by mean Intersection over Union - mIoU) on the RefCOCO validation set for MaskRIS and CARIS. The CARIS w/ Masking line represents CARIS with both image and text masking added as data augmentation techniques.  The figure demonstrates MaskRIS's improved training stability and better performance compared to CARIS, particularly when using masking.", "section": "A. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.19067/x12.png", "caption": "Figure C: Qualitative examples under various occluded contexts on the RefCOCO dataset. Although CARIS\u00a0[33] tends to be inaccurate under occluded contexts, MaskRIS produces accurate predictions, demonstrating its robustness to occlusion and incomplete information. For text context masking, the word highlighted in red is masked.", "description": "Figure C presents a comparison of referring image segmentation (RIS) results between the CARIS model and the MaskRIS model on the RefCOCO dataset.  The figure showcases examples where parts of the image or the text description are masked (occluded).  The examples highlight situations with intra-object occlusion (where parts of the target object are hidden), inter-object occlusion (where objects surrounding the target are hidden), and text-based occlusion (where words in the text description are masked).  The results demonstrate MaskRIS's improved robustness compared to CARIS, particularly in accurately segmenting objects even when there is missing visual or textual information.  Red highlighting indicates masked words in the text queries.", "section": "3.2. Masking Strategy for Image and Text"}]