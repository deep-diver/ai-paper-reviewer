{"references": [{" publication_date": "2020", "fullname_first_author": "Alexander Kolesnikov", "paper_title": "Big transfer (bit): General visual representation learning", "reason": "This paper is foundational for understanding transfer learning techniques, which are relevant to model merging as both processes involve leveraging pre-trained models and adapting them to new tasks. The concept of efficient transfer learning is particularly crucial in model merging, given that it helps in reducing the computational cost of training specialized models for each new task.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Xipeng Qiu", "paper_title": "Pre-trained models for natural language processing: A survey", "reason": "This survey provides a comprehensive overview of pre-trained models (PTMs) in natural language processing, which form the basis for many large language models (LLMs). Understanding PTMs is crucial to model merging because many merging techniques build upon fine-tuned PTMs or directly integrate multiple PTMs.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Amanda Askell", "paper_title": "A general language assistant as a laboratory for alignment", "reason": "This paper discusses a framework for aligning large language models (LLMs) with human values and preferences.  This aspect is indirectly related to model merging as aligned models might exhibit a more consistent behavior and may be more easily integrated, resulting in a smoother and potentially more successful merging process.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper introduces a technique for training language models to follow instructions effectively. This work is important for model merging because well-trained models, capable of following specific instructions, are more likely to result in successful integration during the merging process and better performance in downstream tasks.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yi-Lin Sung", "paper_title": "An empirical study of multimodal model merging", "reason": "This paper offers a thorough empirical study on multimodal model merging.  As the authors study model merging from an empirical perspective, this paper is directly relevant to the current study. The findings and insights from their analysis can guide the design and evaluation of the novel model merging strategies introduced in the current research.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Gabriel Ilharco", "paper_title": "Editing models with task arithmetic", "reason": "This paper introduces a new approach to model editing using task vectors. Understanding how to effectively edit models could significantly inform the process of selecting appropriate models for merging, as the editing process could be viewed as a precursor to identifying model similarity or relatedness, which is the essence of model kinship.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Timur Garipov", "paper_title": "Loss surfaces, mode connectivity, and fast ensembling of dnns", "reason": "This paper explores the loss surface of deep neural networks (DNNs) and their mode connectivity, offering valuable insights into the optimization process during model merging.  Understanding the properties of the loss surface helps in analyzing the potential issues that might arise during model merging, such as converging to local optima, and how to avoid such issues during merging.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Pavel Izmailov", "paper_title": "Averaging weights leads to wider optima and better generalization", "reason": "This paper is highly relevant to the concept of model kinship because it discusses how averaging weights leads to broader optima and improved generalization. This is directly applicable to the study as averaging weights (or differences in weights) is a core component of calculating the model kinship metric proposed in this research.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Jonathan Frankle", "paper_title": "Linear mode connectivity and the lottery ticket hypothesis", "reason": "This paper explores the concept of linear mode connectivity in neural networks, which is relevant to understanding how model parameters relate to each other and how they may change during merging. The insights from this paper help analyze the potential impact of the merging process on the network's behavior and its generalization ability.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Samuel K. Ainsworth", "paper_title": "Git re-basin: Merging models modulo permutation symmetries", "reason": "This paper presents a method for merging models while considering permutation symmetries, a key factor in the design of effective model merging strategies. Understanding how to handle such symmetries is crucial in the design of robust model merging strategies and informs approaches for evaluating model kinship by emphasizing structural similarities that may not be captured by simpler measures.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Takuya Akiba", "paper_title": "Evolutionary optimization of model merging recipes", "reason": "This paper explores evolutionary algorithms for optimizing model merging, which is highly relevant to the current research.  The concepts and techniques presented here, while using different approaches, share a common goal of developing efficient and effective strategies for model merging and evolution. Studying the evolutionary algorithms and optimization process can inform and complement the model kinship approach.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "MohammadReza Davari", "paper_title": "Model breadcrumbs: Scaling multi-task model merging with sparse masks", "reason": "This paper focuses on scaling multi-task model merging using sparse masks. This is important for large language models (LLMs) as it reduces computational costs while improving performance.  Understanding how to efficiently scale model merging is crucial for making the proposed model kinship metric practically applicable to large-scale LLMs.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Edward Beeching", "paper_title": "Open Ilm leaderboard", "reason": "This work provides a comprehensive leaderboard for evaluating large language models.  It is crucial for the current study as it serves as a benchmark for evaluating the performance of model merging strategies. The leaderboard allows researchers to compare their results with existing state-of-the-art models, providing a crucial context for evaluating the improvements achieved by the proposed model merging strategy.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Charles Goddard", "paper_title": "Arcee's mergekit: A toolkit for merging large language models", "reason": "This paper introduces Mergekit, a toolkit specifically designed for model merging.  The availability of such tools is vital for the reproducibility and accessibility of the research presented in the current study. By utilizing Mergekit, the current researchers ensured their proposed strategies could be easily replicated and extended by others in the community, promoting wider adoption and advancement in the field of LLM merging.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "reason": "This paper introduces Mamba, a linear-time sequence modeling technique that could be used to enhance the efficiency of model merging. This work is significant as it explores different techniques to improve the merging process. Understanding how to optimize sequence modeling can inform model merging strategies and can potentially be combined with the proposed model kinship metric to further enhance performance.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Mani Kumar Tellamekala", "paper_title": "COLD fusion: Calibrated and ordinal latent distribution fusion for uncertainty-aware multimodal emotion recognition", "reason": "This paper focuses on multimodal fusion, providing insights into combining different model types.  While it does not directly address LLM merging, it provides valuable insights into the broader problem of model fusion, which is relevant to understanding the complexities of merging LLMs.  Understanding how to combine information from different modalities can inform how to best integrate information from different LLM sources.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Cong Liu", "paper_title": "Cool-fusion: Fuse large language models without training", "reason": "This paper introduces a novel technique for merging large language models without the need for extensive retraining.  The direct relevance to the study is apparent as the core concern in the current research is how to merge models efficiently and effectively.  By presenting a training-free approach, this work adds important context to understanding the current state of LLM merging.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Jinliang Lu", "paper_title": "Merge, ensemble, and cooperate! A survey on collaborative strategies in the era of large language models", "reason": "This paper is a comprehensive survey of collaborative strategies in the era of large language models. The survey is directly relevant to the current study as it provides context on different approaches to combining LLMs.  The insights gained from the survey can help evaluate the proposed model merging strategy in relation to the broader field of collaborative techniques for LLM integration.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Yu Zhang", "paper_title": "A survey on multi-task learning", "reason": "This survey provides a broad overview of multi-task learning, a critical area related to model merging.  Since model merging is a form of multi-task learning, it helps understand different multi-task approaches.  This context helps evaluate the strengths and limitations of the proposed model merging strategies in relation to established multi-task learning techniques.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Wayne Xin Zhao", "paper_title": "A survey of large language models", "reason": "This survey paper offers a comprehensive overview of large language models (LLMs), forming the basis for the current research.  Understanding the landscape of LLMs is crucial to the study as the proposed model merging techniques are specifically designed to improve upon existing LLM development strategies.", "section_number": 1}]}