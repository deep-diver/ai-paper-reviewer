[{"figure_path": "https://arxiv.org/html/2503.01378/extracted/6247550/figures/teaser.png", "caption": "Figure 1: CognitiveDrone is a VLA system for UAVs that generates smooth 4D control commands from first-person visual inputs and natural language instructions. It combines a 7B-parameter VLA model trained on an extensive open-source dataset of cognitive tasks\u2014including reasoning, human recognition, and symbol understanding\u2014with a 7B-parameter VLM reasoning module that refines task directives. The system is evaluated within CognitiveDroneBench\u2014the first evaluation benchmark for VLA systems tailored to cognitive UAVs\u2014where the drone must navigate a track with gates by selecting the appropriate gate through solving cognitive tasks. We have released the complete dataset, benchmark environment, model weights, and training/inference code as open source.", "description": "The figure illustrates the CognitiveDrone system architecture.  It shows how the system processes first-person visual inputs and natural language instructions to generate smooth 4D control commands for a UAV.  The system integrates a large Vision-Language-Action (VLA) model for control and a Vision-Language Model (VLM) for reasoning.  The VLA model generates high-frequency control commands from visual inputs and refined task directives. The VLM reasoning module processes the instructions and simplifies task directives, improving performance in complex cognitive tasks. The system is evaluated using a novel benchmark called CognitiveDroneBench, which involves a drone navigating a track and selecting gates by solving cognitive tasks.  The entire system, including dataset, benchmark, model weights, and code, is open-source.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2503.01378/extracted/6247550/figures/system_architecture.png", "caption": "Figure 2: CognitiveDrone system architecture.", "description": "This figure illustrates the architecture of the CognitiveDrone system, which uses a combination of visual and textual inputs to generate smooth 4D control commands for UAVs.  The system consists of two main modules: a high-frequency Vision-Language-Action (VLA) model for real-time control and a low-frequency Vision-Language Model (VLM) reasoning module for higher-level task understanding and simplification. The VLA model processes first-person view (FPV) images and refined textual instructions from the VLM, generating the drone's actions.  The VLM module, using Qwen2.5-VL, receives the original user instructions and FPV images and processes them to produce clarified instructions for the VLA model.  The system architecture includes various components, such as image and text tokenizers, action de-tokenizers, and an autopilot SITL (Software-In-The-Loop) simulation running in Gazebo.", "section": "III. SYSTEM ARCHITECTURE"}, {"figure_path": "https://arxiv.org/html/2503.01378/extracted/6247550/figures/examples.png", "caption": "Figure 3: Examples of prepared dataset tasks for VLA to solve cognitive tasks adapted for UAVs.", "description": "This figure displays example tasks from the CognitiveDrone dataset designed to evaluate a Vision-Language-Action (VLA) model's cognitive abilities in a UAV context.  Each example shows a textual instruction given to the UAV (e.g., 'Fly through the gate with cat'), a corresponding image from the UAV's perspective, and the correct solution for the UAV to execute (navigating to a specific gate).  The tasks test different cognitive skills, including human recognition, symbol understanding, and reasoning.  The examples showcase the diversity of the dataset used to train and evaluate the CognitiveDrone model.", "section": "IV. DATASET COLLECTION AND TRAINING PIPELINE"}, {"figure_path": "https://arxiv.org/html/2503.01378/extracted/6247550/figures/learning_curve.png", "caption": "Figure 4:  Metrics Overview: (a) L1 loss indicates absolute prediction errors. (b) Action accuracy quantifies the percentage of correct predictions. (c) Cross-entropy loss measures performance on discretized action tokens.", "description": "Figure 4 presents a detailed performance analysis of the model training process.  It consists of three sub-figures: (a) shows the L1 loss, representing the absolute difference between the predicted and actual values, indicating the magnitude of prediction errors. (b) displays the action accuracy, which is the percentage of correctly predicted actions, providing a direct measure of the model's effectiveness. (c) illustrates the cross-entropy loss, measuring the performance based on how well the predicted action probabilities align with the actual actions, giving insights into the model's confidence in its predictions. This three-part visualization offers a comprehensive view of the model's training progress and overall performance.", "section": "IV. DATASET COLLECTION AND TRAINING PIPELINE"}, {"figure_path": "https://arxiv.org/html/2503.01378/extracted/6247550/figures/benchmark_results.png", "caption": "Figure 5: Benchmark performance on CognitiveDroneBench for the RaceVLA, CognitiveDrone, and CognitiveDrone-R1 models. Shown are scores for Reasoning, Human Recognition, and Symbol Understanding tasks, as well as the overall average.", "description": "Figure 5 presents a bar chart comparing the performance of three different models\u2014RaceVLA, CognitiveDrone, and CognitiveDrone-R1\u2014on the CognitiveDroneBench benchmark.  The benchmark evaluates the models' abilities across three cognitive task categories: Reasoning, Human Recognition, and Symbol Understanding.  The chart displays the success rate (percentage) achieved by each model in each category and provides an overall average success rate.  This allows for a direct comparison of the models' strengths and weaknesses in handling different types of cognitive tasks and helps quantify the impact of integrating a reasoning module (as in CognitiveDrone-R1) on overall performance.", "section": "V. EVALUATION"}]