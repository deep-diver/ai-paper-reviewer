[{"figure_path": "2410.10672/tables/table_7_0.html", "caption": "Table 1: CEREBRAS-GPT: Time Comparison between Matrix Entropy and Matrix Nuclear-Norm", "description": "Table 1 compares the computation time of Matrix Entropy and Matrix Nuclear-Norm for various sizes of the CEREBRAS-GPT model, showing that Matrix Nuclear-Norm is significantly faster.", "section": "5.2 A Comparative Analysis of Computational Time"}, {"figure_path": "2410.10672/tables/table_9_0.html", "caption": "Table 2: Analysis of Length Dynamics", "description": "The table presents the Matrix Nuclear-Norm values for varying lengths of text across different sizes of GPT models, showing the trend of increasing values with length.", "section": "5.3.2 Analysis of Length Dynamics"}, {"figure_path": "2410.10672/tables/table_10_0.html", "caption": "Table 3: Results of prompt learning with (Empty Prompt) and without (Prompt 1, 2, 3) the use of prompts. Incorporating prompts as prefixes before the QA pairs enhances the models' ability to achieve better compression.", "description": "Table 3 shows the impact of using different prompts on the Matrix Nuclear-Norm values for three different sizes of GPT models.", "section": "5.3.3 Analysis of Prompt Learning"}, {"figure_path": "2410.10672/tables/table_10_1.html", "caption": "Table 4: Matrix Nuclear-Norms in Vicuna and DeepSeek Responses", "description": "Table 4 presents Matrix Nuclear-Norm values for Vicuna and DeepSeek models across Alpaca and Arena datasets, demonstrating lower values indicating enhanced information processing efficiency with increasing model size.", "section": "6.1 Inference-Based Model Assessment"}, {"figure_path": "2410.10672/tables/table_11_0.html", "caption": "Table 5: Matrix Nuclear-Norm Rankings: A Comparative Analysis of Model Performance", "description": "Table 5 presents a comparative analysis of model performance across different model families on Alpaca and Arena-Hard benchmark datasets using Matrix Nuclear-Norm, showing rankings based on average scores.", "section": "6.1 Inference-Based Model Assessment"}, {"figure_path": "2410.10672/tables/table_16_0.html", "caption": "Table 6: Ablation study of differnet sampling strategies on the Wikimedia[18] dataset.", "description": "The table presents the results of an ablation study evaluating the impact of different sampling strategies on the Matrix Nuclear-Norm metric, demonstrating its robustness across varied sample sizes and random seeds.", "section": "A.1.2 Sampling Strategy"}, {"figure_path": "2410.10672/tables/table_17_0.html", "caption": "Table 7: Pythia Model: Matrix Entropy vs. Matrix Nuclear-Norm Time Comparison", "description": "The table compares the computation times of Matrix Entropy and Matrix Nuclear-Norm for various sizes of Pythia language models, showing that Matrix Nuclear-Norm is significantly faster.", "section": "5.2.1 A Comparative Analysis of Computational Time"}, {"figure_path": "2410.10672/tables/table_17_1.html", "caption": "Table 8: Matrix Nuclear-Norm in QWEN 2 Responses", "description": "Table 8 presents the Matrix Nuclear-Norm values for different sizes of QWEN 2 models, evaluated on Alpaca and Arena datasets.", "section": "6.2 Matrix Nuclear-Norm Benchmarking: Ranking Mid-Sized Models"}, {"figure_path": "2410.10672/tables/table_17_2.html", "caption": "Table 9: Matrix Nuclear-Norm in Llama-3 Responses", "description": "This table shows the Matrix Nuclear-Norm values for Llama-3 models (8B and 70B parameters) across Alpaca and Arena datasets.", "section": "6.2 Matrix Nuclear-Norm Benchmarking: Ranking Mid-Sized Models"}, {"figure_path": "2410.10672/tables/table_18_0.html", "caption": "Table 10: Language modeling indicators on openbookqa, winogrande and piqa.", "description": "Table 10 presents a comparison of various language modeling metrics (accuracy, matrix entropy, loss, perplexity, and matrix nuclear norm) across different model sizes (111M, 256M, 590M, 1.3B, 2.7B, 6.7B, and 13B) for three benchmark datasets: OpenBookQA, Winogrande, and PIQA.", "section": "5.2.3 Relationship of Benchmark INDICATORS"}, {"figure_path": "2410.10672/tables/table_19_0.html", "caption": "Table 12: The language modeling indicators (where lower values indicate better performance) for the Pythia models were evaluated on the Dolly-15k, Wikipedia, OpenWebText2, and HH-RLHF datasets.", "description": "Table 12 presents the language modeling indicators for Pythia models across four datasets, showing Matrix Entropy, Loss, and Matrix Nuclear-Norm values for various model sizes.", "section": "A.2 Supplementary Experiment Results"}, {"figure_path": "2410.10672/tables/table_19_1.html", "caption": "Table 13: The prompts selected from OpenOrca[30] dataset.", "description": "This table lists the three prompts selected from the OpenOrca dataset that were used in the prompt learning experiments.", "section": "5.3.3 Analysis of Prompt Learning"}]