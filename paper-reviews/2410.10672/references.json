{"references": [{" publication_date": "2024", "fullname_first_author": "Yasaman Bahri", "paper_title": "Explaining neural scaling laws", "reason": "This paper is highly relevant as it delves into the \"neural scaling laws,\" which are fundamental to understanding the relationship between model size, compute, and performance in large language models (LLMs).  The study of scaling laws is directly relevant to evaluating and comparing LLMs, as the authors seek to develop a novel metric for evaluating LLM information compression, a crucial aspect of the scaling phenomenon.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "reason": "This paper introduces a method for training helpful and harmless language models using reinforcement learning from human feedback (RLHF).  The use of RLHF is significant because it directly relates to the quality and safety aspects of LLMs, which are becoming increasingly important as LLMs are integrated into various applications.  Hence, this paper's methodologies impact on model compression capabilities.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Stella Biderman", "paper_title": "Pythia: A suite for analyzing large language models across training and scaling", "reason": "This paper is highly relevant because it provides a comprehensive framework for analyzing large language models (LLMs) across training and scaling.  The \"Pythia\" suite offers a multitude of tools and analyses that are useful for understanding LLM behavior and performance.  The evaluation of information compression discussed in this paper is inherently linked to model architecture, training, and scalability.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Yonatan Bisk", "paper_title": "Piqa: Reasoning about physical commonsense in natural language", "reason": "This paper presents PIQA, a benchmark dataset focused on evaluating commonsense reasoning abilities in large language models (LLMs).  The ability to reason about physical commonsense is a crucial aspect of intelligence and is closely tied to the ability to compress and efficiently use information.  This work can be used to evaluate the proposed method.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Asli Celikyilmaz", "paper_title": "Evaluation of text generation: A survey", "reason": "This paper provides a comprehensive survey of text generation evaluation methods, highlighting the challenges and limitations of existing approaches.  Understanding the strengths and weaknesses of existing evaluation metrics is critical when proposing a novel metric.  This paper can be used to justify the need for efficient metrics like the one proposed in the current paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yupeng Chang", "paper_title": "A survey on evaluation of large language models", "reason": "This paper provides a detailed survey on the evaluation of large language models (LLMs), covering various aspects of evaluation and highlighting recent advances and challenges.  It serves as important background material for the current paper, helping to place the proposed metric within the broader context of LLM evaluation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jun Chen", "paper_title": "Information compression in the ai era: Recent advances and future challenges", "reason": "This paper examines the critical role of information compression in the AI era, specifically focusing on recent advances and future challenges.  Understanding this context is essential for evaluating the authors' proposed metric for assessing information compression capabilities of large language models (LLMs).", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Zhikai Chen", "paper_title": "Exploring the potential of large language models (llms) in learning on graphs", "reason": "This paper explores the application of large language models (LLMs) in graph neural networks.  While not directly related to information compression, it shows the broader impact of LLMs in various fields, underscoring the importance of developing effective metrics for evaluating their performance, including information compression.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Chatbot arena: An open platform for evaluating llms by human preference", "reason": "This paper introduces Chatbot Arena, a platform for evaluating large language models (LLMs) through human preference. This platform is highly relevant because it emphasizes the subjective aspects of LLM evaluation and the importance of aligning model performance with human expectations. This aspect is very important for justifying the use of the new evaluation metric.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Aidan Clark", "paper_title": "Unified scaling laws for routed language models", "reason": "This paper presents a unified scaling law theory for language models, offering insights into how model size, dataset size, and computation affect performance. This is highly relevant as the proposed work also touches on the scaling laws observed in model performance and how it informs efficient evaluation metrics.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Mike Conover", "paper_title": "Free dolly: Introducing the world's first truly open instruction-tuned Ilm", "reason": "This paper introduces Dolly, a large language model (LLM) trained using a fully open instruction-tuning process.  This is significant because the availability of open instruction-tuned LLMs allows for broader experimentation and validation of the proposed metric, which is used to assess model compression capabilities.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Gr\u00e9goire Del\u00e9tang", "paper_title": "Language modeling is compression", "reason": "This paper directly addresses the relationship between language modeling and compression, providing a theoretical framework for understanding this connection.  This framework is highly relevant to the authors' work, as they propose a novel metric for assessing the information compression capabilities of large language models (LLMs).", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Nolan Dey", "paper_title": "Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster", "reason": "This paper introduces Cerebras-GPT, a large language model trained on the Cerebras wafer-scale cluster, making it a powerful and high-performing model that can be used to validate the proposed metric. The experiments conducted here use Cerebras-GPT models of different sizes to evaluate the computational efficiency and scaling laws of the new metric.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces a family of LLMs under the Llama 3 framework, providing a diverse set of models with varying sizes and capabilities that can be used to test and validate the proposed metric.  The variety of model sizes allows for a comprehensive evaluation of the metric's effectiveness and scaling laws.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Yann Dubois", "paper_title": "Length-controlled alpacaeval: A simple way to debias automatic evaluators", "reason": "This paper introduces AlpacaEval, a benchmark dataset for evaluating the performance of large language models.  The use of AlpacaEval as a benchmark allows for a comprehensive evaluation of the new metric, assessing its ability to accurately and effectively measure information compression capabilities.", "section_number": 5}, {" publication_date": "2002", "fullname_first_author": "Maryam Fazel", "paper_title": "Matrix rank minimization with applications", "reason": "This is a foundational paper on matrix rank minimization, a problem closely related to the proposed Matrix Nuclear-Norm.  Understanding the mathematical background and properties of matrix rank minimization is essential for a thorough understanding and justification of the proposed metric.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Leo Gao", "paper_title": "The pile: An 800gb dataset of diverse text for language modeling", "reason": "This paper introduces The Pile, a massive dataset used for training large language models.  Since the dataset is used for training LLMs, understanding its characteristics is vital for interpreting the results obtained when evaluating the performance of LLMs trained on this dataset, using the new metric.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Gemini", "paper_title": "Gemini: a family of highly capable multimodal models", "reason": "This paper introduces Gemini, a family of multimodal models, highlighting its capabilities and performance.  The inclusion of Gemini in the experiments allows for a more comprehensive comparison of the proposed metric's effectiveness across different model architectures and capabilities.  This is crucial for validating its generality and applicability.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Josh GPT-4 Achiam", "paper_title": "Gpt-4 technical report", "reason": "This paper provides a technical report on GPT-4, a powerful large language model.  The technical report offers detailed information about the model's architecture, training process, and capabilities, making it valuable for understanding and evaluating the efficiency and effectiveness of the proposed metric which is used for assessing model information compression.", "section_number": 5}]}