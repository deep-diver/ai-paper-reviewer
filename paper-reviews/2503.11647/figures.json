[{"figure_path": "https://arxiv.org/html/2503.11647/x2.png", "caption": "Figure 1: Examples synthesized by ReCamMaster. ReCamMaster re-shoots the source video with novel camera trajectories. We visualized the novel camera trajectories alongside the video frames. Video results are on our project page.", "description": "This figure showcases example videos generated by ReCamMaster, demonstrating its ability to re-render source videos from novel camera perspectives. For each example, the figure presents both the original source video frame and the corresponding video frame generated by ReCamMaster using a new camera trajectory. The novel camera paths are also visualized next to the frames, providing a clear illustration of the camera movement employed during the video re-rendering process.  Additional video results can be found on the project website.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.11647/x3.png", "caption": "Figure 2: Illustration of the dataset construction process. We build the multi-camera synchronized training dataset by rendering in Unreal Engine 5. This is achieved using 3D environments, characters, animations collected from the internet, and our designed massive camera trajectories.", "description": "Figure 2 illustrates the creation of a high-quality, multi-camera synchronized video dataset using Unreal Engine 5.  The process involves four key steps: (a) selecting diverse 3D environments; (b) choosing a variety of characters; (c) utilizing numerous animations; and (d) generating massive camera trajectories.  These elements are combined to create a dataset that closely mimics real-world filming conditions, offering a rich variety of scenes and camera movements.", "section": "3. Multi-Cam Video: A High-Quality Multi-Camera Synchronized Video Dataset"}, {"figure_path": "https://arxiv.org/html/2503.11647/x4.png", "caption": "Figure 3: Overview of ReCamMaster. Left: The training pipeline of ReCamMaster. A latent diffusion model is optimized to reconstruct the target video Vtsubscript\ud835\udc49\ud835\udc61V_{t}italic_V start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, conditioned on the source video Vssubscript\ud835\udc49\ud835\udc60V_{s}italic_V start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, target camera pose c\u2062a\u2062mt\ud835\udc50\ud835\udc4esubscript\ud835\udc5a\ud835\udc61cam_{t}italic_c italic_a italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, and target prompt ptsubscript\ud835\udc5d\ud835\udc61p_{t}italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. Right: Comparison of different video condition techniques. (a) Frame-dimension conditioning used in our paper; (b) Channel-dimension conditioning used in baseline methods [43, 5]; (c) View-dimension conditioning in [3]. We omit the text prompt ptsubscript\ud835\udc5d\ud835\udc61p_{t}italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT in (a)-(c) for simplicity.", "description": "Figure 3 provides a comprehensive overview of the ReCamMaster model. The left panel illustrates the training process, where a latent diffusion model learns to reconstruct a target video (V<sub>t</sub>) based on three inputs: the source video (V<sub>s</sub>), the target camera trajectory (cam<sub>t</sub>), and a text prompt (p<sub>t</sub>). The right panel presents a comparison of three distinct video conditioning techniques: (a) Frame-dimension conditioning (used in this paper), which concatenates source and target video tokens along the frame dimension; (b) Channel-dimension conditioning (used in baseline methods), which concatenates source and target latent videos along the channel dimension; and (c) View-dimension conditioning (from another study), which introduces a specialized attention module for feature aggregation across multiple views.", "section": "4. Camera-Controlled Video Re-Generation"}, {"figure_path": "https://arxiv.org/html/2503.11647/x5.png", "caption": "Figure 4: Comparison with state-of-the-art methods. It shows that ReCamMaster generates videos that maintain appearance consistency and temporal synchronization with the source video.", "description": "Figure 4 presents a comparison of video generation results between ReCamMaster and three state-of-the-art methods (GCD, Trajectory Attention, and DaS).  For each method, the figure shows a source video frame and the corresponding frames generated using that method for the same camera trajectory.  The purpose of the figure is to visually demonstrate that ReCamMaster better preserves the appearance consistency and temporal synchronization with the original video compared to other methods.", "section": "5. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.11647/x6.png", "caption": "Figure 5: Ablation on video conditioning techniques. We compared the channel-/view- concatenation schemes proposed by previous methods and frame-concatenation in ReCamMaster. We observed that both channel-conditioning and view-conditioning schemes suffer from significant artifacts, content inconsistency and asynchronous dynamics with respect to the original video.", "description": "This figure presents an ablation study comparing different video conditioning techniques used in video generation models.  Three methods are compared: channel-dimension conditioning, view-dimension conditioning, and the frame-dimension conditioning proposed by the authors (ReCamMaster).  The results show that channel and view conditioning lead to significant artifacts in the generated videos, including inconsistencies in content and timing (asynchronous dynamics) compared to the original source video. In contrast, frame-dimension conditioning produces superior results.", "section": "4.2 Conditional Video Injection Mechanism"}, {"figure_path": "https://arxiv.org/html/2503.11647/extracted/6281232/figures/fig_basemodel.jpg", "caption": "Figure 6: Applications of ReCamMaster. From top to bottom: video stabilization, video super-resolution, and video outpainting.", "description": "This figure showcases three applications of the ReCamMaster model: video stabilization, video super-resolution, and video outpainting.  The top row demonstrates video stabilization, showing how ReCamMaster smooths out shaky camera movements in a video clip. The middle row shows video super-resolution, where ReCamMaster enhances the resolution of a video clip, revealing finer details. Finally, the bottom row displays video outpainting, illustrating ReCamMaster's ability to expand the boundaries of a video frame, creating new content beyond the original scene.", "section": "6. Applications of ReCamMaster"}, {"figure_path": "https://arxiv.org/html/2503.11647/extracted/6281232/figures/fig_render_data.jpg", "caption": "Figure 7: Overview of the base text-to-video generation model.", "description": "This figure provides a detailed illustration of the base text-to-video generation model used in the ReCamMaster framework.  The model's architecture is shown as a pipeline, beginning with a 3D Variational Autoencoder (VAE) that encodes the input video into a latent space. This latent representation then passes through several Transformer blocks which incorporate spatial, temporal, and cross-attention mechanisms.  These attention mechanisms allow the model to effectively capture the spatiotemporal relationships within the video data. Finally, the model uses a 3D VAE decoder to reconstruct the video in the original pixel space. Time step information is also incorporated, demonstrating a diffusion process. The inclusion of a \"text prompt\" input indicates that text-based conditioning is also part of this model architecture.", "section": "4. Preliminary: Text-to-Video Base Model"}, {"figure_path": "https://arxiv.org/html/2503.11647/x7.png", "caption": "Figure 8: Rendered multi-camera synchronized dataset.", "description": "This figure shows a sample of videos from the dataset created using Unreal Engine 5.  The dataset consists of synchronized multi-camera videos with diverse camera trajectories, featuring various scenes and actions. This dataset is crucial for training ReCamMaster, the proposed model which enables camera-controlled video generation.", "section": "3. Multi-Cam Video: A High-Quality Multi-Camera Synchronized Video Dataset"}, {"figure_path": "https://arxiv.org/html/2503.11647/x8.png", "caption": "Figure 9: Unify camera-controlled tasks with ReCamMaster. ReCamMaster supports T2V, I2V, and V2V camera-controlled generation.", "description": "This figure demonstrates the versatility of the ReCamMaster model. It showcases successful camera-controlled video generation across three distinct tasks: Text-to-Video (T2V), Image-to-Video (I2V), and Video-to-Video (V2V).  Each row displays a source video (left) and its corresponding ReCamMaster-generated video (right), illustrating the model's ability to produce realistic and temporally coherent results regardless of the input type or task.", "section": "5. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.11647/x9.png", "caption": "Figure 10: Visualization of failure cases.", "description": "Figure 10 presents examples where the ReCamMaster model's generation process fails. These failures highlight limitations inherited from the underlying text-to-video model, particularly in generating small objects or intricate hand movements with high fidelity.  The image showcases specific instances where these limitations are evident.", "section": "C.4. Failure Cases Visualization"}, {"figure_path": "https://arxiv.org/html/2503.11647/x10.png", "caption": "Figure 11: More synthesized results of ReCamMaster.", "description": "This figure shows more examples of videos synthesized by the ReCamMaster model.  Each row displays a source video and its corresponding videos generated with novel camera trajectories. The camera trajectories are visualized alongside the video frames, illustrating the model's ability to re-render the input video from new perspectives and angles while maintaining consistency with the original video's content and temporal dynamics.", "section": "5. Experimental Results"}]