[{"figure_path": "https://arxiv.org/html/2410.24024/x1.png", "caption": "(a) Overview of the environment and benchmark of AndroidLab.", "description": "This figure provides a high-level overview of the ANDROIDLAB framework, illustrating its key components: the operation environment, which includes various modalities and action spaces for interacting with Android devices; the actions the agents can perform (Tap, Long Press, Type, Swipe, etc.); the benchmark, which comprises 9 apps and 138 tasks used to evaluate agent performance; and the metrics utilized for evaluation, including Success Rate and Reasonable Operation Rate.", "section": "3 ANDROIDLAB"}, {"figure_path": "https://arxiv.org/html/2410.24024/", "caption": "(b) Results of Closed Models.", "description": "This figure presents the success rates achieved by various closed-source large language models (LLMs) and large multimodal models (LMMs) on the AndroidLab benchmark.  It compares the performance of different models in terms of success rate across different operating modes (XML and SoM) and agent frameworks (ReAct and SeeAct). The chart visually represents the effectiveness of these closed-source models in completing tasks within the Android environment.", "section": "3.2 The Reproducible Benchmark"}, {"figure_path": "https://arxiv.org/html/2410.24024/x3.png", "caption": "Figure 1: \n(a) We design the SoM mode for the multimodal models (LMMs) and the XML mode for the text-only models (LLMs), ensuring an identical action space. We also implement ReAct and SeeAct frameworks in both modes. Based on the environment, we propose the AndroidLab benchmark. (b) AndroidLab benchmark success rates of closed-source models. In the XML mode, GPT-4-1106-Preview has the highest success rate at 31.16%, the same as GPT-4o in the SoM mode.", "description": "Figure 1 illustrates the architecture of AndroidLab and its benchmark results. (a) shows the design of AndroidLab's environment, which includes two operation modes: SoM (for multimodal models) and XML (for text-only models).  Both modes share an identical action space, and incorporate ReAct and SeeAct frameworks. The benchmark is based on this environment. (b) presents the success rates achieved by various closed-source models on the AndroidLab benchmark.  GPT-4-1106-Preview achieves the highest success rate (31.16%) in the XML mode, matching the performance of GPT-4o in the SoM mode.", "section": "3 ANDROIDLAB"}, {"figure_path": "https://arxiv.org/html/2410.24024/x4.png", "caption": "(a) Overview of Android Instruct data collection.", "description": "The figure illustrates the process of collecting the AndroidInstruct dataset, which involves three main steps: task derivation and expansion, self-exploration, and manual annotation.  Task derivation and expansion uses existing academic datasets and manual instruction writing to seed the generation of tasks. Self-exploration employs LLMs and LMMs to automatically explore the Android apps, collecting traces of operations. Finally, manual annotation involves instruction checking by annotators to assess task feasibility, preliminary familiarization with the app interface, the execution of tasks and recording their traces, and cross-verification by a second annotator to ensure data accuracy. The collected data includes tasks, phone screen states, XML information, and operations.", "section": "4 Android Instruction Data"}, {"figure_path": "https://arxiv.org/html/2410.24024/x5.png", "caption": "(b) Success Rates of before and after fine-tuning by Android Instruct.", "description": "This figure shows bar charts illustrating the success rates achieved by six open-source language models (LLMs) and multi-modal models (LMMs) before and after fine-tuning using the AndroidInstruct dataset.  The chart visually compares the model performance improvement after the fine-tuning process on the Android agent tasks, showing the effectiveness of the dataset in improving agent capabilities.", "section": "4 Android Instruction Data"}, {"figure_path": "https://arxiv.org/html/2410.24024/x6.png", "caption": "Figure 2: \n(a) We have collected over 726 traces containing more than 6208 fully aligned steps of XML and SoM mode training data. (b) By using the Android Instruct dataset, we trained six open-source text-only and multimodal models, achieving an average success rate from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs. respectively, reaching a performance level comparable to proprietary models.", "description": "Figure 2 presents data on the Android Instruction dataset and its impact on model training. (a) Details the dataset's composition: 726 traces and over 6208 aligned steps collected in XML and SoM modes. (b) Shows the performance improvement in six open-source LLMs and LMMs after fine-tuning using this dataset.  The average success rate increased significantly\u2014from 4.59% to 21.50% for LLMs and 1.93% to 13.28% for LMMs, reaching a level comparable to closed-source models.", "section": "4 Android Instruction Data"}, {"figure_path": "https://arxiv.org/html/2410.24024/extracted/5975521/picture/subpic/task_steps_histogram.png", "caption": "Figure 3: Task examples and the distribution of all apps and subcategories in the AndroidLab benchmark. We decomposed each task into sub-goals and evaluated them independently. A task is considered complete only if all sub-goals are correctly addressed.", "description": "Figure 3 illustrates example tasks from the AndroidLab benchmark and shows the distribution of tasks across different apps and subcategories.  Each task is broken down into smaller, independent sub-goals.  A task is only marked as successfully completed if all of its sub-goals are correctly addressed. This decomposition allows for a more granular evaluation of the agent's abilities, providing insights into which aspects of a task might be more challenging for the agent.", "section": "3 ANDROIDLAB"}, {"figure_path": "https://arxiv.org/html/2410.24024/extracted/5975521/picture/subpic/top_20_words_histogram.png", "caption": "Figure 4: An example of an agent completing all sub-goals of the entire task. We only present the starting and ending steps, along with the steps where the agent completes each sub-goal. It is essential that we record the completion status of each sub-goal. Without this information, we may not be able to obtain detailed information from the XML of the finished page, which could lead to a misjudgment of the task.", "description": "Figure 4 illustrates a successful task completion by an agent within the ANDROIDLAB environment.  The figure highlights the importance of tracking sub-goal completion status.  It shows only the initial, final, and intermediate steps where sub-goals are achieved.  This granular level of detail is crucial because, without tracking sub-goal success, it's difficult to accurately interpret the final XML page data and correctly assess task completion. Inaccurate interpretation of the final XML could lead to misjudgments about the agent's success.", "section": "3.2 The Reproducible Benchmark"}, {"figure_path": "https://arxiv.org/html/2410.24024/extracted/5975521/picture/subpic/instruction_length_histogram.png", "caption": "(a) Step Distribution Across Tasks", "description": "This histogram shows the distribution of the number of steps required to complete each of the 138 tasks in the ANDROIDLAB benchmark.  The x-axis represents the number of steps, and the y-axis represents the frequency or count of tasks requiring that number of steps. This visualization helps to understand the complexity distribution of tasks within the benchmark, indicating whether most tasks are simple (requiring few steps) or complex (requiring many steps).", "section": "4 Android Instruction Data"}, {"figure_path": "https://arxiv.org/html/2410.24024/extracted/5975521/picture/subpic/app_usage_histogram.png", "caption": "(b) Top 20 Words in Instructions.", "description": "This figure shows the 20 most frequent words used in the instructions given to the Android agents within the Android Instruction dataset.  It provides insight into the common themes, actions, and objects that characterize the tasks the agents were trained on.  This information helps to understand the nature and complexity of the tasks within the ANDROIDLAB benchmark.", "section": "4 Android Instruction Data"}, {"figure_path": "https://arxiv.org/html/2410.24024/extracted/5975521/picture/subpic/actions_frequency_histogram.png", "caption": "(c) Instruction Length Distribution.", "description": "This figure shows the distribution of instruction lengths in the Android Instruct dataset. The x-axis represents the length of instructions (in words), and the y-axis represents the frequency of instructions with that length.  The distribution provides insight into the complexity and variability of the instructions used to train the Android agents.", "section": "4 Android Instruction Data"}, {"figure_path": "https://arxiv.org/html/2410.24024/extracted/5975521/picture/subpic/average_task_length_per_app.png", "caption": "(d) APP Distribution.", "description": "The bar chart displays the frequency distribution of the nine applications (Clock, Contacts, Maps.me, PiMusicPlayer, Calendar, Settings, Cantook, Bluecoins, and Others) used in the ANDROIDLAB benchmark.  The height of each bar represents the number of tasks associated with each application, indicating which apps have a higher concentration of tasks in the benchmark.", "section": "4 Android Instruction Data"}, {"figure_path": "https://arxiv.org/html/2410.24024/x7.png", "caption": "(e) Actions Distribution.", "description": "This figure shows the distribution of action types in the Android Instruction dataset.  It displays the frequency of different actions such as Tap, Type, Swipe, Long Press, Launch, Back, Finish, and other actions, providing insights into the types of interactions captured in the dataset.", "section": "4 Android Instruction Data"}, {"figure_path": "https://arxiv.org/html/2410.24024/x8.png", "caption": "(f) Average Task Length per App", "description": "This figure shows the average number of steps required to complete tasks within each of the nine apps included in the ANDROIDLAB benchmark.  It provides insight into the relative complexity of tasks across different applications.", "section": "4 Android Instruction Data"}, {"figure_path": "https://arxiv.org/html/2410.24024/x9.png", "caption": "Figure 5: Statistics for Android Instruct dataset. We collect 726 traces and 6208 steps across Apps in AndroidLab benchmark.", "description": "This figure presents a statistical overview of the Android Instruct dataset, a key component of the AndroidLab benchmark.  The dataset comprises 726 distinct interaction traces, which represent sequences of user actions within various Android apps.  A total of 6208 individual action steps were recorded across these traces. This data provides valuable insights into the scale and diversity of user interactions captured for training and evaluating Android agents within the AndroidLab framework.", "section": "4 Android Instruction Data"}, {"figure_path": "https://arxiv.org/html/2410.24024/x10.png", "caption": "Figure 6: The performance of four models across four different device types is presented. Among these, the Pixel 3a is a smaller-sized phone, the Pixel 7 Pro and Pixel 8 Pro are of sizes comparable to commonly used phones, and the Pixel Fold is akin to a tablet.", "description": "This figure displays the performance of four different large language models (LLMs) on Android devices with varying screen sizes. The models' success rates are compared across four different phone models: Pixel 3a (smaller screen), Pixel 7 Pro, and Pixel 8 Pro (common screen sizes), and Pixel Fold (tablet-like larger screen).  The results illustrate how screen size affects the performance of the models, suggesting that models perform best on screens similar in size to commonly used smartphones.", "section": "5.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2410.24024/x11.png", "caption": "Figure 7: Prompts of XML Mode for Text-only Testing", "description": "This figure displays the prompts used in the XML mode for text-only models during testing.  It shows the interaction between the user and the model, with examples of how the system provides XML data about the application interface and prompts the model for the next action. The prompts guide the model to perform actions (such as Tap, Type, Swipe) on specified elements of the app's UI using their XML coordinates.", "section": "3.2 The Reproducible Benchmark"}]