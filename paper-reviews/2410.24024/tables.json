[{"content": "| Mode | Model | SR | Sub-SR | RRR | ROR |\n|---|---|---|---|---|---| \n| XML | GPT-4o | 25.36 | 30.56 | **107.45** | 86.56 |\n|  | GPT-4-1106-Preview | **31.16** | **38.21** | 66.34 | 86.24 |\n|  | Gemini-1.5-Pro | 18.84 | 22.40 | 57.72 | 83.99 |\n|  | Gemini-1.0 | 8.70 | 10.75 | 51.80 | 71.08 |\n|  | GLM4-PLUS | 27.54 | 32.08 | 92.35 | 83.41 |\n|  | LLaMA3.1-8B-Instruct | 2.17 | 3.62 | - | 52.77 |\n|  | Qwen2-7B-Instruct | 4.35 | 4.95 | - | 67.26 |\n|  | GLM4-9B-Chat | 7.25 | 9.06 | 54.43 | 58.34 |\n| XML+SFT | LLaMA3.1-8B-**ft** | 23.91 | 30.31 | 75.58 | 92.46 |\n|  | Qwen2-7B-**ft** | 19.57 | 24.40 | 77.31 | 92.48 |\n|  | GLM4-9B-**ft** | 21.01 | 26.45 | 74.81 | **93.25** |\n| SoM | GPT-4o | **31.16** | **35.02** | 87.32 | 85.36 |\n|  | GPT-4-Vision-Preview | 26.09 | 29.53 | 99.22 | 78.79 |\n|  | Gemini-1.5-Pro | 16.67 | 18.48 | 105.95 | **91.52** |\n|  | Gemini-1.0 | 10.87 | 12.56 | 72.52 | 76.70 |\n|  | Claude-3.5-Sonnet | 28.99 | 32.66 | **113.41** | 81.16 |\n|  | Claude-3-Opus | 13.04 | 15.10 | 81.41 | 83.89 |\n|  | CogVLM2 | 0.72 | 0.72 | - | 17.97 |\n|  | LLaMA3.2-11B-Vision-Instruct | 1.45 | 1.45 | - | 50.76 |\n|  | Qwen2-VL-7B-Instruct | 3.62 | 4.59 | - | 84.81 |\n| SoM+SFT | CogVLM2-**ft** | 11.59 | 16.06 | 57.37 | 85.58 |\n|  | LLaMA3.2-11B-Vision-**ft** | 10.14 | 12.98 | 61.67 | 87.85 |\n|  | Qwen2-VL-7B-Instruct-**ft** | 18.12 | 22.64 | 65.23 | 88.29 |", "caption": "Table 1: Main Result of XML and SoM modes. SR, Sub-SR, RRR, and ROR stand for Success Rate, Sub-Goal Success Rate, Reversed Redundancy Ratio, and Reasonable Operation Ratio, respectively. For all these metrics, a higher value means better. -ft represents a finetuned model. In each mode, Bold represents the best result. We do not report RRR score if SR < 5.", "description": "This table presents the main results obtained from evaluating various large language models (LLMs) and large multimodal models (LMMs) using two different operation modes: XML mode (text-only) and SoM mode (multimodal).  The models' performance is assessed across four key metrics: Success Rate (SR), Sub-Goal Success Rate (Sub-SR), Reversed Redundancy Ratio (RRR), and Reasonable Operation Ratio (ROR).  A higher value for each metric indicates better performance.  The table also includes results for fine-tuned (ft) versions of some models, highlighting the impact of fine-tuning.  The best performing model in each mode is indicated in bold. Note that the RRR is not reported for models with a Success Rate (SR) below 5%.", "section": "5.2 Main Results"}, {"content": "| Mode | Model | SR |\n|---|---|---|\n| XML | GPT-4o | 25.36 |\n| XML | Gemini-1.5-Pro | 18.84 |\n| XML+ReAct | GPT-4o | 33.33 |\n| XML+ReAct | Gemini-1.5-Pro | 31.16 |\n| XML+SeeAct | GPT-4o | 24.64 |\n| XML+SeeAct | Gemini-1.5-Pro | 21.01 |\n| SoM | GPT-4o | 31.16 |\n| SoM | Gemini-1.5-Pro | 16.67 |\n| SoM+ReAct | GPT-4o | 31.88 |\n| SoM+ReAct | Gemini-1.5-Pro | 15.94 |\n| SoM+SeeAct | GPT-4o | 30.43 |\n| SoM+SeeAct | Gemini-1.5-Pro | 21.01 |", "caption": "Table 2: The impact of the ReAct and SeeAct frameworks on SR results. Notably, model performance is significantly improved in XML+ReAct mode. Full results of this table are shown in Appendix\u00a0D.3", "description": "This table presents the success rates (SR) achieved by different language models (GPT-40 and Gemini-1.5-Pro) when employing various agent frameworks (ReAct and SeeAct).  The results are categorized by the mode of interaction (XML and SoM) and the agent framework used.  A key finding highlighted in the caption is the significant improvement in model performance observed specifically when the XML mode is combined with the ReAct framework.  The full dataset of results from this table is available in Appendix D.3.", "section": "5 Experiments"}, {"content": "| **Mode** | FT | XML/SoM | ReAct | SeeAct |\n|---|---|---|---|---|\n| #Avg. Gen. Tokens | 4.96 | 23.56 | 67.89 | 129.12 |", "caption": "Table 3: Average generation tokens of different modes. We used the LLaMA3 tokenizer for calculation. FT represents instruction tuning models.", "description": "This table presents the average number of tokens generated by different agent frameworks (XML, SoM, XML+ReAct, XML+SeeAct, SoM+ReAct, SoM+SeeAct) across various models.  The LLaMA3 tokenizer was used for calculating token counts.  The 'FT' designation indicates models that have undergone instruction tuning, highlighting the impact of this training method on the verbosity of the agents' responses.", "section": "5 Experiments"}, {"content": "| APP | Example Task | Sub-Goals | # tasks |\n|---|---|---|---|\n| Bluecoins | Record an income of 8000 CNY in the books, and mark it as \"salary\". | \u00b7 type: income\n\u00b7 cash: 8000 CNY\n\u00b7 note: salary | 15 |\n| Calendar | Edit the event with title \"work\", change the time to be 7:00 PM. | \u00b7 title: work\n\u00b7 state: editing\n\u00b7 date: today\n\u00b7 time: 7 PM | 14 |\n| Cantook | Mark Hamlet as read. | \u00b7 book: Hamlet\n\u00b7 state: 100% read | 12 |\n| Clock | I need set an 10:30PM clock every weekend, and label it as \"Watch Football Games\". | \u00b7 time: 10:30PM\n\u00b7 frequency: every weekend\n\u00b7 label: Watch Football Games | 27 |\n| Contacts | Add a contacts whose name is Xu, set the working phone number to be 12345678, and mobile phone number to be 87654321. | \u00b7 name: Xu\n\u00b7 working phone number: 12345678\n\u00b7 mobile phone number: 87654321 | 15 |\n| Maps.me | Check the driving distance and time between Bus stop of 2700 Coast Avenue and Bus Stop Route 51. | \u00b7 driving distance: 7.0km\n\u00b7 driving time: 8 min | 15 |\n| PiMusic | Sort Pink Floyd\u2019s songs by duration time in descending order. | \u00b7 page: ARTISTS\n\u00b7 artist: Pink Floyd\n\u00b7 order: descending by duration | 12 |\n| Setting | Show battery percentage in status bar. | \u00b7 battery percentage: displayed | 23 |\n| Zoom | I need to join meeting 1234567890 without audio and video. | \u00b7 meeting ID: 1234567890\n\u00b7 audio: off\n\u00b7 video: off | 5 |", "caption": "Table 4: List of Android Eval apps used along with corresponding example task, sub-goals, and the number of tasks.", "description": "This table lists nine Android applications used in the ANDROIDLAB benchmark, along with example tasks, their sub-goals (smaller, more specific tasks that comprise each larger task), and the total number of tasks for each app.  It showcases the variety and complexity of tasks within ANDROIDLAB.", "section": "3.2 The Reproducible Benchmark"}, {"content": "| Record an income of 8000 CNY in | the books, and mark it as \"salary\". |\n|---|---|", "caption": "Table 5: The number of tasks completed by all models across all apps in different modes.", "description": "This table presents a comprehensive overview of the performance of various language models (LLMs and LMMs) across a diverse set of 138 tasks within the AndroidLab benchmark.  It breaks down the number of successfully completed tasks for each model across nine different Android apps, providing detailed insights into model performance in different operational modes (XML and SoM) and across different app categories. This allows for granular comparison of model capabilities and reveals strengths and weaknesses in handling various task types and application contexts.", "section": "Main Results"}, {"content": "| Feature | Value |\n|---|---| \n| type | income |\n| cash | 8000 CNY |\n| note | salary |", "caption": "Table 6: The improvement in model performance after employing the ReAct and SeeAct frameworks, is reflected in the increased number of successfully completed tasks across various apps.", "description": "This table presents a detailed breakdown of how the ReAct and SeeAct agent frameworks impact the number of successfully completed tasks across different apps. It demonstrates the improvement in model performance achieved by incorporating these frameworks, providing granular results for each app and model.", "section": "5.2 Main Results"}, {"content": "| Edit the event with title \"work\", | change the time to be 7:00 PM. |", "caption": "Table 7: Different multi-modal modes of instruction tuning. We use the same set of training data but only add a set-of-mask index on SoM mode. Note that AITW dataset even could not provide accurate bbox, but only point. We use CogVLM2 as base model.", "description": "This table compares the performance of different multi-modal instruction tuning methods.  The experiment uses the same training data across all methods, but only the 'Set of Mask' index is added to the SoM (Set of Mask) mode. Importantly, the caption notes a limitation of the AITW (Android In The Wild) dataset, which only provides point coordinates instead of accurate bounding boxes (bbox), making it a more challenging dataset. CogVLM2 serves as the base model for all experiments. The results are presented in terms of SR (Success Rate), Sub-SR (Sub-Goal Success Rate), RRR (Reversed Redundancy Ratio), and ROR (Reasonable Operation Ratio) for both BBOX (Bounding Box) and SoM modes.", "section": "5.2 Main Results"}, {"content": "| Feature | Description |\n|---|---| \n| title | work |\n| state | editing |\n| date | today |\n| time | 7 PM |", "caption": "Table 8: The impact of the ReAct and SeeAct frameworks. Notably, model performance is significantly improved in XML+ReAct mode.", "description": "This table presents the results of experiments evaluating the impact of the ReAct and SeeAct frameworks on model performance.  It shows the success rate (SR), sub-goal success rate (Sub-SR), reversed redundancy ratio (RRR), and reasonable operation ratio (ROR) for GPT-40 and Gemini-1.5-Pro models across different modes (XML, XML+ReAct, XML+SeeAct, SoM, SoM+ReAct, SoM+SeeAct). The results highlight a significant improvement in model performance, particularly in the XML+ReAct mode, demonstrating the effectiveness of the ReAct framework in enhancing agent capabilities.", "section": "5 Experiments"}]