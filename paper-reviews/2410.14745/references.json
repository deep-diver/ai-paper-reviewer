{"references": [{" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is foundational for the current LLM research.  It demonstrates the capability of LLMs to perform various tasks with few-shot learning, setting the context for the current trend of adapting and fine-tuning LLMs for specific applications. The paper's impact on the field is significant, making it one of the most important references.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "reason": "This paper introduces a unified text-to-text transformer architecture for various NLP tasks, laying the groundwork for many subsequent LLMs.  Its impact on the development of efficient and adaptable LLMs is undeniable, making it a highly important reference in this context.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "BERT is a highly influential LLM architecture that significantly advanced the state-of-the-art in NLP.  Its pre-training approach and effectiveness have impacted many subsequent LLMs, making it a crucial reference for this work.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "reason": "This paper establishes the foundational idea of LLMs as unsupervised multitask learners. Its impact on the understanding and development of LLMs is substantial, and it forms a crucial basis for understanding current LLM advancements.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Or Honovich", "paper_title": "Unnatural instructions: Tuning language models with (almost) no human labor", "reason": "This paper directly addresses the challenges of data scarcity in LLM fine-tuning.  Its focus on efficient methods for training LLMs with limited data directly relates to the core problem being addressed in the current paper, making it a highly relevant and significant reference.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Po-Nien Kung", "paper_title": "Active instruction tuning: Improving cross-task generalization by training on prompt sensitive tasks", "reason": "This paper proposes a method for improving the cross-task generalization ability of LLMs. The focus on efficient training and improved generalization directly addresses the challenges of adapting LLMs to different tasks, making it relevant to the current study.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Kihyuk Sohn", "paper_title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence", "reason": "This work presents FixMatch, a widely cited method in semi-supervised learning. It simplifies semi-supervised learning by focusing on consistency and confidence. Its relevance stems from the fact that the proposed method also incorporates semi-supervised techniques to efficiently utilize both labelled and unlabelled data.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Kaiming He", "paper_title": "Momentum contrast for unsupervised visual representation learning", "reason": "This paper introduces a contrastive learning method which has been influential in self-supervised learning.  While focused on visual data, the core concept of contrastive learning is relevant to the approach in this paper, which also uses a form of self-supervision to improve performance with unlabeled data.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yizhong Wang", "paper_title": "Self-instruct: Aligning language models with self-generated instructions", "reason": "This paper explores methods for aligning LLMs with self-generated instructions. This is directly relevant to the current paper because it addresses the improvement of model performance with generated data, which is partially relevant to the approach of generating pseudo responses in the current paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zhengxiang Shi", "paper_title": "Rethinking semi-supervised learning with language models", "reason": "This paper offers a critical review of semi-supervised learning methods and highlights the need for more effective techniques, particularly for tasks such as those used in the current research. This paper forms a relevant context for the paper under consideration.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "reason": "This paper proposes LoRA, a technique for efficient fine-tuning of large language models.  Its relevance stems from the fact that LoRA is used in the experiments conducted by the authors and forms an important basis for their implementation.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Jason Wei", "paper_title": "Finetuned language models are zero-shot learners", "reason": "This paper shows that fine-tuned language models can be surprisingly good at zero-shot learning. This is relevant to the current research because it demonstrates the value of fine-tuning in improving model performance for tasks that were not directly seen during training.  It's important for understanding the baseline performance against which SEMIEVOL is compared.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Patrick Lewis", "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks", "reason": "This paper introduces RAG, a technique for improving LLM performance by incorporating external knowledge retrieval. This technique is used as a baseline for comparison in the experimental section, making this a relevant and important reference.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Hyung Won Chung", "paper_title": "Scaling instruction-finetuned language models", "reason": "This paper explores scaling instruction-finetuned language models. This work is relevant because the authors' paper also focuses on fine-tuning LLMs for improved performance. Examining the scaling of instruction-finetuned models provides context for evaluating the efficiency of the proposed approach.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Gantavya Bhatt", "paper_title": "An experimental design framework for label-efficient supervised finetuning of large language models", "reason": "This paper investigates label-efficient supervised fine-tuning of large language models. This is directly relevant to the current study as both papers focus on improving efficiency in LLM fine-tuning, although with different methods and approaches.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Alexander Bukharin", "paper_title": "Data diversity matters for robust instruction tuning", "reason": "This paper highlights the significance of data diversity in instruction tuning, a critical aspect of adapting LLMs to specific tasks. The emphasis on data quality and diversity complements the approach in the current paper, which also aims to improve model performance by effectively using available data.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Zhengxiang Shi", "paper_title": "Rethinking semi-supervised learning with language models", "reason": "This paper provides a comprehensive overview of semi-supervised learning methods and their application in the context of LLMs. The paper is highly relevant because the current paper also uses a form of semi-supervised learning, and it is important to understand the existing landscape and limitations of semi-supervised approaches before proposing a new method.", "section_number": 5}, {" publication_date": "2005", "fullname_first_author": "Xiaojin Jerry Zhu", "paper_title": "Semi-supervised learning literature survey", "reason": "This paper offers a comprehensive survey of semi-supervised learning techniques.  It provides important background for understanding the broader context of semi-supervised learning and how the current paper's approach fits into the existing literature.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Yubo Wang", "paper_title": "MMLU-Pro: A more robust and challenging multi-task language understanding benchmark", "reason": "This paper introduces MMLU-Pro, a benchmark dataset used in the experiments of the current study. This benchmark is highly relevant because it forms a critical part of the empirical evaluation conducted by the authors.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Tianlu Wang", "paper_title": "Self-taught evaluators", "reason": "This paper proposes a method for using self-taught evaluators to improve the quality of LLMs. This method addresses the challenges of efficiently utilizing unlabeled data, a topic that is also central to the approach proposed in the paper.", "section_number": 4}]}