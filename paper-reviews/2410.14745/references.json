{"references": [{" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is foundational in establishing the capabilities of large language models (LLMs) as few-shot learners.  It directly relates to the paper's central theme by demonstrating the potential of LLMs to adapt to new tasks with limited examples, underscoring the significance of efficient fine-tuning methods like the one proposed in the paper.  The paper's impact on the field of LLMs and few-shot learning is considerable, making it highly relevant to the current study.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "reason": "This paper is highly relevant because it introduces a unified text-to-text transformer approach that directly addresses the challenge of adapting LLMs to specific tasks. This is crucial to the context of the current paper as it provides a strong base for the development of efficient methods, like semi-supervised fine-tuning, to enhance LLMs' performance on specialized tasks.  The architecture and methodology introduced in this work have influenced subsequent research in LLM adaptation.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "reason": "This paper is seminal in demonstrating the potential of unsupervised pre-training for large language models (LLMs). It underscores the capacity of LLMs to learn from vast amounts of unlabeled data, thereby highlighting the motivation behind the proposed semi-supervised approach in the current paper which seeks to improve LLMs by leveraging both labeled and unlabeled data for enhanced efficiency. The paper's impact on the LLM landscape is vast.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "This paper introduced BERT, a highly influential model architecture that significantly impacted the field of natural language processing (NLP). BERT\u2019s pre-training approach on a massive dataset serves as a critical foundation for many modern LLMs, directly impacting the current paper's research on effective fine-tuning strategies and related techniques used for LLM improvement. The widespread adoption and influence of BERT solidify its importance.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Or Honovich", "paper_title": "Unnatural instructions: Tuning language models with (almost) no human labor", "reason": "This paper is highly relevant due to its direct exploration of data-efficient methods for tuning LLMs.  It focuses on minimizing human annotation effort, which aligns with the key motivation of the current paper\u2014developing a method that effectively leverages both labeled and unlabeled data to overcome the scarcity of labeled data in LLM adaptation. The methods explored in this paper provide a comparative context for the novel approach presented in the current work.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Po-Nien Kung", "paper_title": "Active instruction tuning: Improving cross-task generalization by training on prompt sensitive tasks", "reason": "This paper is relevant to the current work because it explores techniques for enhancing LLMs' generalization capabilities across different tasks.  This relates to the current paper's goal of adapting LLMs to new domains or tasks efficiently.  The investigation of cross-task generalization offers valuable insights and potential synergies with the approach of semi-supervised fine-tuning for LLM adaptation presented in the current paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Alexander Bukharin", "paper_title": "Data diversity matters for robust instruction tuning", "reason": "This paper is highly relevant because it directly addresses the importance of data quality and diversity in fine-tuning LLMs.  This is critical to the current work's methodology which emphasizes both labeled and unlabeled data for optimal model performance.  Understanding the effects of data diversity in fine-tuning can inform the design and evaluation of the semi-supervised approach presented in the paper.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Hyung Won Chung", "paper_title": "Scaling instruction-finetuned language models", "reason": "This paper is highly relevant due to its investigation of instruction-finetuning for LLMs, which is directly related to the current work's exploration of semi-supervised fine-tuning. The work explores the implications of scaling LLMs, which helps to contextualize the efficiency considerations driving the need for semi-supervised fine-tuning.  The findings in this paper can provide valuable comparisons and contextualize the efficiency of the new approach in the current paper.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Kihyuk Sohn", "paper_title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence", "reason": "This paper is highly relevant to the current research because it presents a simplified approach to semi-supervised learning, which is directly applicable to the context of this paper that focuses on semi-supervised fine-tuning of LLMs.  The methods presented provide a foundation and inspiration for the development of techniques used in the current work, especially in aspects related to handling uncertainty in pseudo-labeled data.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Kaiming He", "paper_title": "Momentum contrast for unsupervised visual representation learning", "reason": "While focused on visual representation learning, this paper's exploration of contrastive learning is relevant to the current work's use of multiple LLMs and their self-justification process.  Contrastive learning methods are used to improve the quality of pseudo-labeled data by aligning the representations generated by different models.  Therefore this paper provides a theoretical background for the collaborative learning aspect in the proposed SEMIEVOL framework.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Daixuan Cheng", "paper_title": "Instruction pre-training: Language models are supervised multitask learners", "reason": "This paper focuses on instruction-based pre-training for LLMs.  This is relevant to the current work because it demonstrates the effectiveness of leveraging instructional data, a form of labeled data, to improve the LLMs' capabilities.  This approach's insights complement the current work's semi-supervised method, highlighting how labeled data improves overall model performance.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Daixuan Cheng", "paper_title": "Adapting large language models via reading comprehension", "reason": "This paper explores the use of reading comprehension data to adapt LLMs, which is relevant to the current work's investigation of semi-supervised fine-tuning. The study's focus on adaptation is directly pertinent to the current research, offering comparative insights into the efficiency and effectiveness of different adaptation strategies.  The findings in this paper help contextualize the advancements proposed in this work.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zhengwei Tao", "paper_title": "A survey on self-evolution of large language models", "reason": "This paper provides a comprehensive overview of self-evolution techniques for LLMs.  This is directly relevant to the current work's investigation of semi-supervised learning for LLM adaptation.  The survey helps to contextualize the proposed method, highlighting its unique contributions and potential benefits over existing self-evolution approaches.  It also helps in identifying the limitations of existing self-evolution methods.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "reason": "This paper introduces the LoRA method for efficiently fine-tuning large language models.  This is highly relevant to the current paper's experimental setup, where LoRA is employed for model fine-tuning.  Understanding the advantages and principles of LoRA helps to interpret and contextualize the experimental results presented in the paper, thus strengthening its methodological validity.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Jason Wei", "paper_title": "Finetuned language models are zero-shot learners", "reason": "This paper explores the zero-shot learning capabilities of fine-tuned language models.  This is highly relevant to the current research because it demonstrates the potential of fine-tuning to enhance the generalization capabilities of LLMs.   The study's findings complement the current work's goal of developing efficient adaptation methods, especially in highlighting the potential to improve zero-shot performance through strategic fine-tuning.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Patrick Lewis", "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks", "reason": "This paper introduces retrieval-augmented generation (RAG), a technique that is relevant to the current work\u2019s methodology, specifically in the context of knowledge propagation.  RAG leverages external knowledge sources to enhance LLM performance.  This is analogous to the current work's use of labeled data to propagate knowledge to unlabeled data, enabling a comparison of approaches and understanding their relative strengths and weaknesses.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "David Berthelot", "paper_title": "Mixmatch: A holistic approach to semi-supervised learning", "reason": "This paper is highly relevant because it provides a comprehensive overview of semi-supervised learning techniques.  Its methods and strategies are directly related to the development of the semi-supervised fine-tuning approach in the current work.  The paper serves as a strong theoretical background for the proposed method, helping in understanding the broader context of semi-supervised learning.", "section_number": 5}, {" publication_date": "2005", "fullname_first_author": "Xiaojin Jerry Zhu", "paper_title": "Semi-supervised learning literature survey", "reason": "This paper provides a comprehensive survey of semi-supervised learning, making it a highly relevant reference for the current work.  The survey helps to contextualize the proposed method by establishing its place within the broader field of semi-supervised learning.  It helps in identifying the key challenges and opportunities in semi-supervised learning that directly relate to the issues explored in the paper.", "section_number": 5}, {" publication_date": "2016", "fullname_first_author": "Thomas N Kipf", "paper_title": "Semi-supervised classification with graph convolutional networks", "reason": "This paper introduces graph convolutional networks for semi-supervised classification, offering a different perspective on semi-supervised learning which is relevant to the context of the current paper.  While the application area differs, the underlying principles of leveraging both labeled and unlabeled data in semi-supervised settings are transferable and offer potential insights for designing effective methods for LLM adaptation.", "section_number": 5}]}