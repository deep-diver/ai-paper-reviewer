[{"content": "| Method | Image | Video | Visual Prompts | RES | Ref-VOS | Inter-VOS | GCG | Image-Chat | Video-Chat | Video Caption | Interactive Caption | Training | \n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| LLAVA [53] | 2713 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2713 | 2717 | 2717 | 2717 | 2713 |\n| LLaVA-OneVision [42] | 2713 | 2713 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2713 | 2713 | 2713 | 2717 | 2713 |\n| InternVL 2.0 [9] | 2713 | 2713 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2713 | 2713 | 2713 | 2717 | 2713 |\n| Osprey [92] | 2713 | 2717 | 2713 | 2717 | 2717 | 2717 | 2717 | 2717 | 2713 | 2717 | 2717 | 2717 | 2713 |\n| LISA [40] | 2713 | 2717 | 2713 | 2713 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2713 |\n| GLAMM [66] | 2713 | 2717 | 2713 | 2713 | 2717 | 2717 | 2713 | 2717 | 2717 | 2717 | 2717 | 2717 | 2713 |\n| VIP-LLaVA [5] | 2713 | 2717 | 2713 | 2717 | 2717 | 2717 | 2717 | 2717 | 2713 | 2717 | 2717 | 2717 | 2713 |\n| VISA [85] | 2713 | 2713 | 2713 | 2713 | 2713 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 |\n| OMG-LLaVA [99] | 2713 | 2713 | 2713 | 2713 | 2713 | 2717 | 2717 | 2717 | 2713 | 2713 | 2717 | 2717 | 2713 |\n| PSALM [101] | 2713 | 2713 | 2717 | 2713 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2713 |\n| GSVA [83] | 2713 | 2717 | 2717 | 2713 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2713 |\n| LLaMA-VID [50] | 2713 | 2713 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2713 | 2713 | 2713 | 2717 | 2713 |\n| ST-LLM [56] | 2713 | 2713 | 2717 | 2717 | 2717 | 2717 | 2717 | 2717 | 2713 | 2713 | 2713 | 2717 | 2713 |\n| F-LLM [82] | 2713 | 2717 | 2717 | 2713 | 2717 | 2717 | 2717 | 2717 | 2713 | 2717 | 2717 | 2717 | 2717 |\n| Sa2VA (Ours) | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 |", "caption": "Table 1: Comparison of capabilities of different representative models. Our method supports various tasks and modalities. Benefiting from these interactive features on video, Sa2VA can perform multiple promptable tasks in the video, as shown in Fig.\u00a01 (a) and (b).", "description": "Table 1 compares the capabilities of several representative models in handling various vision-language tasks, particularly focusing on image and video processing.  It highlights the input types each model supports (image, video, visual prompts), and whether they perform dense grounding, conversation, and various specific tasks like referring segmentation (image and video), video object tracking, and grounded caption generation.  It also notes whether each model is trained end-to-end. The table emphasizes that Sa2VA, the authors' model, uniquely supports a wide range of tasks across multiple modalities (image and video) and provides interactive prompting capabilities, an advantage illustrated in Figures 1(a) and 1(b) of the paper. ", "section": "2. Related Work"}, {"content": "| Method | Image Segmentation |  |  | Video Segmentation |  |  | Image Chat |  | Video Chat |  | GCG |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| RefCOCO [36] | RefCOCO+ [36] | RefCOCOg [90] | MeViS [13] | Ref-DAVIS17 [38] | ReVOS [85] | LAVA-1.5-13B [54] | MME [19] | MMBench [59] | SEED-Bench [41] | Video-MME [20] | MMBench-Video [17] | GCG [66] |\n| LLAVA-1.5-13B [54] | - | - | - | - | - | - | 1531 | 68.8 | 70.1 | - | - | - |\n| Video-LLaVA-7B [51] | - | - | - | - | - | - | - | 60.9 | - | 39.9 | 1.03 | - |\n| LLaMA-VID-7B [50] | - | - | - | - | - | - | 1521 | 65.1 | 59.9 | - | 1.08 | - |\n| mPLUG-Owl3-8B [89] | - | - | - | - | - | - | - | 77.6 | - | 53.5 | 1.35 | - |\n| InternVL2-8B [9] | - | - | - | - | - | - | - | 81.7 | 76.2 | 54.0 | 1.28 | - |\n| PixelLM-7B [68] | 73.0 | 66.3 | 69.3 | - | - | - | 309/135 | 17.4 | - | - | - | - |\n| LaSagnA [77] | 76.8 | 66.4 | 70.6 | - | - | - | 0/0 | 0.0 | - | - | - | - |\n| LISA-7B [40] | 74.1 | 62.4 | 66.4 | - | - | - | 1/1 | 0.4 | - | - | - | - |\n| GLaMM-7B [66] | 79.5 | 72.6 | 74.2 | - | - | - | 14/9 | 36.8 | - | - | - | 28.9 |\n| LLaVA-G-7B [96] | 77.1 | 68.8 | 71.5 | - | - | - | - | - | - | - | - | - |\n| GSVA-13B [83] | 79.2 | 70.3 | 75.7 | - | - | - | - | - | - | - | - | - |\n| OMG-LLaVA-7B [99] | 78.0 | 69.1 | 72.9 | - | - | - | 1177/235 | 47.9 | 56.5 | - | - | 29.9 |\n| VISA-13B [85] | 72.4 | 59.8 | 65.5 | 44.5 | 70.4 | 50.9 | - | - | - | - | - | - |\n| Sa2VA-1B (Ours) | 77.4 | 69.9 | 72.3 | 50.8 | 72.3 | 47.6 | 1381/405 | 68.3 | 64.8 | 39.9 | 1.07 | 23.8 |\n| Sa2VA-4B (Ours) | 78.9 | 71.7 | 74.1 | 52.1 | 73.8 | 53.2 | 1536/530 | 77.3 | 73.3 | 50.4 | 1.23 | 28.2 |\n| Sa2VA-8B (Ours) | 81.6 | 76.2 | 78.7 | 57.0 | 75.2 | 57.6 | 1617/511 | 81.6 | 75.1 | 52.1 | 1.34 | 31.0 |\n| Sa2VA-26B (Ours) | 82.5 | 78.8 | 79.7 | 57.3 | 77.0 | 58.4 | 1691/538 | 83.7 | 76.8 | 52.6 | 1.45 | 33.5 |", "caption": "Table 2: Experiment results on image/video referring segmentation benchmarks and image/video chat benchmarks.", "description": "This table presents a comparison of the performance of Sa2VA and other state-of-the-art models on several image and video benchmarks.  These benchmarks cover various tasks including image referring segmentation (RefCOCO, RefCOCO+, RefCOCOg, MeViS, Ref-DAVIS17, ReVOS, MME), video referring segmentation (the same metrics as image), and image and video chat (MMBench, SEED-Bench, Video-MME, MMBench-Video).  The table allows for a direct comparison of Sa2VA's performance across multiple modalities and tasks, highlighting its strengths and demonstrating its performance relative to other leading models.", "section": "4. Experiments"}, {"content": "| Type | Datasets |\n|---|---| \n| Image QA | LLaVA 1.5 (665K) |\n| Image Segmentation | RefCOCO (17K), RefCOCO+ (17K), RefCOCOg (22K), Grand-f (214K) |\n| Video QA | ChatUniVi (100K) |\n| Video Segmentation | Ref-YTVOS (3.5K), MeVIS (0.6K), ReVOS (1.7K), Ref-SAV (37K) |", "caption": "Table 3: Datasets used for experiments.", "description": "This table lists the datasets used to train and evaluate the Sa2VA model.  It breaks down the datasets by task type (Image QA, Image Segmentation, Video QA, Video Segmentation), showing the names and sizes of each dataset used in the respective tasks. This provides context for understanding the model's training and the benchmarks used for evaluation.", "section": "3.3 Ref-SAV Dataset and Benchmark"}, {"content": "| Method | MME<sup>[19]</sup> | MMBench<sup>[59]</sup> | SEED-Bench<sup>[41]</sup> | AI2D<sup>[37]</sup> | MMStar<sup>[7]</sup> | MMMU<sup>[93]</sup> | SQA<sup>test</sup><sup>[60]</sup> | RefCOCO | RefCOCO+ | RefCOCOg |\n|---|---|---|---|---|---|---|---|---|---|---|\n| LISA-7B<sup>[40]</sup> | 1/1 | 0.4 | - | 0.0 | - | - | - | 74.1 | 62.4 | 66.4 |\n| PixelLM-7B<sup>[68]</sup> | 309/135 | 17.4 | - | 0.0 | - | - | - | 73.0 | 66.3 | 69.3 |\n| LaSagnA-7B<sup>[77]</sup> | 0/0 | 0.0 | - | 0.0 | - | - | - | 76.8 | 66.4 | 70.6 |\n| GLaMM-7B<sup>[66]</sup> | 14/9 | 36.8 | - | 28.2 | - | - | - | 79.5 | 72.6 | 74.2 |\n| OMG-LLaVA-7B<sup>[99]</sup> | 1177/235 | 47.9 | 56.5 | 42.9 | - | - | - | 78.0 | 69.1 | 72.9 |\n| Sa2VA-4B (ours) | 1553/540 | 76.8 | 72.6 | 79.9 | 53.7 | 46.2 | 95.8 | 80.4 | 74.3 | 76.7 |\n| Sa2VA-8B (ours) | 1651/578 | 82.4 | 75.5 | 82.1 | 60.3 | 44.7 | 96.8 | 81.9 | 76.5 | 78.9 |", "caption": "Table 4: Performance on image-level benchmarks. Our method achieves the best accuracy trade-off between image chat and referring segmentation datasets.", "description": "Table 4 presents a comparison of different models' performance on several image-level benchmarks.  These benchmarks evaluate two key capabilities: image chat (the model's ability to engage in conversation about images) and referring segmentation (the model's ability to accurately segment specific objects within an image based on textual descriptions). The table highlights that the Sa2VA model achieves the best balance between strong performance on both image chat and referring segmentation tasks, indicating a superior ability to integrate visual understanding and language processing.", "section": "4. Experiments"}, {"content": "| Method | Long J | Long F | Long J&F | Short J | Short F | Short J&F | Overall J | Overall F | Overall J&F |\n|---|---|---|---|---|---|---|---|---|---| \n| UniRef++ [79] (zs) | 14.1 | 10.8 | 12.5 | 9.0 | 8.2 | 8.6 | 11.6 | 9.5 | 10.5 |\n| UNINEXT [84] (zs) | 11.7 | 8.3 | 10.0 | 5.8 | 4.4 | 5.1 | 8.8 | 6.4 | 7.6 |\n| MeVIS [13] (zs) | 12.1 | 7.1 | 11.3 | 6.2 | 5.3 | 5.5 | 12.2 | 9.8 | 10.3 |\n| VISA [85] (zs) | 16.1 | 12.2 | 14.1 | 12.3 | 9.6 | 9.2 | 13.2 | 11.3 | 11.8 |\n| Sa2VA-8b (zs) | 47.7 | 50.9 | 49.3 | 31.5 | 35.0 | 33.3 | 39.6 | 43.0 | 41.3 |\n| Sa2VA-8b (ft ) | 57.0 | 60.4 | 58.7 | 39.5 | 42.9 | 41.2 | 48.3 | 51.7 | 50.0 |", "caption": "Table 5: Ref SAV validation sets. zs: zero-shot testing. ft: trained with our proposed Ref-SAV training dataset.", "description": "Table 5 presents the results of the Ref-SAV validation set.  Two different testing scenarios are shown: zero-shot testing (zs), where the model is evaluated without any training on the Ref-SAV dataset; and fine-tuned testing (ft), where the model is trained using the newly proposed Ref-SAV training dataset. This allows comparison of performance with and without the benefit of the newly created dataset.", "section": "3.3 Ref-SAV Dataset and Benchmark"}, {"content": "| Model Type | RefCOCO | RefCOCO+ | RefCOCOg |\n|---|---|---|---|\n| LAVT [88] | 72.7 | 62.1 | 61.2 |\n| GlaMM-7B [66] | 79.5 | 72.6 | 74.2 |\n| OMG-LLaVA-7B [99] | 78.0 | 69.1 | 72.9 |\n| F-LLM-7B [82] | 76.1 | 65.2 | 68.5 |\n| Sa2VA-4B (ours) | 80.4 | 74.3 | 75.7 |\n| Sa2VA-8B (ours) | 82.3 | 77.3 | 79.3 |", "caption": "Table 6: Comparison with Fine-tuned Models.", "description": "This table presents a comparison of the performance of Sa2VA against other fine-tuned models on the Ref-SAV validation set.  It shows the Jaccard index (J) and F-measure (F) scores for both short and long text descriptions, providing a comprehensive evaluation of the model's ability to perform referring video object segmentation. The results are broken down by different model types, allowing for a direct comparison of Sa2VA's performance against state-of-the-art methods.", "section": "4. Experiments"}, {"content": "| Data | Image Segmentation |  |  | Video Segmentation |  | Image Chat |  |  | Video Chat |  |\n|---|---|---|---|---|---|---|---|---|---|---|\n|  | RefCOCO | RefCOCO+ | RefCOCOg | MeViS | Ref-DAVIS17 | MME | MMBench | SEED-Bench | Video-MME | MMBench-Video |\n| All Data | 77.4 | 69.9 | 72.3 | 50.8 | 72.3 | 1381/405 | 68.3 | 64.8 | 39.9 | 1.07 |\n| w/o Image QA | 78.0 | 70.1 | 72.2 | 48.3 | 73.0 | 1298/359 | 63.4 | 63.8 | 39.7 | 0.39 |\n| w/o Image Segmentation | 20.2 | 20.6 | 23.2 | 38.0 | 48.8 | 1393/408 | 70.1 | 65.7 | 41.2 | 1.08 |\n| w/o Video QA | 78.0 | 70.4 | 72.6 | 50.7 | 74.3 | 1370/402 | 69.1 | 65.0 | 41.3 | 0.71 |\n| w/o Video Segmentation | 77.4 | 69.1 | 72.4 | 44.4 | 69.0 | 1403/398 | 67.8 | 64.9 | 40.4 | 1.04 |", "caption": "Table 7: Ablation study on co-training effect on multiple datasets.", "description": "This table presents the results of an ablation study investigating the impact of co-training on multiple datasets. It shows how the model's performance on various image and video tasks changes when different combinations of datasets (image QA, image segmentation, video QA, video segmentation) are excluded during training. This demonstrates the relative importance of each dataset type for the model's overall performance and highlights the benefits of a comprehensive co-training approach.", "section": "4. Experiments"}, {"content": "| Base | Image Segmentation |  |  |  | Video Segmentation |  | Image Chat |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| MLLM | RefCOCO<sup>[36]</sup> | RefCOCO+<sup>[36]</sup> | RefCOCOg<sup>[90]</sup> | MeViS<sup>[13]</sup> | Ref-DAVIS17<sup>[38]</sup> | MME<sup>[19]</sup> | MMBench<sup>[59]</sup> | SEED-Bench<sup>[41]</sup> | AI2D<sup>[37]</sup> | MMStar<sup>[7]</sup> | SQA<sup>test</sup><sup>[60]</sup> |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| InternVL2.0-4B | 80.4 | 74.3 | 76.7 | 52.1 | 73.8 | 1553/540 | 76.8 | 72.6 | 79.9 | 53.7 | 95.8 |\n| InternVL2.0-8B | 81.9 | 76.5 | 78.9 | 57.0 | 75.2 | 1651/578 | 82.4 | 75.5 | 82.1 | 60.3 | 96.8 |\n| InternVL2.5-4B | 82.4 | 77.6 | 79.7 | 55.9 | 73.7 | 1691/610 | 81.8 | 74.9 | 81.4 | 57.9 | 96.8 |\n| InternVL2.5-8B | 82.6 | 78.0 | 80.3 | 58.9 | 75.9 | 1690/610 | 84.4 | 76.5 | 82.7 | 62.4 | 97.4 |", "caption": "Table 8: Experiment results using stronger InternVL2.5 in our Sa2VA.", "description": "This table presents the performance comparison of Sa2VA models using two different versions of InternVL2 (InternVL2.0 and InternVL2.5) as the base multi-modal large language model. It shows the impact of using a stronger MLLM on the overall performance of Sa2VA across multiple image and video benchmarks, including referring segmentation and image/video chat tasks.", "section": "4. Experiments"}, {"content": "| Type | RefCOCO | RefCOCO+ | RefCOCOg | DAVIS | MeVIS |\n|---|---|---|---|---|---| \n| Single | 77.4 | 69.9 | 72.3 | 72.3 | 50.8 |\n| Repeat | 77.3 | 70.2 | 72.5 | 71.1 | 49.6 |\n| Multiple | 77.6 | 70.3 | 72.4 | 68.6 | 46.3 |", "caption": "Table 9: Ablation study on [SEG] token design.", "description": "This table presents an ablation study analyzing the impact of different strategies for designing the '[SEG]' token within the Sa2VA model.  The '[SEG]' token serves as a bridge between the large language model (LLM) and the SAM-2 model, guiding the segmentation process. The study investigates three distinct '[SEG]' token designs: a single token, repeated tokens, and multiple tokens.  The table shows the performance (measured by J&F scores) of each design on several video object segmentation benchmarks (MeViS, Ref-DAVIS17, ReVOS). This ablation helps assess the impact of this crucial design element on the model's performance.", "section": "4. Experiments"}, {"content": "| Dataset | Size | RefCOCO | RefCOCOg | MMBench | MME | MeVIS |\n|---|---|---|---|---|---|---|\n| baseline | 1.2M | 77.4 | 72.3 | 68.3 | 1381/405 | 50.8 |\n| Inifinity-MM [22] | 1.2M+3M | 77.1(-0.3) | 72.6(+0.3) | 70.4(+2.1) | 1396/346(-44) | 51.2(+0.4) |\n| Ref-SAV | 1.2M+37K | 77.2(-0.2) | 72.6(+0.3) | 68.2(-0.1) | 1384/418(+16) | 52.5(+1.7) |", "caption": "Table 10: Ablation study on using more datasets.", "description": "This table presents the results of an ablation study investigating the impact of using additional datasets for training the Sa2VA model.  It shows how the model's performance changes on various image and video benchmarks when different combinations of datasets (image QA, image segmentation, video QA, and video segmentation) are included or excluded during training. This allows an assessment of the contribution of each dataset type to the overall performance and helps understand the effectiveness of the model's joint co-training strategy.", "section": "4. Experiments"}, {"content": "| Model Type | MeViS | ReVOS | Ref-DAVIS17 |\n|---|---|---|---|\n| PG-Video-LLaVA [61] | 18.87 | - | - |\n| GLaMM + SAM2 [62] | 38.7 | - | - |\n| VideoGLaMM [62] | 45.2 | - | - |\n| ViLLa [102] | - | - | 64.4 |\n| VISA-13B [85] | 44.5 | 50.9 | 70.4 |\n| VideoLISA-3.8B [3] | 44.4 | - | 68.8 |\n| Sa2VA-4B (ours) | 52.1 | 53.2 | 73.8 |", "caption": "Table 11: Comparison with Recent Video MLLMs.", "description": "Table 11 compares Sa2VA's performance with other state-of-the-art video-based large language models (MLLMs) on various video-related benchmarks.  It specifically shows the comparative results for different video object segmentation and video question answering tasks, highlighting Sa2VA's strengths in these areas.", "section": "4. Experiments"}, {"content": "| Model Type | RefCOCO | RefCOCO+ | RefCOCOg | MeVIS | Ref-DAVIS17 |\n|---|---|---|---|---|---| \n| LAVT [<a href=\"https://arxiv.org/html/2501.04001v1#bib.bib88\">88</a>] | 72.7 | 62.1 | 61.2 | - | - |\n| ReferFormer [<a href=\"https://arxiv.org/html/2501.04001v1#bib.bib78\">78</a>] | - | - | - | 31.0 | 61.1 |\n| UniRef++-L [<a href=\"https://arxiv.org/html/2501.04001v1#bib.bib79\">79</a>] | 81.4 | 74.0 | 76.0 | - | 67.2 |\n| EVF-SAM [<a href=\"https://arxiv.org/html/2501.04001v1#bib.bib100\">100</a>] | 82.4 | 76.5 | 78.2 | - | - |\n| LMPM [<a href=\"https://arxiv.org/html/2501.04001v1#bib.bib13\">13</a>] | - | - | - | 37.2 | - |\n| UniVS [<a href=\"https://arxiv.org/html/2501.04001v1#bib.bib46\">46</a>] | - | - | - | - | 59.4 |\n| Sa2VA-26B (ours) | 82.5 | 78.8 | 79.7 | 57.3 | 77.0 |", "caption": "Table 12: Comparison with Vision Expert Models.", "description": "This table compares the performance of Sa2VA against several state-of-the-art vision-expert models on various image and video referring segmentation benchmarks, showcasing Sa2VA's ability to achieve competitive results with vision-specialized models, thereby highlighting its versatility and effectiveness in tackling complex visual tasks.", "section": "4. Experiments"}, {"content": "| MLLM Type | RefCOCO | RefCOCO+ | RefCOCOg | Ref-DAVIS17 | MeVIS | ReVOS |\n|---|---|---|---|---|---|---|\n| Sa2VA: Qwen2-VL-2B [2] | 76.9 | 68.7 | 72.9 | 72.0 | 49.4 | 40.0 |\n| Sa2VA: Intern-VL2-1B | 77.4 | 69.9 | 72.3 | 72.3 | 50.8 | 47.6 |\n| Sa2VA: Intern-VL2-4B | 78.9 | 71.7 | 74.1 | 73.8 | 52.1 | 53.2 |", "caption": "Table 13: Sa2VA with Qwen2-VL model.", "description": "This table presents the performance of the Sa2VA model when using the Qwen2-VL model as its backbone.  It shows the zero-shot and fine-tuned results (trained with the Ref-SAV dataset) on various video object segmentation benchmarks, including MeViS, ReVOS, and Ref-DAVIS17.", "section": "4. Experiments"}, {"content": "| Method | Long J | Long F | Long J&F | Short J | Short F | Short J&F | Overall J | Overall F | Overall J&F |\n|---|---|---|---|---|---|---|---|---|---| \n| UniRef++ (zs) | 14.1 | 10.8 | 12.5 | 9.0 | 8.2 | 8.6 | 11.6 | 9.5 | 10.5 |\n| UniRef++ (ft) | 19.2 | 15.1 | 17.2 | 12.3 | 11.7 | 12.0 | 15.8 | 13.4 | 14.6 |", "caption": "Table 14: Ref SAV validation sets. ft: trained with our proposed Ref-SAV training dataset. zs: zero-shot testing.", "description": "This table presents the results of the Ref-SAV (Referring Video Object Segmentation) validation dataset.  It compares the performance of models tested in two conditions: zero-shot testing (\"zs\"), where the model is evaluated without any fine-tuning on the Ref-SAV dataset, and fine-tuning (\"ft\"), where the model is trained specifically on the Ref-SAV dataset. This comparison demonstrates the effectiveness of the Ref-SAV dataset for improving the performance of referring video object segmentation models.", "section": "3.3 Ref-SAV Dataset and Benchmark"}, {"content": "| Method | Sa2VA-4B | OMG-LLaVA [99] | Osprey [92] | GLaMM [66] | GRIT [81] | Kosmos-2 [64] |\n|---|---|---|---|---|---|---|\n| METEOR | 17.3 | 15.3 | 16.6 | 16.2 | 15.2 | 14.1 |", "caption": "Table 15: Region caption performance on RefCOCOg dataset.", "description": "This table presents the performance comparison of different models on the RefCOCOg dataset for the task of region captioning. It shows the METEOR scores achieved by each model, indicating their ability to generate accurate and relevant captions for specific image regions.", "section": "4. Experiments"}, {"content": "| Property | DAVIS17-RVOS | ReVOS | Ref-YT-VOS | MeVIS | Ours |\n|---|---|---|---|---|---| \n| Short Text | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 |\n| Long Text | \u2717 | \u2713 | \u2717 | \u2717 | \u2713 |\n| Large Object Motion | \u2717 | \u2717 | \u2713 | \u2717 | \u2713 |\n| Large Camera Motion | \u2713 | \u2713 | \u2717 | \u2717 | \u2713 |\n| Heavy Occlusion | \u2717 | \u2717 | \u2717 | \u2717 | \u2713 |", "caption": "Table 16: Comparison with previous Ref-VOS benchmarks.", "description": "Table 16 compares the performance of the Sa2VA model against existing Ref-VOS (Referring Video Object Segmentation) benchmarks. It highlights the differences in several key aspects: the length of text descriptions used, the presence of large object motion, significant camera movement, and heavy occlusion. This comparison helps demonstrate Sa2VA's ability to handle complex scenarios not well-addressed in previous benchmarks.", "section": "3.3 Ref-SAV Dataset and Benchmark"}]