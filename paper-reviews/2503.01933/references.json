{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper is highly influential as it introduced the concept of large language models and their few-shot learning capabilities."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-00-00", "reason": "This paper is important because it introduced the LLaMA model, which is an open and efficient foundation language model."}, {"fullname_first_author": "Manzil Zaheer", "paper_title": "Big bird: Transformers for longer sequences", "publication_date": "2020-00-00", "reason": "This is a seminal work that addresses the challenge of long sequences in transformers, which has been improved upon in the Shakti architecture."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-00-00", "reason": "This paper is critical as it introduces Direct Preference Optimization (DPO), a key technique used to align Shakti models with user preferences efficiently."}, {"fullname_first_author": "Jianlin Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "publication_date": "2021-00-00", "reason": "This paper is critical as it introduces the Rotary Positional Embeddings (RoPE) used within the Shakti architecture."}]}