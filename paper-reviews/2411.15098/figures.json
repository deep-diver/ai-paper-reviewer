[{"figure_path": "https://arxiv.org/html/2411.15098/x2.png", "caption": "Figure 1: \nResults of our OminiControl\u00a0on both subject-driven generation (top) and spatially-aligned tasks (bottom).\nThe small images in red boxes show the input conditions.", "description": "This figure showcases the capabilities of OminiControl, a novel framework for controlling image generation in diffusion transformers.  The top row demonstrates subject-driven generation, where OminiControl generates images featuring a specific subject (e.g., a penguin, a person) based on a small input image of the subject.  The bottom row shows examples of spatially-aligned tasks, in which OminiControl incorporates spatial information from an input image (like edges or depth maps) to guide the generation process.  In all cases, the small images within red boxes represent the input conditions provided to OminiControl.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.15098/x3.png", "caption": "Figure 2: Overview of the Diffusion Transformer (DiT) architecture and integration methods for image conditioning.", "description": "This figure illustrates the architecture of a Diffusion Transformer (DiT) model and details two methods for integrating image-based conditioning.  The DiT processes both noisy image tokens and text condition tokens through multiple transformer blocks, each containing a multi-modal attention (MM-Attention) module.  The figure contrasts two approaches to incorporate image conditions: (b) shows the original DiT without image conditioning; (c) demonstrates a direct addition method where the image condition tokens are simply added to the existing tokens, and (d) presents a more sophisticated MM-Attention integration method where image condition tokens participate in the attention mechanism alongside the noisy image tokens and text tokens, leading to more flexible and efficient multi-modal interactions. The methods are compared in terms of their impact on the model's ability to respond effectively to image-based control signals during image generation.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2411.15098/x4.png", "caption": "Figure 3: Comparison of results using two methods for integrating image conditions. The multi-modal approach demonstrates better condition following compared to direct addition.", "description": "This figure compares two methods for incorporating image conditions into a diffusion model: direct addition and multi-modal attention.  The results show that the multi-modal approach, which uses a shared attention mechanism, better incorporates the image condition into the generated image than simply adding the image condition's features directly to the hidden states of the diffusion model. This leads to generated images that more closely match the desired image condition.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2411.15098/x5.png", "caption": "(a) Training losses for different image condition integration methods.", "description": "This figure shows a comparison of training losses for different methods of integrating image conditions into a diffusion model.  The x-axis represents the number of training steps, and the y-axis represents the loss value.  Different colored lines represent different methods, allowing for a visual comparison of their convergence rates and overall performance during training.  The lower the loss, the better the model's performance.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2411.15098/x6.png", "caption": "(b) Training loss for shared vs. shifting position.", "description": "The figure shows the training loss curves comparing two different positional embedding strategies: one where the condition image tokens share the same position embeddings as the corresponding noisy image tokens (shared position), and another where the condition image tokens' positions are shifted (shifting position).  The x-axis represents the training steps, and the y-axis shows the training loss.  The plot helps to illustrate the impact of these different positional embedding methods on the overall performance of the model during training, demonstrating that shifting the position embeddings leads to faster convergence.", "section": "3.3 Adaptive position embedding"}, {"figure_path": "https://arxiv.org/html/2411.15098/extracted/6018223/fig/data.png", "caption": "Figure 4: Training loss comparisons.", "description": "This figure presents two sub-figures showing training loss curves. Sub-figure (a) compares the training losses of different image condition integration methods (Direct Addition vs. MM Attention), highlighting the superior performance of the MM Attention method which achieves lower loss. Sub-figure (b) demonstrates the effect of shared versus shifting position on the training loss, indicating that shifting the position of the image condition tokens leads to lower loss, thus suggesting that a unified sequence is more effective for processing both aligned and non-aligned image condition tasks.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2411.15098/extracted/6018223/fig/compair.jpg", "caption": "Figure 5: (a) Attention maps for the Canny-to-image task, showing interactions between noisy image tokens X\ud835\udc4bXitalic_X and image condition tokens CIsubscript\ud835\udc36\ud835\udc3cC_{I}italic_C start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT. Strong diagonal patterns indicate effective spatial alignment. (b) Subject-driven generation task, with input condition and output image. Attention maps for X\u2192Ci\u2192\ud835\udc4bsubscript\ud835\udc36\ud835\udc56X\\to C_{i}italic_X \u2192 italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and Ci\u2192X\u2192subscript\ud835\udc36\ud835\udc56\ud835\udc4bC_{i}\\to Xitalic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2192 italic_X illustrate accurate subject-focused attention.", "description": "Figure 5 presents attention maps illustrating the interaction between noisy image tokens and conditional image tokens in two scenarios: Canny-to-image and subject-driven generation. In (a), the Canny-to-image task shows strong diagonal patterns in the attention map, indicating effective spatial alignment between input edges and generated image details. In (b), the subject-driven generation task showcases accurate subject-focused attention in the attention map, demonstrating the model's ability to capture and utilize subject information during generation.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2411.15098/x7.png", "caption": "Figure 6: \nExamples from our Subjects200Kdataset. Each pair of images shows the same object in varying positions, angles, and lighting conditions.\nThe dataset includes diverse objects such as clothing, furniture, vehicles, and animals, totaling over 200,000 images.\nThis dataset, along with the generation pipeline, will be publicly released.", "description": "The figure showcases examples from the Subjects200K dataset, a collection of over 200,000 image pairs. Each pair depicts the same object, but with variations in pose, angle, and lighting. The objects are diverse, including clothing, furniture, vehicles, and animals.  The Subjects200K dataset and its generation pipeline are publicly available.", "section": "3.5. Subjects200K datasets"}, {"figure_path": "https://arxiv.org/html/2411.15098/x8.png", "caption": "Figure 7: \n\nQualitative results comparing different methods.\nLeft: Spatially aligned tasks across Canny, depth, out-painting, deblurring, colorization.\nRight: Subject-driven generation with beverage can, shoes and robot toy.\nOur method demonstrates superior controllability and visual quality across all tasks.", "description": "This figure presents a qualitative comparison of different image generation methods, showcasing their performance on both spatially aligned and subject-driven tasks.  The left side displays results for spatially aligned tasks such as Canny edge detection, depth estimation, out-painting, deblurring, and colorization. The right side shows examples of subject-driven generation, where the model generates images of specific objects (a beverage can, shoes, and a robot toy) based on given input conditions.  The results highlight that the proposed 'Our method' consistently achieves better controllability and superior visual quality in all tested scenarios, compared to existing state-of-the-art methods.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2411.15098/x9.png", "caption": "Figure 8: \nRadar charts visualization comparing our method (blue) with baselines across five evaluation metrics.", "description": "Figure 8 presents a comparison of the proposed method against several baselines across five key evaluation metrics for subject-driven image generation.  The radar chart visualizes the performance of each method on the following: Identity Preservation, Material Quality, Color Fidelity, Natural Appearance, and Modification Accuracy.  Each axis represents one of these metrics, and the distance of each method's data point from the center indicates its performance level on that metric.  The figure allows for a quick visual comparison of the strengths and weaknesses of different approaches in terms of both subject consistency and the accuracy of modifications made during generation.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2411.15098/x10.png", "caption": "Figure 9: \n\nComparison of models trained with different data.\nThe model trained by data augmentation tends to copy inputs directly, while model trained by our Subjects200Kgenerates novel views while preserving identity.", "description": "Figure 9 shows a comparison of image generation results from two different training methods. The left side displays results from a model trained using standard data augmentation techniques. The images produced by this model are very similar to the input images, indicating that the model is simply copying the input. The right side displays results from a model trained using the Subjects200K dataset created by the authors.  The images generated by this model show varied poses and viewpoints of the subject but still maintain the subject's identity and key features. This demonstrates the effectiveness of the Subjects200K dataset in training a model that can generate diverse outputs while preserving the integrity of the subject.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2411.15098/x11.png", "caption": "Figure 10: \n\nComparison of models trained with different data.\nThe model trained by data augmentation tends to copy inputs directly, while model trained by our Subjects200Kgenerates novel views while preserving identity.", "description": "This figure shows a comparison of image generation results from two different training approaches. The left column displays images generated by a model trained using traditional data augmentation techniques.  These results show a tendency to simply copy the input image, lacking originality. In contrast, the right column presents images generated by a model trained with the Subjects200K dataset.  These images demonstrate the model's ability to generate novel views and perspectives of the subject while faithfully maintaining its identity and key characteristics. This highlights the effectiveness of the Subjects200K dataset in enabling identity-preserving subject-driven image generation.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2411.15098/extracted/6018223/fig/dreambooth.jpg", "caption": "Figure S1: \nExamples of successful and failed generation results from Subjects200K\u00a0dataset. Green checks indicate successful cases where subject identity and characteristics are well preserved, while red crosses show failure cases.", "description": "Figure S1 shows examples from the Subjects200K dataset, highlighting the quality control process. Successful generations (green checkmarks) maintain subject identity and characteristics across different scenes.  Failed generations (red crosses) show inconsistencies, such as blurry images, missing parts, or altered identities.", "section": "A. Details of Subjects200K datasets"}, {"figure_path": "https://arxiv.org/html/2411.15098/extracted/6018223/fig/more_results.jpg", "caption": "Figure S2: An example of our structured description format for dataset generation.", "description": "This figure shows the hierarchical structure of the descriptions used to generate the Subjects200K dataset.  It details the three levels of description: (1) a brief description of the object, (2) a list of scene descriptions for placing the object in various settings, and (3) a description of a studio photo of the object.  This structured approach ensured consistency of subject matter across diverse image settings.", "section": "A. Details of Subjects200K datasets"}]