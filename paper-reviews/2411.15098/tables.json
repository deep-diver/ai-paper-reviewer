[{"content": "| Condition | Model | Method | Controllability | FID \u2193 | SSIM \u2191 | MAN-IQA \u2191 | MUSIQ \u2191 | Text Consistency | CLIP-Score \u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Canny | SD1.5 | ControlNet | 0.34 | 18.74 | 0.35 | 0.45 | 67.81 | 0.75 |  |\n|  |  | T2I-Adapter | 0.22 | 20.06 | 0.35 | 0.39 | 67.88 | 0.74 |  |\n|  | FLUX.1 | ControlNet | 0.21 | 98.68 | 0.25 | 0.37 | 56.90 | 0.53 |  |\n|  | Ours |  | 0.38 | 20.63 | 0.40 | 0.61 | 75.91 | 0.76 |  |\n| Depth | SD1.5 | ControlNet | 923 | 23.02 | 0.34 | 0.47 | 70.73 | 0.726 |  |\n|  |  | T2I-Adapter | 1560 | 24.72 | 0.27 | 0.39 | 69.99 | 0.72 |  |\n|  | FLUX.1 | ControlNet | 2958 | 62.20 | 0.26 | 0.38 | 66.84 | 0.54 |  |\n|  | Ours |  | 903 | 27.26 | 0.39 | 0.55 | 75.06 | 0.728 |  |\n| Deblur | FLUX.1 | ControlNet | 572 | 30.38 | 0.74 | 0.31 | 54.37 | 0.78 |  |\n|  | Ours |  | 132 | 11.49 | 0.87 | 0.39 | 67.63 | 0.87 |  |\n| Colorization | FLUX.1 | ControlNet | 351 | 16.27 | 0.64 | 0.43 | 70.95 | 0.85 |  |\n|  | Ours |  | 24 | 10.23 | 0.73 | 0.43 | 70.74 | 0.90 |  |\n| Mask | SD1.5 | ControlNet | 7588 | 13.14 | 0.40 | 0.41 | 67.22 | 0.84 |  |\n|  | FLUX.1 | Ours | 6248 | 15.66 | 0.48 | 0.45 | 72.61 | 0.80 |  |", "caption": "Table 1: Quantitative comparison with baseline methods on five spatially aligned tasks. We evaluate methods based on Controllability (F1-Score for Canny, MSE for others), General Quality (FID, SSIM, MAN-IQA, MUSIQ), and Text Consistency (CLIP-Score). For F1-Score, higher is better; for MSE, lower is better. Best results are shown in bold.", "description": "Table 1 presents a quantitative comparison of different methods on five spatially aligned image generation tasks.  The tasks are Canny edge detection, depth estimation, out-painting, deblurring, and colorization.  The methods being compared include ControlNet, T2I-Adapter, and the authors' proposed method, OminiControl, applied to both Stable Diffusion 1.5 (SD1.5) and FLUX.1 models.  Evaluation metrics are categorized into Controllability (measured by F1-score for Canny and Mean Squared Error (MSE) for the other tasks), General Quality (assessed using Fr\u00e9chet Inception Distance (FID), Structural Similarity Index (SSIM),  Mobile-Intelligence-Quality (MAN-IQA), and Multi-Scale Image Quality (MUSIQ)), and Text Consistency (measured by CLIP-score).  Higher F1-scores and lower MSE values indicate better performance in terms of controllability.  Lower FID values, higher SSIM, MAN-IQA, MUSIQ, and CLIP scores indicate better general image quality and text consistency. The table highlights the best performing method for each metric in bold.", "section": "4. Experiment"}, {"content": "| Methods | Base model | Parameters | Ratio |\n|---|---|---|---| \n| ControlNet | SD1.5 / 860M | 361M | ~42% |\n| T2I-Adapter | 77M | ~9.0% |\n| IP-Adapter | 449M | ~52.2% |\n| ControlNet | FLUX.1 / 12B | 3.3B | ~27.5% |\n| IP-Adapter | 918M | ~7.6% |\n| Ours | FLUX.1 / 12B | 14.5M / 48.7M w/ Encoder | ~0.1% / ~0.4% w/ Encoder |", "caption": "Table 2: Additional parameters introduced by different image conditioning methods. For IP-Adapter, the parameter count includes the CLIP Image encoder. For our method, we also report results when using the original VAE encoder from FLUX.1.", "description": "This table compares the number of additional parameters required by different image conditioning methods when integrated with the FLUX-1 diffusion model.  It highlights the parameter efficiency of the proposed OminiControl method compared to ControlNet, T2I-Adapter, and IP-Adapter.  The comparison includes the parameters of the CLIP image encoder for IP-Adapter and provides results for OminiControl both with and without using the original FLUX-1 VAE encoder for a fair comparison.", "section": "3. Methods"}, {"content": "| Study | Setting | FID \u2193 | SSIM \u2191 | F1 Score \u2191 | CLIP Score \u2191 |\n|---|---|---|---|---|---| \n| LoRA Rank 1 | 1 | 21.09 | 0.412 | 0.385 | **0.765** |\n| LoRA Rank 2 | 2 | 21.28 | 0.411 | 0.377 | 0.751 |\n| LoRA Rank 4 | 4 | 20.63 | 0.407 | 0.380 | 0.761 |\n| LoRA Rank 8 | 8 | 21.40 | 0.404 | 0.3881 | 0.761 |\n| LoRA Rank 16 | 16 | **19.71** | **0.425** | **0.407** | 0.764 |\n| Condition Blocks |  |  |  |  |  |\n| Early | Early | 25.66 | 0.369 | 0.23 | 0.72 |\n| Full | Full | **20.63** | **0.407** | **0.38** | **0.76** |", "caption": "Table 3: Ablation studies on (1) LoRA rank for the Canny-to-image task and (2) condition signal integration approaches. Results show that LoRA rank of 16 and full-depth integration achieve the best performance. Rows with blue background indicate our default settings (LoRA rank=4, Full condition integration). Best results are in bold.", "description": "This ablation study investigates the impact of two hyperparameters on the performance of the Canny-to-image task: LoRA rank and condition signal integration approach.  The table shows that using a LoRA rank of 16 and integrating the condition signal across all transformer blocks (full-depth integration) yields the best results. The rows highlighted with a blue background represent the default hyperparameter settings used in the main experiments of the paper (LoRA rank 4 and full condition integration).  The best performance in each metric is shown in bold.", "section": "4. Experiment"}, {"content": "| Method | Identity | Material | Color | Natural | Modification | Average |\n|---|---|---|---|---|---|---|\n|  | preservation | quality | fidelity | appearance | accuracy | score |\n| Average over 5 random seeds |  |  |  |  |  |  |\n| IP-Adapter (SD 1.5) | 29.4 | 86.1 | 45.3 | 97.9 | 17.0 | 55.1 |\n| SSR-Encoder | 46.0 | 92.0 | 54.2 | 96.3 | 28.5 | 63.4 |\n| IP-Adapter (FLUX) | 11.8 | 65.8 | 30.8 | 98.1 | 57.7 | 52.8 |\n| Ours | **50.6** | 84.3 | **55.0** | **98.5** | **75.8** | **72.8** |\n| Best score over 5 random seeds |  |  |  |  |  |  |\n| IP-Adapter (SD 1.5) | 56.3 | 98.9 | 70.1 | 99.7 | 37.2 | 72.5 |\n| SSR-Encoder | 64.3 | **99.2** | 74.4 | 99.1 | 53.6 | 78.1 |\n| IP-Adapter (FLUX) | 27.5 | 86.1 | 53.6 | 99.9 | 74.9 | 68.4 |\n| Ours | **82.3** | 98.0 | **88.4** | **100.0** | **90.7** | **91.9** |", "caption": "Table S1: Quantitative evaluation results (in percentage) across different evaluation criteria. Higher values indicate better performance.", "description": "Table S1 presents a quantitative comparison of different methods for subject-driven image generation, evaluated across five key criteria: Identity Preservation, Material Quality, Color Fidelity, Natural Appearance, and Modification Accuracy. Each criterion is assessed using a percentage score, with higher scores indicating better performance. The table compares the performance of the proposed method against several baselines, offering a comprehensive view of its strengths and weaknesses in terms of generating images that accurately reflect both the subject's identity and any requested modifications.", "section": "B. Additional experimental results"}]