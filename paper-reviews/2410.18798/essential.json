{"importance": "This paper is highly important for researchers working on multimodal learning and large language models.  It introduces a novel and efficient data synthesis method, addresses the challenges of creating high-quality multimodal datasets, and demonstrates improved performance on various benchmarks.  The proposed method opens new avenues for research in cost-effective data augmentation and the development of advanced visual reasoning capabilities in LLMs.", "summary": "Researchers synthesize a new multimodal dataset, REACHQA, using code as an intermediary to efficiently distill visual chart reasoning abilities from LLMs to MLLMs.", "takeaways": ["Code-as-Intermediary Translation (CIT) is proposed as a cost-effective data synthesis method for improving visual reasoning in MLLMs.", "REACHQA, a new dataset with 3k reasoning-intensive charts and 20k Q&A pairs, is created to enhance both visual recognition and reasoning abilities.", "Fine-tuning models on REACHQA improves performance on chart-related benchmarks and general mathematical reasoning tasks."], "tldr": "This research tackles the challenge of enhancing multimodal large language models (MLLMs) with advanced visual reasoning abilities, particularly for complex chart question answering (CQA).  The key contribution is a novel data synthesis method called Code-as-Intermediary Translation (CIT). CIT uses code as a bridge between visual chart representations and textual descriptions, allowing LLMs to understand and reason about cross-modal information.  This approach is significantly more efficient and cost-effective than traditional manual data collection and annotation.  Using CIT, the researchers created REACHQA, a new dataset with 3,000 reasoning-intensive charts and 20,000 Q&A pairs.  Experiments demonstrate that fine-tuning models on REACHQA substantially improves their performance on various chart-related benchmarks, even generalizing to broader mathematical reasoning tasks.  This work offers a valuable approach for developing advanced visual reasoning capabilities in LLMs and creating high-quality multimodal datasets efficiently."}