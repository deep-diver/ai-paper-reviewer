{"reason": "The research paper introduces Code-as-Intermediary Translation (CIT), a novel method for efficiently creating training data for multimodal large language models (MLLMs) to improve their visual chart reasoning abilities.  The method uses code as an intermediary to translate visual chart representations into text, allowing LLMs to learn from the text and improve their ability to understand and reason about charts.", "summary": "Boosting visual chart reasoning in MLLMs via Code-as-Intermediary Translation (CIT):  efficiently generating high-quality training data by leveraging LLMs.", "takeaways": ["Code-as-Intermediary Translation (CIT) is an efficient and scalable method for creating training data for MLLMs focused on chart understanding.", "The REACHQA dataset (3k charts and 20k Q&A pairs) significantly enhances MLLM performance on chart-related and general mathematical reasoning benchmarks.", "Findings suggest that mixing general-purpose multimodal instruction data with chart-specific data yields better performance than using chart-specific data alone."], "tldr": "This paper tackles the challenge of improving multimodal large language models' (MLLMs) ability to understand and reason using visual chart data.  Manually creating such datasets is expensive and time-consuming. The researchers introduce 'Code-as-Intermediary Translation' (CIT), a clever technique that uses code as a bridge between the visual chart and textual representation.  This allows them to leverage the power of LLMs to generate both the charts and questions efficiently. Using this method, they create REACHQA, a dataset containing 3,000 charts and 20,000 question-answer pairs.  Experiments show that models fine-tuned on this dataset perform much better on various chart-related benchmarks, demonstrating improved visual reasoning skills.  The improvement generalizes even to non-chart-related mathematical problems, highlighting the method's effectiveness.  The researchers also investigate the effect of combining general-purpose multimodal instruction data with their chart-focused data and find that this mixed approach yields even better results."}