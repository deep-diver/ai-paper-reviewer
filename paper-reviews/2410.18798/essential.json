{"reason": "This paper introduces Code-as-Intermediary Translation (CIT), a novel data synthesis method for enhancing visual reasoning in multimodal large language models (MLLMs).  CIT uses code as an intermediary to translate visual chart representations into text, enabling LLMs to understand cross-modal information and generate high-quality chart-related Q&A pairs. The resulting dataset, REACHQA, significantly improves MLLM performance on various benchmarks, showcasing the effectiveness of CIT for cost-efficient data creation and improved multimodal reasoning.", "takeaways": ["Code-as-Intermediary Translation (CIT) is an effective method for synthesizing multimodal instruction data for visual reasoning in LLMs.", "The REACHQA dataset, created using CIT, significantly improves the performance of LLMs on chart-related benchmarks and general multimodal reasoning tasks.", "CIT offers a scalable and cost-effective approach to generating high-quality training data for visual reasoning, addressing the limitations of manual data collection and annotation."], "tldr": "Researchers created a new method called Code-as-Intermediary Translation (CIT) to improve multimodal large language models (MLLMs) understanding of charts. CIT uses code to translate visual charts into text, allowing LLMs to better reason and answer questions about charts.  They created a new dataset, REACHQA, using this method, which significantly improved MLLM performance on various benchmarks. This approach is efficient and scalable, solving the challenge of creating high-quality data for visual reasoning."}