[{"figure_path": "2410.18798/figures/figures_2_0.png", "caption": "Figure 1: Error distribution of incorrect answers by MiniCPM-V2.5-Llama3 (Yao et al., 2024) on ChartQA test set (Masry et al., 2022), as judged by GPT-40. We present an example chart from ChartQA along with two error cases: one for recognition and one for reasoning. The \"Other Errors\" include question misunderstood errors, knowledge and hallucination errors, or refusal to answer.", "description": "The figure displays a pie chart showing the error distribution of a language model (MiniCPM-V2.5-Llama3) on the ChartQA test set.  It breaks down the errors into three categories: recognition error (62%), reasoning error with correct recognition (36%), and other errors (2%). A sample chart from the ChartQA dataset is shown alongside two examples illustrating recognition and reasoning errors. The recognition error example shows the model incorrectly identifying the country represented by a specific color in the bar chart. The reasoning error example shows the model correctly recognizing elements but making an error in its subsequent reasoning to arrive at the final answer.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18798/figures/figures_5_0.png", "caption": "Overview of the Code-as-Intermediary Translation (CIT) method for synthesizing multimodal instruction data. The process begins with 33 seed codes and generates plot codes across various chart types, topics, and complexity levels through the Self-Instruct and Evol-Instruct stages. The chart set and instruction set are constructed bi-directionally, and the final filtered data yields REACHQA, a dataset for distilling visual chart reasoning abilities from LLMs to MLLMs.", "description": "The figure is a flowchart illustrating the Code-as-Intermediary Translation (CIT) method used to create the REACHQA dataset. It starts with 33 seed codes from the Matplotlib gallery, which are then expanded using Self-Instruct and Evol-Instruct methods to generate plot codes with varying complexity and chart types.  These codes are used to generate charts (via Python) and corresponding instructions (via LLMs) in a bi-directional process, ensuring alignment between the two modalities.  The generated charts and instructions are then filtered using multiple open-source LLMs to create the final REACHQA dataset, which contains both recognition-oriented and reasoning-oriented tasks.", "section": "3 REACHQA: SYNTHESIZING CHART Q&A WITH CIT"}, {"figure_path": "2410.18798/figures/figures_10_0.png", "caption": "Figure 5: An example of attention visualization from the ChartQA dataset. The top row shows the results from the vanilla LLaVA-Next-Llama3-8B model, while the bottom row displays the results from our fine-tuned model. For each output, we present the attention distribution (highlighted zones) at three key steps, calculated by averaging the attention values of all tokens in each step.", "description": "This figure presents a comparative case study of attention visualizations from a vanilla and a fine-tuned LLaVA-Next-Llama3-8B model on the ChartQA dataset.  The top row shows the attention distribution of the vanilla model for a question about average job applications on LinkedIn in the US, highlighting the model's scattered attention and ultimately incorrect answer. The bottom row contrasts this with the fine-tuned model's focused attention, correctly identifying key information and providing the accurate answer.  The attention distributions are shown for three key steps in each model's reasoning process, visualized as highlighted zones on the bar charts.", "section": "5.4 INTERPRETABILITY STUDY FROM THE PERSPECTIVE OF ATTENTION"}, {"figure_path": "2410.18798/figures/figures_17_0.png", "caption": "Figure 6: Visualizations of different chart-related training datasets. As shown, REACHQA and ChartGemma exhibit higher chart richness compared to several other datasets. But the charts in ChartGemma require manual collection from multiple sources (Masry et al., 2024b).", "description": "The figure displays visualizations of charts from five different chart-related datasets: ChartQA, ChartBench, ChartAssistant, ChartGemma, and REACHQA. Each dataset's visualization shows a range of chart types and complexities. ChartQA and ChartBench contain fewer chart types and simpler visual designs. ChartAssistant and ChartGemma contain more diverse chart types but also lack visual complexity. In contrast, REACHQA shows more chart types and significantly higher visual complexity, indicating a richer and more varied dataset.", "section": "2 BACKGROUND"}, {"figure_path": "2410.18798/figures/figures_17_1.png", "caption": "Figure 6: Visualizations of different chart-related training datasets. As shown, REACHQA and ChartGemma exhibit higher chart richness compared to several other datasets. But the charts in ChartGemma require manual collection from multiple sources (Masry et al., 2024b).", "description": "This figure displays visualizations of charts from five different chart-related datasets: ChartQA, ChartBench, ChartAssistant, ChartGemma, and REACHQA.  Each section shows a collection of charts, illustrating the variety of chart types and visual complexity within each dataset.  The figure highlights that REACHQA and ChartGemma contain a greater diversity and complexity of charts than the other datasets, indicating richer visual information and more challenging reasoning tasks.  The caption notes that while ChartGemma does have diverse charts, they were manually collected from multiple sources unlike the LLM-generated REACHQA dataset.", "section": "2 BACKGROUND"}, {"figure_path": "2410.18798/figures/figures_17_2.png", "caption": "Figure 6: Visualizations of different chart-related training datasets. As shown, REACHQA and ChartGemma exhibit higher chart richness compared to several other datasets. But the charts in ChartGemma require manual collection from multiple sources (Masry et al., 2024b).", "description": "The figure displays visualizations of charts from five different chart-related datasets: ChartQA, ChartBench, ChartAssistant, ChartGemma, and REACHQA.  Each visualization shows a collection of charts, illustrating the variety of chart types and complexity levels within each dataset.  The figure highlights that REACHQA and ChartGemma contain more visually complex and diverse chart types than the other three datasets.  This difference underscores the increased difficulty and richness of the chart data in REACHQA, achieved through a low-cost synthesis process, compared to the manually curated datasets, which are more limited in their visual complexity and variety.", "section": "2 BACKGROUND"}, {"figure_path": "2410.18798/figures/figures_17_3.png", "caption": "Figure 6: Visualizations of different chart-related training datasets. As shown, REACHQA and ChartGemma exhibit higher chart richness compared to several other datasets. But the charts in ChartGemma require manual collection from multiple sources (Masry et al., 2024b).", "description": "Figure 6 presents visualizations of charts from five different datasets: ChartQA, ChartBench, ChartAssistant, ChartGemma, and REACHQA.  Each visualization shows a collection of charts from the respective datasets, highlighting the variety of chart types and visual complexity within each.  ChartQA and ChartBench/ChartAssistant show relatively simple charts, whereas ChartGemma displays charts that are more visually rich but were collected from various online sources.  REACHQA contains the most visually complex and varied charts in terms of types and visual elements, demonstrating its broader scope and more challenging nature compared to the other datasets.", "section": "2 BACKGROUND"}]