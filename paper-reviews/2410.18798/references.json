{"references": [{" publication_date": "2022", "fullname_first_author": "Ahmed Masry", "paper_title": "ChartQA: A benchmark for question answering about charts with visual and logical reasoning", "reason": "This paper introduced ChartQA, a benchmark dataset crucial for evaluating visual and logical reasoning abilities in multimodal large language models. The dataset's focus on complex chart understanding and the detailed error analysis in the introduction directly relate to this paper's core contribution.", "section_number": 1}, {" publication_date": "2024a", "fullname_first_author": "Ke Wang", "paper_title": "Measuring multimodal mathematical reasoning with math-vision dataset", "reason": "This paper provides a benchmark dataset (Math-Vision) for evaluating multimodal mathematical reasoning, which is a relevant and important area considering the paper's focus on visual reasoning in charts. The dataset's complexity and focus on advanced reasoning tasks align with the broader goals of the paper.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Pan Lu", "paper_title": "MathVista: Evaluating mathematical reasoning of foundation models in visual contexts", "reason": "This paper offers another critical benchmark dataset (MathVista) for evaluating multimodal mathematical reasoning, focusing specifically on mathematical problem-solving in visual contexts.  This is highly relevant to the current paper as it provides another standard for comparing performance.", "section_number": 1}, {" publication_date": "2024c", "fullname_first_author": "Zirui Wang", "paper_title": "CharXiv: Charting gaps in realistic chart understanding in multimodal LLMs", "reason": "This paper introduced CharXiv, a benchmark dataset directly relevant to the present work, assessing both recognition and reasoning abilities. Its insights into challenges in chart understanding and the results achieved contribute to the current study's context and findings.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yucheng Han", "paper_title": "ChartLlama: A multimodal LLM for chart understanding and generation", "reason": "This paper provides another significant approach to the problem of chart understanding, exploring the use of LLMs for both chart understanding and generation.  ChartLlama provides an alternative methodology for comparison and contrast with the Code-as-Intermediary Translation (CIT) method proposed in this paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yizhong Wang", "paper_title": "Self-Instruct: Aligning language models with self-generated instructions", "reason": "This paper introduced the Self-Instruct method, a crucial technique employed in the current paper's methodology. Self-Instruct's use in the dataset creation (REACHQA) highlights its role in data synthesis and generation.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Can Xu", "paper_title": "WizardLM: Empowering large pre-trained language models to follow complex instructions", "reason": "This paper presented WizardLM, a powerful LLM framework that enhances the processing of complex instructions.  This technique's relation to instruction tuning and complex chart understanding is relevant to the current paper's methodology.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Ahmed Masry", "paper_title": "ChartInstruct: Instruction tuning for chart comprehension and reasoning", "reason": "This paper introduced ChartInstruct, a model specifically designed for enhanced chart comprehension and reasoning.  The model's relevance in the field of visual chart reasoning makes it an important reference for comparison in this paper.", "section_number": 4}, {" publication_date": "2024b", "fullname_first_author": "Ahmed Masry", "paper_title": "ChartGemma: Visual instruction-tuning for chart reasoning in the wild", "reason": "This paper introduced ChartGemma, a model which addresses chart understanding using visual instruction-tuning techniques.  Its methodologies and findings are relevant for comparing and contrasting with the approaches and results in the present paper.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Bohao Li", "paper_title": "Seed-bench: Benchmarking multimodal LLMs with generative comprehension", "reason": "This paper provides a valuable benchmarking resource (Seed-bench) for evaluating multimodal large language models (MLLMs). The benchmark's focus on generative comprehension tasks is relevant to evaluating the performance and capabilities of various models.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yifan Li", "paper_title": "Evaluating object hallucination in large vision-language models", "reason": "This paper addresses the challenge of object hallucination in vision-language models, which is a relevant issue given the complexities and potential for error in visual chart interpretation. The findings provide important context related to the potential issues with visual data processing.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Takeshi Kojima", "paper_title": "Large language models are zero-shot reasoners", "reason": "This paper highlights the capacity of large language models to perform zero-shot reasoning, which is a critical aspect of this paper's methodology using LLMs for both the generation and reasoning tasks.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Chaoyou Fu", "paper_title": "MME: A comprehensive evaluation benchmark for multimodal large language models", "reason": "This paper provided MME, a benchmark dataset covering a broad range of multimodal tasks, including visual question answering. MME's comprehensiveness allows for broader evaluation of multimodal reasoning abilities, important for assessing performance beyond chart-specific tasks.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Deqing Fu", "paper_title": "Isobench: Benchmarking multimodal foundation models on isomorphic representations", "reason": "This paper introduced Isobench, offering another relevant and complementary benchmark for multimodal foundation models. This resource provides broader comparisons and insights into the strengths and weaknesses of different models.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Tianrui Guan", "paper_title": "Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models", "reason": "This paper introduced Hallusionbench, offering a benchmark and detailed analysis of hallucinations and illusions in vision-language models. The consideration of hallucinations is important for evaluating the reliability and accuracy of the LLM-generated dataset.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Shankar Kantharaj", "paper_title": "OpenCQA: Open-ended question answering with charts", "reason": "This paper introduced OpenCQA, a dataset relevant to this study focusing on question answering with charts. OpenCQA provides a valuable dataset for comparison and contrast, highlighting differences in dataset design and methodology.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zhengzhuo Xu", "paper_title": "Chartbench: A benchmark for complex visual reasoning in charts", "reason": "This paper introduced ChartBench, a relevant benchmark dataset for assessing complex visual reasoning in charts. Its inclusion provides a standardized comparison against the proposed REACHQA dataset.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Renqiu Xia", "paper_title": "ChartX & ChartVLM: A versatile benchmark and foundation model for complicated chart reasoning", "reason": "ChartX is a benchmark dataset focusing on visual reasoning abilities in charts. The comparison with ChartX provides valuable insights into the design considerations and challenges in evaluating visual reasoning tasks.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Nitesh Methani", "paper_title": "PlotQA: Reasoning over scientific plots", "reason": "PlotQA is an early benchmark dataset focusing on question answering about scientific plots. Its inclusion helps to provide historical context and demonstrates the evolution of datasets in this field.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zhiheng Xi", "paper_title": "Training large language models for reasoning through reverse curriculum reinforcement learning", "reason": "This paper explores the use of reverse curriculum reinforcement learning to train large language models for enhanced reasoning abilities. The approach and findings are relevant considering the paper's focus on improving reasoning capabilities in multimodal models.", "section_number": 5}]}