[{"figure_path": "2410.18798/tables/table_4_0.md", "caption": "Comparison of existing chart-related datasets across three properties. Only the chart question-answering (CQA) task is considered, despite some datasets having multiple tasks. Abbreviations: Vis.=visual, Comp.=complexity, Temp.=template, Refer.=Reference, Reas.=reasoning, Rat. rationale, Annot.=annotation and Scal.=scalable. Cells marked with \"X\" indicate mixed attributes (e.g., partially template-based; scalable Q&A but non-scalable chart data.). \u201c*\u201d indicates that while the chart-plotting codes are public, the Q&A generation still relies on data tables.", "description": "This table compares several existing chart-related datasets across three key properties: Q&A properties (textual format, complexity, template-based nature, visual format, reasoning, and rationale annotations), chart properties (chart type, number of charts, topic diversity, and code availability), and dataset properties (scalability, reference availability, annotation, and availability of train/test sets). Each dataset is evaluated based on these properties, with \"X\" indicating mixed attributes and \"*\" indicating that while code is publicly available, Q&A generation still relies on data tables. The table allows for a comparison of different datasets' characteristics and highlights the unique aspects of REACHQA.", "section": "2 BACKGROUND"}, {"figure_path": "2410.18798/tables/table_6_0.md", "caption": "Comparison of existing chart-related datasets across three properties. Only the chart question-answering (CQA) task is considered, despite some datasets having multiple tasks. Abbreviations: Vis.=visual, Comp.=complexity, Temp.=template, Refer.=Reference, Reas.=reasoning, Rat. rationale, Annot.=annotation and Scal.=scalable. Cells marked with \"X\" indicate mixed attributes (e.g., partially template-based; scalable Q&A but non-scalable chart data.). \u201c*\u201d indicates that while the chart-plotting codes are public, the Q&A generation still relies on data tables.", "description": "This table compares existing chart-related datasets across three key properties: Chart Properties (type, number of charts, topics, textual or visual format, complexity, and template-based or not), Q&A Properties (textual or visual format, complexity, template-based or not, visual references, reasoning, and rationale), and Dataset Properties (scalability, reference availability, annotation, and availability of train and test sets).  Each dataset is evaluated based on the presence or absence of these characteristics, indicated by checkmarks and \"X\" for mixed attributes or partially available properties. The table highlights differences in the visual complexity, diversity of chart types and topics, the methods used to generate Q&A pairs, the inclusion of rationales, and the scalability of the datasets.", "section": "2 BACKGROUND"}, {"figure_path": "2410.18798/tables/table_9_0.md", "caption": "Comparison of existing chart-related datasets across three properties. Only the chart question-answering (CQA) task is considered, despite some datasets having multiple tasks. Abbreviations: Vis.=visual, Comp.=complexity, Temp.=template, Refer.=Reference, Reas.=reasoning, Rat. rationale, Annot.=annotation and Scal.=scalable. Cells marked with \"X\" indicate mixed attributes (e.g., partially template-based; scalable Q&A but non-scalable chart data.). \u201c*\u201d indicates that while the chart-plotting codes are public, the Q&A generation still relies on data tables.", "description": "This table compares existing chart-related datasets across three key properties: Q&A properties (textual format, complexity, template-based or not, visual format, reasoning, rationale availability), chart properties (chart type, number of charts, topic diversity, visual complexity), and dataset properties (scalability, reference availability, annotation methods, train/test set split).  Each dataset is evaluated on these properties, indicated by checkmarks, crosses, or mixed notations depending on its characteristics.  The table highlights the differences between datasets in terms of the complexity of their charts, how the Q&A pairs were generated, and other dataset characteristics, such as the number of charts.", "section": "2 BACKGROUND"}, {"figure_path": "2410.18798/tables/table_15_1.md", "caption": "Comparison of existing chart-related datasets across three properties. Only the chart question-answering (CQA) task is considered, despite some datasets having multiple tasks. Abbreviations: Vis.=visual, Comp.=complexity, Temp.=template, Refer.=Reference, Reas.=reasoning, Rat. rationale, Annot.=annotation and Scal.=scalable. Cells marked with \"X\" indicate mixed attributes (e.g., partially template-based; scalable Q&A but non-scalable chart data.). \u201c*\u201d indicates that while the chart-plotting codes are public, the Q&A generation still relies on data tables.", "description": "The table compares existing chart-related datasets across three key aspects: chart properties (visual format, complexity, type, number of charts, and topics), Q&A properties (textual format, complexity, use of templates, presence of references and rationales, annotation method, and scalability), and dataset properties (scalability, presence of rationales, and availability of train and test sets).  It lists several datasets, including PlotQA, ChartQA, OpenCQA, MathVista, CharXiv, ChartBench, ChartX, MMC, ChartLlama, ChartAst, ChartInstruct, ChartGemma, and REACHQA (the authors' dataset), evaluating each based on the presence or absence of certain characteristics indicated by checkmarks and 'X's, which represent mixed attributes,  to highlight their strengths and weaknesses concerning visual diversity, complexity, and data synthesis methods.", "section": "2 BACKGROUND"}, {"figure_path": "2410.18798/tables/table_16_0.md", "caption": "Table 1: Comparison of existing chart-related datasets across three properties. Only the chart question-answering (CQA) task is considered, despite some datasets having multiple tasks. Abbreviations: Vis.=visual, Comp.=complexity, Temp.=template, Refer.=Reference, Reas.=reasoning, Rat. rationale, Annot.=annotation and Scal.=scalable. Cells marked with \"X\" indicate mixed attributes (e.g., partially template-based; scalable Q&A but non-scalable chart data.). \u201c*\u201d indicates that while the chart-plotting codes are public, the Q&A generation still relies on data tables.", "description": "Table 1 compares nine existing chart-related datasets across three key properties: chart properties (type, number of charts, topic, complexity, and whether visual or textual format is used), Q&A properties (textual format, complexity, presence of templates, visual elements, reasoning aspects, and rationales), and dataset properties (scalability, presence of rationales, and availability of train/test sets).  Each dataset is evaluated on these properties, using checkmarks and \"X\" to indicate the presence or a mixture of attributes. The table highlights the strengths and weaknesses of each dataset in terms of its visual diversity, Q&A quality, and scalability, providing a valuable overview for researchers working on chart-related tasks.", "section": "2 BACKGROUND"}]