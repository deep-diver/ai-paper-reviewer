[{"figure_path": "2410.18798/charts/charts_9_1.png", "caption": "Figure 4: Performance comparison of models on 6 general tasks and 4 specialized tasks.", "description": "This radar chart compares the performance of three different models across ten benchmark tasks.  The tasks are categorized into general-purpose multimodal benchmarks (MME-Reasoning, MME-Perception, SEEDBench_IMG, CCBench, POPE, HallusionBench) and specialized chart-related tasks (ReachQA, CharXiv, MathVista, MATH-V).  Each axis represents a benchmark task, and the distance from the center to the plotted point indicates the model's performance on that task; a larger distance means better performance. Three models are compared: a base model, a model fine-tuned on 20k REACHQA data, and a model fine-tuned on a mix of 20k REACHQA and 20k LLaVA-Next data.  The chart visually represents the relative strengths and weaknesses of each model across different task types, highlighting the effects of specialized training data on overall multimodal reasoning abilities.", "section": "4.2 EXPERIMENTAL RESULTS"}]