[{"page_end_idx": 3, "page_start_idx": 1, "section_number": 1, "section_title": "INTRODUCTION", "details": {"details": "The introduction section highlights the challenges of visual chart reasoning in multimodal large language models (MLLMs).  While MLLMs have made progress in visual recognition, understanding and answering complex questions about charts remains a significant hurdle.  The authors analyze the error distribution in the ChartQA dataset, revealing that 62% of errors stem from misrecognition of visual elements within the charts, while 36% are due to reasoning errors, even after correct recognition. This indicates that improving both visual recognition and reasoning abilities is crucial for advancing MLLM performance in chart-related tasks.  The authors propose that distilling rationales of reasoning from human experts or stronger models is a promising avenue for improvement, but this approach faces challenges in terms of cost and scalability due to the complexities of constructing high-quality training data, involving manual collection, filtering, and extensive human annotation of charts and associated questions.  The section concludes by positioning the remainder of the paper's proposed approach as a cost-effective and efficient data synthesis method to address these challenges.", "first_cons": "Data collection and annotation for complex chart-related tasks is expensive and time-consuming, posing a significant challenge to improving MLLM performance in this area.", "first_pros": "The authors highlight a crucial problem: existing MLLMs struggle with complex chart understanding and reasoning, creating a clear need for improved models and techniques.", "keypoints": ["62% of errors in ChartQA stem from misrecognition", "36% of errors in ChartQA stem from reasoning mistakes (even with correct recognition)", "The complexity of constructing high-quality training data for chart reasoning is a major obstacle"], "second_cons": "Existing benchmarks underscore the need for more advanced and generalized visual reasoning abilities, which are still underdeveloped in current MLLMs.", "second_pros": "The authors propose a promising solution focusing on distilling the rationales of reasoning from experts to create better training data, which directly addresses the core challenges of chart reasoning in MLLMs.", "summary": "The introduction section of the paper addresses the limitations of current multimodal large language models (MLLMs) in understanding and reasoning with visual charts, emphasizing the high cost and time commitment of creating suitable training data.  It highlights the significant error rates in existing benchmarks (62% misrecognition, 36% reasoning errors), emphasizing the need for a more effective approach to data synthesis and model training that can improve both visual recognition and reasoning capabilities."}}, {"page_end_idx": 3, "page_start_idx": 3, "section_number": 2, "section_title": "BACKGROUND", "details": {"details": "Existing chart-related datasets have limitations in terms of chart diversity, Q&A quality, and scalability.  Early datasets like ChartQA and OpenCQA, sourced from limited websites, featured uniform styles with minimal diversity.  Recent works like ChartAst try to synthesize charts with randomized attributes, but this doesn't fully address the lack of visual complexity.  Many datasets use predefined templates or manual annotation to generate Q&A pairs, leading to monotonous and simplistic questions or high costs and limited scalability.   Datasets that use LLMs for Q&A generation often overlook critical visual features and struggle to produce accurate and challenging chart-related Q&A pairs.  Manually annotated datasets provide high-quality data but are impractical for scaling to the size needed for training large models.  Therefore, there is a need for a dataset with a broader range of charts, more complex questions focusing on visual details, and scalability for training purposes.", "first_cons": "Many existing datasets lack visual diversity and complexity in their charts.  This limits the ability of models trained on these datasets to generalize to real-world scenarios where charts may have diverse styles and structures.", "first_pros": "The section clearly identifies the shortcomings of existing chart-related datasets, setting the stage for the introduction of a new dataset that addresses these issues.  This provides a strong motivation and context for the proposed approach.", "keypoints": ["Early datasets like ChartQA and OpenCQA lack visual diversity, relying on limited website sources.", "Recent works like ChartAst synthesize charts but still overlook visual complexity.", "Manual annotation of Q&A pairs leads to high cost and low scalability.", "LLM-based Q&A generation often fails to capture fine-grained visual details and produce challenging questions."], "second_cons": "The analysis of existing datasets' limitations is somewhat high-level, lacking detailed quantitative comparisons or specific examples to illustrate the shortcomings of each dataset.", "second_pros": "The discussion of the limitations effectively highlights the need for a more comprehensive and scalable dataset for training advanced visual reasoning models, creating a strong rationale for the work presented in the paper.", "summary": "This section analyzes existing chart-related datasets and reveals significant limitations. Early datasets lack visual diversity and complexity, while recent approaches using LLMs for Q&A generation still fall short in terms of quality and scalability. Manual annotation is expensive and not scalable. Overall, current benchmarks are inadequate for training models to excel in advanced visual reasoning tasks."}}, {"page_end_idx": 5, "page_start_idx": 3, "section_number": 2, "section_title": "Deficiencies in Existing Chart-Related Datasets", "details": {"details": "Existing chart-related datasets suffer from limitations in both chart properties and Q&A properties, impacting their suitability for training advanced models.  Regarding chart properties, many datasets lack visual diversity, relying on uniform chart styles and limited topics.  Datasets like ChartQA and OpenCQA, sourced from limited websites, exemplify this issue.  While some newer datasets, such as ChartAst, attempt to address this by synthesizing charts with randomized attributes, they often neglect the complexity of visual information that more advanced models should address.  The textual format of chart data also significantly impacts data synthesis and scalability. Simple style changes no longer pose challenges as models evolve, thus requiring more complex designs to assess reasoning.  Concerning Q&A properties, numerous datasets utilize predefined templates or manual annotation to generate Q&A pairs, limiting diversity and increasing cost and time. Datasets like PlotQA and ChartBench show this limitation, and even datasets using LLMs for generation may still be suboptimal and less efficient, as seen in datasets like ChartLlama and ChartInstruct.  In summary, the lack of scalability and the limited diversity and complexity of chart data and Q&A pairs hinder the ability of existing datasets to adequately evaluate and train truly advanced models, particularly those that may require strong visual reasoning abilities.", "first_cons": "Many existing datasets lack visual diversity and complexity in chart properties, limiting their effectiveness for training advanced models.", "first_pros": "The analysis highlights the need for more advanced datasets with greater visual complexity and diversity to properly evaluate and train visual reasoning models.", "keypoints": ["Many existing chart datasets lack visual diversity and complexity (e.g., ChartQA and OpenCQA).", "Recent works like ChartAst synthesize charts with randomized attributes, but often ignore visual complexity.", "Datasets using predefined templates or manual annotation limit diversity and scalability (e.g., PlotQA, ChartBench).", "Even LLM-based Q&A generation may be suboptimal, highlighting the need for more advanced approaches."], "second_cons": "Current Q&A generation methods often lack scalability and fail to capture the fine-grained visual elements necessary for advanced visual reasoning.", "second_pros": "The review provides a comprehensive overview of existing datasets, emphasizing the limitations and highlighting the need for improvements in both chart and Q&A properties to advance visual reasoning in models.", "summary": "Existing chart-related datasets have significant shortcomings in both chart properties (lack of visual diversity and complexity) and Q&A properties (limited scalability and diversity). These deficiencies hinder the development and evaluation of advanced multimodal models capable of robust visual reasoning, creating a need for innovative, more scalable datasets with higher quality and diversity."}}, {"page_end_idx": 5, "page_start_idx": 4, "section_number": 2, "section_title": "Can LLMs Understand Charts Without Visual Input?", "details": {"details": "This section investigates whether LLMs can effectively understand chart information presented solely as textual code, without access to the visual chart itself.  An experiment was conducted using 75 complex charts with three input types: data tables, code, and chart images, all fed to GPT-40 to generate Q&A pairs. Human annotators assessed the accuracy, reasoning complexity, and visual reference score for each input type.  The results show that code outperforms tables in accuracy (2.60 vs. 2.72) and reasoning complexity (2.56 vs. 2.51), while the chart images scored highest on visual reference (2.36), indicating the potential for code as an effective intermediary representation of visual charts for LLMs.  The experiment highlights the surprising ability of LLMs to extract visual information from code alone, achieving a relatively high visual reference score (2.15) without direct image input.  This suggests that code, by precisely encoding chart structures and details, may serve as a better modality for LLMs to interpret cross-modal information compared to using only data tables.", "first_cons": "The experiment's relatively small sample size (75 chart pairs) might limit the generalizability of its findings. More extensive testing with a larger and more diverse dataset would strengthen the conclusions.", "first_pros": "The experiment directly addresses a critical question in multimodal learning: can LLMs effectively use code to represent and understand chart information without visual input? The innovative approach uses a cost-effective method to measure the performance of LLMs on complex chart reasoning tasks.", "keypoints": ["Code outperforms data tables in accuracy (2.60 vs. 2.72) and reasoning complexity (2.56 vs. 2.51) when used as input for generating Q&A pairs by GPT-40.", "Chart images score highest on visual reference (2.36), but code achieves a surprisingly high visual reference score (2.15) without direct image input.", "The experiment involved 75 complex charts, providing valuable insights into LLMs' ability to interpret cross-modal information from code alone.", "The study suggests that code may be a more effective intermediary representation than data tables for translating visual chart representations into textual form for LLMs."], "second_cons": "The reliance on GPT-40 for Q&A generation and human annotators for scoring introduces potential biases that may affect the objectivity and replicability of the results. More objective evaluation methods should be considered.", "second_pros": "The findings offer a novel approach for cost-effective data synthesis for training MLLMs in visual reasoning tasks. The use of code as an intermediary representation opens up new possibilities for bridging the gap between visual and textual modalities in multimodal learning.", "summary": "This section explores the potential of using code as an intermediary representation for enabling LLMs to understand chart information without needing visual input. An experiment comparing code, data tables, and visual charts as input for a chart Q&A generation task showed that code performs comparably well to visual charts on measures of visual understanding and better than tables on measures of accuracy and reasoning complexity, suggesting that code might serve as an efficient and scalable way to train multimodal LLMs in visual reasoning."}}, {"page_end_idx": 7, "page_start_idx": 5, "section_number": 3, "section_title": "REACHQA: Synthesizing Chart Q&A with CIT", "details": {"details": "The REACHQA dataset is created using a novel method called Code-as-Intermediary Translation (CIT).  This method leverages the strengths of LLMs by using code as a bridge between visual chart representations and textual representations.  Starting with 33 seed codes from the Matplotlib gallery, the authors synthesize more chart-plotting codes using text-based techniques like Self-Instruct and Evol-Instruct. This generates a diverse set of charts covering various types and levels of complexity.  The generated codes are used to create the charts themselves.  The instruction set (Q&A pairs) is then generated using LLMs, which produce both recognition-oriented and reasoning-oriented questions and answers.  The resulting dataset, REACHQA, contains 3,249 reasoning-intensive charts and 19,963 Q&A pairs (8k recognition-oriented and 12k reasoning-oriented).  A manual validation step with multiple open-source MLLMs ensures data quality, resulting in a dataset that costs only \\$300 to create.  The authors fine-tune several models on REACHQA and show that they not only perform well on chart-related benchmarks but also demonstrate improved multimodal reasoning abilities on general mathematical benchmarks.", "first_cons": "The reliance on LLMs for data synthesis introduces the risk of generating inaccurate or illogical Q&A pairs, despite efforts to mitigate this through validation and filtering. The quality of the synthesized data is ultimately dependent on the capabilities of the LLMs used.", "first_pros": "The CIT method is cost-effective and scalable, enabling the creation of a large-scale dataset (3,249 charts and 19,963 Q&A pairs) at a significantly lower cost ($300) compared to manual annotation methods.", "keypoints": ["Code-as-Intermediary Translation (CIT) is a novel method that uses code to bridge visual and textual chart representations.", "REACHQA dataset contains 3,249 reasoning-intensive charts and 19,963 Q&A pairs (8,000 recognition-oriented and 12,000 reasoning-oriented).", "The dataset was created at a remarkably low cost of only $300 using LLMs.", "Fine-tuning models on REACHQA improves performance not only on chart-related benchmarks but also on general multimodal reasoning tasks, showing improved generalizability of the model."], "second_cons": "The success of the CIT method depends on the availability of high-quality seed codes and the ability of LLMs to generate diverse and accurate chart-plotting codes and Q&A pairs.  The process may not be fully reproducible if the seed codes or LLMs change.", "second_pros": "The dataset addresses two key challenges in chart understanding: recognition and reasoning.  The inclusion of both recognition-oriented and reasoning-oriented tasks makes it a more comprehensive and challenging benchmark.", "summary": "REACHQA is a novel multimodal chart question-answering dataset synthesized using Code-as-Intermediary Translation (CIT), a cost-effective method that leverages LLMs to translate visual chart representations into textual representations.  The dataset includes 3,249 reasoning-intensive charts and 19,963 Q&A pairs (8k for recognition and 12k for reasoning), created at a cost of just $300.  Fine-tuning models on REACHQA shows improved performance on chart-related benchmarks and general multimodal reasoning tasks."}}, {"page_end_idx": 7, "page_start_idx": 5, "section_number": 3, "section_title": "Intermediary Code Synthesis", "details": {"details": "This section details the synthesis of intermediary code, a crucial step in the Code-as-Intermediary Translation (CIT) method. It begins by collecting 33 seed codes from the Matplotlib gallery, ensuring code quality and diversity.  To expand this initial set, a two-stage process leveraging LLMs is employed. First, the Self-Instruct method is used, prompting the model to generate new code based on the seed codes, resulting in a set of simpler codes (Ceasy).  Then, Evol-Instruct is applied to evolve these simpler codes into more complex and intricate versions (Chard) by introducing four predefined evolutionary strategies: expanding data, modifying visual elements, overlaying a different chart type, or adding subplots.  The generated codes are validated for execution errors, employing self-repair mechanisms to correct faulty code, resulting in a final set of functional and diverse intermediary codes.", "first_cons": "The reliance on LLMs for code generation introduces the potential for inaccuracies and inconsistencies in the generated code, requiring a validation and self-repair process.", "first_pros": "The use of a two-stage process (Self-Instruct followed by Evol-Instruct) allows for the generation of both diverse and complex chart codes, enriching the final dataset.", "keypoints": ["Starts with 33 seed codes from the Matplotlib gallery.", "Employs Self-Instruct to generate diverse simpler codes (Ceasy).", "Uses Evol-Instruct to evolve simpler codes into complex versions (Chard) via four strategies.", "Incorporates a self-repair mechanism to handle faulty code, fixing approximately 15% of the generated code."], "second_cons": "The self-repair mechanism, while effective, may not catch all errors, potentially leaving some faulty codes in the final dataset.", "second_pros": "The method is cost-effective and easily scalable, as it leverages the generative capabilities of LLMs to synthesize a large volume of diverse codes.", "summary": "This section describes the synthesis of intermediary code using a two-stage LLM-based approach.  It starts with 33 seed codes, expands them using Self-Instruct to create simpler codes (Ceasy), and then further evolves these codes into more complex versions (Chard) using Evol-Instruct and four predefined evolutionary strategies.  A self-repair mechanism addresses code errors, resulting in a diverse dataset of functional chart-plotting codes.  This approach is both cost-effective and scalable."}}, {"page_end_idx": 7, "page_start_idx": 6, "section_number": 3, "section_title": "Bi-directional Translation", "details": {"details": "This section details the bi-directional construction of charts and instruction sets within the REACHQA dataset.  It begins by describing the process of chart generation through code execution, where Python code is executed to create charts.  A self-repair mechanism is implemented to address potential code errors, fixing approximately 15% of faulty code generated by GPT-40.  Instruction generation is then handled via guided prompting, using LLMs to synthesize a batch of questions and answers.  The method emphasizes creating two types of instructions: recognition-oriented and reasoning-oriented.  The process employs a step-by-step approach for generating answers to ensure accuracy and detail.  The final step involves quality assurance through a multimodal validation process, which uses multiple open-source MLLMs to verify both generated charts and instructions, and a manual review process to further ensure high-quality data.  This meticulous process results in a multimodal instruction dataset with a total of 3,249 reasoning-intensive charts and 19,963 question-answer pairs.", "first_cons": "The reliance on LLMs for both code generation and instruction generation introduces potential biases and limitations inherent in these models.  The quality of generated data is heavily dependent on the capabilities of the LLMs used, which may not always be perfect.", "first_pros": "The bi-directional approach ensures alignment between the chart and instruction sets, leading to a more coherent and effective dataset.  This is particularly beneficial for training multimodal models that require both accurate chart representations and meaningful associated questions.", "keypoints": ["The process starts with executing 3,000 code samples to create charts, which are then refined through self-repair for code errors, resulting in 3,249 charts.", "Guided prompting uses LLMs to generate 19,963 question-answer pairs, categorized into 8,000 recognition-oriented and 12,000 reasoning-oriented tasks.", "Multimodal validation using multiple open-source MLLMs and a manual review process ensures high-quality data, with the self-repair mechanism fixing roughly 15% of faulty code.", "The bi-directional generation method ensures alignment between chart and instructions."], "second_cons": "The approach is computationally expensive, involving multiple iterations of code execution, LLM prompting, and validation, and thus, it might not be scalable for extremely large datasets.", "second_pros": "The use of both recognition and reasoning oriented instruction allows for a more comprehensive and effective approach to evaluating and training models capable of handling complex chart related tasks.   The method's reliance on code as an intermediary allows the generation of visually complex charts, addressing limitations in prior work.", "summary": "This section describes a bi-directional process for creating a multimodal chart question-answering dataset.  Charts are generated using Python code, with a self-repair mechanism to correct errors, and instructions are generated via guided prompting with LLMs.  The dataset is rigorously validated using multiple models and manual review to ensure high quality, resulting in 3,249 charts and 19,963 question-answer pairs, split between recognition-oriented and reasoning-oriented tasks."}}, {"page_end_idx": 7, "page_start_idx": 7, "section_number": 3, "section_title": "Multimodal Validation for Enhanced Data Quality", "details": {"details": "This section details a multimodal validation step integrated to enhance the quality of the LLM-generated REACHQA dataset.  This step uses multiple open-source models (MiniCPM-V2.5-Llama3, InternVL2-8B, and Qwen2-VL-8B) to verify both the generated charts and their corresponding instructions.  A \"majority voting\" strategy is employed, ensuring reliable visual validation while remaining cost-effective. Charts are scored (1-5) based on defined criteria, and those falling below a threshold are filtered out.  Instructions (Q&A pairs and associated charts) are also evaluated; samples receiving multiple negative votes are discarded.  A similar process is followed for the test set, with stricter filtering criteria and manual review and refinement by annotators, leading to a strong inter-annotator agreement (kappa coefficient of 0.82).  The total cost of data construction, excluding open-source model usage and annotation labor for the testing set, was about $300.", "first_cons": "The reliance on multiple open-source models for validation introduces variability and potential inconsistencies in the evaluation process.  Different models may focus on varying aspects of the images or instruction quality, leading to subjective scoring and potential bias.", "first_pros": "The multimodal validation approach offers a robust and cost-effective method for ensuring data quality.  By incorporating multiple models, the method mitigates potential biases and ensures comprehensive verification of both visual and textual components of the generated data.", "keypoints": ["Multimodal validation uses multiple open-source models (MiniCPM-V2.5-Llama3, InternVL2-8B, and Qwen2-VL-8B) for robustness.", "A \"majority voting\" strategy enhances reliability while controlling costs.", "Charts are scored 1-5 based on defined criteria, with a threshold for filtering.", "Instructions (Q&A pairs and charts) are also verified, with samples receiving multiple negative votes discarded.", "Test set construction employs stricter filtering and manual annotation with 0.82 kappa inter-annotator agreement.", "Total cost for data construction (excluding open-source models and annotation labor) was approximately $300."], "second_cons": "While the approach aims for cost-effectiveness, manual annotation for the testing set still introduces a human element, potentially impacting scalability and overall cost reduction.", "second_pros": "The rigorous validation process results in a high-quality dataset, improving model performance on various benchmarks and ensuring the generalizability of learned multimodal reasoning abilities beyond chart-specific tasks.", "summary": "To enhance data quality, a multimodal validation step was implemented in the creation of the REACHQA dataset. This involved using multiple open-source LLMs to evaluate both the generated charts and instructions using a majority voting strategy. Charts and instructions were given scores between 1 and 5, based on predefined criteria, and those below a threshold were removed.  A similar, more rigorous process was used for the test set, supplemented by manual annotation, achieving a high inter-annotator agreement (kappa = 0.82). The total cost, excluding open-source model usage and annotation labor for the test set, was approximately $300."}}, {"page_end_idx": 8, "page_start_idx": 7, "section_number": 4, "section_title": "EXPERIMENTS", "details": {"details": "This section details the experimental setup and results of evaluating various multimodal large language models (MLLMs) on chart-related tasks.  Three categories of benchmarks were used: traditional chart-related benchmarks focusing on recognition (ChartQA, ChartBench, ChartX); novel chart-related benchmarks assessing both recognition and reasoning (CharXiv, REACHQA); and general multimodal reasoning benchmarks (MathVista, MATH-Vision).  A range of models were evaluated, including proprietary models (GPT-40, GPT-40 mini, Claude 3.5 Sonnet), chart-augmented open-source models (ChartInstruct-7B, ChartAssistant-13B, ChartGemma-3B), and general open-source models (LLaVA-Next-Llama3-8B, MiniCPM-V2.5-Llama3-8B, InternVL2-8B).  Supervised fine-tuning using the REACHQA dataset was performed on the open-source models, with variations in training data focusing on recognition or reasoning aspects or both. The results show that fine-tuning significantly improves performance on chart-related tasks, and the benefits generalize to general mathematical reasoning tasks. Proprietary models show more balanced performance across recognition and reasoning tasks compared to open-source models.", "first_cons": "The experiments primarily focus on evaluating the performance of models on specific benchmarks, lacking a broader assessment of the models' capabilities in real-world applications.", "first_pros": "The study utilizes a diverse set of benchmarks, encompassing traditional and novel chart-related tasks as well as general multimodal reasoning benchmarks.  This comprehensive approach provides a more thorough evaluation of the models' capabilities.", "keypoints": ["Three categories of benchmarks were used: traditional chart-related (recognition), novel chart-related (recognition and reasoning), and general multimodal reasoning.", "Proprietary models demonstrated more balanced performance across recognition and reasoning tasks compared to open-source models.", "Fine-tuning with the REACHQA dataset significantly improved model performance across various benchmarks, with gains generalizing beyond chart-specific tasks.", "LLaVa-Next-Llama3-8B model improved by 34.8% on average after being fine-tuned with REACHQA data.", "The study's results demonstrate the importance of combining recognition and reasoning aspects of training data for optimal model performance."], "second_cons": "The study does not directly address the cost-effectiveness of the proposed approach compared to other data synthesis methods.", "second_pros": "The use of supervised fine-tuning and readily available open-source models makes the experimental approach relatively accessible and reproducible for other researchers.", "summary": "This experiment evaluates various MLLMs on chart-related and general multimodal reasoning tasks, using a variety of benchmarks.  Fine-tuning on the REACHQA dataset substantially improves performance, particularly for open-source models, highlighting the effectiveness of the data synthesis approach. However, proprietary models show more consistent performance across tasks.  The findings stress the importance of a balanced approach to training data that includes both recognition and reasoning components for optimal results in complex tasks."}}, {"page_end_idx": 8, "page_start_idx": 7, "section_number": 4, "section_title": "Experimental Setups", "details": {"details": "The experimental setup section details the evaluation of various multimodal large language models (MLLMs) on chart-related tasks. Three categories of benchmarks were used: traditional chart-related benchmarks focusing on recognition (ChartQA, ChartBench, ChartX); novel chart-related benchmarks assessing both recognition and reasoning (CharXiv, REACHQA); and general multimodal reasoning benchmarks (MathVista, MATH-Vision).  For each general open-source model, supervised fine-tuning (SFT) was conducted using the REACHQA training set, with three versions fine-tuned: using only recognition-oriented Q&A pairs; using only reasoning-oriented Q&A pairs; and using both.  A text-only baseline was included, where GPT-40 attempted to answer without visual input.  The LLM-as-a-judge method, using GPT-40, was used to assess accuracy.  The study also included an investigation into the impact of different training data ratios (recognition vs. reasoning) on model performance and the generalizability of models trained on chart-focused datasets to broader multimodal reasoning tasks.", "first_cons": "The study primarily relies on the LLM-as-a-judge method, which introduces subjective bias. This could skew the results and make them less reliable than those produced by human evaluators. In addition, there may be an over-reliance on existing, limited datasets.", "first_pros": "The experimental setup is comprehensive, employing three categories of benchmarks to fully evaluate model capabilities. The use of three fine-tuning versions allows for granular analysis of model performance. The inclusion of a text-only baseline helps gauge the impact of visual information.", "keypoints": ["Three categories of benchmarks were used to evaluate models: traditional chart-related benchmarks, novel chart-related benchmarks, and general multimodal reasoning benchmarks.", "Supervised fine-tuning (SFT) was conducted using the REACHQA training set in three versions: using recognition, reasoning, and a combination of both data.", "GPT-40 was employed as the judge model using the LLM-as-a-judge method.", "An investigation was conducted into the impact of the ratio of recognition vs. reasoning training data on model performance.", "Results demonstrated that models trained with REACHQA data showed improved performance not only on chart-related benchmarks but also on general mathematical benchmarks, suggesting improved multimodal reasoning abilities."], "second_cons": "The study does not explicitly address the limitations of using SFT, such as potential overfitting or bias introduced during training.  Furthermore, the focus on LLMs may limit the generalizability of the findings to other types of multimodal models.", "second_pros": "The methodology provides a thorough evaluation of model capabilities on various types of chart-related tasks, which are increasingly relevant in real-world applications.  The investigation of training data ratios offers valuable insights into the impact of dataset composition on model performance.", "summary": "This section outlines a comprehensive experimental setup for evaluating multimodal large language models (MLLMs) on chart-related tasks.  The evaluation encompasses three benchmark categories,  incorporates supervised fine-tuning with varying data compositions (recognition and reasoning), employs an LLM-as-judge approach, and investigates the impact of training data ratio and dataset diversity on model performance across both specialized chart reasoning and generalized multimodal reasoning tasks."}}, {"page_end_idx": 8, "page_start_idx": 8, "section_number": 4, "section_title": "Experimental Results", "details": {"details": "The experimental results section evaluates various multimodal large language models (MLLMs) across three categories of tasks: traditional chart-related benchmarks (ChartQA, ChartBench, ChartX), novel chart-related benchmarks assessing both recognition and reasoning (CharXiv, REACHQA), and general multimodal reasoning benchmarks (MathVista, MATH-Vision).  The models are categorized into three groups: proprietary models (GPT-4, GPT-4 mini, Claude 3.5 Sonnet), chart-augmented open-source models (ChartInstruct-7B, ChartAssistant-13B, ChartGemma-3B), and general open-source models (LLaVA-Next-Llama3-8B, MiniCPM-V2.5-Llama3-8B, InternVL2-8B).  A text-only baseline (Random GPT-4) is also included. Supervised fine-tuning using the REACHQA training set was performed on the open-source models, with variations using recognition-oriented, reasoning-oriented, and combined datasets.  The results indicate that proprietary models generally show more balanced performance across all tasks.  Fine-tuning on REACHQA, especially with a combination of recognition and reasoning data, substantially improved open-source models, with LLaVA-Next-Llama3-8B showing a remarkable 34.8% performance boost on average.  This improvement generalizes beyond chart-specific tasks to broader multimodal reasoning tasks. The analysis also highlights the effectiveness of REACHQA in evaluating both recognition and reasoning abilities, and the interplay between these two abilities in overall model performance.", "first_cons": "The study relies heavily on a single, newly created dataset (REACHQA) for training and evaluation.  The generalizability of the findings may therefore be limited, and results might be influenced by characteristics specific to this dataset rather than reflecting broader MLLM capabilities.", "first_pros": "The comprehensive evaluation across various benchmarks and model categories provides a robust assessment of MLLM performance in chart understanding and reasoning. The inclusion of both traditional and novel benchmarks, along with general multimodal reasoning tasks, allows for a thorough analysis of model strengths and weaknesses.", "keypoints": ["Proprietary models (GPT-4, etc.) demonstrate more balanced performance across tasks compared to open-source models.", "Fine-tuning open-source models on REACHQA significantly boosts performance, particularly when using a combined dataset of recognition and reasoning tasks (improvement of 34.8% for LLaVA-Next-Llama3-8B).", "Performance gains generalize to broader multimodal reasoning tasks beyond chart-specific benchmarks.", "REACHQA proves effective at evaluating both recognition and reasoning abilities, showing the interdependency of these skills."], "second_cons": "The experimental setup focuses primarily on quantitative results, neglecting qualitative analysis of model outputs or the reasoning process. A deeper dive into the \u2018why\u2019 behind the model's performance, potentially through qualitative analysis or attention visualization, would strengthen the findings.", "second_pros": "The experiment systematically investigates the effects of different training data types (recognition-oriented vs. reasoning-oriented vs. combined) on model performance, offering valuable insights into data characteristics and their impact on model capabilities.", "summary": "The experimental results demonstrate that proprietary multimodal large language models generally exhibit more balanced performance across various chart understanding and reasoning tasks compared to open-source models.  Fine-tuning open-source models on a newly synthesized dataset, REACHQA, significantly improves their capabilities, particularly when using a combined dataset of recognition and reasoning-oriented questions. These improvements generalize beyond chart-specific tasks to broader multimodal reasoning, highlighting the effectiveness of the dataset and revealing a strong interplay between recognition and reasoning abilities in successful model performance."}}, {"page_end_idx": 10, "page_start_idx": 8, "section_number": 5, "section_title": "DISCUSSION", "details": {"details": "This section delves into the impact of expert rationales on reasoning abilities, exploring the interplay between recognition and reasoning skills, and investigating the balance between specialized and general visual understanding.  An experiment is conducted comparing three open-source datasets (ChartBench, ChartAst, and ChartGemma) to train the LLaVA-Next-Llama3-8B model. The results indicate that the model trained on ChartGemma and REACHQA demonstrated superior performance, highlighting the significance of high-quality expert rationales in enhancing visual reasoning. Further investigation into the balance between recognition and reasoning reveals a complex interaction.  Models trained with a high ratio of recognition-focused data performed exceptionally well on recognition tasks, while those trained on a high ratio of reasoning-focused data yielded diminishing returns, highlighting the dependence of reasoning on solid recognition.  Experiments comparing specialized and general training data reveal that models trained on a mixture of specialized and general-purpose data achieved the best overall performance, demonstrating the transferability of multimodal reasoning abilities.", "first_cons": "The experiments are limited in scope, only training one specific model and using only three datasets.  This limits the generalizability of the findings.", "first_pros": "The section presents a well-designed experiment comparing different datasets to determine the influence of expert rationales on model performance. This provides valuable insights into data quality and its impact.", "keypoints": ["Models trained on ChartGemma and REACHQA significantly outperformed those trained on ChartBench, showcasing the benefits of high-quality expert rationales in improving reasoning performance.", "A strong interaction exists between recognition and reasoning abilities.  A high ratio of reasoning data didn't necessarily lead to superior reasoning results, potentially because of the dependence of reasoning on accurate recognition.", "Combining specialized (REACHQA) and general-purpose multimodal training data resulted in better overall model performance than relying on specialized data alone, demonstrating the transferability of learned abilities."], "second_cons": "The analysis of the attention patterns is limited to a single model and lacks a broader comparison, limiting the generalization of the interpretability insights.", "second_pros": "The study clearly shows how the mixture of specialized and general-purpose training data improves the overall model performance. This provides useful information for building future multimodal datasets.", "summary": "This section investigates the impact of expert rationales and training data composition on multimodal reasoning abilities.  Experiments using different datasets highlight the importance of high-quality rationales and a balanced approach to combining specialized and general training data for achieving optimal performance in both recognition and reasoning tasks.  Analysis of attention mechanisms reveals that effective models integrate recognition and reasoning synergistically."}}, {"page_end_idx": 10, "page_start_idx": 8, "section_number": 5, "section_title": "Distilling Expert Rationales Improves Reasoning", "details": {"details": "This section investigates how incorporating expert rationales into model training affects reasoning abilities.  The authors conduct an experiment comparing three open-source training datasets: ChartBench, ChartAst, and ChartGemma, focusing on their impact on the LLaVA-Next-Llama3-8B model.  ChartBench, lacking detailed reasoning steps, yielded the worst performance. ChartAst, while providing rationales, suffered from simplistic questions limiting its effectiveness.  In contrast, ChartGemma and REACHQA, incorporating higher-quality rationales from stronger models, demonstrated superior performance.  The results highlight the significant impact of high-quality expert rationales on improving model reasoning capabilities, particularly when compared to datasets lacking detailed explanations or relying on simplified question formats.", "first_cons": "The study focuses only on open-source models, limiting the generalizability of the findings to proprietary models which often demonstrate superior performance.", "first_pros": "The experiment directly demonstrates the significant impact of expert rationales on model reasoning abilities.  Models trained on datasets with high-quality rationales (like ChartGemma and REACHQA) significantly outperformed those trained on datasets with less detailed explanations (like ChartBench).", "keypoints": ["The experiment shows a clear performance difference among models trained on datasets with varying levels of detailed rationales.  Models trained on ChartBench (lacking reasoning steps) performed worst; those trained on ChartGemma and REACHQA (with high-quality expert rationales) performed best.", "The study uses the LLaVA-Next-Llama3-8B model for all experiments, ensuring consistency and reducing the influence of model architecture differences.", "The comparison of ChartBench, ChartAst, and ChartGemma highlights the importance of rationale quality and question complexity in model training.", "20,000 instructions are uniformly sampled from each dataset for training to ensure fair comparison and reduce bias"], "second_cons": "The study uses only one model architecture which may impact the broader generalizability of the results. Further investigation is needed to test the effectiveness on other model architectures.", "second_pros": "The experimental design uses a controlled setting, employing consistent model architectures and training procedures to reduce confounding factors and enhance the reliability of the results.", "summary": "This section explores the impact of incorporating expert rationales into training data on multimodal reasoning abilities, demonstrating that models trained on datasets with high-quality, detailed rationales significantly outperform those trained on datasets with less comprehensive explanations. This highlights the importance of providing reasoning steps for improved performance in complex reasoning tasks."}}, {"page_end_idx": 10, "page_start_idx": 9, "section_number": 5, "section_title": "Interaction Between Recognition and Reasoning Abilities", "details": {"details": "This section investigates the interplay between recognition and reasoning abilities in visual reasoning tasks.  The authors hypothesize that strong recognition skills are prerequisites for effective reasoning. To test this, they conduct an experiment where they train a model (LLaVA-Next-Llama3-8B) on various ratios of recognition-focused and reasoning-focused data, keeping the total training data size constant at 8,000 samples.  The results show that increasing the proportion of recognition data leads to improved performance on recognition-focused tasks but diminishing returns in reasoning tasks, indicating that reasoning abilities are partly dependent on recognition skills. Conversely, prioritizing reasoning data yields significant gains in reasoning tasks but may harm recognition performance. They find that a balanced training approach, utilizing both types of data, leads to the most optimal results.  The authors suggest that the interaction between recognition and reasoning is complex and highlights the need for a balanced approach in training data for visual reasoning models to maximize overall performance.", "first_cons": "The experiment only involves a single model (LLaVA-Next-Llama3-8B) and a fixed total training data size (8,000 samples).  The results might not generalize to other models or larger datasets.  Further research with more models and various dataset sizes is required to validate these findings.", "first_pros": "The experiment directly addresses the crucial relationship between recognition and reasoning in visual reasoning. It provides empirical evidence supporting the claim that strong recognition is vital for effective reasoning and a balanced training approach is key to better performance.", "keypoints": ["Recognition skills are prerequisites for effective reasoning. ", "A balanced approach (mixing recognition and reasoning data) leads to optimal performance.", "Prioritizing reasoning data over recognition data can hurt recognition task performance.", "Increasing recognition data shows diminishing returns on reasoning tasks."], "second_cons": "The study focuses solely on the quantitative performance aspect. It lacks a deeper qualitative analysis of the models' reasoning processes and the underlying mechanisms involved in the interaction between recognition and reasoning.", "second_pros": "The findings highlight the importance of a balanced approach in training data, suggesting that a balanced strategy leads to significantly better performance. The results offer valuable insights into the optimal training approach for developing more robust and effective visual reasoning models.", "summary": "This section explores the intricate relationship between recognition and reasoning abilities in visual reasoning.  By varying the ratio of recognition-focused to reasoning-focused data in a fixed-size training set, the authors demonstrate that strong recognition is necessary for robust reasoning.  Optimal performance is achieved through a balanced approach in the training data, highlighting the complex interplay between these two abilities."}}, {"page_end_idx": 10, "page_start_idx": 10, "section_number": 5, "section_title": "Balancing General Visual Understanding and Specialized Abilities", "details": {"details": "This section investigates the performance of models trained on specialized chart datasets (like REACHQA) when applied to general-purpose multimodal tasks.  The experiment uses six general multimodal benchmarks and four specialized chart-related benchmarks. Three versions of the LLaVA-Next-Llama3-8B model were tested: a vanilla model, one trained on 20k REACHQA samples, and one trained on a mix of 20k REACHQA samples and 20k general-purpose multimodal data. The results show that while training solely on specialized data improves performance on chart tasks, it can hurt performance on general tasks. However, combining specialized and general data leads to better results on both specialized and general tasks, demonstrating a balance between specialized and generalized abilities.  The findings highlight the interaction between specialized and general training data and the importance of a balanced training strategy for achieving strong multimodal reasoning abilities.", "first_cons": "Training solely on specialized data, while boosting performance on those specific tasks, can negatively impact performance on more general multimodal tasks. This suggests an overfitting to the specialized domain, hindering the model's ability to generalize its knowledge.", "first_pros": "The combined training approach (using both specialized and general multimodal data) yields superior performance across both specialized chart-related and general multimodal reasoning tasks. This indicates that the model has achieved a good balance between specialized expertise and general understanding, leading to stronger overall capabilities.", "keypoints": ["Combining 20k REACHQA data with 20k general multimodal data significantly improves the model's performance on both specialized and general tasks.", "Training solely on specialized data (REACHQA) leads to improved specialized performance but a decrease in performance on general multimodal tasks.", "The vanilla model performs poorly on reasoning intensive tasks."], "second_cons": "The study is limited in scope, using only one base model (LLaVA-Next-Llama3-8B) and a fixed size of training data (20k for each training condition). More extensive evaluation with various models and training data sizes is needed to confirm the generalizability of these findings.", "second_pros": "The study provides valuable insights into the interaction between specialized and general training data for multimodal reasoning.  The results demonstrate the importance of a balanced training strategy for achieving strong performance across different types of tasks.", "summary": "This section explores the balance between training large language models (LLMs) on specialized chart-reasoning datasets and general-purpose multimodal datasets.  Experiments using three variations of the LLaVA-Next-Llama3-8B model (vanilla, REACHQA only, and REACHQA + general data) on various benchmarks demonstrate that while specialized training enhances chart-related reasoning, it negatively impacts general multimodal performance. A balanced approach, incorporating both types of datasets, leads to superior performance in both specialized and general tasks."}}]