[{"page_end_idx": 1, "page_start_idx": 1, "section_number": 1, "section_title": "INTRODUCTION", "details": {"details": "Multimodal large language models (MLLMs) have made significant advancements in visual recognition tasks, but struggle with complex chart understanding and reasoning-intensive questions.  The paper analyzes the error distribution in ChartQA, revealing that 62% of errors stem from misrecognition and 36% from reasoning mistakes. This highlights the need for improved multimodal reasoning abilities.  One promising strategy is to distill the rationales of reasoning from experts.  However, creating high-quality training data for chart-related tasks is costly and time-consuming, presenting challenges in both constructing chart data and creating relevant question-answer pairs.  The existing approaches for automating data synthesis often overlook critical visual features or rely on data tables, resulting in suboptimal Q&A pairs.", "first_cons": "Creating high-quality training data for chart-related tasks is costly and time-consuming.", "first_pros": "Multimodal large language models (MLLMs) have made significant achievements in visual recognition tasks.", "keypoints": ["62% of errors in ChartQA stem from misrecognition.", "36% of errors in ChartQA stem from reasoning mistakes.", "Existing benchmarks underscore the need for more advanced and generalized visual reasoning abilities."], "second_cons": "Existing methods for automating Q&A synthesis often overlook critical visual features or rely on data tables, resulting in suboptimal Q&A pairs.", "second_pros": "Distilling the rationales of reasoning from experts is a promising strategy to improve visual reasoning performance.", "summary": "The introduction highlights the challenges of complex chart understanding for multimodal large language models (MLLMs), emphasizing the high error rates stemming from both misrecognition (62%) and reasoning mistakes (36%) in existing benchmarks like ChartQA.  It introduces the problem of creating high-quality training data for chart-related tasks due to the cost and time involved in data collection and annotation. The section positions the distillation of reasoning rationales from experts as a potential solution but acknowledges the difficulties in existing data synthesis methods."}}, {"page_end_idx": 3, "page_start_idx": 2, "section_number": 2, "section_title": "BACKGROUND", "details": {"details": "Existing chart-related datasets have limitations in terms of visual diversity, Q&A quality, and scalability.  Early datasets like ChartQA and OpenCQA, sourced from limited websites, featured uniform styles with minimal diversity.  Recent works like ChartAst synthesize charts with randomized attributes (e.g., color, fonts) using LLMs, but these superficial variations in chart appearance don't address the complexity needed for advanced visual reasoning. Datasets like CharXiv and MMC, including complex scientific charts, show greater complexity but lack scalability. Many datasets use predefined templates or manual annotation, resulting in monotonous questions or high costs.  Some leverage LLMs for Q&A generation, but they overlook critical visual features or struggle to produce accurate and challenging chart-related Q&A pairs. Textual chart formats are advantageous for processing via LLMs; however, those lack visual details, and LLMs perform less well when dealing with images directly.", "first_cons": "Most existing datasets lack scalability, hindering the training of large models.", "first_pros": "The textual format of charts is advantageous for processing via LLMs, allowing for scalability.", "keypoints": ["Early datasets (ChartQA, OpenCQA) lack visual diversity and are limited in size.", "Recent datasets (ChartAst) use LLMs for synthesis but lack visual complexity.", "Datasets with complex charts (CharXiv, MMC) are not scalable.", "Manual annotation is costly and limits scalability; predefined templates produce monotonous questions.", "LLMs are effective at processing textual representations of charts, but less so with images."], "second_cons": "Many datasets rely on predefined templates or manual annotation, limiting the quality and diversity of questions and answers.", "second_pros": "Datasets like CharXiv and MMC include visually complex charts more representative of real-world data.", "summary": "Existing chart-related datasets suffer from limitations in visual diversity, Q&A quality, and scalability. Early datasets lacked diversity; recent methods using LLMs for synthesis did not adequately address visual complexity.  Manually annotated datasets are expensive, while template-based approaches generate monotonous questions. LLMs are more effective with textual chart formats, highlighting the need for a more efficient and scalable approach to data synthesis."}}, {"page_end_idx": 5, "page_start_idx": 4, "section_number": 3, "section_title": "REACHQA: SYNTHESIZING CHART Q&A WITH CIT", "details": {"details": "The REACHQA dataset is created using a novel method called Code-as-Intermediary Translation (CIT). CIT leverages the power of LLMs to generate chart-plotting code, which serves as an intermediary representation between visual chart data and textual descriptions. This approach offers several advantages, including cost-effectiveness, efficiency, and scalability.  Starting with 33 seed codes from the Matplotlib gallery, the authors employ text-based synthesizing techniques, specifically Self-Instruct and Evol-Instruct, to generate a diverse range of chart-plotting codes. These codes are then used to create 3,249 charts spanning various types and complexities.  To ensure data quality, a bi-directional translation process generates corresponding Q&A pairs, with 8,000 questions focused on visual recognition and 12,000 on reasoning.  A further 500 charts and 2,000 manually verified Q&A pairs are added for independent assessment. The entire process, including code generation, chart creation, and Q&A synthesis, is managed by LLMs, resulting in a cost of only \\$300.  This innovative approach offers a low-cost, scalable method for generating high-quality multimodal datasets for training and evaluating visual reasoning abilities in MLLMs.", "first_cons": "The reliance on LLMs for all aspects of dataset generation introduces potential biases and limitations inherent to the models used. The quality of the synthetic data might be influenced by limitations in the LLMs' capabilities, and the overall quality needs to be carefully assessed and potentially refined through additional manual verification or filtering processes. Moreover, the dataset is generated synthetically, which might not entirely reflect real-world scenarios. The generated charts may not present the same level of variability in style and content, especially if the underlying LLMs have been primarily trained on a limited range of sources.", "first_pros": "The use of Code-as-Intermediary Translation (CIT) offers a cost-effective and highly scalable method for generating large multimodal datasets. The entire process is driven by LLMs, leading to significant cost savings compared to traditional manual annotation methods. This approach is particularly beneficial for creating datasets with complex charts that typically require substantial manual effort and expense, potentially opening the door for creation of much larger datasets in the future.", "keypoints": ["Code-as-Intermediary Translation (CIT) is used to synthesize the dataset, enabling cost-effective generation of complex chart-plotting code.", "33 seed codes from the Matplotlib gallery were used as a starting point to generate a diverse range of chart-plotting code.", "Self-Instruct and Evol-Instruct methods were employed to expand the diversity and complexity of the generated codes.", "REACHQA contains 3,249 reasoning-intensive charts and 19,963 Q&A pairs, with 8,000 questions focused on visual recognition and 12,000 on reasoning.", "The total cost for creating the dataset was only $300, highlighting the efficiency and scalability of the approach."], "second_cons": "While the methodology aims for diversity, the generated charts may not capture the full spectrum of real-world chart complexities, styles, and visual elements. The reliance on Matplotlib as the underlying chart generation library might also constrain the visual representation, particularly in terms of styling and layout.", "second_pros": "The dataset includes 3,249 reasoning-intensive charts and 19,963 Q&A pairs, providing a substantial amount of data for training and evaluating multimodal LLMs.  The inclusion of both recognition-oriented and reasoning-oriented questions ensures the dataset addresses two key aspects of chart understanding. The creation of a separate test set with manually verified Q&A pairs enables a more robust and reliable evaluation of model performance. The public availability of the code and data supports reproducibility and facilitates further research and development in the field of multimodal chart understanding.", "summary": "REACHQA is a newly created multimodal chart question answering dataset synthesized using a novel Code-as-Intermediary Translation (CIT) method.  This approach leverages LLMs to generate chart-plotting code, which acts as a bridge between visual and textual representations, enabling the cost-effective and scalable creation of 3,249 complex charts with 19,963 corresponding Q&A pairs. The dataset focuses on enhancing both recognition and reasoning abilities, and its creation cost was only $300. "}}, {"page_end_idx": 7, "page_start_idx": 6, "section_number": 4, "section_title": "EXPERIMENTS", "details": {"details": "The experiments section evaluates various multimodal large language models (MLLMs) on three categories of tasks: traditional chart-related benchmarks focusing on recognition, novel chart-related benchmarks assessing both recognition and reasoning, and general multimodal reasoning benchmarks.  Traditional chart benchmarks include ChartQA, ChartBench, and ChartX; novel benchmarks include CharXiv and REACHQA (a new dataset introduced in the paper); and general benchmarks are MathVista and MATH-Vision.  The models tested include proprietary models like GPT-40 and Claude 3.5 Sonnet, chart-augmented open-source models like ChartInstruct-7B and ChartGemma-3B, and latest general open-source models such as LLaVA-Next-Llama3-8B and InternVL2-8B. A text-only baseline using GPT-40 is also included.  Supervised fine-tuning (SFT) using the REACHQA dataset was conducted for the general open-source models, employing three versions for each: using recognition-oriented Q&A, reasoning-oriented Q&A, and combining both.  The results show that models fine-tuned on REACHQA demonstrate substantial performance gains across all benchmarks, highlighting the effectiveness of the proposed dataset. Importantly, these gains generalize beyond chart-specific tasks to broader multimodal reasoning tasks.  The study also investigates the impact of expert rationales on reasoning abilities, the effects of different ratios of recognition and reasoning data, and the benefits of mixing general-purpose multimodal instruction data in training.", "first_cons": "The experiments primarily focus on evaluating model performance on established benchmarks, which might not fully capture the complexity of real-world chart understanding scenarios.  The reliance on existing benchmarks limits the generalizability of the findings to situations outside the scope of these tests.", "first_pros": "The experimental setup is comprehensive, evaluating a diverse range of models on three different task categories, including both chart-specific and general multimodal reasoning tasks. The use of multiple benchmarks and different types of models strengthens the validity and robustness of the findings.", "keypoints": ["Three categories of tasks are used for evaluation: traditional chart-related benchmarks (recognition), novel chart-related benchmarks (recognition and reasoning), and general multimodal reasoning benchmarks.", "A new dataset, REACHQA, shows significant performance gains (over 30% on average for LLaVA-Next-Llama3-8B) after fine-tuning.", "Proprietary models demonstrate more balanced performance across different task types compared to open-source models which excel in recognition but struggle with complex reasoning tasks.", "The study explores deeper insights, including the impact of expert rationales, different training data ratios, and mixing general multimodal data, providing valuable insights into the training methodology and its effect on model capabilities"], "second_cons": "The study does not delve into the specific reasons behind the performance differences between different model architectures, hindering a deeper understanding of the underlying mechanisms involved in chart understanding and reasoning.", "second_pros": "The study demonstrates that synthetic datasets, such as REACHQA, can be just as effective as human-annotated datasets in evaluating model performance.  The findings underscore the value and efficiency of using LLM-generated data for training and evaluation purposes, particularly for complex tasks like chart understanding.", "summary": "The experiments section rigorously evaluates several multimodal large language models (MLLMs) on their ability to understand and reason with chart data.  Using three categories of tasks (traditional chart-related benchmarks, novel chart-related benchmarks, and general mathematical benchmarks), the researchers compare proprietary models, chart-augmented open-source models, and general open-source models. The results highlight the significant performance improvements achieved after fine-tuning models with the newly developed REACHQA dataset, which is itself generated using a code-as-intermediary translation technique.  The study also explores several additional factors that can impact the model's performance in order to provide deeper understanding of its reasoning abilities."}}, {"page_end_idx": 10, "page_start_idx": 8, "section_number": 5, "section_title": "DISCUSSION", "details": {"details": "This section delves into the impact of expert rationales and training data ratios on model reasoning abilities, comparing performance across various datasets.  The authors investigate the effects of using different training datasets (ChartBench, ChartAst, ChartGemma, and REACHQA) on model performance, revealing that models trained on datasets containing expert rationales, such as REACHQA, significantly outperform those trained on simpler datasets.  They find that a balanced approach, including both recognition and reasoning-oriented data, leads to the best overall results, with a combined dataset yielding a 34.8% performance boost for one model.  Furthermore, the experiments explore the interaction between recognition and reasoning abilities, demonstrating a positive correlation, and examine the generalization capabilities of models trained on specialized datasets to broader multimodal reasoning tasks.  The findings suggest that while simply increasing the amount of reasoning data may not always lead to improvement, the inclusion of expert rationales and a balanced approach that combines recognition and reasoning data yields the best results, improving not only chart-specific tasks but also generalized multimodal reasoning abilities.  Attention analysis is conducted to investigate the internal mechanisms underlying these improved performance results, suggesting that the fine-tuned model utilizes attention more effectively than the baseline model.", "first_cons": "The study is limited by the resource constraints that prevented scaling up the dataset size.   A larger dataset could provide further insights into the complex relationship between recognition and reasoning abilities.", "first_pros": "The analysis reveals a strong correlation between the quality of training data (particularly, the inclusion of expert rationales) and the model's ability to perform well on both specialized (chart-related) and generalized (multimodal reasoning) tasks.", "keypoints": ["Models trained on REACHQA (with expert rationales) significantly outperform those trained on other datasets.", "A balanced training approach, incorporating both recognition and reasoning data, yields a 34.8% performance boost in one model.", "The study highlights a positive correlation between recognition and reasoning abilities; improved recognition skills contribute to enhanced reasoning.", "Models trained on specialized datasets show improved performance on generalized multimodal reasoning tasks."], "second_cons": "The analysis focuses primarily on open-source models, with limited exploration of proprietary models.  A broader model comparison would provide a more holistic perspective.", "second_pros": "The attention analysis offers valuable insights into the internal mechanisms driving the improved performance of fine-tuned models, providing interpretability to the results.", "summary": "This section explores the interplay between training data quality, particularly expert rationales, and model performance on both chart-specific and general multimodal reasoning tasks.  Experiments show that models trained on datasets incorporating expert rationales, like REACHQA, significantly outperform models trained on simpler datasets.  A balanced approach combining recognition and reasoning data proves most effective, leading to substantial performance gains (e.g., a 34.8% improvement in one model).  Attention analysis reveals that fine-tuned models use attention more effectively, showing the importance of high-quality data and a balanced training strategy for improved visual reasoning abilities."}}]