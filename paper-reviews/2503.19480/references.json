{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-03-05", "reason": "This paper introduces CLIP, a foundational model for vision-language learning which this study builds upon and aims to enhance."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Improved baselines with visual instruction tuning", "publication_date": "2024-06-01", "reason": "This paper describes LLaVA-1.5, one of the latest MLLMs that is used in the comparative experiments to evaluate the performance of the enhanced visual encoders."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "publication_date": "2021-06-01", "reason": "This paper introduces VQ-GAN that is employed as a discrete denoiser for generating perceptually rich representations in this study."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-06-17", "reason": "This paper introduces latent diffusion models (LDMs), a generative method that is employed in related work DIVA, with which this method is compared."}, {"fullname_first_author": "Wenxuan Wang", "paper_title": "Diffusion feedback helps CLIP see better", "publication_date": "2025-05-01", "reason": "This paper presents DIVA, a related work which also seeks to improve CLIP's visual representations through diffusion feedback and that serves as the primary comparison baseline in the study."}]}