[{"figure_path": "https://arxiv.org/html/2412.18605/x1.png", "caption": "Figure 1: Understanding object orientation is essential for spatial reasoning. However, even advanced VLMs like GPT-4o and Gem- ini-1.5-pro are not yet able to resolve the basic orientation issue.", "description": "This figure demonstrates the limitations of even advanced large language models (LLMs) like GPT-4 and Gemini in understanding basic spatial reasoning tasks involving object orientation.  The example shows a scenario where the models struggle to correctly determine the relative position of Captain America to Falcon from Falcon's point of view, highlighting the challenges in accurately perceiving and interpreting object orientations from image data.", "section": "2. Related Work"}, {"figure_path": "https://arxiv.org/html/2412.18605/extracted/6093642/annotation.png", "caption": "Figure 2: The orientation data collection pipeline is composed of three steps: 1) Canonical 3D Model Filtering: This step removes any 3D objects in tilted poses. 2) Orientation Annotating: An advanced 2D VLM is used to identify the front face from multiple orthogonal perspectives, with view symmetry employed to narrow the potential choices. 3) Free-view Rendering: Rendering images from random and free viewpoints, and the object orientation is represented by the polar \u03b8\ud835\udf03\\thetaitalic_\u03b8, azimuthal \u03c6\ud835\udf11\\varphiitalic_\u03c6 and rotation angle \u03b4\ud835\udeff\\deltaitalic_\u03b4 of the camera.", "description": "This figure illustrates a three-step pipeline for collecting object orientation data.  First, a filtering step removes any 3D models that are not upright. Second, an advanced vision-language model (VLM) identifies the front face of each remaining 3D model using several orthogonal views; symmetry analysis helps refine the selection. Finally, images are rendered from numerous random viewpoints. The resulting images are annotated with the object's orientation, represented by three angles: polar angle (\u03b8), azimuthal angle (\u03c6), and rotation angle (\u03b4) of the camera relative to the object.", "section": "4. Orientation Data Collection"}, {"figure_path": "https://arxiv.org/html/2412.18605/extracted/6093642/model.png", "caption": "Figure 3: Orient Anything consists of a simple visual encoder and multiple prediction heads. It is trained to judge\nif the object in the input image has a meaningful front face and fits the probability distribution of 3D orientation.", "description": "This figure illustrates the architecture of the Orient Anything model.  The model takes an input image and processes it through a visual encoder. This encoder extracts relevant visual features.  Multiple prediction heads then branch off from the encoder, each responsible for estimating a specific aspect of the object's 3D orientation (polar angle, azimuth angle, and rotation angle).  Crucially, a separate confidence head predicts whether the object in the image has a clearly defined front face. This confidence estimation improves the overall robustness of the orientation prediction by identifying and handling cases with poor-defined frontal aspects.  The training process focuses on aligning the predicted orientation parameters to learned probability distributions of these parameters, rather than relying on direct regression, which improves robustness. The model is trained to accurately predict these parameters and whether a front face can be reliably identified in the image.", "section": "5. Orient Anything"}, {"figure_path": "https://arxiv.org/html/2412.18605/extracted/6093642/sigma.png", "caption": "Figure 4: Ablation study for hyper-parameter \u03c3\u03b8subscript\ud835\udf0e\ud835\udf03\\sigma_{\\theta}italic_\u03c3 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT, \u03c3\u03c6subscript\ud835\udf0e\ud835\udf11\\sigma_{\\varphi}italic_\u03c3 start_POSTSUBSCRIPT italic_\u03c6 end_POSTSUBSCRIPT and \u03c3\u03b4subscript\ud835\udf0e\ud835\udeff\\sigma_{\\delta}italic_\u03c3 start_POSTSUBSCRIPT italic_\u03b4 end_POSTSUBSCRIPT.", "description": "This ablation study analyzes the influence of variance hyperparameters (\u03c3\u03b8, \u03c3\u03c6, and \u03c3\u03b4) on the accuracy of angle prediction in the Orient Anything model.  The x-axis represents the values of each hyperparameter, while the y-axis displays the corresponding accuracy (Acc@22.5\u00b0 for azimuth, Acc@5\u00b0 for polar and Acc@5\u00b0 for rotation).  The different colored lines represent the accuracy of different angles.  The results demonstrate the model's robustness and relative insensitivity to the specific choice of these hyperparameters within a reasonable range.", "section": "6. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.18605/extracted/6093642/generation.png", "caption": "Figure 5: Generated images with given textual prompt (left two from DALL-E 3\u00a0[5], right two from FLUX\u00a0[21]). Accurate orientation estimation is helpful to confirm whether generated contents follow the given orientation or perspective condition.", "description": "This figure demonstrates the impact of accurate object orientation estimation on image generation models.  Two pairs of images are shown, each pair generated from the same textual prompt but with differing results. The first pair shows images generated by DALL-E 3, while the second pair is from FLUX.  The differences in the generated images highlight how precise object orientation significantly affects the outcome and shows that accurate object orientation estimation (as provided by the model presented in the paper) can be used to verify whether the generated content adheres to the desired orientation and perspective.", "section": "7. Applications"}, {"figure_path": "https://arxiv.org/html/2412.18605/x6.png", "caption": "Figure 6: Qualitative results on COCO", "description": "This figure displays a qualitative comparison of object orientation estimation results on a subset of images from the COCO dataset.  For each image, the ground truth orientation is shown alongside estimations produced by the Cube RCNN model and the Orient Anything model proposed in the paper. The visual comparison allows for an assessment of the accuracy and robustness of each model in estimating object orientation across various object categories and viewpoints within the COCO dataset.", "section": "6.3. Zero-shot Real-Image Orientation Recognition"}, {"figure_path": "https://arxiv.org/html/2412.18605/x7.png", "caption": "Figure 7: Qualitative results on SUN RGB-D.", "description": "This figure displays a qualitative comparison of object orientation estimation results on the SUN RGB-D dataset.  It shows three columns for each object: Ground Truth (the actual orientation), Cube R-CNN (a previous method's estimation), and Orient Anything (the authors' method's estimation). Each object is shown with its estimated orientation represented by a set of axes; discrepancies between the estimations and the ground truth highlight the performance differences of the methods.", "section": "6.3 Zero-shot Real-Image Orientation Recognition"}, {"figure_path": "https://arxiv.org/html/2412.18605/x8.png", "caption": "Figure 8: Qualitative results on KITTI and nuScenes.", "description": "This figure shows a qualitative comparison of object orientation estimation results on the KITTI and nuScenes datasets.  For each dataset, several example images are displayed, showing the ground truth orientation (represented by coordinate axes), the orientation estimated by the Cube RCNN method, and the orientation estimated by the Orient Anything method.  The results visually demonstrate the superior accuracy of the Orient Anything model in predicting object orientations.", "section": "6.3. Zero-shot Real-Image Orientation Recognition"}, {"figure_path": "https://arxiv.org/html/2412.18605/x9.png", "caption": "Figure 9: Qualitative results on Objectron.", "description": "This figure shows a qualitative comparison of object orientation estimation results on the Objectron dataset.  For several images of objects, it displays the ground truth orientation (the actual orientation), the orientation predicted by the Cube RCNN model, and the orientation estimated by the Orient Anything model. This visual comparison demonstrates the relative accuracy of the different models in estimating 3D object orientation.", "section": "6.3. Zero-shot Real-Image Orientation Recognition"}]