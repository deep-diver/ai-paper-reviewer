[{"figure_path": "https://arxiv.org/html/2502.07737/x1.png", "caption": "Figure 1: 3D discrete token map produced by our video tokenizer. The input video consists of one initial frame, followed by n\ud835\udc5bnitalic_n clips, with each clip containing FTsubscript\ud835\udc39\ud835\udc47F_{T}italic_F start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT frames. xj(i)subscriptsuperscript\ud835\udc65\ud835\udc56\ud835\udc57x^{(i)}_{j}italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT indicates the jt\u2062hsuperscript\ud835\udc57\ud835\udc61\u210ej^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT video token in the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT clip.", "description": "This figure illustrates the 3D discrete token map generated by the video tokenizer used in the paper.  The input video is processed as a sequence. It begins with a single initial frame, followed by a series of clips, each containing a fixed number of frames (FT). Each element in the 3D map, represented as xj(i), signifies a specific video token. The indices (i) and (j) denote the clip number and the token's position within that clip respectively.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.07737/x2.png", "caption": "Figure 2: Examples of block include token-wise, row-wise, and frame-wise representations. When the block size is set to 1\u00d7\\times\u00d71\u00d7\\times\u00d71, it degenerates into a token, as used in vanilla AR modeling. Note that the actual token corresponds to a 3D cube, we omit the time dimension here for clarity.", "description": "This figure illustrates different ways to group tokens in a 3D video representation.  Standard autoregressive (AR) models process tokens individually (token-wise), treating each pixel or voxel as a separate unit in the generation process.  This figure shows alternative block configurations. A row-wise block considers a horizontal line of tokens as a unit, a frame-wise block considers all tokens in one frame as a unit.  This allows for parallel processing across multiple tokens in each block instead of generating tokens one by one. When a block size of 1x1x1 is used, it defaults back to the individual token approach of vanilla AR, showing how the block-based approaches generalize the individual token approach.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.07737/x3.png", "caption": "Figure 3: Comparison between a vanilla autoregressive (AR) framework based on next-token prediction (left) and our semi-AR framework based on next-block prediction (right). xj(i)subscriptsuperscript\ud835\udc65\ud835\udc56\ud835\udc57x^{(i)}_{j}italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT indicates the jt\u2062hsuperscript\ud835\udc57\ud835\udc61\u210ej^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT video token in the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT block, with each block containing L\ud835\udc3fLitalic_L tokens.\nThe dashed line in the right panel presents that the L\ud835\udc3fLitalic_L tokens generated in the current step are duplicated and concatenated with prefix tokens, forming the input for the next step\u2019s prediction during inference.", "description": "This figure compares the vanilla autoregressive (AR) model with next-token prediction and the proposed semi-AR model with next-block prediction.  The vanilla AR model (left) generates one token at a time, sequentially, using a decoder-only transformer. Each token depends only on the preceding tokens. In contrast, the semi-AR model (right) generates a block of tokens simultaneously. Each token in the current block predicts the corresponding token in the next block, taking advantage of bidirectional attention within the block to capture spatial dependencies.  During inference in the semi-AR model, the generated block of tokens is duplicated and concatenated with prefix tokens to form the input for the next prediction step.", "section": "3.3 Semi-AR Modeling via Next Block Modeling"}, {"figure_path": "https://arxiv.org/html/2502.07737/x4.png", "caption": "Figure 4: Causal attention mask in NTP modeling v.s. block-wise attention mask in NBP modeling. The x-axis and y-axis represent keys and queries, respectively.", "description": "This figure compares the attention mechanisms used in traditional Next-Token Prediction (NTP) and the proposed Next-Block Prediction (NBP) models.  In NTP, the attention is strictly causal, meaning each token only attends to previous tokens, represented by a lower triangular attention mask.  This limitation restricts the model's ability to capture long-range dependencies. In contrast, NBP employs a block-wise attention mechanism. Within each block, tokens can attend to all other tokens in the block bidirectionally, enabling richer contextual understanding.  The figure highlights this difference by showing the attention masks for both methods. The x-axis and y-axis represent the keys and queries of the attention mechanism, respectively.", "section": "3.3 Semi-AR Modeling via Next Block Modeling"}, {"figure_path": "https://arxiv.org/html/2502.07737/x5.png", "caption": "Figure 5: Validation loss of various sizes of semi-AR models from 700M to 3B.", "description": "This figure shows the validation loss curves during the training process for semi-autoregressive (semi-AR) video generation models with different parameter scales: 700M, 1.2B, and 3B.  The x-axis represents the number of training steps, and the y-axis shows the validation loss.  The curves illustrate how the model's performance changes as it trains, with different sized models exhibiting various learning rates and convergence behaviors.  The plot allows for comparison of the training efficiency and stability across models of differing sizes.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07737/x6.png", "caption": "Figure 6: Validation loss of various block sizes from 1 to 256.", "description": "This figure shows the validation loss curves for different block sizes used in the Next-Block Prediction (NBP) model during training.  The x-axis represents the number of training steps, and the y-axis represents the validation loss. Multiple lines are plotted, each corresponding to a different block size, ranging from 1 to 256.  The plot illustrates how the model's performance changes with varying block sizes during training. The optimal block size that balances training efficiency and model performance can be determined by analyzing this graph.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07737/x7.png", "caption": "Figure 7: Generation quality (FVD, lower is better) and inference speed (FPS, higher is better) of various block sizes from 1 to 256.", "description": "This figure shows a comparison of the generation quality (measured by Fr\u00e9chet Video Distance, FVD) and inference speed (frames per second, FPS) of the Next-Block Prediction (NBP) model using different block sizes for video generation.  The x-axis represents the block size, ranging from 1 to 256, where a block size of 1 corresponds to the traditional next-token prediction method. The y-axis shows both the FVD score and the FPS.  A lower FVD indicates higher generation quality (closer to the ground truth), while a higher FPS indicates faster inference. The graph visually demonstrates the trade-off between generation quality and inference speed as the block size changes, allowing readers to identify an optimal block size that balances both.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07737/x8.png", "caption": "Figure 8: Visualization of class-conditional generation (UCF-101) results of our method. The text below each video clip is the class name.", "description": "This figure visualizes the results of class-conditional video generation on the UCF-101 dataset using the proposed Next-Block Prediction (NBP) method. Each row presents a different action class from UCF-101.  Multiple video clips generated for each class are displayed, demonstrating the model's ability to generate diverse instances of each action. The text below each set of video clips indicates the specific action class being depicted.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07737/x9.png", "caption": "Figure 9: Visualization of frame prediction (K600) results of our method.", "description": "This figure shows example results of the model's frame prediction capabilities on the Kinetics-600 dataset.  Each row represents a video sequence where the model has predicted subsequent frames based on a given initial set of frames. The results demonstrate the model's ability to generate temporally coherent and visually plausible video frames, accurately continuing the action or scene from the input.  This showcases the model's capacity to generate realistic and smooth video sequences.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07737/x10.png", "caption": "Figure 10: Frame prediction results of OmniTokenizer and our method. The left part is the condition, and the right part is the predicted subsequent sequence.", "description": "This figure displays a comparison of frame prediction results between the OmniTokenizer method and the Next-Block Prediction (NBP) method proposed in the paper.  The left side shows the input frames (the 'condition'), a short video sequence used to initiate the prediction. The right side shows the generated frames (the 'predicted subsequent sequence') which are the frames predicted by each method. By comparing the generated frames with the ground truth, one can visually assess the performance of each method in terms of accuracy and quality in video prediction tasks.  The differences between the OmniTokenizer and NBP predictions highlight the improvements achieved by the NBP model.", "section": "4. Visualizations"}, {"figure_path": "https://arxiv.org/html/2502.07737/x11.png", "caption": "Figure 11: Video reconstruction results (17 frames 128\u00d7\\times\u00d7128 resolution at 25 fps and shown at 6.25 fps) of OmniTokenizer and our method.", "description": "This figure presents a comparison of video reconstruction results between the OmniTokenizer method and the Next Block Prediction (NBP) method proposed in the paper.  It shows sample video frames (17 frames total, 128x128 resolution) generated by each method to illustrate the visual quality differences. The original video is displayed at 25 frames per second (fps), but for easier viewing within the figure, the frames are displayed at 6.25 fps.  A visual inspection allows one to assess the level of detail, artifacts, and overall reconstruction accuracy achieved by each method.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07737/x12.png", "caption": "Figure 12: Visualization of video generation results of various model sizes (700M, 1.2B, and 3B).", "description": "This figure displays video generation results from three different sized models: 700 million parameters, 1.2 billion parameters, and 3 billion parameters.  Each row shows a different video generated by each model. The purpose is to demonstrate how increasing model size impacts the quality of generated videos.  Observe the visual details to compare and contrast the outputs of the three models.  The generated videos shown exemplify the ability of the model to generate different video clips.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07737/x13.png", "caption": "Figure 13: Visualization of video generation results of various block sizes (1\u00d7\\times\u00d71\u00d7\\times\u00d71, 1\u00d7\\times\u00d71\u00d7\\times\u00d716 and 1\u00d7\\times\u00d716\u00d7\\times\u00d716).", "description": "This figure visualizes the video generation results using different block sizes within the Next-Block Prediction (NBP) model.  It shows three sets of video generation outputs, each corresponding to a different block size: 1x1x1 (token-wise), 1x1x16 (row-wise), and 1x1x16 (a larger block spanning multiple tokens). The three rows represent three different sample videos. The aim is to illustrate how altering the size of the processing unit (block size) impacts the model's ability to generate coherent and visually accurate video sequences.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07737/x14.png", "caption": "Figure 14: Attention weights of next-clip prediction on UCF-101. The horizontal and vertical axis represent the keys and queries, respectively. Two red lines on each axis divide the axis into three segments, corresponding to the text (classname), the first clip, and the second clip. The brightness of each pixel reflects the attention score. We downweight the attention to text tokens by 5\u00d75\\times5 \u00d7 to provide a more clear visualization.", "description": "Figure 14 is a heatmap visualizing the attention weights within the model during the next-clip prediction task on the UCF-101 dataset. The x and y axes represent the keys and queries, respectively, with each axis divided into three segments by two red lines: the text (class name), the first video clip, and the second video clip.  Each pixel's brightness corresponds to the attention weight between a specific key and query. The attention weights for text tokens are downweighted by a factor of 5 to enhance visualization clarity, highlighting the attention relationships between the textual description, the previous video clip, and the model's prediction for the next clip.", "section": "3.3 Semi-AR Modeling via Next Block Modeling"}, {"figure_path": "https://arxiv.org/html/2502.07737/x15.png", "caption": "Figure 15: Spatial attention distribution for a specific query (represented by red \u00d7\\times\u00d7) on UCF-101.", "description": "Figure 15 visualizes the spatial attention distribution within the Next-Block Prediction (NBP) model for a single query token (marked in red).  It shows how the attention mechanism weights the relevance of different spatial locations within the video frame when predicting the corresponding token in the next block. This illustrates the model's ability to capture spatial dependencies and relationships across the frame, a key improvement over traditional autoregressive models that rely on unidirectional dependencies.", "section": "Analysis of Attention Pattern"}]