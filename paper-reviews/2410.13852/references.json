{"references": [{" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is foundational for RLHF, a common technique in aligning LLMs with human preferences. The current work contrasts with RLHF by focusing on implicit feedback, offering a novel approach to continual learning without explicit feedback solicitation.  Understanding RLHF is crucial for appreciating the novelty and significance of the proposed implicit feedback method.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Arash Ahmadian", "paper_title": "Back to basics: Revisiting REINFORCE-style optimization for learning from human feedback in LLMs", "reason": "This paper explores REINFORCE-style optimization for LLMs, a learning method used in the current study.  Understanding the performance and characteristics of REINFORCE is crucial to interpreting and contextualizing the results of the current work, particularly in relation to the comparison with other optimization methods like KTO and supervised learning.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Kawin Ethayarajh", "paper_title": "KTO: Model Alignment as Prospect Theoretic Optimization", "reason": "This paper introduces KTO, one of the learning methods used in the current study. Understanding KTO's underlying principles and performance is crucial for evaluating the comparative results and the strengths and weaknesses of different learning approaches adopted in the study.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Noriyuki Kojima", "paper_title": "Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior", "reason": "This paper explores continual learning, a central theme in the current study. This work examines the learning process in the context of instruction generation and shows how continual learning can improve the quality of the generated instructions.  Understanding its approach to continual learning is relevant for appreciating and contrasting the methodology used in the current research.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Shachar Don-Yehiya", "paper_title": "Learning from Naturally Occurring Feedback", "reason": "This paper shares a similar goal to the current research, which uses naturally occurring feedback for learning.  Comparing the approaches, particularly regarding how implicit feedback is extracted and utilized, enables a deeper understanding of the strengths and limitations of each methodology and its applicability in different contexts.", "section_number": 7}, {" publication_date": "2024", "fullname_first_author": "Ge Gao", "paper_title": "Continually improving extractive QA via human feedback", "reason": "This paper explores continual learning in the context of question answering.  Understanding its approach to continual learning, especially how it uses human feedback, enhances the understanding of different continual learning approaches and allows for comparison with the current study's methods.", "section_number": 7}, {" publication_date": "2024", "fullname_first_author": "Ge Gao", "paper_title": "Aligning LLM Agents by Learning Latent Preference from User Edits", "reason": "This paper also explores continual learning, a major aspect of the current work. Its focus on aligning LLMs through user feedback provides a perspective to contrast and compare the different methods for achieving continual learning from various feedback sources.", "section_number": 7}, {" publication_date": "2022", "fullname_first_author": "Anya Ji", "paper_title": "Abstract Visual Reasoning with Tangram Shapes", "reason": "This paper introduces the KILOGRAM dataset, the source of tangram images used in MULTIREF. Understanding the dataset's characteristics, especially the ambiguity and abstract nature of the tangram shapes, is critical for interpreting the results of the experiments in the current study.", "section_number": 3}, {" publication_date": "1986", "fullname_first_author": "Herbert H Clark", "paper_title": "Referring as a collaborative process", "reason": "This paper provides a cognitive science perspective on the collaborative nature of reference, crucial to understanding the MULTIREF scenario's design and the dynamics of human communication that the task aims to capture. This contextual understanding improves the interpretation of the qualitative results and the relevance of the implicit feedback learning.", "section_number": 3}, {" publication_date": "1989", "fullname_first_author": "Michael F Schober", "paper_title": "Understanding by addressees and overhearers", "reason": "This paper contributes to the cognitive science foundation of MULTIREF. It explores how human listeners understand utterances depending on their role (addressee vs. overhearer).  This understanding is critical to interpreting the implicit feedback and the dynamics of human-computer interaction in the MULTIREF scenario.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "John Schulman", "paper_title": "Proximal Policy Optimization Algorithms", "reason": "This paper introduces the Proximal Policy Optimization (PPO) algorithm, an important reinforcement learning method often compared to REINFORCE.  Understanding PPO and its properties helps to evaluate the chosen methods and the rationale behind using REINFORCE instead of PPO in the current study.", "section_number": 5}, {" publication_date": "1999", "fullname_first_author": "Jean E Fox Tree", "paper_title": "Listening in on monologues and dialogues", "reason": "This paper studies human discourse in monologues and dialogues, contributing to the understanding of communication dynamics in MULTIREF. Analyzing various communication strategies enriches the interpretation of implicit feedback signals and provides insights into the subtleties of human interaction during the task.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Robert Hawkins", "paper_title": "Continual Adaptation for Efficient Machine Communication", "reason": "This paper focuses on continual adaptation in machine communication, a directly related topic to the current research.  Understanding its methodology and findings helps to appreciate the challenges and potential solutions to continual learning in complex human-computer interaction scenarios.", "section_number": 7}, {" publication_date": "2021", "fullname_first_author": "Abdullah Almaatouq", "paper_title": "Empirica: A virtual lab for high-throughput macro-level experiments", "reason": "This paper presents Empirica, the platform used in the current work for human-in-the-loop experiments.  Understanding Empirica's capabilities and limitations informs the interpretation of experimental design and helps evaluate the feasibility and scalability of the approach.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "reason": "This paper describes LoRA, a parameter-efficient fine-tuning method used in this research. Understanding LoRA's mechanisms and effectiveness is necessary for interpreting the model initialization and training process in the study.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Hugo Lauren\u00e7on", "paper_title": "What matters when building vision-language models?", "reason": "This work introduces the IDEFICS2-8B model, the core model used in the study. This foundational knowledge is essential for understanding the chosen model's strengths and limitations, influencing the interpretation of experimental results and the impact of different learning methods on the model's performance.", "section_number": 5}, {" publication_date": "1992", "fullname_first_author": "Ronald J Williams", "paper_title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "reason": "This paper is foundational to understanding REINFORCE, one of the learning algorithms used in the study. Understanding the theoretical underpinnings of REINFORCE is critical for interpreting the results and comparing its performance to other methods.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Collin Burns", "paper_title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision", "reason": "This work investigates eliciting strong capabilities in models with weak supervision, which is relevant to the current research's focus on learning from limited and implicit feedback. This helps to place the research in context of the challenges of learning from limited data and the importance of using alternative learning techniques.", "section_number": 7}, {" publication_date": "2023", "fullname_first_author": "Alane Suhr", "paper_title": "Continual learning for instruction following from realtime feedback", "reason": "This paper is highly relevant because it explores continual learning, a core aspect of the current work. It uses real-time feedback to continuously improve instruction following. Comparing this approach to the current work's method of retrospective learning from implicit feedback signals enables a deeper understanding of various continual learning strategies and their effectiveness.", "section_number": 7}, {" publication_date": "2018", "fullname_first_author": "Philipp Moritz", "paper_title": "Ray: A distributed framework for emerging AI applications", "reason": "This paper introduces Ray, a distributed computing framework used in this research. Understanding Ray's capabilities is essential for understanding the deployment and scalability of the experiments described in the study.", "section_number": 5}]}