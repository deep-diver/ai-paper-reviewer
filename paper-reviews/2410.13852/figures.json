[{"figure_path": "2410.13852/figures/figures_1_0.png", "caption": "Figure 1: Learning via RESPECT. We deploy an LLM policy \u03c0\u03b8 (\u03b1|x) in rounds p, to interact with users in multi-turn interactions. Following each round, the LLM reasons retrospectively about each of its actions (highlighted in blue) to decode feedback given the interaction context, including follow up utterances. After each round, the model is retrained using all data aggregated so far D<p. The LLM improves over time without any external annotations. The plot on the right shows the performance curve in our experiments \u2013 the LLM improves from 31% to 82% task completion rate over six rounds.", "description": "The figure illustrates the RESPECT framework, showing how an LLM learns from implicit feedback in multi-turn interactions through retrospection and iterative retraining, resulting in improved task performance over time.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13852/figures/figures_3_0.png", "caption": "Figure 1: Learning via RESPECT. We deploy an LLM policy \u03c0\u03b8 (\u03b1|x) in rounds p, to interact with users in multi-turn interactions. Following each round, the LLM reasons retrospectively about each of its actions (highlighted in blue) to decode feedback given the interaction context, including follow up utterances. After each round, the model is retrained using all data aggregated so far D<p. The LLM improves over time without any external annotations. The plot on the right shows the performance curve in our experiments \u2013 the LLM improves from 31% to 82% task completion rate over six rounds.", "description": "The figure illustrates the RESPECT framework, showing how an LLM learns from implicit feedback in multi-turn interactions by retrospectively analyzing its actions and retraining.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13852/figures/figures_16_0.png", "caption": "Figure 1: Learning via RESPECT. We deploy an LLM policy \u03c0\u03b8(\u03b1|x) in rounds p, to interact with users in multi-turn interactions. Following each round, the LLM reasons retrospectively about each of its actions (highlighted in blue) to decode feedback given the interaction context, including follow up utterances. After each round, the model is retrained using all data aggregated so far D<p. The LLM improves over time without any external annotations. The plot on the right shows the performance curve in our experiments \u2013 the LLM improves from 31% to 82% task completion rate over six rounds.", "description": "The figure illustrates the RESPECT approach, showing how an LLM learns from implicit feedback in multi-turn interactions to improve its task completion rate over time without external annotation.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13852/figures/figures_17_0.png", "caption": "Figure 1: Learning via RESPECT. We deploy an LLM policy \u03c0\u03b8 (\u03b1|x) in rounds p, to interact with users in multi-turn interactions. Following each round, the LLM reasons retrospectively about each of its actions (highlighted in blue) to decode feedback given the interaction context, including follow up utterances. After each round, the model is retrained using all data aggregated so far D<p. The LLM improves over time without any external annotations. The plot on the right shows the performance curve in our experiments \u2013 the LLM improves from 31% to 82% task completion rate over six rounds.", "description": "The figure illustrates the RESPECT framework, showing how an LLM learns from implicit feedback in multi-turn interactions by retrospectively analyzing its actions and retraining.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13852/figures/figures_17_1.png", "caption": "Figure 1: Learning via RESPECT. We deploy an LLM policy \u03c0\u03b8(\u03b1|x) in rounds p, to interact with users in multi-turn interactions. Following each round, the LLM reasons retrospectively about each of its actions (highlighted in blue) to decode feedback given the interaction context, including follow up utterances. After each round, the model is retrained using all data aggregated so far D<p. The LLM improves over time without any external annotations. The plot on the right shows the performance curve in our experiments \u2013 the LLM improves from 31% to 82% task completion rate over six rounds.", "description": "The figure illustrates the RESPECT framework, showing how an LLM learns from implicit feedback in multi-turn interactions by retrospectively analyzing its actions and retraining.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13852/figures/figures_17_2.png", "caption": "Figure 1: Learning via RESPECT. We deploy an LLM policy \u03c0\u03b8 (\u03b1|x) in rounds p, to interact with users in multi-turn interactions. Following each round, the LLM reasons retrospectively about each of its actions (highlighted in blue) to decode feedback given the interaction context, including follow up utterances. After each round, the model is retrained using all data aggregated so far D<p. The LLM improves over time without any external annotations. The plot on the right shows the performance curve in our experiments \u2013 the LLM improves from 31% to 82% task completion rate over six rounds.", "description": "The figure illustrates the RESPECT framework, showing how an LLM learns from implicit feedback in multi-turn interactions through retrospective reasoning and retraining, resulting in improved performance over time.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13852/figures/figures_18_0.png", "caption": "Figure 1: Learning via RESPECT. We deploy an LLM policy \u03c0\u03b8 (\u03b1|x) in rounds p, to interact with users in multi-turn interactions. Following each round, the LLM reasons retrospectively about each of its actions (highlighted in blue) to decode feedback given the interaction context, including follow up utterances. After each round, the model is retrained using all data aggregated so far D<p. The LLM improves over time without any external annotations. The plot on the right shows the performance curve in our experiments \u2013 the LLM improves from 31% to 82% task completion rate over six rounds.", "description": "The figure illustrates the RESPECT framework, showing how an LLM learns from implicit feedback in multi-turn interactions by retrospectively analyzing its actions and retraining.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13852/figures/figures_22_0.png", "caption": "Figure 1: Learning via RESPECT. We deploy an LLM policy \u03c0\u03b8 (\u03b1|x) in rounds p, to interact with users in multi-turn interactions. Following each round, the LLM reasons retrospectively about each of its actions (highlighted in blue) to decode feedback given the interaction context, including follow up utterances. After each round, the model is retrained using all data aggregated so far D<p. The LLM improves over time without any external annotations. The plot on the right shows the performance curve in our experiments \u2013 the LLM improves from 31% to 82% task completion rate over six rounds.", "description": "The figure illustrates the RESPECT framework, showing how an LLM learns from implicit feedback in multi-turn interactions by retrospectively analyzing its actions and retraining.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13852/figures/figures_23_0.png", "caption": "Figure 1: Learning via RESPECT. We deploy an LLM policy \u03c0\u03b8 (\u03b1|x) in rounds p, to interact with users in multi-turn interactions. Following each round, the LLM reasons retrospectively about each of its actions (highlighted in blue) to decode feedback given the interaction context, including follow up utterances. After each round, the model is retrained using all data aggregated so far D<p. The LLM improves over time without any external annotations. The plot on the right shows the performance curve in our experiments \u2013 the LLM improves from 31% to 82% task completion rate over six rounds.", "description": "The figure illustrates the RESPECT framework, showing how an LLM learns from implicit feedback in multi-turn interactions by retrospectively analyzing its actions and retraining.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13852/figures/figures_24_0.png", "caption": "Figure 1: Learning via RESPECT. We deploy an LLM policy \u03c0\u03b8 (\u03b1|x) in rounds p, to interact with users in multi-turn interactions. Following each round, the LLM reasons retrospectively about each of its actions (highlighted in blue) to decode feedback given the interaction context, including follow up utterances. After each round, the model is retrained using all data aggregated so far D<p. The LLM improves over time without any external annotations. The plot on the right shows the performance curve in our experiments \u2013 the LLM improves from 31% to 82% task completion rate over six rounds.", "description": "The figure illustrates the RESPECT framework, showing how an LLM learns from implicit feedback in multi-turn interactions through retrospective reasoning and retraining, leading to improved performance over time.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13852/figures/figures_25_0.png", "caption": "Figure 1: Learning via RESPECT. We deploy an LLM policy \u03c0\u03b8 (\u03b1|x) in rounds p, to interact with users in multi-turn interactions. Following each round, the LLM reasons retrospectively about each of its actions (highlighted in blue) to decode feedback given the interaction context, including follow up utterances. After each round, the model is retrained using all data aggregated so far D<p. The LLM improves over time without any external annotations. The plot on the right shows the performance curve in our experiments \u2013 the LLM improves from 31% to 82% task completion rate over six rounds.", "description": "The figure illustrates the RESPECT framework, showing how an LLM iteratively interacts with users, retrospectively analyzes feedback from interactions, and retrains to improve task completion rate over multiple rounds.", "section": "1 INTRODUCTION"}]