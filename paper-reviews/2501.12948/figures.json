[{"figure_path": "https://arxiv.org/html/2501.12948/x1.png", "caption": "Figure 1: \nBenchmark performance of DeepSeek-R1.", "description": "This figure presents a benchmark comparison of the DeepSeek-R1 model's performance against other models across six different reasoning tasks.  The tasks assessed are AIME 2024, Codeforces, GPQA Diamond, MATH-500, MMLU, and SWE-bench Verified.  Each bar represents the accuracy or percentile achieved by a specific model on each task. DeepSeek-R1 and its variants (DeepSeek-R1-32B) are compared against OpenAI models (OpenAI-01-1217, OpenAI-01-mini) and DeepSeek-V3. The figure showcases DeepSeek-R1's competitive performance, particularly on several tasks, highlighting its advanced reasoning capabilities.", "section": "3 Experiment"}, {"figure_path": "https://arxiv.org/html/2501.12948/extracted/6147501/figures/plot_aime_with_maj.png", "caption": "Figure 2: AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation.", "description": "This figure shows a graph illustrating the performance of the DeepSeek-R1-Zero model on the AIME 2024 benchmark throughout its reinforcement learning (RL) training.  The y-axis represents the model's accuracy (percentage correct), and the x-axis shows the number of training steps. The graph displays two lines: one for pass@1 accuracy (the percentage of times the top predicted answer was correct) and another for cons@64 (the consensus accuracy across 64 samples).  To avoid fluctuations in accuracy, 16 response samples were used for each question and the average accuracy is shown. The plot clearly demonstrates an improvement in accuracy over the course of the RL training, showcasing the model's learning process and its ability to improve its reasoning capabilities through RL without supervised fine-tuning.", "section": "2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero"}, {"figure_path": "https://arxiv.org/html/2501.12948/extracted/6147501/figures/plot_length.png", "caption": "Figure 3: The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.", "description": "This figure shows how the average length of DeepSeek-R1-Zero's responses changed during its reinforcement learning (RL) training.  The x-axis represents the training step, and the y-axis represents the average number of tokens in the model's responses. As training progressed, the model's responses got progressively longer, indicating that it was taking more time to think through and solve increasingly complex reasoning problems. This increase in response length wasn't manually enforced but emerged naturally as a consequence of the RL process, showcasing the model's self-improvement through RL.", "section": "2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero"}]