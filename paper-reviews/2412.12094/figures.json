[{"figure_path": "https://arxiv.org/html/2412.12094/x3.png", "caption": "Figure 1: The loss comparison between vanilla Transformer and proposed SepLLM. SepLLM achieves lower loss at different computation costs and different training time consistently.", "description": "This figure compares the training loss of the proposed SepLLM architecture against a standard Transformer model.  The x-axes represent both computational cost (TFLOPS) and training time (seconds). The y-axis represents the training loss. The results demonstrate that SepLLM consistently achieves a lower training loss for a given computational budget and trains faster to reach the same loss as the baseline Transformer.  Specifically, it shows around 2.60 loss for SepLLM compared to 2.70 for Vanilla Transformer at roughly 0.5 * 10^9 TFLOPS. Similar results can be observed when considering training time with 2.42 loss for SepLLM vs 2.60 loss for Vanilla Transformer at 100,000 seconds of training.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.12094/x4.png", "caption": "Figure 2: The visualization of attention scores among different layers given the input \u201cNatalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. \u2026\u201d. Note that the separator tokens like \u201c,\u201d and \u201c.\u201d contribute massive attentions.", "description": "This figure visualizes the attention scores within a Llama-3-8B-instruct model across different layers, given the example input \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May\u2026\". It highlights the observation that separator tokens (e.g., commas, periods) receive disproportionately high attention scores compared to other semantically meaningful tokens. This suggests that information about the segments between separators is effectively compressed into the separators themselves.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.12094/x5.png", "caption": "Figure 3: The overall paradigm of\u00a0SepLLM. The left side illustrates the attention mask in the training or pre-filling stage given the input \u201cABC,DE.FG\\n\\absent\ud835\udc5b\\backslash n\\ italic_n\u201d. The right side illustrates the KV cache management in the generation stage.", "description": "SepLLM's overall paradigm is visualized in this figure. During training or pre-filling, the attention mask (left side) determines which tokens are attended to. For the given input \u201cABC,DE.FG\n\u201d, only the initial tokens (A, B), separator tokens (',', '.', '\n'), and neighboring tokens (based on distance to the current token, denoted as 'n') are considered. Other tokens are masked.  The right side illustrates how the key-value (KV) cache is managed during generation. Only the KV pairs corresponding to the initial, separator, and neighboring tokens are retained in the cache, leading to a significantly smaller KV cache compared to the standard Transformer.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2412.12094/x6.png", "caption": "Figure 4: Overall framework of the proposed SepLLM\u00a0tailored for streaming applications. The KV pairs are storaged in four cache blocks (displayed as four columns), and are updated in each iteration (shown in a single row). Once the runtime usage S\u2062i\u2062z\u2062er\u2062u\u2062n\ud835\udc46\ud835\udc56\ud835\udc67subscript\ud835\udc52\ud835\udc5f\ud835\udc62\ud835\udc5bSize_{run}italic_S italic_i italic_z italic_e start_POSTSUBSCRIPT italic_r italic_u italic_n end_POSTSUBSCRIPT reach the max capacity c, SepLLM move KV caches of separator tokens in Past Window Cache into Separator Cache and drop other KV caches.", "description": "SepLLM's framework for handling streaming data, showing its four cache blocks (Initial, Separator, Past Window, and Local Window) and how KV pairs are managed across iterations.  When the total cache usage hits maximum capacity, separator tokens are moved to Separator Cache and others discarded, demonstrating SepLLM's approach to handling infinitely long sequences.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.12094/x7.png", "caption": "(a) Loss w.r.t\u00a0steps", "description": "This figure shows the training loss curves with respect to training steps for training Pythia-160m-deduped from scratch. \"SepLLM(n=64, H)\" indicates that only the first self-attention layer is changed to full-attention while other layers are based on the proposed SepLLM architecture with n=64. \"SepLLM(n=64, H/T)\" indicates that both the first and the last self-attention layers are changed to full-attention while other layers are based on the proposed SepLLM architecture with n=64. SepLLM can achieve lower loss compared with Vanilla (original Pythia-160m-deduped with full-attention) and StrmLLM(n=64) during the training process. ", "section": "4.3. Training from Scratch"}, {"figure_path": "https://arxiv.org/html/2412.12094/x8.png", "caption": "(b) Loss Ratio w.r.t\u00a0FLOPs", "description": "This plot showcases the ratio of training loss for different models (SepLLM variants, StrmLLM, and Vanilla Transformer) compared to the Vanilla Transformer's loss, plotted against the computational cost (measured in FLOPs). It demonstrates the relative performance efficiency of each model by illustrating how much loss reduction they achieve for a given computational budget.", "section": "4.3. Training from Scratch"}, {"figure_path": "https://arxiv.org/html/2412.12094/x10.png", "caption": "Figure 5: Training loss curves for training from scratch. 5(b) shows the ratio of the loss values of different methods to that of Vanilla with respect to FLOPs.", "description": "This figure presents training loss curves, comparing the performance of different models (including Vanilla, StrmLLM, and SepLLM variations) with respect to the number of training iterations and FLOPs.  Subfigure (b) specifically highlights the loss ratio of the other methods relative to Vanilla as FLOPs increases, demonstrating SepLLM's efficiency in achieving a lower loss than Vanilla with similar compute.", "section": "4.3. Training from Scratch"}, {"figure_path": "https://arxiv.org/html/2412.12094/x11.png", "caption": "Figure 6: Training loss curves for the post-training.", "description": "This figure shows the training loss curves during the post-training phase of SepLLM with different settings of neighboring tokens (n) and learning rates.  SepLLM (n=64, larger lr) uses a cosine learning rate scheduler with warm-up starting from scratch, while SepLLM (n=64) and SepLLM (n=128) continue the cosine decay from the existing checkpoint.  The graph demonstrates that increasing n and using a larger learning rate can lead to faster convergence (lower loss) during post-training.", "section": "4.4. Post-training"}, {"figure_path": "https://arxiv.org/html/2412.12094/x12.png", "caption": "Figure 7: The evolution of KV caches in the streaming setting.", "description": "This figure visualizes the evolution of key-value (KV) caches in a streaming setting. It demonstrates how the number of neighboring tokens (n) and the total runtime KV cache usage (Sizerun) change over time (m). Both n and Sizerun exhibit periodic behavior after a certain point (m0), indicating a dynamic pattern in cache management during streaming sequence generation.  The figure also highlights key parameters like maximum capacity (c), initial cache size (a), separator cache size (s), and local window cache size (w). It visually explains how these parameters influence the periodic behavior of KV cache usage.", "section": "3. Method"}]