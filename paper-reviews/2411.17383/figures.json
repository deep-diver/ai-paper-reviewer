[{"figure_path": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Intro/reference.png", "caption": "Figure 1: We propose AnchorCrafter, a diffusion-based human video generation framework for creating high-fidelity anchor-style product promotion videos by animating reference human images with specific products and motion controls. By incorporating human-object interaction into the generation process, AnchorCrafter achieves high preservation of object appearance and enhanced interaction awareness.", "description": "AnchorCrafter is a new framework for generating high-quality videos of people promoting products, similar to what you see in online shopping.  It uses diffusion models to animate images of people holding products, precisely controlling their movements.  The key innovation is the inclusion of human-object interaction, resulting in realistic-looking videos where the product and the person naturally interact. This figure shows examples of the system's ability to handle variations: the same person demonstrating different products (a), and different people demonstrating the same product (b).", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Intro/animateanyone.png", "caption": "(a) Reference Input", "description": "This figure demonstrates the input and output of the AnchorCrafter model. (a) shows the reference images used as input to the model: a reference image of an anchor, a product image, and a control image indicating desired interaction.  The generated video frames showcasing the AnchorCrafter's animation capabilities are displayed as output; (a) highlights how the same anchor interacts with different products, (b) illustrates various anchors demonstrating interactions with the same product.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Intro/mimicmotion.png", "caption": "(b) AnimateAnyone", "description": "The figure shows the results of applying the AnimateAnyone method. AnimateAnyone is a method for controlling the human appearance while generating a video. In this figure, the method fails to generate hand-object interactions and treats the object as a static texture which is part of the person, leading to a lack of movement in the video.  This contrasts with the AnchorCrafter method, which successfully animates human-object interactions.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Intro/ours.png", "caption": "(c) MimicMotion", "description": "The figure shows qualitative results comparing the proposed AnchorCrafter model with other state-of-the-art methods in generating anchor-style product promotion videos. It specifically focuses on the results obtained using the MimicMotion method. The results illustrate that MimicMotion, while generating high-quality pose-driven human images, struggles with proper object handling. It often treats objects as static elements attached to clothing or the background, failing to dynamically represent the human-object interactions essential for anchor-style videos.", "section": "5. Main Results"}, {"figure_path": "https://arxiv.org/html/2411.17383/x2.png", "caption": "(d) Ours", "description": "This figure shows results of the proposed AnchorCrafter model for generating anchor-style product promotion videos.  It showcases a comparison to other methods, highlighting the superior performance of AnchorCrafter in preserving object appearance and generating realistic human-object interactions. The video frames show an anchor interacting with a product, demonstrating the model's ability to seamlessly integrate the product into the anchor's movements and maintain visual fidelity.", "section": "5. Main Results"}, {"figure_path": "https://arxiv.org/html/2411.17383/x3.png", "caption": "Figure 2: \nAnimating a reference image of a peron holding an object, existing methods accurately follow human poses but fail to generate hand-object interaction (AnimateAnyone\u00a0[14]) or misinterpret the object as part of the human, leading to no movement (MimicMotion\u00a0[41]). Our method accurately generates human-object interactions while preserving the object\u2019s appearance.", "description": "Figure 2 demonstrates the limitations of existing human animation methods (AnimateAnyone and MimicMotion) when dealing with human-object interaction. While these methods accurately reproduce human poses from a reference image, they fail to realistically animate the interaction between the hand and the held object. AnimateAnyone generates a hand that does not interact with the object and MimicMotion treats the object as part of the human, resulting in no movement of the object.  In contrast, the AnchorCrafter method successfully generates a video with accurate human-object interaction while preserving the object's visual fidelity and natural appearance.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.17383/x4.png", "caption": "Figure 3: \nTraining pipeline for AnchorCrafter: Based on a video diffusion model, AnchorCrafter injects human and multi-view object references into the video via HOI-appearance perception. The motion is controlled through HOI-motion injection, with the training objective reweighted in the HOI region.", "description": "AnchorCrafter, a video generation framework, is trained using a video diffusion model.  The training process incorporates human and multiple-view object references via a module called HOI-appearance perception to ensure realistic object representation.  Human-object interaction motion is controlled using a separate module called HOI-motion injection.  Finally, a reweighting loss function focuses the training on the human-object interaction regions, improving the accuracy of these key areas in the generated video.", "section": "4. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Ablation/noadapter_3.png", "caption": "Figure 4: \nHOI-appearance perception: The feature of the target object fOsubscript\ud835\udc53\ud835\udc42f_{O}italic_f start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT is extracted through multi-view object feature fusion and combined with the human reference feature fHsubscript\ud835\udc53\ud835\udc3bf_{H}italic_f start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT within a human-object dual adapter to achieve improved disentanglement results.", "description": "Figure 4 details the HOI-appearance perception module.  Multi-view object reference images are fed into a pre-trained DINO model to extract features, which are then fused to create a comprehensive object representation (fO). This object feature is combined with human reference features (fH) extracted from a CLIP model within a human-object dual adapter. This adapter is designed to prevent the entanglement of human and object appearances during video generation, resulting in more accurate and realistic depictions of the object.", "section": "4.2. HOI-Appearance Perception"}, {"figure_path": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Ablation/noobj.png", "caption": "Figure 5: \nQualitative comparisons with other methods. Different colored squares highlight the different types of generation artifacts. In the results of MimicMotion and AnimateAnyone, the objects fail to maintain their appearance and cannot move in sync with the hands, while AnyV2V generates apparent artifacts in the edited videos. The combined results of AnyDoor and MimicMotion demonstrate a lack of preservation in object details.", "description": "Figure 5 presents a qualitative comparison of AnchorCrafter with four state-of-the-art methods: MimicMotion, AnimateAnyone, AnyV2V, and AnyDoor+MimicMotion. Each method is evaluated on its ability to generate videos with realistic human-object interactions, focusing on the preservation of object appearance and synchronized motion between the human and the object. The figure highlights the generation artifacts of each method using different colored squares: red indicates inadequate object appearance, orange indicates flawed hand movements, and green indicates unrealistic anchor behavior. The results show that MimicMotion and AnimateAnyone fail to maintain the objects' appearance and their movements do not sync well with the hands. AnyV2V produces noticeable artifacts in the edited videos. The combination of AnyDoor and MimicMotion struggles to maintain the object's details. In contrast, AnchorCrafter shows superior performance in terms of visual fidelity, object appearance, and synchronized object-hand interactions.", "section": "5. Main Results"}, {"figure_path": "https://arxiv.org/html/2411.17383/x5.png", "caption": "(a) w/o Dual Adapter", "description": "This figure shows the results of an ablation study on the HOI-Appearance Perception module. Specifically, it demonstrates the impact of removing the Human-Object Dual Adapter component from the AnchorCrafter system. The image showcases a comparison of video generation results with and without the Dual Adapter, highlighting the role of this module in maintaining consistency and avoiding entanglement between the appearance of human subjects and the objects they interact with.", "section": "5.3. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Ablation/mouse_nohand.png", "caption": "(b) w/o Multi-View Obj.", "description": "This figure shows the ablation study result when removing the multi-view object feature fusion module.  The results demonstrate the impact of this module on object appearance and its importance for accurate object representation in the generated videos.", "section": "5.3. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Ablation/mouse_noloss.png", "caption": "(c) Ours", "description": "This figure shows the results of the proposed AnchorCrafter model in comparison to existing methods for generating videos of a person interacting with objects.  The images illustrate the model's ability to accurately render realistic and natural-looking interactions. It showcases the superior quality of the generated videos in terms of appearance and motion consistency, highlighting the preservation of both the human and object details during interaction. The figure demonstrates AnchorCrafter's ability to generate high-fidelity human-object interaction videos while maintaining consistency in both human appearance and motion.", "section": "5. Main Results"}, {"figure_path": "https://arxiv.org/html/2411.17383/x6.png", "caption": "(d) w/o 3D Hand Mesh", "description": "This figure shows the ablation study result of removing 3D hand mesh information from the HOI-motion injection module.  It visually demonstrates the impact of this component on the overall quality of the generated videos, specifically highlighting how removing this information affects the accuracy and realism of the generated hand movements in relation to the object.", "section": "5.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.17383/x7.png", "caption": "(e) w/o Re. Loss", "description": "This figure shows the qualitative results of the ablation study on the HOI-Region Reweighting Loss.  It visually compares the video generation results when this loss is excluded from the training process. The absence of the loss affects the learning of object details, especially in hand-object interaction regions.  The differences in object appearance and interaction quality between the model trained with and without the reweighting loss are highlighted.", "section": "5.3. Ablation Study"}]