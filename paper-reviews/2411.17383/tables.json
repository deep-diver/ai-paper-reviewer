[{"content": "| Method | FID\u2193 | FVD\u2193 | FID-VID\u2193 | Obj-IoU\u2191 | Obj-CLIP\u2191 | LMD (Hand)\u2193 | LMD (Body)\u2193 | Subj-Cons\u2191 | Back-Cons\u2191 |\n|---|---|---|---|---|---|---|---|---|---| \n| AnyV2V | 234.1 | 2873.3 | 53.1 | 0.241 | 0.744 | 94.6 | 30.5 | 68.2 | 77.9 |\n| MimicMotion+AnyDoor | 167.8 | 1668.9 | 37.7 | 0.647 | 0.863 | 13.2 | 26.0 | 93.4 | 90.7 |\n| AnimateAnyone | 172.8 | 2267.0 | 24.2 | 0.361 | 0.832 | 23.2 | 41.5 | 94.9 | 94.1 |\n| MimicMotion | **138.1** | 1444.9 | 22.3 | 0.411 | 0.876 | 12.1 | **24.3** | 96.3 | 93.9 |\n| Ours | 141.7 | **736.5** | **15.0** | **0.848** | **0.919** | **11.7** | 25.7 | **97.4** | **95.3** |\n| w/o Human-Object Dual Adapter | 170.5 | 913.0 | 24.2 | 0.802 | 0.886 | 12.1 | 20.9 | 96.4 | 94.9 |\n| w/o Multi-View Object Feature Fusion | 177.0 | 1371.9 | 52.0 | 0.845 | 0.908 | 12.0 | 24.6 | 97.2 | 95.3 |\n| w/o 3D Hand Mesh | 164.0 | 920.1 | 21.3 | 0.847 | 0.907 | 12.0 | 22.3 | 97.3 | 95.0 |\n| w/o HOI-Region Reweighting Loss | 164.7 | 807.3 | 69.9 | 0.846 | 0.895 | 11.8 | 22.0 | 97.0 | 94.8 |", "caption": "Table 1: Quantitative results of our method compared with SOTAs and ablation studies. Our method significantly outperforms existing approaches in terms of numerical performance for spatial movement and appearance preservation of objects, while also matching or exceeding current methods in image and video quality, as well as human pose control capability. Subject consistency(Subj-Cons) and background consistency(Back-Cons) are percentages.", "description": "This table presents a quantitative comparison of the proposed AnchorCrafter model against state-of-the-art (SOTA) methods and ablation studies.  The metrics used assess various aspects of video generation quality, including image fidelity (FID, FID-VID, FVD), object detection accuracy (Object-IoU, Object-CLIP), and human pose consistency (LMD for hands and body).  The results demonstrate AnchorCrafter's superior performance in terms of numerical scores related to object spatial movement and appearance preservation, while achieving comparable or better results in image and video quality, and human pose control compared to existing methods.  Finally, the table also shows the consistency scores for subject and background, which are expressed as percentages.", "section": "5. Experiments"}, {"content": "| Method | Appearance |  | Motion |  | Overall |\n|---|---|---|---|---|---|---|\n| AnyV2V [19] | 1.60 | 1.86 | 1.06 | 1.40 | 1.06 |\n| MimicMotion+AnyDoor [6] | 3.13 | 2.33 | 2.53 | 2.06 | 2.13 |\n| AnimateAnyone [14] | 1.81 | 2.13 | 1.61 | 1.72 | 1.95 |\n| MimicMotion [41] | 3.93 | 3.20 | 4.06 | 2.80 | 2.80 |\n| Ours | **4.00** | **4.26** | **4.53** | **4.60** | **4.47** |", "caption": "Table 2: User study scores. The rating score is on a scale from one to five, where five is the highest score, and one is the lowest.", "description": "This table presents the results of a user study evaluating the performance of different video generation methods.  Fifty participants rated 30 videos generated by each method across five criteria: appearance preservation for humans and objects, motion accuracy for humans and objects, and overall video quality. Each criterion was rated on a five-point Likert scale (1-5), with 5 being the highest score and 1 the lowest.  The table shows the average scores for each method across all criteria, providing a quantitative comparison of their user-perceived quality.", "section": "5. Experiments"}]