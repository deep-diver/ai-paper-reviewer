[{"content": "| Methods | Params (B) | # Data (M) | GenEval \u2191 | \n|---|---|---|---| \n| LUMINA-Next [72] | 2.0 | 14 | 0.46 | \n| PixArt-\u03a3 [9] | 0.6 | 33 | 0.54 | \n| SDXL [41] | 2.6 | \u2013 | 0.55 | \n| PlayGroundv2.5 [25] | 2.6 | \u2013 | 0.56 | \n| IF-XL | 5.5 | 1200 | 0.61 | \n| SD3-medium [15] | 2.0 | 1000 | 0.62 | \n| Hunyuan-DiT [28] | 1.5 | \u2013 | 0.63 | \n| DALLE3 | \u2013 | \u2013 | 0.67 | \n| FLUX-dev | 12.0 | \u2013 | 0.67 | \n| FLUX-schnell | 12.0 | \u2013 | **0.71** | \n| OneDiffusion | 2.8 | 75 | _0.65_ | ", "caption": "Table 1: Comparison of text-to-image performance on the GenEval benchmark at a resolution of 1024 \u00d7\\times\u00d7 1024.", "description": "This table presents a comparison of different text-to-image models' performance on the GenEval benchmark.  The benchmark assesses the quality of generated images from text prompts.  The comparison includes various metrics, such as the GenEval score,  a measure of the overall image quality and alignment with the text prompt.  The table also provides information on the number of parameters (in billions) used by each model and the size of the training dataset (in millions of images). All results are evaluated at a resolution of 1024 x 1024 pixels.", "section": "5.1 Text-to-Image"}, {"content": "| Model | Condition | PSNR \u2191 |\n|---|---|---|\n| Zero123 [32] | 1-view | 18.51 |\n| Zero123-XL [12] | 1-view | 18.93 |\n|  | 1-view | 20.24 |\n| EscherNet [24] | 2-view | 22.91 |\n|  | 3-view | 24.09 |\n|  | 1-view | 19.01 |\n|  | 2-view (unknown poses) | 19.83 |\n| OneDiffusion | 2-view (known poses) | 20.22 |\n|  | 3-view (unknown poses) | 20.64 |\n|  | 3-view (known poses) | 21.79 |", "caption": "Table 2: Comparison of NVS metrics across different number of condition view settings. Increasing the number of condition views improves the reconstruction quality.", "description": "This table presents a comparison of the quality of multi-view image generation, as measured by the NVS (Normalized View Synthesis) metric, under different numbers of input views.  The results demonstrate that increasing the number of input views used for conditioning improves the accuracy and quality of the generated multi-view images. This suggests that incorporating more contextual information from multiple perspectives enhances the model's ability to synthesize a more realistic and coherent multi-view representation.", "section": "5.3. Multiview Generation"}, {"content": "| Method | ID \u2191 | CLIP-T \u2191 |\n|---|---|---|\n| PhotoMaker [27] | 0.193 | 27.38 |\n| InstantID [58] | 0.648 | 26.41 |\n| PuLID [21] | **0.654** | **31.23** |\n| Ours | 0.283 | 26.80 |", "caption": "Table 3: Quantitative results on Unsplash-50.", "description": "Table 3 presents a quantitative evaluation of ID customization performance using the Unsplash-50 benchmark dataset.  The table compares the performance of OneDiffusion against three state-of-the-art methods (PhotoMaker, InstantID, and PuLID) across two metrics: ID preservation (ID\u2191) and overall image quality assessed using CLIP (CLIP-T\u2191).  Higher scores indicate better performance in both identity preservation and image quality.", "section": "5.4 ID Customization"}, {"content": "| Method | NYUv2 AbsRel \u2193 | NYUv2 \u03b4\u2081 \u2191 | DIODE AbsRel \u2193 | DIODE \u03b4\u2081 \u2191 |\n|---|---|---|---|---|\n| DiverseDepth [64] | 11.7 | 87.5 | 37.6 | 63.1 |\n| MiDaS [44] | 11.1 | 88.5 | 33.2 | 71.5 |\n| DPT [44] | 9.8 | 90.3 | **18.2** | 75.8 |\n| LeReS [65] | 9.0 | 91.6 | 27.1 | 76.6 |\n| Omnidata [14] | 7.4 | 94.5 | 33.9 | 74.2 |\n| HDN [66] | 6.9 | 94.8 | 24.6 | **78.0** |\n| Marigold [22] | **6.0** | **95.9** | 31.0 | 77.2 |\n| DepthAnything-2 [62] | **4.6** | **97.7** | 27.1 | 74.8 |\n| Ours | 6.8 | 95.2 | 29.4 | 75.2 |", "caption": "Table 4: Comparison of depth estimation methods on NYUv2 and DIODE datasets.", "description": "Table 4 presents a quantitative comparison of various depth estimation methods on two benchmark datasets: NYUv2 and DIODE.  The table evaluates the performance of these methods using standard metrics like absolute relative error (AbsRel) and \u03b4i. Lower AbsRel values indicate better accuracy, while higher \u03b4i values suggest better robustness and precision.  This comparison helps to assess the relative strengths and weaknesses of each method in terms of depth prediction accuracy, allowing for a detailed analysis of their performance on both indoor (NYUv2) and outdoor (DIODE) scenes.", "section": "5.5. Depth Estimation"}, {"content": "| Method | Accuracy |\n|---|---| \n| RayDiffusion [67] | 0.20 |\n| OneDiffusion | 0.32 |", "caption": "Table 5: Comparison of zero-shot camera pose estimation methods on the GSO dataset, evaluated by Camera Center Accuracy at a threshold of 0.3.", "description": "This table presents a comparison of the performance of different zero-shot camera pose estimation methods on the Google Scanned Objects (GSO) dataset.  Zero-shot signifies that the models were not specifically trained on the GSO dataset but rather on other datasets and then evaluated on GSO. The performance metric used is Camera Center Accuracy, calculated at a threshold of 0.3. This accuracy score measures how precisely the estimated center of the camera aligns with the actual center, with higher values indicating greater accuracy.  The table allows for a direct comparison of the effectiveness and robustness of various camera pose estimation models when applied to unseen data.", "section": "5.6. Camera Pose Estimation"}]