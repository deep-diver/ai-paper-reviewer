[{"figure_path": "https://arxiv.org/html/2411.16318/x2.png", "caption": "Figure 1: \nOneDiffusion is a unified diffusion model designed for both image synthesis and understanding across diverse tasks.\nIt supports text-to-image generation (red box), conditional image generation from input images (orange box) and it\u2019s reverse task Image understanding (orange box). It can also perform ID customization (blue box), and multi-view generation (purple box) with arbitrary number of input and output images.", "description": "Figure 1 showcases OneDiffusion, a unified diffusion model capable of both image generation and understanding across various tasks.  The figure visually demonstrates OneDiffusion's capabilities through a grid of example images, each illustrating a different task.  These tasks include text-to-image synthesis (top-left quadrant, highlighted in red), where an image is generated from a text prompt; conditional image generation from input images (central area, orange boxes),  such as generating images from a sketch, edge map, or pose; the inverse of conditional generation: image understanding (central area, orange boxes),  such as depth or semantic map estimation from an input image; ID customization (bottom-left quadrant, blue boxes), where images are altered to represent specific individuals; and multi-view generation (bottom-right quadrant, purple boxes), where multiple views of a single object or scene are generated from a single input image or image set.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.16318/x3.png", "caption": "Figure 2: Illustration of training and inference pipeline for OneDiffusion. We encode the desired task for each sample via a special task token. During training we independently sample different diffusion timesteps for each view and add noise to them accordingly. In inference, we replace input image(s) with Gaussian noises while setting timesteps of conditions to 00.", "description": "Figure 2 illustrates the training and inference processes of the OneDiffusion model.  The model uses a special task token to identify the task for each sample. During training, the model independently samples different diffusion timesteps for each 'view' (representing an image or other input modality) and adds noise accordingly. The training objective is to learn a generalized time-dependent vector field that maps the noisy views back to their original, clean versions.  Inference differs by substituting input images with Gaussian noise and setting the timesteps of the conditioning views to 0, which generates output views by applying the learned vector field. This unified approach allows OneDiffusion to handle diverse generation and image understanding tasks using a consistent framework.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.16318/x4.png", "caption": "Figure 3: High-resolution samples from text of our OneDiffusion model, showcasing its capabilities in precise prompt adherence, attention to fine details, and high image quality across a wide variety of styles.", "description": "Figure 3 showcases the high-resolution image generation capabilities of the OneDiffusion model.  The images demonstrate the model's ability to accurately reflect the textual prompts given to it, paying close attention to even fine details within the images.  The variety of styles represented in the generated images highlights the model's versatility and capacity to produce high-quality outputs across diverse artistic aesthetics.", "section": "5.1 Text-to-Image"}, {"figure_path": "https://arxiv.org/html/2411.16318/x5.png", "caption": "Figure 4: Illustration of our model capability to generate HED, depth, human pose, semantic mask, and bounding box from input image. For semantic segmentation, we segment the  sword (highlighted in yellow) and the moon (highlighted in cyan) the first example, while segmenting  road (yellow), sky (cyan) in the second. For object detection, We localize the head and moon (both highlighted in cyan). Leveraging these conditions, we can reverse the process to recreate a variant of the input image based on the same caption. Additionally, we can edit the image by modifying specific elements, such as replacing the moon with Saturn (last example).", "description": "Figure 4 demonstrates the OneDiffusion model's ability to perform various image understanding tasks by generating different visual representations from a single input image.  The model generates high dynamic range (HDR) edge detection, depth maps, human pose estimation, semantic segmentation masks, and object detection bounding boxes.  Examples of semantic segmentation are provided, showcasing the model's ability to segment specific objects like a sword and moon, and then a road and sky in separate examples. Similarly, object detection examples show localization of a head and moon. Importantly, the figure shows the model can reverse these processes, reconstructing an image based on a specific understanding task. Furthermore, it highlights the capacity to modify image elements, such as replacing the moon with Saturn in one example.", "section": "5. Controllable Image Generation"}, {"figure_path": "https://arxiv.org/html/2411.16318/x6.png", "caption": "Figure 5: Illustration of the multiview generation with single input image. We equally slice the azimuth in range of [\u221245,60]4560[-45,60][ - 45 , 60 ] and elevation in range of [\u221215,45]1545[-15,45][ - 15 , 45 ] for the left scenes. For the right scene, the azimuth range is set to [0;360]0360[0;360][ 0 ; 360 ] and elevation range is set to [\u221215;15]1515[-15;15][ - 15 ; 15 ].", "description": "This figure demonstrates the model's capacity for multi-view image generation using a single input image.  The top row showcases the results of generating multiple views of various objects from a single input image. The azimuth angle is varied from -45 to 60 degrees, and the elevation is varied from -15 to 45 degrees for the objects in the left column. A wider range of azimuth (0-360 degrees) and a narrower range of elevation (-15 to 15 degrees) were used for the objects in the right column. This illustrates the model's ability to produce realistic and consistent views from diverse viewpoints and angles, showing its capability to understand 3D spatial information.", "section": "5.3. Multiview Generation"}, {"figure_path": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/depth_comparison.jpg", "caption": "Figure 6: \nIllustration of ID customization using reference images. Unlike prior methods that rely on face embeddings and often fail to generalize, our model demonstrates superior generalization. It effectively adjusts facial expressions and gaze directions (first row), changes viewpoints (second row), and even customizes non-human IDs (third row). All results in the third row are generated from a single reference image, while InstantID fails as its face detector cannot detect faces in the input.", "description": "Figure 6 showcases the capabilities of OneDiffusion in identity customization. Unlike previous methods that heavily rely on face embeddings and struggle with generalization, OneDiffusion excels by effectively modifying facial expressions, gaze direction, and viewpoints, all from a single reference image.  The figure demonstrates this ability across diverse subjects, including both human and non-human, highlighting its superior generalization capacity.  The third row specifically illustrates successful customization of non-human subjects with a single reference image, a task InstantID fails on due to its face detection limitations.", "section": "5.4 ID Customization"}, {"figure_path": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/poses.png", "caption": "Figure 7: Qualitative comparison between a diffusion-based depth estimation - Marigold [22] and our methods.", "description": "Figure 7 presents a qualitative comparison of depth estimation results between the Marigold method (a diffusion-based approach) and the OneDiffusion model introduced in the paper.  The comparison highlights the relative strengths and weaknesses of each method by visually showcasing their outputs on several example images. The images demonstrate the capability of each model to accurately estimate depth information in various scenarios, including different lighting conditions, object textures, and scene complexities.  By comparing the results side-by-side, the figure allows the reader to assess the performance differences and evaluate the effectiveness of OneDiffusion's approach to depth estimation.", "section": "5.5. Depth Estimation"}, {"figure_path": "https://arxiv.org/html/2411.16318/x7.png", "caption": "Figure 8: Qualitative comparison between RayDiffusion and OneDiffusion on GSO dataset. OneDiffusion yields better prediction.", "description": "Figure 8 presents a qualitative comparison of camera pose estimation results between RayDiffusion and OneDiffusion using the Google Scanned Objects (GSO) dataset.  The figure visually demonstrates that OneDiffusion achieves more accurate pose predictions compared to RayDiffusion.  This improved accuracy is likely due to OneDiffusion's more extensive training dataset, which includes diverse view angles and scenarios, resulting in better generalization.", "section": "5.3. Multiview Generation"}, {"figure_path": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/onediffusion_appendix_multiview.jpg", "caption": "Figure 9: Distribution of training datasets for all tasks. Segments proportional to sampling rates. The inner section shows the super-category of target tasks, it can be observed that we train the model with equal budget for text-to-image, image-to-image and multiview generation. The outer section shows datasets used for each super-category.", "description": "Figure 9 is a pie chart visualizing the distribution of datasets used to train the OneDiffusion model. The chart is divided into an inner section representing the main task categories (text-to-image, image-to-image, and multiview generation) and an outer section detailing the specific datasets within each category.  The inner section clearly shows that the model was trained with an equal proportion of data from each of the three main task categories. The outer section provides a breakdown of the individual datasets used for each category, illustrating the diversity and scale of the training data used to develop OneDiffusion. The datasets are color-coded for clarity and easy identification.", "section": "4. One-Gen Datasets"}, {"figure_path": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/onediffusion_appendix_multiview_2.jpg", "caption": "Figure 10: Qualitative results of image-to-multiview generation. The left most images are input. We equally slice the azimuth in range of [\u221245,60]4560[-45,60][ - 45 , 60 ] and elevation in range of [\u221215,45]1545[-15,45][ - 15 , 45 ] for all scenes.", "description": "Figure 10 showcases the OneDiffusion model's capacity for image-to-multiview generation.  The process starts with a single input image (shown on the far left of each row). From this single image, the model generates multiple views of the same scene, systematically varying the camera's azimuth (horizontal angle) and elevation (vertical angle).  The azimuth ranges from -45 to 60 degrees, and the elevation ranges from -15 to 45 degrees. These angles are evenly spaced across the generated views within each row. The figure demonstrates the model's ability to produce consistent and realistic-looking views of the same subject from different perspectives.", "section": "5.3 Multiview Generation"}, {"figure_path": "https://arxiv.org/html/2411.16318/x8.png", "caption": "Figure 11: Qualitative results of image-to-multiview generation. We equally slice the azimuth in range of [\u221245,60]4560[-45,60][ - 45 , 60 ] and elevation in range of [\u221215,45]1545[-15,45][ - 15 , 45 ] for the first 3 scenes. For the last scene, the azimuth range is set to [0;360]0360[0;360][ 0 ; 360 ] and elevation range is set to [\u221215;15]1515[-15;15][ - 15 ; 15 ].", "description": "Figure 11 showcases the model's capability to generate multiple views of a scene from a single input image. The process simulates different camera angles by systematically varying the azimuth (horizontal angle) and elevation (vertical angle) within specified ranges. For the first three examples, the azimuth ranges from -45 to 60 degrees, and the elevation ranges from -15 to 45 degrees.  These ranges are evenly divided to create the multiple views shown. The final example demonstrates a different approach, where the azimuth spans the entire 360-degree circle, and the elevation ranges from -15 to 15 degrees. This figure demonstrates the versatility of the model to generate consistent views from various viewpoints, showcasing the model's understanding of 3D scene geometry and its ability to render those scenes realistically from various perspectives.", "section": "5.3. Multiview Generation"}, {"figure_path": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/onediffusion_appendix_faceid.jpg", "caption": "Figure 12: Qualitative results of text-to-multiview generation. The azimuth and elevation of left to right columns are [0,30,60,90]0306090[0,30,60,90][ 0 , 30 , 60 , 90 ] and [0,10,20,30]0102030[0,10,20,30][ 0 , 10 , 20 , 30 ], respectively. We use following prefix for all prompts to improve the quality and realism of generated images: \u201cphotorealistic, masterpiece, highly detail, score_9, score_8_up\u201d.", "description": "This figure showcases the model's ability to generate multi-view images from text prompts.  The images demonstrate the model's capacity to create photorealistic and detailed scenes with varied viewpoints. Four different camera angles (azimuth: 0, 30, 60, 90 degrees; elevation: 0, 10, 20, 30 degrees) are shown for each scene.  All prompts used a standardized prefix (\u201cphotorealistic, masterpiece, highly detail, score_9, score_8_up\u201d) to enhance image quality and realism.", "section": "5.1 Text-to-Image"}, {"figure_path": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/onediffusion_appendix_faceid_3.jpg", "caption": "Figure 13: Qualitative results of OneDiffusion for (single reference) ID Customization task with photo of human faces. The left most images are input, target prompts for left to right columns are: 1) \u201cPhoto of a man/woman wearing suit at Shibuya at night. He/She is looking at the camera\u201d, 2) \u201cpixarstyle, cartoon, a person in pixar style sitting on a crowded street\u201d, 3) \u201cwatercolor drawing of a man/woman with Space Needle in background\u201d", "description": "This figure showcases the ID Customization capabilities of the OneDiffusion model.  It demonstrates the model's ability to generate diverse variations of a person's image from a single reference photo, all while maintaining consistent identity. Three different prompts were used, each generating multiple images in a consistent style:\n\n1. **Realistic Style:** \"Photo of a man/woman wearing a suit at Shibuya at night. He/She is looking at the camera.\"  This prompt generates realistic images maintaining the identity of the person in the reference image but modifying the setting, lighting and pose.\n2. **Pixar Style:** \"pixarstyle, cartoon, a person in pixar style sitting on a crowded street.\"  This prompt generates cartoon-style images reminiscent of Pixar films, changing the style while preserving the individual's identity.\n3. **Watercolor Style:** \"watercolor drawing of a man/woman with Space Needle in background.\" This prompt generates stylized images in a watercolor painting style, altering the artistic style and adding a background element (Space Needle) while still maintaining identity consistency. The leftmost column shows the input reference images used for each style generation.", "section": "5.4 ID Customization"}, {"figure_path": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/onediffusion_appendix_depth.jpg", "caption": "Figure 14: Qualitative results of OneDiffusion for (single reference) ID Customization task with photo of of non-human subjects or cartoon style input. OneDiffusion is highly versatile and can produce good results for all kind of input and not limited to photorealistic human images. Since we rely on attention, the model can attend to the condition view and preserve intricate details and is not limited by any bottleneck e.g. latent representation.", "description": "Figure 14 presents qualitative results demonstrating OneDiffusion's ability to perform identity customization using a single reference image.  The examples showcase its versatility by handling diverse input types, including non-human subjects and cartoon-style images.  The model adeptly generates outputs maintaining fine details and stylistic features, demonstrating that it doesn't depend on specific limitations like face embeddings or other architectural bottlenecks.  OneDiffusion's ability to preserve intricate details stems from its use of attention mechanisms, enabling it to effectively use the reference image to condition the generation process.", "section": "5.4 ID Customization"}, {"figure_path": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/onediffusion_appendix_depth_2.jpg", "caption": "Figure 15: Qualitative comparison for depth estimation between OneDiffusion, Marigold [22] and DepthAnything-v2 [62]", "description": "This figure presents a qualitative comparison of depth estimation results obtained using three different methods: OneDiffusion, Marigold [22], and DepthAnything-v2 [62].  For several example images, it shows the original image alongside the depth maps produced by each method. This allows for a visual assessment of the accuracy and quality of depth estimation achieved by each approach, highlighting their relative strengths and weaknesses in handling various scenes and object types. The comparison helps illustrate the performance of OneDiffusion in relation to existing state-of-the-art depth estimation techniques.", "section": "5.5. Depth Estimation"}, {"figure_path": "https://arxiv.org/html/2411.16318/x9.png", "caption": "Figure 16: Qualitative comparison for depth estimation between OneDiffusion, Marigold [22] and DepthAnything-v2 [62]", "description": "Figure 16 presents a qualitative comparison of depth estimation results obtained using three different methods: OneDiffusion (the proposed model), Marigold [22], and DepthAnything-v2 [62].  The figure displays several example images alongside their corresponding depth maps generated by each method. This visual comparison allows for an assessment of the accuracy, detail, and overall quality of depth estimation achieved by each technique, particularly highlighting how well each model handles various image complexities and object types.  The purpose is to demonstrate the relative strengths and weaknesses of OneDiffusion in comparison to existing state-of-the-art approaches for depth estimation.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16318/x10.png", "caption": "Figure 17: Qualitative examples of human pose estimation on COCO datasets.", "description": "This figure showcases several examples of human pose estimation results obtained using the OneDiffusion model on images from the COCO dataset.  It visually demonstrates the model's capacity to accurately identify and locate key body joints (such as shoulders, elbows, wrists, hips, knees, and ankles) in diverse poses and scenarios, highlighting the robustness and accuracy of the pose estimation capabilities of OneDiffusion.", "section": "5.6 Camera Pose Estimation"}, {"figure_path": "https://arxiv.org/html/2411.16318/x11.png", "caption": "Figure 18: Qualitative examples of semantic segmentation on COCO datasets. The target class for each image (from left to right, from top to bottom) are\n(sheep, grass, mountain, sky),\n(apple, person, building),\n(vase, flower, ),\n(dog, book, sheet),\n(umbrella, person, building, gate),\n(boat, dock, drum).", "description": "Figure 18 showcases the model's semantic segmentation capabilities on images from the COCO dataset.  Each row presents a different image with its corresponding segmentation mask. The masks highlight specific object classes as listed in the caption: Row 1: sheep, grass, mountain, sky; Row 2: apple, person, building; Row 3: vase, flower; Row 4: dog, book, sheet; Row 5: umbrella, person, building, gate; Row 6: boat, dock, drum. The figure demonstrates the model's ability to accurately identify and segment diverse objects and scenes within complex images.", "section": "Additional Qualitative results"}]