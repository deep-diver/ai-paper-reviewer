[{"figure_path": "https://arxiv.org/html/2411.07184/x1.png", "caption": "Figure 1: SAMPart3D is able to segment any 3D object into semantic parts across multiple levels of granularity, without the need for predefined part label sets or text prompts. It supports a range of applications, including part-level editing and interactive segmentation.", "description": "This figure showcases the capabilities of SAMPart3D in segmenting 3D objects into their constituent parts at various levels of detail.  The examples demonstrate that the model can accurately identify and delineate semantic parts (e.g., a chair's legs, seat, and back) without requiring predefined labels or textual descriptions. The segmentation is robust across different levels of granularity (e.g., from coarse divisions of an object into a few major parts to fine-grained segmentation of individual components). The figure highlights the versatility of SAMPart3D, showing its application in part-level editing and interactive segmentation workflows.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.07184/x2.png", "caption": "Figure 2: An overview pipeline of SAMPart3D. (a) We first pre-train 3D backbone PTv3-object on 3D large-scale data Objaverse, distilling visual features from FeatUp-DINOv2. (b) Next, we train light-weight MLPs to distill 2D masks to scale-conditioned grouping. (c) Finally, we cluster the feature of point clouds and highlight the consistent 2D part area with 2D-3D mapping on multi-view renderings, and then query semantics from MLLMs.", "description": "This figure illustrates the three main stages of the SAMPart3D pipeline. (a) A 3D backbone network (PTv3-object) is pre-trained on the large-scale Objaverse dataset using visual features distilled from FeatUp-DINOv2. This stage aims to learn rich 3D representations from unlabeled data. (b) Lightweight MLPs are trained to distill 2D segmentation masks from SAM (Segment Anything Model), enabling scale-conditioned grouping of 3D points. This stage introduces flexibility to handle various levels of granularity in part segmentation. (c) Finally, the 3D points are clustered to form parts. The consistent 2D regions from multi-view renderings are highlighted and mapped to the 3D parts, which are further assigned semantic labels using Multimodal Large Language Models (MLLMs).  This last stage ensures that the segmented parts are semantically meaningful.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.07184/x3.png", "caption": "Figure 3: Visualization of PartObjaverse-Tiny with part-level semantic and instance segmentation labels.", "description": "This figure showcases examples from the PartObjaverse-Tiny dataset, highlighting both semantic and instance-level part segmentations.  It visually demonstrates the detailed annotations included in the dataset, showing how objects are divided into their constituent parts with both semantic labels (describing the part's function, e.g., 'wheel', 'seat') and instance labels (identifying individual parts, e.g., 'left wheel', 'right wheel'). This provides a clear illustration of the dataset's complexity and the level of detail achieved in its annotations, which are crucial for evaluating the performance of 3D part segmentation models.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.07184/x4.png", "caption": "Figure 4: Visualization of multi-granularity 3D part segmentation on GSO\u00a0[11], OmniObject3D\u00a0[45], Vroid\u00a0[5] and 3D generated meshes.", "description": "Figure 4 presents a visual comparison of the model's multi-granularity 3D part segmentation capabilities. It showcases the results obtained by applying the model to various datasets: GSO [11], OmniObject3D [45], Vroid [5], and 3D-generated meshes. Each dataset provides a distinct set of 3D objects and demonstrates the model's flexibility in handling different types of 3D models and diverse levels of complexity.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.07184/x5.png", "caption": "Figure 5: Qualitative comparison with PartSLIP\u00a0[25] and SATR\u00a0[1] in the semantic segmentation task on the PartObjaverse-Tiny dataset.", "description": "Figure 5 presents a qualitative comparison of semantic segmentation results on the PartObjaverse-Tiny dataset, comparing the proposed method with two existing methods: PartSLIP and SATR.  It visually demonstrates the differences in performance by showcasing example segmentations of various objects.  The figure offers a side-by-side comparison, allowing for a direct visual assessment of accuracy and the ability to segment different object parts.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.07184/x6.png", "caption": "Figure 6: The resulting 3D part segmentation can directly support various applications, including part segmentation controlled by 2D masks, part material editing, part geometry editing, and click-based hierarchical segmentation.", "description": "Figure 6 showcases the versatility of the SAMPart3D model's output.  The 3D part segmentation results, obtained without text prompts or pre-defined part labels, directly enable several applications. (a) shows how user-provided 2D segmentation masks can control the 3D part segmentation. (b) demonstrates part material editing capabilities: different materials can be applied to individual parts. (c) illustrates part shape editing and animation, allowing for modifications and animations of segmented components. Finally, (d) highlights click-based hierarchical segmentation, where the user can interactively segment a 3D object at different levels of granularity by clicking and selecting a scale.", "section": "4. Applications"}, {"figure_path": "https://arxiv.org/html/2411.07184/x7.png", "caption": "Figure 7: Visualization and qualitative comparison of the features encoded by our backbone, DINOv2, and SAM. Due to the utilization of 3D information from point clouds, our backbone can produce more accurate and fine-grained visual semantic features.", "description": "Figure 7 compares the visual features extracted by three different models: the proposed backbone (PTv3-object), DINOv2, and SAM. It demonstrates that incorporating 3D point cloud information into the PTv3-object backbone leads to more precise and detailed visual semantic features compared to the 2D-based models, DINOv2 and SAM.  The visualization highlights the superior quality and granularity of the features learned by the proposed 3D backbone.", "section": "3.2 Sample-specific Fine-tuning: Distilling 2D Masks for Multi-granularity Segmentation"}, {"figure_path": "https://arxiv.org/html/2411.07184/x8.png", "caption": "Figure 8: Visualization of segmentation results on PartNetE dataset.", "description": "This figure visualizes the results of 3D part segmentation on the PartNetE dataset. It showcases the effectiveness of the proposed method (SAMPart3D) in segmenting various objects from the dataset, highlighting its ability to accurately identify and delineate individual parts even in complex 3D shapes. The segmentation is fine-grained and detailed, demonstrating the model's capacity to handle intricate object geometries and various part configurations.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.07184/x9.png", "caption": "Figure 9: Visualization of multi-granularity segmentation of point clouds and meshes.", "description": "This figure shows examples of multi-granularity segmentation results from the SAMPart3D model.  It demonstrates the model's ability to segment 3D objects (represented as point clouds and meshes) into parts at various levels of detail, from coarse to fine-grained. Each row represents a different object, and each column shows the segmentation results at increasing levels of granularity. This showcases the flexibility of SAMPart3D in adapting to different segmentation needs.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.07184/x10.png", "caption": "Figure 10: Visualization of PartObjaverse-Tiny with part-level annotations with semantic labels for segmentation segmentation.", "description": "This figure visualizes a subset of the PartObjaverse-Tiny dataset.  It shows several example 3D objects from the dataset, with their corresponding part-level annotations. Each object is segmented into various parts, and each part is labeled with its semantic name. This dataset is specifically designed to be diverse and complex to fully evaluate the capabilities of 3D part segmentation models.", "section": "4. Experiments"}]