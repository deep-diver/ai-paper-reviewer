[{"figure_path": "https://arxiv.org/html/2503.12689/extracted/6285141/Fig/frame.jpg", "caption": "Figure 1: \nResults of MagicID. Given a few reference images, our method is capable of generating highly realistic and personalized videos that maintain consistent identity features while exhibiting natural and visually appealing motion dynamics.", "description": "This figure showcases the results of the MagicID model.  The model takes a small number of reference images of a person as input and generates a series of short video clips depicting that person in various actions and settings.  The generated videos demonstrate that the MagicID model is able to create highly realistic and personalized videos while maintaining a consistent identity (i.e., the person's appearance remains the same throughout the video clips) and showcasing dynamic, natural-looking movement.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.12689/extracted/6285141/Fig/step.jpg", "caption": "(a) ID Consistency for longer video.", "description": "This figure shows the impact of video length on identity consistency. The x-axis represents the length of generated videos, and the y-axis represents a metric measuring the consistency of the identity.  Different methods are plotted, demonstrating how their performance changes as the video length increases.  It highlights the challenge of preserving identity over longer videos.", "section": "2. Related Works"}, {"figure_path": "https://arxiv.org/html/2503.12689/x2.png", "caption": "(b) Dynamic Degree for more steps.", "description": "The figure shows a graph that illustrates how the dynamic quality of videos generated by different methods changes over the course of training.  It demonstrates that traditional approaches show a significant reduction in dynamic quality (motion) as the training steps increase.  In contrast, the proposed method (Ours) maintains consistently high dynamic quality throughout the training process.", "section": "2. Related Works"}, {"figure_path": "https://arxiv.org/html/2503.12689/x3.png", "caption": "Figure 2: Analysis of identity degratation and dynamic reduction. (a) We compute the mean identity similarity with the reference images for generated videos of different lengths. As shown, traditional approaches suffer from diminished identity consistency as video length increases. In contrast, our method maintains strong identity robustness throughout prolonged video generations. (b) We calculate the dynamic degree for different training steps. As the customization progresses, traditional methods experience a gradual loss of motion dynamic during customization, whereas our method preserves original video dynamics across the entire training.", "description": "This figure analyzes the effects of video length and training duration on identity consistency and dynamic quality in video generation.  In part (a), it shows that as the length of the generated videos increases, traditional methods experience a decline in identity consistency, while the proposed MagicID method consistently maintains a high level of identity preservation. Part (b) demonstrates that traditional methods show a reduction in dynamic quality over time, whereas MagicID preserves the original motion dynamics throughout the training process.", "section": "2. Related Works"}, {"figure_path": "https://arxiv.org/html/2503.12689/x4.png", "caption": "Figure 3: Overview of pairwise preference video data construction. In Step 1, we construct a preference video repository using videos generated by fine-tuned and Initial T2V models, along with static videos derived from reference images. In Step 2, we evaluate each video sequentially based on ID consistency using ID Encoder\u00a0[6], dynamic degree using optical flow\u00a0[13], and prompt following using VLM[2]. In Step 3, we perform Hybrid Pair Selection, first selecting pairs based on ID consistency differences with a pre-defined dynamic threshold to address identity inconsistency, then selecting pairs based on both dynamic and identity to mitigate the dynamic reduction.", "description": "Figure 3 illustrates the process of creating training data for a video generation model.  First, a repository of videos is built using videos generated by both a fine-tuned and an initial Text-to-Video (T2V) model, as well as static images derived from the user's reference images. Next, each video in the repository is evaluated based on three criteria: identity consistency (using an ID Encoder), dynamic degree (using optical flow), and prompt adherence (using a Vision-Language Model). Finally, video pairs are selected for training using a two-step hybrid approach. The first step prioritizes identity preservation by choosing pairs with significantly different identity scores while maintaining acceptable dynamic scores. The second step focuses on enhancing video dynamics by selecting pairs with a balance of both identity and dynamic scores.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.12689/x5.png", "caption": "Figure 4: Qualitative comparison with tuning-based methods. As observed, both Dreambooth and MagicMe suffer from inferior ID fidelity, while our method maintains consistent identity and natural dynamics.", "description": "Figure 4 presents a qualitative comparison of video generation results between MagicID and two other methods: DreamBooth and MagicMe.  DreamBooth and MagicMe both utilize fine-tuning approaches, which, according to the study, negatively impact the generated videos by causing a degradation of identity consistency over longer videos and a reduction in dynamic motion elements.  In contrast, Figure 4 visually demonstrates how MagicID overcomes these shortcomings, creating videos with significantly better maintenance of consistent identity (ID fidelity) and significantly more natural and dynamic movement.", "section": "4. Main Results"}, {"figure_path": "https://arxiv.org/html/2503.12689/x6.png", "caption": "Figure 5: Qualitative comparison with tuning-based methods. As shown, ID-Animator suffers from poor identity consistency and video quality. While ConsisID improves identity fidelity to some extent, it exhibits severe copy-paste artifacts, demonstrating unnatural motion dynamics and text alignment, as seen in the last example with the helmet. In contrast, our method achieves strong performance in identity consistency, motion dynamics, and text alignment, significantly outperforming the baseline approaches.", "description": "Figure 5 presents a qualitative comparison of video generation results between MagicID and three other methods: ID-Animator, ConsisID, and two tuning-based methods (DreamBooth and MagicMe).  The figure visually demonstrates that ID-Animator fails to produce videos with consistent identity and good quality. While ConsisID shows some improvement in identity consistency, it suffers from severe 'copy-paste' artifacts, resulting in unnatural motion and misalignment with the text prompts, particularly noticeable in the last example (a video of a person wearing a helmet). In contrast, MagicID generates high-quality videos with consistent identity, natural motion, and accurate text alignment, surpassing the other methods.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.12689/extracted/6285141/Fig/user.jpg", "caption": "Figure 6: Ablation study for the hybrid pair selection. Compared to the self-reconstruction approach, training with identity-preferred pairs significantly enhances the identity consistency. On the other hand, the incorporation of dynamic-preferred pairs markedly improves the dynamics of the generated outcomes.", "description": "This ablation study investigates the impact of using different types of video pairs for training a video generation model.  The study compares three training strategies: (1) Self-reconstruction (using only a single image for reconstruction); (2) Identity-preferred pairs (prioritizing pairs with significant differences in identity consistency, while allowing some dynamic variability); and (3) Hybrid pairs (combining both identity-preferred and dynamic-preferred pairs to balance identity and motion quality). The results show that using identity-preferred pairs significantly improves identity consistency, while the hybrid approach achieves the best overall results, offering the best balance between consistent identity and natural dynamic motion.", "section": "3.4 Hybrid Pair Selection"}, {"figure_path": "https://arxiv.org/html/2503.12689/x7.png", "caption": "Figure 7: Ablation study for the customized video rewards. While the ID reward encourages the model to learn consistent identity features, the addition of the dynamic reward leads to generated results with significantly improved motion dynamics. Furthermore, incorporating a semantic reward can effectively enhance video dynamics to some extent and improve prompt-following capabilities, such as the prompts involving turning motions.", "description": "This ablation study analyzes the impact of different reward components on video generation quality.  The baseline uses only an identity consistency reward, which prioritizes maintaining the subject's identity across frames. Adding a dynamic reward significantly improves the motion in the generated videos, making them more fluid and natural. Finally, including a semantic reward further enhances the dynamics and improves how well the generated videos match the text prompt, particularly noticeable in actions like turning.", "section": "4.3. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.12689/x8.png", "caption": "Figure 8: User Study.", "description": "This figure presents the results of a user study comparing MagicID against other methods in terms of identity consistency, dynamic quality, text alignment, and overall quality.  The study involved 25 evaluators assessing 40 video sets generated using the different methods and reference images.  The results visually show MagicID's superior performance across all evaluation criteria.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.12689/x9.png", "caption": "Figure 9: More results of MagicID.", "description": "This figure displays further results generated by the MagicID model, showcasing its ability to produce high-fidelity videos that maintain consistent identity and exhibit significant dynamics based on user-provided reference images.  Multiple examples of diverse scenarios are presented, highlighting the model's ability to generate realistic and personalized videos across various contexts.  Each example shows a series of video frames resulting from the same prompt and reference images.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.12689/x10.png", "caption": "Figure 10: More results of MagicID.", "description": "This figure showcases additional results generated by the MagicID model.  It demonstrates the model's ability to generate high-fidelity videos that maintain consistent identity and exhibit natural dynamics across various scenes and actions.  The example shows a man in a tactical stealth suit on a rooftop at night, highlighting the model's capacity to render detailed textures and realistic lighting conditions.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.12689/x11.png", "caption": "Figure 11: More results of MagicID.", "description": "This figure showcases additional results generated by the MagicID model.  It presents a series of short video clips demonstrating the model's ability to generate realistic and dynamically consistent videos of a tribal warrior in a jungle setting, maintaining a consistent identity based on the input reference images while exhibiting natural motion.", "section": "4. Experiments"}]