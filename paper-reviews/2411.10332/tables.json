[{"content": "| Model | Charades-STA R@0.3 | Charades-STA R@0.5 | Charades-STA R@0.7 | Charades-STA mIoU | ActivityNet R@0.3 | ActivityNet R@0.5 | ActivityNet R@0.7 | ActivityNet mIoU | QVHighlights mAP | QVHighlights HIT@1 | \n|---|---|---|---|---|---|---|---|---|---|---|\n| *VTG-Tuned Vid-LLMs* |  |  |  |  |  |  |  |  |  |  |\n| GroundingGPT [31] | - | 29.6 | 11.9 | - | - | - | - | - | - | - |\n| LITA [22] | - | - | - | - | - | 25.9 | - | 28.6 | - | - |\n| VTG-LLM [17] | 52.0 | 33.8 | 15.7 | - | - | - | - | - | 16.5 | 33.5 |\n| TimeChat [44] | 47.7 | 22.9 | 12.5 | 30.6 | 30.2 | 16.9 | 8.2 | 21.8 | 14.5 | 23.9 |\n| VTimeLLM [21] | 51.0 | 27.5 | 11.4 | 31.2 | 44.0 | 27.8 | 14.3 | 30.4 | - | - |\n| Momentor [42] | 42.9 | 23.0 | 12.4 | 29.3 | 42.6 | 26.6 | 11.6 | 28.5 | 7.6 | - |\n| HawkEye [52] | 50.6 | 31.4 | 14.5 | 33.7 | 49.1 | 29.3 | 10.7 | 32.7 | - | - |\n| *General Vid-LLMs* |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o [41] | 55.0 | 32.0 | 11.5 | 35.4 | 33.3 | 21.2 | 10.4 | 23.7 | 39.5 | 68.7 |\n| +NumPro | 57.1 | 35.5 | 13.5 | 37.6 | 45.5 | 30.8 | 18.4 | 33.6 | 40.5 | 70.7 |\n| Qwen2-VL-7B [51] | 8.7 | 5.4 | 2.4 | 7.9 | 17.0 | 9.4 | 3.9 | 12.5 | 21.5 | 42.2 |\n| +NumPro | 60.7 | 36.8 | 15.9 | 38.5 | 44.2 | 26.4 | 14.4 | 31.3 | 23.6 | 43.4 |\n| LongVA-7B-DPO [65] | 22.6 | 10.1 | 2.2 | 14.6 | 11.8 | 5.3 | 1.9 | 8.2 | 14.2 | 20.4 |\n| +NumPro | 27.2 | 10.3 | 2.9 | 18.9 | 20.1 | 10.8 | 5.4 | 15.2 | 15.3 | 24.3 |\n| +NumPro-FT | 63.8 | 42.0 | 20.6 | 41.4 | 55.6 | 37.5 | 20.6 | 38.8 | 25.0 | 37.2 |", "caption": "Table 1: Comparison of performance on the video temporal grounding task with previous state-of-the-art methods. NumPro refers to the use of number prompts for augmentation during inference, while NumPro-FT indicates fine-tuning with the number prompt augmentation instruction dataset. The best results are highlighted in bold, and the second-best are underlined.", "description": "Table 1 compares the performance of various Video Temporal Grounding (VTG) models on two tasks: Moment Retrieval and Highlight Detection.  It contrasts several state-of-the-art (SOTA) models against models enhanced by the NumPro method (with or without fine-tuning). The table presents several evaluation metrics for both tasks, including mIoU, recall@m (at different thresholds), mAP, and HIT@1.  NumPro's impact is shown by comparing the performance of models with and without NumPro integration (training-free) or fine-tuned with the NumPro dataset (NumPro-FT). Best and second-best results are highlighted.", "section": "4.2 Main Results"}, {"content": "Model|Charades-STA|ActivityNet\n---|---|---\nR@0.3|R@0.5|R@0.7|mIoU|R@0.3|R@0.5|R@0.7|mIoU\nLLaVA-OneVision-7B [28]|22.3|7.9|2.1|15.9|7.1|3.1|1.1|6.1\n+NumPro|42.9<sub>**(\u00a0+20.6)**</sub>|19.4<sub>**(\u00a0+11.5)**</sub>|6.6<sub>**(\u00a0+4.5)**</sub>|28.1<sub>**(\u00a0+12.2)**</sub>|14.4<sub>**(\u00a0+7.3)**</sub>|7.9<sub>**(\u00a0+4.8)**</sub>|3.8<sub>**(\u00a0+2.7)**</sub>|11.3<sub>**(\u00a0+5.2)**</sub>\nLLaVA-Video-7B [67]|11.8|2.7|0.1|9.8|7.4|3.1|1.2|6.2\n+NumPro|56.7<sub>**(\u00a0+44.8)**</sub>|25.6<sub>**(\u00a0+22.9)**</sub>|8.6<sub>**(\u00a0+8.5)**</sub>|34.6<sub>**(\u00a0+24.8)**</sub>|25.2<sub>**(\u00a0+17.8)**</sub>|15.2<sub>**(\u00a0+12.1)**</sub>|8.4<sub>**(\u00a0+7.2)**</sub>|18.6<sub>**(\u00a0+12.4)**</sub>\nQwen2-VL-72B [51]|0.0|0.0|0.0|0.2|1.0|0.6|0.3|1.0\n+NumPro|25.8<sub>**(\u00a0+25.8)**</sub>|9.9<sub>**(\u00a0+9.9)**</sub>|3.0<sub>**(\u00a0+3.0)**</sub>|17.4<sub>**(\u00a0+17.2)**</sub>|35.5<sub>**(\u00a0+34.5)**</sub>|21.4<sub>**(\u00a0+20.8)**</sub>|11.0<sub>**(\u00a0+10.7)**</sub>|25.5<sub>**(\u00a0+24.5)**</sub>\nLongVA-7B-DPO [65]|22.6|10.1|2.2|14.6|11.8|5.3|1.9|8.2\n+FT|62.0|41.6|19.9|40.2|41.8|25.7|13.7|29.0\n+NumPro-FT|63.8<sub>**(\u00a0+41.2)**</sub>|42.0<sub>**(\u00a0+31.9)**</sub>|20.6<sub>**(\u00a0+18.4)**</sub>|41.4<sub>**(\u00a0+26.8)**</sub>|55.6<sub>**(\u00a0+43.8)**</sub>|37.5<sub>**(\u00a0+32.2)**</sub>|20.6<sub>**(\u00a0+18.7)**</sub>|38.8<sub>**(\u00a0+30.6)**</sub>", "caption": "Table 2: Performance of Applying NumPro to Various Vid-LLMs and Ablation Results on NumPro-FT.", "description": "This table presents the performance of several Video Large Language Models (Vid-LLMs) on video temporal grounding tasks, both with and without the Number-Prompt (NumPro) method.  It shows the impact of NumPro on various models across two datasets: Charades-STA and ActivityNet.  The results are broken down by different metrics (R@0.3, R@0.5, R@0.7, mIoU, mAP, HIT@1) for both training-free and fine-tuned settings (with NumPro-FT). It demonstrates the effectiveness of NumPro and NumPro-FT in enhancing the performance of multiple different Vid-LLMs.", "section": "4.2 Main Results"}, {"content": "| Size | Color | Position | Charades-STA |  |  |  |\n|---|---|---|---|---|---|---|\n| 40 | Red | Top Left | 56.7 | 32.9 | 13.8 | 35.8 |\n| 40 | Red | Top Right | 58.2 | 34.0 | 13.0 | 36.8 |\n| 40 | Red | Center | 53.7 | 29.5 | 10.4 | 34.1 |\n| 40 | Red | Bottom Left | **61.6** | **37.8** | **15.9** | **39.3** |\n| 40 | Red | Bottom Right | <s>60.7</s> | <s>36.8</s> | **15.9** | <s>38.5</s> |\n| 20 | Red | Bottom Right | 53.6 | 34.0 | 14.0 | 34.6 |\n| 40 | Red | Bottom Right | **60.7** | **36.8** | **15.9** | **38.5** |\n| 60 | Red | Bottom Right | <s>58.0</s> | <s>34.5</s> | <s>14.1</s> | <s>37.1</s> |\n| 80 | Red | Bottom Right | <s>58.0</s> | 33.9 | 13.7 | 36.9 |\n| 40 | Red | Bottom Right | **60.7** | **36.8** | **15.9** | **38.5** |\n| 40 | Blue | Bottom Right | <s>57.8</s> | 34.2 | <s>14.6</s> | <s>36.6</s> |\n| 40 | Black | Bottom Right | 56.6 | <s>36.0</s> | **15.9** | <s>36.6</s> |\n| 40 | Green | Bottom Right | 56.0 | 33.8 | 14.5 | 36.0 |", "caption": "Table 3: Ablation study on various NumPro designs. We divide the designs into three dimensions: font size, color, and position.", "description": "This ablation study investigates the impact of different Number-Prompt (NumPro) design choices on video temporal grounding performance.  Three key design aspects were varied: font size, color, and position of the overlaid numbers on the video frames.  The table presents the results of these variations, measured using Number Accuracy and Caption Accuracy metrics on the Charades-STA dataset. These metrics help to understand the balance between the clear visibility and recognition of the numbers (Number Accuracy) and the degree to which the numbers disrupt or interfere with the main visual content of the video frames (Caption Accuracy).", "section": "3.3 Design of Numerical Prompt"}, {"content": "| Model | CI | DO | CU | TU | CO |\n|---|---|---|---|---|---| \n| Qwen2-VL | 3.10 | 2.57 | 3.46 | 2.47 | 3.30 |\n| +NumPro | 3.10 | 2.55 | 3.46 | 2.57 | 3.30 |", "caption": "Table 4: The influence of applying NumPro to general video-QA. CI stands for correctness\nof information, DO stands for detail orientation, CU stands for contextual understanding, TU stands for temporal understanding, and CO stands for consistency.", "description": "Table 4 presents the results of applying the Number-Prompt (NumPro) method to general video question-answering (VQA) tasks, evaluating its impact on various aspects of model performance.  It assesses whether NumPro affects the model's ability to provide correct information (CI), focus on details (DO), understand context (CU), grasp temporal information (TU), and maintain consistency (CO) in its responses. The table shows the scores for each of these aspects, both with and without the NumPro technique, to demonstrate how the addition of NumPro impacts overall VQA performance.", "section": "4.5 Influence on General Video-QA"}]