{"references": [{"fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "publication_date": "2023-09-16", "reason": "This paper introduces the Qwen large language model, a foundational model used in several video understanding tasks discussed in the main paper."}, {"fullname_first_author": "Fabian Caba Heilbron", "paper_title": "Activitynet: A large-scale video benchmark for human activity understanding", "publication_date": "2015-00-00", "reason": "ActivityNet is a crucial dataset for video temporal grounding, serving as a primary evaluation benchmark in the main paper's experiments."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "CLIP, introduced in this paper, is a fundamental vision-language model that many of the video large language models (Vid-LLMs) build upon, hence its importance to the current work."}, {"fullname_first_author": "Bin Huang", "paper_title": "VTimeLLM: Empower LLMs to grasp video moments", "publication_date": "2024-00-00", "reason": "VTimeLLM is a state-of-the-art model for video temporal grounding that the authors directly compare against, making it a key reference for their work."}, {"fullname_first_author": "Yongxin Guo", "paper_title": "VTG-LLM: Integrating timestamp knowledge into video LLMs for enhanced video temporal grounding", "publication_date": "2024-00-00", "reason": "This paper proposes another state-of-the-art approach to video temporal grounding, providing a crucial comparison point for assessing the main paper's novelty and effectiveness."}]}