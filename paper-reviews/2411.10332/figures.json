[{"figure_path": "https://arxiv.org/html/2411.10332/x2.png", "caption": "Figure 1: Effectiveness of Adding Frame Numbers for Temporal Grounding: (a) Without numbered images or frames, both humans and Vid-LLMs struggle to locate specific timestamps accurately. (b) Once numbered, grounding temporal cues becomes as intuitive as flipping manga, where timestamps are accessible at a glance.", "description": "Figure 1 demonstrates the impact of adding frame numbers to video frames for temporal grounding. In (a), without frame numbers, both humans and video large language models (Vid-LLMs) have difficulty accurately identifying specific timestamps. In contrast, (b) shows that adding frame numbers makes temporal grounding much more intuitive and efficient, similar to the ease of understanding the timeline of events in a manga comic book where panels are clearly numbered.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.10332/x3.png", "caption": "Figure 2: Attention Analysis between Video Frames and Event Query. Although the model accurately attends to regions of interest related to the query, it struggles to generate precise temporal boundaries in its response.", "description": "Figure 2 presents an attention map visualization that illustrates how a Video Large Language Model (Vid-LLM) processes a video in the context of an event query. The heatmap shows the model's attention distribution across different frames of a video clip.  The darker the color of a frame, the stronger the model's attention.  The model successfully focuses its attention on the relevant parts of the video where the queried event occurs. However, the key observation is that, despite accurately attending to relevant visual information, the model fails to precisely determine the start and end frames of the event, producing imprecise temporal boundaries in its response. This highlights a core challenge in Video Temporal Grounding (VTG) tasks.", "section": "3.1 Attention Analysis"}, {"figure_path": "https://arxiv.org/html/2411.10332/x4.png", "caption": "Figure 3: Framework of Our Approach in Two Settings: (1) Training-free VTG with NumPro, where frame numbers are directly added to video frames, enabling Vid-LLMs to locate events temporally without additional training, and (2) Fine-tuned VTG with NumPro-FT, which further improves VTG performance by fine-tuning Vid-LLMs on a dataset NumPro-enhanced with no architectural modifications.", "description": "This figure illustrates the Number-Prompt (NumPro) approach for Video Temporal Grounding (VTG) in two scenarios. The first is a training-free setting where frame numbers are directly added to the video frames, allowing Vid-LLMs to perform temporal localization without additional training.  The second involves fine-tuning a Vid-LLM using a dataset where frame numbers have been added, significantly enhancing the model's VTG capabilities while avoiding any architectural changes to the model itself. The figure visually represents the workflow and components of both approaches.", "section": "3. Number-Prompt Approach"}, {"figure_path": "https://arxiv.org/html/2411.10332/x5.png", "caption": "Figure 4: Illustration of Our NumPro Design Algorithm.We overlay different numbers onto COCO images and obtain visual and textual representations using CLIP encoders. For each configuration, we calculate Number/Caption Similarity and derive Number/Caption Accuracy, enabling us to identify the optimal NumPro design that balances recognizability and minimal disruption to the visual content.", "description": "This figure details the algorithm used to determine the optimal design for the Number-Prompt (NumPro) method.  The process involves overlaying various numerical identifiers (numbers) onto images from the COCO dataset. CLIP encoders then generate visual and textual representations for each configuration.  The algorithm computes 'Number/Caption Similarity' and 'Number/Caption Accuracy' metrics. The goal is to find the NumPro configuration that maximizes both the ease of number recognition and minimizes the visual interference of the numbers with the original image content.", "section": "3.3 Design of Numerical Prompt"}, {"figure_path": "https://arxiv.org/html/2411.10332/x6.png", "caption": "Figure 5: The Impact of Different Number-Prompt Designs. We categorize the design into three dimensions: font size, position, and color. BL stands for Bottom Left, BR for Bottom Right, TL for Top Left, TR for Top Right, and C for Center.", "description": "This figure analyzes the impact of different Number-Prompt design choices on performance.  Three design aspects are investigated: font size, position (Bottom Left, Bottom Right, Top Left, Top Right, and Center), and color (Black, Red, Blue, and Green).  The results show how each design choice affects Number Accuracy (how well the model identifies the numbers) and Caption Accuracy (how accurately the original caption aligns with frame content after adding numbers).  The goal is to find the Number-Prompt design that balances number readability with minimal disruption to the main video content.", "section": "3.3 Design of Numerical Prompt"}, {"figure_path": "https://arxiv.org/html/2411.10332/x7.png", "caption": "Figure 6: Qualitative Comparison with State-of-the-Art. Our LongVA-7B-DPO model, fine-tuned with NumPro-FT, outperforms TimeChat\u00a0[44] and VTimeLLM\u00a0[21] on ActivityNet by accurately identifying event boundaries in challenging scenes.", "description": "Figure 6 presents a qualitative comparison of video temporal grounding performance between the proposed method (LongVA-7B-DPO model fine-tuned with NumPro-FT), TimeChat [44], and VTimeLLM [21] on the ActivityNet dataset.  Two example video clips with their corresponding ground truth event timestamps, along with the model-predicted timestamps, are shown. This demonstrates the superior accuracy of the proposed method in precisely identifying event boundaries, especially in complex scenes involving subtle changes or distractors. The figure highlights the challenge that existing models (TimeChat and VTimeLLM) face in accurately localizing events. The proposed approach greatly improves upon these methods, achieving more precise and accurate event boundary detection in challenging scenarios.", "section": "4.2 Main Results"}]