[{"content": "| Model | Training Steps | RedPajama | Wiki | C4 |\n|---|---|---|---|---|\n| Dense | 50,000 | 13.01 | 12.95 | 17.41 |\n| SMoE | 50,000 | 11.87 | 10.51 | 15.63 |\n| Fine-grained SMoE | 50,000 | 11.68 | 10.18 | 15.21 |\n| MH-MoE (head=2) | 50,000 | 11.60 | 10.11 | 15.11 |\n| MH-MoE (head=3) | 50,000 | **11.45** | **10.00** | **14.90** |\n| Dense | 100,000 | 12.13 | 11.58 | 16.21 |\n| SMoE | 100,000 | 10.90 | 9.68 | 14.35 |\n| Fine-grained SMoE | 100,000 | 10.74 | 9.38 | 13.97 |\n| MH-MoE (head=2) | 100,000 | 10.70 | 9.26 | 13.80 |\n| MH-MoE (head=3) | 100,000 | **10.51** | **9.18** | **13.63** |", "caption": "Table 1: Validation set perplexity for the language modeling task. All models are matched in terms of parameters and computation.", "description": "This table presents the validation set perplexity results for various language models evaluated on the language modeling task.  The models compared include a dense model, a standard sparse Mixture-of-Experts (MoE) model, a fine-grained version of the sparse MoE model, and several variants of the Multi-Head Mixture-of-Experts (MH-MoE) model.  Importantly, all models in this table were designed and trained to have the same number of parameters and computational costs for a fair comparison. The results are shown for two different training step counts (50,000 and 100,000) and across three different datasets (RedPajama, Wiki, C4), providing a comprehensive evaluation of the models' performance.", "section": "5 Language Modeling Evaluation"}, {"content": "| Model | Training Steps | RedPajama | Wiki | C4 |\n|---|---|---|---|---|\n| SMoE | 50,000 | 11.76 | 10.33 | 15.19 |\n| Fine-grained SMoE |  | 11.51 | 10.06 | 15.01 |\n| MH-MoE (head=2) |  | 11.48 | 9.91 | 14.87 |\n| MH-MoE (head=3) |  | **11.26** | **9.74** | **14.82** |\n| SMoE | 100,000 | 10.41 | 9.44 | 14.30 |\n| Fine-grained SMoE |  | 10.66 | 9.15 | 13.78 |\n| MH-MoE (head=2) |  | 10.36 | 8.79 | 13.66 |\n| MH-MoE (head=3) |  | **10.28** | **8.72** | **13.49** |", "caption": "Table 2: Validation set perplexity for the language modeling task. All MoE models apply a shared expert\u00a0[4] with the same size and matched in terms of parameters and computation.", "description": "This table presents the validation set perplexity results for various language models on the RedPajama, Wiki, and C4 datasets.  All Mixture-of-Experts (MoE) models in this experiment utilize a shared expert layer, as described in reference [4], with consistent model sizes and computational costs. The results compare the standard Sparse MoE, a fine-grained version of Sparse MoE, and the Multi-Head Mixture-of-Experts (MH-MoE) model with two and three heads.  The perplexity is measured after 50,000 and 100,000 training steps to show the effect of training time on performance.", "section": "Experiments"}, {"content": "| Model | Training Steps | RedPajama | Wiki | C4 |\n|---|---|---|---|---|\n| Dense | 50,000 | 32.17 | 27.56 | 35.85 |\n| SMoE | 50,000 | 29.18 | 24.70 | 32.34 |\n| Fine-grained SMoE | 50,000 | 29.04 | 24.51 | 32.03 |\n| MH-MoE (head=2) | 50,000 | 28.84 | 24.27 | 31.86 |\n| MH-MoE (head=3) | 50,000 | **28.77** | **24.13** | **31.81** |\n| Dense | 100,000 | 30.04 | 24.75 | 33.55 |\n| SMoE | 100,000 | 26.78 | 21.54 | 29.73 |\n| Fine-grained SMoE | 100,000 | 26.68 | 21.42 | 29.50 |\n| MH-MoE (head=2) | 100,000 | 26.59 | 21.11 | 29.27 |\n| MH-MoE (head=3) | 100,000 | **26.47** | **21.06** | **29.14** |", "caption": "Table 3: Validation set perplexity for the language modeling task. All dense and MoE models are quantized and trained using BitNet\u00a0[9], and matched in terms of parameters and computation.", "description": "This table presents the validation set perplexity results for various language models evaluated on the RedPajama, Wiki, and C4 datasets.  All models in this experiment have been quantized and trained using the BitNet method, ensuring a fair comparison in terms of parameters and computational requirements.  The models compared include dense models and several Mixture-of-Experts (MoE) architectures, offering insights into the performance of different model designs in a quantized setting.", "section": "6 1-bit MH-MoE"}, {"content": "| Model | w/ head & merge layer | RedPajama | Wiki | C4 |\n|---|---|---|---|---|\n| SMoE | \u2717 | 11.87 | 10.51 | 15.63 |\n| SMoE | \u2713 | 11.84 | 10.48 | 15.61 |\n| Fine-grained SMoE | \u2717 | 11.68 | 10.18 | 15.21 |\n| Fine-grained SMoE | \u2713 | 11.67 | 10.18 | 15.19 |\n| MH-MoE (head=2) | \u2717 | 11.71 | 10.16 | 15.23 |\n| MH-MoE (head=2) | \u2713 | 11.46 | 9.98 | 14.89 |", "caption": "Table 4: Validation set perplexity for different models with and without head and merge layers.", "description": "This table presents the validation set perplexity achieved by different language models on the RedPajama, Wiki, and C4 datasets.  The models are categorized by whether they include a head layer and a merge layer.  The purpose is to show the impact of these layers on model performance for various Sparse Mixture-of-Experts (SMoE) and Multi-Head Mixture-of-Experts (MH-MoE) architectures. The results demonstrate the importance of these layers, especially for MH-MoE, in improving model accuracy.", "section": "7 Ablations"}, {"content": "| w/ head layer | w/ merge layer | RedPajama | Wiki | C4 |\n|---|---|---|---|---|\n| \u2717 | \u2717 | 11.97 | 10.40 | 15.52 |\n| \u2713 | \u2717 | 11.74 | 10.18 | 15.17 |\n| \u2717 | \u2713 | 11.84 | 10.27 | 15.36 |\n| \u2713 | \u2713 | 11.60 | 10.11 | 15.11 |", "caption": "Table 5: Validation set perplexity for ablation of head and merge layers.", "description": "This table presents the results of ablation experiments, evaluating the impact of removing either the head layer or the merge layer (or both) from the MH-MoE model.  It shows the validation set perplexity on the RedPajama, Wiki, and C4 datasets for different model configurations. This allows for assessment of the individual contribution of each layer to overall model performance.", "section": "7 Ablations"}]