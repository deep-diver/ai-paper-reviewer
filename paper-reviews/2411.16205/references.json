{"references": [{"fullname_first_author": "Aidan Clark", "paper_title": "Unified scaling laws for routed language models", "publication_date": "2022-02-01", "reason": "This paper establishes scaling laws for large language models using Mixture of Experts, a crucial theoretical foundation for the current research."}, {"fullname_first_author": "Zewen Chi", "paper_title": "On the representation collapse of sparse mixture of experts", "publication_date": "2022-12-01", "reason": "This paper addresses the issue of representation collapse, a common problem in sparse Mixture of Experts models, and proposes solutions relevant to the current work."}, {"fullname_first_author": "Damai Dai", "paper_title": "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models", "publication_date": "2024-01-01", "reason": "This paper introduces DeepSeekMoE, a novel approach to Mixture of Experts which improves performance and addresses limitations of previous methods, providing context for the current research."}, {"fullname_first_author": "Albert Q Jiang", "paper_title": "Mixtral of experts", "publication_date": "2024-01-01", "reason": "This paper introduces Mixtral, a large language model using Mixture of Experts which achieves state-of-the-art results, forming a key comparison point for the current work."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-00-00", "reason": "This seminal paper introduced the Mixture of Experts layer, which is the foundation upon which the current research builds, and is thus of significant importance."}]}