[{"content": "| Stage | Image Tokens | Text Tokens | Total Tokens |\n|---|---|---|---| \n| Pre-Training | 3.21e8 | 1.52e7 | 3.37e8 |\n| Pre-FineTuning | 4.08e8 | 1.59e8 | 5.67e8 |\n| VIT (332K) | 1.80e8 | 7.71e7 | 2.57e8 |\n| VIT (665K) | 3.60e8 | 1.54e8 | 5.14e8 |", "caption": "Table 1: Token distribution across different stages. VIT denotes Visual Instruction Tuning, with 332K and 665K indicating the number of images used.", "description": "This table shows the number of tokens (units of text data) used in each stage of the model training process. The stages are: pre-training, pre-fine-tuning, and visual instruction tuning (VIT).  For the VIT stage, two different sizes of datasets are used, one with 332,000 images and another with 665,000 images. The total number of tokens in each stage represents the overall amount of training data utilized.  The table is useful for understanding the scale of the dataset and how it changed throughout different training phases.", "section": "4 Experiments"}, {"content": "Data|Model|MoE|Method|AI2D|Text|VQA|GQA|Hallusion|Benchmark|MathVista|Validation|MMBenchEN|dev|MMMU|Validation|MMStar|POPE|SQA|Full|MME|AVEGAGE(w/o MME)|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||SMoE-R|63.67|47.47|59.46|43.32|31.60|66.67|40.11|37.94|86.87|77.23|1,608.21|55.42|\n||Cosine-R|63.31|48.83|59.25|41.54|31.80|67.96|39.56|39.09|86.81|76.96|1,637.99|55.51|\n||Sigmoid-R|63.80|47.74|59.24|41.43|31.40|68.30|40.78|38.70|87.49|77.61|1,611.36|55.65|\n||Hyper-R|64.05|47.76|59.61|41.11|32.50|69.24|41.33|39.27|86.68|77.31|1,602.59|55.89|\n||Perturbed Cosine-R|64.60|47.92|59.08|41.54|30.60|67.87|40.22|38.84|86.81|77.82|1,619.69|55.63|\n||SMoE-R|65.19|39.39|59.55|40.69|29.80|68.99|40.00|40.88|85.88|79.08|1,688.78|54.94|\n||Cosine-R|65.12|40.78|59.41|40.48|31.50|70.10|40.00|40.84|86.58|79.21|1,719.35|55.40|\n||Sigmoid-R|64.48|40.29|59.10|40.06|30.50|69.67|40.89|39.97|86.39|78.81|1,684.78|55.02|\n||Hyper-R|65.15|40.57|58.82|40.80|30.50|70.62|40.56|40.59|85.82|81.66|1,692.64|55.51|\n||Perturbed Cosine-R|65.09|41.09|59.61|40.48|31.60|70.02|40.78|40.72|85.86|79.67|1,707.34|55.49|\n|332k|SigLIP 224 + Phi3.5|Perturbed Cosine-R|64.96|40.63|59.76|42.17|32.00|71.05|41.89|41.72|86.03|79.77|1,711.27|56.00|\n||SMoE-R|64.25|46.57|62.12|40.48|31.00|68.12|39.89|37.13|87.50|77.74|1,700.61|55.48|\n||Cosine-R|64.51|49.79|61.38|40.80|31.30|67.01|40.67|39.36|87.52|77.48|1,687.37|55.98|\n||Sigmoid-R|64.38|47.12|61.65|40.80|31.90|67.87|40.11|39.20|86.93|77.17|1,710.42|55.71|\n||Hyper-R|64.37|47.59|59.70|40.38|31.30|68.30|40.78|38.33|85.70|80.33|1,726.87|55.68|\n||Perturbed Cosine-R|64.70|47.16|61.90|39.43|32.80|69.50|39.89|40.33|87.42|77.64|1,672.70|56.08|\n||SMoE-R|64.35|40.35|60.03|41.75|28.70|67.96|40.22|39.47|84.31|80.71|1,655.81|54.78|\n||Cosine-R|64.60|41.98|60.74|41.43|31.30|70.61|41.22|38.50|86.33|81.49|1,759.21|55.82|\n||Sigmoid-R|64.66|41.05|60.52|40.80|28.80|69.07|40.89|39.29|86.54|80.85|1,766.03|55.25|\n||Hyper-R|65.12|41.67|59.88|41.32|30.30|69.33|41.44|39.86|85.40|79.03|1,752.39|55.34|\n|665k|SigLIP 224 + Phi3.5|Perturbed Cosine-R|65.54|41.85|61.04|41.75|30.50|71.65|43.00|41.72|86.73|78.88|1,688.82|56.27|", "caption": "Table 2: Comparison of MoE algorithms on different models and training data sizes for visual instruction tuning. The data set is constructed from LLaVA-665K \u00a0Liu et\u00a0al. (2023a). We highlight the highest (best) results in bold. Model: We consider five algorithms: SMoE-R (SMoE Router) Shazeer et\u00a0al. (2017), Cosine-R Chi et\u00a0al. (2022), Sigmoid-R (Sigmoid Router) Csord\u00e1s et\u00a0al. (2023), Hyper-R (Hyper Router) Do et\u00a0al. (2023), and Perturbed Cosine-R (Perturbed Cosine Router) Nguyen et\u00a0al. (2024a)", "description": "This table presents a comprehensive comparison of five different Mixture-of-Experts (MoE) algorithms across three different large language models (LLMs) and various training data sizes.  The algorithms compared include SMoE Router, Cosine Router, Sigmoid Router, Hyper Router, and Perturbed Cosine Router.  Each algorithm's performance is evaluated on 11 different zero-shot benchmarks for visual instruction tuning using the LLaVA-665K dataset.  The best performance for each benchmark and LLM is highlighted in bold, allowing for easy identification of top-performing algorithms under different conditions.", "section": "4.2 Main Results"}, {"content": "| MoE |\n|---|---| \n| **Method** |", "caption": "Table 3: Detailed Training Duration and Resource Utilization for MoE Algorithms Across Models and Datasets", "description": "This table details the computational resources and time required to train various Mixture-of-Experts (MoE) algorithms using LibMoE.  It breaks down the training time into three stages: pre-training, pre-fine-tuning, and visual instruction tuning.  Different model configurations (CLIP + Phi3, SigLip 224 + Phi3, SigLip 224 + Phi3.5) and dataset sizes (332K and 665K samples) are considered, along with five distinct MoE algorithms (SMOE-R, Cosine-R, Sigmoid-R, Hyper-R, and Perturbed Cosine-R). The number of GPUs used is also specified for each training scenario.", "section": "3 LIBMOE"}]