{"references": [{"fullname_first_author": "Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-01-17", "reason": "This paper introduces the sparsely-gated Mixture-of-Experts (MoE) layer, a foundational technique for training efficient and effective large language models that is widely used in the field."}, {"fullname_first_author": "Fedus", "paper_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "publication_date": "2022-MM-DD", "reason": "This work introduces Switch Transformers, a significant advancement in MoE, improving training efficiency and scalability for large language models."}, {"fullname_first_author": "Du", "paper_title": "GLAM: Efficient scaling of language models with mixture-of-experts", "publication_date": "2022-MM-DD", "reason": "This paper presents GLAM, a notable model showcasing the effectiveness of MoE in multimodal large language models, demonstrating significant progress in the field."}, {"fullname_first_author": "Chi", "paper_title": "On the representation collapse of sparse mixture of experts", "publication_date": "2022-MM-DD", "reason": "This paper is crucial because it addresses the issue of representation collapse in sparse MoE, a significant challenge that the current work seeks to address."}, {"fullname_first_author": "Komatsuzaki", "paper_title": "Sparse upcycling: Training mixture-of-experts from dense checkpoints", "publication_date": "2022-MM-DD", "reason": "This paper introduces sparse upcycling, a cost-effective technique for integrating MoE into existing models, which is essential for broader accessibility and faster experimentation."}]}