[{"figure_path": "https://arxiv.org/html/2411.00918/x1.png", "caption": "Figure 1: The detailed design of LiBMoE, which comprises three major modules. First, the MoE module implements various MoE algorithms. Second, the training modules handles the training process and supports various configurations. Lastly, the evaluation module supports almost 100 zero-shot benchmarks and a wide-range of metrics.", "description": "LibMoE's architecture is composed of three core modules: the MoE module, responsible for implementing diverse MoE algorithms; the training module, which manages the training process and allows for various configurations; and the evaluation module, which supports a comprehensive set of nearly 100 zero-shot benchmarks and a wide array of metrics for thorough evaluation.", "section": "3 LIBMOE"}, {"figure_path": "https://arxiv.org/html/2411.00918/x2.png", "caption": "Figure 2: Overview of the LibMoE architecture and training process. In the first stage of Dense Training, only the MLP is trained to improve alignment. In the second stage, all parameters are trained. During MoE Training, the feed-forward networks (FFNs) of the Vision Encoder (VE) and MLP Connector are used to initialize the experts within the MoE framework, and all parameters continue to be trained.", "description": "This figure details LibMoE's training pipeline which consists of three stages: Dense Training, Pre-Fine Tuning, and MoE Training.  In the first stage (Dense Training), only the Multi-Layer Perceptron (MLP) is trained to align the vision encoder and language model. The second stage (Pre-Fine Tuning) trains all model parameters. Finally, the third stage (MoE Training) uses the pre-trained weights from the previous stages to initialize the experts within the Mixture-of-Experts (MoE) framework, followed by training all parameters of the MoE model.", "section": "3 LIBMOE"}, {"figure_path": "https://arxiv.org/html/2411.00918/x3.png", "caption": "Figure 3: Comparison of the performance of different MoE algorithms over time. The experiments are conducted on the LLaVa-332K dataset and the CLIP + Phi3 model.", "description": "This figure shows the performance of five different Mixture of Experts (MoE) algorithms over the course of training.  The training was done on the LLaVa-332K dataset, using a model that combines CLIP and Phi3.  The graph displays the performance metrics for each algorithm at different training times, allowing for a comparison of their convergence rates and overall effectiveness.  The x-axis represents the training time (or number of tokens), and the y-axis represents the performance.  This allows readers to see how the performance of different MoE algorithms changes during training, giving insight into their strengths and weaknesses.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2411.00918/x4.png", "caption": "Figure 4: Impact of Training Data Percentage on Expert Selection.", "description": "This figure analyzes how the percentage of training data used affects expert selection in Mixture-of-Experts (MoE) models. It shows the rate of change in expert selection across different training data sizes for three specific benchmarks (MMBench EN, MMStar, and ScienceQA Full).  The x-axis represents the data percentage used for training (10-20%, 20-30%, etc.), and the y-axis shows the rate of change in expert selection. The plot illustrates how the fluctuation in expert allocation decreases as more data is used, indicating that MoE algorithms stabilize expert assignment more effectively with larger datasets.", "section": "4.2 Expert Selection Analysis"}, {"figure_path": "https://arxiv.org/html/2411.00918/x5.png", "caption": "Figure 5: Entropy analysis of expert selection frequency across subtasks in MoE algorithms. The entropy values indicate the tendency of different routers to consistently select specific experts for given subtasks.", "description": "Figure 5 presents an analysis of how frequently different experts are selected for various subtasks within different Mixture of Experts (MoE) algorithms.  The entropy values, displayed for each algorithm and subtask, quantify the diversity of expert selection. Lower entropy indicates that a smaller subset of experts are repeatedly chosen for a given subtask, suggesting specialization; while higher entropy means a more even distribution of expert usage, suggesting a more generalized approach.  This visualization helps understand the extent to which each algorithm exhibits expert specialization for various subtasks.", "section": "4.2.2 Expert Selection Analysis"}, {"figure_path": "https://arxiv.org/html/2411.00918/x6.png", "caption": "Figure 6: Measured confidence levels of various MoE algorithms across tasks. Entropy was computed for each sample and then averaged within each task to illustrate differences in confidence across MoE algorithms. For the Cosine-R and Perturbed Cosine-R algorithms, values on the x-axis (denoted by \u2217) were scaled to enhance visualization of subtle entropy variations. The scaled entropy values are calculated using the transformation (entropy\u22121.999)\u00d710000entropy1.99910000(\\text{entropy}-1.999)\\times 10000( entropy - 1.999 ) \u00d7 10000.", "description": "Figure 6 presents a comparison of the confidence levels exhibited by five different Mixture-of-Experts (MoE) routing algorithms across various tasks.  Confidence is measured using entropy, calculated for each individual sample within each task and then averaged across all samples in that task. This provides a measure of how decisively the algorithms select experts. Because the entropy values for the Cosine Router and Perturbed Cosine Router algorithms were very close, the x-axis values for these two algorithms have been scaled by a factor of 10000 for better visualization of subtle differences. This scaling is done using the formula (entropy -1.999) * 10000. The figure allows for easy comparison of algorithm confidence across different task types (OCR, Coarse-grained, Fine-grained, and Reasoning).", "section": "4.2.2 Expert Selection Analysis"}, {"figure_path": "https://arxiv.org/html/2411.00918/x7.png", "caption": "Figure 7: Expert selection across layers on different tasks in the MME benchmarks. The model uses SigLIP as the vision encoder and Phi 3.5 as the LLM. This figure highlights the distinct expert selection behavior observed in the vision encoder layers.", "description": "This figure visualizes expert selection patterns across various layers of a vision encoder within a Mixture of Experts (MoE) model, focusing on distinct tasks within the MME benchmark.  The model uses SigLIP as its vision encoder and Phi 3.5 as its language model.  The plot reveals how the frequency of each expert being chosen varies across different layers and tasks, showcasing the dynamic specialization of experts during the processing of visual information.  Early layers exhibit less specialization while deeper layers show a stronger tendency towards task-specific expert utilization.", "section": "4.2.3 Impact of the architectural choice on expert selection"}, {"figure_path": "https://arxiv.org/html/2411.00918/x8.png", "caption": "Figure 8: Comparison of the average entropy of the frequency distribution of selected experts across subtasks using different vision encoders: Siglip and CLIP.", "description": "This figure displays a comparison of the average entropy calculated from the frequency distribution of selected experts across various subtasks.  Two different vision encoders, SigLIP and CLIP, were used in the models.  The chart allows for a comparison of expert selection behavior between the two encoders, showing whether they demonstrate consistent or varying selections of experts across multiple subtasks.  Differences in entropy values might suggest that one encoder leads to greater expert specialization or more balanced utilization across subtasks. This visualization helps in understanding the impact of the choice of vision encoder on the MoE algorithm's performance and expert selection patterns.", "section": "4.2.3 Impact of the architectural choice on expert selection"}, {"figure_path": "https://arxiv.org/html/2411.00918/x9.png", "caption": "Figure 9: Comparison of the performance of different MoE algorithms across 11 benchmarks over time. The experiments were conducted using the LLaVa-332K dataset and the CLIP + Phi3 model.", "description": "This figure displays the performance of five different Mixture-of-Experts (MoE) algorithms across eleven benchmarks over the course of training.  The training data used was the LLaVa-332K dataset, and the model employed was CLIP + Phi3.  The graph allows for a visual comparison of how the performance of each algorithm changes over time on various tasks, highlighting the relative strengths and weaknesses of different routing strategies within the MoE framework.", "section": "4.2 Main Results"}]