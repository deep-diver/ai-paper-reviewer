[{"figure_path": "https://arxiv.org/html/2412.03517/x2.png", "caption": "Figure 1: \nAs the number of unposed input views increases, NVComposer (blue circle) effectively uses the extra information to improve NVS quality. In contrast, ViewCrafter\u00a0[43] (green triangle), which relies on external multi-view alignment (via pre-reconstruction from DUSt3R\u00a0[34]), suffers performance degradation as the number of views grows due to instability of the external alignment. This result contradicts the common expectation that \u201cmore views lead to better performance.\u201d Please refer to Sec.\u00a04.2 for full results.", "description": "This figure compares the performance of NVComposer and ViewCrafter for novel view synthesis (NVS) as the number of input views increases.  NVComposer shows improved NVS quality with more views, demonstrating its ability to effectively utilize additional information.  In contrast, ViewCrafter, which depends on external multi-view alignment and pre-reconstruction, experiences degraded performance with more views due to the instability of the external alignment process. This finding contradicts the common assumption that more views always lead to better NVS results.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2412.03517/x3.png", "caption": "Figure 2: Framework illustration of NVComposer. It contains an image-pose dual-stream diffusion model that generates novel views while implicitly estimating camera poses for conditional images, and a geometry-aware feature alignment adapter that uses geometric priors distilled from pretrained dense stereo models\u00a0[34].", "description": "This figure illustrates the architecture of NVComposer, a novel view synthesis model.  It showcases two main components: 1) an image-pose dual-stream diffusion model, which simultaneously generates new views and estimates the corresponding camera poses without explicit external alignment; and 2) a geometry-aware feature alignment adapter that leverages geometric information from a pre-trained dense stereo model (DUSt3R [34]) to improve the accuracy and consistency of generated views. The dual-stream model processes input image-pose bundles, implicitly inferring spatial relationships between multiple conditional views to synthesize novel views.  The adapter incorporates geometric priors to enhance view consistency. This design enables NVComposer to generate high-quality novel views from sparse, unposed images, eliminating the need for explicit multi-view alignment.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2412.03517/x4.png", "caption": "Figure 3: Structure of the geometry-aware feature alignment adapter in\u00a0NVComposer, which aligns the internal features of the dual-stream diffusion models with the 3D point maps produces by DUSt3R\u00a0[34] during training. Block with notation \u201c\u00d72absent2\\times 2\u00d7 2\u201d, \u201c\u00d74absent4\\times 4\u00d7 4\u201d, and \u201c\u00d78absent8\\times 8\u00d7 8\u201d refer to bilinear upsampling on spatial dimensions. The four red bars refer to the channel-wise MLPs.", "description": "The geometry-aware feature alignment adapter in NVComposer enhances the model's understanding of spatial relationships between views.  It takes internal features from the dual-stream diffusion model and aligns them with 3D point maps generated by the external DUSt3R model. This alignment is done using bilinear upsampling (x2, x4, x8) and channel-wise MLPs (red bars) to integrate geometric priors into the generative process, improving the accuracy and consistency of novel view synthesis. This adapter allows NVComposer to implicitly learn geometric relationships from the dense stereo model without requiring explicit reconstruction during inference.", "section": "3.2. Geometry-aware Feature Alignment"}, {"figure_path": "https://arxiv.org/html/2412.03517/x5.png", "caption": "Figure 4: Visual comparison of NVS results on the RealEstate10K\u00a0[47] and DL3DV\u00a0[18] test sets. MotionCtrl\u00a0[36] and CameraCtrl\u00a0[9] uses the first view as input while other methods use two views as input. MotionCtrl and CameraCtrl produce incorrect camera trajectories. DUSt3R and ViewCrafter exhibit better camera control but introduce artifacts due to occlusions or misaligned multi-view inputs. Our model generates views that are visually closer to the reference. We provide zoomed-in details of the first three scenes in white boxes for a closer look. Additional visual comparisons can be found in the supplementary material.", "description": "Figure 4 presents a qualitative comparison of novel view synthesis (NVS) results from different methods on the RealEstate10K and DL3DV datasets.  The methods compared include MotionCtrl, CameraCtrl, DUSt3R, ViewCrafter, and the proposed NVComposer.  MotionCtrl and CameraCtrl, using a single input view, exhibit inaccurate camera trajectory estimations.  While DUSt3R and ViewCrafter demonstrate improved camera control with two input views, they also show artifacts caused by occlusions or misalignments in the multi-view input. In contrast, NVComposer yields results more closely matching the ground truth, showcasing its superior performance in handling occlusions and generating visually accurate novel views.  Detailed zoomed-in regions of the first three scenes are included for closer inspection, with further comparisons available in the supplementary material.", "section": "4.2. Results"}, {"figure_path": "https://arxiv.org/html/2412.03517/x6.png", "caption": "Figure 5: Visual comparison of novel view generation results on the Objaverse\u00a0[4] test set. All input views are unposed and randomly rendered from the same 3D object.", "description": "Figure 5 presents a visual comparison of novel view synthesis results obtained using different methods on the Objaverse dataset.  The test set includes various 3D objects, and for each object, the input views are unposed and randomly rendered from the same 3D model. This ensures a consistent evaluation across different models, showcasing their ability to generate novel viewpoints from various perspectives. The figure facilitates a direct comparison of the visual quality and accuracy of novel view generation across the different approaches.", "section": "4.2.2. Generative NVS in Objects"}]