[{"figure_path": "https://arxiv.org/html/2411.05738/x1.png", "caption": "Figure 1: Our StdGEN\u00a0generates high-quality, decomposed 3D characters from a single reference image.", "description": "This figure showcases the capabilities of the StdGEN model by presenting multiple 3D characters generated from single 2D reference images.  The 3D models are high-quality and semantically decomposed, meaning that individual components like the body, clothes, and hair are separated.  This decomposition is a key feature of StdGEN, facilitating easier editing and animation.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2411.05738/x2.png", "caption": "Figure 2: The overview of our StdGEN\u00a0pipeline. Starting from a single reference image, our method utilizes diffusion models to generate multi-view RGB and normal maps, followed by S-LRM to obtain the color/density and semantic field for 3D reconstruction. Semantic decomposition and part-wise refinement are then applied to produce the final result.", "description": "StdGEN pipeline starts with a single reference image.  A diffusion model generates multiple views (RGB and normal maps) of the image in a canonical A-pose. This is fed into the Semantic-aware Large Reconstruction Model (S-LRM), which outputs color, density, and semantic field data for 3D reconstruction.  The model then performs semantic decomposition to separate parts like body, clothes, and hair. Finally, a multi-layer refinement process refines the mesh quality to produce the final, high-quality, decomposed 3D character model.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.05738/x3.png", "caption": "Figure 3: Demonstration of the structure and intermediate outputs of our semantic-aware large reconstruction model (S-LRM).", "description": "This figure illustrates the architecture and data flow of the Semantic-aware Large Reconstruction Model (S-LRM).  The model takes image tokens as input, processes them through a Vision Transformer (ViT) encoder and a tri-plane decoder. The decoder then generates tri-plane tokens, which are further processed to create geometry, color, and semantic information.  The figure highlights the intermediate outputs at various stages of the process, showing how these individual components are combined to reconstruct a 3D character with semantically distinct parts. The use of LoRA (Low-Rank Adaptation) for efficient training is also shown.", "section": "3.2 Semantic-aware Large Reconstruction Model"}, {"figure_path": "https://arxiv.org/html/2411.05738/x4.png", "caption": "Figure 4: Our semantic-equivalent NeRF and SDF extraction scheme (shown in yellow color).", "description": "This figure illustrates the process of extracting semantic information from both Neural Radiance Fields (NeRF) and Signed Distance Fields (SDF) representations. It highlights how semantic probabilities are used to extract specific layers (such as 'red' and 'green' semantic layers shown in the example) from the combined NeRF and SDF representations, separating different semantic components in a differentiable manner.  This ensures that individual semantic parts can be extracted from the implicit surface for high-quality, semantic-decomposed mesh generation.", "section": "3.2 Semantic-aware Large Reconstruction Model"}, {"figure_path": "https://arxiv.org/html/2411.05738/x5.png", "caption": "Figure 5: Qualitative comparisons on geometry and appearance of generated 3D characters.", "description": "This figure presents a qualitative comparison of 3D character generation results from different methods.  It visually demonstrates the differences in geometry and overall appearance of 3D characters produced by StdGEN (the proposed method), CharacterGen, Unique3D, and InstantMesh. The comparison highlights StdGEN's superiority in generating high-quality, detailed 3D characters.", "section": "4.3 Results and Comparisons"}, {"figure_path": "https://arxiv.org/html/2411.05738/x6.png", "caption": "Figure 6: Decomposed outputs of our method, presented in texture, mesh, and cross-section.", "description": "Figure 6 showcases the decomposable nature of the 3D character generation model. It presents the results in three parts: the texture view showing the appearance of the character with distinct components (body, clothing, hair), a mesh view highlighting the geometric structure of the separated components and their spatial relationships, and cross-sectional views providing insights into the internal structure of the generated components. This visual representation demonstrates the model's ability to generate intricately detailed and semantically decomposed 3D characters.", "section": "4.3 Results and Comparisons"}, {"figure_path": "https://arxiv.org/html/2411.05738/x7.png", "caption": "Figure 7: Ablation study on character decomposition.", "description": "This ablation study compares the results of StdGEN with and without semantic decomposition.  The leftmost image shows a 3D character generated without decomposition.  Note the fusion of hair, clothing, and body, highlighting the limitations of this approach.  The rightmost image shows a character generated using semantic decomposition; individual components (body, clothes, hair) are distinctly separated. This demonstrates the effectiveness of semantic decomposition in generating high-quality and easily editable 3D characters. The visualization clearly shows that separate parts maintain high geometric fidelity without visual artifacts or intersections.", "section": "4.4. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.05738/extracted/5987103/images/anim.png", "caption": "Figure 8: Ablation study on multi-layer refinement. Zoom in for better details.", "description": "Figure 8 shows the results of an ablation study on the multi-layer refinement process used in StdGEN. The left image displays the output of the S-LRM model before the refinement process, showing that while the model successfully decomposes the character into different parts, certain details and precision are lacking.  The image on the right shows that the multi-layer refinement significantly improves the quality of the generated mesh and addresses the issues present in the original output. By zooming in on the images, one can appreciate the improvement in detail and accuracy that multi-layer refinement provides. This demonstrates the effectiveness of this stage in enhancing the overall quality of the 3D character generation.", "section": "3.3 Multi-layer Refinement"}, {"figure_path": "https://arxiv.org/html/2411.05738/x8.png", "caption": "Figure 9: Rigging and animation comparisons on 3D character generation. Our method demonstrates superior performance in human perception and physical characteristics.", "description": "This figure compares the rigging and animation capabilities of the proposed StdGEN method against the CharacterGen method.  It visually demonstrates that StdGEN-generated 3D characters exhibit more realistic and natural-looking movements and physical behavior compared to those from CharacterGen. This difference is primarily attributed to StdGEN's semantic decomposition, enabling easier editing and more accurate control of individual parts during rigging and animation.  The improved accuracy in depicting physical characteristics, such as correct cloth deformation and realistic physics, highlights the superiority of StdGEN for applications requiring high-quality animations.", "section": "4.5. Applications"}]