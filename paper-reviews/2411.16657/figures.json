[{"figure_path": "https://arxiv.org/html/2411.16657/x2.png", "caption": "Figure 1: \nOverall pipeline for DreamRunner. (1) plan generation stage: we employ an LLM to craft a hierarchical video plan (i.e., \u201cHigh-Level Plan\u201d and \u201cFine-Grained Plan\u201d) from a user-provided generic story narration. (2.1) motion retrieval and prior learning stage: we retrieve videos relevant to the desired motions from a video database for learning the motion prior through test-time fine-tuning. (2.2) subject prior learning stage: we use reference images for learning the subject prior through test-time fine-tuning. (3) video generation with region-based diffusion stage: we equipt diffusion model with a novel spatial-temporal region-based 3D attention and prior injection module (i.e., SR3AI) for video generation with fine-grained control.", "description": "DreamRunner's pipeline consists of three stages: plan generation, motion and subject prior learning, and video generation.  In the plan generation stage, a large language model (LLM) creates a hierarchical plan from the user's story, including both high-level scene outlines and detailed, frame-level descriptions.  The next stage uses a video database to find videos relevant to the planned motions and images to represent the characters.  Test-time fine-tuning uses this data to learn priors for motion and character appearance.  The final stage generates the video using a diffusion model enhanced with SR3AI (a novel spatial-temporal region-based 3D attention and prior injection module) for more precise control.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.16657/x3.png", "caption": "Figure 2: \nImplementation details for region-based diffusion. We extend the vanilla self-attention mechanism to spatial-temporal-region-based 3D attention (see upper orange part), which is capable of aligning different regions with their respective text descriptions via region-specific masks. The region-based character and motion LoRAs (see lower yellow and blue parts) are then injected interleavingly to the attention and FFN layers in each transformer block (see the right part). Note that although we resize the visual tokens into sequential 2D latent frames for better visualization, they are flattened and concatenated with all conditions when performing region-based attention.", "description": "This figure details the architecture of the region-based diffusion model used in DREAMRUNNER for video generation.  It shows how the model extends the standard self-attention mechanism to incorporate spatial and temporal information at a regional level. This is achieved through a novel spatial-temporal region-based 3D attention module (shown in orange), which uses region-specific masks to align different regions of the video with their corresponding text descriptions from the input prompt.  Furthermore, character and motion priors (yellow and blue, respectively) are injected into the model via learned LoRA (Low-Rank Adaptation) modules, interleaving them into the attention and feed-forward (FFN) layers of each transformer block.  This allows for fine-grained control over the generation process.  While the visual tokens are shown resized into a 2D sequential format for easier visualization, it's crucial to remember that they are processed in a flattened format and concatenated with the conditions before region-based attention is applied.", "section": "3.3 Sapatial-Temporal-Region-Based Diffusion"}, {"figure_path": "https://arxiv.org/html/2411.16657/x4.png", "caption": "Figure 3: \nQualitative comparison of DreamRunner on multi-scene story generation with multiple objects. DreamRunner generates videos with significantly better character consistency compared to other strong baseline methods, while other methods either fail to maintain consistency for the same object across scenes (e.g., VLogger), fail to generate objects that match the reference images (e.g., VideoDirectorGPT), or fail to generate multiple objects correctly (e.g., CogVideoX w/ Character LoRAs).", "description": "This figure presents a qualitative comparison of DREAMRUNNER's performance against other state-of-the-art story-to-video generation models.  The comparison focuses on multi-scene videos with multiple objects. DREAMRUNNER demonstrates superior character consistency across multiple scenes, a key challenge in storytelling video generation. In contrast, other methods struggle with character consistency, generating objects that don't match reference images, or failing to produce the correct number of objects as specified in the story.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16657/x5.png", "caption": "Figure 4: \nQualitative results of DreamRunner generated with prompts characterizing action binding.", "description": "DREAMRUNNER generates high-quality videos where multiple characters perform distinct actions simultaneously.  The figure showcases several examples of videos generated by DREAMRUNNER, demonstrating the model's ability to generate coherent and realistic videos where multiple characters perform complex actions. Each video clip includes captions that clearly describe the scene and actions being performed, emphasizing the model's precise control over action binding.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16657/x6.png", "caption": "Figure 5: \nQualitative results of DreamRunner generated with prompts characterizing consistent attribute binding.", "description": "Figure 5 showcases DREAMRUNNER's ability to maintain consistent attributes for objects across multiple frames and scenes.  The figure presents several example video sequences generated from text prompts. Each sequence demonstrates that the specified attributes remain consistent even as the object's position or surrounding context changes. This highlights DREAMRUNNER's capacity to generate videos where object properties remain coherent throughout the narrative.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16657/x7.png", "caption": "Figure 6: \nQualitative results of DreamRunner generated with prompts characterizing dynamic attribute binding.", "description": "Figure 6 showcases several examples illustrating DREAMRUNNER's ability to generate videos with dynamic changes in object attributes.  Each example depicts a timelapse, showing gradual evolution of an attribute.  For instance, the first example shows a pumpkin growing from a small bud to a large, ripe fruit. Other examples include a ceramic vase developing cracks and weathering with age, a piece of metal rusting over time, and a flower blooming. This demonstrates DreamRunner\u2019s capacity to generate realistic sequences reflecting temporal changes in object properties. ", "section": "4.6 Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2411.16657/x8.png", "caption": "Figure 7: \nQualitative results of DreamRunner generated with prompts characterizing motion binding.", "description": "Figure 7 showcases several video clips generated by DreamRunner, each illustrating different types of motion.  Each clip demonstrates the model's ability to generate smooth, realistic movements that align with the textual prompt describing the character's actions.  This visualization highlights DreamRunner's capacity for fine-grained control over character motion, ensuring the generated videos accurately reflect the specified actions, such as a kite flying left to right, a cyclist moving along a road, a robot walking through a factory, and bubbles floating upwards.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16657/x9.png", "caption": "Figure 8: \nQualitative results of DreamRunner generated with prompts characterizing object interactions.", "description": "Figure 8 showcases DREAMRUNNER's ability to generate videos depicting complex interactions between multiple objects. The examples demonstrate that DREAMRUNNER accurately renders these interactions according to physical laws and common sense, such as a fork pressing into a cake, volunteers cleaning a beach, pottery shattering on a floor, and a superhero phasing through falling debris.  These examples highlight the model's capacity to generate high-quality videos that faithfully represent multi-object interactions within a scene.", "section": "4.6 Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2411.16657/x10.png", "caption": "Figure 9: \nQualitative results of DreamRunner generated with prompts characterizing spatial relationships.", "description": "Figure 9 showcases qualitative examples from the DreamRunner model. Each example demonstrates the model's ability to generate videos that accurately reflect specified spatial relationships between objects.  The prompts given to the model emphasize spatial relationships (e.g., 'a toddler walking on the left of a dog', 'a duck waddling below a spacecraft'). The resulting videos illustrate that the model successfully renders these relationships within a coherent visual scene, demonstrating strong spatial reasoning and scene composition capabilities.", "section": "4.6 Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2411.16657/x11.png", "caption": "Figure 10: \nQualitative results of DreamRunner generated with a single character (mermaid).", "description": "Figure 10 shows example videos generated by DreamRunner.  The videos showcase the model's ability to maintain character consistency throughout a storyline, even with the complex motion of a mermaid. The images presented demonstrate the model's capability to generate high-quality videos of a single character completing various actions and interacting with its environment.", "section": "4. Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.16657/x12.png", "caption": "Figure 11: \nQualitative results of DreamRunner generated with a single character (astronaut).", "description": "This figure showcases a series of six video frames generated by DreamRunner, depicting an astronaut's journey on an alien planet. The frames illustrate the astronaut's arrival, exploration of diverse terrains, interaction with alien flora and artifacts, and finally, a moment of reflection. Each frame is accompanied by a short description detailing the astronaut's actions and the environment.  The scenes progress from the astronaut's landing, exploring a rocky landscape, discovering a glowing plant in a mysterious forest, examining ancient ruins, and concluding with the astronaut watching a sunset from atop a cliff.", "section": "4.6 Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2411.16657/x13.png", "caption": "Figure 12: \nQualitative results of DreamRunner generated with a single character (mermaid).", "description": "This figure showcases a sequence of video frames generated by the DreamRunner model, depicting a mermaid interacting with various underwater environments. It provides a qualitative assessment of the model's ability to generate consistent and coherent video content featuring a single character across multiple scenes. Each scene highlights the mermaid's actions and interactions in diverse locations, such as a coral reef, a sunken ship, an open ocean, a kelp forest, a sea cave, and a lagoon.  The frames demonstrate seamless transitions between scenes and realistic movement and interactions of the character. The objective is to visually illustrate the model's performance in generating a long-form, coherent video narrative with complex motions and detailed environmental context.", "section": "4. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2411.16657/x14.png", "caption": "Figure 13: \nQualitative results of DreamRunner generated with multiple characters (witch and cat 1).", "description": "This figure showcases a series of video frames generated by DreamRunner, a storytelling video generation model.  The frames depict a witch and her cat engaging in various activities throughout a single day. The scenes progress chronologically, illustrating the witch performing magic, interacting with nature, resting at home, and ending the day in her garden. The model consistently generates coherent and detailed depictions of the witch and her cat, showcasing fine-grained motions and interactions. The video highlights the model's ability to maintain character consistency and produce fluid scene transitions.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16657/x15.png", "caption": "Figure 14: \nQualitative results of DreamRunner generated with multiple characters (warrior and dog 2).", "description": "This figure showcases the qualitative results of the DREAMRUNNER model, demonstrating its ability to generate videos with multiple characters interacting naturally within a scene. The video depicts a warrior and their dog across multiple scenes, each illustrating the model\u2019s fine-grained motion control, character consistency, and ability to generate coherent narratives across various situations. The scenes vary in setting and activity, ranging from the warrior stretching and practicing, to resting by a river, to archery practice in a bamboo forest, and finally, setting up camp at night.  The dog is consistently present and interacts appropriately with the warrior in each setting.", "section": "4. Experiments"}]