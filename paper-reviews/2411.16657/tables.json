[{"content": "| Method | Image (CLIP) | Image (DINO) | Fine-Grained Text (CLIP) | Fine-Grained Text (ViCLIP) | Full Text (CLIP) | Full Text (ViCLIP) | Transition (DINO) |\n|---|---|---|---|---|---|---|---| \n| VideoDirectorGPT [33] | 54.3 | 9.5 | 23.7 | 21.7 | 22.4 | 22.5 | 63.5 |\n| VLogger [74] | 62.5 | 41.3 | 23.5 | 23.1 | 22.5 | 22.2 | 73.6 |\n| DreamRunner (Ours) | **70.7** (+13.1%) | **55.1** (+33.4%) | **24.7** (+5.11%) | **23.7** (+2.60%) | **24.2** (+7.56%) | **24.1** (+8.56%) | **93.6** (+27.2%) |", "caption": "Table 1: Evaluation of story-to-video generation on DreamStorySet. We compare our model with VideoDirectorGPT\u00a0[33] and VLogger\u00a0[74] on character consistency (via CLIP and DINO scores), text instructions following and full prompt adherence (via CLIP and ViCLIP scores), and event transitions smoothness (via DINO score). Our relative improvement over VLogger is highlighted in blue.", "description": "This table presents a quantitative evaluation of story-to-video generation models on the DreamStorySet dataset.  The performance of DREAMRUNNER is compared against two state-of-the-art baselines: VideoDirectorGPT and VLogger.  The evaluation metrics assess three key aspects of video generation quality:\n\n1. **Character Consistency:** Measured using CLIP and DINO scores, this metric evaluates how well the model maintains consistent visual representations of characters across different scenes and throughout the video.\n2. **Text Adherence:** Assessed using CLIP and ViCLIP scores, this metric measures the model's ability to follow both the overall story instructions and the fine-grained descriptions of individual events within each scene.\n3. **Transition Smoothness:**  Evaluated using DINO scores, this metric determines the fluidity and naturalness of transitions between consecutive events within a scene.\n\nThe table highlights DREAMRUNNER's superior performance compared to the baselines across all three metrics.  Improvements relative to the VLogger baseline are explicitly indicated in blue.", "section": "4. Experiments"}, {"content": "| RAG | SR3AI | Fine-Grained Text CLIP | Fine-Grained Text ViCLIP | Full Text CLIP | Full Text ViCLIP | Trans. |\n|---|---|---|---|---|---|---|\n| \u00d7 | \u00d7 | 23.8 | 22.5 | 22.2 | 22.1 | 87.1 |\n| \u00d7 | \u2713 | 23.9 | 22.1 | 22.5 | 22.4 | 92.5 |\n| \u2713 | \u00d7 | 24.7 | 23.5 | 23.9 | 24.0 | 84.6 |\n| \u2713 | \u2713 | **24.7** | **23.7** | **24.2** | **24.1** | **93.6** |", "caption": "Table 2: Ablation studies for the effectiveness of RAG and SR3AI in DreamRunner. Default DreamRunner achieves the best text-following ability and event transition smoothness.", "description": "This ablation study analyzes the individual and combined effects of Retrieval Augmented Generation (RAG) and the Spatial-Temporal Region-Based 3D Attention and Prior Injection module (SR3AI) on the DreamRunner model's performance.  It compares the model's text-following ability (measured by CLIP and ViCLIP scores) and event transition smoothness (measured by DINO score) across four variations:\n\n1. **Baseline:**  The default DreamRunner model.\n2. **With RAG:** The model using RAG but without SR3AI.\n3. **With SR3AI:** The model using SR3AI but without RAG.\n4. **With RAG and SR3AI:** The complete DreamRunner model.\n\nThe results demonstrate the importance of both RAG and SR3AI for optimal performance, with the full model achieving the highest scores in both text alignment and transition quality. ", "section": "4.3 Ablation Studies"}, {"content": "| Model | Consist-attr | Dynamic-attr | Spatial | Motion | Action | Interaction |\n|---|---|---|---|---|---|---|\n| Gen-3 [5] | 0.7045 | 0.2078 | 0.5533 | 0.3111 | 0.6280 | 0.7900 |\n| Dreamina [2] | 0.8220 | 0.2114 | 0.6083 | 0.2391 | 0.6660 | 0.8175 |\n| PixVerse [3] | 0.7370 | 0.1738 | 0.5874 | 0.2178 | 0.6960 | 0.8275 |\n| Kling [6] | 0.8045 | 0.2256 | 0.6150 | 0.2448 | 0.6460 | 0.8475 |\n| VideoCrafter2 [10] | 0.6750 | 0.1850 | 0.4891 | 0.2233 | 0.5800 | 0.7600 |\n| Open-Sora 1.2 [20] | 0.6600 | 0.1714 | 0.5406 | 0.2388 | 0.5717 | 0.7400 |\n| Open-Sora-Plan v1.1.0 [28] | 0.7413 | 0.1770 | 0.5587 | 0.2187 | 0.6780 | 0.7275 |\n| VideoTetris [49] | 0.7125 | 0.2066 | 0.5148 | 0.2204 | 0.5280 | 0.7600 |\n| LVD [31] | 0.5595 | 0.1499 | 0.5469 | 0.2699 | 0.4960 | 0.6100 |\n| CogVideoX-2B [62] | 0.6775 | 0.2118 | 0.4848 | 0.2379 | 0.5700 | 0.7250 |\n| CogVideoX-2B+Ours [62] | 0.7350 | 0.2672 | 0.6040 | 0.2608 | 0.5840 | 0.8225 |", "caption": "Table 3: T2V-CompBench evaluation results. Best/2nd best\nscores for open-sourced models are bolded/underlined.\ngray indicates close-sourced models, and yellow indicates the best score for close-sourced models.", "description": "This table presents a quantitative comparison of DREAMRUNNER against various other models on the T2V-CompBench benchmark.  The benchmark evaluates several aspects of compositional video generation, including attribute binding (consistency of attributes across scenes), dynamic attributes (changes in attributes over time), spatial relationships (correct positioning of objects), motion (naturalness of movements), and action interaction (realistic interactions between objects).  Open-source models are distinguished from closed-source models, and the best and second-best scores for open-source models are highlighted in bold and underlined, respectively. The best score among closed-source models is highlighted in yellow.", "section": "4. Experiments"}, {"content": "| Method | CLIP | ViCLIP |\n|---|---|---|\n| CogVideoX-2B | 23.39 | 20.84 |\n| CogVideoX-2B + RAG | 24.67 | 23.04 |", "caption": "Table 4: Effectiveness of our retrieval-augmented test-time adaptation for learning a better motion prior.", "description": "This table presents an ablation study evaluating the impact of the retrieval-augmented test-time adaptation technique on learning motion priors.  It compares the performance of a baseline model (CogVideoX-2B) against a model enhanced with the retrieval-augmented approach. The comparison is based on CLIP and ViCLIP scores, which measure the alignment between generated videos and textual descriptions.  Higher scores indicate better alignment and thus a more effective motion prior learning.", "section": "4.3 Ablation Studies"}, {"content": "| Max. #Retrieval | CLIP+ViCLIP filter | CLIP | ViCLIP |\n|---|---|---|---| \n| 0 | \u00d7 | 23.42 | 20.56 |\n| 20 | \u00d7 | 24.01 | 22.51 |\n| 3 | \u2713 | 24.45 | 22.80 |\n| 20 | \u2713 | **25.47** | **23.66** |", "caption": "Table 5: Pipeline component ablation on retrieval-augmented test-time adaptation for learning a better motion prior.", "description": "This table presents an ablation study on the retrieval-augmented test-time adaptation pipeline used for learning motion priors. It shows how different components of the pipeline affect the performance, specifically comparing the results with different maximum numbers of retrieved videos and the use of CLIP and ViCLIP for filtering. The goal is to determine the optimal configuration for effective motion prior learning.", "section": "4.3 Ablation Studies"}]