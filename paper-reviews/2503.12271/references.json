{"references": [{"fullname_first_author": "Aditya Ramesh", "paper_title": "Zero-shot text-to-image generation", "publication_date": "2021-01-01", "reason": "This paper is important as it introduces DALL-E, a groundbreaking zero-shot text-to-image generation model, and is cited for its contributions to text-to-image generation techniques."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduces the Transformer architecture, which is a foundational component in many modern text-to-image models, and is cited for its attention mechanism used in the Context Transformer."}, {"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This paper demonstrates the few-shot learning capabilities of large language models, which inspired Reflect-DiT to incorporate in-context learning for refining image generations, and is cited for its influence on inference-time scaling."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This paper introduces Latent Diffusion Models, which is crucial for the development of high-resolution image synthesis, and is cited for its methodology used in generating images."}, {"fullname_first_author": "Enze Xie", "paper_title": "SANA: Efficient high-resolution text-to-image synthesis with linear diffusion transformers", "publication_date": "2025-01-01", "reason": "This paper is important because the Reflect-DiT uses SANA as the base model, and also compare Reflect-DiT's method against SANA, which offers competitive performance while being 106x faster."}]}