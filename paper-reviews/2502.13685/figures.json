[{"figure_path": "https://arxiv.org/html/2502.13685/x1.png", "caption": "Figure 1: Framework of MoM. Each input token selectively activates and updates K\ud835\udc3eKitalic_K memory states, leaving non-activated memory states unchanged to avoid interference from current input. Additionally, we introduce a continuously activated shared memory. This figure presents the basic memory update mechanism; other mechanisms involving gating or more complex updates follow a similar approach.", "description": "The Mixture-of-Memories (MoM) model uses multiple independent memory states to process input tokens.  A router network directs each token to the top K most relevant memory states, where it updates the memory state.  Non-selected memories remain unchanged to prevent interference. A shared memory is continuously active and accessible to all tokens. The diagram shows this memory routing and updating process, illustrating a basic update mechanism. More complex mechanisms, such as gating, would follow a similar approach.", "section": "3.2 MoM: Mixture-of-Memories"}, {"figure_path": "https://arxiv.org/html/2502.13685/x2.png", "caption": "Figure 2: Efficiency of MoM. We demonstrate the inference time and GPU memory consumption required to generate 1K tokens at specific sequence lengths.", "description": "This figure compares the inference time and GPU memory usage of MoM and Transformer++ with flash attention when generating 1K tokens at various sequence lengths.  It visually demonstrates the linear time and space complexity of MoM compared to the quadratic complexity of Transformer++, highlighting MoM's efficiency advantage, especially for longer sequences.", "section": "4.2.5 Efficiency"}, {"figure_path": "https://arxiv.org/html/2502.13685/extracted/6216874/figs/loss.png", "caption": "Figure 3: Training Loss. Loss curves for training 340M models on 15B tokens with a fixed random seed of 42.", "description": "This figure displays the training loss curves for 340 million parameter models trained on 15 billion tokens.  A fixed random seed of 42 was used to ensure reproducibility. The graph allows for a comparison of training loss across different models, showing the rate at which each model's loss decreases over the course of training.", "section": "4.2.6 Training Loss Comparison"}]