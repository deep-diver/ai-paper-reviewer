[{"content": "| Statistic | Number |\n|---|---| \n| **Documents** | 350 |\n| - Domain | 6 |\n| - Total Pages | 4012 |\n| - Avg. Tokens | 482.61/page |\n| - Avg. Data Type | 1.45/page |\n| **Questions** | 4598 |\n| Avg. Question Token | 18.56 |\n| Avg. Answer Token | 7.91 |\n| (Evidence Source) |  |\n| - Text | 3334 (72.5%) |\n| - Formula | 872 (19.0%) |\n| - Table | 392 (8.5%) |\n| (Answer Format) |  |\n| - Short Text Answer | 2482 (54.4%) |\n| - Judgment | 849 (18.5%) |\n| - Numeric | 894 (19.4%) |\n| - Formula | 354 (7.7%) |", "caption": "Table 1: Dataset Statistics", "description": "This table presents a summary of the statistics of the OHRBench dataset, which includes the number of documents, total pages, average tokens per page, average data types per page, question and answer statistics, and the breakdown of evidence sources and answer formats used in the dataset.", "section": "3. OHRBench"}, {"content": "| Metric | OCR | Retrieval |  | Generation |  | Overall |  |\n|---|---|---|---|---|---|---|---|\n|  | Edit Distance \u2193 | LCS@1 \u2191 | LCS@5 \u2191 | EM \u2191 | F1 \u2191 | EM@1 \u2191 | F1@1 \u2191 |\n| Ground Truth | - | 63.53 | 86.22 | 33.54 | 50.19 | 26.42 | 39.77 |\n| _Pipeline-based OCR_ |  |  |  |  |  |  |  |\n| MinerU [32] | 0.2328 | 52.53 | 73.61 | 30.50 | 46.08 | 24.52 | 36.84 |\n| Marker [25] | 0.2621 | 56.94 | 78.53 | 30.08 | 46.02 | 23.89 | 36.51 |\n| _End-to-end OCR_ |  |  |  |  |  |  |  |\n| GOT [34] | 0.2884 | 45.80 | 67.06 | 26.36 | 40.62 | 21.51 | 32.69 |\n| Nougat [2] | 0.3303 | 44.77 | 61.46 | 24.81 | 37.94 | 20.40 | 30.89 |\n| _Vision-Language Model for OCR_ |  |  |  |  |  |  |  |\n| Qwen2-VL-72B [33] | 0.2564 | 53.16 | 72.97 | 26.72 | 41.23 | 23.45 | 35.91 |\n| InternVL2-Llama3-76B [5] | 0.4450 | 42.43 | 57.51 | 20.74 | 32.89 | 20.58 | 31.23 |", "caption": "Table 2: Evaluation of various OCR solutions and their impacts on RAG systems. We report the retrieval performance using top-1 (LCS@1) and top-5 (LCS@5) retrieved chunks.\nOverall performance is presented with top-1 (EM@1, F1@1) and top-5 (EM@5, F1@5) retrieved chunks.\nBold indicates the best performance, and underline indicates the second-best performance.", "description": "This table presents a comprehensive evaluation of different Optical Character Recognition (OCR) methods on their impact on Retrieval-Augmented Generation (RAG) systems.  It compares the performance of various OCR solutions, including pipeline-based, end-to-end, and vision-language models, across multiple metrics.  The metrics used include edit distance (to assess OCR accuracy),  top-1 and top-5 Longest Common Subsequence (LCS) scores for retrieval (to measure how well the retrieved chunks matched the ground truth), and exact match (EM) and F1 scores for generation (to evaluate the quality of LLM answers generated based on the retrieved information). The table highlights the best and second-best performing OCR solutions for each metric, providing insights into the relative strengths and weaknesses of each approach in a RAG context.", "section": "4. Experiments"}, {"content": "| Model | Context Input | Generation EM | Generation F1 |\n|---|---|---|---| \n| GPT-4o | GT Text | 50.60 | 70.66 |\n|  | OCR Text | 45.62 | 64.67 |\n|  | Image | 39.26 | 56.28 |\n|  | Image+OCR Text | 49.97 | 70.05 |\n| Qwen2-VL-7B | GT Text | 48.70 | 66.58 |\n|  | OCR Text | 43.19 | 61.04 |\n|  | Image | 44.09 | 59.63 |\n|  | Image+OCR Text | 48.13 | 63.48 |\n| InternVL2-8B | GT Text | 44.72 | 56.12 |\n|  | OCR Text | 41.22 | 52.53 |\n|  | Image | 31.10 | 51.16 |\n|  | Image+OCR Text | 38.31 | 56.43 |", "caption": "Table 3: Performance of employing VLMs for generation.\nFor context input, GT Text represents ground truth structured data, OCR Text represents the OCR processed data from MinerU, and Image refers to the PDF image containing the clue.", "description": "This table presents the performance of Vision-Language Models (VLMs) in text generation for RAG systems.  Four different types of input contexts are compared: ground truth structured text, OCR-processed text from MinerU, the image from the PDF, and a combination of both image and OCR text. The evaluation metrics are Exact Match (EM) and F1 score, assessing the accuracy and completeness of the generated text.", "section": "4.4 Potential of employing Vision-Language Models in RAG systems"}, {"content": "| Domains | PDFs | Pages | Pages with Q&As |\n|---|---|---|---| \n| Law | 60 | 918 | 505 |\n| Finance | 12 | 1733 | 235 |\n| Textbook | 133 | 133 | 133 |\n| Manual | 15 | 932 | 225 |\n| Newspaper | 116 | 116 | 116 |\n| Academia | 10 | 204 | 156 |\n| Total | 350 | 4012 | 1370 |", "caption": "Table S1: Document statistics of each domain", "description": "Table S1 provides a detailed breakdown of the dataset used in the OHRBench benchmark. It shows the distribution of PDF documents across six real-world application domains (Law, Finance, Textbook, Manual, News, and Academia).  For each domain, the table lists the number of documents, total number of pages in those documents, and the number of pages that contain questions and answers (Q&As) used in the benchmark's evaluation. This information is crucial for understanding the scale and diversity of the dataset, as well as the balance of data across different domains.", "section": "3. OHRBench"}, {"content": "| Attribute | Counts | Description |\n|---|---|---|\n| Multi-Elements Pages | 1848 | Pages containing at least two types of elements, such as text, images, tables, and formulas. |\n| Hard Reading Order | 1025 | Pages not presented in a single-column format. |\n| Complex Formula | 1924 | Pages with formulas requiring at least 10 tokens for description. |\n| Complex Table | 1283 | Pages with tables having more than 4 rows or columns, or containing merged cells. |\n| High Text Density | 1003 | Pages with more than 800 tokens (separated by spaces). |", "caption": "Table S2: Counts and description of each attribute which is present in our datasets", "description": "Table S2 details the characteristics of the PDF documents included in the OHRBench dataset.  It lists five attributes representing common complexities found in real-world documents: the presence of multiple data types (text, formulas, tables), non-standard reading order, complex formulas, complex tables, and high text density. For each attribute, the table provides a count of how many pages in the dataset exhibit that characteristic, offering a quantitative measure of document complexity.", "section": "II. Benchmark Construction Details"}, {"content": "[|System:| System: |---|---|---|---|---|---|---|---|---|---|---|---|---|Given the following document, please generate three RAG-style question-answer pairs based on the document with different levels of difficulty: Easy, Medium, and Hard.|RAG-style refers to a question that needs to be answered by retrieving relevant context from an external document based on the question, so the question MUST obey the following criteria:|1. The question must contain all information and context/background necessary to answer without the document!!! Do not include phrases like \u201caccording to the document\u201d in the question!|2. The question must not contain any ambiguous references, such as \u2019he\u2019, \u2019she\u2019, \u2019it\u2019, \u2019the report\u2019, \u2019the paper\u2019, and \u2019the document\u2019! You MUST use their complete names!|In your output, include the phrase from the document that contains the answer to the question as \u2019context\u2019. This phrase MUST be copied verbatim, word for word, from the document. You must produce the context phrase exactly from the text, with no modifications or truncations.|You MUST obey the following criteria:|- The question MUST be detailed and be based explicitly on information in the document.|- The question MUST include at least one entity.|- The context sentence the question is based on MUST include the name of the entity. For example, an unacceptable context is \u201cHe won a bronze medal in the 4 \u00d7 100 m relay\u201d. An acceptable context is \u201cNils Sandstr\u00f6m was a Swedish sprinter who competed at the 1920 Summer Olympics.\u201d|- The answer form should be as diverse as possible, including [Yes/No, Numeric, Formula, Short Answer].|- The context sentence, which is the evidence source for the question, should be as diverse as possible, including [text, table, formula].|If there are no possible questions that meet these criteria, return \u2019None\u2019 as the question. Output the question in JSON format.|Example Input Format: <Begin Document>\u2026<End Document>|Example Response: \u201cQuestion\u201d: \u201cWho was the commanding general of the Union Army during the American Civil War?\u201d, \u201cAnswer\u201d: \u201cUlysses S. Grant\u201d, \u201cContext\u201d: \u201cAs commanding general, Ulysses S. Grant led the Union Army to victory in the American Civil War in 1865.\u201d, \u201cDifficulty Level\u201d: \u201cEasy\u201d , \u201cAnswer Form\u201d: \u201cShort Answer\u201d, \u201cEvidence Source\u201d: \u201ctext\u201d|[|User:| User:|<Begin Document>{document}<End Document>|]", "caption": "Table S3: Q&A Generation Prompt", "description": "This table presents the prompt template used for generating question-answer pairs for the OHRBench benchmark dataset.  The prompt instructs GPT-4 to generate RAG-style questions with varying difficulty levels (Easy, Medium, Hard), ensuring the questions are self-contained and do not rely on external knowledge, while including specific constraints on entity mention and answer diversity. The output should be a JSON object with the question, answer, context (verbatim excerpt from the document), difficulty level, answer format, and evidence source. ", "section": "I. Instruction Prompts"}, {"content": "| System |  | \n|---|---| \n| **System:** | You are an expert, you have been provided with a question and documents retrieved based on that question. Your task is to search the content and answer these questions using the retrieved information. | \n|  | You **MUST** answer the questions briefly with one or two words or very short sentences, devoid of additional elaborations. | \n|  | Write the answers within <response></response>. | \n| **User:** |  | \n|  | Question: {question} | \n|  | Retrieved Documents: {retrieved_documents} | ", "caption": "Table S4: LLMs prompt for RAG generation", "description": "This table presents the prompt template used for Large Language Models (LLMs) during the Retrieval-Augmented Generation (RAG) process.  It shows the instructions given to the LLM to ensure that the responses are concise and accurate, using only the provided information without additional elaborations. The prompt emphasizes retrieving the precise answer directly from the provided source document.", "section": "I. Instruction Prompts"}, {"content": "| System |  | User |\n|---|---|---|\n| You are an expert, you have been provided with a question and a document image retrieved based on that question. Your task is to answer the question using the content from the given document image. |  | Question: {question} |\n| You **MUST** answer the questions briefly with one or two words or very short sentences, devoid of additional elaborations. |  |  |\n| Write the answers within &lt;response&gt;&lt;/response&gt; |  |  |", "caption": "Table S5: Prompt template for VLM generation with image-only inputs", "description": "This table presents the prompt template used for generating responses using Vision-Language Models (VLMs) when only the image of the document is provided as input.  It shows the instructions given to the VLM to ensure it answers concisely, using only one or two words or very short sentences.", "section": "III. Additional Experimental Results"}, {"content": "| System: |  | \n|---|---| \n| You are an expert, you have been provided with a question, a document image, and its OCR result retrieved based on that question. Your task is to search for the content and answer these questions using both the retrieved information and the document image. |  | \n| You **MUST** answer the questions briefly with one or two words or very short sentences, devoid of additional elaborations. |  | \n| Write the answers within &lt;response&gt;&lt;/response&gt;. |  | \n| User: |  | \n| Question: {question} |  | \n| Retrieved Documents: {retrieved_documents} |  | ", "caption": "Table S6: Prompt template for VLM generation with image+text inputs", "description": "This table presents the prompt template used for Vision-Language Models (VLMs) in the RAG system when both image and text are used as inputs.  It outlines instructions given to the VLM to generate a concise answer (one or two words or a very short sentence) to a given question using the combined image and OCR-processed text.", "section": "III. Additional Experimental Results"}, {"content": "| **Simple Prompt:** |\n|---|---| \n| Please do OCR on the image and give all the text content in markdown format. The formula should be wrapped in $$ and the table should be parsed in LaTeX format. Only output the OCR results without any extra explanations or comments.| \n\n| **Detailed Prompt:** |\n|---|---| \n| You are a powerful OCR assistant tasked with converting PDF images to Markdown format. You MUST obey the following criteria: \n\n1. Plain text processing: \n - Precisely recognize all text in the PDF image without making assumptions in Markdown format. \n\n2. Formula Processing: \n - Convert all formulas to LaTeX. \n - Use $ $ to wrap inline formulas. \n - Use $$ $$ to wrap block formulas. \n\n3. Table Processing: \n - Convert tables to LaTeX. \n - Use \\begin{table} \\end{table} to wrap tables. \n4. Figure Handling: \n - Ignore figures from the PDF image; do not describe or convert images. \n\n5. Output Format: \n - Ensure the Markdown output has a clear structure with appropriate line breaks. \n - Maintain the original layout and format as closely as possible. \n\nFollow these guidelines strictly to ensure accurate and consistent conversion. Only output the OCR results without any extra explanations or comments.|", "caption": "Table S7: Prompt template for VLMs OCR", "description": "This table presents the prompt templates used for training Vision-Language Models (VLMs) for Optical Character Recognition (OCR) in the context of the OHRBench benchmark.  It details two prompt options: a simple prompt providing basic instructions for OCR, and a detailed prompt giving more specific directions to ensure accuracy and consistency in the extracted data.  The detailed instructions cover plain text processing, formula conversion, table parsing, and image handling, with specific formatting requirements in Markdown and LaTeX. The goal is to generate high-quality structured data for use in RAG systems.", "section": "III. Additional Experimental Results"}, {"content": "| System |  |  |\n|---|---|---|\n| **System:** | You are able to mimic the various errors introduced in OCR recognition. You are given examples with ground truth and OCR results with errors. Your job is to refer to the examples and generate three samples with increasing perturbations based on the user input. |  |\n|  | The new perturbed document could include the following OCR errors: |  |\n|  | 1. formatting issues, character recognition errors (like misrecognized letters or numbers), and slight variations in wording. |  |\n|  | 2. Redundant content and typos for plain text and formula. |  |\n|  | 3. Randomly add and delete table structure controlling character '&' for tabular data. |  |\n|  | 4. Randomly add extra columns and rows for tabular data. |  |\n|  | 5. Randomly make misaligned columns and rows for tabular data. |  |\n|  | - In the first perturbed document, make mild perturbation with 10% changes. |  |\n|  | - In the second perturbed document, make moderate perturbation with 30% changes. |  |\n|  | - In the third perturbed document, make severe perturbation with 50% changes. |  |\n|  | Output the three perturbed documents within <response></response>. |  |\n|  | You should refer to the following perturbed example to generate the perturbed document. |  |\n| **Example ground truth:** | {gt_sample} |  |\n| **Example results with mild perturbation:** | {mild_sample} |  |\n| **Example results with moderate perturbation:** | {moderate_sample} |  |\n| **Example results with severe perturbation:** | {severe_sample} |  |\n| **User:** | <Begin Document>{gt_structured_data}<End Document> |  |", "caption": "Table S8: Prompt template for Semantic Noise introduction", "description": "This table presents the prompt template used to introduce semantic noise into the dataset.  The prompt instructs the model to mimic OCR errors (typos, misrecognized numbers/letters, slight wording variations, redundant content, table structure errors) based on provided examples of ground truth and perturbed data, generating samples with increasing levels of noise (mild, moderate, severe). The prompt includes examples of original, mildly, moderately, and severely perturbed data for reference. The goal is to create a more realistic and challenging dataset that reflects the imperfections commonly found in real-world OCR outputs.", "section": "III. Additional Experimental Results"}, {"content": "|                       | Retrieval           |           | Generation         |           | Overall            |           |\n| :-------------------- | :------------------ | :-------- | :------------------ | :-------- | :------------------ | :-------- |\n|                       | LCS@1 \u2191              | LCS@5 \u2191    | EM \u2191                 | F1 \u2191      | EM@1 \u2191              | F1@1 \u2191    |\n| Simple Prompt         |                       |           |                       |           |                       |           |\n| Qwen2-VL-72B          | 52.50               | 72.27     | 26.29               | 40.65     | 23.09               | 35.30     |\n| InternVL2-Llama3-76B | 41.23               | 57.31     | 20.07               | 32.36     | 20.03               | 30.40     |\n| Detailed Prompt       |                       |           |                       |           |                       |           |\n| Qwen2-VL-72B          | 53.16               | 72.97     | 26.72               | 41.23     | 23.45               | 35.91     |\n| InternVL2-Llama3-76B | 42.43               | 57.51     | 20.74               | 32.89     | 20.58               | 31.23     |", "caption": "Table S9: Effects of Different OCR Prompts on VLMs for OCR. The detailed prompt is used due to its consistently superior performance.", "description": "This table presents a comparison of the performance of Vision-Language Models (VLMs) for Optical Character Recognition (OCR) when using different prompts.  It shows that a more detailed prompt consistently yields superior OCR results compared to a simpler prompt.  This highlights the importance of prompt engineering for optimizing VLM performance in OCR tasks.", "section": "4. Experiments"}]