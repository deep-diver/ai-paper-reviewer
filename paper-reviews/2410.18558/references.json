{"references": [{" publication_date": "2024", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "reason": "This paper introduces Qwen-VL, a large vision-language model that serves as a key component in the Aquila-VL-2B architecture.  Its versatility and strong performance make it a crucial element for achieving state-of-the-art results. The model's capability in handling diverse tasks makes its contribution essential to the proposed system.", "section_number": 4}, {" publication_date": "2023a", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "reason": "This foundational paper introduces the Qwen language model, which forms the backbone of the language tower in Aquila-VL-2B.  Its strong performance as an open-source large language model is critical to the success of the proposed multimodal model and makes it a high-impact reference.", "section_number": 4}, {" publication_date": "2024a", "fullname_first_author": "Peng Wang", "paper_title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution", "reason": "This paper presents Qwen2-VL, a significant enhancement to Qwen-VL, with improved capabilities.  The model's advancements in vision-language understanding are instrumental to the creation of a high-performing multimodal model, particularly in the generation of synthetic training data. This is evident in its performance in achieving better results for the proposed model.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Xiaohua Zhai", "paper_title": "PaliGemma: A versatile 3B VLM for transfer", "reason": "The PaliGemma model, a versatile 3B parameter Vision-Language Model, is referenced due to its competitive performance among similar sized models. Its strong performance against benchmarks helps establish the context for the proposed Aquila-VL-2B model, which claims state-of-the-art performance for similar-sized models. ", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Dan Hendrycks", "paper_title": "Gaussian error linear units (gelus)", "reason": "This paper introduces GELU activation functions which are critical components of the Aquila-VL-2B projector, contributing to its performance and ability to effectively integrate visual and textual information. The use of GELU highlights the attention to detail in the design of the proposed model, emphasizing its innovative features.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Bo Li", "paper_title": "LLaVA-onevision: Easy visual task transfer", "reason": "This paper introduces LLaVA-OneVision, the architecture on which the Aquila-VL-2B is built. It provides the fundamental design principles of the language tower, vision tower and projector. The design choices outlined in this paper have direct influence on the architecture and functioning of the proposed model.", "section_number": 4}, {" publication_date": "2024a", "fullname_first_author": "BAAI", "paper_title": "Flagscale", "reason": "The Flagscale toolkit is crucial for training large-scale multimodal models efficiently. Its use allowed the researchers to achieve a 1.7x speedup compared to DeepSpeed. This significant improvement in training efficiency underscores its critical role in the creation and performance of the Aquila-VL-2B.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Haodong Duan", "paper_title": "Vlmevalkit: An open-source toolkit for evaluating large multi-modality models", "reason": "The VLMEvalKit serves as the primary benchmark for assessing the performance of the Aquila-VL-2B model. The results obtained from this toolkit are vital in establishing the model's capabilities and performance compared to other models, thus influencing its position among other models.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Shaikat Galib", "paper_title": "H2ovl-mississippi vision language models technical report", "reason": "This paper introduces the H2OVL-Mississippi model, used as a comparison point for evaluating the performance of Aquila-VL-2B.  The results serve as a benchmark against which the performance of Aquila-VL-2B is measured and its relative strength compared to models of a similar size is determined. ", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Quan Sun", "paper_title": "Emu: Generative pretraining in multimodality", "reason": "This paper introduces Emu, a key dataset used in the creation of Infinity-MM. This dataset forms a substantial part of the training data for the proposed model, and understanding its features is critical in understanding the resulting model.   The paper's influence on the data pipeline and the model's capability makes it highly significant.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Quan Sun", "paper_title": "Generative multimodal models are in-context learners", "reason": "This paper highlights the dataset Emu, a critical source for the Infinity-MM dataset used to train Aquila-VL-2B. The insights gained from this paper into the properties and capabilities of this dataset are highly relevant to understanding the performance and overall approach of the Aquila-VL-2B model. ", "section_number": 3}, {" publication_date": "2024b", "fullname_first_author": "Junnan Li", "paper_title": "Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models", "reason": "This paper is highly relevant to the approach used for training Aquila-VL-2B due to its focus on multimodal learning and its handling of different data types (including multi-image and video data).  Its insights into training techniques, especially for handling varied data types and improving model performance, are important. ", "section_number": 5}, {" publication_date": "2023a", "fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "reason": "The paper introduces the concept of \"visual instruction tuning\", which is a technique for improving the performance of multimodal models.  The work in this paper directly influenced the approach used for training Aquila-VL-2B. The impact of this method is shown in the significant results reported.", "section_number": 2}, {" publication_date": "2024a", "fullname_first_author": "Haotian Liu", "paper_title": "Improved baselines with visual instruction tuning", "reason": "This paper builds upon the concept of visual instruction tuning, providing further insights into its effectiveness for improving the performance of multimodal models. The techniques described in this paper align with the training methods used for Aquila-VL-2B and its use directly improved the results obtained in the proposed model.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This foundational paper on transferable visual models is highly influential in the field of Vision-Language Models (VLMs). Its concepts and techniques directly inform the development and training of many state-of-the-art VLMs, including the Aquila-VL-2B model, thereby making it an essential reference for the research.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Mingxin Huang", "paper_title": "Mini-monkey: Multi-scale adaptive cropping for multimodal large language models", "reason": "This paper introduces the MiniMonkey model, which is used as a benchmark for comparing the performance of Aquila-VL-2B.  The evaluation against MiniMonkey helps demonstrate the strengths and advancements of Aquila-VL-2B in comparison to another model of similar size and design.  This comparison is critical for understanding the relative position of Aquila-VL-2B within the current landscape of VLM research.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Shengbang Tong", "paper_title": "Cambrian-1: A fully open, vision-centric exploration of multimodal llms", "reason": "The Cambrian-1 dataset and the insights from this paper are extremely relevant to understanding the challenges in open-source multimodal instruction datasets and the approaches used to create a high-quality dataset. This understanding is vital in judging the significance of the Infinity-MM dataset proposed in the paper. ", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Hugo Lauren\u00e7on", "paper_title": "Building and better understanding vision-language models: insights and future directions", "reason": "This highly relevant paper offers insights into the construction and functionality of vision-language models (VLMs). This perspective is crucial to understand the background of the research and to contextualize the contributions of the Aquila-VL-2B model, demonstrating its unique features and its position among state-of-the-art models.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Deyao Zhu", "paper_title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models", "reason": "This paper is included because of its relevance to the creation of high-quality multimodal data, a critical factor behind the design of the Aquila-VL-2B model.  This paper explores advanced techniques in vision-language understanding that are directly relevant to creating the dataset and training the final model.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuan Yao", "paper_title": "Minicpm-v: A gpt-4v level mllm on your phone", "reason": "This paper introduces MiniCPM-V, a strong benchmark model for comparison.  The comparison helps establish the positioning of Aquila-VL-2B within the landscape of similar-sized models. The model is evaluated against MiniCPM-V to showcase the relative improvements and contributions of the proposed model.", "section_number": 5}]}