[{"figure_path": "https://arxiv.org/html/2502.11157/x1.png", "caption": "Figure 1: (1) LLM self-reflection is unreliable (2) Binary verification lacks depth, (3) Chain-of-Thought (CoT) verification is deeper but more expensive, (4) GenRM with CoT combines generation and verification without step-wise assessment, (5) Dyve, our proposed framework that dynamically combines fast System 1 and deep System 2 verification.", "description": "Figure 1 illustrates the limitations of existing approaches to process verification in LLMs and introduces Dyve as a superior alternative.  It compares five methods:\n\n1. **LLM Self-Reflection:** Shows the unreliability of LLMs relying solely on their internal reflection mechanisms for identifying errors.\n2. **Binary Verification:** Highlights the lack of depth in simple yes/no verification systems, which fail to capture the nuances of complex reasoning processes.\n3. **Chain-of-Thought (CoT) Verification:** Demonstrates that while deeper analysis offered by CoT methods improves accuracy, it comes at a higher computational cost.\n4. **GenRM with CoT:**  Illustrates that generative models (GenRMs) using CoT may combine generation and verification but lack the step-wise assessment crucial for precise error identification.\n5. **Dyve:** Introduces Dyve, the proposed model, which dynamically combines the speed of System 1 (fast, intuitive verification) and the thoroughness of System 2 (deeper analysis) verification to achieve optimal performance.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.11157/extracted/6207679/images/inference_speed_comparison.png", "caption": "Figure 2: Inference speed comparison on ProcesBench, time per sample in seconds, for System-1, Dyve, and DeepSeek-R1-14B.", "description": "This figure presents a bar chart comparing the inference speed of three different models on the ProcessBench dataset: System-1, Dyve, and DeepSeek-R1-14B.  The y-axis represents the time per sample in seconds, and the x-axis shows the four subsets of ProcessBench (GSM8K, MATH, Olympiad, Omni).  The chart visually demonstrates the relative efficiency of each model across different problem complexities, allowing for a comparison of their computational performance.  It shows that System-1 is the fastest, Dyve is moderately fast, balancing speed and performance, and DeepSeek-R1-14B is the slowest.", "section": "4.1 Benchmarks"}, {"figure_path": "https://arxiv.org/html/2502.11157/extracted/6207679/images/side_by_side_base_and_ablation14B.png", "caption": "Figure 3: Impact of model choice and step-wise consensus filtering on performance across GSM8K, MATH, OlympiadBench, and OmniMATH. The figure illustrates improvements achieved through consensus filtering and step-wise flagging, highlighting the superior performance of the 14B reasoning model over the 7B Llama.", "description": "Figure 3 presents a detailed analysis of how model selection and the step-wise consensus filtering technique impact the performance of a process verification model across four benchmark datasets: GSM8K, MATH, OlympiadBench, and OmniMATH.  The bar charts compare the accuracy of different models (a 7B parameter Llama model and a 14B parameter DeepSeek model) with and without consensus filtering and step-wise flagging. The results clearly demonstrate that employing consensus filtering and step-wise flagging significantly improves accuracy, particularly with the larger, 14B parameter model. This showcases the effectiveness of these techniques in enhancing the model's ability to accurately identify process errors.", "section": "4.1 Benchmarks"}, {"figure_path": "https://arxiv.org/html/2502.11157/extracted/6207679/images/accuracy_vs_exponent.png", "caption": "Figure 4: Comparison of Dyve, Dyve System1 and Majority Vote with different generation budget when integrating with Proposer LLMs (DeepSeek-R1-Distill-Qwen-14B as solid line, Qwen2.5-MATH-7B-Instruct as dotted line).", "description": "This figure compares the performance of three different methods for mathematical problem solving when integrated with two different proposer LLMs. The methods compared are: Dyve (which combines fast and slow thinking), Dyve System 1 (fast thinking only), and Majority Vote (a simpler method).  The two proposer LLMs used are DeepSeek-R1-Distill-Qwen-14B (solid line) and Qwen2.5-MATH-7B-Instruct (dotted line). The x-axis represents the generation budget (number of attempts to generate a solution), and the y-axis shows the accuracy achieved.  The figure demonstrates how Dyve's adaptive approach improves accuracy compared to the other methods, especially with larger generation budgets.", "section": "4.3 Integrating Dyve with Proposer LLMs"}]