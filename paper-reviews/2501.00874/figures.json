[{"figure_path": "https://arxiv.org/html/2501.00874/x1.png", "caption": "Figure 1: Overview of LUSIFER. Left: Align a multilingual encoder with the target English-centric LLM only using English data and a minimal set of trainable parameter. Center: End-to-end representation finetune through contrastive learning on English text-embedding tasks using LoRA. Right: During inference, LUSIFER successfully processes text-embedding tasks across multiple languages.", "description": "This figure illustrates the architecture and training process of the LUSIFER model. The left panel shows how a multilingual encoder is aligned with an English-centric Large Language Model (LLM) using only English data and a small number of trainable parameters. This alignment step allows the LLM to process multilingual information without explicit multilingual training. The center panel depicts the end-to-end fine-tuning of the model's representation through contrastive learning using LoRA on English text embedding tasks.  Finally, the right panel demonstrates the inference stage where the fully trained LUSIFER model successfully processes various text embedding tasks across multiple languages.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2501.00874/x2.png", "caption": "Figure 2: Overview of tasks and datasets in our benchmark. Crosslingual datasets are marked with a blue shade.", "description": "This figure provides a comprehensive overview of the benchmark datasets used in the paper to evaluate multilingual embedding models.  It illustrates the five main embedding tasks (classification, clustering, retrieval, reranking, and semantic textual similarity (STS)) and the 123 diverse datasets used for evaluation across 14 languages.  The datasets are categorized by task, allowing for a clear visualization of the benchmark's scope and the distribution of tasks across languages. Cross-lingual datasets, where queries and documents are in different languages, are highlighted with a blue shade, emphasizing their importance in assessing cross-lingual capabilities.", "section": "4.1 Benchmark"}, {"figure_path": "https://arxiv.org/html/2501.00874/x3.png", "caption": "(a) Classification tasks", "description": "This figure presents a comparison of the performance of LUSIFER and several baseline models on classification tasks.  The figure likely shows the performance metrics (such as accuracy or F1-score) achieved by each model across multiple languages.  It likely visualizes the relative strengths and weaknesses of LUSIFER compared to other state-of-the-art multilingual embedding models, particularly showcasing its ability to enhance performance without the need for explicit multilingual training data. The models' performances are likely displayed using a radar chart, where each axis represents a different language and the length of each spoke on the chart indicates the model's performance.", "section": "4.3 Main Results"}, {"figure_path": "https://arxiv.org/html/2501.00874/x4.png", "caption": "(b) Clustering tasks", "description": "This figure displays a comparison of the performance of LUSIFER and various baseline models on clustering tasks.  It visually represents the effectiveness of each model's ability to group similar data points together accurately.  The plot likely shows a performance metric (e.g., V-measure) across different languages, providing insights into the cross-lingual capabilities of each model.  LUSIFER is expected to demonstrate improvements, particularly for medium and low-resource languages.", "section": "4.3 Main Results"}, {"figure_path": "https://arxiv.org/html/2501.00874/x5.png", "caption": "Figure 3: Performance comparison of LUSIFER and baseline models on Classification and Clustering tasks.", "description": "This figure (Figure 3) presents a comparison of LUSIFER's performance against various baseline models on classification and clustering tasks.  It visually represents the average performance across multiple languages for both task types, highlighting LUSIFER's improvements, particularly for languages with limited resources. The visualization likely uses a radar chart or similar plot type to compare performance across different languages on each task.  It showcases LUSIFER's superior multilingual capabilities compared to existing models.", "section": "4.3 Main Results"}, {"figure_path": "https://arxiv.org/html/2501.00874/x6.png", "caption": "Figure 4: Performance comparison of LUSIFER and baseline models on Reranking tasks.", "description": "This figure displays a comparison of the performance of the LUSIFER model against several baseline models across various reranking tasks.  The results are likely presented visually, possibly as a bar chart or line graph, showing the performance scores (e.g., Mean Average Precision, MAP) for each model on each task. This allows for a direct visual comparison of LUSIFER's effectiveness in reranking compared to established methods.", "section": "4.5 Task-Specific Performance"}, {"figure_path": "https://arxiv.org/html/2501.00874/x7.png", "caption": "(a) Retrieval tasks", "description": "This figure shows a comparison of LUSIFER and baseline models' performance on retrieval tasks.  The radar chart visualizes the average performance across multiple datasets and languages, illustrating LUSIFER's strengths and weaknesses in comparison to existing methods. Each axis represents a specific retrieval task or language, and the radial distance from the center indicates the performance score.  This visualization helps to understand the relative strengths and weaknesses of LUSIFER across different retrieval scenarios and languages.", "section": "4.5 Task-Specific Performance"}, {"figure_path": "https://arxiv.org/html/2501.00874/x8.png", "caption": "(b) STS tasks", "description": "This figure displays a comparison of LUSIFER's performance against various baseline models across different Semantic Textual Similarity (STS) tasks.  The plot likely uses a radar chart or similar visualization to show the relative performance of each model on multiple STS datasets. Each axis represents a different STS task, and the distance from the center to the point on each axis indicates the model's performance on that specific task. This allows for a direct comparison of the models' strengths and weaknesses across the range of STS tasks.", "section": "4.5 Task-Specific Performance"}, {"figure_path": "https://arxiv.org/html/2501.00874/x9.png", "caption": "Figure 5: Performance comparison of LUSIFER and baseline models on Retrieval and STS tasks.", "description": "This figure presents a comparison of LUSIFER's performance against several baseline models across two key natural language processing tasks: Retrieval and Semantic Textual Similarity (STS).  The results are visualized to show the relative performance of LUSIFER and each baseline model across different languages.  This allows for a clear evaluation of LUSIFER's effectiveness in improving multilingual representation capabilities, particularly for languages with limited resources.", "section": "4.5 Task-Specific Performance"}]