[{"figure_path": "https://arxiv.org/html/2502.19587/x1.png", "caption": "Figure 1: GLUE ablation scores on the development set. Modifications in grey are not included in the subsequent models. Increasing data size and diversity leads to the highest relative improvement (M\u20622\ud835\udc402M2italic_M 2, +3.6%percent3.6+3.6\\%+ 3.6 %), followed by the model size (M\u20627\ud835\udc407M7italic_M 7, +2.9%percent2.9+2.9\\%+ 2.9 %). Packing the sequences and using the LLaMA 2 tokenizer cause the largest relative drops (M\u20626\ud835\udc406M6italic_M 6, \u22122.9%percent2.9-2.9\\%- 2.9 %, M\u20623\ud835\udc403M3italic_M 3, \u22122.1%percent2.1-2.1\\%- 2.1 %).", "description": "This figure displays the results of an ablation study conducted on the GLUE benchmark.  The study systematically incorporates modifications to a BERT-base model, evaluating the impact of each change on the overall GLUE score. The x-axis represents the successive models (M0-M10), with each model incorporating a modification. The y-axis shows the GLUE development set score. The figure highlights that increasing dataset size (M2) and model size (M7) lead to the largest positive impact on performance.  Conversely,  modifying the tokenizer (M3) and packing sequences (M6) result in significant performance decreases. The greyed-out modifications indicate changes that were not included in subsequent model iterations.", "section": "Effect of Design Choices"}, {"figure_path": "https://arxiv.org/html/2502.19587/x4.png", "caption": "Figure 2: Pseudo-Perplexity in function of the sequence length for NeoBERT1024 (left) and NeoBERT4096 (right). This validates the effectiveness of the final pre-training stage on NeoBERT\u2019s ability to model long sequences.", "description": "Figure 2 presents a comparison of the pseudo-perplexity scores achieved by two versions of the NeoBERT model \u2013 NeoBERT1024 and NeoBERT4096 \u2013 across varying sequence lengths.  Pseudo-perplexity serves as a measure of how well the model predicts the next token in a sequence; lower scores indicate better performance.  The left panel shows NeoBERT1024's performance, trained with a maximum sequence length of 1024 tokens. The right panel shows NeoBERT4096, which underwent an additional training phase with longer sequences (up to 4096 tokens). The figure demonstrates that extending the pre-training with longer sequences significantly improves the NeoBERT model's ability to handle and generate longer sequences accurately, as evidenced by the lower perplexity scores for NeoBERT4096, particularly at longer sequence lengths.", "section": "3.3 Pre-Training"}, {"figure_path": "https://arxiv.org/html/2502.19587/extracted/6236292/figures/mteb_pretrained.png", "caption": "Figure 3: Model throughput (tokens per second) as a function of sequence length (\u2191\u2191\\uparrow\u2191 is better). Above 1,02410241,0241 , 024 in sequence length, NeoBERT surpasses ModernBERTbase despite having 100\u2062M100\ud835\udc40100M100 italic_M more parameters.", "description": "Figure 3 illustrates the throughput (tokens processed per second) of various language models as the sequence length increases.  The models compared are BERTbase, ROBERTabase, BERTlarge, ROBERTalarge, NeoBERT, ModernBERTbase, and ModernBERTlarge.  The x-axis represents the sequence length, and the y-axis represents the throughput.  The figure shows that NeoBERT, despite having 100 million more parameters than ModernBERTbase, achieves a significantly higher throughput when the sequence length exceeds 1024 tokens. This highlights NeoBERT's efficiency in handling long sequences.", "section": "5.4 Efficiency"}, {"figure_path": "https://arxiv.org/html/2502.19587/x5.png", "caption": "Figure 4: Zero-shot evaluation of BERT and RoBERTa on the English subset of MTEB.", "description": "This figure displays the performance of BERT and RoBERTa models on the English subset of the MTEB benchmark without any fine-tuning.  It demonstrates the zero-shot performance of these models, meaning their performance is evaluated directly after pre-training without any task-specific adaptation.  The graph likely shows the average score across multiple tasks within the MTEB benchmark, indicating the models' inherent abilities to handle various tasks before any further training or optimization.", "section": "5.2 MTEB"}]