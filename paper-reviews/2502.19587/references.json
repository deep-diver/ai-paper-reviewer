{"references": [{"fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-01-01", "reason": "This paper introduced BERT, a foundational model in the field of NLP, which is the base architecture on which NeoBERT is built and compared against."}, {"fullname_first_author": "Yinhan Liu", "paper_title": "Roberta: A robustly optimized bert pretraining approach", "publication_date": "2019-07-01", "reason": "This paper presented RoBERTa, an improved version of BERT, which served as a significant baseline in the experiments and comparisons within the NeoBERT paper."}, {"fullname_first_author": "Alex Wang", "paper_title": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "publication_date": "2019-01-01", "reason": "This paper introduced the GLUE benchmark, a standard evaluation suite used extensively in the NeoBERT paper to assess the model's performance on various NLP tasks."}, {"fullname_first_author": "Niklas Muennighoff", "paper_title": "MTEB: Massive Text Embedding Benchmark", "publication_date": "2023-03-01", "reason": "This paper introduced the MTEB benchmark, a more recent and challenging evaluation suite, used to evaluate the embedding quality of NeoBERT, making it crucial for assessing its effectiveness."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-01", "reason": "This paper introduced the LLaMA 2 language model, and NeoBERT utilizes the tokenizer from this paper as part of its architecture."}]}