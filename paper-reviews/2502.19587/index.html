<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>NeoBERT: A Next-Generation BERT &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="NeoBERT: A Next-Generation BERT &#183; HF Daily Paper Reviews by AI"><meta name=description content="NeoBERT: A new encoder that enhances bidirectional language understanding with cutting-edge architecture, data, and training, achieving SOTA results with only 250M parameters."><meta name=keywords content="Natural Language Processing,Large Language Models,üè¢ Polytechnique Montr√©al,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.19587/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.19587/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="NeoBERT: A Next-Generation BERT"><meta property="og:description" content="NeoBERT: A new encoder that enhances bidirectional language understanding with cutting-edge architecture, data, and training, achieving SOTA results with only 250M parameters."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-02-26T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-26T00:00:00+00:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="üè¢ Polytechnique Montr√©al"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.19587/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.19587/cover.png"><meta name=twitter:title content="NeoBERT: A Next-Generation BERT"><meta name=twitter:description content="NeoBERT: A new encoder that enhances bidirectional language understanding with cutting-edge architecture, data, and training, achieving SOTA results with only 250M parameters."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"NeoBERT: A Next-Generation BERT","headline":"NeoBERT: A Next-Generation BERT","abstract":"NeoBERT: A new encoder that enhances bidirectional language understanding with cutting-edge architecture, data, and training, achieving SOTA results with only 250M parameters.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2502.19587\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-02-26T00:00:00\u002b00:00","datePublished":"2025-02-26T00:00:00\u002b00:00","dateModified":"2025-02-26T00:00:00\u002b00:00","keywords":["Natural Language Processing","Large Language Models","üè¢ Polytechnique Montr√©al"],"mainEntityOfPage":"true","wordCount":"2699"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-20/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-20</p></a><a href=/ai-paper-reviewer/2025-03-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-21</p></a><a href=/ai-paper-reviewer/2025-03-24/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-24</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-20/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-20</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-21</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-24/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-24</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2502.19587/cover_hu8158122700658466157.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2502.19587/>NeoBERT: A Next-Generation BERT</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">NeoBERT: A Next-Generation BERT</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-02-26T00:00:00+00:00>26 February 2025</time><span class="px-2 text-primary-500">&#183;</span><span>2699 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">13 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2502.19587/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2502.19587/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/natural-language-processing/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Natural Language Processing
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/large-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-polytechnique-montr%C3%A9al/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Polytechnique Montr√©al</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#neobert-intro>NeoBERT Intro</a></li><li><a href=#glue-analysis>GLUE Analysis</a></li><li><a href=#mteb-focus>MTEB Focus</a></li><li><a href=#future-encoder>Future Encoder</a></li><li><a href=#training-detail>Training Detail</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#neobert-intro>NeoBERT Intro</a></li><li><a href=#glue-analysis>GLUE Analysis</a></li><li><a href=#mteb-focus>MTEB Focus</a></li><li><a href=#future-encoder>Future Encoder</a></li><li><a href=#training-detail>Training Detail</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2502.19587</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Lola Le Breton et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-02-28</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2502.19587 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2502.19587 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2502.19587/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Recent progress in auto-regressive models has overshadowed advancements in encoders like BERT, crucial for many NLP tasks. There&rsquo;s a growing need for updated encoders leveraging modern techniques. Existing solutions focus on fine-tuning but neglect inherent limitations of pre-trained backbones. The lack of standardized evaluation makes comparison between the pre-trained backbones difficult.</p><p>To tackle this, the study introduces <strong>NeoBERT</strong>, a next-generation encoder with state-of-the-art architecture, data, and training methods. It is a plug-and-play replacement with an optimal depth-to-width ratio and an extended context length. It uses a standardized fine-tuning to ensure fair evaluation and achieves state-of-the-art results on MTEB with only 250M parameters. The released code, data, and checkpoints promote research.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-48a36d0df5b02df60d75d30fd87fe605></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-48a36d0df5b02df60d75d30fd87fe605",{strings:[" NeoBERT, a next-generation BERT, achieves state-of-the-art performance on MTEB with a compact 250M parameter size. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-1880fa1803c4ec09acc3405c3bfa1bf0></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-1880fa1803c4ec09acc3405c3bfa1bf0",{strings:[" The paper introduces a systematic fine-tuning strategy to isolate the impact of pre-training improvements. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-0630cdb97437e1bf4267c92e632c9ca1></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-0630cdb97437e1bf4267c92e632c9ca1",{strings:[" Extensive ablations identify key architectural and training modifications that significantly enhance encoder performance. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>NeoBERT offers researchers a robust, efficient, and accessible encoder model, pushing the boundaries of bidirectional language understanding and providing a valuable tool for diverse NLP applications, especially in resource-constrained environments. Its detailed ablation studies and standardized evaluation framework promote reproducibility and fair comparisons.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.19587/x1.png alt></figure></p><blockquote><p>üîº This figure displays the results of an ablation study conducted on the GLUE benchmark. The study systematically incorporates modifications to a BERT-base model, evaluating the impact of each change on the overall GLUE score. The x-axis represents the successive models (M0-M10), with each model incorporating a modification. The y-axis shows the GLUE development set score. The figure highlights that increasing dataset size (M2) and model size (M7) lead to the largest positive impact on performance. Conversely, modifying the tokenizer (M3) and packing sequences (M6) result in significant performance decreases. The greyed-out modifications indicate changes that were not included in subsequent model iterations.</p><details><summary>read the caption</summary>Figure 1: GLUE ablation scores on the development set. Modifications in grey are not included in the subsequent models. Increasing data size and diversity leads to the highest relative improvement (M‚Å¢2ùëÄ2M2italic_M 2, +3.6%percent3.6+3.6\%+ 3.6 %), followed by the model size (M‚Å¢7ùëÄ7M7italic_M 7, +2.9%percent2.9+2.9\%+ 2.9 %). Packing the sequences and using the LLaMA 2 tokenizer cause the largest relative drops (M‚Å¢6ùëÄ6M6italic_M 6, ‚àí2.9%percent2.9-2.9\%- 2.9 %, M‚Å¢3ùëÄ3M3italic_M 3, ‚àí2.1%percent2.1-2.1\%- 2.1 %).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S3.T1.11.11><tr class=ltx_tr id=S3.T1.11.11.12><td class="ltx_td ltx_border_tt" id=S3.T1.11.11.12.1 style=padding-top:.9pt;padding-bottom:.9pt></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=2 id=S3.T1.11.11.12.2 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.12.2.1 style=font-size:90%>BERT</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=2 id=S3.T1.11.11.12.3 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.12.3.1 style=font-size:90%>RoBERTa</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S3.T1.11.11.12.4 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.12.4.1 style=font-size:90%>NomicBERT</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=2 id=S3.T1.11.11.12.5 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.12.5.1 style=font-size:90%>ModernBERT</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S3.T1.11.11.12.6 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.12.6.1 style=font-size:90%>NeoBERT</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.13><td class=ltx_td id=S3.T1.11.11.13.1 style=padding-top:.9pt;padding-bottom:.9pt></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.13.2 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_italic" id=S3.T1.11.11.13.2.1 style=font-size:90%>base</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.13.3 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_italic" id=S3.T1.11.11.13.3.1 style=font-size:90%>large</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.13.4 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_italic" id=S3.T1.11.11.13.4.1 style=font-size:90%>base</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.13.5 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_italic" id=S3.T1.11.11.13.5.1 style=font-size:90%>large</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.13.6 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_italic" id=S3.T1.11.11.13.6.1 style=font-size:90%>base</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.13.7 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_italic" id=S3.T1.11.11.13.7.1 style=font-size:90%>base</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.13.8 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_italic" id=S3.T1.11.11.13.8.1 style=font-size:90%>large</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.13.9 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_italic" id=S3.T1.11.11.13.9.1 style=font-size:90%>medium</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.14><td class="ltx_td ltx_align_left ltx_border_t" id=S3.T1.11.11.14.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.14.1.1 style=font-size:90%>Layers</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S3.T1.11.11.14.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.14.2.1 style=font-size:90%>12</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S3.T1.11.11.14.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.14.3.1 style=font-size:90%>24</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S3.T1.11.11.14.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.14.4.1 style=font-size:90%>12</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S3.T1.11.11.14.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.14.5.1 style=font-size:90%>24</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S3.T1.11.11.14.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.14.6.1 style=font-size:90%>12</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S3.T1.11.11.14.7 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.14.7.1 style=font-size:90%>22</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S3.T1.11.11.14.8 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.14.8.1 style=font-size:90%>28</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S3.T1.11.11.14.9 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.14.9.1 style=font-size:90%>28</span></td></tr><tr class=ltx_tr id=S3.T1.3.3.3><td class="ltx_td ltx_align_left" id=S3.T1.3.3.3.4 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.3.3.3.4.1 style=font-size:90%>Hidden Size</span></td><td class="ltx_td ltx_align_center" id=S3.T1.3.3.3.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.3.3.3.5.1 style=font-size:90%>768</span></td><td class="ltx_td ltx_align_center" id=S3.T1.1.1.1.1 style=padding-top:.9pt;padding-bottom:.9pt><math alttext="1,024" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.m1.2"><semantics id="S3.T1.1.1.1.1.m1.2a"><mrow id="S3.T1.1.1.1.1.m1.2.3.2" xref="S3.T1.1.1.1.1.m1.2.3.1.cmml"><mn id="S3.T1.1.1.1.1.m1.1.1" mathsize="90%" xref="S3.T1.1.1.1.1.m1.1.1.cmml">1</mn><mo id="S3.T1.1.1.1.1.m1.2.3.2.1" mathsize="90%" xref="S3.T1.1.1.1.1.m1.2.3.1.cmml">,</mo><mn id="S3.T1.1.1.1.1.m1.2.2" mathsize="90%" xref="S3.T1.1.1.1.1.m1.2.2.cmml">024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.2b"><list id="S3.T1.1.1.1.1.m1.2.3.1.cmml" xref="S3.T1.1.1.1.1.m1.2.3.2"><cn id="S3.T1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S3.T1.1.1.1.1.m1.1.1">1</cn><cn id="S3.T1.1.1.1.1.m1.2.2.cmml" type="integer" xref="S3.T1.1.1.1.1.m1.2.2">024</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.2c">1,024</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.m1.2d">1 , 024</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=S3.T1.3.3.3.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.3.3.3.6.1 style=font-size:90%>768</span></td><td class="ltx_td ltx_align_center" id=S3.T1.2.2.2.2 style=padding-top:.9pt;padding-bottom:.9pt><math alttext="1,024" class="ltx_Math" display="inline" id="S3.T1.2.2.2.2.m1.2"><semantics id="S3.T1.2.2.2.2.m1.2a"><mrow id="S3.T1.2.2.2.2.m1.2.3.2" xref="S3.T1.2.2.2.2.m1.2.3.1.cmml"><mn id="S3.T1.2.2.2.2.m1.1.1" mathsize="90%" xref="S3.T1.2.2.2.2.m1.1.1.cmml">1</mn><mo id="S3.T1.2.2.2.2.m1.2.3.2.1" mathsize="90%" xref="S3.T1.2.2.2.2.m1.2.3.1.cmml">,</mo><mn id="S3.T1.2.2.2.2.m1.2.2" mathsize="90%" xref="S3.T1.2.2.2.2.m1.2.2.cmml">024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m1.2b"><list id="S3.T1.2.2.2.2.m1.2.3.1.cmml" xref="S3.T1.2.2.2.2.m1.2.3.2"><cn id="S3.T1.2.2.2.2.m1.1.1.cmml" type="integer" xref="S3.T1.2.2.2.2.m1.1.1">1</cn><cn id="S3.T1.2.2.2.2.m1.2.2.cmml" type="integer" xref="S3.T1.2.2.2.2.m1.2.2">024</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m1.2c">1,024</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.2.m1.2d">1 , 024</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=S3.T1.3.3.3.7 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.3.3.3.7.1 style=font-size:90%>768</span></td><td class="ltx_td ltx_align_center" id=S3.T1.3.3.3.8 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.3.3.3.8.1 style=font-size:90%>768</span></td><td class="ltx_td ltx_align_center" id=S3.T1.3.3.3.3 style=padding-top:.9pt;padding-bottom:.9pt><math alttext="1,024" class="ltx_Math" display="inline" id="S3.T1.3.3.3.3.m1.2"><semantics id="S3.T1.3.3.3.3.m1.2a"><mrow id="S3.T1.3.3.3.3.m1.2.3.2" xref="S3.T1.3.3.3.3.m1.2.3.1.cmml"><mn id="S3.T1.3.3.3.3.m1.1.1" mathsize="90%" xref="S3.T1.3.3.3.3.m1.1.1.cmml">1</mn><mo id="S3.T1.3.3.3.3.m1.2.3.2.1" mathsize="90%" xref="S3.T1.3.3.3.3.m1.2.3.1.cmml">,</mo><mn id="S3.T1.3.3.3.3.m1.2.2" mathsize="90%" xref="S3.T1.3.3.3.3.m1.2.2.cmml">024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.m1.2b"><list id="S3.T1.3.3.3.3.m1.2.3.1.cmml" xref="S3.T1.3.3.3.3.m1.2.3.2"><cn id="S3.T1.3.3.3.3.m1.1.1.cmml" type="integer" xref="S3.T1.3.3.3.3.m1.1.1">1</cn><cn id="S3.T1.3.3.3.3.m1.2.2.cmml" type="integer" xref="S3.T1.3.3.3.3.m1.2.2">024</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.m1.2c">1,024</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.3.m1.2d">1 , 024</annotation></semantics></math></td><td class="ltx_td ltx_align_center" id=S3.T1.3.3.3.9 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.3.3.3.9.1 style=font-size:90%>768</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.15><td class="ltx_td ltx_align_left" id=S3.T1.11.11.15.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.15.1.1 style=font-size:90%>Attention Heads</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.15.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.15.2.1 style=font-size:90%>12</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.15.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.15.3.1 style=font-size:90%>16</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.15.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.15.4.1 style=font-size:90%>12</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.15.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.15.5.1 style=font-size:90%>16</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.15.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.15.6.1 style=font-size:90%>12</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.15.7 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.15.7.1 style=font-size:90%>12</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.15.8 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.15.8.1 style=font-size:90%>16</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.15.9 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.15.9.1 style=font-size:90%>12</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.16><td class="ltx_td ltx_align_left" id=S3.T1.11.11.16.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.16.1.1 style=font-size:90%>Parameters</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.16.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.16.2.1 style=font-size:90%>120M</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.16.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.16.3.1 style=font-size:90%>350M</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.16.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.16.4.1 style=font-size:90%>125M</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.16.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.16.5.1 style=font-size:90%>355M</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.16.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.16.6.1 style=font-size:90%>137M</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.16.7 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.16.7.1 style=font-size:90%>149M</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.16.8 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.16.8.1 style=font-size:90%>395M</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.16.9 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.16.9.1 style=font-size:90%>250M</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.17><td class="ltx_td ltx_align_left" id=S3.T1.11.11.17.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.17.1.1 style=font-size:90%>Activation Function</span></td><td class="ltx_td ltx_align_center" colspan=4 id=S3.T1.11.11.17.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.17.2.1 style=font-size:90%>GeLU</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.17.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.17.3.1 style=font-size:90%>SwiGLU</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.17.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.17.4.1 style=font-size:90%>GeGLU</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.17.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.17.5.1 style=font-size:90%>SwiGLU</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.18><td class="ltx_td ltx_align_left" id=S3.T1.11.11.18.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.18.1.1 style=font-size:90%>Positional Encoding</span></td><td class="ltx_td ltx_align_center" colspan=4 id=S3.T1.11.11.18.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.18.2.1 style=font-size:90%>Positional Embeddings</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.18.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.18.3.1 style=font-size:90%>RoPE</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.18.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.18.4.1 style=font-size:90%>RoPE</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.18.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.18.5.1 style=font-size:90%>RoPE</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.19><td class="ltx_td ltx_align_left" id=S3.T1.11.11.19.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.19.1.1 style=font-size:90%>Normalization</span></td><td class="ltx_td ltx_align_center" colspan=4 id=S3.T1.11.11.19.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.19.2.1 style=font-size:90%>Post-LayerNorm</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.19.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.19.3.1 style=font-size:90%>Post-LayerNorm</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.19.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.19.4.1 style=font-size:90%>Pre-LayerNorm</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.19.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.19.5.1 style=font-size:90%>Pre-RMSNorm</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.20><td class="ltx_td ltx_align_left ltx_border_t" id=S3.T1.11.11.20.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.20.1.1 style=font-size:90%>Data Sources</span></td><td class="ltx_td ltx_align_center ltx_border_t" colspan=2 id=S3.T1.11.11.20.2 style=padding-top:.9pt;padding-bottom:.9pt><table class="ltx_tabular ltx_align_middle" id=S3.T1.11.11.20.2.1><tr class=ltx_tr id=S3.T1.11.11.20.2.1.1><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.20.2.1.1.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.2.1.1.1.1 style=font-size:90%>BooksCorpus</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.20.2.1.2><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.20.2.1.2.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.2.1.2.1.1 style=font-size:90%>Wikipedia</span></td></tr></table></td><td class="ltx_td ltx_align_center ltx_border_t" colspan=2 id=S3.T1.11.11.20.3 style=padding-top:.9pt;padding-bottom:.9pt><table class="ltx_tabular ltx_align_middle" id=S3.T1.11.11.20.3.1><tr class=ltx_tr id=S3.T1.11.11.20.3.1.1><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.20.3.1.1.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.3.1.1.1.1 style=font-size:90%>BooksCorpus</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.20.3.1.2><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.20.3.1.2.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.3.1.2.1.1 style=font-size:90%>OpenWebText</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.20.3.1.3><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.20.3.1.3.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.3.1.3.1.1 style=font-size:90%>Stories / CC-News</span></td></tr></table></td><td class="ltx_td ltx_align_center ltx_border_t" id=S3.T1.11.11.20.4 style=padding-top:.9pt;padding-bottom:.9pt><table class="ltx_tabular ltx_align_middle" id=S3.T1.11.11.20.4.1><tr class=ltx_tr id=S3.T1.11.11.20.4.1.1><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.20.4.1.1.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.4.1.1.1.1 style=font-size:90%>BooksCorpus</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.20.4.1.2><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.20.4.1.2.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.4.1.2.1.1 style=font-size:90%>Wikipedia</span></td></tr></table></td><td class="ltx_td ltx_align_center ltx_border_t" colspan=2 id=S3.T1.11.11.20.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.5.1 style=font-size:90%>Undisclosed</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S3.T1.11.11.20.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.6.1 style=font-size:90%>RefinedWeb</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.21><td class="ltx_td ltx_align_left" id=S3.T1.11.11.21.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.21.1.1 style=font-size:90%>Dataset Size</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.21.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.21.2.1 style=font-size:90%>13GB</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.21.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.21.3.1 style=font-size:90%>160GB</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.21.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.21.4.1 style=font-size:90%>13GB</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.21.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.21.5.1 style=font-size:90%>-</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.21.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.21.6.1 style=font-size:90%>2.8TB</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.22><td class="ltx_td ltx_align_left" id=S3.T1.11.11.22.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.22.1.1 style=font-size:90%>Dataset Year</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.22.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.22.2.1 style=font-size:90%>2019</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.22.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.22.3.1 style=font-size:90%>2019</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.22.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.22.4.1 style=font-size:90%>2023</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.22.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.22.5.1 style=font-size:90%>-</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.22.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.22.6.1 style=font-size:90%>2023</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.23><td class="ltx_td ltx_align_left" id=S3.T1.11.11.23.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.23.1.1 style=font-size:90%>Tokenizer Level</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.23.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.23.2.1 style=font-size:90%>Character</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.23.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.23.3.1 style=font-size:90%>Byte</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.23.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.23.4.1 style=font-size:90%>Character</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.23.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.23.5.1 style=font-size:90%>Character</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.23.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.23.6.1 style=font-size:90%>Character</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.24><td class="ltx_td ltx_align_left" id=S3.T1.11.11.24.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.24.1.1 style=font-size:90%>Vocabulary Size</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.24.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.24.2.1 style=font-size:90%>30K</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.24.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.24.3.1 style=font-size:90%>50K</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.24.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.24.4.1 style=font-size:90%>30K</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.24.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.24.5.1 style=font-size:90%>50K</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.24.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.24.6.1 style=font-size:90%>30K</span></td></tr><tr class=ltx_tr id=S3.T1.10.10.10><td class="ltx_td ltx_align_left ltx_border_t" id=S3.T1.10.10.10.8 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.10.10.10.8.1 style=font-size:90%>Sequence Length</span></td><td class="ltx_td ltx_align_center ltx_border_t" colspan=2 id=S3.T1.10.10.10.9 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.10.10.10.9.1 style=font-size:90%>512</span></td><td class="ltx_td ltx_align_center ltx_border_t" colspan=2 id=S3.T1.10.10.10.10 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.10.10.10.10.1 style=font-size:90%>512</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S3.T1.4.4.4.1 style=padding-top:.9pt;padding-bottom:.9pt><math alttext="2,048" class="ltx_Math" display="inline" id="S3.T1.4.4.4.1.m1.2"><semantics id="S3.T1.4.4.4.1.m1.2a"><mrow id="S3.T1.4.4.4.1.m1.2.3.2" xref="S3.T1.4.4.4.1.m1.2.3.1.cmml"><mn id="S3.T1.4.4.4.1.m1.1.1" mathsize="90%" xref="S3.T1.4.4.4.1.m1.1.1.cmml">2</mn><mo id="S3.T1.4.4.4.1.m1.2.3.2.1" mathsize="90%" xref="S3.T1.4.4.4.1.m1.2.3.1.cmml">,</mo><mn id="S3.T1.4.4.4.1.m1.2.2" mathsize="90%" xref="S3.T1.4.4.4.1.m1.2.2.cmml">048</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.1.m1.2b"><list id="S3.T1.4.4.4.1.m1.2.3.1.cmml" xref="S3.T1.4.4.4.1.m1.2.3.2"><cn id="S3.T1.4.4.4.1.m1.1.1.cmml" type="integer" xref="S3.T1.4.4.4.1.m1.1.1">2</cn><cn id="S3.T1.4.4.4.1.m1.2.2.cmml" type="integer" xref="S3.T1.4.4.4.1.m1.2.2">048</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.1.m1.2c">2,048</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.4.1.m1.2d">2 , 048</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_t" colspan=2 id=S3.T1.7.7.7.4 style=padding-top:.9pt;padding-bottom:.9pt><math alttext="1,024" class="ltx_Math" display="inline" id="S3.T1.5.5.5.2.m1.2"><semantics id="S3.T1.5.5.5.2.m1.2a"><mrow id="S3.T1.5.5.5.2.m1.2.3.2" xref="S3.T1.5.5.5.2.m1.2.3.1.cmml"><mn id="S3.T1.5.5.5.2.m1.1.1" mathsize="90%" xref="S3.T1.5.5.5.2.m1.1.1.cmml">1</mn><mo id="S3.T1.5.5.5.2.m1.2.3.2.1" mathsize="90%" xref="S3.T1.5.5.5.2.m1.2.3.1.cmml">,</mo><mn id="S3.T1.5.5.5.2.m1.2.2" mathsize="90%" xref="S3.T1.5.5.5.2.m1.2.2.cmml">024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.2.m1.2b"><list id="S3.T1.5.5.5.2.m1.2.3.1.cmml" xref="S3.T1.5.5.5.2.m1.2.3.2"><cn id="S3.T1.5.5.5.2.m1.1.1.cmml" type="integer" xref="S3.T1.5.5.5.2.m1.1.1">1</cn><cn id="S3.T1.5.5.5.2.m1.2.2.cmml" type="integer" xref="S3.T1.5.5.5.2.m1.2.2">024</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.2.m1.2c">1,024</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.5.2.m1.2d">1 , 024</annotation></semantics></math><span class=ltx_text id=S3.T1.7.7.7.4.1 style=font-size:90%> </span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.T1.6.6.6.3.m2.1"><semantics id="S3.T1.6.6.6.3.m2.1a"><mo id="S3.T1.6.6.6.3.m2.1.1" mathsize="90%" stretchy="false" xref="S3.T1.6.6.6.3.m2.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.3.m2.1b"><ci id="S3.T1.6.6.6.3.m2.1.1.cmml" xref="S3.T1.6.6.6.3.m2.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.3.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.6.3.m2.1d">‚Üí</annotation></semantics></math><span class=ltx_text id=S3.T1.7.7.7.4.2 style=font-size:90%></span><math alttext="8,192" class="ltx_Math" display="inline" id="S3.T1.7.7.7.4.m3.2"><semantics id="S3.T1.7.7.7.4.m3.2a"><mrow id="S3.T1.7.7.7.4.m3.2.3.2" xref="S3.T1.7.7.7.4.m3.2.3.1.cmml"><mn id="S3.T1.7.7.7.4.m3.1.1" mathsize="90%" xref="S3.T1.7.7.7.4.m3.1.1.cmml">8</mn><mo id="S3.T1.7.7.7.4.m3.2.3.2.1" mathsize="90%" xref="S3.T1.7.7.7.4.m3.2.3.1.cmml">,</mo><mn id="S3.T1.7.7.7.4.m3.2.2" mathsize="90%" xref="S3.T1.7.7.7.4.m3.2.2.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.7.4.m3.2b"><list id="S3.T1.7.7.7.4.m3.2.3.1.cmml" xref="S3.T1.7.7.7.4.m3.2.3.2"><cn id="S3.T1.7.7.7.4.m3.1.1.cmml" type="integer" xref="S3.T1.7.7.7.4.m3.1.1">8</cn><cn id="S3.T1.7.7.7.4.m3.2.2.cmml" type="integer" xref="S3.T1.7.7.7.4.m3.2.2">192</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.7.4.m3.2c">8,192</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.7.7.4.m3.2d">8 , 192</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_t" id=S3.T1.10.10.10.7 style=padding-top:.9pt;padding-bottom:.9pt><math alttext="1,024" class="ltx_Math" display="inline" id="S3.T1.8.8.8.5.m1.2"><semantics id="S3.T1.8.8.8.5.m1.2a"><mrow id="S3.T1.8.8.8.5.m1.2.3.2" xref="S3.T1.8.8.8.5.m1.2.3.1.cmml"><mn id="S3.T1.8.8.8.5.m1.1.1" mathsize="90%" xref="S3.T1.8.8.8.5.m1.1.1.cmml">1</mn><mo id="S3.T1.8.8.8.5.m1.2.3.2.1" mathsize="90%" xref="S3.T1.8.8.8.5.m1.2.3.1.cmml">,</mo><mn id="S3.T1.8.8.8.5.m1.2.2" mathsize="90%" xref="S3.T1.8.8.8.5.m1.2.2.cmml">024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.8.5.m1.2b"><list id="S3.T1.8.8.8.5.m1.2.3.1.cmml" xref="S3.T1.8.8.8.5.m1.2.3.2"><cn id="S3.T1.8.8.8.5.m1.1.1.cmml" type="integer" xref="S3.T1.8.8.8.5.m1.1.1">1</cn><cn id="S3.T1.8.8.8.5.m1.2.2.cmml" type="integer" xref="S3.T1.8.8.8.5.m1.2.2">024</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.8.5.m1.2c">1,024</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.8.8.5.m1.2d">1 , 024</annotation></semantics></math><span class=ltx_text id=S3.T1.10.10.10.7.1 style=font-size:90%> </span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.T1.9.9.9.6.m2.1"><semantics id="S3.T1.9.9.9.6.m2.1a"><mo id="S3.T1.9.9.9.6.m2.1.1" mathsize="90%" stretchy="false" xref="S3.T1.9.9.9.6.m2.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.9.6.m2.1b"><ci id="S3.T1.9.9.9.6.m2.1.1.cmml" xref="S3.T1.9.9.9.6.m2.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.9.6.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.9.9.9.6.m2.1d">‚Üí</annotation></semantics></math><span class=ltx_text id=S3.T1.10.10.10.7.2 style=font-size:90%></span><math alttext="4,096" class="ltx_Math" display="inline" id="S3.T1.10.10.10.7.m3.2"><semantics id="S3.T1.10.10.10.7.m3.2a"><mrow id="S3.T1.10.10.10.7.m3.2.3.2" xref="S3.T1.10.10.10.7.m3.2.3.1.cmml"><mn id="S3.T1.10.10.10.7.m3.1.1" mathsize="90%" xref="S3.T1.10.10.10.7.m3.1.1.cmml">4</mn><mo id="S3.T1.10.10.10.7.m3.2.3.2.1" mathsize="90%" xref="S3.T1.10.10.10.7.m3.2.3.1.cmml">,</mo><mn id="S3.T1.10.10.10.7.m3.2.2" mathsize="90%" xref="S3.T1.10.10.10.7.m3.2.2.cmml">096</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.10.10.10.7.m3.2b"><list id="S3.T1.10.10.10.7.m3.2.3.1.cmml" xref="S3.T1.10.10.10.7.m3.2.3.2"><cn id="S3.T1.10.10.10.7.m3.1.1.cmml" type="integer" xref="S3.T1.10.10.10.7.m3.1.1">4</cn><cn id="S3.T1.10.10.10.7.m3.2.2.cmml" type="integer" xref="S3.T1.10.10.10.7.m3.2.2">096</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.10.10.7.m3.2c">4,096</annotation><annotation encoding="application/x-llamapun" id="S3.T1.10.10.10.7.m3.2d">4 , 096</annotation></semantics></math></td></tr><tr class=ltx_tr id=S3.T1.11.11.25><td class="ltx_td ltx_align_left" id=S3.T1.11.11.25.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.25.1.1 style=font-size:90%>Objective</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.25.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.25.2.1 style=font-size:90%>MLM + NSP</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.25.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.25.3.1 style=font-size:90%>MLM</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.25.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.25.4.1 style=font-size:90%>MLM</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.25.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.25.5.1 style=font-size:90%>MLM</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.25.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.25.6.1 style=font-size:90%>MLM</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.26><td class="ltx_td ltx_align_left" id=S3.T1.11.11.26.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.26.1.1 style=font-size:90%>Masking Rate</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.26.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.26.2.1 style=font-size:90%>15%</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.26.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.26.3.1 style=font-size:90%>15%</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.26.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.26.4.1 style=font-size:90%>30%</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.26.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.26.5.1 style=font-size:90%>30%</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.26.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.26.6.1 style=font-size:90%>20%</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.27><td class="ltx_td ltx_align_left" id=S3.T1.11.11.27.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.27.1.1 style=font-size:90%>Masking Scheme</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.27.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.27.2.1 style=font-size:90%>80/10/10</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.27.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.27.3.1 style=font-size:90%>80/10/10</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.27.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.27.4.1 style=font-size:90%>-</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.27.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.27.5.1 style=font-size:90%>-</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.27.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.27.6.1 style=font-size:90%>100</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.28><td class="ltx_td ltx_align_left" id=S3.T1.11.11.28.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.28.1.1 style=font-size:90%>Optimizer</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.28.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.28.2.1 style=font-size:90%>Adam</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.28.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.28.3.1 style=font-size:90%>Adam</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.28.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.28.4.1 style=font-size:90%>AdamW</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.28.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.28.5.1 style=font-size:90%>StableAdamW</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.28.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.28.6.1 style=font-size:90%>AdamW</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.29><td class="ltx_td ltx_align_left" id=S3.T1.11.11.29.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.29.1.1 style=font-size:90%>Scheduler</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.29.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.29.2.1 style=font-size:90%>-</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.29.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.29.3.1 style=font-size:90%>-</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.29.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.29.4.1 style=font-size:90%>-</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.29.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.29.5.1 style=font-size:90%>WSD</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.29.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.29.6.1 style=font-size:90%>CosineDecay</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.30><td class="ltx_td ltx_align_left" id=S3.T1.11.11.30.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.30.1.1 style=font-size:90%>Batch Size</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.30.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.30.2.1 style=font-size:90%>131k tokens</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.30.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.30.3.1 style=font-size:90%>131k</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.30.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.30.4.1 style=font-size:90%>8M</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.30.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.30.5.1 style=font-size:90%>448k to 5M</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.30.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.30.6.1 style=font-size:90%>2M</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.11><td class="ltx_td ltx_align_left" id=S3.T1.11.11.11.2 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.11.2.1 style=font-size:90%>Tokens Seen</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.11.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.11.3.1 style=font-size:90%>131B</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.11.4 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.11.4.1 style=font-size:90%>131B</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.11.5 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.11.5.1 style=font-size:90%>-</span></td><td class="ltx_td ltx_align_center" colspan=2 id=S3.T1.11.11.11.1 style=padding-top:.9pt;padding-bottom:.9pt><math alttext="\sim" class="ltx_Math" display="inline" id="S3.T1.11.11.11.1.m1.1"><semantics id="S3.T1.11.11.11.1.m1.1a"><mo id="S3.T1.11.11.11.1.m1.1.1" mathsize="90%" xref="S3.T1.11.11.11.1.m1.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S3.T1.11.11.11.1.m1.1b"><csymbol cd="latexml" id="S3.T1.11.11.11.1.m1.1.1.cmml" xref="S3.T1.11.11.11.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.11.11.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.T1.11.11.11.1.m1.1d">‚àº</annotation></semantics></math><span class=ltx_text id=S3.T1.11.11.11.1.1 style=font-size:90%> 2T</span></td><td class="ltx_td ltx_align_center" id=S3.T1.11.11.11.6 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.11.6.1 style=font-size:90%>2.1T</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.31><td class="ltx_td ltx_align_left ltx_border_bb" id=S3.T1.11.11.31.1 style=padding-top:.9pt;padding-bottom:.9pt><span class="ltx_text ltx_font_bold" id=S3.T1.11.11.31.1.1 style=font-size:90%>Training</span></td><td class="ltx_td ltx_align_center ltx_border_bb" colspan=2 id=S3.T1.11.11.31.2 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.2.1 style=font-size:90%>DDP</span></td><td class="ltx_td ltx_align_center ltx_border_bb" colspan=2 id=S3.T1.11.11.31.3 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.3.1 style=font-size:90%>DDP</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S3.T1.11.11.31.4 style=padding-top:.9pt;padding-bottom:.9pt><table class="ltx_tabular ltx_align_middle" id=S3.T1.11.11.31.4.1><tr class=ltx_tr id=S3.T1.11.11.31.4.1.1><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.31.4.1.1.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.4.1.1.1.1 style=font-size:90%>DeepSpeed</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.31.4.1.2><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.31.4.1.2.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.4.1.2.1.1 style=font-size:90%>FlashAttention</span></td></tr></table></td><td class="ltx_td ltx_align_center ltx_border_bb" colspan=2 id=S3.T1.11.11.31.5 style=padding-top:.9pt;padding-bottom:.9pt><table class="ltx_tabular ltx_align_middle" id=S3.T1.11.11.31.5.1><tr class=ltx_tr id=S3.T1.11.11.31.5.1.1><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.31.5.1.1.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.5.1.1.1.1 style=font-size:90%>Alternate Attention</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.31.5.1.2><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.31.5.1.2.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.5.1.2.1.1 style=font-size:90%>Unpadding</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.31.5.1.3><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.31.5.1.3.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.5.1.3.1.1 style=font-size:90%>FlashAttention</span></td></tr></table></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S3.T1.11.11.31.6 style=padding-top:.9pt;padding-bottom:.9pt><table class="ltx_tabular ltx_align_middle" id=S3.T1.11.11.31.6.1><tr class=ltx_tr id=S3.T1.11.11.31.6.1.1><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.31.6.1.1.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.6.1.1.1.1 style=font-size:90%>DeepSpeed</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.31.6.1.2><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.31.6.1.2.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.6.1.2.1.1 style=font-size:90%>FlashAttention</span></td></tr></table></td></tr></table></table></figure><blockquote><p>üîº This table provides a detailed comparison of the architectures, training data, and pre-training configurations used for several BERT-like language models, including BERT, RoBERTa, NomicBERT, ModernBERT, and NeoBERT. For each model, it lists key architectural parameters such as the number of layers, hidden size, attention heads, and the total number of parameters. It also details the training data used (size and source), the vocabulary size, sequence length, the pre-training objective (masked language modeling, and next sentence prediction if used), masking rate, masking scheme, optimizer, learning rate scheduler, batch size, and the total number of tokens seen during training. This allows for a comprehensive understanding of the differences in model design and training procedures across these related models.</p><details><summary>read the caption</summary>Table 1: Comparison of Model Architectures, Training Data, and Pre-Training Configurations.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">NeoBERT Intro<div id=neobert-intro class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#neobert-intro aria-label=Anchor>#</a></span></h4><p><strong>NeoBERT</strong>, a next-generation encoder, aims to bridge the gap between the rapid advancements in auto-regressive language models and the relatively stagnant progress of bidirectional encoders like <strong>BERT</strong> and <strong>RoBERTa</strong>. The paper addresses the need for incorporating state-of-the-art innovations in architecture, data, and pre-training methodologies into BERT-like models. <strong>NeoBERT</strong> is designed for seamless adoption as a plug-and-play replacement, with an optimal depth-to-width ratio and an extended context length. It achieves superior results on the MTEB benchmark while maintaining a compact size, outperforming larger models. The authors also emphasize their commitment to open research by releasing all code, data, checkpoints, and training scripts. This makes <strong>NeoBERT</strong> a valuable contribution to the NLP community.</p><h4 class="relative group">GLUE Analysis<div id=glue-analysis class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#glue-analysis aria-label=Anchor>#</a></span></h4><p>The <strong>GLUE benchmark&rsquo;s role</strong> as a cornerstone for language modeling is discussed, yet its limitations due to its age and tendency for models to overfit are acknowledged. Despite these limitations, the paper uses <strong>GLUE scores</strong> to allow for comparison with existing encoders. To fine-tune, the standard practices are followed: classical hyperparameter search and transfer learning between related tasks. As a result of the above <strong>NeoBERT shows comparable results to large models</strong>, despite being 100M to 150M parameters smaller, and the full results are in Table 3.</p><h4 class="relative group">MTEB Focus<div id=mteb-focus class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#mteb-focus aria-label=Anchor>#</a></span></h4><p>The paper emphasizes MTEB (Massive Text Embedding Benchmark) as a crucial evaluation benchmark, going beyond traditional metrics like GLUE. It highlights MTEB&rsquo;s capacity to assess embedding models across diverse tasks. <strong>A key focus is the decoupling of pre-training and fine-tuning impacts on MTEB performance</strong>. The authors critique existing approaches that heavily rely on complex, task-specific fine-tuning, making it difficult to isolate the benefits of the underlying pre-trained models. They advocate for a standardized, model-agnostic fine-tuning strategy to fairly compare different pre-training techniques. The approach emphasizes the need for simple, reproducible fine-tuning. <strong>The core idea is to establish a clear understanding of how pre-training enhancements translate to downstream performance</strong> without the confounding effects of intricate fine-tuning methods. Ultimately, this helps drive progress in pre-training and unlocks more generalizable encoder models.</p><h4 class="relative group">Future Encoder<div id=future-encoder class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-encoder aria-label=Anchor>#</a></span></h4><p>While the provided paper centers on <strong>NeoBERT, a next-generation encoder model, and doesn&rsquo;t explicitly detail &lsquo;Future Encoder&rsquo; concepts</strong>, one can infer potential advancements. Future encoders will likely leverage <strong>novel architectural designs</strong> beyond the current Transformer, perhaps exploring attention alternatives or incorporating ideas from mixture of experts paradigm. They will be pre-trained on <strong>increasingly massive and diverse datasets</strong>, potentially synthetic or incorporating multi-modal information. Future progress includes <strong>efficient long context handling</strong> using techniques like sparse attention or recurrence, allowing modeling of complex relationships. Crucially, future research will involve <strong>standardizing fine-tuning protocols</strong> and developing <strong>zero-shot evaluation methods</strong> to ensure unbiased assessments and fair comparisons of different encoder architectures, contributing towards robust, adaptable, and high-performing models.</p><h4 class="relative group">Training Detail<div id=training-detail class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#training-detail aria-label=Anchor>#</a></span></h4><p>The training details section is crucial for understanding the experimental setup. <strong>NeoBERT</strong> used 8 H100 GPUs for 1,050,000 steps, totaling 6,000 GPU hours, showcasing resource intensity. A local batch size of 32 was used with 8 gradient accumulation steps, equaling a 2M token batch size. The max sequence length was 1,024 initially, and raised to 4,096 later. <strong>Keeping the batch size fixed while extending sequence length</strong> is vital, influencing model performance. This methodology helps maximize memory and compute resources during training, optimizing the architecture and training hyperparameters.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.19587/x4.png alt></figure></p><blockquote><p>üîº Figure 2 presents a comparison of the pseudo-perplexity scores achieved by two versions of the NeoBERT model ‚Äì NeoBERT1024 and NeoBERT4096 ‚Äì across varying sequence lengths. Pseudo-perplexity serves as a measure of how well the model predicts the next token in a sequence; lower scores indicate better performance. The left panel shows NeoBERT1024&rsquo;s performance, trained with a maximum sequence length of 1024 tokens. The right panel shows NeoBERT4096, which underwent an additional training phase with longer sequences (up to 4096 tokens). The figure demonstrates that extending the pre-training with longer sequences significantly improves the NeoBERT model&rsquo;s ability to handle and generate longer sequences accurately, as evidenced by the lower perplexity scores for NeoBERT4096, particularly at longer sequence lengths.</p><details><summary>read the caption</summary>Figure 2: Pseudo-Perplexity in function of the sequence length for NeoBERT1024 (left) and NeoBERT4096 (right). This validates the effectiveness of the final pre-training stage on NeoBERT‚Äôs ability to model long sequences.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.19587/extracted/6236292/figures/mteb_pretrained.png alt></figure></p><blockquote><p>üîº Figure 3 illustrates the throughput (tokens processed per second) of various language models as the sequence length increases. The models compared are BERTbase, ROBERTabase, BERTlarge, ROBERTalarge, NeoBERT, ModernBERTbase, and ModernBERTlarge. The x-axis represents the sequence length, and the y-axis represents the throughput. The figure shows that NeoBERT, despite having 100 million more parameters than ModernBERTbase, achieves a significantly higher throughput when the sequence length exceeds 1024 tokens. This highlights NeoBERT&rsquo;s efficiency in handling long sequences.</p><details><summary>read the caption</summary>Figure 3: Model throughput (tokens per second) as a function of sequence length (‚Üë‚Üë\uparrow‚Üë is better). Above 1,02410241,0241 , 024 in sequence length, NeoBERT surpasses ModernBERTbase despite having 100‚Å¢M100ùëÄ100M100 italic_M more parameters.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.19587/x5.png alt></figure></p><blockquote><p>üîº This figure displays the performance of BERT and RoBERTa models on the English subset of the MTEB benchmark without any fine-tuning. It demonstrates the zero-shot performance of these models, meaning their performance is evaluated directly after pre-training without any task-specific adaptation. The graph likely shows the average score across multiple tasks within the MTEB benchmark, indicating the models&rsquo; inherent abilities to handle various tasks before any further training or optimization.</p><details><summary>read the caption</summary>Figure 4: Zero-shot evaluation of BERT and RoBERTa on the English subset of MTEB.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S3.T1.11.11.20.2.1><tr class=ltx_tr id=S3.T1.11.11.20.2.1.1><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.20.2.1.1.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.2.1.1.1.1 style=font-size:90%>BooksCorpus</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.20.2.1.2><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.20.2.1.2.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.2.1.2.1.1 style=font-size:90%>Wikipedia</span></td></tr></table></table></figure><blockquote><p>üîº This table details the modifications made during a series of ablation experiments to improve a BERT-like model, ultimately resulting in NeoBERT. It shows the changes introduced iteratively to the base model (M0, similar to BERT) in each step (M1-M9), highlighting modifications to embeddings, activation functions, normalization, datasets, tokenizers, optimizers, schedulers, masking schemes, model size, and context length. The final model, M9, represents NeoBERT.</p><details><summary>read the caption</summary>Table 2: Modifications between successive ablations. The initial M‚Å¢0ùëÄ0M0italic_M 0 baseline corresponds to a model similar to BERT, while M‚Å¢9ùëÄ9M9italic_M 9 corresponds to NeoBERT.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S3.T1.11.11.20.3.1><tr class=ltx_tr id=S3.T1.11.11.20.3.1.1><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.20.3.1.1.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.3.1.1.1.1 style=font-size:90%>BooksCorpus</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.20.3.1.2><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.20.3.1.2.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.3.1.2.1.1 style=font-size:90%>OpenWebText</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.20.3.1.3><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.20.3.1.3.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.3.1.3.1.1 style=font-size:90%>Stories / CC-News</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the GLUE (General Language Understanding Evaluation) benchmark scores achieved by various language models on their development sets. It compares the performance of NeoBERT against several established models including BERT, RoBERTa, DeBERTa, NomicBERT, GTE, and ModernBERT. The scores are broken down by individual tasks within the GLUE benchmark, allowing for a detailed comparison of each model&rsquo;s strengths and weaknesses across different NLP tasks. The table also indicates the size (in parameters) of each model, showing how NeoBERT&rsquo;s performance compares even with smaller model size.</p><details><summary>read the caption</summary>Table 3: GLUE scores on the development set. Baseline scores were retrieved as follows: BERT from Table 1 of Devlin et¬†al. (2019), RoBERTa from Table 8 of Liu et¬†al. (2019), DeBERTa from Table 3 of He et¬†al. (2023), NomicBERT from Table 2 of Nussbaum et¬†al. (2024), GTE from Table 13 of Zhang et¬†al. (2024), and ModernBERT from Table 5 of Warner et¬†al. (2024).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S3.T1.11.11.20.4.1><tr class=ltx_tr id=S3.T1.11.11.20.4.1.1><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.20.4.1.1.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.4.1.1.1.1 style=font-size:90%>BooksCorpus</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.20.4.1.2><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.20.4.1.2.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.20.4.1.2.1.1 style=font-size:90%>Wikipedia</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the results of the MTEB (Massive Text Embedding Benchmark) English subset evaluation. Multiple pre-trained language models were fine-tuned using a contrastive learning approach for 2000 steps. The table shows the performance of each model across seven different tasks within the benchmark (Classification, Clustering, Pair Classification, Reranking, Retrieval, Semantic Textual Similarity (STS), and Summarization), along with the average score across all tasks. The models are categorized by size (Base, Medium, Large), providing a comparison of performance across different model scales.</p><details><summary>read the caption</summary>Table 4: MTEB scores on the English subset after 2,000 steps of fine-tuning with contrastive learning.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S3.T1.11.11.31.4.1><tr class=ltx_tr id=S3.T1.11.11.31.4.1.1><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.31.4.1.1.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.4.1.1.1.1 style=font-size:90%>DeepSpeed</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.31.4.1.2><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.31.4.1.2.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.4.1.2.1.1 style=font-size:90%>FlashAttention</span></td></tr></table></table></figure><blockquote><p>üîº Table 5 presents the optimal hyperparameters found through a grid search for fine-tuning the NeoBERT model on the GLUE benchmark. The search explored various combinations of batch sizes (2, 4, 8, 16, 32), learning rates (5e-6, 6e-6, 8e-6, 1e-5, 2e-5, 3e-5), and weight decay values (1e-2, 1e-5) for each of the GLUE tasks. The table lists the optimal settings discovered for each task, aiding reproducibility and comparison of results.</p><details><summary>read the caption</summary>Table 5: Optimal hyperparameters for GLUE tasks. The grid search was conducted over batch sizes {2,4,8,16,32}2481632\{2,4,8,16,32\}{ 2 , 4 , 8 , 16 , 32 }, learning rates {5‚Å¢e‚àí6,6‚Å¢e‚àí6,8‚Å¢e‚àí6,1‚Å¢e‚àí5,2‚Å¢e‚àí5,3‚Å¢e‚àí5}5ùëí66ùëí68ùëí61ùëí52ùëí53ùëí5\{5e-6,6e-6,8e-6,1e-5,2e-5,3e-5\}{ 5 italic_e - 6 , 6 italic_e - 6 , 8 italic_e - 6 , 1 italic_e - 5 , 2 italic_e - 5 , 3 italic_e - 5 }, and weight decay values {1‚Å¢e‚àí2,1‚Å¢e‚àí5}1ùëí21ùëí5\{1e-2,1e-5\}{ 1 italic_e - 2 , 1 italic_e - 5 }.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S3.T1.11.11.31.5.1><tr class=ltx_tr id=S3.T1.11.11.31.5.1.1><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.31.5.1.1.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.5.1.1.1.1 style=font-size:90%>Alternate Attention</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.31.5.1.2><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.31.5.1.2.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.5.1.2.1.1 style=font-size:90%>Unpadding</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.31.5.1.3><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.31.5.1.3.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.5.1.3.1.1 style=font-size:90%>FlashAttention</span></td></tr></table></table></figure><blockquote><p>üîº This table details the instructions used for fine-tuning various pre-trained models on different contrastive learning datasets. Each row represents a dataset, specifying the task and the instructions given to the model for that task. The instructions provide context to the models, guiding them on how to process the data and generate appropriate outputs. The information is crucial for understanding the fine-tuning process and how the models were prepared for the downstream evaluations.</p><details><summary>read the caption</summary>Table 6: Instructions for fine-tuning on the different contrastive learning datasets.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S3.T1.11.11.31.6.1><tr class=ltx_tr id=S3.T1.11.11.31.6.1.1><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.31.6.1.1.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.6.1.1.1.1 style=font-size:90%>DeepSpeed</span></td></tr><tr class=ltx_tr id=S3.T1.11.11.31.6.1.2><td class="ltx_td ltx_nopad_r ltx_align_center" id=S3.T1.11.11.31.6.1.2.1 style=padding-top:.9pt;padding-bottom:.9pt><span class=ltx_text id=S3.T1.11.11.31.6.1.2.1.1 style=font-size:90%>FlashAttention</span></td></tr></table></table></figure><blockquote><p>üîº This table details the specific instructions used for evaluating model performance on each of the sub-tasks within the MTEB benchmark. For each task, it provides a description outlining the input format and the expected output, clarifying the nature of the prediction required from the language model.</p><details><summary>read the caption</summary>Table 7: Instructions for evaluation on the different MTEB tasks.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S4.T2.14.10><tr class=ltx_tr id=S4.T2.14.10.11><td class="ltx_td ltx_align_center ltx_border_tt" colspan=2 id=S4.T2.14.10.11.1><span class="ltx_text ltx_font_bold" id=S4.T2.14.10.11.1.1>Modification</span></td><td class="ltx_td ltx_align_left ltx_border_tt" id=S4.T2.14.10.11.2><span class="ltx_text ltx_font_bold" id=S4.T2.14.10.11.2.1>Before</span></td><td class="ltx_td ltx_align_left ltx_border_tt" id=S4.T2.14.10.11.3><span class="ltx_text ltx_font_bold" id=S4.T2.14.10.11.3.1>After</span></td></tr><tr class=ltx_tr id=S4.T2.5.1.1><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.5.1.1.1 rowspan=3><span class=ltx_text id=S4.T2.5.1.1.1.1><math alttext="M1" class="ltx_Math" display="inline" id="S4.T2.5.1.1.1.1.m1.1"><semantics id="S4.T2.5.1.1.1.1.m1.1a"><mrow id="S4.T2.5.1.1.1.1.m1.1.1" xref="S4.T2.5.1.1.1.1.m1.1.1.cmml"><mi id="S4.T2.5.1.1.1.1.m1.1.1.2" xref="S4.T2.5.1.1.1.1.m1.1.1.2.cmml">M</mi><mo id="S4.T2.5.1.1.1.1.m1.1.1.1" xref="S4.T2.5.1.1.1.1.m1.1.1.1.cmml">‚Å¢</mo><mn id="S4.T2.5.1.1.1.1.m1.1.1.3" xref="S4.T2.5.1.1.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.5.1.1.1.1.m1.1b"><apply id="S4.T2.5.1.1.1.1.m1.1.1.cmml" xref="S4.T2.5.1.1.1.1.m1.1.1"><times id="S4.T2.5.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.5.1.1.1.1.m1.1.1.1"></times><ci id="S4.T2.5.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.5.1.1.1.1.m1.1.1.2">ùëÄ</ci><cn id="S4.T2.5.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.5.1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.1.1.1.1.m1.1c">M1</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.1.1.1.1.m1.1d">italic_M 1</annotation></semantics></math></span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.5.1.1.2>Embedding</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.5.1.1.3>Positional</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.5.1.1.4>RoPE</td></tr><tr class=ltx_tr id=S4.T2.14.10.12><td class="ltx_td ltx_align_left" id=S4.T2.14.10.12.1>Activation</td><td class="ltx_td ltx_align_left" id=S4.T2.14.10.12.2>GELU</td><td class="ltx_td ltx_align_left" id=S4.T2.14.10.12.3>SwiGLU</td></tr><tr class=ltx_tr id=S4.T2.14.10.13><td class="ltx_td ltx_align_left" id=S4.T2.14.10.13.1>Pre-LN</td><td class="ltx_td ltx_align_left" id=S4.T2.14.10.13.2>LayerNorm</td><td class="ltx_td ltx_align_left" id=S4.T2.14.10.13.3>RMSNorm</td></tr><tr class=ltx_tr id=S4.T2.6.2.2><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.6.2.2.1><math alttext="M2" class="ltx_Math" display="inline" id="S4.T2.6.2.2.1.m1.1"><semantics id="S4.T2.6.2.2.1.m1.1a"><mrow id="S4.T2.6.2.2.1.m1.1.1" xref="S4.T2.6.2.2.1.m1.1.1.cmml"><mi id="S4.T2.6.2.2.1.m1.1.1.2" xref="S4.T2.6.2.2.1.m1.1.1.2.cmml">M</mi><mo id="S4.T2.6.2.2.1.m1.1.1.1" xref="S4.T2.6.2.2.1.m1.1.1.1.cmml">‚Å¢</mo><mn id="S4.T2.6.2.2.1.m1.1.1.3" xref="S4.T2.6.2.2.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.2.2.1.m1.1b"><apply id="S4.T2.6.2.2.1.m1.1.1.cmml" xref="S4.T2.6.2.2.1.m1.1.1"><times id="S4.T2.6.2.2.1.m1.1.1.1.cmml" xref="S4.T2.6.2.2.1.m1.1.1.1"></times><ci id="S4.T2.6.2.2.1.m1.1.1.2.cmml" xref="S4.T2.6.2.2.1.m1.1.1.2">ùëÄ</ci><cn id="S4.T2.6.2.2.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.6.2.2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.2.2.1.m1.1c">M2</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.2.2.1.m1.1d">italic_M 2</annotation></semantics></math></td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.6.2.2.2>Dataset</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.6.2.2.3>Wiki + Book</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.6.2.2.4>RefinedWeb</td></tr><tr class=ltx_tr id=S4.T2.7.3.3><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.7.3.3.1><math alttext="M3" class="ltx_Math" display="inline" id="S4.T2.7.3.3.1.m1.1"><semantics id="S4.T2.7.3.3.1.m1.1a"><mrow id="S4.T2.7.3.3.1.m1.1.1" xref="S4.T2.7.3.3.1.m1.1.1.cmml"><mi id="S4.T2.7.3.3.1.m1.1.1.2" xref="S4.T2.7.3.3.1.m1.1.1.2.cmml">M</mi><mo id="S4.T2.7.3.3.1.m1.1.1.1" xref="S4.T2.7.3.3.1.m1.1.1.1.cmml">‚Å¢</mo><mn id="S4.T2.7.3.3.1.m1.1.1.3" xref="S4.T2.7.3.3.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.7.3.3.1.m1.1b"><apply id="S4.T2.7.3.3.1.m1.1.1.cmml" xref="S4.T2.7.3.3.1.m1.1.1"><times id="S4.T2.7.3.3.1.m1.1.1.1.cmml" xref="S4.T2.7.3.3.1.m1.1.1.1"></times><ci id="S4.T2.7.3.3.1.m1.1.1.2.cmml" xref="S4.T2.7.3.3.1.m1.1.1.2">ùëÄ</ci><cn id="S4.T2.7.3.3.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.7.3.3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.3.3.1.m1.1c">M3</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.3.3.1.m1.1d">italic_M 3</annotation></semantics></math></td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.7.3.3.2>Tokenizer</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.7.3.3.3>Google WordPiece</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.7.3.3.4>LLaMA BPE</td></tr><tr class=ltx_tr id=S4.T2.8.4.4><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.8.4.4.1 rowspan=2><span class=ltx_text id=S4.T2.8.4.4.1.1><math alttext="M4" class="ltx_Math" display="inline" id="S4.T2.8.4.4.1.1.m1.1"><semantics id="S4.T2.8.4.4.1.1.m1.1a"><mrow id="S4.T2.8.4.4.1.1.m1.1.1" xref="S4.T2.8.4.4.1.1.m1.1.1.cmml"><mi id="S4.T2.8.4.4.1.1.m1.1.1.2" xref="S4.T2.8.4.4.1.1.m1.1.1.2.cmml">M</mi><mo id="S4.T2.8.4.4.1.1.m1.1.1.1" xref="S4.T2.8.4.4.1.1.m1.1.1.1.cmml">‚Å¢</mo><mn id="S4.T2.8.4.4.1.1.m1.1.1.3" xref="S4.T2.8.4.4.1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.8.4.4.1.1.m1.1b"><apply id="S4.T2.8.4.4.1.1.m1.1.1.cmml" xref="S4.T2.8.4.4.1.1.m1.1.1"><times id="S4.T2.8.4.4.1.1.m1.1.1.1.cmml" xref="S4.T2.8.4.4.1.1.m1.1.1.1"></times><ci id="S4.T2.8.4.4.1.1.m1.1.1.2.cmml" xref="S4.T2.8.4.4.1.1.m1.1.1.2">ùëÄ</ci><cn id="S4.T2.8.4.4.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.8.4.4.1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.4.4.1.1.m1.1c">M4</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.4.4.1.1.m1.1d">italic_M 4</annotation></semantics></math></span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.8.4.4.2>Optimizer</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.8.4.4.3>Adam</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.8.4.4.4>AdamW</td></tr><tr class=ltx_tr id=S4.T2.14.10.14><td class="ltx_td ltx_align_left" id=S4.T2.14.10.14.1>Scheduler</td><td class="ltx_td ltx_align_left" id=S4.T2.14.10.14.2>Linear</td><td class="ltx_td ltx_align_left" id=S4.T2.14.10.14.3>Cosine</td></tr><tr class=ltx_tr id=S4.T2.9.5.5><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.9.5.5.1><math alttext="M5" class="ltx_Math" display="inline" id="S4.T2.9.5.5.1.m1.1"><semantics id="S4.T2.9.5.5.1.m1.1a"><mrow id="S4.T2.9.5.5.1.m1.1.1" xref="S4.T2.9.5.5.1.m1.1.1.cmml"><mi id="S4.T2.9.5.5.1.m1.1.1.2" xref="S4.T2.9.5.5.1.m1.1.1.2.cmml">M</mi><mo id="S4.T2.9.5.5.1.m1.1.1.1" xref="S4.T2.9.5.5.1.m1.1.1.1.cmml">‚Å¢</mo><mn id="S4.T2.9.5.5.1.m1.1.1.3" xref="S4.T2.9.5.5.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.9.5.5.1.m1.1b"><apply id="S4.T2.9.5.5.1.m1.1.1.cmml" xref="S4.T2.9.5.5.1.m1.1.1"><times id="S4.T2.9.5.5.1.m1.1.1.1.cmml" xref="S4.T2.9.5.5.1.m1.1.1.1"></times><ci id="S4.T2.9.5.5.1.m1.1.1.2.cmml" xref="S4.T2.9.5.5.1.m1.1.1.2">ùëÄ</ci><cn id="S4.T2.9.5.5.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.9.5.5.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.5.5.1.m1.1c">M5</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.5.5.1.m1.1d">italic_M 5</annotation></semantics></math></td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.9.5.5.2>Masking Scheme</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.9.5.5.3>15% (80 / 10 / 10)</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.9.5.5.4>20% (100)</td></tr><tr class=ltx_tr id=S4.T2.10.6.6><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.10.6.6.1><math alttext="M6" class="ltx_Math" display="inline" id="S4.T2.10.6.6.1.m1.1"><semantics id="S4.T2.10.6.6.1.m1.1a"><mrow id="S4.T2.10.6.6.1.m1.1.1" xref="S4.T2.10.6.6.1.m1.1.1.cmml"><mi id="S4.T2.10.6.6.1.m1.1.1.2" xref="S4.T2.10.6.6.1.m1.1.1.2.cmml">M</mi><mo id="S4.T2.10.6.6.1.m1.1.1.1" xref="S4.T2.10.6.6.1.m1.1.1.1.cmml">‚Å¢</mo><mn id="S4.T2.10.6.6.1.m1.1.1.3" xref="S4.T2.10.6.6.1.m1.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.10.6.6.1.m1.1b"><apply id="S4.T2.10.6.6.1.m1.1.1.cmml" xref="S4.T2.10.6.6.1.m1.1.1"><times id="S4.T2.10.6.6.1.m1.1.1.1.cmml" xref="S4.T2.10.6.6.1.m1.1.1.1"></times><ci id="S4.T2.10.6.6.1.m1.1.1.2.cmml" xref="S4.T2.10.6.6.1.m1.1.1.2">ùëÄ</ci><cn id="S4.T2.10.6.6.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.10.6.6.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.6.6.1.m1.1c">M6</annotation><annotation encoding="application/x-llamapun" id="S4.T2.10.6.6.1.m1.1d">italic_M 6</annotation></semantics></math></td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.10.6.6.2>Sequence packing</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.10.6.6.3>False</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.10.6.6.4>True</td></tr><tr class=ltx_tr id=S4.T2.11.7.7><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.11.7.7.1><math alttext="M7" class="ltx_Math" display="inline" id="S4.T2.11.7.7.1.m1.1"><semantics id="S4.T2.11.7.7.1.m1.1a"><mrow id="S4.T2.11.7.7.1.m1.1.1" xref="S4.T2.11.7.7.1.m1.1.1.cmml"><mi id="S4.T2.11.7.7.1.m1.1.1.2" xref="S4.T2.11.7.7.1.m1.1.1.2.cmml">M</mi><mo id="S4.T2.11.7.7.1.m1.1.1.1" xref="S4.T2.11.7.7.1.m1.1.1.1.cmml">‚Å¢</mo><mn id="S4.T2.11.7.7.1.m1.1.1.3" xref="S4.T2.11.7.7.1.m1.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.11.7.7.1.m1.1b"><apply id="S4.T2.11.7.7.1.m1.1.1.cmml" xref="S4.T2.11.7.7.1.m1.1.1"><times id="S4.T2.11.7.7.1.m1.1.1.1.cmml" xref="S4.T2.11.7.7.1.m1.1.1.1"></times><ci id="S4.T2.11.7.7.1.m1.1.1.2.cmml" xref="S4.T2.11.7.7.1.m1.1.1.2">ùëÄ</ci><cn id="S4.T2.11.7.7.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.11.7.7.1.m1.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.7.7.1.m1.1c">M7</annotation><annotation encoding="application/x-llamapun" id="S4.T2.11.7.7.1.m1.1d">italic_M 7</annotation></semantics></math></td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.11.7.7.2>Model Size</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.11.7.7.3>120M</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.11.7.7.4>250M</td></tr><tr class=ltx_tr id=S4.T2.12.8.8><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.12.8.8.1><math alttext="M8" class="ltx_Math" display="inline" id="S4.T2.12.8.8.1.m1.1"><semantics id="S4.T2.12.8.8.1.m1.1a"><mrow id="S4.T2.12.8.8.1.m1.1.1" xref="S4.T2.12.8.8.1.m1.1.1.cmml"><mi id="S4.T2.12.8.8.1.m1.1.1.2" xref="S4.T2.12.8.8.1.m1.1.1.2.cmml">M</mi><mo id="S4.T2.12.8.8.1.m1.1.1.1" xref="S4.T2.12.8.8.1.m1.1.1.1.cmml">‚Å¢</mo><mn id="S4.T2.12.8.8.1.m1.1.1.3" xref="S4.T2.12.8.8.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.12.8.8.1.m1.1b"><apply id="S4.T2.12.8.8.1.m1.1.1.cmml" xref="S4.T2.12.8.8.1.m1.1.1"><times id="S4.T2.12.8.8.1.m1.1.1.1.cmml" xref="S4.T2.12.8.8.1.m1.1.1.1"></times><ci id="S4.T2.12.8.8.1.m1.1.1.2.cmml" xref="S4.T2.12.8.8.1.m1.1.1.2">ùëÄ</ci><cn id="S4.T2.12.8.8.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.12.8.8.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.8.8.1.m1.1c">M8</annotation><annotation encoding="application/x-llamapun" id="S4.T2.12.8.8.1.m1.1d">italic_M 8</annotation></semantics></math></td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.12.8.8.2>Depth - Width</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.12.8.8.3>16 - 1056</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.12.8.8.4>28 - 768</td></tr><tr class=ltx_tr id=S4.T2.13.9.9><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id=S4.T2.13.9.9.1 rowspan=2><span class=ltx_text id=S4.T2.13.9.9.1.1><math alttext="M9" class="ltx_Math" display="inline" id="S4.T2.13.9.9.1.1.m1.1"><semantics id="S4.T2.13.9.9.1.1.m1.1a"><mrow id="S4.T2.13.9.9.1.1.m1.1.1" xref="S4.T2.13.9.9.1.1.m1.1.1.cmml"><mi id="S4.T2.13.9.9.1.1.m1.1.1.2" xref="S4.T2.13.9.9.1.1.m1.1.1.2.cmml">M</mi><mo id="S4.T2.13.9.9.1.1.m1.1.1.1" xref="S4.T2.13.9.9.1.1.m1.1.1.1.cmml">‚Å¢</mo><mn id="S4.T2.13.9.9.1.1.m1.1.1.3" xref="S4.T2.13.9.9.1.1.m1.1.1.3.cmml">9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.13.9.9.1.1.m1.1b"><apply id="S4.T2.13.9.9.1.1.m1.1.1.cmml" xref="S4.T2.13.9.9.1.1.m1.1.1"><times id="S4.T2.13.9.9.1.1.m1.1.1.1.cmml" xref="S4.T2.13.9.9.1.1.m1.1.1.1"></times><ci id="S4.T2.13.9.9.1.1.m1.1.1.2.cmml" xref="S4.T2.13.9.9.1.1.m1.1.1.2">ùëÄ</ci><cn id="S4.T2.13.9.9.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.13.9.9.1.1.m1.1.1.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.9.9.1.1.m1.1c">M9</annotation><annotation encoding="application/x-llamapun" id="S4.T2.13.9.9.1.1.m1.1d">italic_M 9</annotation></semantics></math></span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.13.9.9.2>Batch size</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.13.9.9.3>131k</td><td class="ltx_td ltx_align_left ltx_border_t" id=S4.T2.13.9.9.4>2M</td></tr><tr class=ltx_tr id=S4.T2.14.10.10><td class="ltx_td ltx_align_left ltx_border_bb" id=S4.T2.14.10.10.2>Context length</td><td class="ltx_td ltx_align_left ltx_border_bb" id=S4.T2.14.10.10.3>512</td><td class="ltx_td ltx_align_left ltx_border_bb" id=S4.T2.14.10.10.1><math alttext="4,096" class="ltx_Math" display="inline" id="S4.T2.14.10.10.1.m1.2"><semantics id="S4.T2.14.10.10.1.m1.2a"><mrow id="S4.T2.14.10.10.1.m1.2.3.2" xref="S4.T2.14.10.10.1.m1.2.3.1.cmml"><mn id="S4.T2.14.10.10.1.m1.1.1" xref="S4.T2.14.10.10.1.m1.1.1.cmml">4</mn><mo id="S4.T2.14.10.10.1.m1.2.3.2.1" xref="S4.T2.14.10.10.1.m1.2.3.1.cmml">,</mo><mn id="S4.T2.14.10.10.1.m1.2.2" xref="S4.T2.14.10.10.1.m1.2.2.cmml">096</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.14.10.10.1.m1.2b"><list id="S4.T2.14.10.10.1.m1.2.3.1.cmml" xref="S4.T2.14.10.10.1.m1.2.3.2"><cn id="S4.T2.14.10.10.1.m1.1.1.cmml" type="integer" xref="S4.T2.14.10.10.1.m1.1.1">4</cn><cn id="S4.T2.14.10.10.1.m1.2.2.cmml" type="integer" xref="S4.T2.14.10.10.1.m1.2.2">096</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.10.10.1.m1.2c">4,096</annotation><annotation encoding="application/x-llamapun" id="S4.T2.14.10.10.1.m1.2d">4 , 096</annotation></semantics></math></td></tr></table></table></figure><blockquote><p>üîº This table lists instructions for evaluating various tasks within the MTEB (Massive Text Embedding Benchmark). Each row represents a different task, specifying the type of input given (e.g., a question, a review, a news summary) and what the model is expected to retrieve or classify in response (e.g., relevant documents, sentiment, intents). The table provides a comprehensive overview of the diverse tasks included in MTEB, showing the range of natural language understanding abilities being assessed by the benchmark.</p><details><summary>read the caption</summary>Table 8: Instructions for evaluation on the different MTEB tasks.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S5.T3.6.6><tr class=ltx_tr id=S5.T3.6.6.7><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.6.6.7.1><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.7.1.1>Size</span></td><td class="ltx_td ltx_align_left ltx_border_tt" id=S5.T3.6.6.7.2><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.7.2.1>Model</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.6.6.7.3><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.7.3.1>MNLI</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.6.6.7.4><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.7.4.1>QNLI</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.6.6.7.5><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.7.5.1>QQP</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.6.6.7.6><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.7.6.1>RTE</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.6.6.7.7><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.7.7.1>SST</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.6.6.7.8><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.7.8.1>MRPC</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.6.6.7.9><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.7.9.1>CoLA</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S5.T3.6.6.7.10><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.7.10.1>STS</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T3.6.6.7.11><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.7.11.1>Avg.</span></td></tr><tr class=ltx_tr id=S5.T3.1.1.1><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.1.1.1.1 rowspan=4><span class=ltx_text id=S5.T3.1.1.1.1.1><span class=ltx_text id=S5.T3.1.1.1.1.1.2></span> <span class=ltx_text id=S5.T3.1.1.1.1.1.1><span class="ltx_tabular ltx_align_middle" id=S5.T3.1.1.1.1.1.1.1><span class=ltx_tr id=S5.T3.1.1.1.1.1.1.1.2><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.T3.1.1.1.1.1.1.1.2.1>Base</span></span>
<span class=ltx_tr id=S5.T3.1.1.1.1.1.1.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.T3.1.1.1.1.1.1.1.1.1><span class=ltx_text id=S5.T3.1.1.1.1.1.1.1.1.1.1 style=font-size:90%>(<math alttext="\leq 150M" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml"></mi><mo id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.1" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">‚â§</mo><mrow id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mn id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">150</mn><mo id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.1" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml">M</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1"><leq id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.1"></leq><csymbol cd="latexml" id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">absent</csymbol><apply id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3"><times id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.1"></times><cn id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" type="integer" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2">150</cn><ci id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3">ùëÄ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1c">\leq 150M</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1d">‚â§ 150 italic_M</annotation></semantics></math>)</span></span></span>
</span></span><span class=ltx_text id=S5.T3.1.1.1.1.1.3></span></span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T3.1.1.1.2>BERT</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.1.1.1.3>84.0</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.1.1.1.4>90.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.1.1.1.5>71.2</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.1.1.1.6>66.4</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.1.1.1.7>93.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.1.1.1.8>88.9</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.1.1.1.9>52.1</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S5.T3.1.1.1.10>85.8</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.1.1.1.11>79.6</td></tr><tr class=ltx_tr id=S5.T3.6.6.8><td class="ltx_td ltx_align_left" id=S5.T3.6.6.8.1>RoBERTa</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.8.2>87.6</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.8.3>92.8</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.8.4>91.9</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.8.5>78.7</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.8.6>94.8</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.8.7>90.2</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.8.8>63.6</td><td class="ltx_td ltx_align_center ltx_border_r" id=S5.T3.6.6.8.9>91.2</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.8.10>86.4</td></tr><tr class=ltx_tr id=S5.T3.6.6.9><td class="ltx_td ltx_align_left" id=S5.T3.6.6.9.1>GTE-en-8192</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.9.2>86.7</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.9.3>91.9</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.9.4>88.8</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.9.5>84.8</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.9.6>93.3</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.9.7>92.1</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.9.8>57.0</td><td class="ltx_td ltx_align_center ltx_border_r" id=S5.T3.6.6.9.9>90.2</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.9.10>85.6</td></tr><tr class=ltx_tr id=S5.T3.2.2.2><td class="ltx_td ltx_align_left" id=S5.T3.2.2.2.1>NomicBERT<sub class=ltx_sub id=S5.T3.2.2.2.1.1><span class="ltx_text ltx_font_italic" id=S5.T3.2.2.2.1.1.1>2048</span></sub></td><td class="ltx_td ltx_align_center" id=S5.T3.2.2.2.2>86.0</td><td class="ltx_td ltx_align_center" id=S5.T3.2.2.2.3>92.0</td><td class="ltx_td ltx_align_center" id=S5.T3.2.2.2.4>92.0</td><td class="ltx_td ltx_align_center" id=S5.T3.2.2.2.5>82.0</td><td class="ltx_td ltx_align_center" id=S5.T3.2.2.2.6>93.0</td><td class="ltx_td ltx_align_center" id=S5.T3.2.2.2.7>88.0</td><td class="ltx_td ltx_align_center" id=S5.T3.2.2.2.8>50.0</td><td class="ltx_td ltx_align_center ltx_border_r" id=S5.T3.2.2.2.9>90.0</td><td class="ltx_td ltx_align_center" id=S5.T3.2.2.2.10>84.0</td></tr><tr class=ltx_tr id=S5.T3.6.6.10><td class=ltx_td id=S5.T3.6.6.10.1></td><td class="ltx_td ltx_align_left" id=S5.T3.6.6.10.2>ModernBERT</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.10.3><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.6.6.10.3.1>89.1</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.10.4><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.6.6.10.4.1>93.9</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.10.5><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.6.6.10.5.1>92.1</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.10.6><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.6.6.10.6.1>87.4</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.10.7><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.6.6.10.7.1>96.0</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.10.8><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.6.6.10.8.1>92.2</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.10.9><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.6.6.10.9.1>65.1</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S5.T3.6.6.10.10><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.6.6.10.10.1>91.8</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.10.11><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.6.6.10.11.1>88.5</span></td></tr><tr class=ltx_tr id=S5.T3.4.4.4><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.3.3.3.1 rowspan=2><span class=ltx_text id=S5.T3.3.3.3.1.1><span class=ltx_text id=S5.T3.3.3.3.1.1.2></span> <span class=ltx_text id=S5.T3.3.3.3.1.1.1><span class="ltx_tabular ltx_align_middle" id=S5.T3.3.3.3.1.1.1.1><span class=ltx_tr id=S5.T3.3.3.3.1.1.1.1.2><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.T3.3.3.3.1.1.1.1.2.1>Medium</span></span>
<span class=ltx_tr id=S5.T3.3.3.3.1.1.1.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.T3.3.3.3.1.1.1.1.1.1><math alttext="250M" class="ltx_Math" display="inline" id="S5.T3.3.3.3.1.1.1.1.1.1.m1.1"><semantics id="S5.T3.3.3.3.1.1.1.1.1.1.m1.1a"><mrow id="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1" xref="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1.2" mathsize="90%" xref="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1.2.cmml">250</mn><mo id="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1.1" xref="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1.1.cmml">‚Å¢</mo><mi id="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1.3" mathsize="90%" xref="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.1.1.1.1.1.1.m1.1b"><apply id="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1"><times id="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1.1"></times><cn id="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1.2">250</cn><ci id="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S5.T3.3.3.3.1.1.1.1.1.1.m1.1.1.3">ùëÄ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.1.1.1.1.1.1.m1.1c">250M</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.3.1.1.1.1.1.1.m1.1d">250 italic_M</annotation></semantics></math></span></span>
</span></span><span class=ltx_text id=S5.T3.3.3.3.1.1.3></span></span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T3.4.4.4.2>NeoBERT<sub class=ltx_sub id=S5.T3.4.4.4.2.1><span class="ltx_text ltx_font_italic" id=S5.T3.4.4.4.2.1.1>1024</span></sub></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.4.4.4.3>88.9</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.4.4.4.4><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.4.4.4.4.1>93.9</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.4.4.4.5>90.7</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.4.4.4.6>91.0</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.4.4.4.7><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.4.4.4.7.1>95.8</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.4.4.4.8>93.4</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.4.4.4.9>64.8</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S5.T3.4.4.4.10><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.4.4.4.10.1>92.1</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.4.4.4.11>88.8</td></tr><tr class=ltx_tr id=S5.T3.5.5.5><td class="ltx_td ltx_align_left" id=S5.T3.5.5.5.1>NeoBERT<sub class=ltx_sub id=S5.T3.5.5.5.1.1><span class="ltx_text ltx_font_italic" id=S5.T3.5.5.5.1.1.1>4096</span></sub></td><td class="ltx_td ltx_align_center" id=S5.T3.5.5.5.2><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.5.5.5.2.1>89.0</span></td><td class="ltx_td ltx_align_center" id=S5.T3.5.5.5.3>93.7</td><td class="ltx_td ltx_align_center" id=S5.T3.5.5.5.4><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.5.5.5.4.1>90.7</span></td><td class="ltx_td ltx_align_center" id=S5.T3.5.5.5.5><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.5.5.5.5.1>91.3</span></td><td class="ltx_td ltx_align_center" id=S5.T3.5.5.5.6>95.6</td><td class="ltx_td ltx_align_center" id=S5.T3.5.5.5.7><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.5.5.5.7.1>93.4</span></td><td class="ltx_td ltx_align_center" id=S5.T3.5.5.5.8><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.5.5.5.8.1>66.2</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S5.T3.5.5.5.9>91.8</td><td class="ltx_td ltx_align_center" id=S5.T3.5.5.5.10><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T3.5.5.5.10.1>89.0</span></td></tr><tr class=ltx_tr id=S5.T3.6.6.6><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T3.6.6.6.1 rowspan=5><span class=ltx_text id=S5.T3.6.6.6.1.1><span class=ltx_text id=S5.T3.6.6.6.1.1.2></span> <span class=ltx_text id=S5.T3.6.6.6.1.1.1><span class="ltx_tabular ltx_align_middle" id=S5.T3.6.6.6.1.1.1.1><span class=ltx_tr id=S5.T3.6.6.6.1.1.1.1.2><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.T3.6.6.6.1.1.1.1.2.1>Large</span></span>
<span class=ltx_tr id=S5.T3.6.6.6.1.1.1.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.T3.6.6.6.1.1.1.1.1.1><span class=ltx_text id=S5.T3.6.6.6.1.1.1.1.1.1.1 style=font-size:90%>(<math alttext="\geq 340M" class="ltx_Math" display="inline" id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1"><semantics id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1a"><mrow id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1" xref="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.2" xref="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.2.cmml"></mi><mo id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.1" xref="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.1.cmml">‚â•</mo><mrow id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3" xref="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mn id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">340</mn><mo id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3.1" xref="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3.3" xref="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3.3.cmml">M</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1b"><apply id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1"><geq id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.1"></geq><csymbol cd="latexml" id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.2">absent</csymbol><apply id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3"><times id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3.1"></times><cn id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" type="integer" xref="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3.2">340</cn><ci id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1.1.3.3">ùëÄ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1c">\geq 340M</annotation><annotation encoding="application/x-llamapun" id="S5.T3.6.6.6.1.1.1.1.1.1.1.m1.1d">‚â• 340 italic_M</annotation></semantics></math>)</span></span></span>
</span></span><span class=ltx_text id=S5.T3.6.6.6.1.1.3></span></span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T3.6.6.6.2>BERT</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.6.6.6.3>86.3</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.6.6.6.4>92.7</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.6.6.6.5>72.1</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.6.6.6.6>70.1</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.6.6.6.7>94.9</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.6.6.6.8>89.3</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.6.6.6.9>60.5</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S5.T3.6.6.6.10>86.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.6.6.6.11>82.1</td></tr><tr class=ltx_tr id=S5.T3.6.6.11><td class="ltx_td ltx_align_left" id=S5.T3.6.6.11.1>RoBERTa</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.11.2>90.2</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.11.3>94.7</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.11.4>92.2</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.11.5>86.6</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.11.6>96.4</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.11.7>90.9</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.11.8>68.0</td><td class="ltx_td ltx_align_center ltx_border_r" id=S5.T3.6.6.11.9>92.4</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.11.10>88.9</td></tr><tr class=ltx_tr id=S5.T3.6.6.12><td class="ltx_td ltx_align_left" id=S5.T3.6.6.12.1>DeBERTaV3</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.12.2><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.12.2.1>91.9</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.12.3><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.12.3.1>96.0</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.12.4><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.12.4.1>93.0</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.12.5><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.12.5.1>92.7</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.12.6>96.9</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.12.7>91.9</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.12.8><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.12.8.1>75.3</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S5.T3.6.6.12.9><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.12.9.1>93.0</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.12.10><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.12.10.1>91.4</span></td></tr><tr class=ltx_tr id=S5.T3.6.6.13><td class="ltx_td ltx_align_left" id=S5.T3.6.6.13.1>GTE-en-8192</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.13.2>89.2</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.13.3>93.9</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.13.4>89.2</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.13.5>88.1</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.13.6>95.1</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.13.7><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.13.7.1>93.5</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.13.8>60.4</td><td class="ltx_td ltx_align_center ltx_border_r" id=S5.T3.6.6.13.9>91.4</td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.13.10>87.6</td></tr><tr class=ltx_tr id=S5.T3.6.6.14><td class="ltx_td ltx_align_left ltx_border_bb" id=S5.T3.6.6.14.1>ModernBERT</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.6.6.14.2>90.8</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.6.6.14.3>95.2</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.6.6.14.4>92.7</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.6.6.14.5>92.1</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.6.6.14.6><span class="ltx_text ltx_font_bold" id=S5.T3.6.6.14.6.1>97.1</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.6.6.14.7>91.7</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.6.6.14.8>71.4</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S5.T3.6.6.14.9>92.8</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.6.6.14.10>90.5</td></tr></table></table></figure><blockquote><p>üîº This table presents the throughput, measured in thousands of tokens processed per second, for different language models at various sequence lengths. The throughput is determined using the optimal batch size for each model and sequence length combination. This allows for a comparison of the efficiency of each model in handling different input sizes, which is critical for real-world applications where processing speed is often a major constraint. The models are grouped by size (base, medium, large).</p><details><summary>read the caption</summary>Table 9: Throughput (103superscript10310^{3}10 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT tokens / second) in function of the sequence length, with optimal batch size.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-3d5d620fb9df041e16974f349095d5d3 class=gallery><img src=https://ai-paper-reviewer.com/2502.19587/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.19587/19.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.19587/&amp;title=NeoBERT:%20A%20Next-Generation%20BERT" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.19587/&amp;text=NeoBERT:%20A%20Next-Generation%20BERT" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.19587/&amp;subject=NeoBERT:%20A%20Next-Generation%20BERT" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2502.19587/index.md",oid_likes="likes_paper-reviews/2502.19587/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2502.18965/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-26T00:00:00+00:00>26 February 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2502.18890/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-26T00:00:00+00:00>26 February 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>