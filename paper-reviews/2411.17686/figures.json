[{"figure_path": "https://arxiv.org/html/2411.17686/x3.png", "caption": "Figure 1: (Left) Schematic diagram of our unified \u201cfilter-correlate-compress\u201d paradigm for training-free token reduction in MLLMs. (Right) Performance comparison on TextVQA benchmark\u00a0[32].", "description": "Figure 1 is a two-part illustration summarizing the core concept and experimental results of the proposed method. The left panel presents a schematic diagram outlining the unified \"filter-correlate-compress\" paradigm. This paradigm involves three main stages: filtering redundant tokens, correlating important tokens, and finally compressing the relevant information into a reduced set of tokens, all without requiring model retraining.  The right panel shows a graph comparing the performance of the proposed method (FiCoCo) to other state-of-the-art methods on the TextVQA benchmark. The graph displays accuracy as a function of FLOPs (floating-point operations), demonstrating the efficiency gains achieved by FiCoCo.", "section": "2. A Unified Paradigm of Token Reduction"}, {"figure_path": "https://arxiv.org/html/2411.17686/x4.png", "caption": "Figure 2: \nAn overview of the proposed FiCoCo method series.\nDuring different phases of MLLM inference, FiCoCo-V and FiCoCo-L provide distinct solutions across three stages.", "description": "This figure illustrates the three-stage pipeline of the FiCoCo method series for training-free token reduction in Multimodal Large Language Models (MLLMs).  It shows how FiCoCo-V and FiCoCo-L, which target the visual and language encoding phases respectively, each apply the filter, correlate, and compress stages.  The filter stage identifies redundant tokens. The correlate stage establishes relationships between these and the preserved tokens. Finally, the compress stage integrates the redundant information into the preserved tokens. The figure visually depicts the flow and operations within each stage for both FiCoCo-V and FiCoCo-L, highlighting the differences in their approaches while maintaining a consistent structure across all three stages.", "section": "2. A Unified Paradigm of Token Reduction"}, {"figure_path": "https://arxiv.org/html/2411.17686/x5.png", "caption": "Figure 3: \nVisualizations of token reduction by (a) FiCoCo-V and (b) FiCoCo-L.\nThe red box indicates the traced patch token, while the green box shows where the traced token is merged.", "description": "Figure 3 visualizes the effects of token reduction using FiCoCo-V and FiCoCo-L methods.  In (a), FiCoCo-V's token reduction is shown, highlighting how a key visual token (red box) is merged into another (green box). In (b), FiCoCo-L's token reduction is presented, also demonstrating the merging of a key token (red box) with another token (green box). The process of token merging is tracked visually to show how important information is preserved. This qualitative analysis helps illustrate how the methods maintain relevant information during the reduction process, showing their effectiveness in reducing computational cost without significantly impacting accuracy.", "section": "4.3. Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2411.17686/x6.png", "caption": "Figure 4: Hyperparameter sensitivity analysis of \u03bb\ud835\udf06\\lambdaitalic_\u03bb, \u03b2\ud835\udefd\\betaitalic_\u03b2 and \u03b3\ud835\udefe\\gammaitalic_\u03b3 on TextVQA and SQA benchmarks.", "description": "This figure displays the sensitivity analysis results for three hyperparameters (\u03bb, \u03b2, and \u03b3) used in the FiCoCo model.  The analysis was performed on two benchmarks: TextVQA and SQA. Each subplot shows how changes in a specific hyperparameter affect the accuracy of the model on both benchmarks.  The x-axis represents the value of the hyperparameter, while the y-axis represents the model's accuracy.  The plots provide insights into the optimal ranges and impact of these hyperparameters on the model's performance, guiding hyperparameter tuning for improved results.", "section": "4. Experiments"}]