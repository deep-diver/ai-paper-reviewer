{"references": [{"fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "publication_date": "2023-09-16", "reason": "This paper introduces Qwen, a foundational large language model that serves as a basis for many of the models and techniques discussed in the target paper."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-12-31", "reason": "This paper introduces visual instruction tuning, a technique crucial for training multimodal large language models (MLLMs) and highly relevant to the focus of the target paper on improving MLLM inference."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Improved baselines with visual instruction tuning", "publication_date": "2024-06-19", "reason": "This paper provides improved baselines for visual instruction tuning, directly impacting the evaluation and comparison of methods within the target paper's benchmark experiments."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-19", "reason": "CLIP, introduced in this paper, is a foundational vision model used extensively in MLLMs, making it highly relevant to the target paper's focus on improving MLLM inference."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-07-19", "reason": "BLIP-2, introduced in this paper, is a significant multimodal large language model, providing a direct comparison point for the efficiency improvements proposed in the target paper."}]}