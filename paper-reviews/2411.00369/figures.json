[{"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/hotpot-format-final-2.png", "caption": "Figure 1: \nReasoning graphs constructed based on one QA instance from HotpotQA dataset\u00a0Yang et\u00a0al. (2018) that maps out the logical steps required to arrive at the answer. The left-hand side illustrates the positive reasoning graph, which is constructed from the supporting paragraphs provided in the original dataset. This graph represents the gold reasoning path needed to answer the question. On the right-hand side, two types of negative reasoning graphs are derived from the original positive reasoning graphs by either perturbing the edges (e.g., inversing the edge direction in this case) or adding additional nodes with irrelevant sentences.", "description": "This figure shows how reasoning graphs are constructed for a question-answer pair from the HotpotQA dataset.  The left side displays the positive reasoning graph, a visual representation of the logical steps needed to answer the question, built using sentences from the original dataset's supporting paragraphs. The right side demonstrates two types of negative reasoning graphs. These are created by either modifying the connections (edges) between sentences in the original graph or by adding extra sentences (nodes) that are not relevant to answering the question.  This illustrates how the structure of the reasoning path impacts the LLM's ability to answer the question, and will be investigated in the paper.", "section": "GRS-QA Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/graph_counts_log.png", "caption": "(a) Number of Questions by Graph types in all dataset splits", "description": "This bar chart visualizes the distribution of questions across different reasoning graph types within the GRS-QA dataset.  The x-axis represents the various graph types, categorized based on their structural complexity and logical flow (e.g., comparison, bridge, compositional). The y-axis displays the number of questions belonging to each graph type. The chart provides insights into the frequency of each reasoning structure within the dataset, indicating the balance or imbalance of different question complexities in the GRS-QA dataset.", "section": "2 GRS-QA Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/average_nodes_edges_line_graph.png", "caption": "(b) Average number of nodes and edges in each question type Positive Graphs", "description": "This figure visualizes the average number of nodes and edges present in the positive reasoning graphs for various question types within the GRS-QA dataset.  Nodes represent sentences, and edges represent the logical relationships between sentences in the reasoning path. The graph provides insights into the complexity of different question types, showing how many sentences and relationships are typically involved in reaching the correct answer for each type.", "section": "2 GRS-QA Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/average_token_count_by_type.png", "caption": "(c) Average number of tokens in each question type\u2019s Positive Graphs", "description": "This figure shows the average number of tokens (words and punctuation marks) used in the positive reasoning graphs for different types of questions.  A positive reasoning graph represents the ideal path of reasoning to arrive at the answer.  The x-axis lists the different question types in GRS-QA. Each question type has various levels of reasoning complexity. The y-axis represents the average number of tokens.  This visualization helps understand the relationship between question complexity and the length of the textual content needed to answer the question.", "section": "2.3 Dataset Distribution"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/recall_comparison.png", "caption": "Figure 2: Statistical Analysis of the Distribution of GRS-QA.", "description": "This figure presents a statistical analysis of the GRS-QA dataset, illustrating the distribution of various aspects.  Panel (a) shows the number of questions categorized by their graph types. Panel (b) displays the average number of nodes and edges within each question type's positive graphs, offering insights into the complexity of the reasoning paths involved. Panel (c) shows the average token count in each question type's positive graphs, providing information on the length and textual complexity of the questions.", "section": "2 GRS-QA Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/weighted_recall.png", "caption": "(a) Recall Across Question of Different Reasoning Graphs", "description": "This figure shows the recall performance of three different retrieval methods (BM25, TF-IDF, and DPR) across various question types categorized by their reasoning graph structures.  The x-axis represents the different question types based on their reasoning graph complexity (e.g., bridge_2_1, comparison_2_1, etc.), while the y-axis represents the recall score.  The bars illustrate the recall achieved by each retrieval method for each question type.  The figure helps to visualize how the retrieval performance varies depending on both the retrieval method and the complexity of the reasoning structure inherent in the question.", "section": "3.1 Retrieval Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/question_types_GPT-3.5.png", "caption": "(b) Weighted Recall Across Questions of Different Hops", "description": "This figure shows the weighted average recall across questions grouped by the number of reasoning hops (steps).  It compares the performance of three different retrieval methods (BM25, TF-IDF, and DPR) in retrieving relevant sentences for questions of varying hop lengths.  The higher the hop count, the more complex the reasoning chain, and potentially the more challenging the retrieval task for the models.", "section": "3.1 Retrieval Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/question_types_GPT-4o-mini.png", "caption": "Figure 3: Comparison of BM25, TFIDF, and DPR Recall and Weighted Recall Across Question Types", "description": "This figure compares the recall performance of three different retrieval methods (BM25, TF-IDF, and DPR) across various question types categorized by their reasoning complexity (number of hops).  The bar chart visually represents the recall achieved by each method for each question type.  A second chart presents a weighted average recall score across all question types, again broken down by the number of reasoning hops. This allows for a direct comparison of the effectiveness of the retrieval methods in handling different question complexities.", "section": "3.1 Retrieval Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/question_types_GPT-3.5.png", "caption": "(a) GPT-3.5 as LLM-Judge", "description": "This figure displays the LLM Judge scores for different question types, specifically focusing on the performance of GPT-3.5 as the LLM judge.  The x-axis represents various question types categorized by their reasoning graph complexity (e.g., bridge_2_1, comparison_2_1, etc.),  while the y-axis shows the LLM Judge score.  The bars in the chart visually represent the performance of GPT-3.5 on these different question types, illustrating the model's ability to judge the correctness of answers based on the varying complexities of the questions.  The chart helps analyze how well GPT-3.5 can assess answers considering the nuances of the question's structure. ", "section": "3.3 LLM Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/llm_judge_scores_GPT-3.5.png", "caption": "(b) GPT-4o-mini as LLM-Judge", "description": "This figure shows the performance of the GPT-4o-mini large language model (LLM) as a judge in evaluating the performance of other LLMs on various question types.  The x-axis represents the different types of questions, categorized by their complexity.  The y-axis displays the LLM judge scores which reflect the accuracy of the LLM's answers. Different bars within each question type represent different prompting methods used by the model (best retriever, unstructured gold evidence, positive reasoning graph, negative reasoning graph, no context).  The chart helps to visualize how the model's performance varies based on both question type and prompting approach.", "section": "3.3 LLM Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/llm_judge_scores_GPT-4o-mini.png", "caption": "(c) Llama3 as LLM-Judge", "description": "This figure shows the LLM Judge scores for the Llama 3 model across different question types in the LLM QA performance benchmark.  It displays the exact match, F1 score, and LLM Judge scores for Llama 3 for each of the various question types, categorized by the complexity of their reasoning graphs (2-hop to 5-hop).  The chart helps visualize how Llama 3's performance changes based on the different question types and complexity.", "section": "3.3 LLM QA Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/llm_judge_scores_GPT-3.5.png", "caption": "Figure 4: LLM Judge Scores by Question Type for Different LLMs", "description": "This figure displays the performance of three different Large Language Models (LLMs) \u2013 GPT-3.5, GPT-4-mini, and Llama 3 \u2013 as judged by another LLM (GPT-4-mini) on various question types within the GRS-QA dataset.  Each question type represents different levels of reasoning complexity, allowing for the assessment of LLMs' ability to handle questions with varying reasoning structures.  The bars represent the LLM Judge scores (a combined metric of the performance) for each LLM on each question type. The x-axis shows the various question types within the GRS-QA dataset, and the y-axis displays the LLM Judge Scores, showing how each model performs on different question types with different complexities.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/bm25_metrics.png", "caption": "(a) GPT-3.5 as LLM Judge", "description": "This figure displays the LLM Judge scores generated by GPT-3.5 for various question types within the GRS-QA dataset.  The x-axis represents different question types categorized by their reasoning graph structure (e.g., bridge, comparison, compositional).  The y-axis shows the LLM Judge score, a metric reflecting the overall quality of the LLMs' answers as assessed by GPT-3.5.  The bars illustrate the performance for each question type, providing insights into how well different LLMs perform based on the complexity and structure of the reasoning involved in answering questions.", "section": "3.3 LLM QA Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/dpr_metrics.png", "caption": "(b) GPT-4o-mini as LLM Judge", "description": "This figure displays the LLM judge scores for different question types, specifically focusing on the performance of the GPT-4o-mini model.  The x-axis represents various question types categorized by their reasoning graph complexity (e.g., bridge_2_1, comparison_2_1, etc.), and the y-axis shows the LLM judge score.  The graph allows for a visual comparison of GPT-4o-mini's performance across different question types and complexities.", "section": "3.3 LLM QA Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/tfidf_metrics.png", "caption": "(c) Llama3 as LLM Judge", "description": "This figure displays the performance of Llama3, one of three LLMs (Large Language Models) tested in the study, as evaluated by LLM-as-Judge.  The LLM-as-Judge metric assesses the quality of responses generated by other LLMs by comparing them to the responses of Llama3, specifically focusing on the accuracy and relevance of answers given by Llama3 for various question types. The x-axis shows different types of questions with varying levels of complexity and hop counts, while the y-axis represents the LLM Judge scores, showing how well Llama3's answers align with the ground truth, for each type of question.", "section": "3.3 LLM QA Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/GPT-3.5_prompt_type_4.png", "caption": "Figure 5: LLM Judge Scores by Hop Type for Different LLMs", "description": "This figure displays the LLM Judge scores for different Large Language Models (LLMs) across various hop types in questions.  It provides a visual comparison of the performance of three LLMs (GPT-3.5, GPT-40-mini, and Llama3) when evaluating the quality of answers generated for questions with varying levels of complexity (measured by the number of hops or reasoning steps required). The x-axis represents the hop type, while the y-axis indicates the LLM Judge Score, a metric used to assess the quality of the LLM's generated answers.", "section": "3.3 LLM QA Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/GPT-4o-mini_prompt_type_4.png", "caption": "Figure 6: BM25 Retrieval Across Question Types", "description": "This figure shows the performance of BM25 retrieval across different question types in the GRS-QA dataset.  The x-axis represents the different question types, categorized by their reasoning complexity (e.g., bridge_2_1, comparison_2_1, etc.). The y-axis displays the BM25 retrieval metrics, specifically precision, recall, and F1-score. The bars for each question type represent the corresponding values for each metric. The figure illustrates how the effectiveness of BM25 varies depending on the complexity and structure of the questions.", "section": "3.1 Retrieval Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/Llama3-8B_Instruct_prompt_type_4.png", "caption": "Figure 7: DPR Retrieval Across Question Types", "description": "This bar chart visualizes the performance of Dense Passage Retrieval (DPR) across different question types in the GRS-QA dataset.  Each bar represents a question type, categorized by hop count and structure (e.g., bridge, comparison, compositional). The height of each bar shows the F1 score, precision, and recall achieved by DPR for that specific question type. The chart allows for a comparison of DPR's effectiveness in retrieving relevant information for questions with varying complexities and structures.", "section": "3.1 Retrieval Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/GPT-3.5_prompt_type_0.png", "caption": "Figure 8: TFIDF Retrieval Across Question Types", "description": "This bar chart visualizes the performance of TF-IDF retrieval across different question types within the GRS-QA dataset.  Each bar represents a question type, broken down by the metrics Precision, Recall and F1-Score. The height of each segment within a bar indicates the achieved score for that specific metric on that question type.  This allows for a direct comparison of TF-IDF's effectiveness in retrieving relevant information for various reasoning complexities.", "section": "3.1 Retrieval Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/GPT-4o-mini_prompt_type_0.png", "caption": "Figure 9: GPT-3.5 Metrics - No Context Provided", "description": "This figure presents a performance comparison of the GPT-3.5 language model on various question types within the GRS-QA dataset.  Specifically, it shows the model's performance without providing any supporting context or retrieved evidence. The performance is evaluated using three metrics: Exact Match, F1 Score, and LLM-as-Judge. The x-axis represents the different question types (categorized by reasoning structure complexity), and the y-axis represents the achieved score for each metric.  The graph visually demonstrates how the model's accuracy varies across different question types, highlighting the challenges posed by more complex reasoning structures when no external context is provided.", "section": "3.2 LLM QA Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/Llama3-8B_Instruct_prompt_type_0.png", "caption": "Figure 10: GPT4o-mini Metrics - No Context Provided", "description": "This figure displays the performance of the GPT4o-mini language model on various question types within the GRS-QA dataset, without providing any context. The performance is measured using three metrics: Exact Match, F1 score, and LLM-as-Judge.  Each bar represents a different question type, categorized by their complexity (number of hops and type of reasoning). The height of each bar indicates the score achieved by the model on that question type for each metric.", "section": "3.2 LLM QA Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/GPT-3.5_prompt_type_2.png", "caption": "Figure 11: Llama3 Metrics - No Context Provided", "description": "This figure displays the performance of Llama3 language model on various question types within the GRS-QA dataset when no contextual information is provided.  The metrics displayed likely include Exact Match, F1 Score, and LLM Judge score across different question types (categorized by their reasoning graph complexity, such as bridge_2_1, comparison_2_1 etc.). Each bar represents one question type and the height of each bar shows the score for that metric. The figure helps visualize the model's ability to answer questions with varying reasoning complexities when there is no provided context.", "section": "3.3 LLM QA Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/GPT-4o-mini_prompt_type_2.png", "caption": "Figure 12: GPT-3.5 Metrics - Best Retriever", "description": "This figure displays the performance of the GPT-3.5 large language model (LLM) when using the best retriever (BM25) to obtain relevant information for answering questions.  It shows the exact match accuracy, F1 score, and LLM judge scores across various question types within the GRS-QA dataset. Each bar represents a different question type, categorized by their reasoning graph complexity. The different colors in the bars show the three different metrics used for the evaluation.  This visualization helps understand how effectively GPT-3.5 performs on questions with different reasoning structures when provided with optimal retrieved evidence.", "section": "3.3.1 RAG with Best Retriever"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/Llama3-8B_Instruct_prompt_type_2.png", "caption": "Figure 13: GPT4o-mini Metrics - Best Retriever", "description": "This figure presents the performance metrics of the GPT-4o-mini language model when using the best retriever (BM25) to retrieve relevant evidence for answering questions.  The x-axis represents different question types categorized by reasoning graph structure complexity (e.g., bridge_2_1, comparison_2_1, etc.), while the y-axis displays the metrics: Exact Match, F1 Score, and LLM Judge score. The different colored bars within each question type show the performance across various metrics.  The chart illustrates how the model's performance varies across different question types and reasoning graph complexity levels when provided with top evidence retrieved by the BM25.", "section": "3.3.1 RAG with Best Retriever"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/GPT-3.5_prompt_type_1.png", "caption": "Figure 14: Llama3 Metrics - Best Retriever", "description": "This figure presents the performance metrics of the Llama 3 language model when using the best retriever (BM25) to retrieve relevant evidence for answering questions.  The x-axis represents the various question types within the GRS-QA dataset, categorized by their reasoning structure complexity. The y-axis displays the evaluation metrics (Exact Match, F1 score, and LLM Judge score) for each question type. This visualization showcases how well Llama 3 performs on different question complexities when assisted by the best performing retrieval method. The varying heights of the bars for each metric across the different question types demonstrate the model's performance variability with varying reasoning structure complexities. The overall trend and specific performance details regarding each metric across the diverse question types are presented in the figure.", "section": "3.3.1 RAG with Best Retriever"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/GPT-4o-mini_prompt_type_1.png", "caption": "Figure 15: GPT-3.5 Metrics - Positive Graph of Ground Truth Evidence", "description": "This figure displays the performance of GPT-3.5 on the GRS-QA dataset when provided with positive reasoning graphs as context.  It shows the exact match accuracy, F1 score, and LLM judge score across different question types categorized by the complexity of their reasoning graph structure (number of hops/complexity). The x-axis represents various question types, and the y-axis shows the performance metrics. The figure helps visualize how the explicit provision of the correct reasoning pathways impacts the model's ability to accurately answer questions with varying reasoning complexities.", "section": "3.3.1 RAG with Best Retriever"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/Llama3-8B_Instruct_prompt_type_1.png", "caption": "Figure 16: GPT4o-mini Metrics - Positive Graph of Ground Truth Evidence", "description": "This figure displays the performance metrics of the GPT-4o-mini language model when evaluated using a positive reasoning graph as the context.  The metrics shown likely include precision, recall, F1 score, and potentially exact match, assessing the model's ability to correctly answer questions when the reasoning steps are explicitly provided.  The graph likely displays performance across different types of reasoning graph structures or complexity levels.", "section": "3.3 LLM Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/GPT-3.5_prompt_type_3.png", "caption": "Figure 17: Llama3 Metrics - Positive Graph of Ground Truth Evidence", "description": "This figure displays the performance metrics of the Llama 3 language model on the GRS-QA dataset when using positive reasoning graphs as input.  The metrics shown likely include Exact Match (EM), F1 score, and LLM Judge score. The x-axis represents the different question types within the GRS-QA dataset, categorized by their reasoning graph complexity (e.g., bridge_2_1, comparison_2_1, etc.).  The y-axis represents the metric scores, indicating the model's accuracy and performance for each question type. This visualization allows for a detailed comparison of Llama 3's performance across various reasoning complexities when provided with the correct reasoning pathways (positive graphs).", "section": "3.3 LLM Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/GPT-4o-mini_prompt_type_3.png", "caption": "Figure 18: GPT-3.5 Metrics - Unstructured Ground Truth Evidence", "description": "This figure presents the performance metrics of the GPT-3.5 large language model (LLM) when prompted with unstructured ground truth evidence for various question types in the GRS-QA dataset.  The metrics displayed likely include Exact Match, F1-score, and an LLM Judge score (a metric used to assess the quality of the LLM's response). The x-axis represents different question types categorized by their reasoning graph complexity (e.g., bridge_2_1 indicates a bridge-type question with 2 reasoning steps and 1 node). The y-axis represents the values for each of the metrics.  The graph visually compares the model's performance across different question types based on the complexity of their reasoning pathways, showing how the performance varies with the complexity of the task.", "section": "3.3 LLM Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/Llama3-8B_Instruct_prompt_type_3.png", "caption": "Figure 19: GPT4o-mini Metrics - Unstructured Ground Truth Evidence", "description": "This figure displays the performance metrics of the GPT-40-mini language model when provided with unstructured ground truth evidence for question answering.  It shows the exact match accuracy, F1 score, and LLM Judge score across different question types, categorized by their reasoning graph complexity (number of hops). The goal is to evaluate the model's ability to answer questions when given the correct context but without the structured reasoning pathways presented in the reasoning graphs.", "section": "3.3 LLM Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/weighted_f1.png", "caption": "Figure 20: Llama3 Metrics - Unstructured Ground Truth Evidence", "description": "This figure displays the performance metrics of the Llama 3 language model when provided with unstructured ground truth evidence for question answering.  It shows the exact match accuracy, F1 score, and LLM judge score for Llama 3 across various question types with varying levels of reasoning complexity. The x-axis represents different question types (categorized by the number of reasoning steps and their structure), and the y-axis represents the performance metrics.  The purpose is to evaluate the model's ability to answer questions when given access to all relevant context without any structured guidance or organization. The graph helps researchers to understand how the model's performance changes with the structural complexity of the question.", "section": "3.3 LLM Performance Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.00369/extracted/5970251/images/weighted_precision.png", "caption": "Figure 21: GPT-3.5 Metrics - Negative Graph of Ground Truth Evidence", "description": "This figure displays the performance metrics of the GPT-3.5 large language model (LLM) when prompted with questions paired with negative reasoning graphs.  Negative reasoning graphs are altered versions of the ground truth reasoning graphs, introducing structural errors to isolate the impact of structure on LLM performance. The metrics shown likely include exact match accuracy, F1 score (harmonic mean of precision and recall), and an LLM judge score (a measure of how well the LLM's response aligns with human judgment). The graph likely visualizes these metrics across different types of questions categorized by their reasoning graph complexity (number of reasoning steps, graph structure, etc.). This helps assess how sensitive the LLM's reasoning capabilities are to structural inaccuracies in the provided information.", "section": "3.3 LLM Performance Benchmark"}]