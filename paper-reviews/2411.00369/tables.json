[{"content": "| Graph | Type | Question | Decomposition |\n|---|---|---|---| \n| ![https://arxiv.org/html/2411.00369/x1.png](https://arxiv.org/html/2411.00369/x1.png) | Comparison_2_1 (C-2-1) | Between Athlete and Fun, which band has more members? **Athlete** | **1.** How many members are in Athlete? **Four members** <br> **2.** How many members are in Fun? **Three members** |\n| ![https://arxiv.org/html/2411.00369/x2.png](https://arxiv.org/html/2411.00369/x2.png) | Bridge_2_1 (B-2-1) | Who beat the player that won the 2017 Australian men\u2019s open tennis single title in the US open? **Novak Djokovic** | **1.** Who wins the 2017 australian men\u2019s open tennis single title? **Roger Federer** <br> **2.** Who beat **Roger Federer** in the us open? **Novak Djokovic** |\n| ![https://arxiv.org/html/2411.00369/x3.png](https://arxiv.org/html/2411.00369/x3.png) | Comparison_3_1 (C-3-1) | In which country is the administrative territorial entity for the city where Charlie Harper was born? **United Kingdom** | **1.** Where was Charlie Harper born? **Hackney** <br> **2.** In which administrative territorial entity is **Hackney** located? **Middlesex** <br> **3.** Which country is **Middlesex** located in? **United Kingdom** |\n| ![https://arxiv.org/html/2411.00369/x4.png](https://arxiv.org/html/2411.00369/x4.png) | Bridge_3_1 (B-3-1) | In which country is the administrative territorial entity for the city where Charlie Harper was born? **United Kingdom** | **1.** Where was Charlie Harper born? **Hackney** <br> **2.** In which administrative territorial entity is **Hackney** located? **Middlesex** <br> **3.** Which country is **Middlesex** located in? **United Kingdom** |\n| ![https://arxiv.org/html/2411.00369/x5.png](https://arxiv.org/html/2411.00369/x5.png) | Compositional_3_2 (CO-3-2) | In which country is Midway, in the same county as McRae in the same state as KAGH-FM? **U.S.** | **1.** What state is KAGH-FM located? **Arkansas** <br> **2.** In which administrative territorial entity is McRae located? **White County** <br> **3.** Which country is Midway (near Pleasant Plains), **White County**, **Arkansas** located in? **U.S.** |\n| ![https://arxiv.org/html/2411.00369/x6.png](https://arxiv.org/html/2411.00369/x6.png) | Comparison_4_1 (C-4-1) | Did Albrecht Alt and Asli Hassan Abade have the same occupation? **no** | **1.** [\"Asli Hassan Abade\", \"occupation\", \"pilot\"] <br> **2.** [\"Asli Hassan Abade\", \"occupation\", \"military figure\"], <br> **3.** [\"Asli Hassan Abade\", \"occupation\", \"civil activist\"] <br> **4.** [\"Albrecht Alt\", \"occupation\", \"theologian\"] <br> **5.** [\"Albrecht Alt\", \"occupation\", \"lecturer\"] <br> **6.** [\"Albrecht Alt\", \"occupation\", \"professor\"] <br> \"supporting_facts\": [[\"Asli Hassan Abade\", 0], [\"Albrecht Alt\", 0],[\"Albrecht Alt\", 2], [\"Albrecht Alt\", 6]] |\n| ![https://arxiv.org/html/2411.00369/x7.png](https://arxiv.org/html/2411.00369/x7.png) | Bridge_4_1 (B-4-1) | When did Ukraine gain independence from the first Allied nation to reach the German city where the director of The Man from Morocco was born? **1917** | **1.** Who is the director of The Man from Morocco? **Mutz Greenbaum** <br> **2.** What is the place of birth of **Mutz Greenbaum**? **Berlin** <br> **3.** What allied nation was the first to reach the german capitol of **Berlin**? **Soviet Union** <br> **4.** When did Ukraine gain independence from **Soviet Union**? **1917** |\n| ![https://arxiv.org/html/2411.00369/x8.png](https://arxiv.org/html/2411.00369/x8.png) | Compositional_4_2 (CO-4-2) | Where is the place of death of the man who became leader of the largest country in Europe in square miles after the collapse of the nation Germany agreed to sign a non-aggression pact with in 1939? **Moscow** | **1.** What is the largest country in europe by square miles? **Russia** <br> **2.** In 1939 Germany agreed to sign a non-aggression pact with which country? **the Soviet Union** <br> **3.** Who became leader of **Russia** after the collapse of **the Soviet Union**? **Boris Yeltsin** <br> **4.** Where did **Boris Yeltsin** die? **Moscow** |\n| ![https://arxiv.org/html/2411.00369/x9.png](https://arxiv.org/html/2411.00369/x9.png) | Compositional_4_3 (CO-4-3) | In what country is Tuolumne, which is within a county that borders the county containing Jamestown, and is located within the state where Some Like It Hot was filmed? **United States** | **1.** In which administrative territorial entity is Jamestown located? **Tuolumne County** <br> **2.** Which entities share a border with **Tuolumne County**? **Stanislaus County** <br> **3.** Where did they film some like it hot? **in California** <br> **4.** Which country is **Tuolumne**, **Stanislaus County**, in California located in?? **United States** |\n| ![https://arxiv.org/html/2411.00369/x10.png](https://arxiv.org/html/2411.00369/x10.png) | Bridge_Comparison_4_1 (BC-4-1) | Are both directors of films The Blue Bird (1940 Film) and Bharya Biddalu from the same country? **no** | **1.** [\u2019The Blue Bird (1940 film)\u2019, \u2019director\u2019, **\u2019Walter Lang\u2019**] <br> **2.** [\u2019Bharya Biddalu\u2019, \u2019director\u2019, **\u2019Tatineni Rama Rao\u2019**] <br> **3.** [**\u2019Walter Lang\u2019**, \u2019country of citizenship\u2019, \u2019American\u2019] <br> **4.** [**\u2019Tatineni Rama Rao\u2019**, \u2019country of citizenship\u2019, \u2019India\u2019] |\n| ![https://arxiv.org/html/2411.00369/x11.png](https://arxiv.org/html/2411.00369/x11.png) | Comparison_5_1 (CO-5-1) | Which film has more directors, Red Cow (Film) or Chillerama? **Chillerama** | **1.** [\"Red Cow (film)\", \"director\", \"Tsivia Barkai Yacov\"] <br> **2.** [\"Chillerama\", \"director\", \"Adam Rifkin\"] <br> **3.** [\"Chillerama\", \"director\", \"Tim Sullivan\"] <br> **4.** [\"Chillerama\", \"director\", \"Adam Green\"] <br> **5.** [\"Chillerama\", \"director\", \"Joe Lynch\"] |\n| ![https://arxiv.org/html/2411.00369/x12.png](https://arxiv.org/html/2411.00369/x12.png) | Bridge_Comparison_5_1 (BC-5-1) | \"Do both films The Falcon (Film) and Valentin The Good have the directors from the same country? **no** | **1.** [\"The Falcon (film)\", \"director\", **\"Vatroslav Mimica\"**] <br> **2.** [\"Valentin the Good\", \"director\", **\"Martin Fri0\u030610d\"**] <br> **3.** [**\"Vatroslav Mimica\"**, \"country of citizenship\", \"Croatian\"] <br> **4.** [**\"Vatroslav Mimica\"**, \"country of citizenship\", \"Yugoslavia\"] <br> **5.** [**\"Martin Fri0\u030610d\"**, \"country of citizenship\", \"Czech\"] |", "caption": "Table 1: \nThis table shows the Reasoning graphs of GRS-QA. The reasoning graphs demonstrate the decomposition of the larger question and the reasoning paths to approach the answer. Each of these is constructed using the context and relevant entities for each question. The decomposition is shown with varying formats in the right-most column of the graph, including more questions derived from the original question as well as triples that represent the relations between entities and, in turn, provide subsets of the context. This is consistent with the multiple datasets that each of the question types are extracted from.", "description": "Table 1 presents examples of reasoning graphs from the GRS-QA dataset.  Each row shows a question-answer pair, its corresponding reasoning graph (visualizing the logical steps to reach the answer), and a decomposition of the question into simpler sub-questions. The decomposition utilizes relevant context and entities from multiple datasets to create a more granular understanding of the reasoning process.  The rightmost column illustrates different forms of this decomposition, including both more granular questions and entity triples.  This detailed representation of the reasoning pathway helps researchers evaluate and understand how Large Language Models perform on different reasoning structures.", "section": "2 GRS-QA Dataset Construction"}, {"content": "| Question Type | Train | Val | Test |\n|---|---|---|---|\n| Bridge_2_1 | 58384 | 7298 | 7298 |\n| Comparison_2_1 | 13964 | 1745 | 1747 |\n| **total** | **72348** | **9043** | **9045** |", "caption": "Table 2: Breakdown of Question Types and Unique Question Count for HotpotQA", "description": "This table presents a breakdown of the question types and their counts within the HotpotQA dataset.  It shows how many questions of each type (e.g., Bridge_2_1, Comparison_2_1) are present in the training, validation, and testing sets of the dataset. This provides insight into the distribution of question complexities within the dataset.", "section": "A Dataset Processing"}, {"content": "| Question Type | Train | Val | Test |\n|---|---|---|---|\n| Bridge_2_1 | 61209 | 7651 | 7652 |\n| Comparison_2_1 | 41324 | 5165 | 5167 |\n| Comparison_3_1 | 234 | 29 | 30 |\n| Comparison_4_1 | 10 | 1 | 2 |\n| Comparison_5_1 | - | - | 1 |\n| Compositional_3_2 | 3 | - | 1 |\n| Bridge_Comparison_4_1 | 27266 | 3408 | 3409 |\n| Bridge_Comparison_5_1 | 308 | 38 | 29 |\n| **total** | **130354** | **16292** | **16301** |", "caption": "Table 3: Breakdown of Question Types and Unique Question Count for 2WikiMultiHopQA", "description": "This table presents a detailed breakdown of the question types and their counts within the 2WikiMultiHopQA dataset. It shows the distribution of questions across various categories, specifically highlighting the number of unique questions in the training, validation, and testing sets for each question type. This breakdown is crucial for understanding the dataset's composition and ensuring a balanced evaluation of different question complexities.", "section": "2 GRS-QA Dataset Construction"}, {"content": "| Question Type | Train | Val | Test |\n|---|---|---|---|\n| Bridge_2_1 | 11478 | 1434 | 1436 |\n| Bridge_3_1 | 2987 | 373 | 374 |\n| Compositional_3_2 | 519 | 64 | 66 |\n| Bridge_4_1 | 516 | 64 | 65 |\n| Compositional_4_2 | 101 | 12 | 14 |\n| Compositional_4_3 | 319 | 39 | 41 |\n| **total** | **15920** | **1986** | **1996** |", "caption": "Table 4: Breakdown of Question Types and Unique Question Count for MuSiQue", "description": "Table 4 presents a breakdown of the question types and their counts within the MuSiQue dataset. It details the distribution of questions across different categories, such as \"Bridge_2_1,\" \"Bridge_3_1,\" etc.,  providing the number of training, validation, and test instances for each question type. This table helps to illustrate the composition of the MuSiQue dataset used in the study, which is crucial for evaluating the model's performance on diverse question types and complexities.", "section": "2 GRS-QA Dataset Construction"}, {"content": "| Method | Recall | F1 | Precision |\n|---|---|---|---| \n| BM25 | 0.4921 | 0.1182 | 0.0680 |\n| TF-IDF | 0.1619 | 0.0447 | 0.0261 |\n| DPR | 0.1037 | 0.0285 | 0.0166 |", "caption": "Table 5: Average Retrieval Metrics for BM25, TF-IDF, and DPR", "description": "This table presents the average retrieval performance metrics for three different methods: BM25, TF-IDF, and DPR.  For each method, it shows the average recall, F1 score, and precision across all question types in the GRS-QA dataset.  These metrics provide a quantitative evaluation of the effectiveness of each retrieval method in identifying relevant evidence sentences for answering questions with varying reasoning structures.", "section": "3.1 Retrieval Performance Benchmark"}]