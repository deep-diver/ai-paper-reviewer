{"importance": "This paper introduces LORAM, a novel approach to training large language models using limited resources. It **demonstrates a significant reduction in memory footprint** while maintaining high performance. Thus, researchers can push the boundaries of LLM customization even with hardware limitations and may further explores knowledge alignment techniques.", "summary": "LORAM: Train small, infer large LLMs by memory-efficient LoRA training. Enables 70B parameter model training on a 20G HBM GPU, replacing A100-80G. Reduces parameter storage cost by 15.81x.", "takeaways": ["LORAM, trains pruned models to reduce memory during LoRA fine-tuning and merges pruned low-rank matrices into the original model for inference, improving performance.", "Knowledge alignment between pruned and original models enhances LORAM's efficacy, achieved via low-cost continual pre-training on a small dataset.", "QLORAM, a combination of LORAM with quantization, further reduces memory overhead, achieving better performance than existing methods."], "tldr": "Training large language models (LLMs) is computationally expensive, especially Low-Rank Adaption (LoRA) due to the memory footprint dominated by the original model parameters. This paper addresses the challenge of high memory requirements in LLM fine-tuning. Many neurons in over-parameterized LLMs have low training utility but are essential for inference.\n\nTo mitigate these issues, the paper introduces LORAM, a memory-efficient LoRA training scheme. **LORAM trains on a pruned model and obtains pruned low-rank matrices which are then utilized with the original model for inference**. It presents minimal-cost continual pre-training to aligns knowledge discrepancy between the pruned and original models. Experiments shows LORAM reduces parameter storage cost by 15.81\u00d7 while achieving performance gains over original and LoRA-trained models.", "affiliation": "Zhejiang University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2502.13533/podcast.wav"}