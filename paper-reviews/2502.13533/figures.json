[{"figure_path": "https://arxiv.org/html/2502.13533/x1.png", "caption": "Figure 1: Idea of LoRAM", "description": "The figure illustrates the core concept of LORAM (Low-Rank Adaptation Memory).  Unlike traditional LoRA which uses the same model for training and inference, LORAM employs a pruned (smaller) model for training and the original (larger) model for inference. During training, only a subset of the model's parameters (yellow blocks) are updated using low-rank matrices, significantly reducing memory usage. These updated parameters, along with the knowledge from continual pre-training (offline), are then used to recover the full low-rank matrices which are applied to the original model during inference. This allows LORAM to achieve memory efficiency during training while maintaining performance during inference.", "section": "Memory-Efficient LoRA Training \u2013 LoRAM"}, {"figure_path": "https://arxiv.org/html/2502.13533/x2.png", "caption": "Figure 2: \nComparison of LoRAM and LoRA: Training (subfigures a and b) and Inference (c and d). Key stages include the offline process of the frozen full-rank matrix \ud835\udc160\u2217superscriptsubscript\ud835\udc160\\mathbf{W}_{0}^{*}bold_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT (subfigure e) and the online generation of the learnable low-rank matrix \ud835\udc16\u0394\u2217superscriptsubscript\ud835\udc16\u0394\\mathbf{W}_{\\Delta}^{*}bold_W start_POSTSUBSCRIPT roman_\u0394 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT (f) during LoRAM training (b) and inference (d).", "description": "This figure compares the LoRA and LoRAM training and inference processes. LoRA uses the same original model for both training and inference, while LoRAM uses a pruned model for training and recovers the weights to use the original model for inference.  The figure highlights the key stages: (a) LoRA training, updating low-rank matrices with original weights frozen; (b) LoRAM training, updating low-rank matrices on the pruned model; (c) LoRA inference, using the updated low-rank matrices with original weights; (d) LoRAM inference, using recovered low-rank matrices to integrate with original weights; (e) offline processing on the full-rank matrix; and (f) online generation of the low-rank matrix.", "section": "Memory-efficient LoRA Training - LoRAM"}, {"figure_path": "https://arxiv.org/html/2502.13533/x5.png", "caption": "Figure 3: The test perplexity of training LLaMA-2-13B & LLaMA-2-70B on OpenHermes.", "description": "This figure displays the test perplexity results for fine-tuning two different sizes of LLAMA language models (13B and 70B parameters) using the OpenHermes dataset.  The perplexity is tracked over various training iterations for different training methods, including standard LoRA and several variations of the proposed LORAM method incorporating different pruning strategies (random, structured, semi-structured, and unstructured).  This allows comparison of the training convergence speeds and the resulting test perplexity for these different training approaches, demonstrating the effectiveness of LORAM in improving efficiency without significant loss of performance.", "section": "3 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2502.13533/x6.png", "caption": "Figure 4: The test perplexity of training LLaMA-2-13B & LLaMA-2-70B on OpenOrca.", "description": "This figure displays the test perplexity results for training two large language models, LLaMA-2-13B and LLaMA-2-70B, on the OpenOrca dataset.  The graphs show the perplexity over training iterations for each model, comparing standard LORA fine-tuning with several variants of the proposed memory-efficient LORA training scheme (LORAM) using different pruning strategies:  LORAM-RAND, LORAM-STRU, LORAM-SEMI, and LORAM-UNST.  The figure also includes results for a smaller LLaMA model (7B) trained with LORA to provide a performance baseline for comparison.  This comparison allows assessment of the tradeoffs between model size, memory efficiency, and performance.", "section": "3.2 Fine-tuning Convergence"}, {"figure_path": "https://arxiv.org/html/2502.13533/x7.png", "caption": "Figure 5: The test perplexity & downstream performance of training LLaMA-3.1-70B on OpenHermes.", "description": "This figure displays the results of training the LLaMA-3.1-70B model on the OpenHermes dataset using different training methods.  It shows the test perplexity during training (how well the model predicts the next word in a sequence, lower is better) on the Alpaca dataset (an out-of-domain dataset; that is, a dataset different than the one used for training). Additionally, the figure presents downstream task performance results across various tasks, showing the effectiveness of different training methods on various downstream applications after training.", "section": "3.2 FINE-TUNING CONVERGENCE"}, {"figure_path": "https://arxiv.org/html/2502.13533/x8.png", "caption": "Figure 6: Necessity of Recovery & Alignment across different pruning strategies on LLaMA-2-13B.", "description": "This figure demonstrates the impact of the recovery and alignment steps in the LORAM model training process on LLaMA-2-13B.  It compares the performance of four different pruning strategies (Rand, Stru, Semi, Unst) with and without the recovery and alignment steps. The plots show the test perplexity on the Alpaca dataset for each strategy and configuration. This illustrates that the recovery and alignment steps significantly improve performance, especially under aggressive pruning rates.  The results highlight that simply pruning and then fine-tuning is not enough to achieve high performance; the recovery and alignment steps are crucial in bridging the gap between the pruned model used for training and the full original model used for inference.", "section": "3.5 NECESSITY OF RECOVERY & ALIGNMENT"}, {"figure_path": "https://arxiv.org/html/2502.13533/x9.png", "caption": "Figure 7: Effect of scaling parameter reduction ratio.", "description": "This figure demonstrates the impact of varying parameter reduction ratios on the performance of LoRA and QLORAM-STRU models. The x-axis represents the parameter reduction ratio, ranging from approximately 10x to nearly 30x.  The y-axis shows the test perplexity on the Alpaca dataset. LoRA-trained and QLORAM-STRU-trained models show a trend of improved performance with increasing reduction ratios, but excessive pruning (larger reduction ratios) negatively impacts the model's performance.  A comparison with a naive pruning method demonstrates that LoRAM's method leads to far more robust performance as the parameter reduction increases.", "section": "3.6 SCALING LAWS FOR PARAMETER REDUCTION ON LORAM"}, {"figure_path": "https://arxiv.org/html/2502.13533/x18.png", "caption": "Figure 8: Performance of downstream tasks across different parameter reduction ratios.", "description": "This figure presents the performance of various downstream tasks (GSM8K, MathQA, CSR, HumanEval) for different parameter reduction ratios achieved by QLORAM-STRU on the LLaMA-2-70B model.  It shows how the model's performance on these tasks changes as the parameter reduction ratio increases from approximately 10x to nearly 30x. The graph illustrates a trade-off: increasing the reduction ratio initially improves performance, reaching an optimal point, before performance starts to decrease again at the most aggressive pruning ratios. This demonstrates the effectiveness of QLORAM-STRU in balancing memory efficiency and task performance up to a certain level of model compression, beyond which the level of pruning negatively affects performance.", "section": "3.3 Downstream Task Performance"}]