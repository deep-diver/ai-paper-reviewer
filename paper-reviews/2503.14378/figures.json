[{"figure_path": "https://arxiv.org/html/2503.14378/x2.png", "caption": "Figure 1: \nImpossible Video Examples with Impossible Type and Explanation.", "description": "This figure showcases several examples of impossible videos generated by various AI models. Each example shows a short video clip alongside a description of the impossible event depicted, categorized by the type of impossible law or principle that is violated (e.g., physical, biological, geographical, social).  This demonstrates the diversity and range of impossible video scenarios the authors explore in their work, highlighting how these scenarios defy common sense and the established laws of physics and nature.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.14378/x3.png", "caption": "Figure 2: \nOverview of the IPV-Bench Benchmark.\nIPV-Bench is structured with a comprehensive taxonomy, enabling the creation of a diverse prompt suite (IPV-Txt) and a high-quality video dataset (IPV-Vid). These components facilitate the evaluation of popular video generation and understanding models.", "description": "The figure illustrates the architecture of the IPV-Bench benchmark, a novel dataset designed to evaluate video generation and understanding models' capabilities in handling impossible videos.  The benchmark is built upon a comprehensive taxonomy categorizing impossible scenarios across four domains: Physical, Biological, Geographical, and Social laws. This taxonomy facilitates the creation of two key components: a diverse prompt suite (IPV-Txt) which includes various text descriptions of impossible events, and a high-quality video dataset (IPV-Vid) that visually represents these events.  The integration of these components allows for a thorough evaluation of models on both their ability to generate such videos and to understand their content.", "section": "3. IPV-BENCH"}, {"figure_path": "https://arxiv.org/html/2503.14378/x4.png", "caption": "Figure 3: \nQuestionnaire used for collecting impossible text prompts for IPV-Txt.", "description": "This figure shows the questionnaire used to collect impossible video scenarios for the IPV-TXT benchmark. The questionnaire guides participants to brainstorm dynamic video scenarios that are impossible or extremely difficult to achieve in the real world.  It emphasizes the importance of dynamic scenes (not static), the use of common, everyday objects for creative and unexpected scenarios, and the consideration of scenarios violating social norms, expectations, or physical laws.  Participants are explicitly instructed *not* to use large language models like ChatGPT during the brainstorming process.", "section": "3.1 IPV Taxonomy and Prompt Suite"}, {"figure_path": "https://arxiv.org/html/2503.14378/x5.png", "caption": "Figure 4: \nDistribution of the Prompt Suite Across the Taxonomy.", "description": "This figure visualizes the distribution of prompts within the IPV-TXT benchmark across its taxonomy. The taxonomy categorizes impossible scenarios into four main categories: Physical Laws, Biological Laws, Geographical Laws, and Social Laws. Each category is further divided into sub-categories.  The figure shows the number or proportion of prompts belonging to each category and sub-category, providing a clear overview of the benchmark's coverage across different types of impossible scenarios.", "section": "3.1 IPV Taxonomy and Prompt Suite"}, {"figure_path": "https://arxiv.org/html/2503.14378/x6.png", "caption": "Figure 5: \nSources of Impossible Videos.", "description": "Figure 5 is a bar chart illustrating the sources of the impossible videos used in the IPV-VID dataset.  It shows the relative contribution of various sources, including AI model outputs (Sora, Kling, Hailuo, Luma, Mochi1, PyramidFlow, HunyuanVideo, Open-Sora, CogVidX, and LTX), and internet-sourced videos. This visualization helps to understand the dataset's composition and the diversity of methods employed in generating the impossible video samples.", "section": "3.2 IPV-VID"}, {"figure_path": "https://arxiv.org/html/2503.14378/x7.png", "caption": "Figure 6: \nFailure case of impossible video generation.", "description": "This figure showcases two primary failure modes in generating impossible videos. The first example depicts a failure in visual quality, where attempting to generate impossible scenes results in visual artifacts or generation failures, likely due to the impossible prompts representing out-of-distribution data for the model. The second example highlights a failure to adhere to impossible prompts, where videos capture semantic aspects but not the crucial impossible events, likely due to an overemphasis on conforming to real-world physical laws. These examples illustrate limitations of current video generation models in handling impossible scenarios.", "section": "4. Evaluate Impossible Video Generation"}, {"figure_path": "https://arxiv.org/html/2503.14378/x8.png", "caption": "Figure 7: \nExample of the MCQA task.\nWe highlight the correct option in red and the incorrect option in green.", "description": "This figure shows an example of the multiple choice question answering (MCQA) task used in the Impossible Video Understanding section of the paper.  It displays a video still along with a multiple choice question and its options. The correct answer is highlighted in red, and one incorrect option is highlighted in green. The MCQA task assesses a model's ability to correctly identify the impossible phenomenon depicted in a video by choosing from several plausible options.  The distractors (incorrect options) are carefully designed to challenge the model's reasoning abilities, ensuring that simple visual element grounding is not sufficient to arrive at the correct response.", "section": "3.2.3. Task Design"}, {"figure_path": "https://arxiv.org/html/2503.14378/extracted/6287007/figs/anno_tool.png", "caption": "Figure 8: \nExample of the OpenQA task.\nWe ask state-of-the-art video understanding models to analyze whether the video is impossible or not.\nWe highlight the correct analysis in red and the incorrect analysis in green.", "description": "This figure showcases an example of the open-ended question answering (OpenQA) task within the Impossible Video Understanding section of the paper.  The task challenges video understanding models to determine if a given video depicts an impossible event.  The figure presents a video alongside multiple model responses, differentiating between correct and incorrect answers using color-coding (red for correct, green for incorrect).  Each response highlights the model's reasoning behind its classification.", "section": "5. Evaluate Impossible Video Understanding"}, {"figure_path": "https://arxiv.org/html/2503.14378/x9.png", "caption": "Figure 9: \nScreenshot of the annotation tool.", "description": "The figure displays a screenshot of the video annotation tool used in the IPV-BENCH benchmark.  The tool is divided into two main sections: the \"Display Zone\" which shows the video being annotated, the associated text prompt describing the impossible scenario, and the taxonomy label categorizing the type of impossibility (e.g., physical, biological, geographical, social laws). The \"Annotation Zone\" presents a structured form for annotators to provide various labels including: visual quality assessment, prompt following accuracy, whether the impossibility is evident in a single frame or requires temporal analysis, and a textual explanation of the impossible scenario shown in the video.  This tool facilitates efficient and consistent annotation for the dataset.", "section": "3.2.2. HUMAN ANNOTATION"}, {"figure_path": "https://arxiv.org/html/2503.14378/x10.png", "caption": "Figure 10: \nSpearman\u2019s correlation coefficient \u03c1\ud835\udf0c\\rhoitalic_\u03c1 between automatic evaluation score and human annotation score on 10 different video generation models.", "description": "This figure displays the correlation between automatic and human evaluations of impossible video generation.  Spearman's correlation coefficient (\u03c1) is calculated and shown for each of ten different video generation models across three metrics: visual quality, prompt following, and the combined IPV (Impossible Video) score.  The high correlation coefficients indicate a strong agreement between automatic and human evaluation, demonstrating the reliability of the automated assessment method.", "section": "4.1 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2503.14378/x11.png", "caption": "Figure 11: \nInstructional Prompt of MCQA Task.", "description": "This figure presents the detailed instructions given to the large language model (LLM) for generating multiple-choice questions (MCQs) for evaluating video understanding models.  The instructions emphasize creating high-quality distractors that challenge the model's ability to distinguish between correct and incorrect answers.  These instructions focus on various aspects to make the distractors realistic and difficult:  diverse objects and attributes in the video, including details about objects and their properties. These instructions also include guidelines for creating distractors based on impossible phenomenons not explicitly shown in the video. The goal is to test the model's ability to reason about counterfactual events and understand the content of the video instead of just relying on surface-level visual information.", "section": "3. IPV-BENCH"}, {"figure_path": "https://arxiv.org/html/2503.14378/x12.png", "caption": "Figure 12: CoT examples we use to prompt GPT-4o for impossible prompt following evaluation. Videos may contain impossible events that are outside the scope of the prompt. We ignore such events when evaluating impossible prompt following.", "description": "Figure 12 shows examples of chain-of-thought (CoT) prompts used to evaluate the ability of GPT-4 to assess whether video generation models correctly follow impossible prompts.  The prompts instruct GPT-4 to analyze video frames and determine if they accurately depict the impossible event described in the text prompt.  Crucially, the instructions emphasize that GPT-4 should only focus on the specific impossible event and disregard other factors like video quality. The examples highlight GPT-4's reasoning process by showing how it identifies the key elements of the impossible event, checks for their presence in the video, and then provides a [Yes] or [No] conclusion.  It's important to note that videos might include additional impossible events not mentioned in the prompt; these events are to be ignored in the evaluation.", "section": "4. Evaluate Impossible Video Generation"}, {"figure_path": "https://arxiv.org/html/2503.14378/x13.png", "caption": "Figure 13: \nMore examples of impossible videos.", "description": "Figure 13 presents a collection of diverse impossible video examples. Each example showcases a scene that violates established physical, biological, geographical, or social laws.  These examples are visually rich and highlight the varied ways in which a video can depict an impossible scenario, including changes in material properties (paper turning to smoke), violations of mechanics (a car flying), optical illusions (a reflection mismatch), and more.", "section": "3. IPV-BENCH"}, {"figure_path": "https://arxiv.org/html/2503.14378/x14.png", "caption": "Figure 14: Failures in Generating Impossible Videos.", "description": "This figure showcases examples where video generation models fail to accurately create impossible scenarios. Each row presents a different type of impossible event and shows how the model's output deviates from the intended result. The deviations highlight challenges models face when generating scenarios that defy established laws of physics, biology, geography, or social norms.", "section": "3.2 IPV-VID"}, {"figure_path": "https://arxiv.org/html/2503.14378/x15.png", "caption": "Figure 15: \nCase study of impossible video understanding.", "description": "This figure presents a case study illustrating the challenges faced by video understanding models when dealing with impossible videos.  The figure showcases several video understanding models' responses to a video depicting a person seemingly biting into a piece of bread, only for the bread to vanish.  The responses highlight the varying degrees of understanding, from correctly identifying the impossibility to providing plausible but incorrect explanations.", "section": "5. Evaluate Impossible Video Understanding"}, {"figure_path": "https://arxiv.org/html/2503.14378/x16.png", "caption": "Figure 16: \nCase study of impossible video understanding.", "description": "This figure shows a case study of a video which is impossible in the real world.  The video shows two identical bottles of water, one frozen and one not.  Different video understanding models interpret this in various ways; some correctly identify the impossibility (violating expectations of phase transition), others fail to recognize the impossibility, and still others focus on seemingly secondary aspects of the video (e.g., the unnatural shaking of the bottles).", "section": "5. Case Study of Impossible Video Understanding"}, {"figure_path": "https://arxiv.org/html/2503.14378/x17.png", "caption": "Figure 17: \nCase study of impossible video understanding.", "description": "This figure presents a case study illustrating the challenges in impossible video understanding.  It shows a video depicting a person pouring liquid into a container filled with sand.  Several video understanding models are evaluated on their ability to identify what makes this video impossible or unusual in a real-world context.  The responses from different models highlight varying levels of understanding, ranging from correctly identifying the impossible event (the sand not absorbing water) to incorrectly identifying other aspects of the scene as unusual or impossible. This case study emphasizes the difficulty that current models face in reasoning about counterfactual or physically impossible events shown in video.", "section": "5. Case Study of Impossible Video Understanding"}, {"figure_path": "https://arxiv.org/html/2503.14378/x18.png", "caption": "Figure 18: \nCase study of impossible video understanding.", "description": "This figure shows a case study of impossible video understanding.  The task is to identify what makes the video impossible or unusual in a real-world setting.  Different video understanding models provide their answers, revealing their capabilities and limitations in understanding situations that defy real-world physics or common sense. The example video shows a tomato being cut, but the cut is not consistent with how a real tomato would be cut.  Models' answers reveal various levels of comprehension of the impossible phenomenon.", "section": "5. Evaluate Impossible Video Understanding"}, {"figure_path": "https://arxiv.org/html/2503.14378/x19.png", "caption": "Figure 19: \nCase study of impossible video understanding.", "description": "This figure presents a case study illustrating the challenges video understanding models face when dealing with impossible videos.  The example shows a book seemingly opening and closing on its own without external interaction.  The responses of various video-language models to this impossible event are analyzed, highlighting their limitations in reasoning about impossible scenarios and applying world knowledge to video analysis.", "section": "5. Case Study of Impossible Video Understanding"}]