[{"figure_path": "https://arxiv.org/html/2412.17739/x1.png", "caption": "(a) Accuracy on Passkey Retrieval (higher is better)", "description": "The figure displays the accuracy of different language models (with varying sizes: 60M, 180M, and 1.2B parameters) on the Passkey Retrieval task.  The x-axis represents the sequence length, and the y-axis shows the accuracy (higher is better). The models were trained with a maximum sequence length of 512 tokens and evaluated on sequences of varying lengths to test their length generalization capabilities. Different lines represent different models (FoPE, ROPE, and ALiBi).  The plot demonstrates the relative performance of these models as the sequence length increases, showing how well each handles longer contexts.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.17739/x2.png", "caption": "(b) Perplexity on C4 (lower is better)", "description": "This figure shows the perplexity achieved on the C4 dataset for different language models across various sequence lengths.  Lower perplexity indicates better performance.  The perplexity is evaluated after training the models with a maximum sequence length of 512 tokens.  Multiple models (60M, 180M, and 1.2B parameters) are compared using different position embedding methods (FoPE, RoPE, and ALiBi). The graph visualizes how well each model generalizes to longer sequence lengths unseen during training.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.17739/x3.png", "caption": "Figure 1: Training with max_seq_length=512.", "description": "This figure displays the results of training language models with a maximum sequence length of 512 tokens.  It presents two subfigures: (a) shows the accuracy on a passkey retrieval task, and (b) shows the perplexity on the C4 dataset.  Both subfigures compare the performance of three different models: FoPE, RoPE, and ALiBi, demonstrating the impact of the different position embedding methods on model performance across various sequence lengths.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2412.17739/x4.png", "caption": "Figure 2: The reasons why RoPE\u2019s periodic extension deteriorates and how FoPE addresses these issues to improve length generalization. (a) As signals pass through linear and nonlinear transformations, this causes spectral leakage and distortion, mixing multiple frequencies into a single dimension. Under RoPE, each dimension is treated as a single-frequency component. By contrast, FoPE models each dimension as a Fourier series of different frequency components, thereby separating information more effectively and mitigating spectral damage. (b) FoPE eliminates inadequately trained frequency components, which are harmful for periodic extension. By preserving only the zero-frequency component, FoPE safeguards periodic extension and delivers more robust length generalization.", "description": "Figure 2 illustrates the limitations of Rotary Position Embedding (RoPE) and introduces Fourier Position Embedding (FoPE) as a solution.  Panel (a) shows how RoPE, by treating each dimension as a single frequency, suffers from spectral leakage and distortion as signals pass through linear and non-linear layers. This mixing of frequencies hinders long-range dependencies. In contrast, FoPE models each dimension as a Fourier series containing multiple frequencies, thus separating information and reducing spectral damage. Panel (b) explains that RoPE's periodic extension is further weakened by inadequately trained frequency components in lower frequencies. FoPE mitigates this by zeroing out these components while retaining the zero-frequency component, which enhances periodic extension.  This leads to improved length generalization.", "section": "Building Fourier Position Embedding (FoPE)"}, {"figure_path": "https://arxiv.org/html/2412.17739/x5.png", "caption": "(a) Accuracy on Passkey Retrieval (higher is better)", "description": "This figure shows the accuracy of different models (FoPE, ROPE, ALiBi) on the Passkey Retrieval task across various sequence lengths.  Passkey Retrieval is a benchmark task where a short sequence (passkey) needs to be identified within a longer sequence of text. Higher accuracy indicates better performance in identifying the passkey. The results show how the accuracy of each model changes as the length of the input sequence increases.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.17739/x6.png", "caption": "(b) Perplexity on C4 (lower is better)", "description": "This figure shows the perplexity results on the C4 dataset for different models (FoPE, ROPE, and ALiBi) and model sizes (60M, 180M, and 1.2B parameters).  Lower perplexity indicates better performance. The x-axis represents the sequence length, and the y-axis represents perplexity.  This graph illustrates how well each model can predict the next word in a sequence, particularly as the context length (sequence length) increases. This is a measure of generalization to unseen data.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.17739/x7.png", "caption": "Figure 3: Effectiveness of FoPE in length extrapolation. Starting point models trained with a maximum sequence length of 512 are extrapolated using YARN and FoPE on a corpus with a maximum sequence length of 1024.", "description": "This figure displays the results of an experiment evaluating the effectiveness of Fourier Position Embedding (FoPE) in extrapolating models to longer sequence lengths.  Models were initially trained on sequences with a maximum length of 512 tokens.  The experiment then tested how well these models generalized to longer sequences (up to 16384 tokens) using two different extrapolation methods: YARN and FoPE. The figure presents both accuracy and perplexity metrics to assess the performance of the models across various sequence lengths. The goal is to show that FoPE helps maintain consistent accuracy and perplexity even when dealing with much longer sequences than those seen during the initial training phase.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.17739/x8.png", "caption": "Figure 4: Training with max_seq_length=512 on Gutenberg Books and evaluating on a validation set of C4, FoPE also demonstrates its ability to generalize across different data distributions.", "description": "Figure 4 presents perplexity results for various sequence lengths when using different position embedding methods (FoPE, ROPE, ALiBi) after pre-training language models.  The models were pre-trained on Gutenberg Books (a dataset with varied writing styles and authors spanning different time periods) with a maximum sequence length of 512 tokens, and then evaluated on the C4 validation set (a more consistent and modern dataset).  The results demonstrate that FoPE maintains considerably more stable perplexity across varying sequence lengths compared to ROPE and ALiBi, highlighting its superior ability to generalize to different data distributions and longer sequences.", "section": "5.2 Length Generalization after Pre-Training"}, {"figure_path": "https://arxiv.org/html/2412.17739/x9.png", "caption": "(a) Ablation for different sub-methods", "description": "This ablation study investigates the individual contributions of the two main components of FoPE: Fourier Series (FS) and Clip Floor to Zero (CF).  It compares the performance of FoPE with both components, FoPE without FS, FoPE without CF, a variant with doubled attention head dimensions, and a variant with other dimensions doubled. The results illustrate the impact of each component and of increasing the model's capacity on perplexity and accuracy.", "section": "5.4 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.17739/x10.png", "caption": "(b) Ablation for different \u03c3\ud835\udf0e\\sigmaitalic_\u03c3", "description": "This ablation study investigates the impact of the hyperparameter \\sigma (sigma) on the performance of the FoPE model.  \\sigma controls the variance of the randomly initialized weights used in constructing the Fourier Series component of FoPE. The graph likely shows perplexity ratio (PPL ratio) and accuracy on a downstream task (Passkey) as a function of  \\sigma, across different sequence lengths.  It helps determine the optimal value of  \\sigma that balances model performance and robustness to noise or spectral damage.", "section": "5.4 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.17739/x11.png", "caption": "(c) Ablation for different D\ud835\udc37Ditalic_D", "description": "This ablation study investigates the effect of varying the number of frequencies (D) in FoPE on model performance.  It shows how changing this hyperparameter impacts perplexity and accuracy across different sequence lengths, providing insights into the optimal value of D for balancing model complexity and generalization ability.", "section": "5.4 Ablation Studies"}]