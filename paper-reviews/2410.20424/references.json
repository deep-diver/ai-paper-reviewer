{"references": [{" publication_date": "2022", "fullname_first_author": "Ryan Holbrook Addison Howard", "paper_title": "Spaceship titanic", "reason": "This paper is a dataset used in the AutoKaggle experiments, which serves as a real-world, complex data science task, and is thus vital to demonstrating AutoKaggle's capabilities in handling such challenges.  Its inclusion allows for a realistic evaluation of the framework's ability to address complex data science problems.", "section_number": 4}, {" publication_date": "2016", "fullname_first_author": "DataCanary Anna Montoya", "paper_title": "House prices advanced regression techniques", "reason": "This dataset provides a complex regression task ideal for evaluating AutoKaggle's capacity to handle nuanced real-world data. The complexity of the dataset makes it a crucial benchmark for assessing AutoKaggle's performance in advanced machine learning scenarios.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "reason": "This foundational paper establishes the potential of large language models (LLMs) in few-shot learning and is essential for understanding AutoKaggle's foundation.  As AutoKaggle relies heavily on LLMs for various tasks, this work provides fundamental context and justification for its approach.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jun Shern Chan", "paper_title": "Mle-bench: Evaluating machine learning agents on machine learning engineering", "reason": "MLE-Bench provides a benchmark for evaluating language agents on machine learning tasks.  Its inclusion is relevant for the evaluation of AutoKaggle, providing context for comparative analysis and highlighting AutoKaggle's strengths and weaknesses in handling the diverse challenges posed by MLE-Bench's tabular datasets.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yizhou Chi", "paper_title": "Sela: Tree-search enhanced Ilm agents for automated machine learning", "reason": "This paper is highly relevant to AutoKaggle's methodology as it focuses on using LLMs for automated machine learning.  The comparison allows for a deeper understanding of the strengths and weaknesses of different techniques for automating the machine learning process and provides a baseline for performance evaluation.  It helps validate the effectiveness of AutoKaggle\u2019s techniques.", "section_number": 2}, {" publication_date": "2012", "fullname_first_author": "Will Cukierski", "paper_title": "Titanic - machine learning from disaster", "reason": "The Titanic dataset is a classic and widely used dataset in machine learning, particularly relevant for illustrating AutoKaggle's workflow on a well-known and thoroughly researched problem. Its inclusion as a case study demonstrates AutoKaggle's applicability to real-world problems.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Qixin Deng", "paper_title": "Composerx: Multi-agent symbolic music composition with llms", "reason": "This research showcases multi-agent systems, similar to AutoKaggle, for solving complex tasks.  This is relevant for AutoKaggle's design, providing a comparative context and justifying its choice of architecture.  The work on multi-agent systems enhances the understanding of AutoKaggle's overall design and its potential.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zhibin Gou", "paper_title": "Critic: Large language models can self-correct with tool-interactive critiquing", "reason": "This work demonstrates the effectiveness of self-correction in LLMs, which is relevant to AutoKaggle\u2019s design and supports its use of debugging and unit testing to ensure the accuracy of code. The techniques described in this paper provide valuable insights into improving the robustness of the AutoKaggle system.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Siyuan Guo", "paper_title": "Ds-agent: Automated data science by empowering large language models with case-based reasoning", "reason": "This paper addresses the challenge of automating data science tasks using LLMs, similar to the goal of AutoKaggle. The comparative analysis helps to understand the unique contributions of AutoKaggle and its advantages over existing methods.  The comparative analysis helps to illuminate the innovative aspects of AutoKaggle and its superior performance.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Md Mahadi Hassan", "paper_title": "Chatgpt as your personal data scientist", "reason": "This paper highlights the potential of LLMs in automating data science tasks. It serves as a baseline comparison for AutoKaggle, demonstrating the advancements of AutoKaggle in terms of features like iterative debugging, unit testing, and collaborative multi-agent system.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "Wendy Kan", "paper_title": "Ghouls, goblins, and ghosts... boo!", "reason": "This Kaggle competition is another dataset used for the AutoKaggle experiments.  The inclusion of this dataset adds more variety and enhances the analysis of AutoKaggle\u2019s performance across a broader range of data and task complexities.  Its inclusion supports robustness testing of AutoKaggle\u2019s capacity.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Mike Lewis", "paper_title": "Deal or no deal? end-to-end learning for negotiation dialogues", "reason": "This paper introduces the concept of multi-agent systems for complex tasks. The multi-agent system employed by AutoKaggle draws inspiration from and builds upon such approaches.  It provides important conceptual background and context for evaluating AutoKaggle's design.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Killian Lucas", "paper_title": "GitHub - KillianLucas/open-interpreter: A natural language interface for computers", "reason": "This work focuses on natural language interfaces for computers, which is highly relevant to AutoKaggle's use of LLMs and prompts for agent communication and task execution. It provides context and justification for the design choices made in the AutoKaggle framework.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Aman Madaan", "paper_title": "Self-refine: Iterative refinement with self-feedback", "reason": "This paper discusses iterative refinement, a key component of AutoKaggle\u2019s design for ensuring code quality. The iterative refinement techniques described in this paper are directly applicable to AutoKaggle\u2019s developer agent, and thus provide support for the choice of design methodology.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "OpenAI", "paper_title": "Chatgpt: Optimizing language models for dialogue", "reason": "This paper introduces ChatGPT, a large language model that is foundational to AutoKaggle's operation.  The work demonstrates the capabilities of LLMs in natural language processing, which are leveraged by AutoKaggle for numerous tasks such as code generation and plan formulation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "OpenAI", "paper_title": "Gpt-4 technical report", "reason": "This paper provides technical details about GPT-4, a large language model that is used in AutoKaggle. Understanding its capabilities is vital for appreciating the framework's functionality and limitations.  The technical report provides essential context and justification for the choice of LLM in AutoKaggle.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Dominik Schmidt", "paper_title": "Introducing Weco AIDE", "reason": "AIDE is a baseline model against which AutoKaggle's performance is measured.  This provides a critical standard for comparison, validating AutoKaggle's improvements and highlighting its superior performance characteristics. This comparative analysis is important in demonstrating the advancements achieved by AutoKaggle.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Noah Shinn", "paper_title": "Reflexion: Language agents with verbal reinforcement learning", "reason": "This paper describes language agents with verbal reinforcement learning.  This is highly relevant to understanding the design choices made in AutoKaggle and its multi-agent approach, offering valuable insights into the framework\u2019s overall architecture and methodology. This offers a benchmark for evaluating AutoKaggle\u2019s approach.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yashar Talebirad", "paper_title": "Multi-agent collaboration: Harnessing the power of intelligent Ilm agents", "reason": "This paper explores the benefits of multi-agent collaboration, which is central to AutoKaggle\u2019s architecture.  It provides theoretical support for AutoKaggle\u2019s design and helps justify the use of specialized agents working in conjunction to solve complex data science tasks.  It offers justification for the choice of a multi-agent approach in AutoKaggle.", "section_number": 2}]}