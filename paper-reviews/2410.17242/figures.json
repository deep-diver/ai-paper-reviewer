[{"figure_path": "2410.17242/figures/figures_2_0.png", "caption": "Figure 1: LVSM supports feed-forward novel view synthesis from sparse posed image inputs (even from a single view) on both objects and scenes. LVSM achieves significant quality improvements compared with the previous SOTA method, i.e., GS-LRM (Zhang et al., 2024). (Please zoom in for more details.)", "description": "Figure 1 shows examples of novel view synthesis results of LVSM on both object and scene levels, comparing against the previous state-of-the-art method.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.17242/figures/figures_4_0.png", "caption": "Figure 2: LVSM model architecture. LVSM first patchifies the posed input images into tokens. The target view to be synthesized is represented by its Pl\u00fccker ray embeddings and is also tokenized. The input view and target tokens are sent to a full transformer-based model to predict the tokens that are used to regress the target view pixels. We study two LVSM transformer architectures, as a Decoder-only architecture (left) and a Encoder-Decoder architecture (right).", "description": "This figure illustrates the two main architectures of the Large View Synthesis Model (LVSM): a decoder-only architecture and an encoder-decoder architecture, both using transformers to synthesize novel views from input images.", "section": "3.1 OVERVIEW"}, {"figure_path": "2410.17242/figures/figures_7_0.png", "caption": "Figure 3: Object-level visual comparison at 512 resolution. Given 4 sparse input posed images (leftmost column), we compare our high-res object-level novel-view rendering results with two baselines: Instant3D\u2019s Triplane-LRM (Li et al., 2023) and GS-LRM (Res-512) (Zhang et al., 2024). Both our Encoder-Decoder and Decoder-Only models exhibit fewer floaters (first example) and fewer blurry artifacts (second example), compared to the baselines. Our Decoder-Only model effectively handles complex geometry, including small holes (third example) and thin structures (fourth example). Additionally, it preserves the details of high-frequency texture (last example).", "description": "Figure 3 shows a comparison of object-level novel view synthesis results between the proposed LVSM and two baseline methods, highlighting the improved quality and handling of complex geometry by LVSM.", "section": "4.3 EVALUATION AGAINST BASELINES"}, {"figure_path": "2410.17242/figures/figures_8_0.png", "caption": "Figure 4: Scene-level visual comparison. We evaluate our encoder-decoder and decoder-only models on scene-level view synthesis, comparing them against the prior leading baseline methods, namely pixelSplat (Charatan et al., 2024), MVSplat (Chen et al., 2024), and GS-LRM (Zhang et al., 2024). Our methods exhibit fewer texture and geometric artifacts, generate more accurate and realistic specular reflections, and are closer to the ground truth images.", "description": "The figure compares the scene-level view synthesis results of the proposed LVSM models against several baseline methods, highlighting improvements in accuracy and realism.", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.17242/figures/figures_16_0.png", "caption": "Figure 3: Object-level visual comparison at 512 resolution. Given 4 sparse input posed images (leftmost column), we compare our high-res object-level novel-view rendering results with two baselines: Instant3D\u2019s Triplane-LRM (Li et al., 2023) and GS-LRM (Res-512) (Zhang et al., 2024). Both our Encoder-Decoder and Decoder-Only models exhibit fewer floaters (first example) and fewer blurry artifacts (second example), compared to the baselines. Our Decoder-Only model effectively handles complex geometry, including small holes (third example) and thin structures (fourth example). Additionally, it preserves the details of high-frequency texture (last example).", "description": "Figure 3 shows a comparison of object-level novel view rendering results using four input views with the proposed LVSM model and two baseline methods, highlighting the superior quality and detail preservation of the proposed model.", "section": "4.3 EVALUATION AGAINST BASELINES"}]