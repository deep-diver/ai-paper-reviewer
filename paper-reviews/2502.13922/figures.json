[{"figure_path": "https://arxiv.org/html/2502.13922/x1.png", "caption": "Figure 1: The comparison of long-context (InfiniteBench) and short-context (MMLU) performance among GPT-4-128K and smaller LLMs.", "description": "This figure compares the performance of several large language models (LLMs) on both long-context and short-context tasks.  The long-context performance is measured using the InfiniteBench benchmark, while the short-context performance is measured using the MMLU benchmark.  The models compared include GPT-4-128K (a high-performing model), and several smaller LLMs. The figure visually demonstrates the performance gap between short and long-context tasks, showing that even high-performing, short-context models can underperform on long-context benchmarks.  It highlights a key challenge addressed in the paper:  the difficulty of aligning LLMs to perform well in both short and long-context scenarios.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2502.13922/extracted/6217667/figures/longpo_refined.png", "caption": "Figure 2: The procedure of generating short-to-long preference data from step 1 to 7.", "description": "This figure illustrates the process of creating short-to-long preference data pairs.  It begins with a long document. A short chunk of text is extracted from that document. An instruction is generated based on this short chunk. Then the model generates a response based on this short chunk (chosen response) and the entire long document (rejected response). These two responses paired with the same instruction form a data pair showing preference for one over the other. This process is repeated many times to build up a dataset.", "section": "3.3 SELF-EVOLVING"}, {"figure_path": "https://arxiv.org/html/2502.13922/x2.png", "caption": "Figure 3: The margins of the short-context performance of Mistral-7B-LongPO and baselines relative to corresponding base model. GLM and LWM refer to the margins of GLM-9B-1M and LWM-7B-1M over GLM-9B-128K and LWM-7B-128K, respectively. MT-Bench metrics (\u2208\\in\u2208[0, 10]) are linearly scaled to [0, 100] for better comparability across tasks. See numerical results in\u00a0Table\u00a03.", "description": "This figure compares the short-context performance of Mistral-7B-LongPO and several baseline models relative to their corresponding base models. It shows the improvement or degradation in short-context performance after long-context alignment.  The baselines include models trained with supervised fine-tuning (SFT) and direct preference optimization (DPO).  For reference, the improvements for GLM-9B-1M and LWM-7B-1M over their 128K counterparts (GLM-9B-128K and LWM-7B-128K) are also displayed. The MT-Bench metric, which originally ranges from 0 to 10, is linearly scaled to 0 to 100 for easier comparison across different tasks. Detailed numerical results can be found in Table 3.", "section": "5 Results and Analyses"}, {"figure_path": "https://arxiv.org/html/2502.13922/x3.png", "caption": "Figure 4: Long- and short-context performance comparison among LongPO, SFT on chosen responses (SFT-Chosen), SFT on rejected responses (SFT-Rejected), DPO, and SFT on chosen responses with short-to-long constraint (SFT-Chosen-Constraint).", "description": "This figure compares the performance of LongPO with several baseline methods across both long- and short-context tasks.  The baselines include: Supervised Fine-Tuning (SFT) using only the chosen responses, SFT using only the rejected responses, Direct Preference Optimization (DPO), and SFT with an added short-to-long constraint. The plots show the training progress over a number of steps, tracking performance on the RULER-NIAH (long-context) and MMLU (short-context) benchmarks.  The figure illustrates LongPO's ability to maintain strong performance on short-context tasks while simultaneously improving long-context performance, a key advantage over the baseline methods.", "section": "5.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13922/extracted/6217667/figures/prompt.png", "caption": "Figure 5: The prompt for generating instruction pool.", "description": "This figure shows the prompt used to instruct a large language model (LLM) to generate a pool of instructions for creating short-to-long preference data.  The prompt guides the LLM to produce diverse, comprehensive questions that test a reader's comprehension, analytical abilities, and the capacity to interconnect key themes across a document. These questions are designed to facilitate the creation of high-quality preference pairs by ensuring that each short-to-long comparison is meaningful and covers a broad range of the document's content.", "section": "3.3 SELF-EVOLVING"}, {"figure_path": "https://arxiv.org/html/2502.13922/x4.png", "caption": "(a) The rewards for chosen response during training.", "description": "This figure shows the reward trend for chosen responses during the training process of Mistral-7B-LongPO-128K.  The reward, reflecting the model's performance on selecting preferred responses, initially fluctuates before increasing steadily. This suggests the model effectively learns from the short-to-long preferences, improving its long-context capabilities over time.", "section": "5.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13922/x5.png", "caption": "(b) The rewards for rejected response during training.", "description": "This figure shows the reward values for rejected responses during the training process of the Mistral-7B-LongPO-128K model.  The rewards reflect the model's preference for the chosen response over the rejected response as training progresses. Negative reward values indicate that the model prefers the chosen response.  Observing the trend in the rewards for rejected responses can help assess the model's alignment with the intended preferences and reveal if the model is learning to generate better long-context responses over the course of training.", "section": "5.3 Ablation Studies"}]