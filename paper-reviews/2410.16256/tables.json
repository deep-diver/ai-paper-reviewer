[{"figure_path": "2410.16256/tables/table_3_0.html", "caption": "Table 1: Training Data Construction of CompassJudger-1, Pointwise indicates that the data contains only one model's response along with the score given by the Judge model/Reward model. Pairwise indicates that the data includes responses from two models and the comparison result given by the Judge model/Reward model. Generative indicates that the data includes the Judge results as well as the reasoning process of the Judge. The number of each dataset refers to the number of candidates in the Training Data pool, not the final amount of training data.", "description": "Table 1 details the composition of the training dataset for CompassJudger-1, specifying the source, format, size, and language of each dataset.", "section": "2.1 Data Collection"}, {"figure_path": "2410.16256/tables/table_4_0.html", "caption": "Table 2: Ablation Study About the Proportion of Reward Data.", "description": "The table presents the ablation study results on the proportion of reward data used for training CompassJudger-1, showing the model's performance on RewardBench, JudgerBench, and their average across different reward data proportions.", "section": "2.2 Training Strategy and Ablation Study"}, {"figure_path": "2410.16256/tables/table_5_0.html", "caption": "Table 3: Ablation Study of General SFT Data. \"Judge Average\" refers to the evaluation score that encompasses the judging capabilities of both RewardBench and JudgerBench, while \"Subjective Average\" is the evaluation score on several subjective datasets listed in the table. The relevant evaluation results are obtained using OpenCompass (Contributors, 2023a). All results from the corresponding datasets have been normalized to percentages.", "description": "The table presents the ablation study results of the impact of general SFT data on the CompassJudger model's performance across various benchmarks.", "section": "3.2 JudgerBench Results"}, {"figure_path": "2410.16256/tables/table_6_0.html", "caption": "Table 4: Detailed Introduction of Subjective Evaluation Datasets in JDB-B The official FoFo dataset includes only English, and we created the Chinese portion. Additionally, due to the outdated references in AlignBench, we changed its evaluation method from Pointwise to Pairwise.", "description": "Table 4 details the subjective evaluation datasets used in JudgerBench part B, specifying their data format, number of turns, scenario label, and language.", "section": "3.1 JudgerBench Construction"}, {"figure_path": "2410.16256/tables/table_6_1.html", "caption": "Table 5: Results on RewardBench and JudgerBench, Which JDB-A means JudgerBench partA, JDB-B means JudgerBench partB.", "description": "Table 5 presents the results of several models on RewardBench and JudgerBench, showing their performance on different evaluation metrics.", "section": "3.2 JudgerBench Results"}, {"figure_path": "2410.16256/tables/table_7_0.html", "caption": "Table 6: Detailed Results on RewardBench.", "description": "The table presents a detailed breakdown of the performance of various models (including CompassJudger series and other LLMs) on the RewardBench dataset, showing their scores across different categories: Chat, Chat Hard, Safety, and Reasoning.", "section": "3.2 JudgerBench Results"}, {"figure_path": "2410.16256/tables/table_8_0.html", "caption": "Table 7: Detailed Results on JDB-A-EN.", "description": "This table presents the detailed results of different models on the English section of the JudgerBench Arena component, categorized by task type.", "section": "3.2 JudgerBench Results"}, {"figure_path": "2410.16256/tables/table_8_1.html", "caption": "Table 8: Detailed Results on JDB-A-CN.", "description": "Table 8 presents the detailed performance of different models on the Chinese section of JudgerBench A, broken down by task category.", "section": "3.2 JudgerBench Results"}, {"figure_path": "2410.16256/tables/table_8_2.html", "caption": "Table 9: Detailed Accuracy Results on JDB-B.", "description": "Table 9 presents the accuracy results of different models on the JudgerBench B, which includes four datasets: AlignBench, FoFo, WildBench, and ArenaHard, showing the accuracy of each model on each dataset and the average accuracy across all four datasets.", "section": "3.2 JudgerBench Results"}, {"figure_path": "2410.16256/tables/table_9_0.html", "caption": "Table 10: Detailed Correlation Results on JDB-B.", "description": "The table presents the correlation results of several models' judgments on the JudgerBench B dataset, using the accuracy rate per question and the correlation results based on the overall model scores as evaluation metrics.", "section": "3.2 JudgerBench Results"}, {"figure_path": "2410.16256/tables/table_10_0.html", "caption": "Table 2: Ablation Study About the Proportion of Reward Data.", "description": "The table presents the ablation study results on different proportions of reward data used for training CompassJudger-1, evaluating performance across RewardBench, JudgerBench, and their average.", "section": "2.2 Training Strategy and Ablation Study"}, {"figure_path": "2410.16256/tables/table_11_0.html", "caption": "Table 1: Training Data Construction of CompassJudger-1, Pointwise indicates that the data contains only one model's response along with the score given by the Judge model/Reward model. Pairwise indicates that the data includes responses from two models and the comparison result given by the Judge model/Reward model. Generative indicates that the data includes the Judge results as well as the reasoning process of the Judge. The number of each dataset refers to the number of candidates in the Training Data pool, not the final amount of training data.", "description": "Table 1 details the composition of the CompassJudger-1 training dataset, specifying the source, format, size, and language of each dataset.", "section": "2 CompassJudger-1"}]