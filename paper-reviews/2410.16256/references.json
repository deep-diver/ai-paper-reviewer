{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "This paper is foundational for understanding the capabilities of GPT-4, which is used as a benchmark in the JudgerBench dataset, therefore significantly influencing the comparative performance of the CompassJudger model.  Its technical details provide a basis for evaluating the proposed model's efficacy and advancements.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zheng Cai", "paper_title": "Internlm2 technical report", "reason": "InternLM2 is mentioned as a model undergoing iterative training in the paper.  This reference is vital because it demonstrates the potential of using iterative training techniques to improve model performance, particularly in tasks similar to those performed by CompassJudger.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Chatbot arena: An open platform for evaluating llms by human preference", "reason": "This paper introduces Chatbot Arena, which is a major data source for the JudgerBench dataset used to evaluate CompassJudger. Its methodology and data are integral to the creation and evaluation of the CompassJudger model, hence its importance in the study.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "OpenCompass Contributors", "paper_title": "Opencompass: A universal evaluation platform for foundation models", "reason": "OpenCompass is the platform on which the CompassJudger model was developed and evaluated.  Understanding its architecture and functionality is essential to comprehending the methods and results of this research.  The paper is critical in demonstrating the practical application of the presented model.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "XTuner Contributors", "paper_title": "Xtuner: A toolkit for efficiently fine-tuning llm", "reason": "XTuner is the training framework used in this research.  Understanding its capabilities is crucial to assessing the methodological rigor and reproducibility of the training process used to develop CompassJudger-1.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Ganqu Cui", "paper_title": "Ultrafeedback: Boosting language models with high-quality feedback", "reason": "UltraFeedback is one of the datasets utilized in training CompassJudger-1. Understanding its design and data composition is vital to evaluating the model's training process and its resulting performance.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Hanze Dong", "paper_title": "Rlhf workflow: From reward modeling to online rlhf", "reason": "This paper provides valuable insights into the reinforcement learning from human feedback (RLHF) workflow, a technique highly relevant to the training of judge models like CompassJudger-1, and to understanding the context of reward model tuning used in this research.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yann Dubois", "paper_title": "Alpacaeval: A simple way to debias automatic evaluators", "reason": "This paper discusses AlpacaEval, one of the benchmarks used in the JudgerBench. Understanding its properties and how it was used is vital to contextualizing the performance evaluation of CompassJudger.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yann Dubois", "paper_title": "Alpacafarm: A simulation framework for methods that learn from human feedback", "reason": "AlpacaFarm is a dataset directly used to train CompassJudger. This citation is essential for understanding the composition of the training data and its impact on the final model's performance and capabilities.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Pei Ke", "paper_title": "Critiquellm: Scaling Ilm-as-critic for effective and explainable evaluation of large language model generation", "reason": "CritiqueLLM is a relevant open-source model mentioned in the paper; this comparison highlights the unique characteristics of CompassJudger and its advancements over existing alternatives. Understanding CritiqueLLM is essential to comparing it to CompassJudger-1 and its contributions.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Seungone Kim", "paper_title": "The biggen bench: A principled benchmark for fine-grained evaluation of language models with language models", "reason": "BigGen Bench is another benchmark model referenced. Understanding this benchmark is vital to evaluating and comparing the performance of CompassJudger and assessing its capabilities in comparison to other evaluation frameworks.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Nathan Lambert", "paper_title": "Rewardbench: Evaluating reward models for language modeling", "reason": "RewardBench is a benchmark dataset specifically mentioned in the paper.  This makes the cited paper crucial to the discussion of relevant evaluation methodologies and data for judge models. It provides crucial context for the JudgerBench dataset introduced later in the paper.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Tian Lan", "paper_title": "Criticbench: Evaluating large language models as critic", "reason": "CritiqueBench is another benchmark dataset mentioned in the paper.  Understanding this dataset is crucial to the discussion of relevant evaluation methodologies and data for judge models, adding context to JudgerBench.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Junlong Li", "paper_title": "Generative judge for evaluating alignment", "reason": "This paper explores generative judge models.  Since CompassJudger-1 is also a generative model, this citation is significant for understanding the existing research in this area and for comparing CompassJudger-1's capabilities to other generative judge models.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tianle Li", "paper_title": "From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline", "reason": "ArenaHard is a key dataset used in the JudgerBench dataset, and this paper provides essential background for the design and the methodology behind ArenaHard. The details about data curation and creation are essential to the overall evaluation.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Bill Yuchen Lin", "paper_title": "Wildbench: Benchmarking llms with challenging tasks from real users in the wild", "reason": "WildBench is another significant dataset included in the JudgerBench, and this paper describes its creation and the evaluation methodologies used.  Understanding the details of WildBench and its use is critical for evaluating the overall performance of CompassJudger.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Xiao Liu", "paper_title": "Alignbench: Benchmarking chinese alignment of large language models", "reason": "AlignBench is a benchmark dataset used in the JudgerBench dataset, and this paper describes its design and methodology. This paper provides essential context to understand the performance evaluation of CompassJudger.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Junsoo Park", "paper_title": "Offsetbias: Leveraging debiased data for tuning evaluators", "reason": "This paper explores debiasing techniques for evaluating LLMs. This is highly relevant to the CompassJudger-1 evaluation because the approach addresses biases in subjective evaluation data that the authors aimed to mitigate in their training data.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tu Shiwen", "paper_title": "Skywork critic model series", "reason": "Skywork is another open-source model that is mentioned in the paper and is directly compared to CompassJudger.  This comparison provides valuable context for understanding the relative strengths and weaknesses of each model. The results presented in this research highlight the superiority of CompassJudger-1 compared to other publicly available judge LLMs.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Yidong Wang", "paper_title": "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization", "reason": "PandaLM is a dataset used in the CompassJudger-1 training.  Understanding its design and data is vital for evaluating the training data and its influence on CompassJudger's performance. It provides important context for the overall composition of the training dataset.", "section_number": 2}]}