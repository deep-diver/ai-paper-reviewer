{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "This paper is foundational because it provides the technical details of GPT-4, a model frequently used as a benchmark for comparing the performance of other LLMs, and is directly referenced as a benchmark in the introduction and JudgerBench sections. The introduction highlights human-based subjective evaluation's high cost and lack of reproducibility.  This paper details the limitations of existing objective and subjective methods and therefore helps establish context for why a new approach is needed.  The detailed technical description of GPT-4 is a key piece of information to understand the baseline performance of the benchmark used to evaluate the proposed CompassJudger-1.  The methods and results presented are relevant to the overall discussion of LLM evaluation methodologies and the development of more efficient and accurate evaluation techniques. The results also help in highlighting the value and effectiveness of the proposed methods for improving LLM evaluation processes.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zheng Cai", "paper_title": "Internlm2 technical report", "reason": "This paper is crucial for the methodology section as it explains the iterative training strategies used in the development of InternLM2.5, a key component of the JudgerBench benchmark. The model comparison and training techniques described are essential for understanding the broader context of how LLMs are being trained and evaluated. The details of the data collection methods, especially the balancing of the various class labels in the training data, are relevant to the similar techniques used in CompassJudger-1's training process.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Chatbot arena: An open platform for evaluating llms by human preference", "reason": "This work is highly relevant to the JudgerBench section of the paper as it describes the creation of Chatbot Arena, a key data source used in building the JudgerBench benchmark. The methods of evaluating LLMs through human preference, the creation of the benchmark, and the focus on human preferences are all directly relevant to the design and creation of JudgerBench in the CompassJudger-1 paper.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "OpenCompass Contributors", "paper_title": "Opencompass: A universal evaluation platform for foundation models", "reason": "This paper is significant because it details the OpenCompass platform, a universal evaluation platform for foundation models. The platform's functions, including normalization of results, are directly referenced in the experiments section of the paper. The platform's ability to provide a unified setting for evaluating different judge models is crucial for a fair and comparable evaluation, and therefore, understanding the platform's functionality is crucial to evaluating the performance of CompassJudger-1 within the scope of the overall work.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Ganqu Cui", "paper_title": "Ultrafeedback: Boosting language models with high-quality feedback", "reason": "The paper is particularly relevant to the data collection and training strategies used in developing CompassJudger-1, described in section 2. The techniques described in this paper, particularly those relating to improving training data quality through re-evaluation and categorization, directly influence the methodologies used in building CompassJudger-1 and thus are critical to understanding the process.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Hanze Dong", "paper_title": "Rlhf workflow: From reward modeling to online rlhf", "reason": "This paper's discussion of Reward Modeling is directly relevant to the CompassJudger-1's training process which incorporates reward data. The use of reward data in the training of CompassJudger-1 is crucial for its functionality.  Therefore understanding reward modeling techniques is directly relevant to the methods used in the main paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yann Dubois", "paper_title": "Alpacaeval: A simple way to debias automatic evaluators", "reason": "This paper is highly relevant to the JudgerBench section of the CompassJudger-1 paper as it details a method for automatically debiasing evaluators which is directly relevant to the methodologies used to ensure fair and unbiased evaluation results in JudgerBench. AlpacaEval is referenced in the construction of JudgerBench and understanding its methodologies is directly relevant to understanding the process involved in building JudgerBench.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yann Dubois", "paper_title": "Alpacafarm: A simulation framework for methods that learn from human feedback", "reason": "This paper introduces AlpacaFarm, a key dataset used in constructing the JudgerBench benchmark in the CompassJudger-1 paper.  Understanding the methods of AlpacaFarm, the dataset used to train CompassJudger-1, and how its simulation framework is designed are directly relevant to understanding the approach taken in building the JudgerBench benchmark. The specific details of how human feedback is used in AlpacaFarm and how that relates to the CompassJudger-1's training process is extremely important for context.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Pei Ke", "paper_title": "Critiquellm: Scaling Ilm-as-critic for effective and explainable evaluation of large language model generation", "reason": "This paper is directly relevant to the CompassJudger-1's design and function as an all-in-one model capable of both scoring and critique generation, which is described in Section 2.  The development and methods used to train and build CritiqueLLM are directly comparable and relevant to those used in CompassJudger-1, which expands the scope of its functionality beyond scoring.  Therefore, the methods used in training and evaluation are directly relevant to the methods used in CompassJudger-1.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Seungone Kim", "paper_title": "The biggen bench: A principled benchmark for fine-grained evaluation of language models with language models", "reason": "This paper is relevant to the JudgerBench section as it introduces a new benchmark for fine-grained evaluation of LLMs. The benchmark's design principles, such as its focus on various subjective evaluation tasks, are directly relevant to the development and design principles behind the JudgerBench. Thus, understanding Biggen Bench is important for understanding the overall context and relative performance of JudgerBench.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Nathan Lambert", "paper_title": "Rewardbench: Evaluating reward models for language modeling", "reason": "This paper introduces RewardBench, a dataset used for evaluating reward models. The dataset\u2019s design and methodology are directly relevant to the development of the JudgerBench dataset used to evaluate CompassJudger-1. This reference helps clarify the design rationale and provides a comparison point for evaluating the proposed methodology.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Junlong Li", "paper_title": "Generative judge for evaluating alignment", "reason": "This paper introduces a generative judge model, providing a direct comparison point for understanding the innovation behind CompassJudger-1's all-in-one functionality, which is highlighted in Section 2.  Understanding the capabilities and limitations of other generative judge models is crucial for putting CompassJudger-1 into context.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tianle Li", "paper_title": "From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline", "reason": "This work is highly relevant to the JudgerBench section as it discusses the pipeline used to create high-quality benchmarks, especially focusing on ArenaHard which is included in JudgerBench.  The methods and techniques detailed in this paper are directly relevant to the creation and design of JudgerBench, and therefore, understanding this reference helps contextualize and validate the methodology used in building the benchmark.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Bill Yuchen Lin", "paper_title": "Wildbench: Benchmarking llms with challenging tasks from real users in the wild", "reason": "This paper presents WildBench, a dataset included in the JudgerBench benchmark. Understanding the design and scope of WildBench is critical for understanding the diversity and scope of the evaluation tasks in JudgerBench, and how this impacts the evaluation and performance of CompassJudger-1 in realistic scenarios.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Xiao Liu", "paper_title": "Alignbench: Benchmarking chinese alignment of large language models", "reason": "This paper is highly relevant to the JudgerBench section because it introduces AlignBench, a dataset that forms a significant part of the JudgerBench benchmark. Understanding AlignBench's design, evaluation tasks, and methodologies is crucial for assessing the overall comprehensiveness and reliability of JudgerBench as a benchmark for evaluating judge models.  The dataset's characteristics are directly relevant to the composition and capabilities of JudgerBench.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Junsoo Park", "paper_title": "Offsetbias: Leveraging debiased data for tuning evaluators", "reason": "This paper discusses techniques for debiasing data used in tuning evaluators. These techniques are highly relevant to the data processing methods employed in creating the CompassJudger-1 model, as detailed in section 2, which focuses on addressing data imbalance and mitigating the risk of rigid response patterns.  The methods used in this paper are very relevant to the techniques used in the primary paper to build a more robust model.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tu Shiwen", "paper_title": "Skywork critic model series", "reason": "This paper is relevant as it describes the Skywork model, one of the models used as a baseline for comparison in the results section of the CompassJudger-1 paper.  The comparison provides crucial context to understand the performance and relative strengths of CompassJudger-1 compared to other existing models in the field of judge LLMs.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Yidong Wang", "paper_title": "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization", "reason": "PandaLM is another important dataset used in the construction of the JudgerBench benchmark. Understanding the dataset's characteristics, evaluation tasks, and methodologies is crucial for a complete understanding of JudgerBench's design and the performance of CompassJudger-1.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Congying Xia", "paper_title": "Fofo: A benchmark to evaluate llms' format-following capability", "reason": "This paper is relevant to the JudgerBench section because it introduces FoFo, a dataset included in the JudgerBench benchmark.  Understanding the characteristics and evaluation metrics of FoFo is essential for assessing the overall quality and scope of the JudgerBench benchmark, and how this impacts the evaluation and performance of CompassJudger-1.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "An Yang", "paper_title": "Qwen2 technical report", "reason": "This paper provides technical details of Qwen2.5, a foundational model for the CompassJudger-1 model, which is described in Section 2.  Understanding the technical details of Qwen2.5 is crucial for understanding the capabilities and limitations of the models used to build CompassJudger-1 and the influence that the base model had on the capabilities of the final model.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Lianghui Zhu", "paper_title": "Judgelm: Fine-tuned large language models are scalable judges", "reason": "JudgeLM is another significant dataset used in creating the JudgerBench benchmark.  Understanding the design principles, methodologies, and evaluation metrics of JudgeLM is crucial for understanding the scope and quality of JudgerBench, and thus allows for a more comprehensive understanding of the CompassJudger-1 model evaluation and comparison.", "section_number": 3}]}