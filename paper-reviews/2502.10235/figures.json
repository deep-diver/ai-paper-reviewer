[{"figure_path": "https://arxiv.org/html/2502.10235/x1.png", "caption": "(a)", "description": "The figure shows how AdaPTS improves the accuracy of probabilistic predictions compared to ground truth.  The plot displays the predictions (Moment+AdaPTS) and the ground truth for the ETTh1 dataset over time steps, clearly showing that AdaPTS improves accuracy over the Moment model alone. This is particularly evident because the prediction using AdaPTS closely follows the trends in the ground truth data.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.10235/extracted/6204734/figures/pipeline.png", "caption": "(b)", "description": "This figure shows the AdaPTS framework's architecture.  A multivariate time series (X) is input.  An adapter (phi) projects it into a stochastic latent space (Z), which is then fed into a pre-trained univariate foundation model (FM).  The model's output in the latent space is then inversely projected by the adapter's inverse function (phi^-1) to obtain a probabilistic prediction (Y) in the original feature space. The diagram uses a fire symbol to indicate trainable weights and a snowflake to indicate frozen parameters from the FM. ", "section": "3. Adapters"}, {"figure_path": "https://arxiv.org/html/2502.10235/x2.png", "caption": "Figure 1: (a) Augmenting Moment time series foundation model with the AdaPTS framework provides probabilistic and more accurate predictions. (b) The AdaPTS framework: The input time series is transformed through a feature space transformation \u03c6\ud835\udf11\\varphiitalic_\u03c6 that maps into a stochastic latent space. The prediction is then conducted using a pre-trained FM before transforming back the predicted, now distribution, to the original feature space. The fire symbol indicate trainable weights while the snowflake implicates that the parameters of the FM are kept frozen.", "description": "Figure 1 illustrates the AdaPTS framework and its application. Panel (a) shows an example of how AdaPTS improves predictions by adding probability and accuracy compared to a standard model. Panel (b) details the framework's architecture.  A multivariate input time series is first transformed into a lower-dimensional stochastic latent space using a learnable feature transformation  \u03c6. Then, a pre-trained univariate foundation model (FM) processes each dimension independently in this latent space. Finally, an inverse transformation maps the predictions back into the original multivariate space, producing a probabilistic forecast. The figure uses visual cues to distinguish trainable parameters (fire symbol) from frozen parameters of the FM (snowflake).", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.10235/x3.png", "caption": "Figure 2: Optimality of \ud835\udc16\u03c6\u2217superscriptsubscript\ud835\udc16\ud835\udf11\\mathbf{W}_{\\varphi}^{*}bold_W start_POSTSUBSCRIPT italic_\u03c6 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT. Comparing the MSE obtained with \ud835\udc16\u03c6\u2217superscriptsubscript\ud835\udc16\ud835\udf11\\mathbf{W}_{\\varphi}^{*}bold_W start_POSTSUBSCRIPT italic_\u03c6 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT against the baseline, for 1000 randomly generated linear FM.", "description": "This figure demonstrates the superiority of the optimal linear adapter,  represented as  \\(\\mathbf{W}_{\\varphi}^{*}\\), over a baseline approach for multivariate time series forecasting.  1000 linear foundation models (FMs) were randomly generated. For each FM, the mean squared error (MSE) was calculated using both the optimal adapter \\(\\mathbf{W}_{\\varphi}^{*}\\) and a baseline (identity matrix). The results show that  \\(\\mathbf{W}_{\\varphi}^{*}\\) consistently achieves lower MSE, indicating its effectiveness in improving forecasting accuracy compared to the baseline.", "section": "3. Adapters"}, {"figure_path": "https://arxiv.org/html/2502.10235/x4.png", "caption": "Figure 3: Impact of the number of components on model performance. The dashed line indicates Moment performance without adapters, the shaded area its standard deviation, and the vertical line the number of original features.", "description": "This figure displays the effect of varying the number of components within the adapter on the model's forecasting accuracy, using the Mean Squared Error (MSE) as a metric.  The results are shown for four different datasets (ETTh1, Illness, Weather, and ExchangeRate) across various prediction horizons (H). The dashed line represents the baseline performance of the Moment model without any adapters. The shaded region surrounding this line indicates the standard deviation. The vertical line marks the number of original features in the input dataset. This visualization helps to understand the optimal number of components needed for each adapter to balance model complexity and predictive performance.", "section": "5. Experiments & Results"}, {"figure_path": "https://arxiv.org/html/2502.10235/x5.png", "caption": "Figure 4: Visualization of the latent representation obtained by different adapters (with number of components equal to 2) on Illness(H=24\ud835\udc3b24H=24italic_H = 24). Shaded colors indicate the time dimension, with lighter colors representing earlier timesteps.", "description": "This figure visualizes the latent representations learned by different adapter types (PCA, dropoutLinearAE, LinearVAE, and VAE) when applied to the Illness dataset with a prediction horizon of 24 time steps.  Each adapter reduces the dimensionality of the input data to two components.  The plot shows a scatter plot of the latent representations, where the shading of each point indicates the time step within the sequence. Lighter colors correspond to earlier time points in the sequence, and darker colors to later time points. This visualization allows for the comparison of the distinct latent spaces generated by each adapter, highlighting the effects of each adapter on the data's structure and distribution.", "section": "5.3. Interpretability of the latent representations"}, {"figure_path": "https://arxiv.org/html/2502.10235/x6.png", "caption": "Figure 5: Reliability diagram for the first feature of the ETTh1 (H=96\ud835\udc3b96H=96italic_H = 96) dataset using LinearVAE.", "description": "This reliability diagram visualizes the calibration of probabilistic forecasts generated by the LinearVAE adapter for the first feature of the ETTh1 dataset with a prediction horizon of 96 time steps.  It plots the observed proportion of instances where the predicted quantile range contains the actual value against the predicted quantile. A perfectly calibrated model would show a diagonal line, indicating that predictions align with observed values. Deviations from this diagonal reveal over- or underestimation of uncertainty at different quantiles.", "section": "5.4. On the calibration of the probabilistic adapters"}, {"figure_path": "https://arxiv.org/html/2502.10235/x7.png", "caption": "Figure 6: \u03b2\ud835\udefd\\betaitalic_\u03b2 and log\u2061\u03c32superscript\ud835\udf0e2\\log\\sigma^{2}roman_log italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT VAE hyperparameters ablation on the Illness(H=24\ud835\udc3b24H=24italic_H = 24) dataset. For reference, the Moment baseline score on this task is 2.902\u00b10.023subscript2.902plus-or-minus0.0232.902_{\\pm 0.023}2.902 start_POSTSUBSCRIPT \u00b1 0.023 end_POSTSUBSCRIPT.", "description": "This figure shows the results of an ablation study on the VAE adapter's hyperparameters:  \u03b2 (beta), which controls the disentanglement of the latent representation, and log \u03c3\u00b2 (log sigma squared), representing the noise scale in the likelihood model. The heatmaps illustrate how different combinations of \u03b2 and log \u03c3\u00b2 affect the Mean Squared Error (MSE) and Expected Calibration Error (ECE) on the Illness dataset with a forecasting horizon of 24 time steps.  Lower MSE values indicate better forecasting accuracy, and lower ECE values suggest better calibration (confidence matches accuracy). The study examines the impact of these hyperparameters on both MSE and ECE, providing insights into how their tuning affects performance and calibration. The baseline MSE score for the Illness dataset is also given as a reference.", "section": "5.5 Ablation studies"}, {"figure_path": "https://arxiv.org/html/2502.10235/x8.png", "caption": "Figure 7: LinearAE components ablation.", "description": "This ablation study investigates the individual contributions of the encoder and decoder components within the Linear AutoEncoder (LinearAE) adapter architecture.  The experiment compares the full LinearAE against versions using only the encoder (LinearEncoder) or only the decoder (LinearDecoder). The results show the impact of each component on forecasting accuracy across three datasets: ETTh1, Weather, and ExchangeRate. This helps to understand which part of the LinearAE plays the most crucial role in improving the forecasting performance and whether the inclusion of both encoder and decoder is necessary for optimal results.", "section": "5.5 Ablation studies"}, {"figure_path": "https://arxiv.org/html/2502.10235/x9.png", "caption": "(a) Independent", "description": "This figure displays the results of applying the Moment model to synthetic datasets, specifically focusing on the Mean Squared Error (MSE). Subfigure (a) showcases the MSE for a synthetic dataset with independent components. The comparison includes the results of using PCA as an adapter, the adapter with parameters learned directly (\u03b8), and the composition of the learned parameters with PCA (\u03b8* \u2022 Wpca).  The x-axis represents the type of adapter used, while the y-axis shows the MSE.  Subfigure (b) shows the same comparison but uses a correlated synthetic dataset.", "section": "5. Experiments & Results"}, {"figure_path": "https://arxiv.org/html/2502.10235/x10.png", "caption": "(b) Correlated", "description": "This figure shows the mean squared error (MSE) achieved by different adapter methods on a synthetic dataset with correlated features. The x-axis represents different adapter types, including the baseline (PCA), and the proposed linear adapters. The y-axis represents the MSE values. The figure illustrates that the proposed linear adapter (W\u03c6) outperforms other methods, including PCA, in terms of MSE, indicating its effectiveness in improving forecasting accuracy in multivariate time series with complex inter-feature relationships.", "section": "3.3 Working example"}, {"figure_path": "https://arxiv.org/html/2502.10235/x11.png", "caption": "Figure 8: Moment on simulated independent data.", "description": "This figure displays the mean squared error (MSE) achieved by different adapter types when applied to the Moment model on simulated independent and correlated datasets.  The x-axis represents the adapter type, including the baseline PCA (Principal Component Analysis) and the proposed methods.  The y-axis represents the MSE.  The plot visualizes a comparison between using only PCA, a standard linear adapter, and a combination of PCA and a learned linear adapter. The results illustrate the efficacy of the proposed adapters, showing that the combination of PCA and the learned adapter significantly reduces MSE compared to the baseline methods, particularly on the correlated dataset.", "section": "5. Experiments & Results"}]