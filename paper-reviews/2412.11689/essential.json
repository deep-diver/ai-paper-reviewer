{"importance": "**This work has significant implications for privacy in Vertical Federated Learning (VFL).** By demonstrating the resilience of MLP-based models to feature reconstruction attacks, it challenges the current focus on complex cryptographic and obfuscation methods. **This opens new avenues for simpler, more efficient privacy-preserving techniques in VFL** and encourages researchers to reconsider architectural design choices for enhanced privacy. It also highlights the limitations of certain attack strategies and the importance of evaluating privacy risks using human-centric metrics like FID.", "summary": "Simple tweak, big privacy win: MLP-based architectures boost data protection in federated learning.", "takeaways": ["Feature reconstruction attacks in Split Learning are ineffective against MLP-based client models.", "Orthogonal transformations of data and weights do not affect training in SGD-based split learning.", "Prior knowledge of the data distribution is crucial for the success of feature reconstruction attacks, and lacking this knowledge, even malicious servers struggle to reconstruct private data in MLP-based VFL settings. "], "tldr": "Vertical Federated Learning (VFL) allows multiple parties to collaboratively train a shared machine learning model on datasets with different features without directly sharing raw data. However, **VFL is still susceptible to privacy attacks**, like feature reconstruction, where attackers attempt to rebuild the private data. Existing attacks, like Model Inversion and Feature-space Hijacking, exploit knowledge of the data distribution and focus on models using Convolutional Neural Networks (CNNs). These attacks work by accessing the model's architecture or utilizing an auxiliary public dataset to infer the private data.\nThis paper demonstrates that a **simple change in model architecture** can significantly enhance data protection in VFL. The researchers theoretically and empirically prove that Multi-Layer Perceptrons (MLPs), networks made up of simple interconnected layers, are **robust to feature reconstruction attacks**, particularly state-of-the-art attacks like Model Inversion and Feature-space Hijacking. **They highlight that these attacks are successful mostly because they use prior knowledge about the data distribution and CNNs** lack specific architectural properties that MLPs possess. This work theoretically explains this behaviour and provides experiments confirming their results, suggesting that **MLPs offer an effective defense mechanism against feature reconstruction attacks by themselves** and do not require additional complex changes to their privacy-preserving structure.", "affiliation": "MIPT", "categories": {"main_category": "Machine Learning", "sub_category": "Federated Learning"}, "podcast_path": "2412.11689/podcast.wav"}