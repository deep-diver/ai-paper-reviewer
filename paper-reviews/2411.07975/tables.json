[{"content": "|                       | Stage 1        | Stage 2        | Stage 3        |\n|-----------------------|-----------------|-----------------|-----------------|\n| Learning Rate          | 1.0e-04          | 1e-04           | 2.0e-05          |\n| LR Scheduler           | Constant        | Constant        | Constant        |\n| Weight Decay           | 0.0              | 0.0              | 0.0              |\n| Gradient Clip          | 1.0              | 1.0              | 1.0              |\n| Optimizer              | AdamW (\u03b2\u2081=0.9,\u03b2\u2082=0.95) | AdamW (\u03b2\u2081=0.9,\u03b2\u2082=0.95) | AdamW (\u03b2\u2081=0.9,\u03b2\u2082=0.95) |\n| Warm-up Steps         | 2,000            | 0                | 1,000            |\n| Training Steps        | 10,000           | 380,000          | 26,000           |\n| Batch Size             | 512              | 512              | 256              |\n| Data Ratio             | 50:50:0          | 14:80:6          | 21:70:9          |", "caption": "Table 1: Hyper-parameters of the proposed JanusFlow. Data ratio denotes the proportion of multimodal understanding data, image generation data and text-only data. In the initial 10,0001000010,00010 , 000 steps of Stage 2, we apply a data ratio of 30:50:20:3050:2030:50:2030 : 50 : 20 to boost the understanding ability.", "description": "This table details the hyperparameters used during the three training stages of the JanusFlow model.  It shows the learning rate, learning rate scheduler, weight decay, gradient clipping value, optimizer used, warm-up steps, total training steps, batch size, and the data ratio for each stage. The data ratio specifies the proportion of multimodal understanding data, image generation data, and text-only data used in each training stage.  Note that the initial 10,000 steps of Stage 2 utilize a specific data ratio (30:50:20) to prioritize the model's understanding capabilities before transitioning to a different ratio for the remainder of Stage 2.", "section": "3.3 Training Schemes"}, {"content": "Type|Method|Params|Single Obj.|Two Obj.|Count.|Colors|Pos.|Color Attri.|Overall \u2191\n---|---|---|---|---|---|---|---|---|---\nGen. Only|LlamaGen [83]|0.8B|0.71|0.34|0.21|0.58|0.07|0.04|0.32\n|LDM [75]|1.4B|0.92|0.29|0.23|0.70|0.02|0.05|0.37\n|SDv1.5 [75]|0.9B|0.97|0.38|0.35|0.76|0.04|0.06|0.43\n|PixArt-\u03b1 [9]|0.6B|0.98|0.50|0.44|0.80|0.08|0.07|0.48\n|SDv2.1 [75]|0.9B|0.98|0.51|0.44|0.85|0.07|0.17|0.50\n|DALL-E 2 [74]|6.5B|0.94|0.66|0.49|0.77|0.10|0.19|0.52\n|Emu3-Gen [91]|8B|0.98|0.71|0.34|0.81|0.17|0.21|0.54\n|SDXL [71]|2.6B|0.98|0.74|0.39|0.85|0.15|0.23|0.55\n|IF-XL [17]|4.3B|0.97|0.74|0.66|0.81|0.13|0.35|0.61\n|DALL-E 3 [6]| -|0.96|0.87|0.47|0.83|0.43|0.45|0.67\nUnified|Chameleon [85]|34B|-|-|-|-|-|-|0.39\n|LWM [58]|7B|0.93|0.41|0.46|0.79|0.09|0.15|0.47\n|SEED-X \u2020 [27]|17B|0.97|0.58|0.26|0.80|0.19|0.14|0.49\n|Show-o [96]|1.3B|0.95|0.52|0.49|0.82|0.11|0.28|0.53\n|Janus [93]|1.3B|0.97|0.68|0.30|0.84|0.46|0.42|0.61\nJanusFlow (Ours)|1.3B|0.97|0.59|0.45|0.83|0.53|0.42|0.63", "caption": "Table 2: Performances on GenEval benchmark. \u201cGen.\u201d denotes \u201cgeneration\u201d and \u201cUnified\u201d denotes unified understanding and generation models. Models using external pre-trained generative models are signed with \u2020.", "description": "Table 2 presents the results of the GenEval benchmark, a test designed to evaluate the image generation capabilities of different models.  It compares the performance of various models, categorized as either 'generation-only' or 'unified' (combining understanding and generation).  The benchmark assesses generation quality across several sub-tasks: single object, two objects, counting, colors, position, color attributes, and an overall score.  Models using external, pre-trained generative models are marked with a \u2020 symbol. The table allows for a direct comparison of specialized image generation models against unified multimodal models, highlighting the tradeoffs between specialized and general-purpose approaches.", "section": "4.4 Quantitative Results"}, {"content": "| Method | Global | Entity | Attribute | Relation | Other | Overall \u2191 |\n|---|---|---|---|---|---|---|\n| SDv1.5 [75] | 74.63 | 74.23 | 75.39 | 73.49 | 67.81 | 63.18 |\n| PixArt-\u03b1 [9] | 74.97 | 79.32 | 78.60 | 82.57 | 76.96 | 71.11 |\n| Lumina-Next [105] | 82.82 | 88.65 | 86.44 | 80.53 | 81.82 | 74.63 |\n| SDXL [71] | 83.27 | 82.43 | 80.91 | 86.76 | 80.41 | 74.65 |\n| Playground v2.5 [48] | 83.06 | 82.59 | 81.20 | 84.08 | 83.50 | 75.47 |\n| Hunyuan-DiT [54] | 84.59 | 80.59 | 88.01 | 74.36 | 86.41 | 78.87 |\n| PixArt-\u03a3 [10] | 86.89 | 82.89 | 88.94 | 86.59 | 87.68 | 80.54 |\n| Emu3-Gen [91] | 85.21 | 86.68 | 86.84 | 90.22 | 83.15 | 80.60 |\n| JanusFlow (Ours) | 87.03 | 87.31 | 87.39 | 89.79 | 88.10 | 80.09 |", "caption": "Table 3: Performances on DPG-Bench. The methods in this table are all generation-specific models except our method.", "description": "This table presents a comparison of performance scores on the DPG-Bench benchmark across various generation-specific models and the JanusFlow model.  DPG-Bench is a metric that evaluates the quality of image generation, specifically focusing on aspects such as overall image quality, entity and attribute accuracy, relation accuracy, and handling of other scene elements. The table shows that JanusFlow, a unified multimodal model (capable of both image understanding and generation), outperforms most generation-specific models on this benchmark. This highlights JanusFlow's ability to achieve competitive or superior results on generation tasks compared to models solely focused on that aspect.", "section": "4.4 Quantitative Results"}, {"content": "| Method | Params | FID\u2193 |\n|---|---|---|\n| LWM [58] | 7B | 17.77 |\n| VILA-U 256 [95] | 7B | 12.81 |\n| VILA-U 384 [95] | 7B | 7.69 |\n| Show-o [96] | 1.3B | 15.18 |\n| Janus [93] | 1.3B | 10.10 |\n| JanusFlow (Ours) | 1.3B | 9.51 |", "caption": "Table 4: Results of MJHQ FID-30k.\nThe models which have similar scales to our model are marked with blue background.\nJanusFlow\u00a0achieves the best FID among 1.3B models.", "description": "Table 4 presents the Fr\u00e9chet Inception Distance (FID) scores on the MJHQ FID-30k benchmark.  The FID score is a metric used to evaluate the quality of generated images, lower scores indicating better image quality.  The table compares JanusFlow's performance against other models with similar parameter counts (around 1.3 billion parameters), highlighting that JanusFlow achieves the lowest FID score among its peers, signifying superior image generation quality.", "section": "4.4 Quantitative Results"}, {"content": "Type|Model|LLM Params|POPE\u2191|MME-P\u2191|MMB<sub>dev</sub>\u2191|SEED\u2191|VQAv2<sub>test</sub>\u2191|GQA\u2191|MMMU\u2191|MM-Vet\u2191\n---|---|---|---|---|---|---|---|---|---|---\nUnd. Only|MobileVLM [12]|2.7B|84.9|1288.9|59.6|-|-|59.0|-|-\nUnd. Only|MobileVLM-V2 [13]|2.7B|84.7|1440.5|63.2|-|-|61.1|-|-\nUnd. Only|LLaVA-Phi [104]|2.7B|85.0|1335.1|59.8|-|71.4|-|28.9|-\nUnd. Only|LLaVA [57]|7B|76.3|809.6|38.7|33.5|-|-|25.5|-|-\nUnd. Only|LLaVA-v1.5 [56]|7B|85.9|1510.7|64.3|58.6|78.5|62.0|35.4|31.1\nUnd. Only|InstructBLIP [15]|7B|-|-|36.0|53.4|-|49.2|-|26.2\nUnd. Only|Qwen-VL-Chat [4]|7B|-|1487.5|60.6|58.2|78.2|57.5|-|-\nUnd. Only|IDEFICS-9B [44]|8B|-|-|48.2|-|50.9|38.4|-|-\nUnd. Only|Emu3-Chat [91]|8B|85.2|-|58.5|68.2|75.1|60.3|31.6|-\nUnd. Only|InstructBLIP [15]|13B|78.9|1212.8|-|-|-|49.5|-|25.6\n---|---|---|---|---|---|---|---|---|---|---\nLLaVA-v1.5-Phi-1.5 [96]|1.3B|84.1|1128.0|-|-|75.3|56.5|30.7|-\nMobileVLM [12]|1.4B|84.5|1196.2|53.2|-|-|56.1|-|-\nMobileVLM-V2 [13]|1.4B|84.3|1302.8|57.7|-|-|59.3|-|-\nUnified|Gemini-Nano-1 [86]|1.8B|-|-|-|-|-|62.7|-|-\nUnified|LWM [58]|7B|75.2|-|-|-|55.8|44.8|-|9.6\nUnified|VILA-U [95]|7B|85.8|1401.8|-|59.0|79.4|60.8|-|33.5\nUnified|Chameleon [85]|7B|-|-|-|-|-|-|-|22.4|8.3\nUnified|DreamLLM<sup>\u2020</sup> [19]|7B|-|-|-|-|72.9|-|-|36.6\nUnified|LaVIT<sup>\u2020</sup> [37]|7B|-|-|-|-|66.0|46.8|-|-\nUnified|Emu<sup>\u2020</sup> [84]|13B|-|-|-|-|52.0|-|-|-\nUnified|NExT-GPT<sup>\u2020</sup> [94]|13B|-|-|-|-|66.7|-|-|-\nJanus [93]|1.3B|87.0|1338.0|69.4|63.7|77.3|59.1|30.5|34.3\nJanusFlow (Ours)|1.3B|88.0|1333.1|74.9|70.5|79.8|60.3|29.3|30.9", "caption": "Table 5: Comparison with other methods on multimodal understanding benchmarks. \u201cUnd.\u201d denotes \u201cunderstanding\u201d and \u201cUnified\u201d denotes unified understanding and generation models. The models employing external pre-trained generative models are marked with \u2020. The models with LLMs which have similar number of parameters to us are marked with blue background under the line of dashes.", "description": "Table 5 presents a comparison of various multimodal understanding models' performance across several benchmark datasets.  It contrasts the performance of understanding-only models, unified (understanding and generation) models, and models that leverage externally pre-trained generative models.  The table highlights the number of parameters in each model's large language model (LLM), making it easier to compare models with similar computational complexity.  Models using LLMs with a similar parameter count to the authors' JanusFlow model are visually distinguished with a blue background.", "section": "4.4 Quantitative Results"}, {"content": "| Exp. ID | REPA | Und. Modules | Gen. Modules | Type | Train. Iter. | POPE\u2191 | VQAv2<sub>val</sub>\u2191 | GQA\u2191 | FID\u2193 | CLIP \u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|\n| A | \u00d7 | SigLIP | VAE<sup>\u2020</sup>+ConvNeXt | Unified | 50,000 | 82.40 | 69.62 | 54.43 | 19.84 | 24.94 |\n| B | \u2713 |  | Shared VAE<sup>\u2020</sup>+ConvNeXt | Unified | 50,000 | 78.13 | 53.94 | 44.04 | 18.05 | 26.38 |\n| C | \u2713 | VAE+ConvNeXt | VAE<sup>\u2020</sup>+ConvNeXt | Unified | 50,000 | 75.30 | 55.41 | 44.44 | 17.53 | 26.32 |\n| D | \u2713 | SigLIP | - | Und. Only | 13,000 | 85.03 | 69.10 | 54.23 | - | - |\n| E | \u2713 | - | VAE<sup>\u2020</sup>+ConvNeXt | Gen. Only | 37,000 | - | - | - | 16.69 | 26.89 |\n| F | \u2713 | SigLIP | VAE<sup>\u2020</sup>+ConvNeXt | Unified | 50,000 | 84.73 | 69.20 | 54.83 | 17.61 | 26.40 |", "caption": "Table 6: Ablation studies. The weights of the modules with \u2020 are frozen during training. \u201cExp.\u201d denotes \u201cexperiment\u201d. \u201cFID\u201d in this table is MJHQ FID-10k with CFG factor w=7.5\ud835\udc647.5w=7.5italic_w = 7.5 and 30 steps. \u201cCLIP\u201d denotes CLIP similarity with the backbone of CLIP-ViT-Large-Patch/14. Exp. F is the final configuration for training JanusFlow.", "description": "This ablation study analyzes the impact of different model components and training strategies on JanusFlow's performance.  It compares various configurations, including whether certain modules are frozen during training, and uses different visual encoders. The results, measured by MJHQ FID-10k (a visual quality metric) and CLIP similarity (a semantic similarity metric), demonstrate the effectiveness of key design choices like representation alignment and decoupled encoders.  The CFG (classifier-free guidance) factor is fixed at 7.5, and 30 sampling steps are used for all FID calculations.  Experiment F represents the final, optimal configuration used for JanusFlow.", "section": "3.3 Training Schemes"}, {"content": "| Model | LLM Params | POPE\u2191 | MME-P\u2191 | MMB<sub>dev</sub>\u2191 | SEED\u2191 | VQAv2<sub>test</sub>\u2191 | GQA\u2191 | MM-Vet\u2191 |\n|---|---|---|---|---|---|---|---|---|\n| JanusFlow 256 | 1.3B | 85.3 | 1203.0 | 71.9 | 67.6 | 76.3 | 58.4 | 27.4 |\n| JanusFlow 384 | 1.3B | 88.0 | 1333.1 | 74.9 | 70.5 | 79.8 | 60.3 | 30.9 |", "caption": "Table 1: Results on visual understanding tasks.", "description": "This table presents a quantitative evaluation of the JanusFlow model's performance on various visual understanding tasks.  It shows the model's scores across multiple benchmarks, comparing its capabilities to those of other state-of-the-art models in the field.  Each column represents a different benchmark, measuring aspects such as image captioning, question answering, visual reasoning, etc., reflecting the model's ability to comprehend and interact with visual information in diverse scenarios.", "section": "4.4 Quantitative Results"}, {"content": "| Method | LLM Params | Single Obj. | Two Obj. | Count. | Colors | Pos. | Color Attri. | Overall\u2191 |\n|---|---|---|---|---|---|---|---|---|\n| JanusFlow 256 | 1.3B | 0.98 | 0.73 | 0.54 | 0.83 | 0.63 | 0.53 | 0.70 |\n| JanusFlow 384 | 1.3B | 0.97 | 0.59 | 0.45 | 0.83 | 0.53 | 0.42 | 0.63 |", "caption": "Table 2: Results on GenEval\u00a0[28].", "description": "This table presents a comparison of JanusFlow's performance on the GenEval benchmark [28] against other state-of-the-art models for image generation.  GenEval assesses image generation quality across various aspects including object presence, attribute accuracy, color fidelity, counting accuracy and scene composition. The table shows the performance of different models across these subtasks and provides an overall score. It allows for a comprehensive comparison of JanusFlow's capabilities with respect to both generation-only models and unified models.", "section": "4.4 Quantitative Results"}, {"content": "| Method | Global \u2191 | Entity \u2191 | Attribute \u2191 | Relation \u2191 | Other \u2191 | Overall \u2191 | MJHQ FID-30k \u2193 |\n|---|---|---|---|---|---|---|---| \n| JanusFlow 256 | 91.20 | 88.83 | 88.00 | 87.60 | 89.53 | 81.23 | 12.70 |\n| JanusFlow 384 | 87.03 | 87.31 | 87.39 | 89.79 | 88.10 | 80.09 | 9.51 |", "caption": "Table 3: Results on DPG-Bench\u00a0[34] and MJHQ FID-30k\u00a0[48].", "description": "This table presents a quantitative comparison of JanusFlow's performance against other state-of-the-art image generation models on two key benchmarks: DPG-Bench and MJHQ FID-30k.  DPG-Bench assesses the model's ability to generate images that accurately reflect the attributes, relationships, and overall composition described in a textual prompt, while MJHQ FID-30k measures the visual fidelity of generated images by comparing them against a database of high-quality images. The table highlights JanusFlow's performance metrics on each benchmark, providing granular scores for attributes like global consistency, entity accuracy, attribute precision, and relationship accuracy, and a final overall score. This allows for a detailed assessment of JanusFlow's strengths and weaknesses in image generation compared to existing methods.", "section": "4.4 Quantitative Results"}]