[{"figure_path": "https://arxiv.org/html/2411.07975/x1.png", "caption": "(a) Benchmark Performances.", "description": "This figure presents a comparison of JanusFlow's performance against other state-of-the-art models on various benchmark datasets.  The benchmark results cover both multimodal understanding (e.g., VQA, GQA, MMBench) and image generation (e.g., MJHQ FID, GenEval).  The visualization allows for a direct comparison of JanusFlow's performance relative to specialized models and other unified multimodal models, highlighting its competitive advantage in both multimodal understanding and image generation tasks.", "section": "1. Multimodal understanding and image generation with JanusFlow"}, {"figure_path": "https://arxiv.org/html/2411.07975/x2.png", "caption": "(b) Visual Generation Results.", "description": "This figure showcases examples of images generated by the JanusFlow model.  The images demonstrate the model's ability to generate high-quality images with a resolution of 384 x 384 pixels, based on textual descriptions or prompts. The variety of images presented highlights JanusFlow's diverse capabilities in generating different styles, objects, and scenes.", "section": "3. JanusFlow"}, {"figure_path": "https://arxiv.org/html/2411.07975/x3.png", "caption": "Figure 1: \nMultimodal understanding and image generation with JanusFlow. JanusFlow\u00a0surpasses the state-of-the-art unified multimodal models and several task-specific understanding models on visual understanding benchmarks. It is also capable of generating high-quality images. The resolution of the images is 384\u00d7384384384384\\times 384384 \u00d7 384.", "description": "JanusFlow, a novel multimodal model, significantly outperforms existing unified models and several task-specific models in visual understanding benchmarks while producing high-quality images (384x384 resolution).  The figure showcases both quantitative benchmark results and qualitative examples of generated images, demonstrating the model's capabilities in both understanding and generation tasks.", "section": "3. JanusFlow"}, {"figure_path": "https://arxiv.org/html/2411.07975/x4.png", "caption": "Figure 2: \nArchitecture of the proposed JanusFlow. For visual understanding, the LLM performs autoregressive next-token prediction to generate responses. For image generation, the LLM employs images with rectified flow. Starting from Gaussian noise at t=0\ud835\udc610t=0italic_t = 0, the LLM iteratively updates ztsubscript\ud835\udc67\ud835\udc61z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT by predicting velocity vectors until reaching t=1\ud835\udc611t=1italic_t = 1. We omit the VAE encoder, the skip connection leveraged in generation and the linear layer after fe\u2062n\u2062csubscript\ud835\udc53\ud835\udc52\ud835\udc5b\ud835\udc50f_{enc}italic_f start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT for simplicity.", "description": "JanusFlow uses a Large Language Model (LLM) for both visual understanding and image generation.  In the visual understanding task (left panel), an understanding encoder processes the image and the text prompt, creating an input sequence for the LLM. The LLM then uses autoregressive prediction to generate a textual response. In the image generation task (right panel), a generation encoder processes a text prompt and Gaussian noise. The LLM iteratively updates the noise using rectified flow, predicting velocity vectors at each step until a complete image is generated in the latent space.  A decoder then transforms this latent representation into a final image. The diagram simplifies the architecture by omitting details such as the VAE encoder and skip connections for clarity.", "section": "3. JanusFlow"}, {"figure_path": "https://arxiv.org/html/2411.07975/x5.png", "caption": "Figure 3: Three training stages of JanusFlow.\nThe trainable modules are marked with flame and the frozen modules are marked with snowflakes.", "description": "This figure illustrates the three-stage training process of the JanusFlow model. Stage 1 focuses on adapting newly initialized components (generation encoder and decoder) to work effectively with the pre-trained LLM and SigLIP encoder.  Stage 2 involves unified pre-training of the entire model (except the visual encoder), using multimodal understanding, image generation, and text-only data.  Finally, Stage 3 performs supervised fine-tuning using instruction tuning data to enhance the model's ability to respond to user instructions for both multimodal understanding and image generation tasks.  Trainable modules are highlighted with flames, while frozen modules are shown with snowflakes.", "section": "3. JanusFlow"}, {"figure_path": "https://arxiv.org/html/2411.07975/x6.png", "caption": "Figure 4: Image generation results of JanusFlow.\nOur model can generate high-quality images that are semantically consistent with text prompts.", "description": "JanusFlow generates high-quality, semantically consistent images from text prompts.  The figure displays several example images generated by the model, showcasing its ability to accurately interpret and visualize a range of descriptive text inputs. The images demonstrate both the visual quality and semantic accuracy of the model's image generation capabilities.", "section": "3.2 A Unified Framework for Multimodal Understanding and Generation"}, {"figure_path": "https://arxiv.org/html/2411.07975/x7.png", "caption": "Figure 5: Visual Understanding with JanusFlow. Our model effectively handles various visual understanding tasks, such as question answering, plot interpretation and object counting.", "description": "Figure 5 presents qualitative examples showcasing JanusFlow's capabilities in visual understanding tasks.  The examples demonstrate successful question answering, plot interpretation, and object counting. The figure visually shows how the model interacts with images and provides textual responses, illustrating its ability to process various forms of visual content and reason about them in natural language.", "section": "4.6. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2411.07975/x8.png", "caption": "(a) Results of varying CFG Factors", "description": "This figure shows the impact of varying classifier-free guidance (CFG) factors on the Fr\u00e9chet Inception Distance (FID) and CLIP similarity scores during image generation.  The number of sampling steps was held constant at 30.  The x-axis represents the CFG factor, and the y-axis shows the FID score (lower is better) and CLIP similarity (higher is better). The plot illustrates the optimal CFG factor for achieving a balance between visual quality and semantic alignment.", "section": "4.4 Quantitative Results"}, {"figure_path": "https://arxiv.org/html/2411.07975/x9.png", "caption": "(b) Results of Varying Numbers of Sampling Steps", "description": "This figure shows the impact of varying the number of sampling steps on the model's performance, specifically measuring the Fr\u00e9chet Inception Distance (FID) and CLIP similarity scores.  The CFG factor is held constant at a value of 2. The x-axis represents the number of sampling steps, while the y-axis displays both the FID and CLIP similarity scores.  The plot illustrates how the choice of the number of sampling steps affects the trade-off between generation quality and computational efficiency.", "section": "4.5 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.07975/x10.png", "caption": "Figure 1: Results of varying CFG factors and numbers of sampling steps. In Fig.\u00a0(a), the number of sampling steps is set to 30. In Fig.\u00a0(b), the CFG factor is set to 2.", "description": "This figure shows the impact of varying classifier-free guidance (CFG) factors and the number of sampling steps on the quality of generated images, measured by FID and CLIP similarity scores.  The left subplot (a) shows the FID and CLIP similarity scores obtained by varying the CFG factor while keeping the number of sampling steps constant at 30. The right subplot (b) shows the FID and CLIP similarity scores obtained by varying the number of sampling steps while keeping the CFG factor constant at 2. The plots illustrate how different values for these hyperparameters affect the trade-off between image quality and computational cost.", "section": "4.4 Quantitative Results"}, {"figure_path": "https://arxiv.org/html/2411.07975/x11.png", "caption": "Figure 2: More multimodal understanding cases.", "description": "This figure showcases additional examples of JanusFlow's multimodal understanding capabilities.  It demonstrates the model's ability to perform various tasks, such as generating Python code for a bar chart based on a visual input, interpreting the humor in an image of a dog depicted as the Mona Lisa, identifying a person in an image (George W. Bush), and summarizing a text passage.  These examples highlight the model's versatility and its capacity to effectively process both visual and textual information, enabling it to perform a range of complex understanding tasks.", "section": "3.2. A Unified Framework for Multimodal Understanding and Generation"}]