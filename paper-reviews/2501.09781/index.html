<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>VideoWorld: Exploring Knowledge Learning from Unlabeled Videos &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="VideoWorld: Exploring Knowledge Learning from Unlabeled Videos &#183; HF Daily Paper Reviews by AI"><meta name=description content="VideoWorld shows AI can learn complex reasoning and planning skills from unlabeled videos alone, achieving professional-level performance in Go and robotics."><meta name=keywords content="Computer Vision,Video Understanding,üè¢ ByteDance Seed,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09781/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09781/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="VideoWorld: Exploring Knowledge Learning from Unlabeled Videos"><meta property="og:description" content="VideoWorld shows AI can learn complex reasoning and planning skills from unlabeled videos alone, achieving professional-level performance in Go and robotics."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-01-16T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-16T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Video Understanding"><meta property="article:tag" content="üè¢ ByteDance Seed"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09781/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09781/cover.png"><meta name=twitter:title content="VideoWorld: Exploring Knowledge Learning from Unlabeled Videos"><meta name=twitter:description content="VideoWorld shows AI can learn complex reasoning and planning skills from unlabeled videos alone, achieving professional-level performance in Go and robotics."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"VideoWorld: Exploring Knowledge Learning from Unlabeled Videos","headline":"VideoWorld: Exploring Knowledge Learning from Unlabeled Videos","abstract":"VideoWorld shows AI can learn complex reasoning and planning skills from unlabeled videos alone, achieving professional-level performance in Go and robotics.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2501.09781\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-01-16T00:00:00\u002b00:00","datePublished":"2025-01-16T00:00:00\u002b00:00","dateModified":"2025-01-16T00:00:00\u002b00:00","keywords":["Computer Vision","Video Understanding","üè¢ ByteDance Seed"],"mainEntityOfPage":"true","wordCount":"3696"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-26/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-26</p></a><a href=/ai-paper-reviewer/2025-03-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-27</p></a><a href=/ai-paper-reviewer/2025-03-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-28</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-26/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-26</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-27</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-28</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2501.09781/cover_hu9331976065950807958.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2501.09781/>VideoWorld: Exploring Knowledge Learning from Unlabeled Videos</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">VideoWorld: Exploring Knowledge Learning from Unlabeled Videos</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-01-16T00:00:00+00:00>16 January 2025</time><span class="px-2 text-primary-500">&#183;</span><span>3696 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">18 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2501.09781/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2501.09781/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/video-understanding/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Video Understanding
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-bytedance-seed/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ ByteDance Seed</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#visual-knowledge>Visual Knowledge</a></li><li><a href=#ldms-role>LDM&rsquo;s Role</a></li><li><a href=#videoworlds-power>VideoWorld&rsquo;s Power</a></li><li><a href=#benchmarking>Benchmarking</a></li><li><a href=#future-works>Future Works</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#visual-knowledge>Visual Knowledge</a></li><li><a href=#ldms-role>LDM&rsquo;s Role</a></li><li><a href=#videoworlds-power>VideoWorld&rsquo;s Power</a></li><li><a href=#benchmarking>Benchmarking</a></li><li><a href=#future-works>Future Works</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2501.09781</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Zhongwei Ren et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-01-21</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2501.09781 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2501.09781 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2501.09781/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current AI research heavily relies on text-based models for knowledge learning, neglecting the vast amount of knowledge present in visual data, especially videos. This paper explores an alternative approach by training a deep generative model, called VideoWorld, solely on unlabeled video data. The model&rsquo;s ability to learn complex concepts such as rules and reasoning is evaluated through video-based Go and robotic control tasks. The existing methods struggles to learn from pure visual data and usually relies on text or labels or need to use reinforcement learning with rewards which are not always available.</p><p>VideoWorld uses a novel latent dynamics model to efficiently represent visual changes which enhances its knowledge acquisition ability. The results demonstrate that video-only training is sufficient for learning knowledge, including rules, reasoning, and planning. VideoWorld achieves remarkable success, reaching a professional level in Go and exhibiting strong generalization in robotic control. The model&rsquo;s code and data are open-sourced, furthering the research and development of knowledge acquisition from purely visual data.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-320eb0d2ce9c1c967d21bf9ad0519a10></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-320eb0d2ce9c1c967d21bf9ad0519a10",{strings:[" AI models can learn sophisticated knowledge from only visual data, bypassing the need for text or labels. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-2a6f9765db20d7061ac3180aa2665924></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-2a6f9765db20d7061ac3180aa2665924",{strings:[" Representing visual changes efficiently is crucial for effective knowledge learning in video-based AI. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-53d71292463ccd72b00f3edab9f62095></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-53d71292463ccd72b00f3edab9f62095",{strings:[" VideoWorld, a novel video generation model, achieves state-of-the-art performance in video-based Go and robotic control tasks. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is important because it challenges the prevailing paradigm of text-based knowledge learning by demonstrating that <strong>deep generative models can acquire complex knowledge solely from visual input</strong>. This opens up new avenues for AI research, particularly in robotics and areas where visual information is primary. The development of VideoWorld and its open-sourcing facilitates further research and advancements in this exciting field.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.09781/extracted/6136615/figures/figure1_showv7.png alt></figure></p><blockquote><p>üîº The figure illustrates VideoWorld, a novel approach to learning knowledge directly from unlabeled video data. It contrasts VideoWorld with traditional methods like reinforcement learning (RL), supervised learning (SL), and text-based learning. The core idea is that VideoWorld learns complex knowledge, including task-specific rules, reasoning, and planning abilities, solely by observing videos. The figure highlights three key advantages of VideoWorld: First, its unified visual representation enables better generalization across various tasks and interfaces compared to RL and SL. Second, it significantly reduces the need for manual annotation, a significant advantage over SL and text-based methods. Third, learning directly from video data allows VideoWorld to acquire richer real-world information than methods relying solely on text.</p><details><summary>read the caption</summary>Figure 1: VideoWorld explores learning knowledge from unlabeled videos, ranging from task-specific rules to high-level reasoning and planning capabilities. Compared to other learning methods: reinforcement learning (RL), supervised learning (SL) and text-based learning, it offers three advantages: 1) better generalization with unified visual representation for various tasks and interfaces, 2) lower manual annotation burden, and 3) learning richer real-world information than text description.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T1.7><tr class=ltx_tr id=S5.T1.7.8><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id=S5.T1.7.8.1><span class="ltx_inline-block ltx_align_top" id=S5.T1.7.8.1.1><span class=ltx_p id=S5.T1.7.8.1.1.1 style=width:2.8pt><span class=ltx_text id=S5.T1.7.8.1.1.1.1 style=font-size:80%>Idx</span></span></span></td><td class="ltx_td ltx_align_left ltx_border_tt" id=S5.T1.7.8.2><span class=ltx_text id=S5.T1.7.8.2.1 style=font-size:80%>Agent</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T1.7.8.3><span class=ltx_text id=S5.T1.7.8.3.1 style=font-size:80%>Train</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T1.7.8.4><span class=ltx_text id=S5.T1.7.8.4.1 style=font-size:80%>w/o Search</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T1.7.8.5><span class=ltx_text id=S5.T1.7.8.5.1 style=font-size:80%>Input</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T1.7.8.6><span class=ltx_text id=S5.T1.7.8.6.1 style=font-size:80%>Legal rate (%)</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T1.7.8.7><span class=ltx_text id=S5.T1.7.8.7.1 style=font-size:80%>Action-Value (%)</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T1.7.8.8><span class=ltx_text id=S5.T1.7.8.8.1 style=font-size:80%>Best Action Acc. (%)</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T1.7.8.9><span class=ltx_text id=S5.T1.7.8.9.1 style=font-size:80%>Tournament Elo</span></td></tr><tr class=ltx_tr id=S5.T1.1.1><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=S5.T1.1.1.2><span class="ltx_inline-block ltx_align_top" id=S5.T1.1.1.2.1><span class=ltx_p id=S5.T1.1.1.2.1.1 style=width:2.8pt><span class=ltx_text id=S5.T1.1.1.2.1.1.1 style=font-size:80%>1</span></span></span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T1.1.1.3><span class=ltx_text id=S5.T1.1.1.3.1 style=font-size:80%>KataGO-human-1d</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.1.1.4><span class=ltx_text id=S5.T1.1.1.4.1 style=font-size:80%>RL</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.1.1.5><span class=ltx_text id=S5.T1.1.1.5.1 style=font-size:80%>‚úó</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.1.1.6><span class=ltx_text id=S5.T1.1.1.6.1 style=font-size:80%>State</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.1.1.7><span class=ltx_text id=S5.T1.1.1.7.1 style=font-size:80%>100</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.1.1.8><span class=ltx_text id=S5.T1.1.1.8.1 style=font-size:80%>67.6</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.1.1.9><span class=ltx_text id=S5.T1.1.1.9.1 style=font-size:80%>64.5</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.1.1.1><span class=ltx_text id=S5.T1.1.1.1.1 style=font-size:80%>2019</span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.m1.1.1" mathsize="50%" xref="S5.T1.1.1.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.m1.1d">¬±</annotation></semantics></math><span class=ltx_text id=S5.T1.1.1.1.2 style=font-size:50%>23</span></td></tr><tr class=ltx_tr id=S5.T1.2.2><td class="ltx_td ltx_align_justify ltx_align_top" id=S5.T1.2.2.2><span class="ltx_inline-block ltx_align_top" id=S5.T1.2.2.2.1><span class=ltx_p id=S5.T1.2.2.2.1.1 style=width:2.8pt><span class=ltx_text id=S5.T1.2.2.2.1.1.1 style=font-size:80%>2</span></span></span></td><td class="ltx_td ltx_align_left" id=S5.T1.2.2.3><span class=ltx_text id=S5.T1.2.2.3.1 style=font-size:80%>KataGO-human-5d</span></td><td class="ltx_td ltx_align_center" id=S5.T1.2.2.4><span class=ltx_text id=S5.T1.2.2.4.1 style=font-size:80%>RL</span></td><td class="ltx_td ltx_align_center" id=S5.T1.2.2.5><span class=ltx_text id=S5.T1.2.2.5.1 style=font-size:80%>‚úó</span></td><td class="ltx_td ltx_align_center" id=S5.T1.2.2.6><span class=ltx_text id=S5.T1.2.2.6.1 style=font-size:80%>State</span></td><td class="ltx_td ltx_align_center" id=S5.T1.2.2.7><span class=ltx_text id=S5.T1.2.2.7.1 style=font-size:80%>100</span></td><td class="ltx_td ltx_align_center" id=S5.T1.2.2.8><span class=ltx_text id=S5.T1.2.2.8.1 style=font-size:80%>83.5</span></td><td class="ltx_td ltx_align_center" id=S5.T1.2.2.9><span class=ltx_text id=S5.T1.2.2.9.1 style=font-size:80%>83.7</span></td><td class="ltx_td ltx_align_center" id=S5.T1.2.2.1><span class=ltx_text id=S5.T1.2.2.1.1 style=font-size:80%>2253</span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.2.2.1.m1.1"><semantics id="S5.T1.2.2.1.m1.1a"><mo id="S5.T1.2.2.1.m1.1.1" mathsize="50%" xref="S5.T1.2.2.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.1.m1.1b"><csymbol cd="latexml" id="S5.T1.2.2.1.m1.1.1.cmml" xref="S5.T1.2.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.1.m1.1d">¬±</annotation></semantics></math><span class=ltx_text id=S5.T1.2.2.1.2 style=font-size:50%>20</span></td></tr><tr class=ltx_tr id=S5.T1.7.9><td class="ltx_td ltx_align_justify ltx_align_top" id=S5.T1.7.9.1><span class="ltx_inline-block ltx_align_top" id=S5.T1.7.9.1.1><span class=ltx_p id=S5.T1.7.9.1.1.1 style=width:2.8pt><span class=ltx_text id=S5.T1.7.9.1.1.1.1 style=font-size:80%>3</span></span></span></td><td class="ltx_td ltx_align_left" id=S5.T1.7.9.2><span class=ltx_text id=S5.T1.7.9.2.1 style=font-size:80%;color:#bfbfbf>KataGO-human-9d (Oracle)</span></td><td class="ltx_td ltx_align_center" id=S5.T1.7.9.3><span class=ltx_text id=S5.T1.7.9.3.1 style=font-size:80%;color:#bfbfbf>RL</span></td><td class="ltx_td ltx_align_center" id=S5.T1.7.9.4><span class=ltx_text id=S5.T1.7.9.4.1 style=font-size:80%;color:#bfbfbf>‚úó</span></td><td class="ltx_td ltx_align_center" id=S5.T1.7.9.5><span class=ltx_text id=S5.T1.7.9.5.1 style=font-size:80%;color:#bfbfbf>State</span></td><td class="ltx_td ltx_align_center" id=S5.T1.7.9.6><span class=ltx_text id=S5.T1.7.9.6.1 style=font-size:80%;color:#bfbfbf>100</span></td><td class="ltx_td ltx_align_center" id=S5.T1.7.9.7><span class=ltx_text id=S5.T1.7.9.7.1 style=font-size:80%;color:#bfbfbf>100</span></td><td class="ltx_td ltx_align_center" id=S5.T1.7.9.8><span class=ltx_text id=S5.T1.7.9.8.1 style=font-size:80%;color:#bfbfbf>100</span></td><td class="ltx_td ltx_align_center" id=S5.T1.7.9.9><span class=ltx_text id=S5.T1.7.9.9.1 style=font-size:80%;color:#bfbfbf>2700</span></td></tr><tr class=ltx_tr id=S5.T1.3.3><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=S5.T1.3.3.2><span class="ltx_inline-block ltx_align_top" id=S5.T1.3.3.2.1><span class=ltx_p id=S5.T1.3.3.2.1.1 style=width:2.8pt><span class=ltx_text id=S5.T1.3.3.2.1.1.1 style=font-size:80%>4</span></span></span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T1.3.3.3><span class=ltx_text id=S5.T1.3.3.3.1 style=font-size:80%>Transformer 300M</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.3.3.4><span class=ltx_text id=S5.T1.3.3.4.1 style=font-size:80%>SL</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.3.3.5><span class=ltx_text id=S5.T1.3.3.5.1 style=font-size:80%>‚úì</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.3.3.6><span class=ltx_text id=S5.T1.3.3.6.1 style=font-size:80%>State</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.3.3.7><span class=ltx_text id=S5.T1.3.3.7.1 style=font-size:80%>99.8</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.3.3.8><span class=ltx_text id=S5.T1.3.3.8.1 style=font-size:80%>79.7</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.3.3.9><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T1.3.3.9.1 style=font-size:80%>87.2</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.3.3.1><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T1.3.3.1.1 style=font-size:80%>2308</span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.3.3.1.m1.1"><semantics id="S5.T1.3.3.1.m1.1a"><mo id="S5.T1.3.3.1.m1.1.1" mathsize="50%" xref="S5.T1.3.3.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.1.m1.1b"><csymbol cd="latexml" id="S5.T1.3.3.1.m1.1.1.cmml" xref="S5.T1.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.1.m1.1d">¬±</annotation></semantics></math><span class=ltx_text id=S5.T1.3.3.1.2 style=font-size:50%>21</span></td></tr><tr class=ltx_tr id=S5.T1.4.4><td class="ltx_td ltx_align_justify ltx_align_top" id=S5.T1.4.4.2><span class="ltx_inline-block ltx_align_top" id=S5.T1.4.4.2.1><span class=ltx_p id=S5.T1.4.4.2.1.1 style=width:2.8pt><span class=ltx_text id=S5.T1.4.4.2.1.1.1 style=font-size:80%>5</span></span></span></td><td class="ltx_td ltx_align_left" id=S5.T1.4.4.3><span class=ltx_text id=S5.T1.4.4.3.1 style=font-size:80%>Transformer 300M</span></td><td class="ltx_td ltx_align_center" id=S5.T1.4.4.4><span class=ltx_text id=S5.T1.4.4.4.1 style=font-size:80%>SL</span></td><td class="ltx_td ltx_align_center" id=S5.T1.4.4.5><span class=ltx_text id=S5.T1.4.4.5.1 style=font-size:80%>‚úì</span></td><td class="ltx_td ltx_align_center" id=S5.T1.4.4.6><span class=ltx_text id=S5.T1.4.4.6.1 style=font-size:80%>Video</span></td><td class="ltx_td ltx_align_center" id=S5.T1.4.4.7><span class=ltx_text id=S5.T1.4.4.7.1 style=font-size:80%>99.6</span></td><td class="ltx_td ltx_align_center" id=S5.T1.4.4.8><span class=ltx_text id=S5.T1.4.4.8.1 style=font-size:80%>59.7</span></td><td class="ltx_td ltx_align_center" id=S5.T1.4.4.9><span class=ltx_text id=S5.T1.4.4.9.1 style=font-size:80%>58.9</span></td><td class="ltx_td ltx_align_center" id=S5.T1.4.4.1><span class=ltx_text id=S5.T1.4.4.1.1 style=font-size:80%>1998</span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.4.4.1.m1.1"><semantics id="S5.T1.4.4.1.m1.1a"><mo id="S5.T1.4.4.1.m1.1.1" mathsize="50%" xref="S5.T1.4.4.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.1.m1.1b"><csymbol cd="latexml" id="S5.T1.4.4.1.m1.1.1.cmml" xref="S5.T1.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.4.1.m1.1d">¬±</annotation></semantics></math><span class=ltx_text id=S5.T1.4.4.1.2 style=font-size:50%>38</span></td></tr><tr class=ltx_tr id=S5.T1.5.5><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id=S5.T1.5.5.2><span class="ltx_inline-block ltx_align_top" id=S5.T1.5.5.2.1><span class=ltx_p id=S5.T1.5.5.2.1.1 style=width:2.8pt><span class=ltx_text id=S5.T1.5.5.2.1.1.1 style=font-size:80%>6</span></span></span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T1.5.5.3><span class=ltx_text id=S5.T1.5.5.3.1 style=font-size:80%>VideoWorld 50M (ours)</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.5.5.4><span class=ltx_text id=S5.T1.5.5.4.1 style=font-size:80%>SL</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.5.5.5><span class=ltx_text id=S5.T1.5.5.5.1 style=font-size:80%>‚úì</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.5.5.6><span class=ltx_text id=S5.T1.5.5.6.1 style=font-size:80%>Video</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.5.5.7><span class=ltx_text id=S5.T1.5.5.7.1 style=font-size:80%>99.5</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.5.5.8><span class=ltx_text id=S5.T1.5.5.8.1 style=font-size:80%>73.9</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.5.5.9><span class=ltx_text id=S5.T1.5.5.9.1 style=font-size:80%>80.9</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.5.5.1><span class=ltx_text id=S5.T1.5.5.1.1 style=font-size:80%>2093</span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.5.5.1.m1.1"><semantics id="S5.T1.5.5.1.m1.1a"><mo id="S5.T1.5.5.1.m1.1.1" mathsize="50%" xref="S5.T1.5.5.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.1.m1.1b"><csymbol cd="latexml" id="S5.T1.5.5.1.m1.1.1.cmml" xref="S5.T1.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.5.5.1.m1.1d">¬±</annotation></semantics></math><span class=ltx_text id=S5.T1.5.5.1.2 style=font-size:50%>25</span></td></tr><tr class=ltx_tr id=S5.T1.6.6><td class="ltx_td ltx_align_justify ltx_align_top" id=S5.T1.6.6.2><span class="ltx_inline-block ltx_align_top" id=S5.T1.6.6.2.1><span class=ltx_p id=S5.T1.6.6.2.1.1 style=width:2.8pt><span class=ltx_text id=S5.T1.6.6.2.1.1.1 style=font-size:80%>7</span></span></span></td><td class="ltx_td ltx_align_left" id=S5.T1.6.6.3><span class=ltx_text id=S5.T1.6.6.3.1 style=font-size:80%>VideoWorld 150M (ours)</span></td><td class="ltx_td ltx_align_center" id=S5.T1.6.6.4><span class=ltx_text id=S5.T1.6.6.4.1 style=font-size:80%>SL</span></td><td class="ltx_td ltx_align_center" id=S5.T1.6.6.5><span class=ltx_text id=S5.T1.6.6.5.1 style=font-size:80%>‚úì</span></td><td class="ltx_td ltx_align_center" id=S5.T1.6.6.6><span class=ltx_text id=S5.T1.6.6.6.1 style=font-size:80%>Video</span></td><td class="ltx_td ltx_align_center" id=S5.T1.6.6.7><span class="ltx_text ltx_font_bold" id=S5.T1.6.6.7.1 style=font-size:80%>99.7</span></td><td class="ltx_td ltx_align_center" id=S5.T1.6.6.8><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.T1.6.6.8.1 style=font-size:80%>82.0</span></td><td class="ltx_td ltx_align_center" id=S5.T1.6.6.9><span class=ltx_text id=S5.T1.6.6.9.1 style=font-size:80%>86.7</span></td><td class="ltx_td ltx_align_center" id=S5.T1.6.6.1><span class=ltx_text id=S5.T1.6.6.1.1 style=font-size:80%>2218</span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.6.6.1.m1.1"><semantics id="S5.T1.6.6.1.m1.1a"><mo id="S5.T1.6.6.1.m1.1.1" mathsize="50%" xref="S5.T1.6.6.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.1.m1.1b"><csymbol cd="latexml" id="S5.T1.6.6.1.m1.1.1.cmml" xref="S5.T1.6.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.6.6.1.m1.1d">¬±</annotation></semantics></math><span class=ltx_text id=S5.T1.6.6.1.2 style=font-size:50%>23</span></td></tr><tr class=ltx_tr id=S5.T1.7.7><td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id=S5.T1.7.7.2><span class="ltx_inline-block ltx_align_top" id=S5.T1.7.7.2.1><span class=ltx_p id=S5.T1.7.7.2.1.1 style=width:2.8pt><span class=ltx_text id=S5.T1.7.7.2.1.1.1 style=font-size:80%>8</span></span></span></td><td class="ltx_td ltx_align_left ltx_border_bb" id=S5.T1.7.7.3><span class=ltx_text id=S5.T1.7.7.3.1 style=font-size:80%>VideoWorld 300M (ours)</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T1.7.7.4><span class=ltx_text id=S5.T1.7.7.4.1 style=font-size:80%>SL</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T1.7.7.5><span class=ltx_text id=S5.T1.7.7.5.1 style=font-size:80%>‚úì</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T1.7.7.6><span class=ltx_text id=S5.T1.7.7.6.1 style=font-size:80%>Video</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T1.7.7.7><span class="ltx_text ltx_font_bold" id=S5.T1.7.7.7.1 style=font-size:80%>99.7</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T1.7.7.8><span class="ltx_text ltx_font_bold" id=S5.T1.7.7.8.1 style=font-size:80%>83.7</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T1.7.7.9><span class="ltx_text ltx_font_bold" id=S5.T1.7.7.9.1 style=font-size:80%>88.1</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T1.7.7.1><span class="ltx_text ltx_font_bold" id=S5.T1.7.7.1.1 style=font-size:80%>2317<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.7.7.1.1.m1.1"><semantics id="S5.T1.7.7.1.1.m1.1a"><mo id="S5.T1.7.7.1.1.m1.1.1" mathsize="63%" xref="S5.T1.7.7.1.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.7.7.1.1.m1.1.1.cmml" xref="S5.T1.7.7.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.7.7.1.1.m1.1d">¬±</annotation></semantics></math><span class=ltx_text id=S5.T1.7.7.1.1.1 style=font-size:63%>25</span></span></td></tr></table></table></figure><blockquote><p>üîº This table compares the performance of different models on the Video-GoBench benchmark, a purely visual Go game environment. The models are evaluated against three versions of the KataGo engine (trained to match human skill levels of 1-dan, 5-dan, and 9-dan) and against a model trained using only the state information (not video). The performance metrics reported are the legal move rate (percentage of legal moves made), action-value (average score for moves made, using KataGo-9d as a reference), best action accuracy (percentage of moves matching KataGo-9d&rsquo;s best choice), and tournament Elo rating (relative skill level based on pairwise comparisons). The KataGo-human-9d model serves as an oracle, representing the highest level of human performance.</p><details><summary>read the caption</summary>Table 1: Comparison on Video-GoBench. KataGO-human-9d represents the highest human level and serve as the Oracle for best actions.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Visual Knowledge<div id=visual-knowledge class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-knowledge aria-label=Anchor>#</a></span></h4><p>The concept of &ldquo;Visual Knowledge&rdquo; in AI research is fascinating and complex. It explores how AI systems can acquire and utilize knowledge directly from visual data, <strong>without relying on textual descriptions or pre-defined labels</strong>. This approach is crucial because a significant portion of real-world information exists in visual form, inaccessible to text-based models. <strong>VideoWorld&rsquo;s innovative methodology</strong> addresses this by training a generative model on unlabeled videos, enabling it to learn rules, reasoning, and even planning capabilities. This demonstrates the richness of visual information for knowledge acquisition and the potential to overcome limitations of text-dependent AI. <strong>A critical factor</strong> identified is the effective representation of visual change over time, as simply processing raw video frames is insufficient for efficient learning. The model&rsquo;s successful performance in complex tasks like Go and robotic control underlines the power of this paradigm and the immense potential of directly leveraging the vast, unlabeled repository of visual data available. Further research into efficient visual representation methods and scaling up this approach will likely unlock further breakthroughs in AI&rsquo;s capacity for visual learning and understanding.</p><h4 class="relative group">LDM&rsquo;s Role<div id=ldms-role class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#ldms-role aria-label=Anchor>#</a></span></h4><p>The Latent Dynamics Model (LDM) plays a crucial role in VideoWorld by <strong>efficiently representing the temporal dynamics of visual changes</strong> in videos. Unlike simply encoding raw video frames, which can be inefficient and lead to redundant information, the LDM compresses multi-step visual changes into compact latent codes. This not only enhances learning efficiency but also allows the model to better capture and reason about complex visual information crucial for tasks involving multi-step planning and decision-making, such as Go and robotic manipulation. The LDM&rsquo;s design and implementation showcase the significance of focusing on representing relevant visual changes instead of redundant visual data for effective knowledge learning. The results clearly demonstrate the <strong>significant performance improvement</strong> achieved by incorporating the LDM, highlighting its importance in enabling VideoWorld to reach advanced levels in Go and robotic tasks. <strong>Integrating the LDM with an auto-regressive transformer further enhances the model&rsquo;s ability to learn complex knowledge</strong> solely from visual data. The LDM&rsquo;s impact on VideoWorld&rsquo;s success underscores the importance of efficient and effective visual representation for knowledge acquisition in AI.</p><h4 class="relative group">VideoWorld&rsquo;s Power<div id=videoworlds-power class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#videoworlds-power aria-label=Anchor>#</a></span></h4><p><strong>VideoWorld demonstrates remarkable capabilities in learning complex knowledge solely from unlabeled video data.</strong> Unlike text-based models, it leverages the next-token prediction paradigm on raw video, acquiring rules, reasoning, and planning abilities. Its success in Go, achieving a 5-dan professional level, showcases its ability to understand intricate rules and strategies. Further, its performance on robotic control tasks, approaching that of oracle models in CALVIN and RLBench, reveals strong generalization across environments. <strong>VideoWorld&rsquo;s strength lies in its ability to learn from purely visual information, opening new avenues for AI that surpasses traditional methods relying on explicit labels or rewards.</strong> The Latent Dynamics Model (LDM) significantly boosts its efficiency by compactly representing visual changes, making it more adept at handling long-term, complex tasks. <strong>This innovative approach offers a promising path towards creating truly general AI agents capable of learning sophisticated knowledge through observation, mirroring the learning processes of biological organisms.</strong></p><h4 class="relative group">Benchmarking<div id=benchmarking class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#benchmarking aria-label=Anchor>#</a></span></h4><p>The benchmarking section of a research paper is crucial for evaluating the performance of a proposed model or algorithm. A robust benchmarking strategy should involve multiple, relevant datasets to demonstrate generalizability beyond specific test cases. It&rsquo;s essential to compare against established state-of-the-art baselines and to employ a variety of metrics that capture different aspects of performance. <strong>Quantitative metrics</strong> are vital, such as accuracy, precision, and recall, alongside qualitative analyses that offer a deeper understanding. The paper should transparently explain experimental setups, ensuring reproducibility. Furthermore, <strong>a thorough discussion of results</strong> is key, highlighting both strengths and limitations of the model in relation to benchmarks. This includes analyzing scenarios where the model excels or underperforms, offering insights into its capabilities and potential for future improvement. <strong>Clearly articulated conclusions</strong> summarizing the benchmarking findings and their implications for the broader research area are essential.</p><h4 class="relative group">Future Works<div id=future-works class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-works aria-label=Anchor>#</a></span></h4><p>Future research directions stemming from this work on knowledge learning from unlabeled videos could <strong>explore more complex and diverse tasks</strong>. The current benchmarks (Go and robotics) provide valuable insights, but expanding to other domains, such as visual reasoning in natural scenes or multi-agent interactions, would demonstrate broader capabilities. <strong>Improving the visual representation and incorporating temporal dynamics</strong> is also crucial. While the Latent Dynamics Model (LDM) shows promise, further refinement and exploration of different architectural choices could lead to even more efficient and effective knowledge acquisition. It would be beneficial to conduct <strong>extensive scalability studies</strong> using larger video datasets and more powerful computing resources to better understand the model&rsquo;s limitations and potential. Furthermore, investigating <strong>methods for interpretability</strong> is important; understanding how the model reasons and makes decisions from visual input alone is key to building trust and facilitating further development. Finally, it will be imperative to <strong>explore the implications</strong> of knowledge acquisition from purely visual data within ethical and societal considerations. This includes researching bias detection and mitigation techniques to ensure the fairness and robustness of future models.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.09781/extracted/6136615/figures/rep_space_v3.png alt></figure></p><blockquote><p>üîº The figure compares three different approaches to predicting the next move in a game of Go: using only the game state, using raw video, and using video enhanced with latent codes to represent future visual changes. The graph shows that using video with latent dynamics improves learning efficiency. The &lsquo;Action-Value&rsquo; metric represents the quality of each move prediction.</p><details><summary>read the caption</summary>Figure 2: Comparison of prediction targets. ‚ÄúState‚Äù, ‚ÄúVideo‚Äù and ‚ÄúVideo w/ LDM‚Äù refer to three different prediction targets: a state sequence (e.g., labeled positions of moves in Go), a raw video sequence, and a video sequence augmented with latent codes representing future visual changes (this approach is adopted by VideoWorld). ‚ÄúAction-Value‚Äù denotes the score for each move in the game, with details provided in Sec.¬†4.2. By combining rich video information with a compact representation of visual changes, VideoWorld enables more effective learning.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.09781/extracted/6136615/figures/overview_v6.png alt></figure></p><blockquote><p>üîº Figure 3 illustrates the architecture of VideoWorld, a novel video generation model designed for knowledge learning from unlabeled videos. The left panel shows the overall architecture, which comprises a VQ-VAE (Vector Quantized Variational Autoencoder) for encoding video frames into discrete tokens, and an autoregressive transformer for predicting the next token (or next frame) based on the previous tokens. The right panel focuses on the Latent Dynamics Model (LDM), a key component of VideoWorld. The LDM efficiently handles long-range dependencies in video sequences by first compressing the visual changes from each frame to its subsequent H frames into a set of latent codes and then seamlessly integrating these codes with the next token prediction paradigm of the autoregressive transformer. This two-stage process enhances both efficiency and effectiveness of the video generation and knowledge acquisition in VideoWorld.</p><details><summary>read the caption</summary>Figure 3: Overview of the proposed VideoWorld model architecture. (Left) Overall architecture. (Right) The proposed latent dynamics model (LDM). First, LDM compresses the visual changes from each frame to its subsequent HùêªHitalic_H frames into a set of latent codes. Then, an auto-regressive transformer seamlessly integrates the output of LDM with the next token prediction paradigm.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.09781/extracted/6136615/figures/umap_v4.png alt></figure></p><blockquote><p>üîº This figure visualizes the latent codes learned by the Latent Dynamics Model (LDM) during training on Go and robotic manipulation tasks. The left panel shows UMAP projections of latent codes from the Go game, where each point represents a latent code generated by the LDM before quantization. Odd steps correspond to white player moves and even steps to black player moves. Black moves in steps 2, 4, and 6 are highlighted to demonstrate common patterns for new black moves. These patterns are further clarified with additional color and lines on the board. The right panel shows UMAP projections of latent codes from the robotic arm&rsquo;s movement in the CALVIN dataset. Here, each point represents a latent code, and the points are color-coded according to the magnitude of displacement along the X, Y, and Z axes at intervals of 1, 5, and 10 frames. Purple and red colors indicate the maximum displacement in opposite directions.</p><details><summary>read the caption</summary>Figure 4: UMAP projection¬†[34] of the learned latent code on the Go (Left) and CALVIN (right) training set. Each point represents the continuous (pre-quantization) latent code generated by the LDM. In Go examples, odd steps represent white‚Äôs moves, and even steps represent black‚Äôs moves. We visualize the latent codes of black moves in steps 2/4/6. The legend shows examples of common patterns learned for new black moves. For clarity, these moves are highlighted on the board with added colors and lines to indicate new patterns. On the right, we visualize the latent codes of the robotic arm‚Äôs movement along the X/Y/Z axes at intervals of 1, 5, and 10 frames. Points are color-coded by displacement range, with purple and red indicating the maximum displacement in opposite directions along each axis.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.09781/extracted/6136615/figures/umap_test_v5.png alt></figure></p><blockquote><p>üîº This figure demonstrates a game of Go between the VideoWorld model and KataGO. The VideoWorld model plays as black. The visualization uses UMAP to project the latent codes generated by the model&rsquo;s Latent Dynamics Model (LDM). The colors of the new stones placed on the Go board correspond to the colors of the projected latent codes, showing how the model&rsquo;s internal representation (latent codes) relates to its actions in the game. The clustering of the latent codes suggests that the model is not just reacting to the immediate game state but also considering future possibilities, demonstrating forward planning capabilities.</p><details><summary>read the caption</summary>Figure 5: Illustration of playing against KataGO and UMAP projection¬†[34] of the predicted latent code. Our model plays as black. The generated latent code is visualized through the LDM decoder and new stones in the visualization are marked with colors to match the legend. The visualization serves as a probe, indicating that the model shows signs of forward planning.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.09781/extracted/6136615/figures/umap_calvin.png alt></figure></p><blockquote><p>üîº This figure shows the comparison of prediction targets. It compares three different prediction targets: a state sequence (e.g., labeled positions of moves in Go), a raw video sequence, and a video sequence augmented with latent codes representing future visual changes. The x-axis represents the number of seen samples and the y-axis represents the Action-Value. It demonstrates that VideoWorld, by combining rich video information with a compact representation of visual changes, achieves superior training efficiency compared to using only state or video information.</p><details><summary>read the caption</summary>(a)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.09781/extracted/6136615/figures/supp_vis_cap.png alt></figure></p><blockquote><p>üîº This figure shows the results of an ablation study on the compression length of the latent dynamics model (LDM) in the CALVIN environment. The x-axis represents different compression lengths (H), indicating how many future frames are compressed into a latent code. The y-axis shows the task success rate for three robotic manipulation tasks: Push, Open/Close, and Turn On/Off. The baseline represents the performance without LDM. Different compression lengths were tested, revealing the optimal H value for each task that balances compression and information retention for effective learning.</p><details><summary>read the caption</summary>(b)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.09781/extracted/6136615/figures/supp_vis_sac.png alt></figure></p><blockquote><p>üîº The figure shows the results of intervening latent codes with different indices. It demonstrates the impact of altering latent codes at different time steps on the model&rsquo;s performance. By replacing latent codes with random tokens, the experiment shows how altering earlier codes (those representing immediate next steps) has a greater effect than altering later codes. This highlights the importance of the causal relationships and temporal ordering of information within the latent code sequence for effective reasoning and task completion.</p><details><summary>read the caption</summary>(c)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.09781/extracted/6136615/figures/supp_vis_calvin_3.png alt></figure></p><blockquote><p>üîº The figure shows the ablation study of different codebook sizes in the latent dynamics model (LDM) on the performance of Go and CALVIN tasks. The results demonstrate how different codebook sizes (729, 15625, 64000, and 262144) impact the model&rsquo;s ability to learn and achieve high accuracy in both Go and CALVIN tasks, showcasing the importance of selecting an appropriate codebook size for effective knowledge acquisition.</p><details><summary>read the caption</summary>(d)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.09781/extracted/6136615/figures/supp_vis_rlbench.png alt></figure></p><blockquote><p>üîº This ablation study investigates the impact of the data source on the performance of the VideoWorld model. It compares the model&rsquo;s performance using only human-generated Go data, only KataGo-generated data, and a combination of both. The results demonstrate how different data sources affect the model&rsquo;s ability to learn and perform the game, highlighting the role of data quality and diversity in knowledge acquisition.</p><details><summary>read the caption</summary>(e)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.09781/extracted/6136615/figures/state_count.png alt></figure></p><blockquote><p>üîº This figure visualizes the latent codes generated by the Latent Dynamics Model (LDM) during inference for robotic manipulation tasks. The UMAP projection shows how these latent codes (9 codes representing 9 future time steps, H=9) cluster based on the task being performed. Each point in the UMAP plot represents a latent code, and the color indicates the specific task. The images on the right side show the model&rsquo;s actions. Yellow-background images depict the actual robotic arm movements during inference. Green-background images show the model&rsquo;s predictions of the next frames while it was training, illustrating its planning ability.</p><details><summary>read the caption</summary>Figure 6: Illustration of robotic manipulation and UMAP projection of the predicted latent code during inference. Latent codes are visualized through the LDM decoder. The UMAP projection illustrates the 9 predicted latent codes (i.e. H=9ùêª9H=9italic_H = 9) across different tasks, with each point color-coded by task type. Visualizations with a yellow background show the model‚Äôs actual robotic arm control during inference, while those with a green background represent the model‚Äôs next-frame predictions during training.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.09781/extracted/6136615/figures/rep.png alt></figure></p><blockquote><p>üîº This figure shows the UMAP projection of the learned latent codes on the Go (left) and CALVIN (right) training sets. Each point represents the continuous (pre-quantization) latent code generated by the LDM. The Go visualizations show odd steps representing white&rsquo;s moves and even steps representing black&rsquo;s moves. The legend shows examples of common patterns learned for new black moves; these are highlighted on the board with colors and lines. The CALVIN visualizations show the latent codes of robotic arm movements along the X, Y, and Z axes at different frame intervals. Points are color-coded by displacement range, with purple and red indicating maximum displacement in opposite directions.</p><details><summary>read the caption</summary>(a)</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S5.T2.2.2><tr class=ltx_tr id=S5.T2.2.2.3><td class="ltx_td ltx_align_left ltx_border_tt" id=S5.T2.2.2.3.1 rowspan=2><span class=ltx_text id=S5.T2.2.2.3.1.1 style=font-size:80%>Agents</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T2.2.2.3.2 rowspan=2><span class=ltx_text id=S5.T2.2.2.3.2.1 style=font-size:80%>Input/Ouput</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=3 id=S5.T2.2.2.3.3><span class=ltx_text id=S5.T2.2.2.3.3.1 style=font-size:80%>Task Success Rate (%)</span></td></tr><tr class=ltx_tr id=S5.T2.2.2.4><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.2.2.4.1><span class=ltx_text id=S5.T2.2.2.4.1.1 style=font-size:80%>Push</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.2.2.4.2><span class=ltx_text id=S5.T2.2.2.4.2.1 style=font-size:80%>Open/Close</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.2.2.4.3><span class=ltx_text id=S5.T2.2.2.4.3.1 style=font-size:80%>Turn on/off</span></td></tr><tr class=ltx_tr id=S5.T2.2.2.5><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.2.2.5.1><span class=ltx_text id=S5.T2.2.2.5.1.1 style=font-size:80%>MCIL¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class=ltx_text id=S5.T2.2.2.5.1.2.1 style=font-size:80%>[</span><a class=ltx_ref href=https://arxiv.org/html/2501.09781v1#bib.bib39 title><span class=ltx_text style=font-size:90%>39</span></a><span class=ltx_text id=S5.T2.2.2.5.1.3.2 style=font-size:80%>]</span></cite></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.2.2.5.2><span class=ltx_text id=S5.T2.2.2.5.2.1 style=font-size:80%>Video/Lab. Action</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.2.2.5.3><span class=ltx_text id=S5.T2.2.2.5.3.1 style=font-size:80%>33.0</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.2.2.5.4><span class=ltx_text id=S5.T2.2.2.5.4.1 style=font-size:80%>38.7</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.2.2.5.5><span class=ltx_text id=S5.T2.2.2.5.5.1 style=font-size:80%>41.2</span></td></tr><tr class=ltx_tr id=S5.T2.2.2.6><td class="ltx_td ltx_align_left" id=S5.T2.2.2.6.1><span class=ltx_text id=S5.T2.2.2.6.1.1 style=font-size:80%>HULC¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class=ltx_text id=S5.T2.2.2.6.1.2.1 style=font-size:80%>[</span><a class=ltx_ref href=https://arxiv.org/html/2501.09781v1#bib.bib38 title><span class=ltx_text style=font-size:90%>38</span></a><span class=ltx_text id=S5.T2.2.2.6.1.3.2 style=font-size:80%>]</span></cite></td><td class="ltx_td ltx_align_center" id=S5.T2.2.2.6.2><span class=ltx_text id=S5.T2.2.2.6.2.1 style=font-size:80%>Video/Lab. Action</span></td><td class="ltx_td ltx_align_center" id=S5.T2.2.2.6.3><span class=ltx_text id=S5.T2.2.2.6.3.1 style=font-size:80%>65.8</span></td><td class="ltx_td ltx_align_center" id=S5.T2.2.2.6.4><span class=ltx_text id=S5.T2.2.2.6.4.1 style=font-size:80%>80.9</span></td><td class="ltx_td ltx_align_center" id=S5.T2.2.2.6.5><span class=ltx_text id=S5.T2.2.2.6.5.1 style=font-size:80%>85.3</span></td></tr><tr class=ltx_tr id=S5.T2.2.2.7><td class="ltx_td ltx_align_left" id=S5.T2.2.2.7.1><span class=ltx_text id=S5.T2.2.2.7.1.1 style=font-size:80%;color:#bfbfbf>Transformer (Oracle)</span></td><td class="ltx_td ltx_align_center" id=S5.T2.2.2.7.2><span class=ltx_text id=S5.T2.2.2.7.2.1 style=font-size:80%;color:#bfbfbf>Video/Lab. Action</span></td><td class="ltx_td ltx_align_center" id=S5.T2.2.2.7.3><span class=ltx_text id=S5.T2.2.2.7.3.1 style=font-size:80%;color:#bfbfbf>75.4</span></td><td class="ltx_td ltx_align_center" id=S5.T2.2.2.7.4><span class=ltx_text id=S5.T2.2.2.7.4.1 style=font-size:80%;color:#bfbfbf>95.3</span></td><td class="ltx_td ltx_align_center" id=S5.T2.2.2.7.5><span class=ltx_text id=S5.T2.2.2.7.5.1 style=font-size:80%;color:#bfbfbf>96.2</span></td></tr><tr class=ltx_tr id=S5.T2.2.2.8><td class="ltx_td ltx_align_left" id=S5.T2.2.2.8.1><span class=ltx_text id=S5.T2.2.2.8.1.1 style=font-size:80%>Transformer</span></td><td class="ltx_td ltx_align_center" id=S5.T2.2.2.8.2><span class=ltx_text id=S5.T2.2.2.8.2.1 style=font-size:80%>Video</span></td><td class="ltx_td ltx_align_center" id=S5.T2.2.2.8.3><span class=ltx_text id=S5.T2.2.2.8.3.1 style=font-size:80%>17.3</span></td><td class="ltx_td ltx_align_center" id=S5.T2.2.2.8.4><span class=ltx_text id=S5.T2.2.2.8.4.1 style=font-size:80%>24.1</span></td><td class="ltx_td ltx_align_center" id=S5.T2.2.2.8.5><span class=ltx_text id=S5.T2.2.2.8.5.1 style=font-size:80%>19.2</span></td></tr><tr class=ltx_tr id=S5.T2.2.2.9><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T2.2.2.9.1><span class=ltx_text id=S5.T2.2.2.9.1.1 style=font-size:80%>VideoWorld</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.2.2.9.2><span class=ltx_text id=S5.T2.2.2.9.2.1 style=font-size:80%>Video</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.2.2.9.3><span class=ltx_text id=S5.T2.2.2.9.3.1 style=font-size:80%>56.2</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.2.2.9.4><span class=ltx_text id=S5.T2.2.2.9.4.1 style=font-size:80%>75.4</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.2.2.9.5><span class=ltx_text id=S5.T2.2.2.9.5.1 style=font-size:80%>72.1</span></td></tr><tr class=ltx_tr id=S5.T2.1.1.1><td class="ltx_td ltx_align_left" id=S5.T2.1.1.1.1><span class=ltx_text id=S5.T2.1.1.1.1.1 style=font-size:80%>VideoWorld (+10k data)</span><sup class=ltx_sup id=S5.T2.1.1.1.1.2><span class="ltx_text ltx_font_italic" id=S5.T2.1.1.1.1.2.1 style=font-size:80%>‚Ä†</span></sup></td><td class="ltx_td ltx_align_center" id=S5.T2.1.1.1.2><span class=ltx_text id=S5.T2.1.1.1.2.1 style=font-size:80%>Video</span></td><td class="ltx_td ltx_align_center" id=S5.T2.1.1.1.3><span class=ltx_text id=S5.T2.1.1.1.3.1 style=font-size:80%>65.3</span></td><td class="ltx_td ltx_align_center" id=S5.T2.1.1.1.4><span class=ltx_text id=S5.T2.1.1.1.4.1 style=font-size:80%>81.2</span></td><td class="ltx_td ltx_align_center" id=S5.T2.1.1.1.5><span class=ltx_text id=S5.T2.1.1.1.5.1 style=font-size:80%>79.3</span></td></tr><tr class=ltx_tr id=S5.T2.2.2.2><td class="ltx_td ltx_align_left ltx_border_bb" id=S5.T2.2.2.2.1><span class=ltx_text id=S5.T2.2.2.2.1.1 style=font-size:80%>VideoWorld (+30k data)</span><sup class=ltx_sup id=S5.T2.2.2.2.1.2><span class="ltx_text ltx_font_italic" id=S5.T2.2.2.2.1.2.1 style=font-size:80%>‚Ä°</span></sup></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.2.2.2.2><span class=ltx_text id=S5.T2.2.2.2.2.1 style=font-size:80%>Video</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.2.2.2.3><span class=ltx_text id=S5.T2.2.2.2.3.1 style=font-size:80%>72.7</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.2.2.2.4><span class=ltx_text id=S5.T2.2.2.2.4.1 style=font-size:80%>91.0</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.2.2.2.5><span class=ltx_text id=S5.T2.2.2.2.5.1 style=font-size:80%>93.8</span></td></tr></table></table></figure><blockquote><p>üîº This table presents a comparison of different models&rsquo; performance on the CALVIN benchmark, a robotic manipulation task. The benchmark evaluates the ability of models to perform three tasks: Push Blocks, Open/Close Drawer, and Turn On/Off Light. The table compares a baseline model (MCIL), a state-of-the-art model (HULC), a Transformer model trained with labelled data (Oracle), and the VideoWorld model at varying training data sizes. The &lsquo;Input/Output&rsquo; column specifies whether the models use video input and labelled action output (Lab), or only video input. The success rate for each task is given as a percentage for each model. The table highlights that VideoWorld, trained solely on unlabeled video data, performs comparably to models trained with labelled data, demonstrating its ability to learn complex tasks from visual data alone.</p><details><summary>read the caption</summary>Table 2: Comparison on CALVIN benchmark. ‚ÄúLab.‚Äù means annotated labels. All models have 300M parameters. ‚Ä† and ‚Ä° denote using an additional 10k and 30k CALVIN trajectories for training, respectively.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S5.T3.2.1><tr class=ltx_tr id=S5.T3.2.1.1><td class="ltx_td ltx_align_left ltx_border_tt" id=S5.T3.2.1.1.1 rowspan=2><span class=ltx_text id=S5.T3.2.1.1.1.1 style=font-size:80%>Agents</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=3 id=S5.T3.2.1.1.2><span class=ltx_text id=S5.T3.2.1.1.2.1 style=font-size:80%>CALVIN</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=2 id=S5.T3.2.1.1.3><span class=ltx_text id=S5.T3.2.1.1.3.1 style=font-size:80%>RLBench</span></td></tr><tr class=ltx_tr id=S5.T3.2.1.2><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.2.1.2.1><span class=ltx_text id=S5.T3.2.1.2.1.1 style=font-size:80%>Push</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.2.1.2.2><span class=ltx_text id=S5.T3.2.1.2.2.1 style=font-size:80%>Open/Close</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.2.1.2.3><span class=ltx_text id=S5.T3.2.1.2.3.1 style=font-size:80%>Turn on/off</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.2.1.2.4><span class=ltx_text id=S5.T3.2.1.2.4.1 style=font-size:80%>Microwave</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.2.1.2.5><span class=ltx_text id=S5.T3.2.1.2.5.1 style=font-size:80%>Fridge</span></td></tr><tr class=ltx_tr id=S5.T3.2.1.3><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T3.2.1.3.1><span class=ltx_text id=S5.T3.2.1.3.1.1 style=font-size:80%;color:#bfbfbf>Transformer (Oracle)</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.2.1.3.2><span class=ltx_text id=S5.T3.2.1.3.2.1 style=font-size:80%;color:#bfbfbf>61.3</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.2.1.3.3><span class=ltx_text id=S5.T3.2.1.3.3.1 style=font-size:80%;color:#bfbfbf>79.5</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.2.1.3.4><span class=ltx_text id=S5.T3.2.1.3.4.1 style=font-size:80%;color:#bfbfbf>78.0</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.2.1.3.5><span class=ltx_text id=S5.T3.2.1.3.5.1 style=font-size:80%;color:#bfbfbf>72.1</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.2.1.3.6><span class=ltx_text id=S5.T3.2.1.3.6.1 style=font-size:80%;color:#bfbfbf>69.0</span></td></tr><tr class=ltx_tr id=S5.T3.2.1.4><td class="ltx_td ltx_align_left" id=S5.T3.2.1.4.1><span class=ltx_text id=S5.T3.2.1.4.1.1 style=font-size:80%>Transformer</span></td><td class="ltx_td ltx_align_center" id=S5.T3.2.1.4.2><span class=ltx_text id=S5.T3.2.1.4.2.1 style=font-size:80%>6.5</span></td><td class="ltx_td ltx_align_center" id=S5.T3.2.1.4.3><span class=ltx_text id=S5.T3.2.1.4.3.1 style=font-size:80%>13.0</span></td><td class="ltx_td ltx_align_center" id=S5.T3.2.1.4.4><span class=ltx_text id=S5.T3.2.1.4.4.1 style=font-size:80%>15.6</span></td><td class="ltx_td ltx_align_center" id=S5.T3.2.1.4.5><span class=ltx_text id=S5.T3.2.1.4.5.1 style=font-size:80%>12.0</span></td><td class="ltx_td ltx_align_center" id=S5.T3.2.1.4.6><span class=ltx_text id=S5.T3.2.1.4.6.1 style=font-size:80%>10.9</span></td></tr><tr class=ltx_tr id=S5.T3.2.1.5><td class="ltx_td ltx_align_left ltx_border_bb" id=S5.T3.2.1.5.1><span class=ltx_text id=S5.T3.2.1.5.1.1 style=font-size:80%>VideoWorld</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.2.1.5.2><span class=ltx_text id=S5.T3.2.1.5.2.1 style=font-size:80%>56.0</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.2.1.5.3><span class=ltx_text id=S5.T3.2.1.5.3.1 style=font-size:80%>74.8</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.2.1.5.4><span class=ltx_text id=S5.T3.2.1.5.4.1 style=font-size:80%>74.5</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.2.1.5.5><span class=ltx_text id=S5.T3.2.1.5.5.1 style=font-size:80%>67.1</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.2.1.5.6><span class=ltx_text id=S5.T3.2.1.5.6.1 style=font-size:80%>62.5</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the results of a joint training experiment on two robotic manipulation benchmarks: CALVIN and RLBench. It compares the performance of a VideoWorld model trained only on visual data to an oracle model (Transformer) trained with ground truth action labels. The table shows success rates (percentage of successful task completions) for three distinct tasks across both benchmarks: Push, Open/Close, and Turn on/off in CALVIN; and Close Microwave and Close Fridge in RLBench. The comparison highlights the VideoWorld model&rsquo;s ability to achieve promising results despite lacking ground truth action labels, demonstrating its capacity for knowledge acquisition from visual inputs alone.</p><details><summary>read the caption</summary>Table 3: Results of joint training on CALVIN and RLBench.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_minipage ltx_align_middle" id=S5.F6.sf1.1.1.1 style=width:125.7pt><tr class=ltx_tr id=S5.F6.sf1.1.1.1.1><td class="ltx_td ltx_align_center ltx_border_r" id=S5.F6.sf1.1.1.1.1.1 rowspan=2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf1.1.1.1.1.1.1 style=font-size:80%><span class=ltx_text id=S5.F6.sf1.1.1.1.1.1.1.2></span> <span class=ltx_text id=S5.F6.sf1.1.1.1.1.1.1.1><span class="ltx_tabular ltx_align_middle" id=S5.F6.sf1.1.1.1.1.1.1.1.1><span class=ltx_tr id=S5.F6.sf1.1.1.1.1.1.1.1.1.2><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.F6.sf1.1.1.1.1.1.1.1.1.2.1 style=padding-left:5pt;padding-right:5pt>Compression</span></span>
<span class=ltx_tr id=S5.F6.sf1.1.1.1.1.1.1.1.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.F6.sf1.1.1.1.1.1.1.1.1.1.1 style=padding-left:5pt;padding-right:5pt>length <math alttext="H" class="ltx_Math" display="inline" id="S5.F6.sf1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F6.sf1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S5.F6.sf1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F6.sf1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S5.F6.sf1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S5.F6.sf1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F6.sf1.1.1.1.1.1.1.1.1.1.1.m1.1.1">ùêª</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F6.sf1.1.1.1.1.1.1.1.1.1.1.m1.1c">H</annotation><annotation encoding="application/x-llamapun" id="S5.F6.sf1.1.1.1.1.1.1.1.1.1.1.m1.1d">italic_H</annotation></semantics></math></span></span>
</span></span><span class=ltx_text id=S5.F6.sf1.1.1.1.1.1.1.3></span></span></td><td class="ltx_td ltx_align_center" colspan=2 id=S5.F6.sf1.1.1.1.1.2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf1.1.1.1.1.2.1 style=font-size:80%>Go</span></td></tr><tr class=ltx_tr id=S5.F6.sf1.1.1.1.2><td class="ltx_td ltx_align_center" id=S5.F6.sf1.1.1.1.2.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf1.1.1.1.2.1.1 style=font-size:80%>Act-Value</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf1.1.1.1.2.2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf1.1.1.1.2.2.1 style=font-size:80%>Act-Acc.</span></td></tr><tr class=ltx_tr id=S5.F6.sf1.1.1.1.3><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S5.F6.sf1.1.1.1.3.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf1.1.1.1.3.1.1 style=font-size:80%>baseline</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.F6.sf1.1.1.1.3.2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf1.1.1.1.3.2.1 style=font-size:80%>47.5</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.F6.sf1.1.1.1.3.3 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf1.1.1.1.3.3.1 style=font-size:80%>44.3</span></td></tr><tr class=ltx_tr id=S5.F6.sf1.1.1.1.4><td class="ltx_td ltx_align_center ltx_border_r" id=S5.F6.sf1.1.1.1.4.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf1.1.1.1.4.1.1 style=font-size:80%>1</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf1.1.1.1.4.2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf1.1.1.1.4.2.1 style=font-size:80%>70.3</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf1.1.1.1.4.3 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf1.1.1.1.4.3.1 style=font-size:80%>77.0</span></td></tr><tr class=ltx_tr id=S5.F6.sf1.1.1.1.5><td class="ltx_td ltx_align_center ltx_border_r" id=S5.F6.sf1.1.1.1.5.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf1.1.1.1.5.1.1 style=font-size:80%>3</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf1.1.1.1.5.2 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.F6.sf1.1.1.1.5.2.1 style=font-size:80%>72.5</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf1.1.1.1.5.3 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.F6.sf1.1.1.1.5.3.1 style=font-size:80%>80.6</span></td></tr><tr class=ltx_tr id=S5.F6.sf1.1.1.1.6><td class="ltx_td ltx_align_center ltx_border_r" id=S5.F6.sf1.1.1.1.6.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf1.1.1.1.6.1.1 style=font-size:80%>5</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf1.1.1.1.6.2 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=S5.F6.sf1.1.1.1.6.2.1 style=font-size:80%>73.9</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf1.1.1.1.6.3 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=S5.F6.sf1.1.1.1.6.3.1 style=font-size:80%>80.9</span></td></tr></table></table></figure><blockquote><p>üîº This table presents ablation studies and analysis on the VideoWorld model (50M parameter version). It shows the impact of different design choices on model performance, including varying compression lengths for the latent dynamics model (LDM) in Go and CALVIN environments, experimenting with different codebook sizes in the LDM, and analyzing the effect of different data sources (human vs. KataGo). The results demonstrate the influence of these choices on both action value and accuracy metrics.</p><details><summary>read the caption</summary>Table 4: Ablations and analysis. We conduct all experiments based on our 50M model.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_minipage ltx_align_middle" id=S5.F6.sf2.1.1.1 style=width:143.1pt><tr class=ltx_tr id=S5.F6.sf2.1.1.1.1><td class="ltx_td ltx_align_center ltx_border_r" id=S5.F6.sf2.1.1.1.1.1 rowspan=2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf2.1.1.1.1.1.1 style=font-size:80%><span class=ltx_text id=S5.F6.sf2.1.1.1.1.1.1.2></span> <span class=ltx_text id=S5.F6.sf2.1.1.1.1.1.1.1><span class="ltx_tabular ltx_align_middle" id=S5.F6.sf2.1.1.1.1.1.1.1.1><span class=ltx_tr id=S5.F6.sf2.1.1.1.1.1.1.1.1.2><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.F6.sf2.1.1.1.1.1.1.1.1.2.1 style=padding-left:5pt;padding-right:5pt>Compression</span></span>
<span class=ltx_tr id=S5.F6.sf2.1.1.1.1.1.1.1.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.F6.sf2.1.1.1.1.1.1.1.1.1.1 style=padding-left:5pt;padding-right:5pt>length <math alttext="H" class="ltx_Math" display="inline" id="S5.F6.sf2.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F6.sf2.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S5.F6.sf2.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F6.sf2.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S5.F6.sf2.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S5.F6.sf2.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F6.sf2.1.1.1.1.1.1.1.1.1.1.m1.1.1">ùêª</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F6.sf2.1.1.1.1.1.1.1.1.1.1.m1.1c">H</annotation><annotation encoding="application/x-llamapun" id="S5.F6.sf2.1.1.1.1.1.1.1.1.1.1.m1.1d">italic_H</annotation></semantics></math></span></span>
</span></span><span class=ltx_text id=S5.F6.sf2.1.1.1.1.1.1.3></span></span></td><td class="ltx_td ltx_align_center" colspan=3 id=S5.F6.sf2.1.1.1.1.2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf2.1.1.1.1.2.1 style=font-size:80%>CALVIN</span></td></tr><tr class=ltx_tr id=S5.F6.sf2.1.1.1.2><td class="ltx_td ltx_align_center" id=S5.F6.sf2.1.1.1.2.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf2.1.1.1.2.1.1 style=font-size:80%>Push</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf2.1.1.1.2.2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf2.1.1.1.2.2.1 style=font-size:80%>Open/Close</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf2.1.1.1.2.3 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf2.1.1.1.2.3.1 style=font-size:80%>Turn On/Off</span></td></tr><tr class=ltx_tr id=S5.F6.sf2.1.1.1.3><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S5.F6.sf2.1.1.1.3.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf2.1.1.1.3.1.1 style=font-size:80%>baseline</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.F6.sf2.1.1.1.3.2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf2.1.1.1.3.2.1 style=font-size:80%>12.7</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.F6.sf2.1.1.1.3.3 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf2.1.1.1.3.3.1 style=font-size:80%>20.8</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.F6.sf2.1.1.1.3.4 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf2.1.1.1.3.4.1 style=font-size:80%>15.6</span></td></tr><tr class=ltx_tr id=S5.F6.sf2.1.1.1.4><td class="ltx_td ltx_align_center ltx_border_r" id=S5.F6.sf2.1.1.1.4.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf2.1.1.1.4.1.1 style=font-size:80%>1</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf2.1.1.1.4.2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf2.1.1.1.4.2.1 style=font-size:80%>33.7</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf2.1.1.1.4.3 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf2.1.1.1.4.3.1 style=font-size:80%>53.6</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf2.1.1.1.4.4 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf2.1.1.1.4.4.1 style=font-size:80%>67.3</span></td></tr><tr class=ltx_tr id=S5.F6.sf2.1.1.1.5><td class="ltx_td ltx_align_center ltx_border_r" id=S5.F6.sf2.1.1.1.5.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf2.1.1.1.5.1.1 style=font-size:80%>5</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf2.1.1.1.5.2 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.F6.sf2.1.1.1.5.2.1 style=font-size:80%>46.8</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf2.1.1.1.5.3 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.F6.sf2.1.1.1.5.3.1 style=font-size:80%>66.1</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf2.1.1.1.5.4 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.F6.sf2.1.1.1.5.4.1 style=font-size:80%>69.6</span></td></tr><tr class=ltx_tr id=S5.F6.sf2.1.1.1.6><td class="ltx_td ltx_align_center ltx_border_r" id=S5.F6.sf2.1.1.1.6.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf2.1.1.1.6.1.1 style=font-size:80%>10</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf2.1.1.1.6.2 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=S5.F6.sf2.1.1.1.6.2.1 style=font-size:80%>50.3</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf2.1.1.1.6.3 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=S5.F6.sf2.1.1.1.6.3.1 style=font-size:80%>71.1</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf2.1.1.1.6.4 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=S5.F6.sf2.1.1.1.6.4.1 style=font-size:80%>69.7</span></td></tr></table></table></figure><blockquote><p>üîº This table presents ablation study results, focusing on the impact of using only latent codes for prediction, without including the video frames. It compares the performance of a 50M parameter model on Go and CALVIN tasks when predicting only latent codes versus predicting both latent codes and video frames, highlighting the contribution of latent codes to model performance. The metrics used are Action-Value and Action Accuracy, demonstrating how the model&rsquo;s ability to predict and utilize the latent codes translates to overall task performance.</p><details><summary>read the caption</summary>Table 5: Latent code prediction only with 50M parameters.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_minipage ltx_align_middle" id=S5.F6.sf3.2 style=width:125.7pt><tr class=ltx_tr id=S5.F6.sf3.2.1><td class="ltx_td ltx_align_center ltx_border_r" id=S5.F6.sf3.2.1.1 rowspan=2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf3.2.1.1.1 style=font-size:80%><span class=ltx_text id=S5.F6.sf3.2.1.1.1.1></span> <span class=ltx_text id=S5.F6.sf3.2.1.1.1.2><span class="ltx_tabular ltx_align_middle" id=S5.F6.sf3.2.1.1.1.2.1><span class=ltx_tr id=S5.F6.sf3.2.1.1.1.2.1.1><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.F6.sf3.2.1.1.1.2.1.1.1 style=padding-left:5pt;padding-right:5pt>Code</span></span>
<span class=ltx_tr id=S5.F6.sf3.2.1.1.1.2.1.2><span class="ltx_td ltx_nopad_r ltx_align_center" id=S5.F6.sf3.2.1.1.1.2.1.2.1 style=padding-left:5pt;padding-right:5pt>Index</span></span>
</span></span><span class=ltx_text id=S5.F6.sf3.2.1.1.1.3></span></span></td><td class="ltx_td ltx_align_center" colspan=2 id=S5.F6.sf3.2.1.2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf3.2.1.2.1 style=font-size:80%>Go</span></td></tr><tr class=ltx_tr id=S5.F6.sf3.2.2><td class="ltx_td ltx_align_center" id=S5.F6.sf3.2.2.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf3.2.2.1.1 style=font-size:80%>Act-Value</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf3.2.2.2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf3.2.2.2.1 style=font-size:80%>Act-Acc.</span></td></tr><tr class=ltx_tr id=S5.F6.sf3.2.3><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S5.F6.sf3.2.3.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf3.2.3.1.1 style=font-size:80%>None</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.F6.sf3.2.3.2 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=S5.F6.sf3.2.3.2.1 style=font-size:80%>73.9</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.F6.sf3.2.3.3 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_font_bold" id=S5.F6.sf3.2.3.3.1 style=font-size:80%>80.9</span></td></tr><tr class=ltx_tr id=S5.F6.sf3.2.4><td class="ltx_td ltx_align_center ltx_border_r" id=S5.F6.sf3.2.4.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf3.2.4.1.1 style=font-size:80%>1</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf3.2.4.2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf3.2.4.2.1 style=font-size:80%>46.2</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf3.2.4.3 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf3.2.4.3.1 style=font-size:80%>42.1</span></td></tr><tr class=ltx_tr id=S5.F6.sf3.2.5><td class="ltx_td ltx_align_center ltx_border_r" id=S5.F6.sf3.2.5.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf3.2.5.1.1 style=font-size:80%>2</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf3.2.5.2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf3.2.5.2.1 style=font-size:80%>69.5</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf3.2.5.3 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf3.2.5.3.1 style=font-size:80%>76.6</span></td></tr><tr class=ltx_tr id=S5.F6.sf3.2.6><td class="ltx_td ltx_align_center ltx_border_r" id=S5.F6.sf3.2.6.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf3.2.6.1.1 style=font-size:80%>3</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf3.2.6.2 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.F6.sf3.2.6.2.1 style=font-size:80%>72.1</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf3.2.6.3 style=padding-left:5pt;padding-right:5pt><span class="ltx_text ltx_framed ltx_framed_underline" id=S5.F6.sf3.2.6.3.1 style=font-size:80%>80.6</span></td></tr><tr class=ltx_tr id=S5.F6.sf3.2.7><td class="ltx_td ltx_align_center ltx_border_r" id=S5.F6.sf3.2.7.1 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf3.2.7.1.1 style=font-size:80%>All</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf3.2.7.2 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf3.2.7.2.1 style=font-size:80%>45.8</span></td><td class="ltx_td ltx_align_center" id=S5.F6.sf3.2.7.3 style=padding-left:5pt;padding-right:5pt><span class=ltx_text id=S5.F6.sf3.2.7.3.1 style=font-size:80%>43.7</span></td></tr></table></table></figure><blockquote><p>üîº This table details the hyperparameters used for training both the latent dynamics model (LDM) and the autoregressive transformer. It lists the optimizer, learning rate, weight decay, momentum, batch size, learning rate schedule, warmup iterations, maximum iterations, augmentations, training loss type, and the target for training (reconstruction or next token prediction). Separate configurations are provided for the LDM and the transformer.</p><details><summary>read the caption</summary>Table A.1: Training configurations for the latent dynamics model (LDM) and auto-regressive (AR) transformer.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-b326f40b37319394454fb4f0da2c0eb2 class=gallery><img src=https://ai-paper-reviewer.com/2501.09781/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.09781/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.09781/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.09781/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.09781/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.09781/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.09781/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.09781/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.09781/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.09781/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.09781/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.09781/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.09781/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.09781/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.09781/15.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09781/&amp;title=VideoWorld:%20Exploring%20Knowledge%20Learning%20from%20Unlabeled%20Videos" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09781/&amp;text=VideoWorld:%20Exploring%20Knowledge%20Learning%20from%20Unlabeled%20Videos" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09781/&amp;subject=VideoWorld:%20Exploring%20Knowledge%20Learning%20from%20Unlabeled%20Videos" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2501.09781/index.md",oid_likes="likes_paper-reviews/2501.09781/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2501.08983/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-01-15T00:00:00+00:00>15 January 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2501.09686/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-01-16T00:00:00+00:00>16 January 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>