{"references": [{"fullname_first_author": "J. Ainslie", "paper_title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-05-13", "reason": "This paper proposes GQA, a crucial architecture for efficient decoding that significantly reduces memory access bottlenecks, a key focus of this paper's improvements."}, {"fullname_first_author": "D. Dai", "paper_title": "DeepSeekMOE: Towards ultimate expert specialization in mixture-of-experts language models", "publication_date": "2024-01-06", "reason": "As this paper uses a similar model architecture (MoE) and the authors are affiliated with the same institution, this is a highly relevant reference."}, {"fullname_first_author": "S. Ge", "paper_title": "Model tells you what to discard: Adaptive KV cache compression for LLMs", "publication_date": "2023-10-01", "reason": "This paper directly addresses the efficiency challenges of handling long sequences by proposing adaptive compression methods for keys and values, a key aspect of the current work."}, {"fullname_first_author": "H. Jiang", "paper_title": "Minference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention", "publication_date": "2024-07-02", "reason": "This paper also tackles long-context efficiency, using dynamic sparse attention; comparing and contrasting methods is critical for establishing the novelty of the current work."}, {"fullname_first_author": "Y. Li", "paper_title": "SnapKV: LLM knows what you are looking for before generation", "publication_date": "2024-04-14", "reason": "This paper focuses on optimizing KV-cache usage, a major efficiency concern of the current paper. Comparing and contrasting this approach helps highlight the advancements in the current work."}]}