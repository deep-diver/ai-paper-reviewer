[{"content": "| Dataset | Items | # Turns | Prop. |\n|---|---|---|---| \n| Mesh Generation\u2020 | 125k | 8\u00d7 | 40% |\n| Mesh Understanding\u2020 | 125k | 4\u00d7 | 20% |\n| General Conversation [15] | 1M | 1\u00d7 | 40% |", "caption": "Table 1: \nDataset Statistics. We list each dataset\u2019s number of items, number of training turns per item, and the total sample proportions. Training is performed on a combined dataset, with each dataset resampled according to the ratio. We use a mix of mesh generation, mesh understanding, and general conversation data to equip LLMs with 3D capabilities while maintaining their language abilities. Datasets marked with \u2020 are those we constructed.", "description": "This table details the composition of the dataset used to fine-tune the LLAMA-MESH model. It breaks down the dataset into three parts: mesh generation, mesh understanding, and general conversation data. For each part, it provides the number of data items, the number of training turns per item, and the overall proportion of that data type within the whole dataset.  The table clarifies that the training is done on a combined dataset where each data type's contribution is weighted based on these proportions. It also notes that some datasets were specifically constructed for this research.", "section": "4.1 Implementation Details"}, {"content": "| Method | MeshXL [7] | MeshXL [7] | Llama-Mesh |\n|---|---|---|---|\n| Model Size | 350M | 1.3B | 8B |\n| GPU hours | 6000 | 23232 | 2400 |", "caption": "Table 2: \nTraining time comparison. Compared to MeshXL\u00a0[7], Llama-Mesh uses far fewer GPU hours despite its larger model size, benefiting from using pretrained LLM weights.", "description": "This table compares the training time and computational resources used by Llama-Mesh and MeshXL, highlighting Llama-Mesh's efficiency despite having a larger model size.  This efficiency is attributed to Llama-Mesh leveraging pre-trained large language model weights, significantly reducing the training time compared to MeshXL which trained from scratch.", "section": "4. Experiments"}, {"content": "| Metric | LLaMA3.1 (8B) | Llama-Mesh (8B) | LLaMA3.2 (3B) | LLaMA3.2 (1B) |\n|---|---|---|---|---|\n| MMLU (5-shot) | 66.07 | 61.74 | 59.44 | 44.17 |\n| PIQA (0-shot) | 81.01 | 79.16 | 75.52 | 74.10 |\n| Hellaswag (0-shot) | 79.19 | 77.35 | 70.47 | 60.80 |\n| GSM8K (8-shot) | 77.18 | 62.09 | 66.94 | 34.27 |", "caption": "Table 3: \nDoes Llama-Mesh preserve language capabilities? We report the performance of Llama-Mesh (8B) and compare it with base models of different sizes: LLaMA3.1 (8B), LLaMA3.2 (3B), and LLaMA3.2 (1B). The metrics include MMLU (5-shot), PIQA (0-shot), HellaSwag (0-shot), and GSM8K (8-shot), which assess the model\u2019s general knowledge, commonsense reasoning, and mathematical problem-solving abilities. Takeaway: Our method (in the blue column), after being fine-tuned to generate OBJ files, maintains language understanding and reasoning capabilities comparable to the base model while extending its functionality to 3D mesh generation.", "description": "This table compares the performance of Llama-Mesh (8B) with several baseline LLMs of different sizes on various language understanding benchmarks.  These benchmarks (MMLU, PIQA, HellaSwag, GSM8K) test general knowledge, common sense reasoning, and mathematical problem-solving skills.  The results show that Llama-Mesh, despite being fine-tuned for 3D mesh generation, maintains comparable language understanding and reasoning abilities to the baseline models.", "section": "4.2.2 Language and Conversational Abilities"}]