[{"figure_path": "https://arxiv.org/html/2411.09595/x2.png", "caption": "Figure 1: \nAn illustration of our method, Llama-Mesh, which enables the generation of 3D meshes from human instructions via a conversational interface. Users provide textual prompts, and the model responds with both text and 3D mesh outputs, facilitating interactive 3D content creation. Llama-Mesh allows large language models to generate and interpret 3D meshes from text directly, seamlessly unifying language and 3D modalities within a single model.", "description": "Llama-Mesh is a novel method that allows users to generate 3D meshes through conversational interaction with a language model.  The user provides a text prompt describing the desired 3D object. The model then responds by generating both a textual description and the 3D mesh itself, directly in OBJ format. This seamless integration of text and 3D modalities within a single model is a key feature of Llama-Mesh, enabling interactive 3D content creation.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.09595/x3.png", "caption": "Figure 2: Overview of our method. Llama-Mesh unifies text and 3D mesh in a uniform format by representing the numerical values of vertex coordinates and face definitions of a 3D mesh as plain text. Our model is trained using text and 3D interleaved data end-to-end. Therefore, with a single, unified model, we can generate both text and 3D meshes.", "description": "LLaMA-Mesh processes both text and 3D mesh data in a unified manner.  Instead of using separate encodings, it represents the numerical vertex coordinates and face definitions of a 3D mesh as plain text. This allows for seamless integration with large language models (LLMs). The model is trained end-to-end on interleaved text and 3D mesh data, enabling it to generate both text and 3D mesh outputs from a single model. The figure visually depicts this process.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.09595/x4.png", "caption": "Figure 3: \nGallery of generations from Llama-Mesh. We can generate high-quality and diverse meshes with artist-like created topology.", "description": "This figure showcases a variety of 3D models generated by the LLaMA-Mesh model.  The models demonstrate the model's ability to produce high-quality, diverse meshes with complex, artistic-style topologies, highlighting its advanced capabilities in 3D mesh generation. The examples illustrate the range of shapes and forms that the model can create, showcasing its versatility.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.09595/x5.png", "caption": "Figure 4: \nIllustration of our 3D representation approach. Left: A snippet of an OBJ file represented as plain text, containing vertex (v) and face (f) definitions. Right: The 3D object rendered from the OBJ file. We enable the LLM to process and generate 3D meshes by converting the mesh data into a textual format.", "description": "This figure illustrates how the authors represent 3D mesh data as plain text for processing by large language models (LLMs).  The left panel shows a snippet of an OBJ file (a common text-based 3D model format) which contains vertex coordinates (v) and face definitions (f). The numerical values are treated as text sequences. The right panel displays the 3D object that is rendered from this textual representation of the OBJ file. This demonstrates how the method converts mesh data into a format that LLMs can directly process, eliminating the need for complex tokenization schemes or vocabulary expansion.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.09595/x6.png", "caption": "Figure 5: \nIllustration of our vertex quantization method. Top: The original OBJ file represents vertex coordinates in decimal values, splitting a single coordinate into several tokens. Bottom: After quantization, we represent the vertices as integers containing fewer tokens and are processed by LLM more efficiently.", "description": "This figure illustrates the vertex quantization method used to improve the efficiency of processing 3D mesh data with LLMs.  The top panel shows how vertex coordinates are originally represented as floating-point numbers in the OBJ file format, leading to long token sequences that are inefficient for LLMs. The bottom panel demonstrates that after quantization, the coordinates are represented as integers using fewer tokens, enabling more efficient processing by the LLM.", "section": "3.1 3D Representation"}, {"figure_path": "https://arxiv.org/html/2411.09595/x7.png", "caption": "Figure 6: \nIllustration of mesh generation capability from an LLM without finetuning. Left: results from ChatGPT-4o. Right: results from LLaMA 3.1 8B-Instruct. Pretrained LLMs can generate simple 3D objects in text format; however, mesh quality and complexity are often unsatisfactory. OBJ files from the internet may vary slightly in format. The [\u2026] indicates omitted text.", "description": "This figure demonstrates the zero-shot mesh generation capabilities of different pretrained LLMs.  The left panel shows the output from ChatGPT 40, and the right panel shows the output from LLaMA 3.1 8B-Instruct.  Both models were prompted to generate a 3D mesh in OBJ format without any prior fine-tuning on 3D data. While the LLMs can generate simple 3D objects, the results highlight limitations in terms of mesh quality and complexity, demonstrating the need for fine-tuning to achieve high-quality 3D mesh generation.  The ellipsis (...) indicates that parts of the generated OBJ files have been omitted for brevity.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.09595/x8.png", "caption": "Figure 7: More dialog results. Llama-Mesh achieves several new tasks, including mesh generation and understanding, while completing other tasks like the original LLM. [\u2026]: we omit some text to make the snippet fit into the page.", "description": "Figure 7 showcases Llama-Mesh's expanded capabilities beyond the original LLaMA model.  It demonstrates the model's ability to perform novel tasks such as 3D mesh generation and understanding, in addition to maintaining its proficiency in tasks like text generation and mathematical problem-solving. Examples show interactive dialogues where users describe 3D objects, request mesh creation, ask for explanations of provided meshes, and even inquire about building a wooden house.  The examples highlight the model's capacity to seamlessly integrate 3D processing with its existing language and reasoning capabilities.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.09595/x10.png", "caption": "Figure 8: Training dataset curated for Llama-Mesh. We use a combination of rule-based methods in (a) and (b) and LLM-augmented methods in (c) and (d) to construct an SFT dataset for mesh generation and understanding. <start/end of mesh> is shown here for illustration only and does not appear in the training data.", "description": "Figure 8 illustrates the dataset used to fine-tune the Llama-Mesh model.  The dataset combines rule-based and LLM-augmented approaches to generate a supervised fine-tuning (SFT) dataset for both mesh generation and mesh understanding tasks.  Rule-based methods are shown in (a) and (b), while LLM-augmented methods are in (c) and (d).  Note that the '<start of mesh>' and '<end of mesh>' tags are for illustrative purposes only and are not part of the actual training data.", "section": "3.3 3D-task Finetuning"}, {"figure_path": "https://arxiv.org/html/2411.09595/x11.png", "caption": "Figure 9: \nTraining loss of Llama-Mesh.\nThe model adapts quickly to the new modality. We do not observe loss instabilities during training.\nTotal training time comparisons are in Table\u00a02.", "description": "The plot shows the training loss curve for the LLAMA-Mesh model.  The rapid decrease in loss indicates that the model quickly learned to generate 3D meshes, adapting effectively to this new modality. Notably, there are no significant fluctuations or instabilities in the loss, suggesting a stable and consistent training process.  Table 2 provides a quantitative comparison of the total training time taken by the model, compared to other approaches.", "section": "4. Experiments"}]