[{"figure_path": "https://arxiv.org/html/2411.15611/extracted/6018876/img/moongate/1.png", "caption": "(a) CLIP (B) Top-3 zero-shot predictions: (\u201cTriumphal Arch\u201d, \u201cStone Wall\u201d, \u201cSteel Arch Bridge\u201d).\nCLIP (B) w/ Knowledge Transfer Top-3 zero-shot predictions: (\u201cMoongate\u201d, \u201cTriumphal Arch\u201d, \u201cStone Wall\u201d)", "description": "This figure shows the effectiveness of Knowledge Transfer in introducing new concepts to a pre-trained CLIP model.  The image depicts a 'Moongate', which was not in the model's training data. Subfigure (a) shows that a standard CLIP model (CLIP (B)) incorrectly identifies the Moongate as a 'Triumphal Arch', 'Stone Wall', or 'Steel Arch Bridge'.  However, after applying the Knowledge Transfer method described in the paper (CLIP (B) w/ Knowledge Transfer), the model now correctly identifies the Moongate as a 'Moongate', indicating that the model successfully learned a novel concept from a textual description alone.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.15611/extracted/6018876/img/tonometer/4.png", "caption": "(b) CLIP (L) Top-3 zero-shot predictions: (\u201cCocktail Shaker\u201d, \u201cOdometer\u201d, \u201cDragonfly\u201d).\nCLIP (L) w/ Knowledge Transfer Top-3 zero-shot predictions: (\u201cTonometer\u201d, \u201cCocktail Shaker\u201d, \u201cEspresso Maker\u201d)", "description": "This figure shows the top three predictions made by a large CLIP model (CLIP ViT-L/14) for two different scenarios. In the first scenario (before Knowledge Transfer), the model is given an image and makes predictions based on its pre-trained knowledge. The predictions are \"Cocktail Shaker\", \"Odometer\", and \"Dragonfly\". The second scenario demonstrates the improvement achieved through Knowledge Transfer. The same model is provided with a textual description of the object instead of an image.  The updated predictions are \"Tonometer\", \"Cocktail Shaker\", and \"Espresso Maker\", showcasing the model's ability to correctly identify the object after it is provided with the textual description.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.15611/x1.png", "caption": "Figure 1: Knowledge Transfer can introduce novel concepts in a multimodal model, by leveraging prior visual knowledge of the visual encoder and a textual description of the target concept. In the example, a CLIP model\u00a0[45] learns the concepts Moongate and Tonometer, without using any real image, while retaining a good accuracy on general zero-shot classification (58.10% vs 56.43% and 70.79% vs 70.61% on ImageNet-1k).", "description": "This figure demonstrates the core concept of Knowledge Transfer, a method for teaching a multimodal model new concepts using only textual descriptions.  Leveraging the pre-existing knowledge of low-level visual features within a pre-trained visual encoder (like CLIP), the model can create visual representations of unseen concepts based solely on text.  The example shows a CLIP model successfully learning the concepts of \"Moongate\" and \"Tonometer\",  without access to any actual images. Importantly, the model maintains its good accuracy in general zero-shot image classification on ImageNet-1k (demonstrated by the percentage comparisons of 58.10% vs 56.43% and 70.79% vs 70.61%). This highlights the efficiency and effectiveness of the proposed knowledge transfer approach.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.15611/x2.png", "caption": "Figure 2: Graphical overview of Knowledge Transfer. Starting from a textual description of the target concept, we synthesize images via model inversion (left) then, using an image-text matching loss, we fine-tune the visual encoder to match the concept (right). In this way, we leverage prior knowledge contained in the model (from pre-training) to learn novel concepts.", "description": "This figure illustrates the two-stage process of Explicit Knowledge Transfer.  The left side shows the model inversion stage. Starting with a textual description of a new concept (e.g., 'A moongate is...'), the model inverts this text into a corresponding image representation.  The right side depicts the fine-tuning stage. The synthesized image and its corresponding text description are then used to fine-tune the visual encoder of a pre-trained model (like CLIP).  The fine-tuning process refines the model's ability to connect low-level visual features (already learned during pre-training) with the high-level semantic understanding provided by the textual description, effectively allowing the model to learn the new concept without ever seeing an actual image of it. This approach leverages the existing knowledge embedded in the pre-trained model to efficiently learn new visual concepts.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.15611/x3.png", "caption": "(a) Moongate. Caption: A perfectly circular archway built from uniformly cut stones or bricks, set into a larger wall. It forms a smooth circle, framing views of gardens or landscapes beyond, creating a picturesque portal.", "description": "A moongate is a perfectly circular archway, usually made of stone or brick, that is set into a larger wall.  Its perfectly round shape frames views of gardens or landscapes beyond, creating a visually appealing and picturesque portal. This architectural feature is often found in gardens and parks.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2411.15611/extracted/6018876/img/segmentation/breast/4_image.png", "caption": "(b) Tonometer. Caption: A slender, pen-like probe attached to a small base equipped with precise dials and gauges. This tool is often part of a larger medical apparatus, featuring a metallic finish and a refined, professional appearance.", "description": "The image shows a tonometer, a medical device used to measure intraocular pressure.  It consists of a slender, pen-like probe connected to a small base with dials and gauges. The instrument typically has a metallic finish and a sleek, professional appearance, often integrating into larger ophthalmologic equipment.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2411.15611/extracted/6018876/img/segmentation/breast/4_gt_mask.png", "caption": "Figure 3: Example of inverted images (top) and real images (bottom) from rare concepts that CLIP struggles to classify correctly.", "description": "Figure 3 presents a comparison of images generated through model inversion and real-world images.  The 'inverted images' (top row) were created by the model attempting to reconstruct visual representations based solely on textual descriptions of rare concepts. These concepts are difficult for CLIP, a pre-trained vision-language model, to classify accurately. The real-world images (bottom row) are the actual images corresponding to these same rare concepts. The figure demonstrates the capability and limitations of the model inversion process: it can generate plausible images based on text, but these images don't always perfectly match the appearance of their real-world counterparts, which highlights the challenge in learning rare visual concepts from only textual descriptions.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.15611/extracted/6018876/img/segmentation/breast/4_coarse_baseline_mask.png", "caption": "Figure 4: Comparison of fine-tuning strategies. Fine-tuning both the text and the visual encoders, or just the text encoder leads to a collapse in accuracy. Fine-tuning only the visual encoder correctly aligns prior visual features to the novel concept. A good choice of learning rate leads to higher accuracy on the novel concept (target) while limiting catastrophic forgetting on previous tasks (imagenet).", "description": "This figure compares three different fine-tuning strategies for a model learning new visual concepts from text descriptions.  The x-axis represents the learning rate used during fine-tuning, and the y-axis shows the accuracy achieved on both the new concept (target) and existing concepts from the ImageNet dataset (imagenet). The three strategies are: 1) Fine-tuning both the text and visual encoders, 2) Fine-tuning only the text encoder, and 3) Fine-tuning only the visual encoder.  The results demonstrate that fine-tuning both encoders or only the text encoder leads to significantly reduced accuracy on both target and existing tasks (catastrophic forgetting), implying that the model's existing knowledge is disrupted. In contrast, fine-tuning only the visual encoder successfully incorporates the new concept without harming performance on existing concepts.  The optimal learning rate for this strategy balances high accuracy on the new concept with minimal impact on prior knowledge.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.15611/extracted/6018876/img/segmentation/breast/4_baseline_pred_mask.png", "caption": "Figure 5: Ablation study on caption construction for finetuning.", "description": "This ablation study investigates the impact of different captioning strategies during the fine-tuning stage of the Knowledge Transfer process on the accuracy of the model in classifying three rare concepts: Moongate, Tonometer, and Gyroscope. It compares results where the concept name is prepended to the caption versus scenarios where it's absent. The x-axis represents the learning rate used, and the y-axis shows the zero-shot accuracy. The figure illustrates how including the concept name during fine-tuning significantly improves the model's accuracy, indicating the importance of explicitly connecting low-level visual features to the high-level concept being learned.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.15611/extracted/6018876/img/segmentation/breast/4_coarse_mask.png", "caption": "(a) Image", "description": "This figure shows an example of a breast ultrasound image, the corresponding ground truth segmentation mask, the activation map generated by MedCLIP-SAMv2 (baseline), and the final segmentation result (baseline and after knowledge transfer). It visually demonstrates how knowledge transfer improves the segmentation quality by comparing the activation maps and the resulting segmentations.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2411.15611/extracted/6018876/img/segmentation/breast/4_pred_mask.png", "caption": "(b) Ground Truth", "description": "The figure shows a ground truth segmentation mask for a breast ultrasound image.  It highlights the region of the breast tumor, providing a precise delineation of its boundaries for comparison against model-generated segmentations.", "section": "4.4.1. Segmentation"}, {"figure_path": "https://arxiv.org/html/2411.15611/extracted/6018876/img/segmentation/brain/4_image.png", "caption": "(c) M2IB Map\n(Baseline)", "description": "This subfigure shows the Multimodal Information Bottleneck Attribution (M2IB) map generated by the baseline MedCLIP-SAMv2 model before Knowledge Transfer.  The M2IB map highlights the regions of the input medical image that are most relevant for predicting the presence of a specific anatomical structure (in this case, a tumor or nodule). Brighter regions indicate stronger relevance.  It provides a visual representation of the model's attention mechanism and how it focuses on specific image features for the segmentation task.", "section": "4.4.1 Segmentation"}, {"figure_path": "https://arxiv.org/html/2411.15611/extracted/6018876/img/segmentation/brain/4_gt_mask.png", "caption": "(d) Final Segmentation (Baseline)", "description": "This image shows the segmentation results obtained using the baseline MedCLIP-SAMv2 model without any knowledge transfer.  It displays a visual comparison of the model's prediction against the ground truth segmentation mask. The goal is to illustrate the model's performance before any improvements are made using the proposed Knowledge Transfer technique.", "section": "4.4.1. Segmentation"}, {"figure_path": "https://arxiv.org/html/2411.15611/extracted/6018876/img/segmentation/brain/4_coarse_baseline_mask.png", "caption": "(e) M2IB Map\n(Knowledge Transfer)", "description": "Multimodal Information Bottleneck Attribution (M2IB) activation map after applying Knowledge Transfer. This visualization highlights the areas of the image that the model focuses on after being fine-tuned with synthetic images generated from textual descriptions of the target concept, showing how the model's attention has shifted compared to the baseline (pre-transfer) M2IB map.", "section": "4.4.1. Segmentation"}, {"figure_path": "https://arxiv.org/html/2411.15611/extracted/6018876/img/segmentation/brain/4_baseline_pred_mask.png", "caption": "(f) Final Segmentation (Knowledge Transfer)", "description": "The image displays the results of a segmentation task after applying the Knowledge Transfer method.  The segmentation is specifically of a brain tumor (glioma) in a brain MRI scan.  This shows the improved segmentation that results from using the Knowledge Transfer approach as compared to the baseline results. ", "section": "4.3.2 Experiments with MedCLIP"}, {"figure_path": "https://arxiv.org/html/2411.15611/extracted/6018876/img/segmentation/brain/4_coarse_mask.png", "caption": "Figure 6: Qualitative evaluation of knowledge transfer on breast tumor segmentation (UDIAT dataset). We report the top ten most illustrative examples in which knowledge transfer improved segmentation, in terms of DSC.", "description": "This figure shows a qualitative evaluation of how knowledge transfer improves breast tumor segmentation using the UDIAT dataset.  For ten of the most visually clear examples, it presents a comparison between the baseline segmentation results (without knowledge transfer) and those obtained after applying knowledge transfer. The comparison is made using the Dice Similarity Coefficient (DSC) metric, a common measure of overlap between the predicted segmentation and the ground truth. The images show the original image, the ground truth segmentation, the segmentation prediction from the baseline model, and the improved segmentation after knowledge transfer. This visualization allows readers to directly assess the improvement achieved through the application of the knowledge transfer method.", "section": "4.4.1. Segmentation"}, {"figure_path": "https://arxiv.org/html/2411.15611/extracted/6018876/img/segmentation/brain/4_pred_mask.png", "caption": "(a) Image", "description": "This figure shows a qualitative evaluation of knowledge transfer on breast tumor segmentation (UDIAT dataset).  The image (a) is an example ultrasound image, (b) shows the corresponding ground truth segmentation, (c) displays the multi-modal information bottleneck attribution (M2IB) map from the baseline MedCLIP-SAMv2 model, (d) is the resulting segmentation from the baseline model, (e) shows the M2IB map after applying knowledge transfer, and (f) is the final segmentation resulting from the model after knowledge transfer.  The figure highlights cases where applying knowledge transfer led to improvements in segmentation accuracy, as measured by the Dice-S\u00f8rensen Coefficient (DSC).", "section": "4.3.2 Experiments with MedCLIP"}]