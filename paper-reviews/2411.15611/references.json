{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces the CLIP model, a foundational model for the work presented, demonstrating the effectiveness of cross-modal learning from natural language supervision."}, {"fullname_first_author": "Wonjae Kim", "paper_title": "VILT: Vision-and-Language Transformer without Convolution or Region Supervision", "publication_date": "2021-07-01", "reason": "ViLT is another important foundational model leveraged in the paper, showing the potential of Vision-Language Transformers for zero-shot and few-shot learning."}, {"fullname_first_author": "Alexander Kirillov", "paper_title": "Segment Anything", "publication_date": "2023-10-01", "reason": "The Segment Anything Model (SAM) is crucial for the paper's segmentation experiments, enabling zero-shot segmentation by leveraging the visual features from a pre-trained visual encoder."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "Latent diffusion models are used in the image inversion process, a core part of the proposed method, enabling the generation of realistic images from textual descriptions."}, {"fullname_first_author": "Jiahui Yu", "paper_title": "CoCa: Contrastive captioners are image-text foundation models", "publication_date": "2022-01-01", "reason": "The CoCa model is used for the paper's captioning experiments and demonstrates the potential of contrastive learning for image-text tasks."}]}