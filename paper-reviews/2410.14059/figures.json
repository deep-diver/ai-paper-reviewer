[{"figure_path": "2410.14059/figures/figures_2_0.png", "caption": "Figure 1: Overview framework of the UCFE Benchmark.", "description": "The figure illustrates the overall framework of the User-Centric Financial Expertise (UCFE) benchmark, showing its key components and workflow.", "section": "1 Introduction"}, {"figure_path": "2410.14059/figures/figures_7_0.png", "caption": "Figure 4: The evaluation pipeline of the UCFE Benchmark involves the following steps: \u2460 selecting the model and task, \u2461 generating dialogues between the user and AI assistant via a user simulator, \u2462 creating evaluation prompts based on source information to assess model performance, \u2463 pairwise comparison of dialogue outputs by evaluators, aligned with human expert judgments, and \u2464 computing Elo scores based on win-loss outcomes.", "description": "The figure illustrates the evaluation pipeline of the UCFE benchmark, showing the steps involved in evaluating large language models' performance on financial tasks using a user simulator, human evaluators, and Elo rating system.", "section": "5 UCFE Benchmark"}, {"figure_path": "2410.14059/figures/figures_8_0.png", "caption": "Figure 4: The evaluation pipeline of the UCFE Benchmark involves the following steps: \u2460 selecting the model and task, \u2461 generating dialogues between the user and AI assistant via a user simulator, \u2462 creating evaluation prompts based on source information to assess model performance, \u2463 pairwise comparison of dialogue outputs by evaluators, aligned with human expert judgments, and \u2464 computing Elo scores based on win-loss outcomes.", "description": "The figure illustrates the five-stage evaluation pipeline of the UCFE benchmark, showing the process from selecting models and tasks to computing Elo scores based on human evaluation.", "section": "5 UCFE Benchmark"}, {"figure_path": "2410.14059/figures/figures_15_0.png", "caption": "Figure 4: The evaluation pipeline of the UCFE Benchmark involves the following steps: \u2460 selecting the model and task, \u2461 generating dialogues between the user and AI assistant via a user simulator, \u2462 creating evaluation prompts based on source information to assess model performance, \u2463 pairwise comparison of dialogue outputs by evaluators, aligned with human expert judgments, and \u2464 computing Elo scores based on win-loss outcomes.", "description": "The figure illustrates the five-step evaluation pipeline of the UCFE benchmark, showing the process from model and task selection to final Elo score computation based on human expert judgments.", "section": "5 UCFE Benchmark"}]