[{"figure_path": "2410.14059/figures/figures_2_0.png", "caption": "Figure 1: Overview framework of the UCFE Benchmark.", "description": "The figure illustrates the UCFE benchmark framework, showing the user interaction, financial tasks, and the process of evaluating large language models in real-world financial scenarios.", "section": "1 Introduction"}, {"figure_path": "2410.14059/figures/figures_7_0.png", "caption": "Figure 4: The evaluation pipeline of the UCFE Benchmark involves the following steps: \u2460 selecting the model and task, \u2461 generating dialogues between the user and AI assistant via a user simulator, \u2462 creating evaluation prompts based on source information to assess model performance, \u2463 pairwise comparison of dialogue outputs by evaluators, aligned with human expert judgments, and \u2464 computing Elo scores based on win-loss outcomes.", "description": "The figure illustrates the five-stage evaluation pipeline of the UCFE benchmark, highlighting the roles of user simulator, LLM as AI assistant, evaluator, and human expert in assessing model performance.", "section": "5 UCFE Benchmark"}, {"figure_path": "2410.14059/figures/figures_8_0.png", "caption": "Figure 4: The evaluation pipeline of the UCFE Benchmark involves the following steps: \u2460 selecting the model and task, \u2461 generating dialogues between the user and AI assistant via a user simulator, \u2462 creating evaluation prompts based on source information to assess model performance, \u2463 pairwise comparison of dialogue outputs by evaluators, aligned with human expert judgments, and \u2464 computing Elo scores based on win-loss outcomes.", "description": "The figure illustrates the five-step evaluation pipeline of the UCFE benchmark, showing the process from model and task selection to final Elo score computation.", "section": "5 UCFE Benchmark"}, {"figure_path": "2410.14059/figures/figures_15_0.png", "caption": "Figure 4: The evaluation pipeline of the UCFE Benchmark involves the following steps: \u2460 selecting the model and task, \u2461 generating dialogues between the user and AI assistant via a user simulator, \u2462 creating evaluation prompts based on source information to assess model performance, \u2463 pairwise comparison of dialogue outputs by evaluators, aligned with human expert judgments, and \u2464 computing Elo scores based on win-loss outcomes.", "description": "The figure illustrates the five-stage evaluation pipeline of the UCFE benchmark, showing the process from selecting models and tasks to computing Elo scores based on human evaluations.", "section": "5 UCFE Benchmark"}]