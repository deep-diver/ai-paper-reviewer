{"references": [{" publication_date": "2023", "fullname_first_author": "Wu", "paper_title": "Bloomberggpt: A large language model for finance", "reason": "This paper is highly relevant to the topic of the current paper as it introduces a large language model specifically trained for finance, highlighting the growing trend and potential of LLMs in this domain. The methodology and results presented in this paper can serve as a benchmark for future research and could directly influence the development of the proposed user-centric benchmark in the current work.  Further, its discussion of challenges in developing real-world applications in finance is directly relevant to the stated goals of the current paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Huang", "paper_title": "Finbert: A large language model for extracting information from financial text", "reason": "This paper focuses on creating a large language model specifically designed for financial text analysis.  This directly relates to the current paper's goal of creating a more comprehensive benchmark for evaluating LLMs in finance. The techniques used to evaluate and compare the model's performance in this research provide insights that could be valuable in the current study's methodology development. Its findings regarding the effectiveness of large language models for financial text analysis are relevant to the context of the current paper. ", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Kim", "paper_title": "Financial statement analysis with large language models", "reason": "This paper directly addresses the application of LLMs to a specific financial task, which is directly relevant to the main theme of the current paper. The challenges and opportunities highlighted in this paper in solving specialized financial tasks are directly relevant to the context of the current paper and could help to inform the design of the proposed benchmark. Understanding how LLMs handle specific financial tasks, as explored in this paper, is critical for building a robust benchmark. ", "section_number": 1}, {" publication_date": "2023a", "fullname_first_author": "Li", "paper_title": "Cfgpt: Chinese financial assistant with large language model", "reason": "This paper presents a large language model specifically designed for Chinese financial tasks.  Its focus on a specific language context demonstrates an awareness of the variability that exists in models.  The methodology employed for creating and evaluating this model provides useful insights for creating a user-centric benchmark, especially as the model evaluates an AI assistant designed to directly support end-users. ", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Xie", "paper_title": "Pixiu: A large language model, instruction data and evaluation benchmark for finance", "reason": "This paper is highly relevant because it introduces a benchmark specifically designed for evaluating LLMs in finance.  The benchmark's design and evaluation methodology provide valuable insights for the current research.  Analyzing the strengths and weaknesses of this existing benchmark allows for a more informed design of the new framework proposed in this paper.  Its insights could be used to avoid past pitfalls and to develop a superior benchmark.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Zhang", "paper_title": "Fineval: A chinese financial domain knowledge evaluation benchmark for large language models", "reason": "This paper is highly relevant because it focuses on a benchmark for evaluating LLMs in finance. The paper's discussion of challenges in evaluating LLMs' financial knowledge provides valuable insights for the current study.  The benchmark's design and evaluation metrics could inform the design of the proposed benchmark. The paper's detailed analysis of the strengths and weaknesses of existing methods are directly relevant to the current work's goal.", "section_number": 2}, {" publication_date": "2023b", "fullname_first_author": "Zhang", "paper_title": "Llama3-Xuan Yuan3-70B-Chat", "reason": "This paper is highly relevant because it focuses on a specific large language model designed for finance and its performance on financial tasks.  The paper's findings regarding the model's performance on financial tasks provide valuable insights for the current study. The current work could compare its own benchmark's results to this model's performance and highlight any differences or similarities.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuan", "paper_title": "R-judge: Benchmarking safety risk awareness for llm agents", "reason": "This paper directly addresses the safety aspects of LLMs in financial tasks.  This is relevant to the current study as it highlights the importance of assessing the risks associated with using LLMs in financial applications.  The benchmark's design and evaluation methodology could inform the design of the proposed benchmark in this paper.  Analyzing its methods to assess risks in LLMs could improve this paper's design.", "section_number": 2}, {" publication_date": "2024a", "fullname_first_author": "Yue", "paper_title": "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi", "reason": "This paper is highly relevant because it presents a multimodal benchmark for evaluating LLMs.   The paper's focus on multimodal inputs directly relates to the current paper's goal of creating a benchmark capable of handling diverse data types in financial tasks.   The benchmark's design and evaluation methodology could inform the design of the new framework proposed in this paper.", "section_number": 2}, {" publication_date": "2024b", "fullname_first_author": "Yue", "paper_title": "Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark", "reason": "This paper is relevant because it introduces an improved multimodal benchmark for evaluating LLMs.  The advancements in multimodal benchmark design and evaluation presented in this paper are valuable in creating the proposed user-centric benchmark.  Learning from the design decisions and insights provided could contribute to the current paper's objective of creating a superior financial task benchmark.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Xie", "paper_title": "FLARE", "reason": "This paper is highly relevant to the current paper as it introduces a benchmark specifically designed for evaluating LLMs in finance.  Analyzing the strengths and weaknesses of this existing benchmark informs the design of the new framework proposed in this paper.  Understanding how this benchmark handles various aspects of financial tasks provides valuable insights that help improve the new benchmark. ", "section_number": 2}, {" publication_date": "2023a", "fullname_first_author": "Li", "paper_title": "Cfgpt: Chinese financial assistant with large language model", "reason": "This paper presents a large language model specifically designed for Chinese financial tasks.  Its focus on a specific language context demonstrates an awareness of the variability that exists in models.  The methodology employed for creating and evaluating this model provides useful insights for creating a user-centric benchmark, especially as the model evaluates an AI assistant designed to directly support end-users.", "section_number": 2}, {" publication_date": "2023b", "fullname_first_author": "Li", "paper_title": "Are chatgpt and gpt-4 general-purpose solvers for financial text analytics?", "reason": "This paper directly addresses the application of LLMs to financial text analysis.  Its findings regarding the capabilities and limitations of LLMs in this domain are directly relevant to the context of the current paper and could help to inform the design of the proposed benchmark. Understanding the strengths and limitations of general-purpose LLMs in financial tasks helps determine which aspects to focus on for the benchmark. ", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuan", "paper_title": "FLARE", "reason": "This paper is highly relevant because it introduces a benchmark specifically designed for evaluating LLMs in finance.  Analyzing the strengths and weaknesses of this existing benchmark informs the design of the new framework proposed in this paper.  Understanding how this benchmark handles various aspects of financial tasks provides valuable insights that help improve the new benchmark. ", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yang", "paper_title": "Fingpt: Open-source financial large language models", "reason": "This paper is highly relevant because it introduces a large language model specifically designed for financial tasks. The paper's discussion of challenges in developing LLMs for finance provides valuable insights for the current study.  The model's design and performance data inform the creation of a more comprehensive user-centric benchmark. The paper's insights into the challenges and opportunities associated with developing and applying LLMs in finance directly inform the present paper's goals.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Xie", "paper_title": "Open-finllms: Open multimodal large language models for financial applications", "reason": "This paper is relevant because it introduces a set of open multimodal large language models specifically designed for financial applications. This paper is directly relevant to the current paper as it explores the potential of LLMs for handling various data types and user interactions within the financial domain.  The insights provided could improve the design of the proposed user-centric benchmark.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Nie", "paper_title": "A survey of large language models for financial applications: Progress, prospects and challenges", "reason": "This paper provides a comprehensive overview of the current state of LLMs in finance, highlighting both progress and challenges. Its detailed analysis of challenges directly addresses the motivations of the current paper and could be used to justify the design choices of the proposed benchmark.  Understanding the current limitations and prospects in the field provides insights into the type of benchmark that will be most helpful in advancing the field. ", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Kaplan", "paper_title": "Scaling laws for neural language models", "reason": "This paper is foundational to understanding the scaling properties of LLMs.  The current work is relevant as it investigates the impact of model size on performance within a specific financial domain.  Understanding how LLMs scale helps to inform the selection of models for the benchmark and to interpret the results. The insights on scaling laws directly influence the design and interpretation of the benchmark results. ", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Ruan", "paper_title": "Observational scaling laws and the predictability of language model performance", "reason": "This paper focuses on the scaling laws of LLMs, a highly relevant topic since it directly impacts the design of the benchmark in this work. The findings on the predictability of LLM performance based on size inform the selection of models used in the benchmark, helping optimize for performance. It's important to understand the relationship between model size and capability when comparing models in the benchmark.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Liu", "paper_title": "LLm-as-judge framework for evaluation", "reason": "This paper introduces the LLM-as-judge framework, which is the core methodology used in evaluating the UCFE benchmark.   The paper provides a justification for this approach and discusses its strengths and limitations.  A thorough understanding of this methodology is critical to interpreting the results presented in this paper.", "section_number": 5}, {" publication_date": "2010", "fullname_first_author": "Hvattum", "paper_title": "Using elo ratings for match result prediction in association football", "reason": "This paper is highly relevant because it introduces the Elo rating system, which is used in this paper for evaluating the LLMs.  The Elo system is a robust method for comparing the relative performance of multiple models, especially in dynamic and competitive settings.   Understanding how the Elo rating system works and its advantages is crucial for interpreting the results presented in this paper.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Chiang", "paper_title": "Chatbot arena: An open platform for evaluating llms by human preference", "reason": "This paper introduces a framework for evaluating LLMs using human preferences, which is relevant to the current work's hybrid approach.  The insights into human preference evaluation inform the methodology for comparing benchmark results with human judgments, ultimately increasing the benchmark's validity.  Comparing human preference methods across different tasks contributes to the development of a more effective user-centric framework.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Chiang", "paper_title": "Over-reasoning and redundant calculation of large language models", "reason": "This paper is highly relevant to the current paper because it addresses a common issue in large language models: over-reasoning and generating unnecessarily lengthy responses.  The insights are relevant to the design of the benchmark, particularly when handling complex financial tasks where conciseness and accuracy are crucial.  The authors' findings inform how to evaluate outputs efficiently and minimize the impact of verbose responses on evaluation metrics. ", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Li", "paper_title": "Mitigating positional bias in llm evaluations", "reason": "This paper introduces a method to mitigate positional bias in LLM evaluations, which is highly relevant to the current study's methodology.  The paper presents a technique to reduce bias during evaluation.  This is crucial for ensuring that the benchmark's results are not skewed due to the order of input or presentation of information during evaluation. The paper's findings improve the fairness of model comparisons in the benchmark.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Talboy", "paper_title": "Challenging the appearance of machine intelligence: Cognitive bias in llms and best practices for adoption", "reason": "This paper addresses the issue of cognitive bias in LLMs which is highly relevant to the current work's evaluation methodology. The paper highlights potential biases introduced by human evaluators in assessing LLMs and discusses strategies to mitigate such biases.  This directly informs the design of the evaluation process in this paper and enhances its accuracy and fairness.  The findings on bias mitigation are critical for ensuring the validity of the benchmark.", "section_number": 5}]}