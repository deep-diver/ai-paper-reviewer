[{"figure_path": "https://arxiv.org/html/2412.03069/x1.png", "caption": "Figure 1: Multimodal Understanding Results with TokenFlow. We demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2% average improvement.", "description": "TokenFlow significantly outperforms LLaVA-1.5 13B on several multimodal understanding benchmarks.  The chart visually represents this improvement across different benchmark datasets (MME-Perception, SEEDBench, MM-Vet, VQAv2, TextVQA, GQA).  Importantly, this is the first demonstration that a model using discrete visual input (as opposed to continuous representations) achieves better performance than LLaVA-1.5 13B, showcasing a 7.2% average performance increase.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.03069/x2.png", "caption": "Figure 2: Visual Generation Results with TokenFlow. We present diverse 256\u00d7256 results across various styles, subjects, and scenarios.", "description": "This figure showcases the diverse image generation capabilities of the TokenFlow model.  It displays a variety of 256x256 pixel images, demonstrating the model's ability to generate images in different artistic styles, depicting a wide range of subjects and scenes. The visual variety highlights the model's flexibility and robustness across diverse image generation tasks.", "section": "3. Visual Generation with TokenFlow"}, {"figure_path": "https://arxiv.org/html/2412.03069/x3.png", "caption": "Figure 3: Overview of TokenFlow. We incorporate dual encoders and codebooks with a shared mapping, enabling the joint optimization of high-level semantics and low-level pixel details. For a given input image, distances dsemsubscript\ud835\udc51semd_{\\text{sem}}italic_d start_POSTSUBSCRIPT sem end_POSTSUBSCRIPT and dpixsubscript\ud835\udc51pixd_{\\text{pix}}italic_d start_POSTSUBSCRIPT pix end_POSTSUBSCRIPT are calculated from the pixel-level and semantic-level codebooks, respectively, with the final codebook index and features determined by minimizing the weighted sum dsem+wdis\u22c5dpixsubscript\ud835\udc51sem\u22c5subscript\ud835\udc64dissubscript\ud835\udc51pixd_{\\text{sem}}+w_{\\text{dis}}\\cdot d_{\\text{pix}}italic_d start_POSTSUBSCRIPT sem end_POSTSUBSCRIPT + italic_w start_POSTSUBSCRIPT dis end_POSTSUBSCRIPT \u22c5 italic_d start_POSTSUBSCRIPT pix end_POSTSUBSCRIPT. The resulting quantized features are independently decoded for both semantic alignment and image reconstruction training, and then concatenated to provide a unified representation for downstream tasks in understanding and generation.", "description": "TokenFlow uses a dual-encoder architecture with two codebooks (semantic and pixel) and a shared mapping mechanism.  The semantic encoder focuses on high-level semantic features, while the pixel encoder captures fine-grained visual details. Both encoders' outputs are quantized using a weighted sum of their distances to the respective codebooks, resulting in a unified index and features that represent both semantic and pixel information. These features are then independently decoded for semantic alignment and image reconstruction before being concatenated into a unified representation used by downstream tasks (multimodal understanding and generation).", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.03069/x4.png", "caption": "Figure 4: Visualization of images clustered by (a) VQKD [35], (b) VQGAN [13], and (c) Our TokenFlow. VQKD clusters exhibit semantic similarity, while VQGAN clusters exhibit low-level similarity (i.e. color). Our TokenFlow can successfully combine both semantic and low-level similarity. Implementation details of image clustering can be found in Sec.\u00a0A.1.", "description": "This figure compares the clustering results of three different image tokenizers: VQKD, VQGAN, and TokenFlow.  VQKD, designed for semantic understanding, groups images based on their semantic similarity, even if they have different visual appearances. VQGAN, focused on image generation, groups images based on low-level visual features like color and texture, even if semantically different. TokenFlow, the proposed method, successfully combines both semantic and low-level visual features in its clustering, demonstrating its ability to capture a richer representation of image content. This is crucial for both image understanding and generation tasks.", "section": "3.2 Unified Image Tokenizer"}, {"figure_path": "https://arxiv.org/html/2412.03069/x5.png", "caption": "Figure 5: Qualitative comparison of different sampling strategies in our framework. (a) Single-pass top-k\ud835\udc58kitalic_k (k\ud835\udc58kitalic_k=1200) and top-p\ud835\udc5dpitalic_p (p\ud835\udc5dpitalic_p=0.8) sampling exhibits inconsistent patterns and artifacts. (b) Our proposed multi-step sampling strategy produces more coherent and visually appealing results. Best zoomed in for details.", "description": "This figure compares two image sampling strategies.  The first uses a single-pass approach with top-k=1200 and top-p=0.8, resulting in images with inconsistent patterns and artifacts.  The second, proposed by the authors, is a multi-step strategy that yields more coherent and visually pleasing results, as shown in the images.  Zooming in on the images reveals more detail.", "section": "3.3. Visual Generation with TokenFlow"}, {"figure_path": "https://arxiv.org/html/2412.03069/x6.png", "caption": "Figure 6: Impact of codebook size on reconstruction quality, class-conditional generation, and multimodal understanding benchmarks. MME is divide by 28 to have the same scale.", "description": "This figure analyzes how varying the codebook size in the TokenFlow model affects its performance across three key areas: image reconstruction quality (measured by FID), class-conditional image generation (measured by FID), and multimodal understanding benchmarks (measured by MME, SEED-Bench, and TextVQA).  The results show the trade-off between these aspects as the codebook size increases. The MME score is divided by 28 for consistent scaling.", "section": "4.5. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.03069/x7.png", "caption": "Figure 7: Comparison of cluster size distributions between VQKD [35], VQGAN [13], and TokenFlow (ours), with a fixed codebook size of 8,192. Analysis performed on 50,000 images from the ImageNet-1k validation set. TokenFlow exhibits significantly smoother distribution compared to others, attributed to our shared mapping design that learns joint distributions of semantic and pixel-level features. This joint learning approach helps maintain high codebook utilization (95%+) even with large-scale codebooks containing over 131K entries.", "description": "This figure compares the distribution of cluster sizes for three different image tokenization methods: VQKD, VQGAN, and TokenFlow.  Using a fixed codebook size of 8,192, the methods were tested on 50,000 images from the ImageNet-1k validation set. The plot shows that TokenFlow's distribution of cluster sizes is significantly more uniform than those of VQKD and VQGAN. This smoother distribution is attributed to TokenFlow's novel shared mapping design, which learns joint distributions of semantic and pixel-level features. This dual-feature learning approach results in consistently high codebook utilization (over 95%), even with significantly larger codebooks (over 131,000 entries). This high utilization rate highlights the efficiency of TokenFlow's architecture and its effectiveness in capturing both high-level semantic information and fine-grained visual detail.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.03069/x8.png", "caption": "Figure 8: Comparison of original images and their reconstructions from quantized semantic features extracted by VQKD\u00a0[35]. The reconstructed images preserve the semantic content but exhibit significant loss of high-frequency details.", "description": "This figure displays a comparison between original images from the ImageNet-1k dataset and their reconstructions from quantized semantic features using VQKD [35].  The reconstruction process, while preserving the overall semantic meaning of the images (e.g., the subject matter), results in a significant loss of fine details and high-frequency information. This demonstrates a key limitation of solely relying on semantic features for image reconstruction; such an approach struggles to capture the sharpness and intricate texture present in the original images.", "section": "3.1. Motivation"}, {"figure_path": "https://arxiv.org/html/2412.03069/x9.png", "caption": "Figure 9: Qualitative comparison of visual generation capabilities between 1B and 7B models. Prompts (from left to right): (1) \u201dA pizza sitting on top of a wooden cutting board\u201d, (2) \u201dTelevision set being held by a hand\u201d, (3) \u201dThe guy is nicely dressed in a suit and tie\u201d, and (4) \u201dA sailing ship rests on waters\u201d. The 7B model demonstrates enhanced quality compared to its 1B counterpart.", "description": "This figure showcases a qualitative comparison of image generation results from two different sized models: a 1B parameter model and a 7B parameter model.  Four different prompts were used to generate the images.  The prompts are: 1) \"A pizza sitting on top of a wooden cutting board\", 2) \"Television set being held by a hand\", 3) \"The guy is nicely dressed in a suit and tie\", and 4) \"A sailing ship rests on waters\". The results clearly demonstrate that the larger, 7B parameter model produces images with significantly higher visual fidelity and detail compared to the smaller model.", "section": "4.4. Visual Generation"}, {"figure_path": "https://arxiv.org/html/2412.03069/x10.png", "caption": "Figure 10: Comparison of image reconstruction quality. (a) Original images. (b) Reconstructions using the base pixel decoder. (c) Reconstructions using the enhanced (2\u00d72\\times2 \u00d7 capacity) decoder. The enhanced decoder demonstrates superior preservation of fine-grained details, particularly in facial details and textual elements.", "description": "This figure compares the image reconstruction quality using different decoders.  The original images are shown in (a). (b) shows reconstructions using the base pixel decoder, which has limitations in preserving fine details. (c) shows the improvement achieved by using an enhanced decoder with double the capacity, resulting in significantly better reconstruction of fine details such as facial features and text.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.03069/x11.png", "caption": "Figure 11: Qualitative comparison of images clustered by VQKD [35], VQGAN [13] and our TokenFlow. VQKD clusters exhibit semantic similarity, while VQGAN clusters exhibit low-level similarity (i.e. color and texture). Our TokenFlow can successfully combine both semantic and low-level similarity (e.g. birds with different background can be mapped into two different index).", "description": "This figure compares the clustering results of three different image tokenizers: VQKD, VQGAN, and TokenFlow.  VQKD, focused on semantic meaning, groups images with similar concepts together regardless of visual details. VQGAN, prioritizing low-level visual features, clusters images based on similarities in color and texture.  TokenFlow, in contrast, successfully combines both semantic and low-level features in its clustering. This allows it to group images that share both conceptual meaning and visual characteristics. The example given highlights this advantage by showing how birds with different backgrounds are clustered together by TokenFlow, reflecting both their semantic (birds) and visual (shared feature) similarities, whereas the other two methods would separate them.", "section": "3.2. Unified Image Tokenizer"}, {"figure_path": "https://arxiv.org/html/2412.03069/x12.png", "caption": "Figure 12: More Visual Generation Results with TokenFlow. We present diverse 256\u00d7256 results across various styles, subjects, and scenarios.", "description": "This figure showcases the diverse image generation capabilities of the TokenFlow model.  It presents a variety of 256x256 pixel images, demonstrating the model's ability to generate images across a wide range of styles, subjects, and scenarios. The examples highlight the model's versatility in generating photorealistic images, artistic renderings, and imaginative compositions.", "section": "3. Visual Generation with TokenFlow"}]