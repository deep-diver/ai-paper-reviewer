[{"content": "|       | HMDB51 | UCF101 | UCF101 |\n|---|---|---|---|\n|       | fine-tune | lin. prob | fine-tune |\n| Random initialization | 18.2 | 8.9 | 51.4 |\n| Static circles | 29.2 | 13.2 | 67.8 |\n| Moving circles | 52.0 | 15.5 | 85.2 |\n| Moving shapes | 56.1 | 20.4 | 86.9 |\n| Moving and transforming shapes | 57.6 | 18.8 | 87.7 |\n| Acc. and transforming shapes | 58.9 | 18.9 | 88.1 |\n| Acc. and transforming textures | 62.4 | 20.9 | 89.4 |\n| Acc. and transforming StyleGAN crops | **64.1** | <span class=\"ltx_text ltx_framed ltx_framed_underline\">25.2</span> | <span class=\"ltx_text ltx_framed ltx_framed_underline\">90.2</span> |\n| Acc. and transforming image crops | **64.1** | 24.8 | **91.3** |\n| UCF101 | <span class=\"ltx_text ltx_framed ltx_framed_underline\">63.0</span> | **48.0** | **91.3** |", "caption": "Table 1: Additional action recognition results (ViT-B). We present the classification accuracy on HMDB51 after fine-tuning and on UCF101 after linear probing/fine-tuning for all the pre-training datasets in our progression and the two baselines.", "description": "This table presents the classification accuracy achieved on two action recognition datasets, HMDB51 and UCF101, using a VideoMAE model (ViT-B).  The model was pre-trained on a series of synthetic video datasets with increasing complexity, reflecting a progression from simple to more realistic video characteristics.  The table shows the performance after fine-tuning on HMDB51 and after either fine-tuning or linear probing on UCF101. This allows for a comparison of the model's performance across different levels of synthetic data realism and training methods, and a comparison to baseline models (random initialization and UCF101 pre-training).", "section": "4.1 Fine-Tuning"}, {"content": "| Configuration | Accuracy (%) |\n|---|---| \n| 300k images | 90.5 |\n| 150k images & 150k StyleGAN | 90.6 |\n| 300k StyleGAN | 90.2 |\n| 300k statistical textures | 89.4 |\n| 1.3M images | 91.3 |\n| Replacing 5% of videos <br> w/ static images | 88.5 |", "caption": "Table 2: Incorporating natural images into training (ViT-B). We ablate different approaches for incorporating natural images during training, and evaluate them on UCF101.", "description": "This table presents the results of experiments evaluating different methods for incorporating natural images into the training process of a ViT-B (Vision Transformer - Base) model.  The goal is to determine the impact of various amounts and ways of including natural images on the model's performance when evaluated on the UCF101 action recognition dataset.  The table shows the accuracy achieved by the model trained with varying configurations,  such as different numbers of natural images (300k, 150k, etc.), and in combination with StyleGAN-generated synthetic textures.", "section": "4.1 Fine-Tuning"}, {"content": "| Configuration | Accuracy (%) |\n|---|---| \n| Static StyleGAN crops | 90.2 |\n| Dynamic StyleGAN crops | 89.2 |\n| Dynamic StyleGAN videos | 68.7 |", "caption": "Table 3: Incorporating synthetic textures into training (ViT-B). Introducing dynamics to the StyleGAN textures does not improve performance.", "description": "This table presents the results of pre-training a ViT-B VideoMAE model on datasets using synthetic StyleGAN textures, comparing static textures to those with added dynamics.  The goal was to determine if introducing movement to the textures improved the model's performance on downstream tasks. The results show that adding dynamics to the StyleGAN textures did not lead to performance improvements, indicating that static StyleGAN textures are sufficient for pre-training in this context.", "section": "3.2 Pre-training Protocol"}, {"content": "| Hyperparameter | Value |\n|---|---| \n| masking ratio | 0.75 |\n| training epochs | 3200 |\n| optimizer | AdamW |\n| base learning | 3e-4 |\n| weight decay | 0.05 |\n| optimizer momentum | \u03b2\u2081=0.9, \u03b2\u2082=0.95 |\n| batch size | 256 |\n| learning rate schedule | cosine decay |\n| warmup epochs | 40 |\n| augmentation | MultiScaleCrop |", "caption": "Table 4: Pre-training settings (ViT-B).", "description": "This table details the hyperparameters used for pre-training the ViT-B (Vision Transformer - Base) model using the VideoMAE (Video Masked Autoencoder) method.  It lists the values for parameters such as masking ratio, number of training epochs, optimizer, base learning rate, weight decay, momentum, batch size, learning rate schedule, warmup epochs, and augmentation techniques.", "section": "3.2 Pre-training Protocol"}, {"content": "| Hyperparameter | Value |\n|---|---| \n| training epochs | 100 |\n| optimizer | AdamW |\n| base learning | 1e-3 |\n| weight decay | 0.05 |\n| optimizer momentum | \\(\\beta_{1}=0.9,\\beta_{2}=0.95\\) |\n| batch size | 256 |\n| learning rate schedule | cosine decay |\n| warmup epochs | 5 |\n| flip augmentation | yes |\n| RandAug | (9, 0.5) |\n| label smoothing | 0.1 |\n| mixup | 0.8 |\n| cutmix | 1.0 |\n| drop path | 0.2 |\n| dropout | 0.0 |\n| layer-wise lr decay | 0.7 |\n| test clips | 5 |\n| test crops | 3 |", "caption": "Table 5: Fine-tuning settings (ViT-B)", "description": "This table details the hyperparameters used for fine-tuning the ViT-B model on the UCF101 dataset.  It includes settings for the optimizer (AdamW), learning rate, weight decay, batch size, learning rate schedule, and data augmentation techniques (flip, RandAug, label smoothing, mixup, cutmix, drop path, and dropout).  These settings were used to evaluate the performance of the VideoMAE model pre-trained on the synthetic datasets.", "section": "3.2 Pre-training Protocol"}, {"content": "| Hyperparameter | Value |\n|---|---| \n| training epochs | 100 |\n| optimizer | AdamW |\n| base learning | 1e-2 |\n| weight decay | 0.0 |", "caption": "Table 6: Linear probing settings (ViT-B)", "description": "This table details the hyperparameters used for the linear probing experiment on the ViT-B model. Linear probing is a method used to evaluate the quality of pre-trained models by adding a linear layer on top of the pre-trained model and training only that new layer.  It shows the settings for the optimization process (optimizer, learning rate, weight decay, etc.), data augmentation, and other relevant parameters used during the linear probing phase.", "section": "3.2 Pre-training Protocol"}, {"content": "| Hyperparameter | Value |\n|---|---| \n| Initial speed range | (1.2, 3.0) |\n| Acceleration speed range | (-0.06, 0.06) |\n| Rotation speed range | (-\u03c0/100, \u03c0/100) |\n| Scale X speed range | (-0.005, 0.005) |\n| Scale Y speed range | (-0.005, 0.005) |\n| Shear X speed range | (-0.005, 0.005) |\n| Shear Y speed range | (-0.005, 0.005) |", "caption": "Table 7: Dataset generation settings", "description": "Table 7 presents the hyperparameters used in generating the synthetic video datasets.  It details the ranges or values for parameters such as initial speed, acceleration, rotation, scaling, and shearing, which control the visual characteristics (movement, transformations) of the objects within the generated videos. These settings are crucial for creating the progression of datasets used in the experiments, offering a controllable and transparent method for studying the effect of progressively complex video features on downstream task performance.", "section": "3.1 PROGRESSION OF VIDEO GENERATION PROCESSES"}, {"content": "| Pre-training Dataset | Accuracy |\n|---|---| \n| Scratch | 68.8 |\n| Accelerating and transforming image crops | 79.1 |\n| Kinetics-400 | 80.7 |", "caption": "Table 8: Results on Kinetics-400 test set\u00a0(Kay et\u00a0al., 2017). The kinetics-400 result is obtained by fine-tuning from the official pretrained VideoMAE checkpoint\u00a0(Tong et\u00a0al., 2022).", "description": "Table 8 presents the results of the Kinetics-400 action recognition task.  The performance of a model fine-tuned on the Kinetics-400 dataset after pre-training on the final synthetic video dataset (accelerating and transforming image crops) is compared to the performance of a model trained from scratch and a model using the official pre-trained VideoMAE weights on Kinetics-400. This comparison demonstrates the effectiveness of the synthetic video dataset in closing the gap between training from scratch and using natural video data for pre-training.", "section": "4.1 FINE-TUNING"}, {"content": "| Dataset configuration | UCF101 |\n|---|---| \n| Moving circles | 84.9 |\n| Moving shapes | 88.3 |\n| Moving and transforming shapes | 88.3 |\n| Accelerating and transforming shapes | 88.6 |\n| Accelerating and transforming textures | 90.9 |", "caption": "Table 9: Additional datasets (ViT-B). Moving objects with slower speed", "description": "This table presents the results of experiments using  Vision Transformer - base (ViT-B) model pre-trained on variations of synthetic video datasets, focusing on the impact of slower object speeds.  The datasets are similar to those described in the main progression of the paper but with object speeds reduced by 50%.  The accuracy is measured on the UCF101 action recognition task after fine-tuning the pre-trained model. This allows for a comparison of performance with the original, faster-moving object datasets, showing the effect of this specific parameter change.", "section": "4.1 Fine-Tuning"}, {"content": "| Dataset configuration | UCF101 |\n|---|---| \n| Dynamic StylaGAN high-greq | 68.7 |\n| Replacing 5% of videos w/ StyleGAN | 88.2 |\n| 150k images & 150k statistical textures | 89.7 |\n| 300k images w/ colored background | 89.9 |\n| 300k images w/ image background | 91.0 |", "caption": "Table 10: Additional datasets (ViT-B). More texture types and more diverse background", "description": "This table presents additional experimental results obtained using variations of the ViT-B model, focusing on the impact of different texture types and background diversity on the model's performance.  Specifically, it explores various configurations, including the use of Dynamic StyleGAN textures, combinations of real images and synthetic textures, and the effect of colored or image backgrounds, highlighting their contributions to action recognition accuracy on the UCF101 dataset.", "section": "4.1 Fine-tuning"}, {"content": "| Dataset configuration | UCF101 |\n|---|---| \n| Accelerating and transforming shapes, 25% w/ UCF101 | 90.4 |\n| Accelerating and transforming shapes, 75% w/ UCF101 | 90.6 |\n| Accelerating and transforming image crops, 50% w/ UCF101 | 92.0 |", "caption": "Table 11: Additional datasets (ViT-B). Mix with real videos", "description": "This table presents the results of additional experiments conducted to evaluate the impact of mixing real-world video data from the UCF101 dataset with synthetic data during the pre-training phase. Three different combinations of real and synthetic data are tested, varying the proportion of real video data included.  The experiments aim to assess whether including real video clips alongside synthetic videos improves downstream performance on the action recognition task using the ViT-B model.", "section": "4.2 Distribution Shift"}, {"content": "| Dataset configuration | UCF101 |\n|---|---| \n| Statistical textures | 88.9 |\n| Statistical textures w/ colored background | 87.8 |\n| Moving Dynamic StyleGAN crops | 87.5 |\n| 300k image crops | 90.1 |\n| 150k image crops & 150 statistical textures | 89.2 |\n| 300k image crops w/ colored background | 89.5 |\n| 300k image crops w/ image background | 89.5 |\n| 1.3M image crops | 89.8 |", "caption": "Table 12: Additional datasets (ViT-B). Saturated textures", "description": "This table presents the UCF101 classification accuracy achieved by fine-tuning a ViT-B model pre-trained on various datasets with saturated textures.  These datasets explore different texture types and image background variations to assess their impact on model performance. The results highlight the effect of altering texture saturation and the inclusion of colored or image backgrounds on downstream action recognition accuracy.", "section": "5. DATASETS ANALYSIS"}]