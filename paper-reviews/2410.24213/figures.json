[{"figure_path": "https://arxiv.org/html/2410.24213/x1.png", "caption": "Figure 1: Samples from our progression of video generation models and additionally included image datasets. We present 4 frames from timestamps t\u2208{0,10,20,30}\ud835\udc610102030t\\in\\{0,10,20,30\\}italic_t \u2208 { 0 , 10 , 20 , 30 } of a randomly sampled video from each of our generated datasets, and UCF101 (left to right).", "description": "Figure 1 visualizes the progression of video datasets generated synthetically, culminating in datasets that incorporate natural image crops. Each dataset models increasingly complex aspects of natural videos (e.g., motion, acceleration, texture). Four frames (t=0, 10, 20, 30) from a randomly selected video of each synthetic dataset and a sample video from the UCF101 dataset are displayed for comparison, illustrating the increasing realism of the generated videos.  The progression demonstrates the evolution from simple static shapes to more dynamic and textured videos, which are increasingly similar in appearance to real-world video data.", "section": "3 PRE-TRAINING VIDEO MODELS WITHOUT NATURAL VIDEOS"}, {"figure_path": "https://arxiv.org/html/2410.24213/x2.png", "caption": "Figure 2: Action recognition accuracy on UCF101. We present the UCF101 classification accuracy of the progression of models {Mi}subscript\ud835\udc40\ud835\udc56\\{M_{i}\\}{ italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }, after fine-tuning each of them on UCF101. The accuracy increases along the progression.", "description": "This figure displays the UCF101 action recognition accuracy for a series of models (M<sub>i</sub>). Each model (M<sub>i</sub>) in the series was trained on a different synthetic dataset, designed with increasing complexity and realism (see figure 1). The x-axis represents the different datasets used to pre-train the models, beginning with simple static circles and culminating in datasets incorporating dynamic transformations and natural image crops.  The y-axis shows the classification accuracy achieved on the UCF101 benchmark after fine-tuning each model. The graph clearly demonstrates that as the complexity and realism of the training dataset increase, the accuracy on UCF101 also improves.", "section": "3.3 EVALUATION PROTOCOLS"}, {"figure_path": "https://arxiv.org/html/2410.24213/x3.png", "caption": "Figure 3: Distribution Shift results on UCF101-P\u00a0(Schiappa et\u00a0al., 2023) (ViT-B) The last model in our progression outperforms pre-training on natural videos for 11 out of 14 corruption datasets.", "description": "This figure presents the performance comparison of different video models on the UCF101-P dataset, which contains corrupted versions of UCF101 videos.  The models tested include those pre-trained on synthetic datasets created using a progression of generative models and a VideoMAE model pre-trained on natural UCF101 videos (a standard baseline). The x-axis shows the different types of corruptions applied to the UCF101-P videos (e.g., blur, noise, camera motion). The y-axis shows the accuracy of each model on these corrupted videos. The key observation is that the model pre-trained on the final synthetic dataset in the progression significantly outperforms the model pre-trained on natural videos in 11 out of the 14 corruption types.  This demonstrates the effectiveness of the synthetic data approach in learning robust video representations that generalize well to noisy or corrupted data.", "section": "4.2 Distribution Shift"}, {"figure_path": "https://arxiv.org/html/2410.24213/x4.png", "caption": "Figure 4: Dataset properties compared to downstream performance. We compare the downstream classification accuracy on UCF101 after fine-tuning to frame and video properties of all the dataset variants we used in our analysis (see datasets list in\u00a0Section\u00a0A.1).", "description": "This figure visualizes the correlation between various properties of the synthetic video datasets and their corresponding downstream performance on the UCF101 action recognition task.  The datasets, generated using different generative processes and incorporating increasing levels of realism, are evaluated on several metrics reflecting frame and video properties: Frame Similarity (FID score measuring visual similarity to UCF101 frames), Video Similarity (FVD score measuring video-level similarity to UCF101 videos), Frame Diversity (measuring diversity within each dataset), Frame Spectrum (analyzing the frequency distribution of the frames), and Color Distribution (comparing color distributions to that of UCF101). Scatter plots illustrate the relationship between each dataset's performance (measured as accuracy on UCF101 after fine-tuning) and its value on the different metrics.  The analysis aims to identify which low-level video properties are most strongly correlated with achieving high accuracy, providing insights into the design of effective synthetic video datasets for pre-training.", "section": "5 DATASETS ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2410.24213/x5.png", "caption": "Figure 5: Feature visualizations for pre-trained models. We present the 3 principal components of the attention keys of the last encoder layer, for all Misubscript\ud835\udc40\ud835\udc56M_{i}italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as the three color channels. Different object parts start to appear as the datasets progress.", "description": "This figure visualizes the learned representations from the VideoMAE model's encoder after training on a series of synthetic video datasets. Each dataset progressively incorporates more realistic video properties, such as object movement, shape transformation, and texture.  The visualization uses the three principal components of the attention keys from the last encoder layer as red, green, and blue color channels. By observing the changes across the different datasets (represented as M subscript i), we can see how the model's understanding of the video content evolves. In the earlier datasets, representations are relatively simple; however, they become increasingly complex as the datasets reflect more realistic properties and incorporate natural images. The appearance of different object parts in the visualization highlights this improvement.", "section": "4.1 Fine-tuning"}]