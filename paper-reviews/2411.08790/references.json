{"references": [{"fullname_first_author": "Usman Anwar", "paper_title": "Foundational challenges in assuring alignment and safety of large language models", "publication_date": "2024-04-09", "reason": "This paper is foundational to the study of alignment and safety in large language models, providing context for the importance of steering vector interpretability."}, {"fullname_first_author": "Andy Arditi", "paper_title": "Refusal in language models is mediated by a single direction", "publication_date": "2024-06-11", "reason": "This paper investigates the mechanisms behind refusal in language models, a behavior that steering vectors aim to influence, offering insights into the complexities of such mechanisms."}, {"fullname_first_author": "Trenton Bricken", "paper_title": "Towards monosemanticity: Decomposing language models with dictionary learning", "publication_date": "2023-00-00", "reason": "This paper introduces a novel approach to decomposing language model activations, providing a foundation for the sparse autoencoder methods explored in the target paper."}, {"fullname_first_author": "Arthur Conmy", "paper_title": "Progress update #1 from the GDM mech interp team", "publication_date": "2024-00-00", "reason": "This paper, a progress report, shows preliminary work using sparse autoencoders to investigate steering vectors, directly motivating the research in the target paper."}, {"fullname_first_author": "Hoagy Cunningham", "paper_title": "Sparse autoencoders find highly interpretable features in language models", "publication_date": "2023-09-08", "reason": "This paper demonstrates the effectiveness of sparse autoencoders in discovering interpretable features in language models, supporting the use of SAEs in the target paper for interpreting steering vectors."}]}