[{"figure_path": "2410.18252/tables/table_16_0.md", "caption": "Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training", "description": "Table 1 presents the baseline performance of three different-sized Pythia models (410m, 1B, and 2.8B) after supervised fine-tuning (SFT) before reinforcement learning from human feedback (RLHF).  For each model size, the table shows the win rate (percentage of times the model's summary was preferred over another) and the KL (Kullback-Leibler) perplexity, a measure of how different the model's output distribution is from the initial SFT model's distribution. These metrics provide a benchmark against which to evaluate the improvements achieved by the subsequent RLHF process.", "section": "3.1 Off-Policy RLHF"}, {"figure_path": "2410.18252/tables/table_16_1.md", "caption": "Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training", "description": "The table presents the baseline performance of three different-sized Pythia language models (410m, 1B, and 2.8B parameters) after supervised fine-tuning before reinforcement learning from human feedback (RLHF).  For each model size, the table shows the win rate (percentage of times the model's summary is preferred over a human-written summary by a reward model) and the KL perplexity (a measure of how much the model's probability distribution deviates from an initial model after finetuning). These metrics provide a reference point for evaluating the improvements achieved through subsequent RLHF training.", "section": "3.1 Off-Policy RLHF"}, {"figure_path": "2410.18252/tables/table_17_0.md", "caption": "Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training", "description": "Table 1 presents the baseline performance of three Pythia models (410M, 1B, and 2.8B parameters) after supervised fine-tuning but before reinforcement learning from human feedback (RLHF).  For each model, the table shows the win rate (percentage of times the model's summary was preferred over another summary) and the KL (Kullback-Leibler) perplexity, a measure of how different the model's distribution is from an initial state.  The win rate is a performance metric indicating how well the model performs summarization on a specific dataset, while the KL perplexity gives insight into how much the model diverged during supervised training from its initial state.", "section": "A.1 TLDR Summarization"}, {"figure_path": "2410.18252/tables/table_17_1.md", "caption": "Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training", "description": "The table presents the baseline performance of three different-sized language models (410m, 1B, and 2.8B parameters) after supervised fine-tuning.  For each model, the table shows the win rate (percentage of times the model's generated summary was rated higher than a human-written summary by a gold standard reward model) and the KL perplexity (a measure of how different the model's output distribution is from the initial model's distribution). These metrics provide a baseline performance before reinforcement learning from human feedback (RLHF) is applied.", "section": "3.1 OFF-POLICY RLHF"}, {"figure_path": "2410.18252/tables/table_17_2.md", "caption": "Table 6: The trained models' GPT4-0 win rate against the human-written responses on the test split of the No Robots dataset (Rajani et al., 2023)", "description": "This table presents the results of a final evaluation comparing the performance of different models on the No Robots dataset.  It shows the win rate, as judged by GPT-4, of generated summaries against human-written summaries for three models: the model after supervised fine-tuning (SFT), and two models trained with asynchronous and synchronous online DPO respectively. The average response sequence length for each model is also provided.", "section": "5.1 Large-Scale General Instruction-Following"}, {"figure_path": "2410.18252/tables/table_18_0.md", "caption": "Table 6: The trained models\u2019 GPT-4 win rate against the human-written responses on the test split of the No Robots dataset (Rajani et al., 2023)", "description": "This table presents the performance comparison of different language models on the No Robots dataset.  It shows the win rate, as evaluated by GPT-4,  for three models: the initial Supervised Fine-Tuning (SFT) model, an asynchronously trained model using Online DPO, and a synchronously trained model using Online DPO.  The win rate represents the percentage of times a model\u2019s generated response was preferred over a human-written response.  The table also includes the average response sequence length for each model, indicating the length of generated text.  Note that the asynchronously and synchronously trained Online DPO models achieve the same win rate.", "section": "5.2 Large-Scale General Instruction-Following"}]