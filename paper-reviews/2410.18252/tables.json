[{"figure_path": "2410.18252/tables/table_1_0.html", "caption": "Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training", "description": "Table 1 presents the win rate and perplexity scores achieved by three different sized models (410m, 1B, and 2.8B) after supervised fine-tuning, before reinforcement learning from human feedback (RLHF) is applied.", "section": "3.1 Off-Policy RLHF"}, {"figure_path": "2410.18252/tables/table_16_0.html", "caption": "Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training", "description": "Table 1 presents the win rate and KL perplexity of the Pythia models of different sizes after supervised fine-tuning with SFT data, before reinforcement learning from human feedback is applied.", "section": "A.1 TLDR SUMMARIZATION"}, {"figure_path": "2410.18252/tables/table_16_1.html", "caption": "Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training", "description": "Table 1 presents the win rate and perplexity scores achieved by three different sized models (410m, 1B, and 2.8B) after undergoing supervised fine-tuning, but before reinforcement learning from human feedback (RLHF).", "section": "3.1 Off-Policy RLHF"}, {"figure_path": "2410.18252/tables/table_17_0.html", "caption": "Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training", "description": "Table 1 presents the win rate and perplexity scores achieved by different sized models after supervised fine-tuning, before reinforcement learning from human feedback (RLHF) is applied.", "section": "A.1 TLDR Summarization"}, {"figure_path": "2410.18252/tables/table_17_1.html", "caption": "Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training", "description": "Table 1 presents the win rate and perplexity scores achieved by different sized language models after supervised fine-tuning, prior to reinforcement learning from human feedback (RLHF).", "section": "3.2 OFF-POLICY WIN-RATE AND KL"}, {"figure_path": "2410.18252/tables/table_17_2.html", "caption": "Table 6: The trained models' GPT4-0 win rate against the human-written responses on the test split of the No Robots dataset (Rajani et al., 2023)", "description": "The table presents the win rates and average response sequence lengths achieved by different models (SFT, Async Online DPO, Sync Online DPO, and Human) on the No Robots dataset.", "section": "5 Large-Scale Asynchronous RLHF"}, {"figure_path": "2410.18252/tables/table_18_0.html", "caption": "Table 6: The trained models\u2019 GPT-4 win rate against the human-written responses on the test split of the No Robots dataset (Rajani et al., 2023)", "description": "Table 6 presents the win rates and average response sequence lengths of three language models (SFT, Async Online DPO, Sync Online DPO) and human-written responses on the No Robots dataset.", "section": "5.1 Large-Scale General Instruction-Following"}]