[{"figure_path": "2410.18252/tables/table_16_0.html", "caption": "Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training", "description": "Table 1 presents the win rate and KL (perplexity) of three different sized Pythia models after supervised fine-tuning, before reinforcement learning from human feedback (RLHF) training.", "section": "A.1 TLDR Summarization"}, {"figure_path": "2410.18252/tables/table_16_1.html", "caption": "Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training", "description": "Table 1 presents the win rate and perplexity scores achieved by different sized language models after supervised fine-tuning, but before undergoing reinforcement learning from human feedback.", "section": "3.1 OFF-POLICY RLHF"}, {"figure_path": "2410.18252/tables/table_17_0.html", "caption": "Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training", "description": "Table 1 presents the win rate and perplexity scores achieved by three different sized language models after undergoing supervised fine-tuning, prior to reinforcement learning from human feedback (RLHF).", "section": "A.1 TLDR SUMMARIZATION"}, {"figure_path": "2410.18252/tables/table_17_1.html", "caption": "Table 1: The win-rate and perplexity of models after supervised finetuning, before RLHF training", "description": "The table presents the win rate and perplexity scores achieved by three different sized models (410m, 1B, and 2.8B) after supervised fine-tuning, before reinforcement learning from human feedback (RLHF) is applied.", "section": "3.2 OFF-POLICY WIN-RATE AND KL"}, {"figure_path": "2410.18252/tables/table_17_2.html", "caption": "Table 6: The trained models\u2019 GPT-4 win rate against the human-written responses on the test split of the No Robots dataset (Rajani et al., 2023)", "description": "Table 6 presents the win rate and average response sequence length achieved by different models on the No Robots dataset, comparing the SFT model, synchronous online DPO, asynchronous online DPO, and human performance.", "section": "5.1 Large-Scale General Instruction-Following"}, {"figure_path": "2410.18252/tables/table_18_0.html", "caption": "Table 6: The trained models\u2019 GPT-4 win rate against the human-written responses on the test split of the No Robots dataset (Rajani et al., 2023)", "description": "Table 6 presents the win rate and average response sequence length for the SFT model, asynchronous online DPO, synchronous online DPO and human-written responses on the test split of the No Robots dataset.", "section": "5 Large-Scale Asynchronous RLHF"}]