[{"figure_path": "2410.18252/charts/charts_2_0.png", "caption": "Figure 1: Asynchronous off-policy RLHF is more computationally efficient, and matches the win-rate of synchronous on-policy RLHF on TLDR across model scales. On 4\u00d7A100 GPUs, it results in training a 2.8B Pythia model 25% faster and improvements in speed increase with scale.", "description": "The chart is a scatter plot showing the relationship between compute time (in minutes) and gold win-rate for asynchronous and synchronous RLHF methods across three different model scales (410M, 1B, and 2.8B parameters).  The x-axis represents compute time, and the y-axis represents the gold win-rate. Each point represents a single training run, with different colors representing the training method (asynchronous or synchronous) and different marker sizes representing the model scale. The chart demonstrates that asynchronous off-policy RLHF achieves comparable or better win-rates to synchronous on-policy RLHF, but with significantly less compute time, particularly for larger models. The dashed lines connect points of the same method and model size to aid visualization of trends. ", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18252/charts/charts_3_0.png", "caption": "Figure 1: Asynchronous off-policy RLHF is more computationally efficient, and matches the win-rate of synchronous on-policy RLHF on TLDR across model scales. On 4\u00d7A100 GPUs, it results in training a 2.8B Pythia model 25% faster and improvements in speed increase with scale.", "description": "The chart is a line graph comparing the computational efficiency of asynchronous off-policy RLHF against synchronous on-policy RLHF across different model scales (410M, 1B, and 2.8B parameters). The x-axis represents compute time (in minutes), and the y-axis shows the gold win-rate, a measure of model performance. Separate lines depict results for asynchronous and synchronous training methods.  The graph demonstrates that asynchronous off-policy RLHF achieves comparable gold win-rates to synchronous on-policy RLHF but with significantly less compute time. The difference in compute time increases with larger model sizes.", "section": "3 ASYNCHRONOUS OFF-POLICY RLHF"}, {"figure_path": "2410.18252/charts/charts_5_0.png", "caption": "Figure 3: Trade-off between Win-Rate and KL in Off-Policy PPO. PPO performance decreases as learning becomes more off-policy. Win-rate is highest when learning is fully on-policy (generate then train on N = 1 mini-batches). As we increase N, our model must take more steps on data generated by the same old policy. This increases off-policyness and reduces win-rate. Left: Gold win-rate over training Middle: KL (perplexity) over training, higher is further from initial model Right: Gold win-rate vs KL", "description": "This figure illustrates the trade-off between win-rate and KL divergence (a measure of the policy's drift from the initial model) in off-policy proximal policy optimization (PPO) for reinforcement learning from human feedback (RLHF).  The left panel shows the gold win-rate (a measure of performance) over training episodes for different values of N (number of mini-batches), representing varying levels of off-policyness. The middle panel displays the KL divergence over training episodes for the same N values. The right panel presents a scatter plot showing the relationship between the final gold win-rate and KL divergence for different N values. The plots demonstrate that as N increases (more off-policy data), the win-rate decreases, and the KL divergence increases, indicating a trade-off between faster training and model performance. The results suggest that fully on-policy learning (N=1) yields the best performance, while increasing off-policyness compromises performance.", "section": "3.2 OFF-POLICY WIN-RATE AND KL"}, {"figure_path": "2410.18252/charts/charts_5_1.png", "caption": "Figure 4: Robustness of RLHF Losses to Off-Policyness. Online DPO is more robust to off-policyness than PPO, RLOO (Left) or Best-of-2 SFT (Right). Performance is shown across levels of off-policyness as mediated by number of mini-batches N\u2208 {1,2,4,8,16}. With higher N increasing off-policyness, Online DPO retains much more performance than other methods, as evidenced by off-policy points still being clustered close to optimal performance.", "description": "This chart displays the robustness of four different RLHF loss functions (Online DPO, PPO, RLOO, and Best-of-2) to varying degrees of off-policyness.  The x-axis represents the Kullback-Leibler (KL) divergence, a measure of how much the model's policy has drifted from its initial state, while the y-axis represents the gold win-rate, a measure of the model's performance.  Each loss function is represented by a different color and marker, with different marker shapes indicating the number of mini-batches used (1, 2, 4, 8, 16), which directly affects the level of off-policyness. The chart shows that Online DPO consistently maintains high performance (high win rate, low KL) even as off-policyness increases, while the other methods experience a significant drop in performance. This indicates Online DPO's superior robustness to off-policy data during asynchronous RLHF training.", "section": "3.2 OFF-POLICY WIN-RATE AND KL"}, {"figure_path": "2410.18252/charts/charts_6_0.png", "caption": "Figure 5: Scaling Model Size with Off-Policy RLHF. Plotting the final win-rate vs KL for N = 1 \u2192 64 mini-batches, covering a spectrum of on-policy to off-policy RL. Scaling policy size (left) improves off-policy robustness as seen by tighter clustering of points. But scaling reward model size (right) does not, even though it reduces overoptimization, achieving reward with smaller KL.", "description": "This figure displays two scatter plots illustrating the relationship between final win-rate and KL (perplexity) when scaling model size in off-policy RLHF. The left plot shows the effect of scaling the policy model size (410m, 1B, and 2.8B parameters), while keeping the reward model size constant. As the policy size increases, the data points cluster more tightly around the optimal region, indicating improved robustness to off-policy data. The right plot demonstrates the impact of scaling the reward model size, while keeping the policy model size fixed. In contrast to the left plot, scaling the reward model does not improve off-policy robustness, although it does lead to a reduction in KL, suggesting that overoptimization is reduced.  In both plots, different shades represent different numbers of mini-batches (N), reflecting varying degrees of off-policy learning.", "section": "3.4 SCALING MODEL SIZE WITH OFF-POLICY RLHF"}, {"figure_path": "2410.18252/charts/charts_7_0.png", "caption": "Figure 7: Optimizing Generation-Bound RLHF. We can leverage extra training GPU cycles to do multiple updates on the same generated mini-batch (\"ppo epochs\"). Left: At 410m and 1B scales, more updates per batch increases the win-rate achieved at any given episode, making training more data efficient. Right: Across scales, more updates change the pareto frontier and cause models to achieve the same win-rate at a higher KL.", "description": "This chart visualizes the impact of multiple updates per mini-batch on the performance of generation-bound RLHF across different model sizes (410M, 1B, and 2.8B). The left panel shows the gold win-rate over training episodes for various numbers of mini-batch updates (1, 2, and 3), demonstrating improved win-rates with more updates, especially at smaller scales.  The right panel presents a Pareto frontier, illustrating the trade-off between gold win-rate and KL (perplexity). It reveals that while more updates per batch lead to higher win-rates, they also result in higher KL values, indicating a greater divergence from the initial model.", "section": "Optimizing Generation-Bound RLHF"}, {"figure_path": "2410.18252/charts/charts_8_0.png", "caption": "Figure 8: Optimizing Training-Bound RLHF. We can leverage extra generation GPU cycles to sample K completions per prompt instead of 2. Left: Sampling K = 4 improves the gradient such that we can train for half the number of steps and, across scales, achieve the same final win-rate at a fraction of the compute time. Right: The trade-off is that increasing K causes models to drift more in terms of KL in order to achieve the same win-rate.", "description": "This figure displays the results of optimizing training-bound RLHF by increasing the number of samples (K) generated per prompt from 2 to 4. The left panel shows the effect on compute time and gold win-rate across different model scales (410m, 1b, and 2.8b). Increasing K to 4 reduces compute time significantly while maintaining similar gold win-rates, suggesting improved training efficiency. The right panel illustrates the trade-off between gold win-rate and KL (perplexity), indicating that higher K values lead to greater KL divergence, implying a potential cost in terms of model alignment.", "section": "4 OPTIMIZING ASYNCHRONOUS RLHF"}, {"figure_path": "2410.18252/charts/charts_9_0.png", "caption": "Figure 9: Large-Scale Asynchronous RLHF. Comparing synchronous and asynchronous online DPO for training an 8B general-purpose chatbot. Asynchronous learning achieves the same reward model score at a lower KL and 30% faster.", "description": "This chart displays the results of large-scale asynchronous RLHF training of an 8B general-purpose chatbot, comparing it to synchronous training. Two line graphs are shown: one for the reward model score and one for the KL divergence. In both graphs, the x-axis represents relative time in minutes, and the y-axis represents reward model score and KL divergence, respectively.  The blue lines represent asynchronous training, while the orange lines represent synchronous training. The chart demonstrates that asynchronous learning achieves a similar reward model score but with a lower KL divergence (indicating less overfitting) and in approximately 30% less time compared to synchronous training. This suggests that asynchronous RLHF is a more efficient approach for training large language models.", "section": "5 LARGE-SCALE ASYNCHRONOUS RLHF"}]