[{"figure_path": "2410.18252/charts/charts_2_0.png", "caption": "Figure 1: Asynchronous off-policy RLHF is more computationally efficient, and matches the win-rate of synchronous on-policy RLHF on TLDR across model scales. On 4\u00d7A100 GPUs, it results in training a 2.8B Pythia model 25% faster and improvements in speed increase with scale.", "description": "The chart displays the computational efficiency and win-rate performance of asynchronous off-policy RLHF compared to synchronous on-policy RLHF across different model scales, demonstrating faster training times with matching performance.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18252/charts/charts_3_0.png", "caption": "Figure 1: Asynchronous off-policy RLHF is more computationally efficient, and matches the win-rate of synchronous on-policy RLHF on TLDR across model scales. On 4\u00d7A100 GPUs, it results in training a 2.8B Pythia model 25% faster and improvements in speed increase with scale.", "description": "The chart shows that asynchronous off-policy RLHF is more computationally efficient than synchronous on-policy RLHF while achieving the same performance across different model scales.", "section": "3 ASYNCHRONOUS OFF-POLICY RLHF"}, {"figure_path": "2410.18252/charts/charts_5_0.png", "caption": "Figure 3: Trade-off between Win-Rate and KL in Off-Policy PPO. PPO performance decreases as learning becomes more off-policy. Win-rate is highest when learning is fully on-policy (generate then train on N = 1 mini-batches). As we increase N, our model must take more steps on data generated by the same old policy. This increases off-policyness and reduces win-rate. Left: Gold win-rate over training Middle: KL (perplexity) over training, higher is further from initial model Right: Gold win-rate vs KL", "description": "The chart displays the trade-off between win-rate and KL divergence in off-policy PPO, showing decreasing performance with increasing off-policyness.", "section": "3.2 OFF-POLICY WIN-RATE AND KL"}, {"figure_path": "2410.18252/charts/charts_5_1.png", "caption": "Figure 4: Robustness of RLHF Losses to Off-Policyness. Online DPO is more robust to off-policyness than PPO, RLOO (Left) or Best-of-2 SFT (Right). Performance is shown across levels of off-policyness as mediated by number of mini-batches N\u2208 {1,2,4,8,16}. With higher N increasing off-policyness, Online DPO retains much more performance than other methods, as evidenced by off-policy points still being clustered close to optimal performance.", "description": "The chart compares the robustness of different RLHF loss functions (Online DPO, PPO, RLOO, Best-of-2) to varying degrees of off-policyness, showing Online DPO's superior performance.", "section": "3.2 OFF-POLICY WIN-RATE AND KL"}, {"figure_path": "2410.18252/charts/charts_6_0.png", "caption": "Figure 5: Scaling Model Size with Off-Policy RLHF. Plotting the final win-rate vs KL for N = 1 \u2192 64 mini-batches, covering a spectrum of on-policy to off-policy RL. Scaling policy size (left) improves off-policy robustness as seen by tighter clustering of points. But scaling reward model size (right) does not, even though it reduces overoptimization, achieving reward with smaller KL.", "description": "The chart shows the effect of scaling policy and reward model sizes on the robustness of off-policy reinforcement learning from human feedback (RLHF) in terms of win rate and KL divergence.", "section": "3.4 SCALING MODEL SIZE WITH OFF-POLICY RLHF"}, {"figure_path": "2410.18252/charts/charts_7_0.png", "caption": "Figure 7: Optimizing Generation-Bound RLHF. We can leverage extra training GPU cycles to do multiple updates on the same generated mini-batch (\"ppo epochs\"). Left: At 410m and 1B scales, more updates per batch increases the win-rate achieved at any given episode, making training more data efficient. Right: Across scales, more updates change the pareto frontier and cause models to achieve the same win-rate at a higher KL.", "description": "The chart shows the impact of multiple updates per batch on the win-rate and KL (perplexity) in generation-bound asynchronous RLHF across different model scales.", "section": "4 OPTIMIZING ASYNCHRONOUS RLHF"}, {"figure_path": "2410.18252/charts/charts_8_0.png", "caption": "Figure 8: Optimizing Training-Bound RLHF. We can leverage extra generation GPU cycles to sample K completions per prompt instead of 2. Left: Sampling K = 4 improves the gradient such that we can train for half the number of steps and, across scales, achieve the same final win-rate at a fraction of the compute time. Right: The trade-off is that increasing K causes models to drift more in terms of KL in order to achieve the same win-rate.", "description": "The chart shows the trade-off between compute time, win-rate, and KL divergence when optimizing training-bound RLHF by adjusting the number of samples per prompt.", "section": "4 OPTIMIZING ASYNCHRONOUS RLHF"}, {"figure_path": "2410.18252/charts/charts_9_0.png", "caption": "Figure 9: Large-Scale Asynchronous RLHF. Comparing synchronous and asynchronous online DPO for training an 8B general-purpose chatbot. Asynchronous learning achieves the same reward model score at a lower KL and 30% faster.", "description": "The chart compares the performance of synchronous and asynchronous online DPO for training a large language model, showing that asynchronous learning achieves the same reward model score with lower KL divergence and 30% faster training time.", "section": "5 LARGE-SCALE ASYNCHRONOUS RLHF"}]