[{"figure_path": "2410.18252/charts/charts_2_0.png", "caption": "Figure 1: Asynchronous off-policy RLHF is more computationally efficient, and matches the win-rate of synchronous on-policy RLHF on TLDR across model scales. On 4\u00d7A100 GPUs, it results in training a 2.8B Pythia model 25% faster and improvements in speed increase with scale.", "description": "The chart shows that asynchronous off-policy RLHF training is more computationally efficient than synchronous on-policy RLHF, achieving comparable performance while reducing training time.", "section": "3 ASYNCHRONOUS OFF-POLICY RLHF"}, {"figure_path": "2410.18252/charts/charts_3_0.png", "caption": "Figure 1: Asynchronous off-policy RLHF is more computationally efficient, and matches the win-rate of synchronous on-policy RLHF on TLDR across model scales. On 4\u00d7A100 GPUs, it results in training a 2.8B Pythia model 25% faster and improvements in speed increase with scale.", "description": "The chart shows that asynchronous off-policy RLHF is more computationally efficient than synchronous on-policy RLHF across different model scales, achieving similar performance with faster training times.", "section": "3 ASYNCHRONOUS OFF-POLICY RLHF"}, {"figure_path": "2410.18252/charts/charts_5_0.png", "caption": "Figure 3: Trade-off between Win-Rate and KL in Off-Policy PPO. PPO performance decreases as learning becomes more off-policy. Win-rate is highest when learning is fully on-policy (generate then train on N = 1 mini-batches). As we increase N, our model must take more steps on data generated by the same old policy. This increases off-policyness and reduces win-rate. Left: Gold win-rate over training Middle: KL (perplexity) over training, higher is further from initial model Right: Gold win-rate vs KL", "description": "The chart illustrates the trade-off between win-rate and KL divergence (a measure of how much the model has drifted from its initial state) in off-policy PPO, showing how performance decreases as the learning becomes more off-policy.", "section": "3.2 OFF-POLICY WIN-RATE AND KL"}, {"figure_path": "2410.18252/charts/charts_5_1.png", "caption": "Figure 4: Robustness of RLHF Losses to Off-Policyness. Online DPO is more robust to off-policyness than PPO, RLOO (Left) or Best-of-2 SFT (Right). Performance is shown across levels of off-policyness as mediated by number of mini-batches N\u2208 {1,2,4,8,16}. With higher N increasing off-policyness, Online DPO retains much more performance than other methods, as evidenced by off-policy points still being clustered close to optimal performance.", "description": "The chart displays the robustness of different RLHF loss functions (Online DPO, PPO, RLOO, Best-of-2) to varying degrees of off-policyness, showing Online DPO's superior performance.", "section": "3.2 OFF-POLICY WIN-RATE AND KL"}, {"figure_path": "2410.18252/charts/charts_6_0.png", "caption": "Figure 5: Scaling Model Size with Off-Policy RLHF. Plotting the final win-rate vs KL for N = 1 \u2192 64 mini-batches, covering a spectrum of on-policy to off-policy RL. Scaling policy size (left) improves off-policy robustness as seen by tighter clustering of points. But scaling reward model size (right) does not, even though it reduces overoptimization, achieving reward with smaller KL.", "description": "The chart displays the relationship between final win-rate, KL divergence, and model size (both policy and reward models) across various levels of off-policy learning.", "section": "3.4 SCALING MODEL SIZE WITH OFF-POLICY RLHF"}, {"figure_path": "2410.18252/charts/charts_7_0.png", "caption": "Figure 7: Optimizing Generation-Bound RLHF. We can leverage extra training GPU cycles to do multiple updates on the same generated mini-batch (\"ppo epochs\"). Left: At 410m and 1B scales, more updates per batch increases the win-rate achieved at any given episode, making training more data efficient. Right: Across scales, more updates change the pareto frontier and cause models to achieve the same win-rate at a higher KL.", "description": "The chart shows the effect of multiple training updates per mini-batch on the win-rate and KL divergence in generation-bound asynchronous RLHF across different model scales.", "section": "Optimizing Asynchronous RLHF"}, {"figure_path": "2410.18252/charts/charts_8_0.png", "caption": "Figure 8: Optimizing Training-Bound RLHF. We can leverage extra generation GPU cycles to sample K completions per prompt instead of 2. Left: Sampling K = 4 improves the gradient such that we can train for half the number of steps and, across scales, achieve the same final win-rate at a fraction of the compute time. Right: The trade-off is that increasing K causes models to drift more in terms of KL in order to achieve the same win-rate.", "description": "The chart displays the trade-off between compute efficiency and model performance (measured by KL divergence) when optimizing training-bound RLHF by varying the number of samples generated per prompt.", "section": "4 OPTIMIZING ASYNCHRONOUS RLHF"}, {"figure_path": "2410.18252/charts/charts_9_0.png", "caption": "Figure 9: Large-Scale Asynchronous RLHF. Comparing synchronous and asynchronous online DPO for training an 8B general-purpose chatbot. Asynchronous learning achieves the same reward model score at a lower KL and 30% faster.", "description": "The chart compares the performance of synchronous and asynchronous online DPO for training a large language model (LLM), showing that asynchronous learning achieves similar reward scores with lower KL divergence and faster training time.", "section": "5 LARGE-SCALE ASYNCHRONOUS RLHF"}]