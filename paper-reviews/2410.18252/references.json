{"references": [{" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is foundational to the field of RLHF, introducing the dominant on-policy paradigm that the current work aims to improve upon by proposing a more efficient asynchronous approach.  It's a highly cited and influential work in the area, establishing the basic framework for RLHF and highlighting the limitations of the on-policy approach in terms of scalability and efficiency.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Yuntao Bai", "paper_title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "reason": "This paper is important because it presents a successful RLHF approach with key insights into how to train LLMs to be helpful and harmless.  It provides valuable context and background for the current paper, which builds upon this foundation and seeks to improve upon the existing limitations in terms of efficiency.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "R\u00e9mi Rafailov", "paper_title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", "reason": "This work introduces a novel offline method for RLHF that, although underperforming online methods according to recent findings, was influential in the development of the field and provides a valuable contrast to the online approach explored in the current paper. The efficient offline nature of this method makes it a key comparative point for evaluating the improvements offered by the proposed asynchronous approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Arash Ahmadian", "paper_title": "Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs", "reason": "This recent work provides a valuable comparative point, revisiting the fundamentals of RLHF and suggesting potential improvements for RLHF. It directly relates to the current work by exploring alternative optimization methods for RLHF that can potentially improve efficiency, which is the central goal of the proposed asynchronous approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Meta AI Llama Team", "paper_title": "The Llama 3 Herd of Models", "reason": "This paper describes a large language model which is used in the large-scale experiments of the current paper, making it a critical component of the empirical results and validation. Its significance lies in its scale and general-purpose nature, enabling the evaluation of the proposed asynchronous approach on a realistic and challenging task.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Rishabh Agarwal", "paper_title": "Generalized Knowledge Distillation for Auto-regressive Language Models", "reason": "This paper shows a method to optimize autoregressive language models using knowledge distillation. The current work also involves a similar process of finetuning the models, making this citation highly relevant to the understanding of the baseline model's performance and the finetuning steps.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Paul F Christiano", "paper_title": "Deep Reinforcement Learning from Human Preferences", "reason": "This paper is foundational to the field of RLHF, providing the basic definition and framework of the approach.  Understanding this early work is critical for contextualizing the current paper's novel approach of asynchronous off-policy RLHF and its potential improvements over existing methods.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Amanda Askell", "paper_title": "A General Language Assistant as a Laboratory for Alignment", "reason": "This work presents an approach for aligning LLMs with human values and preferences. Understanding this is critical for interpreting the challenges and tradeoffs in the current paper's exploration of asynchronous RLHF.  The methods used for alignment and safety in this paper are crucial for evaluating the proposed approach's benefits.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "John Schulman", "paper_title": "Trust Region Policy Optimization", "reason": "Proximal Policy Optimization (PPO) is a widely used RL algorithm, and this paper introduces it.  Understanding PPO is fundamental to understanding one of the key algorithms compared in the paper's evaluation of asynchronous off-policy RLHF. The choice of PPO and its performance characteristics are central to interpreting the results.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "Volodymyr Mnih", "paper_title": "Asynchronous Methods for Deep Reinforcement Learning", "reason": "This seminal work introduced asynchronous methods in Deep RL, which directly inspires the asynchronous approach for RLHF proposed in the current paper.  The concept of separating generation and learning, key to the efficiency gains, is rooted in the ideas presented in this publication.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Leo Gao", "paper_title": "Scaling Laws for Reward Model Overoptimization", "reason": "This paper investigates the effects of overoptimization in reward models used in RLHF. This is highly relevant to the current paper because it addresses potential limitations of reward models and their interaction with the training process in RLHF and offers insights that may impact the approach.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Shengyi Huang", "paper_title": "The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization", "reason": "This paper offers implementation details of RLHF with PPO which the current work builds upon.  Its relevance stems from the fact that the current work improves upon the existing RLHF workflow by introducing asynchronous off-policy methods, making this a significant baseline for evaluating the improvements.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Woosuk Kwon", "paper_title": "Efficient Memory Management for Large Language Model Serving with PagedAttention", "reason": "This paper focuses on efficient LLM inference, a crucial component of the asynchronous RLHF approach.  The efficiency gains of asynchronous RLHF are partly attributed to the use of efficient LLM inference libraries like vllm, making understanding this paper essential for contextualizing those improvements.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Shengyi Huang", "paper_title": "Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform", "reason": "Cleanba is the asynchronous RLHF framework used in this paper.  The design and implementation details of Cleanba are crucial for understanding the setup of the experiments and the computational gains achieved by the asynchronous approach.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Zhiyu Mei", "paper_title": "ReaLHF: Optimized RLHF Training for Large Language Models through Parameter Reallocation", "reason": "This paper addresses the engineering challenges involved in efficient RLHF training. The current work is related to it as both papers address RLHF training and aim to improve efficiency, but they take different approaches.  Comparing these approaches provides insights into the advantages and limitations of each.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Rafael Rafailov", "paper_title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function", "reason": "This paper provides further advancements on the offline RLHF approach, providing another perspective on the efficiency tradeoffs compared to online approaches.  Since the work introduces a novel online asynchronous approach, understanding the state-of-the-art in both online and offline RLHF methods is crucial for accurate evaluation.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Yunhao Tang", "paper_title": "Understanding the performance gap between online and offline alignment algorithms", "reason": "This paper directly addresses the performance gap between online and offline RLHF, providing important context for the current paper's focus on improving the efficiency of online approaches, specifically through asynchronous methods.  It highlights the critical role of online data in achieving high-performing RLHF systems, a key justification for the asynchronous online approach introduced in the current work.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Yunhao Tang", "paper_title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment", "reason": "This paper offers a unified perspective on preference optimization techniques in RLHF.  It presents a valuable comparison point to highlight the specific contributions of the asynchronous approach proposed in the current paper, particularly regarding improvements in both efficiency and scalability.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Fahim Tajwar", "paper_title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data", "reason": "This paper explores the role of on-policy data in preference finetuning, which is directly relevant to the discussion of off-policy learning in asynchronous RLHF in the current work. It highlights the importance of on-policy data for optimal performance, providing context for the challenges and trade-offs involved in using off-policy data in asynchronous RLHF.", "section_number": 6}]}