{"references": [{"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-01-01", "reason": "This paper is considered among the most important because it introduces the Chain-of-Thought (CoT) prompting technique, which is fundamental to eliciting reasoning in large language models and is heavily referenced throughout the survey."}, {"fullname_first_author": "Daya Guo", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning", "publication_date": "2025-01-01", "reason": "This paper is crucial because it presents DeepSeek-R1, a prominent Large Reasoning Model (LRM), and details its training process using reinforcement learning, serving as a key example in the survey's discussion of efficient reasoning."}, {"fullname_first_author": "Xuezhi Wang", "paper_title": "Self-consistency improves chain of thought reasoning in language models", "publication_date": "2023-01-01", "reason": "This paper is important because it introduces Self-consistency CoT, a notable CoT variant that replaces the greedy-like thinking chain generation by sampling more diverse reasoning paths and then selects the most consistent answer by marginalizing and aggregating such paths."}, {"fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2022-01-01", "reason": "This paper is important because it introduces LORA, a parameter-efficient fine-tuning method often used in adapting LLMs to short reasoning steps with minimal parameter tuning, and cited for its relevance to efficient data and models."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-01-01", "reason": "This paper is crucial because it introduces PPO (Proximal Policy Optimization), a widely used reinforcement learning technique in combination with length-based reward to control the length of CoT reasoning."}]}