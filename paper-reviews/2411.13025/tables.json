[{"content": "| Dataset | Method | BLUE@1 | BLUE@2 | BLUE@3 | BLUE@4 | METOR | ROUGE-L |\n|---|---|---|---|---|---|---|---| \n|  | DCL [34] | - | - | - | 0.163 | 0.193 | 0.383 |\n|  | MMTN [5] | 0.486 | 0.321 | 0.232 | 0.175 | - | 0.375 |\n| IU- | M2KT [61] | 0.497 | 0.319 | 0.230 | 0.174 | - | 0.399 |\n| Xray | C2M-DOT [54] | 0.475 | 0.309 | 0.222 | 0.170 | 0.191 | 0.375 |\n|  | CMMRL [44] | 0.494 | 0.321 | 0.235 | 0.181 | 0.201 | 0.384 |\n|  | XPRONET* [53] | 0.501 | 0.324 | 0.224 | 0.165 | 0.204 | 0.380 |\n|  | R2GenCMN* [43] | 0.475 | 0.309 | 0.222 | 0.165 | 0.187 | 0.371 |\n|  | ORID(Ours) | **0.501** | **0.351** | **0.261** | **0.198** | **0.211** | **0.400** |\n|  | DCL [34] | - | - | - | 0.109 | 0.150 | 0.284 |\n|  | MMTN [5] | 0.379 | 0.238 | 0.159 | 0.116 | **0.160** | 0.283 |\n| MIMIC | M2KT [61] | 0.386 | 0.237 | 0.157 | 0.111 | - | 0.274 |\n| CXR | Lgi-MIMIC [65] | 0.343 | 0.210 | 0.140 | 0.099 | 0.137 | 0.271 |\n|  | CMMRL [44] | 0.353 | 0.218 | 0.148 | 0.106 | 0.142 | 0.278 |\n|  | XPRONET [53] | 0.344 | 0.215 | 0.146 | 0.105 | 0.138 | 0.279 |\n|  | R2GenCMN* [43] | 0.347 | 0.221 | 0.139 | 0.097 | 0.138 | 0.274 |\n|  | ORID(Ours) | **0.386** | **0.238** | **0.163** | **0.117** | 0.150 | **0.284** |", "caption": "Table 1: The results of the ORID model and other tested models in IU-Xray and MIMIC-CXR benchmarks. \u2217*\u2217 indicates we reproduced. The results for other models are obtained from their original papers. The best result is presented in bold. The most important metric has been marked in grey.", "description": "This table presents a comparison of the performance of the proposed ORID model against several state-of-the-art models on two benchmark datasets: IU-Xray and MIMIC-CXR.  The evaluation metrics used are BLEU (at various n-gram levels), METEOR, and ROUGE-L, which are standard metrics for evaluating natural language generation. The results for the ORID model are directly from the authors' experiments. Results for other models were taken from their respective papers. The best score for each metric is highlighted in bold, and the most important metric (ROUGE-L) is shown in gray.", "section": "4. Experiments"}, {"content": "| Method | Precision | Recall | F1-Score |\n|---|---|---|---|\n| R2Gen [7] | 0.333 | 0.273 | 0.276 |\n| CMMRL [43] | 0.342 | 0.294 | 0.292 |\n| R2GenCMN [6] | 0.334 | 0.275 | 0.278 |\n| METransformer [56] | 0.364 | **0.309** | 0.311 |\n| ORID(Ours) | **0.435** | 0.295 | **0.352** |", "caption": "Table 2: Comparison of clinical efficacy metrics for the MIMIC-CXR dataset. The best result is presented in bold. The critical metrics have been shaded in grey.", "description": "This table presents a comparison of clinical efficacy metrics for different radiology report generation models using the MIMIC-CXR dataset.  The metrics evaluated assess the precision, recall, and F1-score of the generated reports in identifying clinically significant observations.  The best performing model for each metric is highlighted in bold, and the most important metrics are shaded in grey to emphasize their relative importance in evaluating the overall clinical effectiveness of the generated reports.  This allows readers to directly compare the performance of various models in terms of their ability to produce clinically relevant and accurate radiology reports.", "section": "4. Experiments"}, {"content": "| Diagnosis Model | B@1 | B@4 | MTR. | RGL. |\n|---|---|---|---|---|\n| LLaVA-Med [32] | 0.441 | 0.158 | 0.179 | 0.378 |\n| LLaVA-Med-RRG | **0.501** | **0.198** | **0.211** | **0.400** |", "caption": "Table 3: Experiment comparison between LLaVA-Med-RRG and LLaVA-Med. The best result is presented in bold. The most important metric is marked in grey.", "description": "This table presents a quantitative comparison of the performance of two models: LLaVA-Med-RRG (the model proposed by the authors) and LLaVA-Med (a baseline model) on the task of radiology report generation.  The results are presented in terms of four standard metrics used to evaluate natural language generation: BLEU, METEOR, ROUGE-L, and B@4.  The best score for each metric is highlighted in bold, and the most important metric (which is indicated as ROUGE-L in the original caption) is shown in gray.  The table provides a concise overview of the comparative performance of the two models and is intended to demonstrate the improvement in the report generation quality achieved by the authors' proposed model.", "section": "5. Experiments"}, {"content": "| # | BL. | Mask | OCF F | OCF C | OICA | Dataset: IU-Xray [10] B@1 | Dataset: IU-Xray [10] B@4 | Dataset: IU-Xray [10] MTR. | Dataset: IU-Xray [10] RGL. |\n|---|---|---|---|---|---|---|---|---|---|\n| 1 | \u2713 |  |  |  |  | 0.475 | 0.165 | 0.187 | 0.371 |\n| 2 | \u2713 | \u2713 |  |  |  | 0.498 | 0.159 | 0.187 | 0.374 |\n| 3 | \u2713 | \u2713 | \u2713 |  |  | 0.501 | 0.170 | 0.206 | 0.360 |\n| 4 | \u2713 | \u2713 | \u2713 | \u2713 |  | **0.503** | 0.172 | 0.211 | 0.354 |\n| 5 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | 0.501 | **0.198** | **0.211** | **0.400** |", "caption": "Table 4: Ablation study on different modules of ORID. The best result is presented in bold. The most important metric is marked in grey.", "description": "This ablation study analyzes the impact of different components within the Organ-Regional Information Driven (ORID) framework on the performance of radiology report generation.  It compares the baseline model against variations that include or exclude specific modules: the organ mask, organ-based cross-modal fusion (OCF), fine-grained analysis (F), coarse-grained analysis (C), and the organ importance coefficient analysis (OICA). The results are evaluated using four metrics: BLEU@1, BLEU@4, METEOR, and ROUGE-L, with the best-performing metric (ROUGE-L) highlighted in gray.  The table demonstrates how each component contributes to the model's overall performance, illustrating their individual effects and the synergistic benefits when combined.", "section": "5.3 Qualitative Analysis"}, {"content": "| Dataset | IU-Xray [10] |  |  | MIMIC-CXR [26] |  |  |\n|---|---|---|---|---|---|---|\n| Train | Val. | Test | Train | Val. | Test |\n| Image | 5.2K | 0.7K | 1.5K | 369.0K | 3.0K | 5.2K |\n| Report | 2.8K | 0.4K | 0.8K | 222.8K | 1.8K | 3.3K |\n| Patient | 2.8K | 0.4K | 0.8K | 64.6K | 0.5K | 0.3K |\n| Avg. Len. | 37.6 | 36.8 | 33.6 | 53.0 | 53.1 | 66.4 |", "caption": "Table 5: The specifications of two benchmark datasets that will be utilized to test the ORID model.", "description": "This table presents a detailed comparison of two benchmark datasets: IU-Xray and MIMIC-CXR, used to evaluate the performance of the ORID model for radiology report generation.  It shows the number of images, reports, and patients in the training, validation, and testing sets for each dataset.  Additionally, it provides the average length of radiology reports in each dataset.", "section": "4. Experiment Settings"}, {"content": "| Organ Mask | Num. | Region | Total Mask |\n|---|---|---|---|\n| Lung lobes | 5 | Lung | 159 |\n| Lung zones | 8 | Lung |  |\n| Lung halves | 2 | Lung |  |\n| Heart region | 6 | Heart |  |\n| Mediastinum | 6 | Mediastinum |  |\n| Diaphragm | 3 | Mediastinum |  |\n| Ribs | 46 | Bone |  |\n| Ribs super | 24 | Bone |  |\n| Trachea | 2 | Pleural |  |\n| Vessels | 6 | Pleural |  |\n| Breast Tissue | 2 | Pleural |  |\n| \u2026 | \u2026 | \u2026 |  |", "caption": "Table 6: The specific information of masks generated by the CXAS model [45], as well as the mask images we ultimately used.", "description": "Table 6 provides a detailed breakdown of the organ masks generated using the CXAS model [45]. It lists the number of regions identified for each organ (lung, heart, mediastinum, bone, and pleura), and shows the total number of masks used in the study after combining these regions.  This table is essential for understanding the data used in the Organ Importance Coefficient Analysis Module and how the organ-specific masks are used in the cross-modal fusion of visual and textual features. This detailed description of mask generation is important for reproducibility of the results and understanding the framework's data processing pipeline.", "section": "A. Appendix"}]