[{"figure_path": "https://arxiv.org/html/2412.04139/x1.png", "caption": "Figure 1: Architectural comparison of expert scaling approaches in large language models. (1)\u00a0PEER stores N\ud835\udc41Nitalic_N standalone experts accessed via product key retrieval, resulting in memory usage that grows linearly with the number of experts, O\u2062(N)\ud835\udc42\ud835\udc41O(N)italic_O ( italic_N ). (2)\u00a0Our proposed Monet-HD (Horizontal Decomposition) partitions experts into bottom and top layers, dynamically composing experts. This reduces space complexity to O\u2062(N)\ud835\udc42\ud835\udc41O(\\sqrt{N})italic_O ( square-root start_ARG italic_N end_ARG ). (3)\u00a0Monet-VD (Vertical Decomposition) orthogonally partitions layers with left and right segments,\nwhile maintaining the same space complexity.", "description": "Figure 1 compares three different approaches for scaling the number of experts in large language models.  The first, PEER, uses a straightforward method where each of N experts is stored separately and retrieved using product keys. This leads to memory usage that increases linearly with N (O(N)).  The second approach, Monet-HD (Horizontal Decomposition), improves efficiency by splitting each expert into a \"bottom\" and \"top\" layer.  During inference, these components are combined dynamically, reducing the memory requirement to the square root of N (O(\u221aN)). The third approach, Monet-VD (Vertical Decomposition), employs a similar strategy but divides the layers into left and right segments, maintaining the same memory efficiency as Monet-HD (O(\u221aN)).", "section": "3 MONET: MIXTURE OF MONOSEMANTIC EXPERTS FOR TRANSFORMERS"}]