[{"content": "| name | date | size | type | Dutch-specific | data transparency | training transparency | finetuned from | wiki fertility | wiki tps | wiki s |\n|---|---|---|---|---|---|---|---|---|---|---|\n| **fietje-2b** | 4/24 | 2.8B | base | yes | yes | yes | phi-2 | 2.05 | 9501.41 \u00b1 0.66 | 440.41 \u00b1 0.03 |\n| **fietje-2b-chat** | 4/24 | 2.8B | chat | yes | yes | yes | fietje-2b-instruct | 2.05 | 9501.41 \u00b1 0.66 | 440.41 \u00b1 0.03 |\n| **fietje-2b-instruct** | 4/24 | 2.8B | instruct | yes | yes | yes | fietje-2b | 2.05 | 9501.70 \u00b1 4.72 | 440.40 \u00b1 0.22 |\n| **GEITje-7B-ultra** | 1/24 | 7.2B | chat | yes | yes | yes | GEITje-7B | 1.97 | 4035.27 \u00b1 0.64 | 999.42 \u00b1 0.16 |\n| **Llama-3.2-3B-Instruct** | 9/24 | 3.2B | instruct | no | partly | partly | Llama-3.2-3B | 1.74 | 7884.63 \u00b1 0.36 | 451.97 \u00b1 0.02 |\n| **phi-2** | 12/23 | 2.8B | base | no | no | no | none | 2.05 | 9631.95 \u00b1 16.12 | 434.44 \u00b1 0.73 |\n| **Phi-3.5-mini-instruct** | 8/24 | 3.8B | instruct | underspecified | no | no | none | 1.89 | 6633.85 \u00b1 0.68 | 584.14 \u00b1 0.06 |\n| **Mistral-7B-Instruct-v0.1** | 9/23 | 7.2B | instruct | no | unclear<sup class=\"ltx_note_mark\">16</sup> | no | none | 1.97 | 4027.81 \u00b1 1.14 | 1001.27 \u00b1 0.28 |\n| **Mistral-7B-v0.1** | 9/23 | 7.2B | base | no | no | no | Mistral-7B-v0.1 | 1.97 | 4046.46 \u00b1 0.67 | 996.66 \u00b1 0.16 |\n| **Qwen2.5-3B-Instruct** | 9/24 | 3.1B | instruct | underspecified | no | no | Qwen2.5-3B | 1.82 | 8094.26 \u00b1 0.53 | 459.94 \u00b1 0.03 |\n| **GEITje-7B** | 12/23 | 7.2B | base | yes | partly | yes | Mistral-7B-v0.1 | 1.97 | 4021.61 \u00b1 1.64 | 1002.82 \u00b1 0.41 |\n| **tweety-7b-dutch-v24a** | 5/24 | 7.4B | base | yes | yes | partly | Mistral-7B-v0.1 | 1.41 | 3979.88 \u00b1 2.12 | 728.56 \u00b1 0.39 |\n| **Boreas-7B** | 4/24 | 7.2B | base | yes | partly | partly | Mistral-7B-v0.1 | 1.97 | 4032.05 \u00b1 15.28 | 1000.23 \u00b1 3.78 |\n| **Boreas-7B-chat** | 4/24 | 7.2B | instruct | yes | partly | partly | Boreas-7B | 1.97 | 4034.36 \u00b1 0.69 | 999.65 \u00b1 0.17 |", "caption": "Table 1: Overview of benchmarked models. Dutch-specific: did the model undergo (re-)training specifically for Dutch? data/training transparency: are the data and training procedure described in detail (reproducible) and is the data and training code publicly available? wiki fertility: how many tokens are needed on average to encode one word, calculated on full Dutch Wikipedia. Lower = more efficient. wiki tps: Tokens-per-second throughput on first 10,000 Wikipedia documents. How many tokens can the model process per second. wiki s: Processing time of first 10,000 Wikipedia documents. Lower = faster. wiki tps and wiki s were calculated on an isolated RTX 3090 in bfloat16 with Flash Attention 2 enabled. Batch size was 1 and all models\u2019 maximum context length was used, or 8192 at most. The reported mean metrics and their CI are based on the results of three runs.", "description": "This table compares various large language models (LLMs), focusing on their suitability for Dutch language processing.  It details model characteristics such as size (in parameters), type (base, instruct, or chat), whether it was specifically trained for Dutch, the model it was fine-tuned from (if applicable), and the transparency of its data and training procedures (i.e., how reproducible the model is).  Key performance metrics are provided, including 'wiki fertility' (average number of tokens required to encode a single word in Dutch Wikipedia), 'wiki tps' (tokens processed per second on a portion of Dutch Wikipedia), and 'wiki s' (processing time for the same portion). These metrics provide insights into each model's efficiency and speed. The data for 'wiki tps' and 'wiki s' were collected under controlled conditions on a single RTX 3090 GPU using bfloat16 precision and Flash Attention 2, with a batch size of 1 and context length at the model's maximum or 8192 tokens. Reported means and confidence intervals (CI) are based on three runs.", "section": "4. Model overview"}, {"content": "| Model | Global MMLU | Global MMLU rank | Global DBRD | Global DBRD rank | Dutch CoLA | Dutch CoLA rank | Global ARC | Global ARC rank | Global XLWIC | Global XLWIC rank | median rank |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| **Phi-3.5-mini-instruct** | 48.34 \u00b1 0.10 | 2 | 92.31 \u00b1 0.13 | 2 | 58.43 \u00b1 0.09 | 2 | 65.31 \u00b1 0.22 | 2 | 37.39 \u00b1 0.31 | 8 | 1.0 |\n| **Qwen2.5-3B-Instruct** | 50.33 \u00b1 0.14 | 1 | 91.70 \u00b1 0.15 | 3 | 63.74 \u00b1 0.17 | 1 | 66.97 \u00b1 0.45 | 1 | 36.05 \u00b1 0.38 | 12 | 2.0 |\n| **Boreas-7B-chat** | 44.93 \u00b1 0.15 | 3 | 94.38 \u00b1 0.27 | 1 | 52.87 \u00b1 0.42 | 4 | 59.88 \u00b1 0.66 | 3 | 33.78 \u00b1 0.34 | 14 | 3.5 |\n| **Llama-3.2-3B-Instruct** | 35.59 \u00b1 0.22 | 4 | 59.74 \u00b1 0.97 | 8 | 55.35 \u00b1 1.45 | 3 | 42.80 \u00b1 0.93 | 4 | 42.72 \u00b1 0.76 | 3 | 3.5 |\n| **Mistral-7B-Instruct-v0.1** | 32.30 \u00b1 0.38 | 5 | 79.73 \u00b1 0.67 | 5 | 40.29 \u00b1 0.75 | 14 | 36.72 \u00b1 1.06 | 5 | 40.03 \u00b1 0.53 | 4 | 5.0 |\n| **GEITje-7B-ultra** | 24.39 \u00b1 0.18 | 12 | 90.00 \u00b1 0.37 | 4 | 46.57 \u00b1 0.59 | 9 | 29.10 \u00b1 0.56 | 8 | 44.45 \u00b1 0.79 | 1 | 6.0 |\n| **tweety-7b-dutch-v24a** | 27.36 \u00b1 0.32 | 7 | 40.22 \u00b1 1.04 | 14 | 51.27 \u00b1 1.00 | 5 | 29.46 \u00b1 1.25 | 7 | 43.23 \u00b1 1.07 | 2 | 7.0 |\n| **fietje-2b-chat** | 26.36 \u00b1 0.25 | 9 | 58.78 \u00b1 0.58 | 9 | 45.45 \u00b1 0.64 | 10 | 31.56 \u00b1 0.78 | 6 | 39.24 \u00b1 0.94 | 5 | 8.0 |\n| **Boreas-7B** | 27.02 \u00b1 0.63 | 8 | 70.33 \u00b1 1.12 | 6 | 49.34 \u00b1 0.51 | 6 | 26.19 \u00b1 0.85 | 12 | 37.24 \u00b1 0.98 | 10 | 9.5 |\n| **fietje-2b-instruct** | 24.93 \u00b1 0.27 | 11 | 51.38 \u00b1 0.76 | 12 | 49.31 \u00b1 0.78 | 7 | 28.70 \u00b1 0.82 | 9 | 38.61 \u00b1 1.00 | 6 | 9.5 |\n| **Mistral-7B-v0.1** | 27.51 \u00b1 0.15 | 6 | 63.69 \u00b1 0.85 | 7 | 48.00 \u00b1 0.51 | 8 | 26.82 \u00b1 0.85 | 11 | 37.27 \u00b1 0.88 | 9 | 11.0 |\n| **GEITje-7B** | 25.12 \u00b1 0.37 | 10 | 46.28 \u00b1 0.78 | 13 | 43.67 \u00b1 0.72 | 11 | 27.61 \u00b1 1.30 | 10 | 37.64 \u00b1 0.75 | 7 | 12.0 |\n| **phi-2** | 20.82 \u00b1 0.28 | 14 | 51.45 \u00b1 0.66 | 11 | 42.29 \u00b1 0.75 | 12 | 18.07 \u00b1 0.52 | 14 | 36.55 \u00b1 0.97 | 11 | 13.0 |\n| **fietje-2b** | 24.09 \u00b1 0.43 | 13 | 52.44 \u00b1 1.23 | 10 | 41.41 \u00b1 0.37 | 13 | 24.44 \u00b1 0.89 | 13 | 34.28 \u00b1 0.70 | 13 | 14.0 |", "caption": "Table 2: Benchmark results, showing weighted F1 score and the 95% confidence interval (obtained by running each benchmark five times on each model). Models\u2019 ranks are also given, although they should be taken with a grain of salt considering overlapping confidence intervals. The last column illustrates the final median ranking across all benchmarks.", "description": "This table presents the performance of various large language models (LLMs) across five benchmark tasks: ARC (reasoning), DBRD (sentiment analysis), Dutch CoLA (grammatical acceptability), Global MMLU (multilingual understanding), and XLWIC-NL (word sense disambiguation).  For each model, the weighted F1 score and its 95% confidence interval are shown, reflecting the average performance across five runs of each benchmark.  Model ranks are also provided, but these should be interpreted cautiously due to potential overlaps in confidence intervals. The final column displays the median rank of each model across all five benchmarks, offering a summarized performance comparison.", "section": "4. Results"}]