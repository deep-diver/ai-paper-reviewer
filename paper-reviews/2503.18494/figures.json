[{"figure_path": "https://arxiv.org/html/2503.18494/extracted/6304269/images/cura-arch.jpg", "caption": "Figure 1: The CURA architecture: a process-supervised reasoning framework incorporating verbal reward signals.", "description": "The CURA architecture is an iterative process-supervised reasoning framework for code generation.  It starts with understanding the problem, generating test cases, and then generating code through solution reasoning. The generated code is executed in a sandbox to verify its correctness. At each step, a process reward model provides verbal reward signals, guiding the model towards better solutions. These signals refine the model's behavior without the need for fine-tuning, enhancing its performance iteratively.", "section": "3.1 CURA Architecture"}, {"figure_path": "https://arxiv.org/html/2503.18494/extracted/6304269/images/output-bigcodebench-hard-o3-mini.png", "caption": "Figure 2: Comparison of o3-mini Baseline vs. o3-mini CURA with VPS on the BigCodeBench (Hard) dataset.\nThe y-axis shows the score (in %), while the x-axis shows three different evaluation modes\n(Complete, Instruct, and the Average of all modes).\nNotice that o3-mini VPS shows an improvement in all categories, with the largest gain in the \u201cComplete\u201d mode.", "description": "Figure 2 presents a comparison of the performance of two models on the BigCodeBench (Hard) dataset: the baseline o3-mini model and the o3-mini model enhanced with CURA and Verbal Process Supervision (VPS). The bar chart displays the scores (in percentages) achieved by each model across three evaluation categories: Complete, Instruct, and Average. The y-axis represents the scores, while the x-axis represents the evaluation categories.  The results reveal that the o3-mini model with CURA and VPS demonstrates improved performance across all categories compared to the baseline, with the most significant improvement observed in the 'Complete' category.", "section": "4.1 BigCodeBench with Reasoning Model"}, {"figure_path": "https://arxiv.org/html/2503.18494/extracted/6304269/images/hard-comparsion.png", "caption": "Figure 3: Performance comparison of GPT-4o-mini and Mistral Large Latest on the BigCodeBench using CURA architecture with VPS technique - Hard Benchmark across different temperature settings. The models are evaluated in three categories: Complete, Instruct, and Average. Results indicate that deterministic decoding (Temp=0) generally leads to higher scores, particularly in the Complete category where Mistral Large Latest outperforms other configurations. Increasing temperature (Temp=1) negatively impacts performance across all categories, highlighting the trade-offs between deterministic and stochastic decoding in code generation tasks.", "description": "This figure displays a performance comparison between two large language models (LLMs), GPT-40-mini and Mistral Large Latest, on the BigCodeBench-Hard benchmark.  The models were tested using the CURA architecture with Verbal Process Supervision (VPS) at two different temperature settings: 0 (deterministic) and 1 (stochastic). The performance is broken down into three categories: Complete (generating complete code), Instruct (following instructions), and Average (overall performance). The results show that deterministic decoding (Temp=0) generally leads to better scores, particularly in the Complete code generation category, where Mistral Large Latest outperforms GPT-40-mini. Increasing the temperature to 1 introduces more randomness which negatively affects performance across all categories, highlighting a trade-off between deterministic and stochastic code generation methods.", "section": "4 Experiments"}]