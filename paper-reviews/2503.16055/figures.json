[{"figure_path": "https://arxiv.org/html/2503.16055/x1.png", "caption": "Figure 1: Overview of the SALT architecture. Starting from the segmentation model and providing a detailed view of the SALT mechanism within the Multi-Head Attention layers of the image encoder.", "description": "Figure 1 illustrates the SALT (Singular Value Adaptation with Low-Rank Transformation) architecture, which enhances the Segment Anything Model (SAM) for medical image segmentation.  The diagram shows the overall model workflow, starting with the input image and prompt, progressing through the image encoder, Multi-Head Attention layers, and decoder to produce the final segmentation mask.  The detailed view focuses on the SALT mechanism integrated into the Multi-Head Attention layers of the image encoder.  This mechanism is key to the algorithm's parameter efficiency and ability to adapt the SAM to the unique characteristics of medical images.  SALT achieves this by selectively scaling and shifting the most influential singular values while simultaneously adding a low-rank update for the remaining values, thus improving performance without increasing the overall model size.", "section": "2 Methodology"}, {"figure_path": "https://arxiv.org/html/2503.16055/x2.png", "caption": "Figure 2: Comparison of PEFT methods: (A) Full fine-tuning updates all W\ud835\udc4aWitalic_W parameters. (B) LoRA updates low-rank matrices. (C) SVD fine-tunes scale and shift components of W\ud835\udc4aWitalic_W. (D) SALT (Ours) integrates scale, shift, and low-rank updates for singular values.", "description": "Figure 2 illustrates four different parameter-efficient fine-tuning (PEFT) methods for adapting pre-trained models to new datasets. (A) Full fine-tuning, the most computationally expensive method, updates all the model's parameters. (B) LoRA (Low-Rank Adaptation) uses low-rank matrices for efficient updates, only modifying a small subset of parameters. (C) SVD (Singular Value Decomposition)-based methods update the model's singular values, representing another efficient update strategy. (D) SALT (Singular Value Adaptation with Low-Rank Transformation), the proposed method, combines the strengths of both LoRA and SVD by selectively fine-tuning the most important singular values with scale and shift parameters, and also applying low-rank updates to the remaining subspace.  This hybrid approach aims for better adaptation compared to using only LoRA or SVD.", "section": "2 Methodology"}, {"figure_path": "https://arxiv.org/html/2503.16055/x3.png", "caption": "Figure 3: Qualitative Vessel Segmentation. The figure displays segmentation results for five datasets (Arcade [17], Rose [14], Dias [12], X-ray Angio [2], Drive [22]). Columns show the ground truth, SALT, SAM-based PEFT methods, and traditional DL models. Predicted masks are white, and false negatives are red.", "description": "Figure 3 showcases a qualitative comparison of vessel segmentation results across five different medical datasets: ARCADE [17], ROSE [14], DIAS [12], X-ray Angio [2], and DRIVE [22].  Each row represents a single dataset and shows the original image, the ground truth segmentation, and segmentation results from several different methods.  These methods include U-Net, UNETR, LoRA, S-SAM (other SAM-based PEFT methods), and the proposed SALT method. The predicted masks are shown in white, while any false negatives (areas missed by the segmentation) are highlighted in red. This visual comparison demonstrates SALT's performance relative to other state-of-the-art segmentation techniques in diverse medical imaging scenarios.", "section": "4 Results and Discussion"}, {"figure_path": "https://arxiv.org/html/2503.16055/x4.png", "caption": "(a)", "description": "The figure shows an ablation study on SALT configurations and adaptability to SAM2.  Subfigure (a) presents a comparison of Dice scores across different combinations of LoRA and SALT ranks for Multi-Head Attention (MHA), convolutional layers, and MLP modules. Circle size corresponds to the percentage of trainable parameters.  This illustrates the impact of varying the rank (the number of top singular values modified using scale and shift) on model performance and parameter efficiency.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.16055/x5.png", "caption": "(b)", "description": "The figure shows an ablation study on SALT configurations and their adaptability to SAM2.  It compares the performance of different methods (SALT, LoRA, SVD) on two datasets of varying sizes by assessing Dice scores and the number of trainable parameters used. The x-axis indicates various rank combinations for convolutional, MLP, and multi-head attention layers, illustrating the impact of rank selection on performance. The y-axis represents the Dice score. Circle size is proportional to the percentage of trainable parameters.", "section": "3 Experiments"}]