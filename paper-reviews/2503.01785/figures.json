[{"figure_path": "https://arxiv.org/html/2503.01785/x1.png", "caption": "Figure 1: Our Visual Reinforcement Fine-Tuning (Visual-RFT) performs better than previous Supervised Fine-Tuning (SFT) on a variety of tasks, such as Open Vocabulary(OV)/Few-shot Detection, Reasoning Grounding, and Fine-grained Classification.", "description": "Figure 1 showcases the superior performance of Visual Reinforcement Fine-Tuning (Visual-RFT) compared to Supervised Fine-Tuning (SFT) across various visual tasks.  The figure presents several bar charts and graphs that illustrate the improvement in accuracy achieved by Visual-RFT over SFT in open vocabulary (OV) and few-shot object detection, reasoning grounding, and fine-grained image classification. Each chart displays metrics like mean average precision (mAP) or Intersection over Union (IoU), highlighting the significant gains obtained through the Visual-RFT method. The visualization effectively demonstrates that Visual-RFT is a data-efficient technique that surpasses SFT by a significant margin, particularly beneficial when fine-tuning data is scarce.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.01785/x2.png", "caption": "Figure 2: Overview of Visual-RFT. Compared to the (a) Visual Instruction Tuning that is data-hungry, (b) our Visual Reinforcement Fine-Tuning (Visual-RFT) is more data efficient with limited data.\n(c) We successfully empower Large Vision-Language Models (LVLMs) with RFT on a series of multi-modal tasks, and present examples of the model\u2019s reasoning process at the bottom.", "description": "Figure 2 illustrates the core concept of Visual-RFT and its advantages over traditional Visual Instruction Tuning.  Panel (a) shows Visual Instruction Tuning, which requires large-scale datasets for training. In contrast, panel (b) presents Visual-RFT, highlighting its data efficiency by requiring only limited data (10-1000 samples) for effective fine-tuning.  This is achieved through reinforcement learning with verifiable rewards, shown schematically. Panel (c) demonstrates the successful application of Visual-RFT to various multi-modal tasks (detection, grounding, and classification), showcasing the model's ability to perform reasoning and generate detailed answers, as exemplified in the detailed examples at the bottom of the figure. ", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.01785/x3.png", "caption": "Figure 3: Framework of Visual-RFT. Given the question and visual image inputs, the policy model generates multiple responses containing reasoning steps. Then the verifiable reward such as IoU reward and CLS reward is used with the policy gradient optimization algorithm to update the policy model.", "description": "Visual-RFT's framework involves receiving a question and a visual image as input.  A policy model processes this input and generates multiple response options, each including reasoning steps and a final answer. The quality of each response is assessed using a verifiable reward function, specifically the Intersection over Union (IoU) reward for object detection tasks and a Classification (CLS) reward for classification tasks.  These reward signals, combined with a policy gradient optimization algorithm, are then used to iteratively refine the policy model, improving its ability to generate accurate and well-reasoned responses.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.01785/x4.png", "caption": "Figure 4: Qualitative results of Fine-Grained Image Classification. The thinking process significantly improves the reasoning ability of LVLMs, leading to higher image classification performance.", "description": "Figure 4 presents a qualitative comparison of fine-grained image classification results between standard supervised fine-tuning (SFT) and the proposed Visual-RFT method.  The figure showcases examples where Visual-RFT significantly enhances the reasoning process of Large Vision-Language Models (LVLMs). Visual-RFT leverages a verifiable reward mechanism during reinforcement learning, guiding the model toward more accurate classification by explicitly incorporating a 'thinking' step.  This contrasts with the direct prediction of SFT. The improved reasoning process, as demonstrated by the more detailed and accurate reasoning steps within the 'thinking' section of the Visual-RFT outputs, leads to substantially better classification accuracy. The figure provides visual examples of the differences in reasoning quality and accuracy between both approaches for fine-grained classification tasks.", "section": "3.2.1 Verifiable Reward in Visual Perception"}, {"figure_path": "https://arxiv.org/html/2503.01785/x5.png", "caption": "Figure 5: Qualitative results of reasoning grounding on LISA\u00a0[11]. Thinking process significantly improves reasoning grounding ability with Visual-RFT.", "description": "Figure 5 presents a qualitative comparison of reasoning grounding results on the LISA dataset between the standard Supervised Fine-Tuning (SFT) approach and the proposed Visual-RFT method.  The figure showcases several examples where Visual-RFT significantly enhances reasoning capabilities leading to more accurate and contextually appropriate grounding of objects within images.  Each example consists of a question, SFT model's reasoning and answer, and the corresponding results from the Visual-RFT model. The Visual-RFT responses show a clear improvement in the quality of the reasoning process and in the accuracy of the grounding boxes.", "section": "4. Experiments"}]