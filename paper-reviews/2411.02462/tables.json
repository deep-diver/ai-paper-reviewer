[{"content": "| Hyperparameter | Method | Value |\n|---|---|---|\n| **Common** |  |  |\n| Optimizer | - | AdamW |\n| LR schedule | - | Linear |\n| LR warmup ratio | - | 0.1 |\n| Batch size | - | 1 |\n| Gradient accumulation steps | - | 8 |\n| # Epochs | - | 3 |\n| Precision | - | Mixed |\n| Learning rate | Full fine-tuning | 5E-5 |\n|  | LoRA | 3E-4 |\n|  | (IA)<sup class=\"ltx_sup\">3</sup> | 3E-4 |\n|  | Prompt tuning | 3E-3 |\n| **Method specific** |  |  |\n| Alpha | LoRA | 32 |\n| Dropout | LoRA | 0.1 |\n| Rank | LoRA | 16 |\n| Virtual tokens | Prompt tuning | 20 |", "caption": "TABLE I: Model-agnostic hyperparameters for training.", "description": "This table lists the hyperparameters used during the training process, excluding those specific to individual models.  It shows settings common to all training methods, including the optimizer used (AdamW), the learning rate schedule (linear), the learning rate warmup ratio, batch size, gradient accumulation steps, number of epochs, and the precision used.  It also includes the specific learning rates used for each of the training methods: full fine-tuning, LoRA, (IA)\u00b3, and prompt tuning.", "section": "IV. EXPERIMENTAL DESIGN"}, {"content": "| Hyperparameter | Method | Model | Value |\n|---|---|---|---| \n| Targeted attention modules | LoRA, (IA)<sup>3</sup> | codegen-350M-multi | qkv_proj |\n|  |  | Salesforce/codegen2-1B_P | qkv_proj |\n|  |  | Salesforce/codegen2-3_7B_P | qkv_proj |\n|  |  | Salesforce/codegen2-7B_P | qkv_proj |\n|  |  | Salesforce/codegen2-16B_P | qkv_proj |\n|  |  | meta-llama/CodeLlama-7b-hf | q_proj, v_proj |\n|  |  | bigcode/starcoderbase | c_attn |\n|  |  | bigcode/starcoder2-3b | q_proj, v_proj |\n|  |  | bigcode/starcoder2-7b | q_proj, v_proj |\n|  |  | bigcode/starcoder2-15b | q_proj, v_proj |\n| Targeted feedforward modules | (IA)<sup>3</sup> | codegen-350M-multi | fc_out |\n|  |  | Salesforce/codegen2-1B_P | fc_out |\n|  |  | Salesforce/codegen2-3_7B_P | fc_out |\n|  |  | Salesforce/codegen2-7B_P | fc_out |\n|  |  | Salesforce/codegen2-16B_P | fc_out |\n|  |  | meta-llama/CodeLlama-7b-hf | down_proj |\n|  |  | bigcode/starcoderbase | mlp.c_proj |\n|  |  | bigcode/starcoder2-3b | q_proj, c_proj |\n|  |  | bigcode/starcoder2-7b | q_proj, c_proj |\n|  |  | bigcode/starcoder2-15b | q_proj, c_proj |", "caption": "TABLE II: Model-specific hyperparameters for training.", "description": "This table details the model-specific hyperparameters used during the training phase of the experiment.  It shows which specific modules within each model architecture were targeted for modification by the different parameter-efficient fine-tuning (PEFT) methods used in the study.  Specifically, it indicates which attention and feed-forward modules were adjusted for LoRA and (IA)\u00b3 methods. The table is crucial for reproducibility as it provides the exact configurations used in the PEFT training process for each model, allowing researchers to recreate the experimental setup.", "section": "IV. Experimental Design"}, {"content": "| Hyperparameters | Value |\n|---|---| \n| Do sample | False |\n| Temperature | 0 |\n| Top p | 0 |\n| Frequency penalty | 0 |\n| Max length | 2048 |", "caption": "TABLE III: Hyperparameters for generation.", "description": "This table lists the hyperparameters used during the unit test generation phase of the experiment.  It includes parameters such as whether sampling is enabled (`Do sample`), temperature, top p, frequency penalty, and the maximum sequence length allowed. These hyperparameters control the randomness and length of the generated unit tests.", "section": "IV. Experimental Design"}, {"content": "Model|Method|Trainable params|Methods2Test<sub>small</sub>|Methods2Test<sub>small</sub>|HumanEval-X<sub>java</sub>|HumanEval-X<sub>java</sub>|HumanEval-X<sub>java</sub>|HumanEval-X<sub>java</sub>|HumanEval-X<sub>java</sub>\n---|---|---|---|---|---|---|---|---\nCodeGen-350M-multi|None|0|95.43%|0.2170|100%|0.3608|0.0671|97.33%|89.77%\n|Full fine-tuning|304.23M|97.87%|0.2988|100%|0.3293|0.0366|100%|83.33%\n|LoRA|1.31M|95.22%|0.2553|100%|0.3907|0.0671|98.69%|89.29%\n|(IA)<sup>3</sup>|0.14M|95.53%|0.2266|100%|0.3583|0.0549|97.69%|94.32%\n|Prompt tuning|0.02M|96.03%|0.2208|100%|0.3290|0.0427|96.56%|91.67%\nCodeGen2-1B|None|0|0%|0|0%|0|0|0%|0%\n|Full fine-tuning|1,015.31M|76.73%|0.1474|5.49%|0.0359|0|0%|0%\n|LoRA|2.10M|41.16%|0.0484|8.54%|0.0117|0|0%|0%\n|(IA)<sup>3</sup>|0.23M|1.52%|0.2553|0%|0|0|0%|0%\n|Prompt tuning|0.04M|66.63%|0.2568|7.93%|0.2547|0|0%|0%\nStarCoder2-3B|None|0|85.23%|0.1543|100%|0.4264|0.3152|9.89%|85.67%\n|Full fine-tuning|3,030.37M|96.71%|0.2786|100%|0.4969|0.3494|99.40%|86.37%\n|LoRA|4.55M|97.11%|0.2901|100%|0.4169|0.3675|99.19%|58.84%\n|(IA)<sup>3</sup>|0.47M|87.43%|0.2513|100%|0.4250|0.2744|99.67%|83.81%\n|Prompt tuning|0.06M|86.43%|0.1742|100%|0.4309|0.2470|99.6%|75.85%\nCodeGen2-3.7B|None|0|0%|0|0%|0|0|0%|0%\n|Full fine-tuning|3,641.17M|50.51%|0.1006|73.78%|0.2621|0|0%|0%\n|LoRA|4.19M|52.24%|0.0997|40.24%|0.1384|0|0%|0%\n|(IA)<sup>3</sup>|0.46M|0%|0|0%|0|0|0%|0%\n|Prompt tuning|0.08M|23.50%|0.2562|0%|0|0|0%|0%\nCodeLlama-7B|None|0|97.66%|0.3107|99.39%|0.4861|0.3293|98.33%|84.46%\n|Full fine-tuning|6,607.41M|96.44%|0.3012|100%|0.4994|0.3373|98.95%|86.37%\n|LoRA|8.39M|97.36%|0.3277|99.39%|0.4291|0.3129|99.61%|72.47%\n|(IA)<sup>3</sup>|0.61M|97.05%|0.3011|100%|0.4802|0.3232|98.77%|84.72%\n|Prompt tuning|0.08M|95.93%|0.2885|99.39%|0.4617|0.2761|98.38%|82.25%\nCodeGen2-7B|None|0|96.95%|0.2848|100%|0.4736|0.2256|98.31%|81.45%\n|Full fine-tuning|6,862.87M|97.56%|0.3107|100%|0.4398|0.1280|99.75%|70.00%\n|LoRA|8.39M|97.87%|0.3164|100%|0.4636|0.2073|98.06%|75.35%\n|(IA)<sup>3</sup>|0.92M|97.36%|0.2904|100%|0.4898|0.1829|98.55%|79.50%\n|Prompt tuning|0.08M|96.64%|0.2775|100%|0.4407|0.2012|99.10%|69.40%\nStarCoder2-7B|None|0|84.13%|0.1610|100%|0.4027|0.3758|99.07%|83.16%\n|Full fine-tuning|7,173.92M|97.21%|0.3009|100%|0.4389|0.3675|99.15%|90.80%\n|LoRA|7.34M|96.91%|0.3068|100%|0.5179|0.3394|99.35%|87.06%\n|(IA)<sup>3</sup>|0.75M|94.83%|0.2903|100%|0.4213|0.3697|99.40%|88.39%\n|Prompt tuning|0.09M|83.03%|0.3030|100%|0.5057|0.3476|99.38%|86.02%\nStarCoderBase|None|0|84.63%|0.1563|98.78%|0.4338|0.2963|99.07%|81.48%\n|Full fine-tuning|15,517.46M|96.81%|0.3123|100%|0.4830|0.3293|99.16%|75.20%\n|LoRA|8.03M|95.71%|0.3152|98.78%|0.3905|0.2963|99.07%|73.89%\n|(IA)<sup>3</sup>|1.24M|84.63%|0.1553|98.78%|0.4344|0.1562|98.72%|81.48%\n|Prompt tuning|0.12M|85.73%|0.1518|78.05%|0.2315|0.3025|99.76%|67.62%\nStarCoder2-15B|None|0|85.43%|0.1898|100%|0.3724|0.4085|98.93%|87.69%\n|Full fine-tuning|15,655.90M|97.90%|0.3323|99.39%|0.4886|0.3758|99.52%|81.3%\n|LoRA|12.12M|97.01%|0.3272|100%|0.4633|0.4146|98.95%|82.88%\n|(IA)<sup>3</sup>|1.25M|85.43%|0.1901|100%|0.3725|0.4578|99.57%|87.89%\n|Prompt tuning|0.12M|97.60%|0.3133|100%|0.5352|0.3939|99.32%|82.89%\nCodeGen2-16B|None|0|97.87%|0.2784|100%|0.4779|0.2012|98.66%|80.56%\n|Full fine-tuning|16,032.16M|97.56%|0.3383|98.17%|0.3774|0.1180|99.52%|78.07%\n|LoRA|13.37M|98.68%|0.3186|100%|0.4714|0.2012|98.66%|82.06%\n|(IA)<sup>3</sup>|1.46M|97.87%|0.2790|100%|0.4780|0.2134|97.46%|80.56%\n|Prompt tuning|0.08M|97.97%|0.2954|100%|0.4679|0.2195|98.62%|71.07%", "caption": "TABLE IV: Comparison of syntactical validity and CodeBLEU scores from experiments using different tuning methods across various models on testing split of Methods2Testsmall and HumanEval-Xjava datasets.", "description": "Table IV presents a detailed comparison of the performance of various parameter-efficient fine-tuning (PEFT) methods and full fine-tuning for unit test generation.  It assesses performance across ten different large language models (LLMs) of varying sizes and architectures, utilizing two benchmark datasets: METHODS2Testsmall and HumanEval-Xjava. The table shows the syntactical validity of the generated unit tests (percentage of syntactically correct tests), the CodeBLEU scores (measuring similarity to reference tests), pass@1 (the percentage of tests that passed), instruction coverage, and branch coverage for each LLM and tuning method (LoRA, (IA)\u00b3, prompt tuning, and full fine-tuning). This provides a comprehensive analysis of the effectiveness and efficiency of each method for unit test generation.", "section": "V. Experimental Results"}]