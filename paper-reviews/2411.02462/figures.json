[{"figure_path": "https://arxiv.org/html/2411.02462/x1.png", "caption": "(a) Diagram of LoRA (based on [15]).", "description": "This figure shows a diagram of the LoRA (Low-Rank Adaptation) method for parameter-efficient fine-tuning.  It illustrates how LoRA works by adding low-rank updates to the weight matrices of the pre-trained model's attention layers, instead of fine-tuning all parameters. The diagram highlights the original weight matrices (K and V projection matrices), the low-rank matrices (WA and WB) added by LoRA, and how they are combined. The other modules of the pre-trained model remain unchanged.", "section": "II. PRELIMINARIES"}, {"figure_path": "https://arxiv.org/html/2411.02462/x2.png", "caption": "(b) Diagram of (IA)3 (based on [16]).", "description": "This figure shows the architecture of the Infused Adapter by Inhibiting and Amplifying Inner Activations (IA)\u00b3 method.  It's a type of parameter-efficient fine-tuning (PEFT) technique.  The diagram illustrates how (IA)\u00b3 works by adding three small adapter modules to the pre-trained language model.  These adapters (represented by magenta colored blocks) are trained, while the rest of the pre-trained model's parameters (striped blocks) remain frozen. Each adapter module modifies the flow of information through a specific part of the model, making it more efficient and less computationally expensive compared to full fine-tuning.", "section": "II. PRELIMINARIES"}, {"figure_path": "https://arxiv.org/html/2411.02462/x3.png", "caption": "(c) Diagram of prompt tuning (based on [17]).", "description": "This figure shows an illustration of the prompt tuning method.  In prompt tuning, a small set of trainable parameters, often referred to as \"soft prompts\", are prepended to the input embeddings of the language model.  Only these additional parameters are trained during the fine-tuning process, while the original model weights remain frozen. This approach enables adaptation to a specific task without adjusting all the model parameters, thus improving efficiency and potentially reducing the risk of overfitting or catastrophic forgetting. The diagram depicts the addition of these 'soft prompt' parameters to the input before processing by the main language model. ", "section": "II. PRELIMINARIES"}]