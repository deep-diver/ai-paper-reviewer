[{"figure_path": "https://arxiv.org/html/2502.12574/x1.png", "caption": "Figure 1: Estimated memory consumption of inference a Llama-3-8B model with 1 million token on a single GPU.", "description": "The bar chart visualizes the memory usage breakdown for inferencing a Llama-3-8B model with a 1 million token input sequence on a single GPU.  It compares the memory consumption of the key-value cache (KV cache), activations, and model weights.  The chart highlights the disproportionately large memory footprint of the KV cache compared to the other components, emphasizing the memory challenge posed by long context inference.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.12574/x2.png", "caption": "Figure 2: Demonstrations of KV cache policies in inference. Full KV cache contains two main dimensions: layer and head. Layer-wise offloads KV cache in the layer\u2019s dimension, with a cache budget of all heads per layer. HeadInfer further reduces GPU memory by adaptively reallocating cache budgets in the head\u2019s dimension, with a cache budget of one head.", "description": "This figure illustrates different strategies for managing the key-value (KV) cache in large language model (LLM) inference.  The KV cache stores intermediate activations crucial for efficient attention computation.  The figure shows three scenarios: (1) No offloading: the entire KV cache resides in GPU memory, which is inefficient for long sequences; (2) Layer-wise offloading: KV cache for each layer is treated as a single unit, and a portion may be offloaded to CPU RAM, reducing GPU memory footprint, but the granularity is quite coarse; (3) HeadInfer: the KV cache is managed at a much finer granularity - per attention head.  Only the necessary heads' KV cache remain in GPU memory, with others offloaded to CPU RAM, offering a significant reduction in GPU memory use compared to Layer-wise offloading.  This head-wise adaptive strategy optimizes memory usage dynamically during inference.", "section": "3. HEADINFER: Head-wise Offload"}, {"figure_path": "https://arxiv.org/html/2502.12574/x3.png", "caption": "Figure 3: Granularity of different methods. Each cube represents the entire attention process along three dimensions: Sequence (S), Layers (L), and Heads (H). Standard inference puts everything on the GPU.\nChunked-prefill fetches only a part of the sequence dimension of all tokens on the GPU at a time.\nLayer-wise offloading fetches a subset of layers on the GPU, offloading the rest.\nHeadInfer introduces an even finer approach that maintains only selected heads within a layer.", "description": "Figure 3 illustrates the different granularities of various LLMs inference methods.  Each cube visually represents the entire attention process, broken down into three dimensions: sequence length (S), number of layers (L), and number of attention heads (H).  Standard inference loads the entire computation onto the GPU, resulting in high memory consumption. Chunked pre-filling optimizes memory usage by processing only a portion of the sequence at a time. Layer-wise offloading reduces memory footprint by moving some layers from GPU memory to CPU memory.  Finally, HeadInfer employs a head-wise approach, significantly reducing memory needs by keeping only a small subset of attention heads in GPU memory and offloading the rest to the CPU, offering the finest granularity.", "section": "3. HEADINFER: Head-wise Offload"}, {"figure_path": "https://arxiv.org/html/2502.12574/x4.png", "caption": "Figure 4: HeadInfer snapshot. All parameters are stored on the GPU. Head-wise partitioned KV cache is moved across GPU and CPU with the ping-pong memory.", "description": "This figure illustrates the memory management strategy of HEADINFER.  Unlike traditional methods where the entire key-value (KV) cache resides on the GPU, HEADINFER partitions the KV cache into individual heads. Only a single head's KV cache is held on the GPU at any given time, with the remaining heads stored in CPU RAM.  The ping-pong memory mechanism is used to efficiently transfer data between the GPU and CPU, ensuring that computation on the GPU is not stalled by data movement. This approach drastically reduces the GPU memory footprint, enabling inference with significantly longer context lengths.", "section": "HEADINFER for Memory-Efficient Inference"}, {"figure_path": "https://arxiv.org/html/2502.12574/x5.png", "caption": "Figure 5: Flashattention in the roofline plot analysis using the RTX-4090 device setting.", "description": "This roofline plot visualizes the performance of FlashAttention on an NVIDIA RTX 4090 GPU, contrasting different inference methods with varying context lengths. The plot shows the arithmetic intensity (FLOPs/Byte) on the x-axis, representing the computational efficiency, and the performance in TFLOPs/s on the y-axis.  Separate lines represent baseline, offloading, and HEADINFER methods for both prefill (input processing) and decoding (output generation) phases.  The plot helps to understand how these methods perform under different levels of computational intensity and whether they are compute-bound or memory-bound. The plot illustrates HEADINFER's performance relative to other methods under varying context lengths.", "section": "5. Analysis"}, {"figure_path": "https://arxiv.org/html/2502.12574/x6.png", "caption": "Figure 6: HeadInfer provides equal accuracy as standard inference on the Needle-in-a-Haystack benchmark", "description": "The figure displays the results of the Needle-in-a-Haystack benchmark, which assesses a model's ability to accurately identify and retrieve information from a long context.  It shows that HeadInfer achieves the same accuracy as standard inference methods, demonstrating its effectiveness in long-context tasks without sacrificing accuracy. The graph likely shows accuracy or other relevant metric across various context lengths, highlighting HeadInfer's performance consistency across different context sizes.", "section": "Performance Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.12574/x7.png", "caption": "Figure 7: Token eviction methods cannot work when querying the less relevant information to the main theme. Here, we use a 10K document from LongBench (Bai et\u00a0al., 2023b) and add one sentence that is not relevant to the main theme. In this case, H2O discards tokens less relevant to the main theme, leading to error generation. StreamingLLM discards tokens based on the query but remaining question tokens, making it Hallucinations. HeadInfer can successfully output the exact information from the lengthy input, even when we compress 75% of the KV cache", "description": "Figure 7 demonstrates the limitations of token eviction methods in handling less relevant information within long contexts.  A 10,000-token document from the LongBench benchmark is used, with an added sentence unrelated to the main theme.  Existing methods like H2O, which discards less relevant tokens, produce erroneous outputs.  Similarly, StreamingLLM, which discards tokens based on the query but retains question tokens, results in hallucinations. In contrast, HeadInfer correctly retrieves the target information, even with a 75% reduction in KV cache size, highlighting its robustness and efficiency.", "section": "Performance Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.12574/x8.png", "caption": "Figure 8: Flashattention in the roofline plot analysis using A100 device setting.", "description": "This roofline plot visualizes the performance of FlashAttention on an NVIDIA A100 GPU, comparing the performance of standard inference, offloading, and HEADINFER methods at different arithmetic intensity levels.  It shows the peak performance achievable by the GPU's memory bandwidth and computational capabilities, revealing whether the kernel is compute-bound or memory-bound under various conditions.  This helps in understanding how different inference strategies affect performance by comparing their arithmetic intensity and peak throughput against the GPU's theoretical limits.", "section": "5. Analysis"}, {"figure_path": "https://arxiv.org/html/2502.12574/x9.png", "caption": "Figure 9: Demonstrations of KV cache policies in inference from the head-wise view. Upper plots illustrate symbolic plots of an attention map deploying different policies in LLM generation. Lower: the overview of HeadInfer.", "description": "This figure compares different KV cache management strategies for LLM inference. The upper part uses symbolic cubes to visualize the different granularities of the approaches.  Standard inference keeps all keys and values in GPU memory. Chunked prefill loads only a portion of the sequence into GPU memory at a time. Layer-wise offloading loads a subset of layers into memory at once, while head-wise offloading loads only a small number of attention heads into memory at once. The lower part illustrates the HeadInfer architecture, showing the interplay between CPU and GPU and the adaptive head-wise offloading strategy.", "section": "3. HEADINFER: Head-wise Offload"}, {"figure_path": "https://arxiv.org/html/2502.12574/x10.png", "caption": "Figure 10: Workflow of HeadInfer generating a model with (n+1) layers and (j+1) attention heads.", "description": "This figure illustrates the workflow of HeadInfer, a memory-efficient inference framework, during the generation of a model with (n+1) layers and (j+1) attention heads. It showcases the interplay between the GPU and CPU in managing the KV cache, highlighting the asynchronous data transfer and prefetching mechanisms. The GPU processes one head at a time while concurrently prefetching the next head's data from CPU memory, enabling overlapping computations and data transfers. The CPU manages the offloaded KV cache for the remaining heads. Asynchronous data transfers between the GPU and CPU are facilitated using PCIe, minimizing transfer delays.  The diagram visualizes the movement of data between the GPU and CPU memories during prefill and decoding stages, emphasizing the optimized utilization of both GPU and CPU resources for efficient long-context LLM inference.", "section": "4. HEADINFER for Memory-Efficient Inference"}]