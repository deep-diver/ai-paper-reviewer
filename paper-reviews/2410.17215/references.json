{"references": [{" publication_date": "2020", "fullname_first_author": "J. Kaplan", "paper_title": "Scaling laws for neural language models", "reason": "This paper is foundational to the field, establishing scaling laws that govern the performance and cost of training large language models.  Understanding these scaling laws is crucial for the efficient training of small models and forms the basis for the computational considerations in the MINIPLM paper, specifically in controlling training computation and extrapolating to larger scales.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "reason": "This highly influential paper demonstrates the impressive few-shot learning capabilities of large language models, setting the stage for exploring techniques like knowledge distillation to transfer those capabilities to smaller models.  The MINIPLM paper builds upon this foundation by showing how knowledge distillation in pre-training can enhance the zero-shot performance of smaller language models on downstream tasks.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "R. Bommasani", "paper_title": "On the opportunities and risks of foundation models", "reason": "This comprehensive survey paper highlights the societal impact of large language models (LLMs), emphasizing the need for efficient training methods. MINIPLM directly addresses this need by providing an efficient knowledge distillation approach that significantly reduces the computational cost of training small, yet effective, LLMs.", "section_number": 1}, {" publication_date": "2016", "fullname_first_author": "Y. Kim", "paper_title": "Sequence-level knowledge distillation", "reason": "This paper is a seminal work on offline knowledge distillation, providing a foundation for the approach taken in MINIPLM.  While MINIPLM improves upon this by addressing issues of data difficulty and diversity,  SeqKD serves as a key baseline and important comparison point for demonstrating the novel aspects of MINIPLM.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "R. Agarwal", "paper_title": "On-policy distillation of language models: Learning from self-generated mistakes", "reason": "This paper explores online knowledge distillation, providing a contrasting approach to the offline method presented in MINIPLM.  The comparison highlights MINIPLM's computational advantages and ability to efficiently distill knowledge into multiple student models concurrently.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "J. Bai", "paper_title": "Qwen technical report", "reason": "This paper introduces the Qwen-1.5 model, which serves as the teacher model for MINIPLM's knowledge distillation experiments.  The details of this model are essential for understanding and replicating the results of the MINIPLM paper, as the performance is directly measured against Qwen-1.5.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "L. Gao", "paper_title": "The pile: An 800gb dataset of diverse text for language modeling", "reason": "This paper introduces the Pile dataset, a massive dataset used for pre-training in MINIPLM's experiments. The dataset's scale and diversity are critical factors affecting the model's performance and generalization capabilities, making it an important aspect of the experimental setup.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "V. Sanh", "paper_title": "DistilBERT, a distilled version of bert: smaller, faster, cheaper and lighter", "reason": "This paper introduces DistilBERT, a distilled version of BERT, which is highly relevant to the MINIPLM approach.  DistilBERT demonstrates the effectiveness of knowledge distillation in reducing the size and computational cost of language models, providing a key comparison point for MINIPLM's efficiency gains.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Y. Gu", "paper_title": "MiniLLM: Knowledge distillation of large language models", "reason": "This paper introduces MiniLLM, another knowledge distillation method, which serves as a key comparison point in evaluating the performance of MINIPLM.  The comparison highlights the efficiency advantages and flexibility of MINIPLM.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "J. Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "reason": "This paper introduced BERT, a highly influential model that has been the subject of numerous distillation efforts.  The success of BERT and its variants underscores the value and potential of knowledge distillation techniques, providing context for MINIPLM's contribution to the field.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "A. Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces the Llama models, which are used in the cross-model knowledge distillation experiments of the MINIPLM paper. The successful adaptation of MINIPLM's methodology to the Llama models showcases the generalizability and applicability of the approach across different model families.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "N. Muennighoff", "paper_title": "Scaling data-constrained language models", "reason": "This paper explores the challenges and solutions related to training language models under data constraints. This aligns directly with the data-limited setting experiments in MINIPLM. The methodology and findings from this paper offer valuable insights and provide a relevant comparison for evaluating MINIPLM's performance in low-data regimes.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "J. Hoffmann", "paper_title": "Training compute-optimal large language models", "reason": "This paper is crucial for understanding the computational aspects of training large language models.  The scaling laws presented in this paper are used in the MINIPLM experiments to extrapolate the results to even larger scales, providing strong evidence of MINIPLM's effectiveness and efficiency across different training sizes.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "S. Mehta", "paper_title": "OpenELM: An efficient language model family with open-source training and inference framework", "reason": "This paper discusses efficient language models, which is directly related to MINIPLM's goal of efficiently training high-performing small language models.  The comparison between OpenELM and MINIPLM provides valuable insights into the relative strengths and weaknesses of different approaches towards efficient LLM training.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "O. Lieber", "paper_title": "Jamba: A hybrid transformer-mamba language model", "reason": "This paper introduces Jamba, another advanced model architecture, further highlighting the flexibility and broad applicability of MINIPLM's knowledge distillation approach across diverse architectures. The inclusion of cross-model family knowledge distillation further emphasizes the versatility and potential of MINIPLM.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "S. I. Mirzadeh", "paper_title": "Improved knowledge distillation via teacher assistant", "reason": "This paper presents an improvement to knowledge distillation techniques, offering valuable insight into the nuances of knowledge transfer between models of different sizes.  This is particularly relevant to MINIPLM, as the choice of teacher and reference model sizes significantly impact performance.", "section_number": 3}, {" publication_date": "2015", "fullname_first_author": "G. Hinton", "paper_title": "Distilling the knowledge in a neural network", "reason": "This paper is a foundational work on knowledge distillation, introducing the core concepts and methods that underlie many subsequent KD techniques, including MINIPLM. Understanding this foundational work is critical to understanding the context and significance of MINIPLM's contributions.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "J. Hoffmann", "paper_title": "Training compute-optimal large language models", "reason": "This paper is highly relevant to the MINIPLM work because it provides the theoretical framework for understanding the scaling laws that govern the computational costs of training large language models.  The insights from this work are directly applied in the experimental section of the MINIPLM paper, particularly for extrapolating results to larger scales.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "J. Kaplan", "paper_title": "Scaling laws for neural language models", "reason": "This foundational work establishes scaling laws that guide the resource allocation for training large language models.  MINIPLM explicitly addresses the challenge of efficient training within these scaling laws, highlighting the computational efficiency gains of their approach.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "H. Touvron", "paper_title": "LLaMA: Open and efficient foundation language models", "reason": "This paper introduces LLaMA, a family of open-source language models, which are used in the experimental section of the MINIPLM paper to demonstrate the generalizability of the approach across different model architectures.  The comparison with LLaMA underscores MINIPLM's flexibility and broader applicability.", "section_number": 4}]}