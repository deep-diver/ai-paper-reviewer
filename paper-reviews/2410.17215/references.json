{"references": [{" publication_date": "2020", "fullname_first_author": "J. Kaplan", "paper_title": "Scaling laws for neural language models", "reason": "This paper introduces scaling laws that provide insights into the relationship between model size, dataset size, and performance, which is crucial for understanding the challenges and opportunities in training smaller, more efficient language models.  The scaling laws are foundational to the efficient training of language models, as addressed in the current study.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "reason": "This highly influential paper demonstrates the remarkable few-shot learning capabilities of large language models. It sets the stage for exploring methods to transfer these capabilities to smaller models, which is the core goal of knowledge distillation as applied in this work. The paper's findings have significantly shaped the field of language model research and the approach to pre-training language models.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "R. Bommasani", "paper_title": "On the opportunities and risks of foundation models", "reason": "This paper provides a comprehensive overview of foundation models, highlighting their potential and risks.  This sets the context for the current work which aims to improve the efficiency and effectiveness of training smaller, more deployable language models, addressing some of the challenges highlighted in this foundational study.", "section_number": 1}, {" publication_date": "2016", "fullname_first_author": "Y. Kim", "paper_title": "Sequence-level knowledge distillation", "reason": "This paper introduces sequence-level knowledge distillation, which provides a foundation for the offline knowledge distillation methods explored in the current work.  It is among the early works on applying knowledge distillation for language models, establishing the groundwork for efficient training of smaller models.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "J. Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "reason": "This paper presents BERT, a highly influential model that significantly advanced the field of natural language processing. It's important because it establishes the foundation of large language models, against which the efficiency and effectiveness of distillation methods are often compared.  The work builds on and compares against the achievements made possible by BERT.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "J. Hoffmann", "paper_title": "Training compute-optimal large language models", "reason": "This highly relevant work focuses on training compute-optimal large language models, directly addressing the efficiency challenges discussed in this study.  The scaling laws and optimization strategies introduced here inform the optimization and efficiency considerations in the present research.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "R. Agarwal", "paper_title": "On-policy distillation of language models: Learning from self-generated mistakes", "reason": "This paper explores on-policy distillation, a method related to the offline knowledge distillation approach used in this work.  It provides insights into alternative KD strategies and their performance, which helps to contextualize and justify the choices made in the present research.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "V. Sanh", "paper_title": "DistilBERT, a distilled version of bert: smaller, faster, cheaper and lighter", "reason": "This paper presents DistilBERT, a smaller and more efficient version of BERT, demonstrating the feasibility and effectiveness of knowledge distillation for creating compact models. This work directly relates to the current study's goal of efficient pre-training for small language models.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "A. Chowdhery", "paper_title": "Palm: Scaling language modeling with pathways", "reason": "This paper presents Pathways Language Model (PaLM), showing the potential of scaling language models efficiently. This is relevant to the current study as it highlights the advancements and challenges in scaling language models and provides a context for efficient pre-training strategies, such as knowledge distillation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "J. Bai", "paper_title": "Qwen technical report", "reason": "This paper introduces the Qwen model architecture and pre-training approach, which is used in the current study as the teacher model. It is crucial because the teacher model is a key component of the knowledge distillation framework, and its characteristics heavily influence the performance of the student model.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Y. Gu", "paper_title": "MiniLLM: Knowledge distillation of large language models", "reason": "This paper introduces MiniLLM, an online knowledge distillation framework. Comparing MINIPLM to MiniLLM provides insight into different approaches to knowledge distillation and their relative efficiencies, which helps justify the choices and design of the proposed method.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "N. Muennighoff", "paper_title": "Scaling data-constrained language models", "reason": "This paper addresses the challenges of training language models with limited data, which is highly relevant to this work's goal of efficient pre-training.  The strategies and analysis presented in this paper provide a relevant context for optimizing data utilization and managing computational constraints in pre-training.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "H. Touvron", "paper_title": "LLaMA: Open and efficient foundation language models", "reason": "This paper presents LLaMA, a family of language models focusing on efficiency and openness.  Its significance is in highlighting the current trends toward more efficient language models, which provides context for the proposed work's aim to improve the efficiency and scalability of pre-training smaller language models.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "J. Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "reason": "This paper is included again due to its central role as a foundational model in the field, frequently used as a comparison point for newer techniques.  The study directly compares the performance of the proposed approach against BERT-based models, emphasizing its potential advancements.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "L. Gao", "paper_title": "The pile: An 800gb dataset of diverse text for language modeling", "reason": "This paper introduces The Pile dataset, a significant resource used for training language models.  Its importance stems from its use in the current study's experiments, providing a common benchmark and dataset for comparison across different methods and model sizes.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "D. Cheng", "paper_title": "Instruction pre-training: Language models are supervised multitask learners", "reason": "This paper explores instruction pre-training, a technique that has significantly improved the performance of language models. This is relevant to the current study as it highlights different approaches to pre-training and their effects on model performance.  Comparing against this advanced method adds weight to the performance gains obtained by the proposed method.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "S. Mehta", "paper_title": "OpenELM: An efficient language model family with open-source training and inference framework", "reason": "This paper introduces OpenELM, another family of efficient language models focusing on open-source training and inference.  This relates to the current study as it provides a context for the importance of efficiency in language model development and the general effort toward developing more accessible and efficient models.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "N. Stiennon", "paper_title": "Learning to summarize with human feedback", "reason": "This paper explores using human feedback to improve language model performance. This is relevant to the current study's goal of enhancing language model capabilities and provides context for how human feedback could further enhance the performance of models trained with the proposed knowledge distillation method.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "H. Touvron", "paper_title": "LLaMA: Open and efficient foundation language models", "reason": "This is included again because of its significance in showcasing the trend towards efficient language models.  The study compares the proposed method against LLaMA, highlighting the benefits in terms of both performance and efficiency.", "section_number": 4}, {" publication_date": "2015", "fullname_first_author": "G. Hinton", "paper_title": "Distilling the knowledge in a neural network", "reason": "This foundational paper introduced the concept of knowledge distillation, providing the theoretical underpinning for the techniques used in the current study.  It is a seminal work in the field and crucial for understanding the context and motivation behind the current research.", "section_number": 4}]}