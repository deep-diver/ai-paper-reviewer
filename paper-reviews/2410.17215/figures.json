[{"figure_path": "2410.17215/figures/figures_3_0.png", "caption": "Figure 3: MINIPLM. (a): Training framework. MINIPLM distills the knowledge of the teacher LM into the student LM by adjusting the pre-training corpus of the student LM (qe) through offline Difference Sampling, based on the output probability discrepancy between the teacher LM (p) and a small reference LM (pref). (b): Illustration of the effect of Difference Sampling, which down-samples common easy instances, up-samples hard valuable instances, and removes noisy harmful instances.", "description": "The figure illustrates the MINIPLM training framework, including offline difference sampling and its effect on refining the pre-training corpus.", "section": "2 MINIPLM: KD for Pre-training LMs"}]