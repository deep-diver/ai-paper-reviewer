[{"figure_path": "2410.17215/tables/table_6_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents the zero-shot accuracy scores achieved by different language models (with varying sizes) on nine downstream tasks, comparing models pre-trained without knowledge distillation and those trained with various knowledge distillation methods.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_7_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents the zero-shot accuracy scores of student language models (with varying sizes) pre-trained using different knowledge distillation methods on nine downstream tasks, comparing their performance with a teacher LM of 1.8B parameters.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_8_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "This table presents the zero-shot accuracy scores achieved by different language models (200M, 500M, and 1.2B parameters) on nine downstream tasks, comparing models pre-trained without knowledge distillation, with Vanilla KD, MiniLLM, SeqKD, and MINIPLM.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_9_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents the zero-shot accuracy scores achieved by different language models (pre-trained with different methods) on nine downstream tasks, comparing models of varying sizes and training methods.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_9_1.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "This table presents the zero-shot accuracy scores achieved by various language models (200M, 500M, and 1.2B parameters) on nine downstream tasks, comparing models trained with different methods (Pre-Train w/o KD, Vanilla KD, MiniLLM, SeqKD, and MINIPLM).", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_18_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents the zero-shot accuracy scores achieved by student language models of varying sizes (200M, 500M, and 1.2B parameters) on nine downstream tasks, comparing different knowledge distillation methods against a baseline of no knowledge distillation.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_19_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents the zero-shot accuracy scores achieved by various language models (pre-trained with different methods) on nine downstream tasks, comparing models of different sizes and pre-training approaches.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_20_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "The table presents the zero-shot accuracy scores achieved by different language models (with varying sizes) on nine downstream tasks, comparing models pre-trained with and without knowledge distillation, using different methods.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_20_1.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents the zero-shot accuracy scores of student language models of varying sizes (200M, 500M, and 1.2B parameters) pre-trained using different methods on nine downstream NLP tasks, comparing their performance against a teacher LM (1.8B parameters).", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_23_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents the zero-shot accuracy scores achieved by student language models of varying sizes (200M, 500M, and 1.2B parameters) pre-trained using different methods (including MINIPLM) on nine downstream tasks, showing performance improvements with MINIPLM.", "section": "3.2 Main Results"}]