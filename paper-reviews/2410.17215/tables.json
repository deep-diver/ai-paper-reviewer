[{"figure_path": "2410.17215/tables/table_6_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents the zero-shot accuracy scores achieved by student language models of various sizes (200M, 500M, and 1.2B parameters) pre-trained using different methods (Pre-Train w/o KD, Vanilla KD, MiniLLM, SeqKD, and MINIPLM),  evaluated on nine downstream NLP tasks, showing the impact of the proposed MINIPLM method on model performance.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_7_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents the zero-shot accuracy scores achieved by student language models of varying sizes (200M, 500M, and 1.2B parameters) on nine downstream tasks, comparing performance across different knowledge distillation methods.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_8_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents zero-shot accuracy scores on nine downstream tasks for language models (LMs) of varying sizes (200M, 500M, and 1.2B parameters) trained using different knowledge distillation (KD) methods, including MINIPLM and several baselines, with a 1.8B parameter LM as the teacher.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_9_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents the zero-shot accuracy scores achieved by different language models (with varying sizes) on nine downstream tasks, comparing the performance of models trained with different knowledge distillation methods against a baseline model trained without knowledge distillation.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_9_1.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents the zero-shot accuracy scores of student language models (200M, 500M, and 1.2B parameters) pre-trained with different methods on nine downstream tasks, using a 1.8B parameter model as the teacher.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_18_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents the zero-shot accuracy scores achieved by student language models of varying sizes (200M, 500M, 1.2B parameters) on nine downstream tasks, comparing different knowledge distillation methods and a baseline with no knowledge distillation.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_19_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents zero-shot accuracy scores on nine downstream tasks for language models of various sizes trained using different methods.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_20_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents the zero-shot accuracy scores of student language models of varying sizes (200M, 500M, and 1.2B parameters) pre-trained using different methods (Pre-Train w/o KD, Vanilla KD, MiniLLM, SeqKD, and MINIPLM) across nine downstream NLP tasks.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_20_1.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents the zero-shot accuracy scores of student language models with various sizes pre-trained using different knowledge distillation methods on nine downstream tasks, comparing their performance with a baseline of pre-training without knowledge distillation.", "section": "3.2 Main Results"}, {"figure_path": "2410.17215/tables/table_23_0.html", "caption": "Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced.", "description": "Table 1 presents the zero-shot accuracy scores achieved by student language models of varying sizes (200M, 500M, and 1.2B parameters) on nine downstream tasks, comparing different knowledge distillation (KD) methods against a baseline model trained without KD.", "section": "3.2 Main Results"}]