[{"figure_path": "2410.17215/charts/charts_1_0.png", "caption": "Figure 1: Computation (a) and model size (b) scaling curves of student LMs pre-trained from scratch with Vanilla KD\u00b9 and MINIPLM. The teacher LM has 1.8B parameters. \u201c1.8B\u2192500M\u201d means we use a 500M student LM. Training-time computation is kept constant for LMs of the same size in model scaling. The y-axis represents the LMs' zero-shot performance on 9 downstream NLP tasks.", "description": "The chart displays the computation and model size scaling curves for student language models pre-trained using knowledge distillation with Vanilla KD and the proposed MINIPLM method, demonstrating MINIPLM's improved efficiency and performance.", "section": "Experiments"}, {"figure_path": "2410.17215/charts/charts_1_1.png", "caption": "Figure 1: Computation (a) and model size (b) scaling curves of student LMs pre-trained from scratch with Vanilla KD\u00b9 and MINIPLM. The teacher LM has 1.8B parameters. \u201c1.8B\u2192500M\u201d means we use a 500M student LM. Training-time computation is kept constant for LMs of the same size in model scaling. The y-axis represents the LMs' zero-shot performance on 9 downstream NLP tasks.", "description": "The chart shows the scaling curves of student language models pre-trained using knowledge distillation, comparing MINIPLM with Vanilla KD across different computational budgets and model sizes.", "section": "Experiments"}, {"figure_path": "2410.17215/charts/charts_2_0.png", "caption": "Figure 2: Results of applying KD methods in fine-tuning to pre-train a 200M student LM, using a 1.8B teacher LM. See Section 3.1 for method and evaluation details. When the training FLOPs are controlled, all KD methods perform similar or worse than Pre-Train w/o KD.", "description": "The chart compares the performance of various knowledge distillation methods (Vanilla KD, MiniLLM, SeqKD) against a baseline (Pre-Train w/o KD) for pre-training a 200M student language model, showing that when training FLOPs are controlled, all KD methods perform similarly to or worse than the baseline.", "section": "1 Introduction"}, {"figure_path": "2410.17215/charts/charts_7_0.png", "caption": "Figure 1: Computation (a) and model size (b) scaling curves of student LMs pre-trained from scratch with Vanilla KD\u00b9 and MINIPLM. The teacher LM has 1.8B parameters. \u201c1.8B\u2192500M\u201d means we use a 500M student LM. Training-time computation is kept constant for LMs of the same size in model scaling. The y-axis represents the LMs' zero-shot performance on 9 downstream NLP tasks.", "description": "The chart displays the scaling curves of student language models pre-trained using knowledge distillation, comparing MINIPLM's performance against a vanilla knowledge distillation method across varying computational budgets and model sizes.", "section": "Experiments"}, {"figure_path": "2410.17215/charts/charts_8_0.png", "caption": "Figure 1: Computation (a) and model size (b) scaling curves of student LMs pre-trained from scratch with Vanilla KD\u00b9 and MINIPLM. The teacher LM has 1.8B parameters. \u201c1.8B\u2192500M\u201d means we use a 500M student LM. Training-time computation is kept constant for LMs of the same size in model scaling. The y-axis represents the LMs' zero-shot performance on 9 downstream NLP tasks.", "description": "The chart displays the computation and model size scaling curves for student language models pre-trained using knowledge distillation with Vanilla KD and the proposed MINIPLM method, showcasing MINIPLM's efficiency gains and performance improvements.", "section": "Experiments"}, {"figure_path": "2410.17215/charts/charts_9_0.png", "caption": "Figure 6: Impact of the teacher LM's sizes on Vanilla KD and MINIPLM, with the pre-training FLOPs aligned. The y-axis represents the average zero-shot accuracy on the downstream tasks.", "description": "The chart displays the average zero-shot accuracy on downstream tasks for Vanilla KD and MINIPLM models trained with varying teacher LM sizes, while keeping pre-training FLOPs constant.", "section": "3.5 Analysis"}, {"figure_path": "2410.17215/charts/charts_19_0.png", "caption": "Figure 1: Computation (a) and model size (b) scaling curves of student LMs pre-trained from scratch with Vanilla KD\u00b9 and MINIPLM. The teacher LM has 1.8B parameters. \u201c1.8B\u2192500M\u201d means we use a 500M student LM. Training-time computation is kept constant for LMs of the same size in model scaling. The y-axis represents the LMs' zero-shot performance on 9 downstream NLP tasks.", "description": "The chart displays the computation and model size scaling curves for student language models pre-trained using knowledge distillation, comparing the performance of Vanilla KD and the proposed MINIPLM method.", "section": "Experiments"}, {"figure_path": "2410.17215/charts/charts_21_0.png", "caption": "Figure 8: Impact of the reference model size. We use the 1.8B LM as the teacher and the 200M LM as the student. We report the average zero-shot accuracy on the downstream tasks of the LMs trained with MINIPLM and compare it with Vanilla KD.", "description": "The chart displays the average zero-shot accuracy on downstream tasks for models trained with MINIPLM and Vanilla KD, showing the impact of different reference model sizes.", "section": "3.4 Data-Limited Setting"}, {"figure_path": "2410.17215/charts/charts_21_1.png", "caption": "Figure 9: Impact of the difference sampling ratio \u03b1. We report the average zero-shot accuracy on the downstream tasks of the LMs trained with MINIPLM, using \u03b1 \u2208 [0.3, 0.4, 0.5, 0.6, 0.7, 0.9] and compare it with Vanilla KD.", "description": "The chart displays the impact of the difference sampling ratio on the average zero-shot accuracy of language models trained with MINIPLM and Vanilla KD on downstream tasks.", "section": "3.2 Main Results"}]