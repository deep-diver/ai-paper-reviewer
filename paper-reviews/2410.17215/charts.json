[{"figure_path": "2410.17215/charts/charts_1_0.png", "caption": "Figure 1: Computation (a) and model size (b) scaling curves of student LMs pre-trained from scratch with Vanilla KD\u00b9 and MINIPLM. The teacher LM has 1.8B parameters. \u201c1.8B\u2192500M\u201d means we use a 500M student LM. Training-time computation is kept constant for LMs of the same size in model scaling. The y-axis represents the LMs' zero-shot performance on 9 downstream NLP tasks.", "description": "The chart displays the computation and model size scaling curves for student language models pre-trained using Vanilla KD and the proposed MINIPLM method, demonstrating MINIPLM's efficiency and effectiveness.", "section": "Experiments"}, {"figure_path": "2410.17215/charts/charts_1_1.png", "caption": "Figure 1: Computation (a) and model size (b) scaling curves of student LMs pre-trained from scratch with Vanilla KD\u00b9 and MINIPLM. The teacher LM has 1.8B parameters. \u201c1.8B\u2192500M\u201d means we use a 500M student LM. Training-time computation is kept constant for LMs of the same size in model scaling. The y-axis represents the LMs' zero-shot performance on 9 downstream NLP tasks.", "description": "The chart displays the scaling curves of student language models pre-trained using two different knowledge distillation methods, Vanilla KD and MINIPLM, showing computation and model size scaling.", "section": "Experiments"}, {"figure_path": "2410.17215/charts/charts_2_0.png", "caption": "Figure 2: Results of applying KD methods in fine-tuning to pre-train a 200M student LM, using a 1.8B teacher LM. See Section 3.1 for method and evaluation details. When the training FLOPs are controlled, all KD methods perform similar or worse than Pre-Train w/o KD.", "description": "The chart compares the performance of various knowledge distillation methods for pre-training a 200M student language model when controlling either the number of training steps or the total training FLOPs.", "section": "1 Introduction"}, {"figure_path": "2410.17215/charts/charts_7_0.png", "caption": "Figure 1: Computation (a) and model size (b) scaling curves of student LMs pre-trained from scratch with Vanilla KD\u00b9 and MINIPLM. The teacher LM has 1.8B parameters. \u201c1.8B\u2192500M\u201d means we use a 500M student LM. Training-time computation is kept constant for LMs of the same size in model scaling. The y-axis represents the LMs' zero-shot performance on 9 downstream NLP tasks.", "description": "The chart displays the computation and model size scaling curves for student language models pre-trained using Vanilla KD and the proposed MINIPLM method, showing MINIPLM's superior performance and efficiency.", "section": "Experiments"}, {"figure_path": "2410.17215/charts/charts_8_0.png", "caption": "Figure 1: Computation (a) and model size (b) scaling curves of student LMs pre-trained from scratch with Vanilla KD\u00b9 and MINIPLM. The teacher LM has 1.8B parameters. \u201c1.8B\u2192500M\u201d means we use a 500M student LM. Training-time computation is kept constant for LMs of the same size in model scaling. The y-axis represents the LMs' zero-shot performance on 9 downstream NLP tasks.", "description": "The chart displays the scaling curves of student language models pre-trained using knowledge distillation, comparing MINIPLM with Vanilla KD across computation and model size, showing improvements in performance with MINIPLM.", "section": "Experiments"}, {"figure_path": "2410.17215/charts/charts_9_0.png", "caption": "Figure 6: Impact of the teacher LM's sizes on Vanilla KD and MINIPLM, with the pre-training FLOPs aligned. The y-axis represents the average zero-shot accuracy on the downstream tasks.", "description": "The chart displays the impact of different teacher LM sizes on the average zero-shot accuracy of downstream tasks for Vanilla KD and MINIPLM, while maintaining consistent pre-training FLOPs.", "section": "3.5 Analysis"}, {"figure_path": "2410.17215/charts/charts_19_0.png", "caption": "Figure 1: Computation (a) and model size (b) scaling curves of student LMs pre-trained from scratch with Vanilla KD\u00b9 and MINIPLM. The teacher LM has 1.8B parameters. \u201c1.8B\u2192500M\u201d means we use a 500M student LM. Training-time computation is kept constant for LMs of the same size in model scaling. The y-axis represents the LMs' zero-shot performance on 9 downstream NLP tasks.", "description": "The chart displays the computation and model size scaling curves for student language models pre-trained using knowledge distillation with Vanilla KD and MINIPLM, showing the impact on downstream task performance.", "section": "Experiments"}, {"figure_path": "2410.17215/charts/charts_21_0.png", "caption": "Figure 8: Impact of the reference model size. We use the 1.8B LM as the teacher and the 200M LM as the student. We report the average zero-shot accuracy on the downstream tasks of the LMs trained with MINIPLM and compare it with Vanilla KD.", "description": "The chart displays the impact of different sizes of reference models on the average zero-shot accuracy of language models trained with MINIPLM and Vanilla KD on downstream tasks.", "section": "3. Experiments"}, {"figure_path": "2410.17215/charts/charts_21_1.png", "caption": "Figure 9: Impact of the difference sampling ratio \u03b1. We report the average zero-shot accuracy on the downstream tasks of the LMs trained with MINIPLM, using \u03b1 \u2208 [0.3, 0.4, 0.5, 0.6, 0.7, 0.9] and compare it with Vanilla KD.", "description": "The chart displays the impact of the difference sampling ratio on the average zero-shot accuracy of language models trained with MINIPLM compared to Vanilla KD.", "section": "3.2 Main Results"}]