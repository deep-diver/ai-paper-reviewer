<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Multimodal Latent Language Modeling with Next-Token Diffusion &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="Multimodal Latent Language Modeling with Next-Token Diffusion &#183; HF Daily Paper Reviews by AI"><meta name=description content="LatentLM: a novel multimodal model unifying discrete & continuous data via next-token diffusion, surpassing existing methods in performance & scalability across various tasks."><meta name=keywords content="Multimodal Learning,Multimodal Generation,üè¢ Microsoft Research,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08635/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08635/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="Multimodal Latent Language Modeling with Next-Token Diffusion"><meta property="og:description" content="LatentLM: a novel multimodal model unifying discrete & continuous data via next-token diffusion, surpassing existing methods in performance & scalability across various tasks."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-11T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-11T00:00:00+00:00"><meta property="article:tag" content="Multimodal Learning"><meta property="article:tag" content="Multimodal Generation"><meta property="article:tag" content="üè¢ Microsoft Research"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08635/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08635/cover.png"><meta name=twitter:title content="Multimodal Latent Language Modeling with Next-Token Diffusion"><meta name=twitter:description content="LatentLM: a novel multimodal model unifying discrete & continuous data via next-token diffusion, surpassing existing methods in performance & scalability across various tasks."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Multimodal Latent Language Modeling with Next-Token Diffusion","headline":"Multimodal Latent Language Modeling with Next-Token Diffusion","abstract":"LatentLM: a novel multimodal model unifying discrete \u0026amp; continuous data via next-token diffusion, surpassing existing methods in performance \u0026amp; scalability across various tasks.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.08635\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2024","dateCreated":"2024-12-11T00:00:00\u002b00:00","datePublished":"2024-12-11T00:00:00\u002b00:00","dateModified":"2024-12-11T00:00:00\u002b00:00","keywords":["Multimodal Learning","Multimodal Generation","üè¢ Microsoft Research"],"mainEntityOfPage":"true","wordCount":"4442"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About This Project">About</p></a><a href=/ai-paper-reviewer/2025-02-05/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-02-05s>2025-02-05</p></a><a href=/ai-paper-reviewer/2025-02-06/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-02-06s>2025-02-06</p></a><a href=/ai-paper-reviewer/2025-02-07/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-02-07s>2025-02-07</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="Paper Reviews by AI">Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About This Project">About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-05/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-02-05s>2025-02-05</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-06/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-02-06s>2025-02-06</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-07/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-02-07s>2025-02-07</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="Paper Reviews by AI">Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.08635/cover_hu_fcc74148854b961b.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.08635/>Multimodal Latent Language Modeling with Next-Token Diffusion</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Multimodal Latent Language Modeling with Next-Token Diffusion</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-11T00:00:00+00:00>11 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>4442 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">21 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.08635/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.08635/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-generation/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Generation
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-microsoft-research/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Microsoft Research</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu_97e7d424fadd1c26.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#multimodal-fusion>Multimodal Fusion</a></li><li><a href=#diffusion-modeling>Diffusion Modeling</a></li><li><a href=#œÉ-vae-encoding>œÉ-VAE Encoding</a></li><li><a href=#scalability-analysis>Scalability Analysis</a></li><li><a href=#future-research>Future Research</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#multimodal-fusion>Multimodal Fusion</a></li><li><a href=#diffusion-modeling>Diffusion Modeling</a></li><li><a href=#œÉ-vae-encoding>œÉ-VAE Encoding</a></li><li><a href=#scalability-analysis>Scalability Analysis</a></li><li><a href=#future-research>Future Research</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.08635</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Yutao Sun et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2024-12-13</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.08635 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.08635 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/multimodal-latent-language-modeling-with-next target=_self role=button>‚Üó Papers with Code</a></p><audio controls><source src=https://ai-paper-reviewer.com/2412.08635/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current multimodal generative models struggle with integrating discrete (text, code) and continuous (image, audio) data, often relying on separate modules leading to information loss and suboptimal performance. Previous attempts to unify these data types either suffered from lossy quantization or compromised the performance of discrete data.</p><p>LatentLM addresses these shortcomings by using a variational autoencoder (VAE) to represent continuous data as latent vectors, which are then autoregressively generated using a novel &rsquo;next-token diffusion&rsquo; method. The use of causal transformers further enhances performance. Experiments demonstrate LatentLM&rsquo;s effectiveness across image generation, multimodal LLMs, and text-to-speech, significantly outperforming existing state-of-the-art approaches in various metrics, while also being more scalable.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-49ac374db826bf93a7f31ea4bff1f09c></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-49ac374db826bf93a7f31ea4bff1f09c",{strings:[" LatentLM effectively integrates discrete and continuous data using causal transformers and next-token diffusion. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-8f4adcaa57c32e5ddbe7f9c73c18d3a8></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-8f4adcaa57c32e5ddbe7f9c73c18d3a8",{strings:[" LatentLM demonstrates superior performance and scalability compared to existing methods in image generation, text-to-speech synthesis, and multimodal LLMs. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-44fca29029a0ee02b89a26182f0e37f0></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-44fca29029a0ee02b89a26182f0e37f0",{strings:[" œÉ-VAE, a novel VAE variant, is proposed to address the variance collapse issue crucial for autoregressive modeling in LatentLM. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial for researchers in multimodal learning because it introduces a <strong>scalable and efficient approach</strong> for handling both discrete and continuous data within a unified framework. The <strong>unified interface</strong> offered by LatentLM allows for seamless integration of various modalities, paving the way for more sophisticated and versatile multimodal AI systems. Its success in image generation, text-to-speech, and multimodal LLMs demonstrates its broad applicability and potential to accelerate progress in the field.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/x1.png alt></figure></p><blockquote><p>üîº The figure illustrates the architecture of Latent Language Modeling (LatentLM). LatentLM uses causal transformers to process both continuous data (like images, audio, and video) and discrete data (such as text and code). Continuous data is encoded into latent vectors using a variational autoencoder (VAE). The core innovation is the use of &rsquo;next-token diffusion.&rsquo; This process autoregressively generates the latent vectors representing the continuous data, one token at a time. This is in contrast to traditional methods which often treat continuous and discrete data separately. The combined approach provides a unified framework for multimodal generation and understanding, enabling seamless integration of various data types within a single model.</p><details><summary>read the caption</summary>Figure 1: Latent Language Modeling (LatentLM) seamlessly handles continuous (e.g., image, audio, video) and discrete (e.g., text and code) data using causal Transformers. We introduce next-token diffusion to autoregressively generate the latent vectors one by one. The proposed method provides a general-purpose interface that unifies multimodal generation and understanding.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Type</th><th>Model</th><th>#Params</th><th>#Epochs</th><th>FID‚Üì</th><th>IS‚Üë</th></tr></thead><tbody><tr><td><em>Non-Causal-Masking Generation</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Diffusion</td><td>LDM-4 [53]</td><td>400M</td><td>‚Äî</td><td>3.60</td><td>247.7</td></tr><tr><td></td><td>DiT-XL/2 [51]</td><td>675M</td><td>400</td><td>2.27</td><td>278.2</td></tr><tr><td></td><td>U-ViT-H/2 [4]</td><td>501M</td><td>400</td><td>2.29</td><td>263.9</td></tr><tr><td>Masked Generative</td><td>MaskGIT [13]</td><td>227M</td><td>300</td><td>4.02</td><td>355.6</td></tr><tr><td></td><td>MAR-L [43]</td><td>479M</td><td>800</td><td>1.78</td><td>296.0</td></tr><tr><td><em>Causal-Masking Generation</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Causal-Discrete</td><td>VQGAN [18]</td><td>1.4B</td><td>240</td><td>5.20</td><td>280.3</td></tr><tr><td></td><td>ViT-VQGAN [79]</td><td>1.7B</td><td>240</td><td>3.04</td><td>227.4</td></tr><tr><td></td><td>LlamaGen-XL [66]</td><td>775M</td><td>300</td><td>2.62</td><td>244.1</td></tr><tr><td></td><td>LlamaGen-XXL [66]</td><td>1.4B</td><td>300</td><td>2.34</td><td>253.9</td></tr><tr><td>Causal-Continuous</td><td>GIVT-Causal-L+A [70]</td><td>1.67B</td><td>500</td><td>2.59</td><td>‚Äî</td></tr><tr><td></td><td>LatentLM-L (This Work)</td><td>479M</td><td>400</td><td>2.24</td><td>253.8</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 1 presents a comparison of image generation performance on the ImageNet dataset between LatentLM and other state-of-the-art models. It evaluates both FID (Fr√©chet Inception Distance) and IS (Inception Score), commonly used metrics for assessing the quality of generated images. The results highlight LatentLM&rsquo;s competitive performance, particularly in comparison to other models using the causal masking generation approach.</p><details><summary>read the caption</summary>Table 1: Image generation results on ImageNet¬†[15]. We evaluate FID¬†[27] and IS¬†[62]. LatentLM achieves competitive performance, especially compared with other causal-masking image generation models.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Multimodal Fusion<div id=multimodal-fusion class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multimodal-fusion aria-label=Anchor>#</a></span></h4><p>Multimodal fusion, in the context of this research paper, likely refers to the method of combining information from multiple modalities such as text, images, audio, and video to create a unified representation and understanding. <strong>The core idea is to leverage the strengths of each modality to compensate for the limitations of others</strong>, improving overall performance in tasks like generation and understanding. The paper likely explores various fusion strategies, discussing their advantages and disadvantages in detail. <strong>Success hinges on effective model architecture and training techniques capable of handling the diverse nature of the input data</strong>. A key aspect would likely involve how the model learns to relate and associate information across modalities. <strong>The effectiveness of fusion depends heavily on the chosen representation for each modality</strong> and how well these representations are integrated within the model. The paper may also delve into the computational cost and scalability of different fusion methods, ultimately aiming to improve the efficiency and quality of multimodal processing. <strong>Evaluation might involve comparisons against unimodal baselines and other multimodal methods</strong>, demonstrating superior performance in tasks benefiting from the integrated information.</p><h4 class="relative group">Diffusion Modeling<div id=diffusion-modeling class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#diffusion-modeling aria-label=Anchor>#</a></span></h4><p>Diffusion models are a powerful class of generative models that have achieved state-of-the-art results in various domains, including image generation. They work by gradually adding noise to data until it becomes pure noise, and then learning to reverse this process, which enables the generation of new data samples. <strong>The key advantage of diffusion models is their ability to generate high-quality samples with a high degree of diversity.</strong> However, they often require significant computational resources and training time, making them less accessible for practical applications. <strong>Recent advancements have focused on improving the efficiency and scalability of diffusion models, such as through the use of more efficient architectures and training techniques.</strong> There is also ongoing research to better understand and control the generation process, allowing users to guide the model towards specific desired outputs, rather than relying solely on random sampling. <strong>Another area of active research involves applying diffusion models to multimodal data, enabling the generation of more complex and realistic data samples that incorporate different types of information.</strong> This opens up exciting possibilities for future applications, such as in generating realistic video, 3D models, or even interactive simulations.</p><h4 class="relative group">œÉ-VAE Encoding<div id=%CF%83-vae-encoding class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%CF%83-vae-encoding aria-label=Anchor>#</a></span></h4><p>The concept of &ldquo;œÉ-VAE Encoding&rdquo; suggests a modification to the standard Variational Autoencoder (VAE) framework. Standard VAEs learn a posterior distribution q(z|x) to represent the input data x in a latent space z, aiming to minimize the reconstruction error and the Kullback-Leibler (KL) divergence between q(z|x) and a prior distribution, typically a standard normal. However, <strong>VAEs can suffer from variance collapse</strong>, where the learned posterior collapses to a low-variance distribution, hindering effective autoregressive generation. The proposed &ldquo;œÉ-VAE&rdquo; likely addresses this by introducing a fixed variance parameter œÉ, sampled from a specified distribution (e.g. Normal), for the latent space. This fixed variance prevents the posterior from collapsing, thus <strong>ensuring sufficient variability in the latent representations</strong>. This is crucial for autoregressive models, which progressively generate the latent variables one by one, as sufficient variance supports the model&rsquo;s ability to capture diversity and avoid the generation of repetitive samples. The use of a fixed variance introduces an additional regularization term in the VAE objective function, striking a balance between reconstruction quality and maintaining the desired variance in the latent space. The choice of the distribution for œÉ and its hyperparameters are crucial for the effectiveness of this modification, dictating the trade-off between reconstruction accuracy and the level of diversity in latent representations. The success of œÉ-VAE hinges upon striking this balance effectively, resulting in robust latent representations suitable for autoregressive generation in multimodal contexts.</p><h4 class="relative group">Scalability Analysis<div id=scalability-analysis class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#scalability-analysis aria-label=Anchor>#</a></span></h4><p>A thorough scalability analysis of a multimodal large language model (MLLM) would involve investigating its performance and resource consumption as the model size, training data volume, and input sequence length increase. <strong>Crucial aspects would include evaluating inference speed, memory usage, and computational cost.</strong> The analysis should explore how different architectural choices, like the number of layers, attention heads, and hidden dimensions, affect scalability. <strong>The impact of various optimization techniques, including quantization and pruning, should be assessed to determine their effectiveness in reducing resource requirements without compromising performance.</strong> Furthermore, a detailed comparison with state-of-the-art MLLMs would benchmark the model&rsquo;s scalability, highlighting relative strengths and weaknesses. <strong>Benchmarking should cover various tasks like image generation, text-to-speech, and multimodal understanding to provide a holistic picture of the model&rsquo;s capabilities at different scales.</strong> Finally, the analysis must quantify the trade-offs between scalability, accuracy, and resource efficiency to guide efficient deployment and scaling strategies for real-world applications.</p><h4 class="relative group">Future Research<div id=future-research class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-research aria-label=Anchor>#</a></span></h4><p>Future research directions for multimodal latent language models like LatentLM should prioritize several key areas. <strong>Improving the efficiency and scalability of the diffusion process</strong> is crucial. Current methods, while effective, can be computationally expensive, especially for high-resolution data like video. Exploring more efficient diffusion architectures or sampling techniques is vital. Furthermore, <strong>expanding the range of modalities</strong> supported by the model is essential. While the paper demonstrates success with text, image, and audio, integrating other modalities such as 3D data, tactile data, and sensor readings is key to true multimodal understanding. <strong>Addressing the issue of bias and fairness in multimodal generation</strong> is another critical area. LatentLM, like other large language models, is susceptible to biases present in the training data. Developing methods to mitigate these biases and ensure fair and equitable generation across different modalities is needed. Finally, <strong>research into the theoretical underpinnings of multimodal latent representations</strong> is crucial. While the paper showcases the effectiveness of LatentLM, a deeper theoretical understanding of how these latent spaces capture the relationships between different modalities will allow for better model design and improved performance. This understanding will pave the way for more robust and reliable multimodal AI systems.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/x2.png alt></figure></p><blockquote><p>üîº LatentLM unifies the handling of both continuous and discrete data by using a causal Transformer. Continuous data is first encoded into latent vectors using a œÉ-VAE (sigma-Variational Autoencoder). Then, the model employs next-token diffusion: a process where a diffusion head autoregressively generates these latent vectors, one at a time, conditioned on the Transformer&rsquo;s previous output states. Finally, a decoder reconstructs the original continuous data from the predicted latent vectors. Discrete data is processed directly by the Transformer using standard next-token prediction techniques.</p><details><summary>read the caption</summary>Figure 2: LatentLM unifies the modeling of continuous and discrete data. We introduce œÉùúé\sigmaitalic_œÉ-VAE (Section¬†2.3) to represent continuous data as latent vectors. We perform next-token diffusion (Section¬†2.1) to autoregressively predict the latent vectors one by one. The diffusion head generates vectors by conditioning on the output states of Transformer. The predicted vectors can be decoded to produce the final outputs.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/x3.png alt></figure></p><blockquote><p>üîº Figure 3 illustrates the architecture of the proposed œÉ-VAE, comparing it to the standard VAE. In contrast to the VAE, which learns the variance of the latent space, œÉ-VAE uses a fixed variance. This fixed variance, denoted as œÉ, is sampled from a normal distribution with a mean of 0 and a variance controlled by the hyperparameter CœÉ. This modification is crucial for preventing variance collapse, a common issue in VAEs, and helps maintain the stability of the latent space, especially important during the autoregressive generation process within LatentLM. By ensuring a consistent variance, the model becomes less susceptible to exposure bias which negatively impacts the quality of autoregressive generation.</p><details><summary>read the caption</summary>Figure 3: Compared to variational autoencoder (VAE), œÉùúé\sigmaitalic_œÉ-VAE uses a fixed variance for the latent space. It avoids variance collapse and makes LatentLM more robust to exposure bias during autoregressive generation. In our method, œÉùúé\sigmaitalic_œÉ is a scalar that is sampled from ùí©‚Å¢(0,CœÉ)ùí©0subscriptùê∂ùúé\mathcal{N}(0,C_{\sigma})caligraphic_N ( 0 , italic_C start_POSTSUBSCRIPT italic_œÉ end_POSTSUBSCRIPT ) for each example.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/scaling_curve.png alt></figure></p><blockquote><p>üîº Figure 4 illustrates the scalability of both Diffusion Transformer (DiT) and LatentLM models by comparing their Fr√©chet Inception Distance (FID) scores across different model sizes. The FID score, a metric evaluating the quality of generated images, consistently decreases (improving) as the model size increases for both architectures. However, LatentLM demonstrates a more substantial reduction in FID with increasing model size compared to DiT, suggesting improved scaling efficiency and overall image generation quality.</p><details><summary>read the caption</summary>Figure 4: Scaling curves of DiT and LatentLM. FID¬†[27] consistently becomes better with larger model size.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/fid_combined.png alt></figure></p><blockquote><p>üîº This figure displays sample images generated by the LatentLM model after training on the ImageNet dataset. The images showcase the model&rsquo;s ability to generate high-resolution (384x384 pixels) images. The specific model architecture and training details are described in Section 3.1.2 of the paper. The variety of images demonstrates the model&rsquo;s capacity to generate diverse and visually appealing results across different ImageNet categories.</p><details><summary>read the caption</summary>Figure 5: Samples of LatentLM trained on ImageNet. The resolution is 384√ó\times√ó384. The image is generated by models described in Section¬†3.1.2.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_bsz128.png alt></figure></p><blockquote><p>üîº Figure 6 presents a comparative analysis of image generation performance between Diffusion Transformer (DiT) and LatentLM on the ImageNet dataset. The key focus is on how different tokenizer variances and classifier-free guidance (CFG) scales affect the FID scores (a lower FID score indicates better image quality). The results reveal that LatentLM significantly benefits from tokenizers with larger variance, unlike DiT. The figure also highlights the ineffectiveness of tokenizers optimized for previous image diffusion models when used with LatentLM, emphasizing the unique characteristics of LatentLM&rsquo;s architecture.</p><details><summary>read the caption</summary>Figure 6: Image generation results of Diffusion Transformer (DiT)¬†[51] and LatentLM on ImageNet. We report FID¬†[27] scores (lower is better) in the settings of different tokenizer variance and CFG¬†[28] scale. The ‚Äústars‚Äù represent the tokenizers tuned for previous image-level diffusion models¬†[53], which are ineffective for LatentLM. The results indicate that LatentLM favors tokenizers with larger variances.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_XL.png alt></figure></p><blockquote><p>üîº This figure compares the inference throughput of Diffusion Transformer (DiT) and LatentLM models with varying model sizes. It shows that LatentLM&rsquo;s throughput scales more favorably with increasing model size than DiT, demonstrating significant improvements particularly for larger models, which is attributed to LatentLM&rsquo;s more efficient use of computational resources.</p><details><summary>read the caption</summary>(a) Throughput with increasing model sizes.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/mllm_fid.png alt></figure></p><blockquote><p>üîº This figure shows the inference throughput of the Diffusion Transformer (DiT) and LatentLM models with varying batch sizes. The throughput is measured on a single H100 GPU, using 20 diffusion inference steps. The results demonstrate how the throughput of LatentLM scales favorably with increasing batch size, showcasing the model&rsquo;s efficient use of resources compared to DiT. The graph also includes results for LatentLM with group-query attention (GQA) demonstrating further improvements in throughput.</p><details><summary>read the caption</summary>(b) Throughput with increasing batch sizes.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/mllm_ppl.png alt></figure></p><blockquote><p>üîº This figure compares the inference throughput (speed of generating outputs) of two different models, Diffusion Transformer (DiT) and LatentLM, under varying conditions. It shows how the throughput changes with different model sizes (smaller to larger) and different batch sizes (number of inputs processed simultaneously). The results demonstrate LatentLM&rsquo;s efficiency and scalability, particularly when using larger models and batch sizes. The inclusion of &lsquo;GQA&rsquo; (group-query attention) further enhances LatentLM&rsquo;s performance.</p><details><summary>read the caption</summary>Figure 7: We compare the inference throughput of Diffusion Transformer (DiT)¬†[51] and LatentLM in the settings of different model size and batch size. ‚ÄúGQA‚Äù stands for group-query attention¬†[2].</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/text-to-image/mountain.png alt></figure></p><blockquote><p>üîº Figure 8(a) presents a graph showing the FID (Fr√©chet Inception Distance) scores for text-to-image generation as the number of training tokens increases. The lower FID score indicates better image quality. The graph compares the performance of LatentLM against VQ-MLLM (a model using vector quantization for images) and Transfusion (a model combining language modeling and diffusion). The x-axis represents the number of training tokens (in billions), and the y-axis represents the FID score. This graph demonstrates LatentLM&rsquo;s scalability and superior performance compared to other approaches, showcasing improved image generation with larger datasets.</p><details><summary>read the caption</summary>(a) Text-to-image FID¬†[27].</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/text-to-image/city.png alt></figure></p><blockquote><p>üîº This figure shows how the perplexity of image-to-text generation changes as the number of training tokens increases. Perplexity is a measure of how well a language model predicts a sequence; lower perplexity indicates better performance. The graph allows for comparison across different models, revealing trends and relative performance differences in handling image-to-text tasks as training data scales.</p><details><summary>read the caption</summary>(b) Image-to-text validation perplexity.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/text-to-image/lake.png alt></figure></p><blockquote><p>üîº Figure 8 presents a scalability analysis of multimodal large language models, focusing on the impact of increasing the number of training tokens. The experiment compares LatentLM&rsquo;s performance against two other methods: vector quantized models (VQ-MLLM) and the Transfusion model. The evaluation metrics used are FID (Fr√©chet Inception Distance) scores for text-to-image generation, and perplexity for image-to-text generation, with the MS-COCO dataset serving as the benchmark. The results visually demonstrate that as the quantity of training tokens increases, LatentLM consistently achieves lower FID scores and perplexity than both VQ-MLLM and Transfusion, highlighting its superior scalability and performance in multimodal generation and understanding tasks.</p><details><summary>read the caption</summary>Figure 8: We scale up the number of training tokens for multimodal large language models. LatentLM outperforms vector quantized models (VQ-MLLM) and Transfusion¬†[82] for both text-to-image and image-to-text generation. The FID scores are evaluated on MS-COCO¬†[40].</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/text-to-image/house.png alt></figure></p><blockquote><p>üîº A photograph depicting a majestic mountain range completely covered in a blanket of pristine, white snow. The image likely showcases a vast, expansive landscape, possibly in a mountainous region known for its snow-covered peaks. The snow appears undisturbed and fresh, highlighting the beauty and serenity of the natural scene. The image is sharp and detailed, likely taken with professional equipment in good weather conditions.</p><details><summary>read the caption</summary>(c) A majestic mountain range covered in snow.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/tts_cfg_scale.png alt></figure></p><blockquote><p>üîº The image shows a city street at night, brightly lit by various light sources. The scene is likely urban, given the presence of buildings and streetlights. The illumination suggests an active, bustling atmosphere, even though no people are visible in the specific image shown.</p><details><summary>read the caption</summary>(d) A city street illuminated by lights.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/tts_sampling_steps.png alt></figure></p><blockquote><p>üîº An image depicting a serene crystal-clear lake nestled amidst a vibrant autumnal forest. The trees surrounding the lake showcase a rich palette of fall colors, reflecting the tranquility of the scene. The overall mood is peaceful and picturesque, showcasing the beauty of nature during the autumn season.</p><details><summary>read the caption</summary>(e) A crystal lake surrounded by autumn trees.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_Large.png alt></figure></p><blockquote><p>üîº The image shows a small house, seemingly made of wood, situated in a setting bathed in the warm, golden light of sunset. The scene is peaceful and evokes a sense of tranquility.</p><details><summary>read the caption</summary>(f) A small house in a wooden at sunset.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_3B.png alt></figure></p><blockquote><p>üîº Figure 9 presents four examples of images generated by the LatentLM model using text prompts. Each image showcases the model&rsquo;s ability to generate realistic and detailed visuals based on diverse and concise textual descriptions. The captions illustrate the range of concepts and styles that LatentLM can effectively translate into visual representations, highlighting its versatility in the text-to-image task.</p><details><summary>read the caption</summary>Figure 9: Text-to-image examples of LatentLM.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_7B.png alt></figure></p><blockquote><p>üîº This figure shows ablation study results on the effect of classifier-free guidance (CFG) scale on the performance of zero-shot speech synthesis. The x-axis represents different CFG scales, and the y-axis shows the metrics of SIM (speaker similarity), WER-C (word error rate using Conformer-Transducer), and WER-H (word error rate using HuBERT-Large). The plot illustrates how varying the CFG scale impacts the quality and accuracy of the generated speech.</p><details><summary>read the caption</summary>(a) Results using different CFG scales.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_13B.png alt></figure></p><blockquote><p>üîº This figure shows the effects of varying the number of sampling steps during the inference phase of a diffusion model used for zero-shot speech synthesis. The x-axis represents the number of sampling steps, while the y-axis displays the resulting values for the speaker similarity (SIM) metric and the word error rate (WER-C). The results demonstrate how the model&rsquo;s performance changes as the number of inference steps increases. The plot helps to determine an optimal number of sampling steps that balances performance and computational efficiency.</p><details><summary>read the caption</summary>(b) Results using different sampling steps.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Resolution</th><th>FID-50k ‚Üì</th></tr></thead><tbody><tr><td>256 √ó 256</td><td>3.19</td></tr><tr><td>384 √ó 384</td><td><strong>2.51</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the Fr√©chet Inception Distance (FID) scores, a metric used to evaluate the quality of generated images, for different image resolutions. Specifically, it shows how the FID score changes when scaling up the resolution of generated images from 256x256 pixels to 384x384 pixels. A lower FID score indicates better image quality.</p><details><summary>read the caption</summary>Table 2: FID¬†[27] of scaling up image resolution.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Text Valid PPL ‚Üì</th><th>Text-to-Image FID ‚Üì</th><th>Text-to-Image CLIP ‚Üë</th><th>Image-to-Text MS-COCO ‚Üë</th><th>Image-to-Text VQAv2 ‚Üë</th></tr></thead><tbody><tr><td>VQ-MLLM</td><td>2.79</td><td>16.92</td><td><strong>29.33</strong></td><td>37.4</td><td>30.19</td></tr><tr><td>Transfusion</td><td>2.74</td><td>16.10</td><td>28.66</td><td>43.4</td><td>35.36</td></tr><tr><td>LatentLM</td><td><strong>2.73</strong></td><td><strong>14.54</strong></td><td>28.75</td><td><strong>54.5</strong></td><td><strong>38.72</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of different multimodal large language models on three key tasks: text language modeling, image-to-text generation, and text-to-image generation. The models compared include the proposed LatentLM, Transfusion (a state-of-the-art baseline), and VQ-MLLM (which uses vector quantization for image representation). The evaluation metrics used provide a comprehensive assessment across all three tasks, using perplexity for text modeling, CLIP score for similarity between text and images, CIDEr score for image caption quality (MS-COCO dataset), and accuracy for visual question answering (VQAv2 dataset). This allows for a thorough comparison of the models&rsquo; performance in handling both discrete and continuous data.</p><details><summary>read the caption</summary>Table 3: Results of multimodal large language models on text language modeling, image-to-text, and text-to-image generation. We compare with Transfusion¬†[82] and vector quantized models (VQ-MLLM; i.e., using discrete code to represent images). ‚ÄúPPL‚Äù is perplexity. CLIP¬†[54] score measures the similarity. We report CIDEr¬†[76] score for MS-COCO¬†[40] and accuracy for VQAv2¬†[21].</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>System</th><th>Frame Rate (Length/s) ‚Üì</th><th></th><th>Ref Utterance as Prompt</th><th>SIM ‚Üë</th><th>WER-C ‚Üì</th><th>WER-H ‚Üì</th><th>3s Prefix as Prompt</th><th>SIM ‚Üë</th><th>WER-C ‚Üì</th><th>WER-H ‚Üì</th></tr></thead><tbody><tr><td>Ground Truth</td><td>-</td><td></td><td></td><td>0.779</td><td>1.6</td><td>2.2</td><td></td><td>0.668</td><td>1.6</td><td>2.2</td></tr><tr><td>VALL-E 2 [10]</td><td>75</td><td></td><td></td><td>0.643</td><td>1.5</td><td>2.4</td><td></td><td>0.504</td><td>1.6</td><td>2.3</td></tr><tr><td>Voicebox [44]</td><td>100</td><td></td><td></td><td>0.662</td><td>-</td><td>1.9</td><td></td><td>0.593</td><td>-</td><td>2.0</td></tr><tr><td>MELLE [48]</td><td>62</td><td></td><td></td><td>0.625</td><td>1.5</td><td>2.1</td><td></td><td>0.508</td><td>1.5</td><td>2.0</td></tr><tr><td>LatentLM</td><td>15</td><td></td><td></td><td>0.697</td><td>1.2</td><td>1.8</td><td></td><td>0.571</td><td>1.4</td><td>2.0</td></tr><tr><td>LatentLM</td><td>7.5</td><td></td><td></td><td>0.656</td><td>1.2</td><td>1.7</td><td></td><td>0.532</td><td>1.6</td><td>2.3</td></tr><tr><td>LatentLM</td><td>3.75</td><td></td><td></td><td>0.598</td><td>1.7</td><td>2.3</td><td></td><td>0.467</td><td>3.1</td><td>4.5</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 4 presents a comparison of LatentLM&rsquo;s performance against other state-of-the-art models on zero-shot speech synthesis. The evaluation considers two scenarios: using the entire reference utterance as a prompt, and using only the first 3 seconds of the utterance as a prompt. The metrics used include Speech Similarity (SIM), Word Error Rate using the Conformer-Transducer model (WER-C), and Word Error Rate using the HuBERT-Large model (WER-H). The table highlights that LatentLM achieves better results in terms of SIM, WER-C, and WER-H compared to previous systems. A key advantage of LatentLM is its significantly reduced number of decoding steps, leading to much faster inference speed. The results are based on the LibriSpeech test-clean dataset.</p><details><summary>read the caption</summary>Table 4: LatentLM outperforms previous systems on zero-shot speech synthesis in both settings. Moreover, the number of decoding steps is much less than others, achieving faster inference speed. The results are reported on LibriSpeech test-clean set. The WER-H and SIM results of VALL-E 2 using 3s prefix as prompt are from [48].</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Tokenizer</th><th>N<sub>q</sub> ‚Üì</th><th>Frame ‚Üì</th><th>Comp. ‚Üë</th><th>Mel Dist. ‚Üì</th><th>PESQ ‚Üë</th><th>STOI ‚Üë</th><th>VISQOL ‚Üë</th><th>UTMOS ‚Üë</th></tr></thead><tbody><tr><td><em>Tokenizers with lower compression ratio</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Encodec [14]</td><td>32</td><td>75</td><td>10</td><td>0.823</td><td>3.591</td><td>0.962</td><td>4.536</td><td>3.195</td></tr><tr><td>DAC [34]</td><td>32</td><td>75</td><td>10</td><td><strong>0.355</strong></td><td><strong>4.424</strong></td><td><strong>0.987</strong></td><td><strong>4.914</strong></td><td><strong>3.469</strong></td></tr><tr><td>Encodec [14]</td><td>8</td><td>75</td><td>40</td><td>0.987</td><td>2.687</td><td>0.925</td><td>4.258</td><td>2.656</td></tr><tr><td>DAC [34]</td><td>8</td><td>75</td><td>40</td><td>0.707</td><td>3.329</td><td>0.941</td><td>4.485</td><td>3.133</td></tr><tr><td>DAC<sub>low</sub> [59]</td><td>4</td><td>75</td><td>80</td><td>0.753</td><td>3.107</td><td>0.938</td><td>4.391</td><td>3.453</td></tr><tr><td>DAC<sub>low</sub> [59]</td><td>2</td><td>75</td><td>160</td><td>0.916</td><td>2.269</td><td>0.896</td><td>3.981</td><td>3.297</td></tr><tr><td>Mimi [17]</td><td>8</td><td>12.5</td><td>240</td><td>0.987</td><td>3.217</td><td>0.946</td><td>4.332</td><td>3.375</td></tr><tr><td><em>Tokenizers with higher compression ratio</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>WavTokenizer [31]</td><td>1</td><td>75</td><td>320</td><td>0.871</td><td>2.266</td><td>0.891</td><td>4.120</td><td>3.432</td></tr><tr><td>Mimi [17]</td><td>4</td><td>12.5</td><td>480</td><td>1.458</td><td>1.568</td><td>0.826</td><td>3.390</td><td>2.652</td></tr><tr><td>WavTokenizer [31]</td><td>1</td><td>40</td><td>600</td><td>1.037</td><td>1.670</td><td>0.834</td><td>3.782</td><td>3.053</td></tr><tr><td>œÉ-VAE<sub>32</sub></td><td>1</td><td>15</td><td>1600</td><td>0.813</td><td>2.724</td><td>0.926</td><td>4.268</td><td>3.491</td></tr><tr><td>œÉ-VAE<sub>64</sub></td><td>1</td><td>7.5</td><td>3200</td><td><strong>0.798</strong></td><td><strong>2.756</strong></td><td><strong>0.929</strong></td><td><strong>4.289</strong></td><td><strong>3.505</strong></td></tr><tr><td>œÉ-VAE<sub>128</sub></td><td>1</td><td>3.75</td><td>6400</td><td>0.852</td><td>2.533</td><td>0.916</td><td>4.165</td><td>3.460</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 5 presents a comparison of various audio codec models, focusing on their compression ratios, reconstruction quality, and other relevant metrics. The models are evaluated on the LibriTTS test-other dataset. The table highlights the performance of the œÉ-VAE tokenizer, demonstrating its ability to achieve high compression ratios while maintaining good reconstruction quality. The compression ratio is calculated by dividing the audio sample rate by the number of quantizers (Nq) and the frame rate. The latent dimension of the tokenizer is also specified (e.g., œÉ-VAE32 indicates a latent dimension of 32).</p><details><summary>read the caption</summary>Table 5: The œÉùúé\sigmaitalic_œÉ-VAE tokenizers obtain competitive reconstruction quality while having high compression ratio. We report results on the LibriTTS test-other set. ‚ÄúNqsubscriptùëÅùëûN_{q}italic_N start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT‚Äù represents the number of quantizers. We define the compression ratio as the audio sample rate divided by NqsubscriptùëÅùëûN_{q}italic_N start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT and the frame rate. ‚ÄúœÉùúé\sigmaitalic_œÉ-VAE32‚Äù denotes that the latent dimension of the tokenizer is 32.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Frame</th><th>Rate</th></tr></thead><tbody></tbody></table></table></figure><blockquote><p>üîº This table presents an ablation study analyzing the impact of different compression ratios and latent dimensions within the œÉ-VAE (sigma-Variational Autoencoder) component of the LatentLM model on both the quality of the tokenizer&rsquo;s reconstruction of speech data and the performance of zero-shot speech synthesis. It shows how changes in these hyperparameters affect various metrics like Mel-cepstral distance, speaker similarity (SIM), and word error rate (WER), providing insights into the trade-offs between compression efficiency and speech generation quality.</p><details><summary>read the caption</summary>Table 6: Ablation results of different œÉùúé\sigmaitalic_œÉ-VAE compression ratios and latent dimensions. We report tokenizer reconstruction quality and zero-shot speech synthesis.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Comp.</th><th>Ratio</th></tr></thead><tbody></tbody></table></table></figure><blockquote><p>üîº This table details the model architecture configurations used in the scalability experiments of Section 3.1.2. It lists the model size (in parameters), the hidden dimension of the Transformer layers, the number of layers, the number of attention heads in each layer, and the learning rate used during training. These different configurations allowed the authors to assess how the performance of their model scales with increasing size and complexity.</p><details><summary>read the caption</summary>Table 7: Model size and hyperparameters used for the scaling experiments in¬†Section¬†3.1.2.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Mel</th><th>Dist.</th></tr></thead><tbody></tbody></table></table></figure><blockquote><p>üîº This table lists the hyperparameters used to train the multimodal large language models described in Section 3.2 of the paper. It includes the number of layers, hidden size, feedforward network (FFN) size, vocabulary size, number of attention heads, Adam optimizer beta parameters, learning rate, batch size, warmup steps, weight decay, and number of head layers.</p><details><summary>read the caption</summary>Table 8: Hyperparameters used for multimodal large language models in¬†Section¬†3.2.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Compression Ratio</th><th>Frame Rate</th><th>Latent Dimension</th><th>œÉ-VAE Reconstruction</th><th></th><th>Zero-Shot TTS</th><th></th><th></th><th></th></tr></thead><tbody><tr><td><strong>640 √ó</strong></td><td>37.5</td><td>16</td><td></td><td>Mel Dist. ‚Üì</td><td>SIM ‚Üë</td><td>WER-C ‚Üì</td><td></td><td></td></tr><tr><td><strong>1600 √ó</strong></td><td>15</td><td>16</td><td></td><td>0.929</td><td>0.866</td><td>1.9</td><td></td><td>0.655</td></tr><tr><td><strong>1600 √ó</strong></td><td>15</td><td>16</td><td></td><td>1.080</td><td>0.700</td><td>2.7</td><td></td><td>0.545</td></tr><tr><td><strong>1600 √ó</strong></td><td>15</td><td>32</td><td></td><td>0.950</td><td>0.870</td><td>1.9</td><td></td><td>0.661</td></tr></tbody></table></table></figure><blockquote><p>üîº This table details the hyperparameters used in the training of the text-to-speech synthesis model described in Section 3.3 of the paper. It includes specifications for the number of layers, hidden layer size, feed-forward network (FFN) size, number of attention heads, Adam optimizer parameters (beta1 and beta2), learning rate, learning rate scheduling method, batch size, number of warmup steps during training, weight decay, and the number of layers in the diffusion head.</p><details><summary>read the caption</summary>Table 9: Hyperparameters used for text-to-speech synthesis in Section¬†3.3.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-94f237015f1d53f06d9bab9194556dd4 class=gallery><img src=https://ai-paper-reviewer.com/2412.08635/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.08635/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08635/&amp;title=Multimodal%20Latent%20Language%20Modeling%20with%20Next-Token%20Diffusion" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08635/&amp;text=Multimodal%20Latent%20Language%20Modeling%20with%20Next-Token%20Diffusion" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08635/&amp;subject=Multimodal%20Latent%20Language%20Modeling%20with%20Next-Token%20Diffusion" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.08635/index.md",oid_likes="likes_paper-reviews/2412.08635/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.08347/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-11T00:00:00+00:00>11 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.09370/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Word Sense Linking: Disambiguating Outside the Sandbox</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-12T00:00:00+00:00>12 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title=Tags>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>