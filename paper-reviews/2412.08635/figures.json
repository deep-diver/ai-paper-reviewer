[{"figure_path": "https://arxiv.org/html/2412.08635/x1.png", "caption": "Figure 1: Latent Language Modeling (LatentLM) seamlessly handles continuous (e.g., image, audio, video) and discrete (e.g., text and code) data using causal Transformers. We introduce next-token diffusion to autoregressively generate the latent vectors one by one.\nThe proposed method provides a general-purpose interface that unifies multimodal generation and understanding.", "description": "The figure illustrates the architecture of Latent Language Modeling (LatentLM).  LatentLM uses causal transformers to process both continuous data (like images, audio, and video) and discrete data (such as text and code). Continuous data is encoded into latent vectors using a variational autoencoder (VAE).  The core innovation is the use of \"next-token diffusion.\"  This process autoregressively generates the latent vectors representing the continuous data, one token at a time. This is in contrast to traditional methods which often treat continuous and discrete data separately. The combined approach provides a unified framework for multimodal generation and understanding, enabling seamless integration of various data types within a single model.", "section": "2 Latent Language Modeling"}, {"figure_path": "https://arxiv.org/html/2412.08635/x2.png", "caption": "Figure 2: LatentLM unifies the modeling of continuous and discrete data. We introduce \u03c3\ud835\udf0e\\sigmaitalic_\u03c3-VAE (Section\u00a02.3) to represent continuous data as latent vectors. We perform next-token diffusion (Section\u00a02.1) to autoregressively predict the latent vectors one by one. The diffusion head generates vectors by conditioning on the output states of Transformer. The predicted vectors can be decoded to produce the final outputs.", "description": "LatentLM unifies the handling of both continuous and discrete data by using a causal Transformer.  Continuous data is first encoded into latent vectors using a \u03c3-VAE (sigma-Variational Autoencoder). Then, the model employs next-token diffusion: a process where a diffusion head autoregressively generates these latent vectors, one at a time, conditioned on the Transformer's previous output states. Finally, a decoder reconstructs the original continuous data from the predicted latent vectors.  Discrete data is processed directly by the Transformer using standard next-token prediction techniques.", "section": "2 Latent Language Modeling"}, {"figure_path": "https://arxiv.org/html/2412.08635/x3.png", "caption": "Figure 3: Compared to variational autoencoder (VAE), \u03c3\ud835\udf0e\\sigmaitalic_\u03c3-VAE uses a fixed variance for the latent space. It avoids variance collapse and makes LatentLM more robust to exposure bias during autoregressive generation. In our method, \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 is a scalar that is sampled from \ud835\udca9\u2062(0,C\u03c3)\ud835\udca90subscript\ud835\udc36\ud835\udf0e\\mathcal{N}(0,C_{\\sigma})caligraphic_N ( 0 , italic_C start_POSTSUBSCRIPT italic_\u03c3 end_POSTSUBSCRIPT ) for each example.", "description": "Figure 3 illustrates the architecture of the proposed \u03c3-VAE, comparing it to the standard VAE.  In contrast to the VAE, which learns the variance of the latent space, \u03c3-VAE uses a fixed variance. This fixed variance, denoted as \u03c3, is sampled from a normal distribution with a mean of 0 and a variance controlled by the hyperparameter C\u03c3. This modification is crucial for preventing variance collapse, a common issue in VAEs, and helps maintain the stability of the latent space, especially important during the autoregressive generation process within LatentLM. By ensuring a consistent variance, the model becomes less susceptible to exposure bias which negatively impacts the quality of autoregressive generation.", "section": "2.3 Latent Vector Representation of Continuous Data"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/scaling_curve.png", "caption": "Figure 4: Scaling curves of DiT and LatentLM. FID\u00a0[27] consistently becomes better with larger model size.", "description": "Figure 4 illustrates the scalability of both Diffusion Transformer (DiT) and LatentLM models by comparing their Fr\u00e9chet Inception Distance (FID) scores across different model sizes.  The FID score, a metric evaluating the quality of generated images, consistently decreases (improving) as the model size increases for both architectures.  However, LatentLM demonstrates a more substantial reduction in FID with increasing model size compared to DiT, suggesting improved scaling efficiency and overall image generation quality.", "section": "3.1 Image Generation: Scalable Autoregressive Modeling"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/fid_combined.png", "caption": "Figure 5: Samples of LatentLM trained on ImageNet. The resolution is 384\u00d7\\times\u00d7384. The image is generated by models described in Section\u00a03.1.2.", "description": "This figure displays sample images generated by the LatentLM model after training on the ImageNet dataset.  The images showcase the model's ability to generate high-resolution (384x384 pixels) images.  The specific model architecture and training details are described in Section 3.1.2 of the paper.  The variety of images demonstrates the model's capacity to generate diverse and visually appealing results across different ImageNet categories.", "section": "3.1 Image Generation: Scalable Autoregressive Modeling"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_bsz128.png", "caption": "Figure 6: Image generation results of Diffusion Transformer (DiT)\u00a0[51] and LatentLM on ImageNet. We report FID\u00a0[27] scores (lower is better) in the settings of different tokenizer variance and CFG\u00a0[28] scale.\nThe \u201cstars\u201d represent the tokenizers tuned for previous image-level diffusion models\u00a0[53], which are ineffective for LatentLM. The results indicate that LatentLM favors tokenizers with larger variances.", "description": "Figure 6 presents a comparative analysis of image generation performance between Diffusion Transformer (DiT) and LatentLM on the ImageNet dataset.  The key focus is on how different tokenizer variances and classifier-free guidance (CFG) scales affect the FID scores (a lower FID score indicates better image quality). The results reveal that LatentLM significantly benefits from tokenizers with larger variance, unlike DiT.  The figure also highlights the ineffectiveness of tokenizers optimized for previous image diffusion models when used with LatentLM, emphasizing the unique characteristics of LatentLM's architecture.", "section": "3.1 Image Generation: Scalable Autoregressive Modeling"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_XL.png", "caption": "(a) Throughput with increasing model sizes.", "description": "This figure compares the inference throughput of Diffusion Transformer (DiT) and LatentLM models with varying model sizes.  It shows that LatentLM's throughput scales more favorably with increasing model size than DiT, demonstrating significant improvements particularly for larger models, which is attributed to LatentLM's more efficient use of computational resources.", "section": "3.1 Image Generation: Scalable Autoregressive Modeling"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/mllm_fid.png", "caption": "(b) Throughput with increasing batch sizes.", "description": "This figure shows the inference throughput of the Diffusion Transformer (DiT) and LatentLM models with varying batch sizes.  The throughput is measured on a single H100 GPU, using 20 diffusion inference steps.  The results demonstrate how the throughput of LatentLM scales favorably with increasing batch size, showcasing the model's efficient use of resources compared to DiT.  The graph also includes results for LatentLM with group-query attention (GQA) demonstrating further improvements in throughput.", "section": "3.1 Image Generation: Scalable Autoregressive Modeling"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/mllm_ppl.png", "caption": "Figure 7: We compare the inference throughput of Diffusion Transformer (DiT)\u00a0[51] and LatentLM in the settings of different model size and batch size. \u201cGQA\u201d stands for group-query attention\u00a0[2].", "description": "This figure compares the inference throughput (speed of generating outputs) of two different models, Diffusion Transformer (DiT) and LatentLM, under varying conditions.  It shows how the throughput changes with different model sizes (smaller to larger) and different batch sizes (number of inputs processed simultaneously).  The results demonstrate LatentLM's efficiency and scalability, particularly when using larger models and batch sizes. The inclusion of \"GQA\" (group-query attention) further enhances LatentLM's performance.", "section": "3.1 Image Generation: Scalable Autoregressive Modeling"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/text-to-image/mountain.png", "caption": "(a) Text-to-image FID\u00a0[27].", "description": "Figure 8(a) presents a graph showing the FID (Fr\u00e9chet Inception Distance) scores for text-to-image generation as the number of training tokens increases.  The lower FID score indicates better image quality. The graph compares the performance of LatentLM against VQ-MLLM (a model using vector quantization for images) and Transfusion (a model combining language modeling and diffusion).  The x-axis represents the number of training tokens (in billions), and the y-axis represents the FID score. This graph demonstrates LatentLM's scalability and superior performance compared to other approaches, showcasing improved image generation with larger datasets.", "section": "3.2 Multimodal LLMs: Unified Understanding and Generation"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/text-to-image/city.png", "caption": "(b) Image-to-text validation perplexity.", "description": "This figure shows how the perplexity of image-to-text generation changes as the number of training tokens increases.  Perplexity is a measure of how well a language model predicts a sequence; lower perplexity indicates better performance. The graph allows for comparison across different models, revealing trends and relative performance differences in handling image-to-text tasks as training data scales.", "section": "3.2 Multimodal LLMs: Unified Understanding and Generation"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/text-to-image/lake.png", "caption": "Figure 8: We scale up the number of training tokens for multimodal large language models. LatentLM outperforms vector quantized models (VQ-MLLM) and Transfusion\u00a0[82] for both text-to-image and image-to-text generation. The FID scores are evaluated on MS-COCO\u00a0[40].", "description": "Figure 8 presents a scalability analysis of multimodal large language models, focusing on the impact of increasing the number of training tokens.  The experiment compares LatentLM's performance against two other methods: vector quantized models (VQ-MLLM) and the Transfusion model.  The evaluation metrics used are FID (Fr\u00e9chet Inception Distance) scores for text-to-image generation, and perplexity for image-to-text generation, with the MS-COCO dataset serving as the benchmark.  The results visually demonstrate that as the quantity of training tokens increases, LatentLM consistently achieves lower FID scores and perplexity than both VQ-MLLM and Transfusion, highlighting its superior scalability and performance in multimodal generation and understanding tasks.", "section": "3.2 Multimodal LLMs: Unified Understanding and Generation"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/text-to-image/house.png", "caption": "(c) A majestic mountain range covered in snow.", "description": "A photograph depicting a majestic mountain range completely covered in a blanket of pristine, white snow. The image likely showcases a vast, expansive landscape, possibly in a mountainous region known for its snow-covered peaks.  The snow appears undisturbed and fresh, highlighting the beauty and serenity of the natural scene. The image is sharp and detailed, likely taken with professional equipment in good weather conditions.", "section": "3.2 Multimodal LLMs: Unified Understanding and Generation"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/tts_cfg_scale.png", "caption": "(d) A city street illuminated by lights.", "description": "The image shows a city street at night, brightly lit by various light sources.  The scene is likely urban, given the presence of buildings and streetlights. The illumination suggests an active, bustling atmosphere, even though no people are visible in the specific image shown.", "section": "3.1 Image Generation: Scalable Autoregressive Modeling"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/tts_sampling_steps.png", "caption": "(e) A crystal lake surrounded by autumn trees.", "description": "An image depicting a serene crystal-clear lake nestled amidst a vibrant autumnal forest. The trees surrounding the lake showcase a rich palette of fall colors, reflecting the tranquility of the scene. The overall mood is peaceful and picturesque, showcasing the beauty of nature during the autumn season.", "section": "3.2 Multimodal LLMs: Unified Understanding and Generation"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_Large.png", "caption": "(f) A small house in a wooden at sunset.", "description": "The image shows a small house, seemingly made of wood, situated in a setting bathed in the warm, golden light of sunset.  The scene is peaceful and evokes a sense of tranquility.", "section": "3.2 Multimodal LLMs: Unified Understanding and Generation"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_3B.png", "caption": "Figure 9: Text-to-image examples of LatentLM.", "description": "Figure 9 presents four examples of images generated by the LatentLM model using text prompts.  Each image showcases the model's ability to generate realistic and detailed visuals based on diverse and concise textual descriptions.  The captions illustrate the range of concepts and styles that LatentLM can effectively translate into visual representations, highlighting its versatility in the text-to-image task.", "section": "3.2 Multimodal LLMs: Unified Understanding and Generation"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_7B.png", "caption": "(a) Results using different CFG scales.", "description": "This figure shows ablation study results on the effect of classifier-free guidance (CFG) scale on the performance of zero-shot speech synthesis.  The x-axis represents different CFG scales, and the y-axis shows the metrics of SIM (speaker similarity), WER-C (word error rate using Conformer-Transducer), and WER-H (word error rate using HuBERT-Large).  The plot illustrates how varying the CFG scale impacts the quality and accuracy of the generated speech.", "section": "3.3 Text-to-Speech Synthesis: Higher Compression Ratio, Fewer Decoding Steps"}, {"figure_path": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_13B.png", "caption": "(b) Results using different sampling steps.", "description": "This figure shows the effects of varying the number of sampling steps during the inference phase of a diffusion model used for zero-shot speech synthesis.  The x-axis represents the number of sampling steps, while the y-axis displays the resulting values for the speaker similarity (SIM) metric and the word error rate (WER-C).  The results demonstrate how the model's performance changes as the number of inference steps increases.  The plot helps to determine an optimal number of sampling steps that balances performance and computational efficiency.", "section": "3.3 Text-to-Speech Synthesis: Higher Compression Ratio, Fewer Decoding Steps"}]