[{"content": "| Type | Model | #Params | #Epochs | FID\u2193 | IS\u2191 |\n|---|---|---|---|---|---| \n| *Non-Causal-Masking Generation* |  |  |  |  |  |\n| Diffusion | LDM-4 [53] | 400M | \u2014 | 3.60 | 247.7 |\n|  | DiT-XL/2 [51] | 675M | 400 | 2.27 | 278.2 |\n|  | U-ViT-H/2 [4] | 501M | 400 | 2.29 | 263.9 |\n| Masked Generative | MaskGIT [13] | 227M | 300 | 4.02 | 355.6 |\n|  | MAR-L [43] | 479M | 800 | 1.78 | 296.0 |\n| *Causal-Masking Generation* |  |  |  |  |  |\n| Causal-Discrete | VQGAN [18] | 1.4B | 240 | 5.20 | 280.3 |\n|  | ViT-VQGAN [79] | 1.7B | 240 | 3.04 | 227.4 |\n|  | LlamaGen-XL [66] | 775M | 300 | 2.62 | 244.1 |\n|  | LlamaGen-XXL [66] | 1.4B | 300 | 2.34 | 253.9 |\n| Causal-Continuous | GIVT-Causal-L+A [70] | 1.67B | 500 | 2.59 | \u2014 |\n|  | LatentLM-L (This Work) | 479M | 400 | 2.24 | 253.8 |", "caption": "Table 1: Image generation results on ImageNet\u00a0[15]. We evaluate FID\u00a0[27] and IS\u00a0[62]. LatentLM achieves competitive performance, especially compared with other causal-masking image generation models.", "description": "Table 1 presents a comparison of image generation performance on the ImageNet dataset between LatentLM and other state-of-the-art models.  It evaluates both FID (Fr\u00e9chet Inception Distance) and IS (Inception Score), commonly used metrics for assessing the quality of generated images. The results highlight LatentLM's competitive performance, particularly in comparison to other models using the causal masking generation approach.", "section": "3.1 Image Generation: Scalable Autoregressive Modeling"}, {"content": "| Resolution | FID-50k \u2193 | \n|---|---| \n| 256 \u00d7 256 | 3.19 | \n| 384 \u00d7 384 | **2.51** | ", "caption": "Table 2: FID\u00a0[27] of scaling up image resolution.", "description": "This table presents the Fr\u00e9chet Inception Distance (FID) scores, a metric used to evaluate the quality of generated images, for different image resolutions.  Specifically, it shows how the FID score changes when scaling up the resolution of generated images from 256x256 pixels to 384x384 pixels. A lower FID score indicates better image quality.", "section": "3.1 Image Generation: Scalable Autoregressive Modeling"}, {"content": "| Model | Text Valid PPL \u2193 | Text-to-Image FID \u2193 | Text-to-Image CLIP \u2191 | Image-to-Text MS-COCO \u2191 | Image-to-Text VQAv2 \u2191 |\n|---|---|---|---|---|---| \n| VQ-MLLM | 2.79 | 16.92 | **29.33** | 37.4 | 30.19 |\n| Transfusion | 2.74 | 16.10 | 28.66 | 43.4 | 35.36 |\n| LatentLM | **2.73** | **14.54** | 28.75 | **54.5** | **38.72** |", "caption": "Table 3: Results of multimodal large language models on text language modeling, image-to-text, and text-to-image generation.\nWe compare with Transfusion\u00a0[82] and vector quantized models (VQ-MLLM; i.e., using discrete code to represent images).\n\u201cPPL\u201d is perplexity. CLIP\u00a0[54] score measures the similarity. We report CIDEr\u00a0[76] score for MS-COCO\u00a0[40] and accuracy for VQAv2\u00a0[21].", "description": "This table presents a comparison of different multimodal large language models on three key tasks: text language modeling, image-to-text generation, and text-to-image generation.  The models compared include the proposed LatentLM, Transfusion (a state-of-the-art baseline), and VQ-MLLM (which uses vector quantization for image representation).  The evaluation metrics used provide a comprehensive assessment across all three tasks, using perplexity for text modeling, CLIP score for similarity between text and images, CIDEr score for image caption quality (MS-COCO dataset), and accuracy for visual question answering (VQAv2 dataset). This allows for a thorough comparison of the models' performance in handling both discrete and continuous data.", "section": "3.2 Multimodal LLMs: Unified Understanding and Generation"}, {"content": "| System | Frame Rate (Length/s) \u2193 |  | Ref Utterance as Prompt | SIM \u2191 | WER-C \u2193 | WER-H \u2193 | 3s Prefix as Prompt | SIM \u2191 | WER-C \u2193 | WER-H \u2193 |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Ground Truth | - |  |  | 0.779 | 1.6 | 2.2 |  | 0.668 | 1.6 | 2.2 |\n| VALL-E 2 [10] | 75 |  |  | 0.643 | 1.5 | 2.4 |  | 0.504 | 1.6 | 2.3 |\n| Voicebox [44] | 100 |  |  | 0.662 | - | 1.9 |  | 0.593 | - | 2.0 |\n| MELLE [48] | 62 |  |  | 0.625 | 1.5 | 2.1 |  | 0.508 | 1.5 | 2.0 |\n| LatentLM | 15 |  |  | 0.697 | 1.2 | 1.8 |  | 0.571 | 1.4 | 2.0 |\n| LatentLM | 7.5 |  |  | 0.656 | 1.2 | 1.7 |  | 0.532 | 1.6 | 2.3 |\n| LatentLM | 3.75 |  |  | 0.598 | 1.7 | 2.3 |  | 0.467 | 3.1 | 4.5 |", "caption": "Table 4: LatentLM outperforms previous systems on zero-shot speech synthesis in both settings. Moreover, the number of decoding steps is much less than others, achieving faster inference speed. The results are reported on LibriSpeech test-clean set. The WER-H and SIM results of VALL-E 2 using 3s prefix as prompt are from [48].", "description": "Table 4 presents a comparison of LatentLM's performance against other state-of-the-art models on zero-shot speech synthesis.  The evaluation considers two scenarios: using the entire reference utterance as a prompt, and using only the first 3 seconds of the utterance as a prompt.  The metrics used include Speech Similarity (SIM), Word Error Rate using the Conformer-Transducer model (WER-C), and Word Error Rate using the HuBERT-Large model (WER-H).  The table highlights that LatentLM achieves better results in terms of SIM, WER-C, and WER-H compared to previous systems. A key advantage of LatentLM is its significantly reduced number of decoding steps, leading to much faster inference speed. The results are based on the LibriSpeech test-clean dataset.", "section": "3.3 Text-to-Speech Synthesis: Higher Compression Ratio, Fewer Decoding Steps"}, {"content": "| Tokenizer | N<sub>q</sub> \u2193 | Frame \u2193 | Comp. \u2191 | Mel Dist. \u2193 | PESQ \u2191 | STOI \u2191 | VISQOL \u2191 | UTMOS \u2191 |\n|---|---|---|---|---|---|---|---|---|\n| *Tokenizers with lower compression ratio* |\n| Encodec [14] | 32 | 75 | 10 | 0.823 | 3.591 | 0.962 | 4.536 | 3.195 |\n| DAC [34] | 32 | 75 | 10 | **0.355** | **4.424** | **0.987** | **4.914** | **3.469** |\n| Encodec [14] | 8 | 75 | 40 | 0.987 | 2.687 | 0.925 | 4.258 | 2.656 |\n| DAC [34] | 8 | 75 | 40 | 0.707 | 3.329 | 0.941 | 4.485 | 3.133 |\n| DAC<sub>low</sub> [59] | 4 | 75 | 80 | 0.753 | 3.107 | 0.938 | 4.391 | 3.453 |\n| DAC<sub>low</sub> [59] | 2 | 75 | 160 | 0.916 | 2.269 | 0.896 | 3.981 | 3.297 |\n| Mimi [17] | 8 | 12.5 | 240 | 0.987 | 3.217 | 0.946 | 4.332 | 3.375 |\n| *Tokenizers with higher compression ratio* |\n| WavTokenizer [31] | 1 | 75 | 320 | 0.871 | 2.266 | 0.891 | 4.120 | 3.432 |\n| Mimi [17] | 4 | 12.5 | 480 | 1.458 | 1.568 | 0.826 | 3.390 | 2.652 |\n| WavTokenizer [31] | 1 | 40 | 600 | 1.037 | 1.670 | 0.834 | 3.782 | 3.053 |\n| \u03c3-VAE<sub>32</sub> | 1 | 15 | 1600 | 0.813 | 2.724 | 0.926 | 4.268 | 3.491 |\n| \u03c3-VAE<sub>64</sub> | 1 | 7.5 | 3200 | **0.798** | **2.756** | **0.929** | **4.289** | **3.505** |\n| \u03c3-VAE<sub>128</sub> | 1 | 3.75 | 6400 | 0.852 | 2.533 | 0.916 | 4.165 | 3.460 |", "caption": "Table 5: The \u03c3\ud835\udf0e\\sigmaitalic_\u03c3-VAE tokenizers obtain competitive reconstruction quality while having high compression ratio.\nWe report results on the LibriTTS test-other set.\n\u201cNqsubscript\ud835\udc41\ud835\udc5eN_{q}italic_N start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT\u201d represents the number of quantizers.\nWe define the compression ratio as the audio sample rate divided by Nqsubscript\ud835\udc41\ud835\udc5eN_{q}italic_N start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT and the frame rate.\n\u201c\u03c3\ud835\udf0e\\sigmaitalic_\u03c3-VAE32\u201d denotes that the latent dimension of the tokenizer is 32.", "description": "Table 5 presents a comparison of various audio codec models, focusing on their compression ratios, reconstruction quality, and other relevant metrics.  The models are evaluated on the LibriTTS test-other dataset.  The table highlights the performance of the \u03c3-VAE tokenizer, demonstrating its ability to achieve high compression ratios while maintaining good reconstruction quality.  The compression ratio is calculated by dividing the audio sample rate by the number of quantizers (Nq) and the frame rate.  The latent dimension of the tokenizer is also specified (e.g., \u03c3-VAE32 indicates a latent dimension of 32).", "section": "3.3 Text-to-Speech Synthesis: Higher Compression Ratio, Fewer Decoding Steps"}, {"content": "| Frame | Rate |\n|---|---|", "caption": "Table 6: Ablation results of different \u03c3\ud835\udf0e\\sigmaitalic_\u03c3-VAE compression ratios and latent dimensions. We report tokenizer reconstruction quality and zero-shot speech synthesis.", "description": "This table presents an ablation study analyzing the impact of different compression ratios and latent dimensions within the \u03c3-VAE (sigma-Variational Autoencoder) component of the LatentLM model on both the quality of the tokenizer's reconstruction of speech data and the performance of zero-shot speech synthesis.  It shows how changes in these hyperparameters affect various metrics like Mel-cepstral distance, speaker similarity (SIM), and word error rate (WER), providing insights into the trade-offs between compression efficiency and speech generation quality.", "section": "3.3 Text-to-Speech Synthesis: Higher Compression Ratio, Fewer Decoding Steps"}, {"content": "| Comp. | Ratio |\n|---|---|", "caption": "Table 7: Model size and hyperparameters used for the scaling experiments in\u00a0Section\u00a03.1.2.", "description": "This table details the model architecture configurations used in the scalability experiments of Section 3.1.2.  It lists the model size (in parameters), the hidden dimension of the Transformer layers, the number of layers, the number of attention heads in each layer, and the learning rate used during training.  These different configurations allowed the authors to assess how the performance of their model scales with increasing size and complexity.", "section": "3.1 Image Generation: Scalable Autoregressive Modeling"}, {"content": "| Mel | Dist. |\n|---|---|", "caption": "Table 8: Hyperparameters used for multimodal large language models in\u00a0Section\u00a03.2.", "description": "This table lists the hyperparameters used to train the multimodal large language models described in Section 3.2 of the paper.  It includes the number of layers, hidden size, feedforward network (FFN) size, vocabulary size, number of attention heads, Adam optimizer beta parameters, learning rate, batch size, warmup steps, weight decay, and number of head layers.", "section": "3.2 Multimodal LLMs: Unified Understanding and Generation"}, {"content": "| Compression Ratio | Frame Rate | Latent Dimension |  \u03c3-VAE Reconstruction  |  | Zero-Shot TTS | | \n|---|---|---|---|---|---|---|---|---|\n| **640 \u00d7** | 37.5 | 16  |  | Mel Dist. \u2193 | SIM \u2191 | WER-C \u2193 |  |  |\n| **1600 \u00d7** | 15 | 16 |  | 0.929 | 0.866 | 1.9 |  | 0.655 | 1.4 |\n| **1600 \u00d7** | 15 | 16 |  | 1.080 | 0.700 | 2.7 |  | 0.545 | 1.6 |\n| **1600 \u00d7** | 15 | 32 |  | 0.950 | 0.870 | 1.9 |  | 0.661 | 1.5 |", "caption": "Table 9: Hyperparameters used for text-to-speech synthesis in Section\u00a03.3.", "description": "This table details the hyperparameters used in the training of the text-to-speech synthesis model described in Section 3.3 of the paper.  It includes specifications for the number of layers, hidden layer size, feed-forward network (FFN) size, number of attention heads, Adam optimizer parameters (beta1 and beta2), learning rate, learning rate scheduling method, batch size, number of warmup steps during training, weight decay, and the number of layers in the diffusion head.", "section": "3.3 Text-to-Speech Synthesis: Higher Compression Ratio, Fewer Decoding Steps"}]