{"references": [{" publication_date": "2021", "fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper introduces CLIP, a foundational vision-language model that is central to the TransAgent's design.  The core idea of transferring knowledge from a large-scale pre-trained model to improve downstream performance is directly related to TransAgent's methodology. CLIP's success and limitations in dealing with domain shifts are extensively discussed, providing a strong justification for TransAgent's innovative approach.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper highlights the remarkable few-shot learning capabilities of large language models (LLMs). This is highly relevant to TransAgent's goal of improving few-shot learning performance in vision-language models by leveraging the knowledge from various pre-trained agents, including LLMs.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "J.-B. Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "This paper presents Flamingo, a visual language model that demonstrates strong few-shot learning capabilities.  TransAgent's design aims to improve similar capabilities by integrating knowledge from multiple sources, including models that share conceptual similarities to Flamingo.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "reason": "This paper introduces the Vision Transformer (ViT), a fundamental architecture widely used in vision models. Many of the agents used in TransAgent leverage ViT or its variants, demonstrating the influence of this paper and establishing the foundation for some of TransAgent's core components.  The paper also discusses the scalability aspect, indirectly relating to TransAgent's ambitious use of multiple agents.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "M. Caron", "paper_title": "Emerging properties in self-supervised vision transformers", "reason": "This work introduces a self-supervised approach for training Vision Transformers that is closely related to the knowledge distillation methods employed in TransAgent. The methods for extracting knowledge from various vision agents in TransAgent build upon the foundation established by this paper's self-supervised training techniques.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "K. He", "paper_title": "Masked autoencoders are scalable vision learners", "reason": "This paper introduces MAE, a prominent self-supervised learning method for vision. MAE is one of the crucial agents integrated into TransAgent's framework.  Its design heavily influences the approach to knowledge extraction and transfer from the vision agents within TransAgent's framework.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "A. Kirillov", "paper_title": "Segment anything", "reason": "This paper presents SAM, a powerful segmentation model, which is another critical component of TransAgent's vision agent collaboration. The ability of SAM to segment diverse objects and scenes is directly integrated to enhance TransAgent's ability to perform visual recognition and understanding.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Y. Li", "paper_title": "Exploring plain vision transformer backbones for object detection", "reason": "This paper introduces ViTDet, an important component of TransAgent's vision agents, which contributes to the framework's improved visual recognition capabilities. ViTDet's focus on object detection and fine-grained visual understanding is directly leveraged within TransAgent.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "J. Li", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "reason": "This paper presents BLIP-2, a crucial multi-modal agent in TransAgent's framework. BLIP-2's capabilities in image-to-text generation are leveraged to extract and transfer multi-modal knowledge which significantly enhance TransAgent's vision-language alignment and understanding.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "K. Chen", "paper_title": "Shikra: Unleashing multimodal LLM's referential dialogue magic", "reason": "Shikra, a powerful multi-modal language model, plays a key role in TransAgent's framework. Its capacity for image-to-text generation with detailed description is leveraged to extract and integrate multi-modal knowledge, further enhancing TransAgent's performance in tasks involving vision-language understanding.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "J. Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This paper is foundational to diffusion models which are utilized by TransAgent's multi-modal collaboration to improve the fine-grained visual understanding. The underlying principles and techniques employed in diffusion models directly influence TransAgent's approach to image generation and analysis.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "R. Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "This work details a high-resolution image synthesis model which is relevant to TransAgent's multi-modal agents for fine-grained visual understanding.  TransAgent leverages techniques from this paper to enhance image representations and visual comprehension.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "M. U. Khattak", "paper_title": "Maple: Multi-modal prompt learning", "reason": "This work explores multi-modal prompt learning which is relevant to TransAgent. Although TransAgent uses a distinct approach, this work provides insights into the potential of prompt engineering in multi-modal learning, providing an alternative approach to the knowledge transfer techniques used in TransAgent.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "K. Zhou", "paper_title": "Learning to prompt for vision-language models", "reason": "This paper is highly influential in the domain of prompt learning for vision-language models, which is directly relevant to the methods explored in TransAgent.  It explores the effectiveness of prompt learning for better model generalization and provides insight into how prompting techniques can be adapted to enhance model performance.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "C. Jia", "paper_title": "Scaling up visual and vision-language representation learning with noisy text supervision", "reason": "This paper is relevant as it explores large-scale vision-language representation learning, which is fundamental to TransAgent's approach. The techniques and concepts related to pre-training vision-language models using large datasets influence TransAgent's design and the methods used to integrate knowledge from multiple agents.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "R. Zhang", "paper_title": "Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners", "reason": "This paper explores the use of a cascade of foundation models for improving few-shot learning performance, which is conceptually related to TransAgent's design.  It is a competing method that TransAgent aims to improve upon using its more efficient and flexible approach for knowledge integration.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "K. He", "paper_title": "Masked autoencoders are scalable vision learners", "reason": "MAE provides a scalable self-supervised learning technique for vision. TransAgent uses MAE as one of its key agents, and MAE's ability to learn effective representations from masked images directly impacts TransAgent's performance.   The method of knowledge extraction and integration developed in TransAgent is directly influenced by the features of MAE.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "reason": "This seminal paper demonstrates the remarkable few-shot learning capability of large language models (LLMs).  TransAgent leverages the knowledge distilled from LLMs as one of its key components. The fundamental insights provided by this paper on how LLMs can generalize to new tasks are crucial to TransAgent's methodology and its success.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "N. Carion", "paper_title": "End-to-end object detection with transformers", "reason": "This paper introduces an end-to-end object detection method which is important for TransAgent.  The architecture and techniques related to object detection directly influence how TransAgent handles visual data and extracts relevant information from different vision agents within its framework.", "section_number": 2}]}