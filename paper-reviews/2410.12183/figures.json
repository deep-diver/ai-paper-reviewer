[{"figure_path": "2410.12183/figures/figures_2_0.png", "caption": "Figure 1: An overview of our TransAgent. (a) TransAgent transfers multi-source knowledge from heterogeneous agents to enhance the generalization ability of vision-language foundation models. It demonstrates knowledge versatility, transfer flexibility and deployment efficiency through elaborate agent collaboration and knowledge ensemble strategy. (b) SOTA comparison for base-to-novel generalization on 11 visual recognition benchmarks. Our method outperforms previous SOTA, especially on the more diversified target domains.", "description": "This figure illustrates the TransAgent framework, showing how it leverages multiple heterogeneous agents to improve the generalization ability of vision-language foundation models, and compares its performance against state-of-the-art methods.", "section": "1 Introduction"}, {"figure_path": "2410.12183/figures/figures_4_0.png", "caption": "Figure 2: Vision Agent Collaboration and Language Agent Collaboration. (a) VAC integrates visual knowledge via MoA gating and transfers the knowledge through layer-wise feature distillation. (b) LAC enhances the textual representations through class-specific feature distillation between the prompted textual feature and the gated textual feature.", "description": "Figure 2 illustrates the TransAgent framework's vision and language agent collaboration, detailing knowledge integration and distillation processes for enhanced model performance.", "section": "3 Method"}, {"figure_path": "2410.12183/figures/figures_6_0.png", "caption": "Figure 3: Multi-modal Agent Collaboration. Top left: We first extract the cross attention maps from the T2I agents and then obtain the score vectors through LSE pooling. Top right: We compute the score vectors from the I2T agents as the cosine similarity between the projected visual feature and the LLM's textual feature. Finally, we perform score distillation between the learned score vectors and the gated score vectors to further align the learnable prompts.", "description": "This figure illustrates the multi-modal agent collaboration in TransAgent, showing how cross attention maps are extracted from T2I and I2T agents, processed, and used to align learnable prompts via score distillation.", "section": "3 Method"}, {"figure_path": "2410.12183/figures/figures_8_0.png", "caption": "Figure 4: Accuracy comparison in few-shot classification. TransAgent demonstrates state-of-the-art performance for all few-shot settings on different datasets, which proves promising learning capability even under extremely limited supervision.", "description": "Figure 4 shows the accuracy comparison of TransAgent and other methods in few-shot classification settings on eleven different datasets.", "section": "4 Experiments"}]