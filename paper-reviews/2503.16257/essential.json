{"importance": "This paper is crucial for researchers working on efficient video processing. By introducing a method to significantly compress KV cache with minimal performance loss, it addresses a key bottleneck in **VideoLLMs**. This work has great potential of enabling faster and more scalable video understanding and analysis. It opens new research avenues in low-bit quantization techniques for video data.", "summary": "VidKV: Achieves 1.5x-bit KV cache quantization for VideoLLMs, maintaining performance without retraining.", "takeaways": ["VidKV introduces a mixed-precision quantization scheme for key and value caches in VideoLLMs.", "The method uses FFT to stabilize key cache distributions and a per-channel quantization for value caches, improving efficiency.", "Experiments show that VidKV achieves significant KV cache compression with almost no performance drop on several benchmarks."], "tldr": "Video Large Language Models(**VideoLLMs**) are powerful for understanding video content, but they require a lot of memory for key-value (KV) caches, which limits their efficiency. Existing KV cache quantization techniques reduce memory usage, but their applicability to VideoLLMs remains largely unexplored. This paper shows a basic group-wise 2-bit KV cache quantization has achieved promising performance for VideoLLMs.\n\nTo improve upon existing methods, this paper introduces **VidKV**, a training-free method that employs mixed-precision quantization for the key cache (2-bit for anomalous channels, 1-bit with FFT for normal channels) and 1.58-bit quantization for the value cache, selectively preserving semantically salient tokens. VidKV effectively compresses the KV cache with minimal performance loss.", "affiliation": "Westlake University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.16257/podcast.wav"}