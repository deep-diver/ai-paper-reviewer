[{"heading_title": "Agentic Program DAG", "details": {"summary": "Agentic program DAGs represent a shift from static LLM applications to dynamic, general-purpose programs. They model the complex interactions between LLM calls, external tools, and human inputs as a directed acyclic graph, capturing the program's control flow. **This is a crucial abstraction because it acknowledges dependencies between LLM calls, enabling optimization opportunities.** Traditional LLM serving systems, designed for independent requests, often miss these dependencies, leading to suboptimal performance. By treating programs as first-class citizens and understanding their DAG structure, it becomes possible to prioritize and schedule LLM calls more effectively, reducing end-to-end latency and improving overall system throughput. The DAG representation also allows for identifying critical paths and prioritizing calls that contribute most to program completion."}}, {"heading_title": "Program-Aware Scheduling", "details": {"summary": "**Program-aware scheduling** is a critical area in optimizing LLM serving systems. Traditional schedulers often treat LLM requests in isolation, neglecting dependencies and context within programs. This leads to suboptimal performance, as evidenced by head-of-line blocking and inefficient resource utilization. Program-aware scheduling aims to address these limitations by incorporating program-level information, such as dependencies between calls and cumulative execution time, into scheduling decisions. This approach allows for more informed prioritization, preemption, and resource allocation, ultimately reducing end-to-end latency and improving overall system throughput. By understanding the context of each LLM call within its parent program, the scheduler can make more intelligent decisions, leading to improved efficiency and fairness."}}, {"heading_title": "Autellix Architecture", "details": {"summary": "Autellix's architecture centers around a **stateful backend** that contrasts with typical stateless LLM engines. Programs execute from the user's local machine, initiating a session with the Autellix backend and sending LLM calls with session IDs. A **global process table** tracks program metadata like service time and thread information, guiding the scheduler and load balancer. The architecture aims to improve end-to-end latency, maximize GPU utilization, and prevent program starvation. It utilizes a **non-clairvoyant approach**, dynamically building an internal representation of the program's execution graph as it runs, enabling generalization to diverse programs using LLM calls. Figure 8 illustrates the architecture's components, including LLM engines, priority function, memory manager, model executor, and scheduler, working in concert to optimize performance."}}, {"heading_title": "KV Cache Locality", "details": {"summary": "**KV cache locality** is critical for LLM serving efficiency.  **Reusing cached key-value states** across LLM calls reduces redundant computation and speeds up inference. Agentic workloads highlight two key aspects: **intra-program locality** where calls within the same program often share common prefixes (e.g., system prompts, conversation history) leading to high cache hit rates and **inter-program locality** where calls across programs might only share the system prompt leading to lower hit rates. Efficiently exploiting intra-program locality via techniques like prefix caching and shared memory is key. The load balancer is critical to maintain locality, while balancing load across multiple engines. Prioritizing routing request to the right engine depending on program characteristics, it is crucial to improve overall efficiency. For short LLM calls with minimal history, common system prompts may negate the need to maintain data locality."}}, {"heading_title": "ATLAS:Thread Aware", "details": {"summary": "While the specific heading 'ATLAS: Thread Aware' might not be directly present, the paper discusses 'ATLAS (Adaptive Thread-Level Attained Service),' an algorithm designed to manage multi-threaded programs, where awareness of individual thread progress and inter-dependencies is crucial. It likely alludes to the algorithm's capability to prioritize LLM calls stemming from critical threads within a program, particularly in dynamic DAGs representing complex agentic tasks. **The core concept is to avoid straggler threads that could delay overall program completion, aligning with the goal of minimizing end-to-end latency.** The \u2018thread aware\u2019 aspect indicates that ATLAS doesn't treat all threads equally; instead, it dynamically adapts its scheduling based on the cumulative service time and estimated critical path of each thread. **This contrasts with simpler policies that treat all LLM calls uniformly, regardless of their importance to the overall program's progress.** By accounting for thread-level contributions to the critical path, ATLAS likely enables a more efficient resource allocation and improved program completion times, ensuring that threads essential for rapid progress receive the necessary priority."}}]