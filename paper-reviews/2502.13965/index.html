<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Autellix: An Efficient Serving Engine for LLM Agents as General Programs &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="Autellix: An Efficient Serving Engine for LLM Agents as General Programs &#183; HF Daily Paper Reviews by AI"><meta name=description content="Autellix: Efficient LLM Serving for Agents"><meta name=keywords content="Natural Language Processing,Large Language Models,üè¢ UC Berkeley,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13965/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13965/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="Autellix: An Efficient Serving Engine for LLM Agents as General Programs"><meta property="og:description" content="Autellix: Efficient LLM Serving for Agents"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-02-19T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-19T00:00:00+00:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="üè¢ UC Berkeley"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13965/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13965/cover.png"><meta name=twitter:title content="Autellix: An Efficient Serving Engine for LLM Agents as General Programs"><meta name=twitter:description content="Autellix: Efficient LLM Serving for Agents"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Autellix: An Efficient Serving Engine for LLM Agents as General Programs","headline":"Autellix: An Efficient Serving Engine for LLM Agents as General Programs","abstract":"Autellix: Efficient LLM Serving for Agents","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2502.13965\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-02-19T00:00:00\u002b00:00","datePublished":"2025-02-19T00:00:00\u002b00:00","dateModified":"2025-02-19T00:00:00\u002b00:00","keywords":["Natural Language Processing","Large Language Models","üè¢ UC Berkeley"],"mainEntityOfPage":"true","wordCount":"4705"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-02-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-02-21</p></a><a href=/ai-paper-reviewer/2025-02-24/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-02-24</p></a><a href=/ai-paper-reviewer/2025-02-25/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-02-25</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-02-21</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-24/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-02-24</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-25/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-02-25</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2502.13965/cover_hu15389805956958687483.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2502.13965/>Autellix: An Efficient Serving Engine for LLM Agents as General Programs</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Autellix: An Efficient Serving Engine for LLM Agents as General Programs</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-02-19T00:00:00+00:00>19 February 2025</time><span class="px-2 text-primary-500">&#183;</span><span>4705 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">23 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2502.13965/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2502.13965/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/natural-language-processing/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Natural Language Processing
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/large-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-uc-berkeley/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ UC Berkeley</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#agentic-program-dag>Agentic Program DAG</a></li><li><a href=#program-aware-scheduling>Program-Aware Scheduling</a></li><li><a href=#autellix-architecture>Autellix Architecture</a></li><li><a href=#kv-cache-locality>KV Cache Locality</a></li><li><a href=#atlasthread-aware>ATLAS:Thread Aware</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#agentic-program-dag>Agentic Program DAG</a></li><li><a href=#program-aware-scheduling>Program-Aware Scheduling</a></li><li><a href=#autellix-architecture>Autellix Architecture</a></li><li><a href=#kv-cache-locality>KV Cache Locality</a></li><li><a href=#atlasthread-aware>ATLAS:Thread Aware</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2502.13965</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Michael Luo et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-02-20</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2502.13965 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2502.13965 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2502.13965/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current Large Language Model (LLM) serving systems are not optimized for the evolving landscape of dynamic, general-purpose agentic programs. Existing systems often ignore dependencies between programs and calls, leading to missed optimization opportunities and significant waiting times. This results from head-of-line blocking at both the individual LLM request and the program levels. Addressing these issues is critical for enhancing the efficiency and responsiveness of AI agents.</p><p>To tackle these challenges, this paper introduces Autellix, an LLM serving system designed to treat programs as first-class citizens. Autellix intercepts LLM calls and enriches schedulers with program-level context, utilizing two scheduling algorithms: Program-Level Attained Service (PLAS) for single-threaded programs and Adaptive Thread-Level Attained Service (ATLAS) for distributed programs. Evaluation demonstrates that Autellix improves throughput by 4-15x compared to state-of-the-art systems, while maintaining comparable latency.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-63029edeb7e41eea213a57bdae1e01d7></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-63029edeb7e41eea213a57bdae1e01d7",{strings:[" LLM serving systems must evolve to address dependencies and optimize programs. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-8c3750437b95551cd5fea5cafccdb40c></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-8c3750437b95551cd5fea5cafccdb40c",{strings:[" Program-aware scheduling significantly reduces latency and improves throughput. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-4feb049109645e34c7a4d12125e7d751></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-4feb049109645e34c7a4d12125e7d751",{strings:[" Data locality-aware load balancing enhances performance in multi-engine deployments. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper introduces <strong>Autellix, a novel LLM serving system, addressing key challenges in deploying LLM agents</strong>. Its program-aware scheduling and load balancing improve efficiency and reduce latency. As LLM agents become more prevalent, <strong>Autellix provides valuable insights and methods for researchers to optimize their performance and scalability</strong>, paving the way for more efficient and responsive AI systems.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x1.png alt></figure></p><blockquote><p>üîº This figure shows a simple chatbot interaction workflow. The workflow is represented as a sequence diagram, showing the interaction between a human user, the chatbot agent (which uses an LLM internally), and an optional tool. The user inputs a message, the chatbot agent receives the message and uses its internal LLM to generate a response. The chatbot may then optionally interact with a tool to assist in generating the response (e.g., accessing a search engine or database). Finally, the chatbot agent returns its response to the human user. This is a straightforward, sequential process, representing one type of agentic program where each agent‚Äôs activities are fairly simple and directly respond to the user.</p><details><summary>read the caption</summary>(a) Chatbot</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Agentic Program DAG<div id=agentic-program-dag class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#agentic-program-dag aria-label=Anchor>#</a></span></h4><p>Agentic program DAGs represent a shift from static LLM applications to dynamic, general-purpose programs. They model the complex interactions between LLM calls, external tools, and human inputs as a directed acyclic graph, capturing the program&rsquo;s control flow. <strong>This is a crucial abstraction because it acknowledges dependencies between LLM calls, enabling optimization opportunities.</strong> Traditional LLM serving systems, designed for independent requests, often miss these dependencies, leading to suboptimal performance. By treating programs as first-class citizens and understanding their DAG structure, it becomes possible to prioritize and schedule LLM calls more effectively, reducing end-to-end latency and improving overall system throughput. The DAG representation also allows for identifying critical paths and prioritizing calls that contribute most to program completion.</p><h4 class="relative group">Program-Aware Scheduling<div id=program-aware-scheduling class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#program-aware-scheduling aria-label=Anchor>#</a></span></h4><p><strong>Program-aware scheduling</strong> is a critical area in optimizing LLM serving systems. Traditional schedulers often treat LLM requests in isolation, neglecting dependencies and context within programs. This leads to suboptimal performance, as evidenced by head-of-line blocking and inefficient resource utilization. Program-aware scheduling aims to address these limitations by incorporating program-level information, such as dependencies between calls and cumulative execution time, into scheduling decisions. This approach allows for more informed prioritization, preemption, and resource allocation, ultimately reducing end-to-end latency and improving overall system throughput. By understanding the context of each LLM call within its parent program, the scheduler can make more intelligent decisions, leading to improved efficiency and fairness.</p><h4 class="relative group">Autellix Architecture<div id=autellix-architecture class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#autellix-architecture aria-label=Anchor>#</a></span></h4><p>Autellix&rsquo;s architecture centers around a <strong>stateful backend</strong> that contrasts with typical stateless LLM engines. Programs execute from the user&rsquo;s local machine, initiating a session with the Autellix backend and sending LLM calls with session IDs. A <strong>global process table</strong> tracks program metadata like service time and thread information, guiding the scheduler and load balancer. The architecture aims to improve end-to-end latency, maximize GPU utilization, and prevent program starvation. It utilizes a <strong>non-clairvoyant approach</strong>, dynamically building an internal representation of the program&rsquo;s execution graph as it runs, enabling generalization to diverse programs using LLM calls. Figure 8 illustrates the architecture&rsquo;s components, including LLM engines, priority function, memory manager, model executor, and scheduler, working in concert to optimize performance.</p><h4 class="relative group">KV Cache Locality<div id=kv-cache-locality class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#kv-cache-locality aria-label=Anchor>#</a></span></h4><p><strong>KV cache locality</strong> is critical for LLM serving efficiency. <strong>Reusing cached key-value states</strong> across LLM calls reduces redundant computation and speeds up inference. Agentic workloads highlight two key aspects: <strong>intra-program locality</strong> where calls within the same program often share common prefixes (e.g., system prompts, conversation history) leading to high cache hit rates and <strong>inter-program locality</strong> where calls across programs might only share the system prompt leading to lower hit rates. Efficiently exploiting intra-program locality via techniques like prefix caching and shared memory is key. The load balancer is critical to maintain locality, while balancing load across multiple engines. Prioritizing routing request to the right engine depending on program characteristics, it is crucial to improve overall efficiency. For short LLM calls with minimal history, common system prompts may negate the need to maintain data locality.</p><h4 class="relative group">ATLAS:Thread Aware<div id=atlasthread-aware class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#atlasthread-aware aria-label=Anchor>#</a></span></h4><p>While the specific heading &lsquo;ATLAS: Thread Aware&rsquo; might not be directly present, the paper discusses &lsquo;ATLAS (Adaptive Thread-Level Attained Service),&rsquo; an algorithm designed to manage multi-threaded programs, where awareness of individual thread progress and inter-dependencies is crucial. It likely alludes to the algorithm&rsquo;s capability to prioritize LLM calls stemming from critical threads within a program, particularly in dynamic DAGs representing complex agentic tasks. <strong>The core concept is to avoid straggler threads that could delay overall program completion, aligning with the goal of minimizing end-to-end latency.</strong> The ‚Äòthread aware‚Äô aspect indicates that ATLAS doesn&rsquo;t treat all threads equally; instead, it dynamically adapts its scheduling based on the cumulative service time and estimated critical path of each thread. <strong>This contrasts with simpler policies that treat all LLM calls uniformly, regardless of their importance to the overall program&rsquo;s progress.</strong> By accounting for thread-level contributions to the critical path, ATLAS likely enables a more efficient resource allocation and improved program completion times, ensuring that threads essential for rapid progress receive the necessary priority.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x2.png alt></figure></p><blockquote><p>üîº This figure illustrates the execution workflow of a ReAct (Reasoning and Acting) agent, a type of agentic program. The ReAct agent iteratively cycles between generating text with an LLM and interacting with its environment (using tools or human input). Each cycle begins with an LLM call (represented by the rectangle with &lsquo;LLM Agent&rsquo; inside), producing an action or response. The agent then executes an action in the environment (represented by a rectangle with &lsquo;Tool&rsquo;). Subsequent LLM calls build upon the outcome of tool use or human interaction, iterating the process until the task is solved or a termination condition is met. The workflow is dynamic and non-deterministic, as the specific number and order of LLM calls and actions depend on the situation and the LLM&rsquo;s output.</p><details><summary>read the caption</summary>(b) ReAct Agent</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x3.png alt></figure></p><blockquote><p>üîº This figure shows a MapReduce workflow as an example of a multi-threaded agentic program. MapReduce consists of multiple agents working in parallel on sub-tasks and combines the results at the end. The figure illustrates the dynamic nature of the workflow with agents performing LLM calls, accessing tools, and ultimately merging to produce a final result. Each agent&rsquo;s path is depicted as a directed acyclic graph (DAG), showing multiple stages involving calls to LLMs and interactions with external tools.</p><details><summary>read the caption</summary>(c) Map-Reduce</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x4.png alt></figure></p><blockquote><p>üîº This figure illustrates a Monte Carlo Tree Search (MCTS) agentic program. MCTS is a widely used search and planning algorithm where multiple threads are created and used to search the decision space. Each thread represents a sequence of LLM calls and external interrupts (tool calls or human inputs) that can vary over time. The figure shows that multi-threaded programs generally form directed acyclic graphs (DAGs) where the number of threads that fork and merge varies. The complexity of the workflow depends on the number of threads and their respective sequences of LLM calls and interrupts.</p><details><summary>read the caption</summary>(d) Monte Carlo Tree Search</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x5.png alt></figure></p><blockquote><p>üîº This figure illustrates the dynamic nature of agentic programs, which are AI programs that use LLMs to reason and interact with tools and humans. Unlike simple chatbots, these programs don&rsquo;t follow a predetermined flow; instead, their execution path resembles a directed acyclic graph (DAG). The nodes in the DAG represent LLM calls made by one or more LLM agents, while the edges show the flow of execution. The figure shows four examples: (a) a simple chatbot, where interactions are sequential; (b) a ReAct agent, combining reasoning and action; (c) a Map-Reduce agent, performing parallel processing; and (d) a Monte Carlo Tree Search agent, using a tree-like structure for planning and decision making. Each example highlights the highly dynamic nature of these programs, where the sequence and number of LLM calls and interactions (tools or human input) are not fixed beforehand but depend on the specific situation and context.</p><details><summary>read the caption</summary>Figure 1: Execution workflows for Agentic Programs. Agentic programs are highly dynamic execution workflows that follow a directed acyclic graph (DAG). It consists of LLM calls from one or more LLM agents and external interrupts (i.e. tool calls, humans).</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x6.png alt></figure></p><blockquote><p>üîº This figure shows the arrival of four different programs at time t=0. Each program consists of a different number of LLM calls, and the number of decode steps per LLM call also varies among programs. This illustrates the diversity in workloads that an LLM serving engine needs to handle.</p><details><summary>read the caption</summary>(a) Programs, arriving at t=0</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x7.png alt></figure></p><blockquote><p>üîº This Gantt chart visualizes the execution of four programs (A, B, C, D) with varying numbers of LLM calls and decoding steps per call, under a First-Come, First-Served (FCFS) scheduling policy. The chart demonstrates how longer programs (A and B) block the execution of shorter programs (C and D) due to head-of-line (HOL) blocking, resulting in significant waiting times. This illustrates a key performance challenge that the paper addresses: program-level blocking.</p><details><summary>read the caption</summary>(b) First-Come First-Served (FCFS)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x8.png alt></figure></p><blockquote><p>üîº This Gantt chart illustrates the execution of LLM calls from four programs (A, B, C, D) with varying numbers of calls and decode steps per call, under the Multilevel Feedback Queue (MLFQ) scheduling algorithm. MLFQ is a preemptive scheduling algorithm that prioritizes shorter tasks. In this figure, we observe that although MLFQ successfully preempts the long calls from programs A and B to allow the execution of short calls from programs C and D, it still incurs program-level blocking due to repeatedly prioritizing the new calls from programs A and B. This results in a total waiting time of 18 units, same as FCFS, showing that preemption alone without considering program-level context is not enough to solve the head-of-line blocking problem.</p><details><summary>read the caption</summary>(c) Multilevel Feedback Queue (MLFQ)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x9.png alt></figure></p><blockquote><p>üîº This Gantt chart illustrates the execution of LLM calls from four programs (A, B, C, D) with varying lengths and decode steps, using Program-Level Attained Service (PLAS). Unlike First-Come, First-Served (FCFS) and Multilevel Feedback Queue (MLFQ) scheduling, PLAS prioritizes LLM calls based on the program&rsquo;s previously completed calls. This addresses head-of-line blocking at both individual LLM request and program levels, enabling short programs to complete faster. In this example, PLAS prioritizes programs C and D&rsquo;s calls over A and B&rsquo;s resulting in a total wait time of 12 units compared to 18 units for FCFS and MLFQ.</p><details><summary>read the caption</summary>(d) Program-Level Attained Service (PLAS)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x10.png alt></figure></p><blockquote><p>üîº This figure demonstrates how different scheduling algorithms handle LLM call execution within an LLM serving engine. Four programs with varying numbers of LLM calls and decode steps are introduced. (a) shows the program structure. (b) shows the First-Come First-Served (FCFS) algorithm, where long calls block short calls, resulting in 18 units of waiting time. (c) depicts Multilevel Feedback Queue (MLFQ), which reduces blocking through preemption but still suffers from program-level blocking, with a waiting time of 18 units. (d) illustrates Program-Level Attained Service (PLAS), which uses program-level statistics to prioritize calls and reduce waiting time to 12 units.</p><details><summary>read the caption</summary>Figure 2: Gantt chart of LLM call execution on an LLM serving engine with a max batch size (BS) of 2 (Y-axis) over decoding steps (X-axis). (a) Four programs vary in the number of LLM calls and decode steps per call. Long programs (A, B) and short programs (C, D) are shown. (b) First-Come First-Served (FCFS) incurs head-of-line blocking as long LLM calls delay short LLM calls, resulting in a waiting time of 18 units. (c) Multilevel Feedback Queue (MLFQ) reduces blocking with preemption but still incurs program-level blocking. Programs A and B‚Äôs new LLM calls are placed in the highest priority queue, delaying Program D, incurring 18 units of waiting time. (d) Program-Level Attained Service (PLAS) leverages program-level statistics, delaying subsequent calls in A and B to prioritize programs C and D, reducing waiting time to 12 units.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x11.png alt></figure></p><blockquote><p>üîº This figure illustrates the architecture of an AI agent infrastructure, showing how developers and users interact with the system. The top half depicts the high-level interaction, where users create agentic programs (dynamic workflows involving multiple agents, tools and human interaction). These programs orchestrate the execution of tasks, maintaining a global history of actions and outputs. The bottom half shows the LLM serving layer, responsible for processing the LLM calls generated by the agentic programs. This layer manages the calls, routing them across multiple LLM engines if needed for efficiency and scalability. This distribution of workload across engines handles the computational demands of the agentic programs, allowing for more concurrent task execution.</p><details><summary>read the caption</summary>Figure 3: AI Agent Infrastructure. Top: Developers and users build and execute agentic programs that orchestrate execution and persist global, cumulative history across agents, tools, and humans. Bottom: LLM serving systems process agents‚Äô LLM calls and route calls across one or more LLM engines.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x12.png alt></figure></p><blockquote><p>üîº This figure shows the number of LLM calls processed by the LLM serving engine over an hour, comparing the performance of Autellix with that of the baseline system vLLM. The graph clearly indicates that by optimizing for program wait times, Autellix is able to process a significantly higher number of LLM calls within the same timeframe. This demonstrates that reducing wait times not only improves individual program performance but also boosts the overall throughput of the entire LLM serving system.</p><details><summary>read the caption</summary>Figure 4: Number of LLM calls in serving engine during steady state over 1 hour. Optimizing programs‚Äô wait times increases the volume of LLM calls at steady state.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x13.png alt></figure></p><blockquote><p>üîº This figure displays a breakdown of the time spent by different agentic programs (Chatbot, ReAct, MCTS) on program execution versus waiting time, under various system loads. The key takeaway is that under moderate to high system loads, a significant portion of a program&rsquo;s runtime is spent waiting instead of actively executing LLM calls or interacting with other program components. The specific duration of waiting time varies depending on the characteristics of the program itself.</p><details><summary>read the caption</summary>Figure 5: Program execution and wait times, over different programs and system loads. With moderate loads, programs spend the most time waiting. The duration of waiting depends on the workload.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x14.png alt></figure></p><blockquote><p>üîº This figure shows the ratio of waiting time to execution time for LLM calls in a chatbot program as a function of the number of decoding steps. It compares three scheduling policies: First-Come First-Served (FCFS), Multi-Level Feedback Queue (MLFQ), and Autellix. The figure demonstrates that FCFS suffers from significant call-level head-of-line blocking, resulting in long wait times for short LLM calls. MLFQ mitigates this issue through preemption but still experiences significant wait times. Autellix achieves the lowest wait times by effectively reducing both call-level and program-level head-of-line blocking.</p><details><summary>read the caption</summary>(a) Chatbot, LLM Calls</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x15.png alt></figure></p><blockquote><p>üîº This figure shows the ratio of waiting time to execution time for chatbot programs, plotted against the number of LLM calls. It demonstrates the impact of different scheduling algorithms (FCFS, MLFQ, Autellix) on program wait times. Autellix effectively reduces wait times, improving program latency and throughput.</p><details><summary>read the caption</summary>(b) Chatbot, Programs</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x16.png alt></figure></p><blockquote><p>üîº This figure shows a Gantt chart illustrating the execution of LLM calls within a Monte Carlo Tree Search (MCTS) program. The chart specifically focuses on LLM calls, illustrating how call-level head-of-line (HoL) blocking and program-level HoL blocking impact the execution time and waiting time of different calls. The x-axis represents decoding steps, and the y-axis represents different LLM calls, grouped by program. The chart compares three scheduling algorithms: First-Come First-Served (FCFS), Multilevel Feedback Queue (MLFQ), and Autellix&rsquo;s Program-Level Attained Service (PLAS), visualizing the effects of each scheduler on waiting times and execution order.</p><details><summary>read the caption</summary>(c) MCTS, LLM Calls</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x17.png alt></figure></p><blockquote><p>üîº This figure shows a Gantt chart illustrating the execution of LLM calls in Monte Carlo Tree Search (MCTS) programs under different scheduling policies. It highlights how program-level head-of-line (HoL) blocking affects program execution time and latency. The chart compares the performance of the First-Come-First-Served (FCFS) policy, the Multilevel Feedback Queue (MLFQ) algorithm, and the proposed Program-Level Attained Service (PLAS) algorithm. The chart visualizes the waiting and execution times of various LLM calls within multiple MCTS programs, demonstrating how PLAS effectively prioritizes LLM calls to reduce wait times and enhance throughput.</p><details><summary>read the caption</summary>(d) MCTS, Programs</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x18.png alt></figure></p><blockquote><p>üîº This figure displays the ratio of waiting time to execution time for both individual LLM calls and entire programs. It demonstrates the impact of head-of-line (HOL) blocking. HOL blocking occurs when longer calls or programs delay the execution of shorter ones, leading to disproportionately long wait times for the shorter tasks. The figure uses two workloads (Chatbot and MCTS) to illustrate how the ratio of waiting time to execution time changes with the number of LLM calls and the number of decoding steps. It compares the performance of three scheduling policies: First-Come, First-Served (FCFS), Multi-Level Feedback Queue (MLFQ), and Autellix (the proposed system). The results show that Autellix is significantly more effective at reducing HOL blocking and therefore improving wait times compared to the other two scheduling algorithms.</p><details><summary>read the caption</summary>Figure 6: Ratio of Waiting to Execution Time for LLM Calls and Programs. Head-of-line blocking occurs when short LLM calls and programs wait significantly longer than their execution times.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x19.png alt></figure></p><blockquote><p>üîº This figure shows the cache hit rates for LLM calls within and across programs in a single-threaded Chatbot program. It illustrates how LLM calls within the same program tend to share common prefixes and therefore benefit from cache hits (high cache hit rate), while LLM calls across different programs show significantly lower cache hit rates due to a lack of shared context.</p><details><summary>read the caption</summary>(a) Single thread: Chatbot</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x20.png alt></figure></p><blockquote><p>üîº This figure shows the Gantt chart of a Monte Carlo Tree Search (MCTS) program with multiple threads. It illustrates how multiple threads concurrently execute LLM calls and interrupts. Each thread represents a separate sequence of LLM calls and tools calls, and the overall program progresses as these threads interact and merge. The figure demonstrates the dynamic, non-deterministic, and concurrent nature of multi-threaded agentic programs.</p><details><summary>read the caption</summary>(b) Multiple threads: MCTS</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x21.png alt></figure></p><blockquote><p>üîº This figure illustrates the cache hit rates for prefix caching in Large Language Models (LLMs). Prefix caching is a technique to improve efficiency by storing and reusing previously computed results. The figure shows that LLM calls within the same program tend to share a lot of common prefixes, resulting in high cache hit rates. In contrast, LLM calls from different programs typically do not share many common prefixes, leading to lower cache hit rates. This demonstrates the importance of considering the program-level context for optimizing LLM serving systems.</p><details><summary>read the caption</summary>Figure 7: Prefix cache hit rates for LLM calls within and across programs. LLM calls within the same program often share KV cache, whereas LLM calls across programs typically do not.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x22.png alt></figure></p><blockquote><p>üîº Autellix&rsquo;s system architecture consists of a frontend where users run their agentic programs locally. Each program initiates a stateful session with Autellix&rsquo;s backend, which is responsible for managing the lifecycle of LLM calls. The backend uses a global process table to maintain a record of the status of all running programs. This table provides contextual information to Autellix&rsquo;s custom load balancer, which distributes the LLM calls across available LLM engines based on various factors like data locality, and to the scheduler, which prioritizes LLM calls to reduce waiting times and improve throughput.</p><details><summary>read the caption</summary>Figure 8: Autellix‚Äôs system architecture. Users run their programs locally, which initiates a stateful session and submits LLM calls to Autellix‚Äôs backend. Autellix leverages a global process table to track sessions and better inform its custom load-balancer and scheduler.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x23.png alt></figure></p><blockquote><p>üîº This figure illustrates the concept of a critical path in multi-threaded programs, which are represented as directed acyclic graphs (DAGs). The left panel shows an example DAG and highlights the critical path, which is the longest sequence of dependent tasks that determines the overall program execution time. The right panel compares the best-case and worst-case makespans (total execution times) for this particular example DAG. The best-case scenario shows a makespan of 14 units, while the worst-case scenario shows a makespan of 11 units. This difference highlights the impact of task scheduling and resource allocation on the overall efficiency of multi-threaded programs.</p><details><summary>read the caption</summary>Figure 9: Critical path for multi-threaded programs. (Left) Example of a critical path through a DAG. (Right) Best-case scenario makespan, 14 units, versus worst-case makespan. 11 units.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x24.png alt></figure></p><blockquote><p>üîº This figure illustrates the lifecycle of an LLM call within Autellix&rsquo;s program-aware scheduler. The scheduler uses discretized priorities, assigning LLM calls to different queues (Q1, Q2,&mldr;QK) based on their program&rsquo;s priority, rather than using continuous priority values. Calls within a queue are processed in FCFS (First-Come, First-Served) order. A call that exhausts its time quantum in a queue is demoted to a lower-priority queue, while calls waiting too long are promoted to the highest priority queue via anti-starvation mechanisms. This approach ensures efficient processing of calls while preventing starvation of low-priority programs.</p><details><summary>read the caption</summary>Figure 10: LLM call lifecycle based on discretized prioritization.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x25.png alt></figure></p><blockquote><p>üîº Figure 11(a) shows the distribution of the number of prefill and decode tokens used in the ShareGPT dataset. The x-axis represents the number of tokens, and the y-axis represents the density of the token counts. The figure helps to illustrate the characteristics of the ShareGPT dataset, showing a heavy-tailed distribution with a larger number of shorter conversations and a smaller number of very long conversations. This distribution is important because it impacts the performance of LLM serving systems, which must handle a wide range of request sizes efficiently.</p><details><summary>read the caption</summary>(a) ShareGPT</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x26.png alt></figure></p><blockquote><p>üîº This figure (Figure 11(b) in the paper) shows the distribution of prefill and decode tokens for the BFCL (Berkeley Function Calling Leaderboard) workload. The BFCL workload involves multi-turn, multi-step tool usage tasks. The x-axis represents the number of tokens, while the y-axis represents the probability density. The figure demonstrates that BFCL tasks are characterized by long prefill sequences (mean of 735 tokens) compared to relatively short decode sequences (mean of 34 tokens). This is because the prefill phase involves detailed system prompts and tool signatures, while the decoding phase mainly focuses on the agent&rsquo;s responses.</p><details><summary>read the caption</summary>(b) BFCL</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x27.png alt></figure></p><blockquote><p>üîº This figure (c) shows the distribution of the number of LLM calls and the distribution of prefill and decode tokens for the LATS (Large Language Model Agents for Tree Search) workload. LATS involves multi-threaded programs with many parallel LLM calls, resulting in a significant number of LLM calls per program and a higher average number of tokens compared to other workloads (ShareGPT and BFCL). These distributions highlight the computationally intensive and iterative nature of MCTS (Monte Carlo Tree Search) which this workload is based on, presenting significant challenges for LLM serving systems.</p><details><summary>read the caption</summary>(c) LATS</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x28.png alt></figure></p><blockquote><p>üîº This figure is a histogram showing the distribution of the number of LLM calls made by different agentic programs across four different workloads: ShareGPT, BFCL, LATS, and a mixed workload combining all three. The x-axis represents the number of LLM calls, and the y-axis represents the density or frequency of programs with that number of calls. It illustrates the variability in the number of LLM calls required by different agentic programs and the prevalence of long-tailed distributions, where some programs make many more calls than others.</p><details><summary>read the caption</summary>(d) Number of LLM calls</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x29.png alt></figure></p><blockquote><p>üîº Figure 11 presents a detailed analysis of the characteristics of four different agentic program workloads: ShareGPT, BFCL, LATS, and a mixed workload combining all three. The figure is divided into four subfigures. (a), (b), and (c) show the distributions of input and output token lengths for ShareGPT, BFCL, and LATS, respectively. These distributions reveal the varying lengths of prompts and responses in each dataset. (d) displays the distribution of the total number of LLM calls used within each type of program, highlighting the differences in the complexity and scale of the tasks undertaken.</p><details><summary>read the caption</summary>Figure 11: Workload analysis. LLM call statistics of programs from each workload. Input and output length distributions for (a) ShareGPT, (b) BFCL, and (c) LATS. Subfigure (d) plots the distribution of number of LLM calls in each workload.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x30.png alt></figure></p><blockquote><p>üîº This figure displays the average latency (in seconds per token) of four different LLM serving systems (vLLM, vLLM-opt, MLFQ, and Autellix) across four distinct real-world workloads (ShareGPT, BFCL, LATS, and Mixed). The x-axis represents the arrival rate of programs (programs per second), and the y-axis represents the average latency. The figure helps to illustrate the performance gains achieved by Autellix compared to the baseline systems under various workload conditions and arrival rates. It shows that Autellix consistently provides lower latency at higher arrival rates.</p><details><summary>read the caption</summary>Figure 12: Single Engine, Main Results. Average latency for different LLM serving systems across four real-world workloads.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x31.png alt></figure></p><blockquote><p>üîº This figure compares the tail latencies (95th and 99th percentiles) of four different LLM serving systems (vLLM, vLLM-opt, MLFQ, and Autellix) across four real-world workloads (ShareGPT, BFCL, LATS, and Mixed). It shows how the different systems perform under varying arrival rates of programs and highlights the impact of different scheduling strategies on the latency distribution, particularly focusing on how well each system handles extreme latency outliers.</p><details><summary>read the caption</summary>Figure 13: Single Engine, Tail Latencies. 95th (P95) and 99th (P99) percentile latencies of different serving systems.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x32.png alt></figure></p><blockquote><p>üîº This figure compares the performance of different load balancing strategies (Round Robin, Least Used, and Autellix) across multiple LLM engines. It shows the average latency, 95th percentile latency (P95), and 99th percentile latency (P99) for two different workloads (ShareGPT and LATS) and two different model sizes (LLaMA 3.1-8B and LLaMA 3.1-70B). The results demonstrate the effectiveness of Autellix&rsquo;s data locality-aware load balancing in maintaining low average and tail latencies across multiple engines, compared to round-robin and least-used strategies.</p><details><summary>read the caption</summary>Figure 14: Multi-engine, Main Results. Latencies (Avg., P95/99) w.r.t. different load balancing policies.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x33.png alt></figure></p><blockquote><p>üîº This figure demonstrates the scalability of Autellix by showing that as the number of LLM engines (replicas) increases, the maximum program arrival rate that Autellix can handle while maintaining the same service level objective (SLO, measured as seconds per token) also increases linearly. This highlights Autellix&rsquo;s ability to efficiently distribute the workload across multiple engines, ensuring consistent performance even under high loads. The experiment uses the ShareGPT workload to evaluate the performance.</p><details><summary>read the caption</summary>Figure 15: Scalability Experiments. Given same SLO (defined as s/tok), Autellix‚Äôs max arrival rate (program/s) scales linearly w.r.t number of replicas, or LLM engines.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.13965/x34.png alt></figure></p><blockquote><p>üîº This figure shows the results of an experiment comparing the time taken to process a batch of programs offline (without interactive user input). The experiment compares Autellix to other systems (VLLM, vLLM-opt, MLFQ). The y-axis represents the total time (makespan) for the entire batch to complete, and the x-axis represents the number of programs in the batch. The graph demonstrates that Autellix completes the same batch of programs significantly faster than the other systems. This improvement is attributed to Autellix&rsquo;s program-level scheduling and load balancing capabilities which prioritize program completion over processing individual LLM calls.</p><details><summary>read the caption</summary>Figure 16: Offline batch inference. Autellix decreases the time, or makespan, required to process a batch of programs.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-22a4f5034776cdfe1eae2f018b345647 class=gallery><img src=https://ai-paper-reviewer.com/2502.13965/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.13965/17.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13965/&amp;title=Autellix:%20An%20Efficient%20Serving%20Engine%20for%20LLM%20Agents%20as%20General%20Programs" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13965/&amp;text=Autellix:%20An%20Efficient%20Serving%20Engine%20for%20LLM%20Agents%20as%20General%20Programs" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13965/&amp;subject=Autellix:%20An%20Efficient%20Serving%20Engine%20for%20LLM%20Agents%20as%20General%20Programs" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2502.13965/index.md",oid_likes="likes_paper-reviews/2502.13965/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2502.13347/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Craw4LLM: Efficient Web Crawling for LLM Pretraining</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-19T00:00:00+00:00>19 February 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2502.13943/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-19T00:00:00+00:00>19 February 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>