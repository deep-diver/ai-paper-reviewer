[{"figure_path": "https://arxiv.org/html/2502.03621/x1.png", "caption": "Figure 1.  DynVFX augments real-world videos with new dynamic content described via simple user-provided text instruction.", "description": "This figure showcases the DynVFX method's ability to seamlessly integrate user-specified dynamic content into existing real-world videos.  Two examples are presented. In the first, a majestic whale is added to a video's background. The second example shows a puppy's head peeking out from a box.  The input video on the left of each example shows the original scene. The output video on the right illustrates the natural integration of the newly generated dynamic content. The user provides simple text instructions (e.g., \"Add a majestic whale in the background\") to specify the desired additions.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.03621/x2.png", "caption": "Figure 2. Controlling fidelity to the original scene using different extended attention mechanisms. (a-b) SDEdit suffers from the original scene preservation/edit fidelity trade-off. (c-e) Three Extended Attention variants during sampling demonstrate different control levels: Full Extended Attention closely reconstructs the input scene, Masked Extended Attention proves too constrained in overlapping regions despite allowing new content emergence, and our Anchor Extended Attn. achieves optimal results by applying dropout \u2013 extending attention only at sparse points within selected regions.", "description": "Figure 2 illustrates the impact of different extended attention mechanisms on balancing fidelity to the original video scene with the successful integration of newly generated content.  Panels (a) and (b) showcase the limitations of a baseline method (SDEdit), highlighting its inability to simultaneously maintain the integrity of the original scene and effectively incorporate the new elements. The trade-off is apparent: high noise levels lead to poor preservation of the original while low levels restrict the generation of new content. Panels (c), (d), and (e) demonstrate the effectiveness of three variations of an extended attention mechanism during the sampling process. The 'Full Extended Attention' method closely reconstructs the original input scene; 'Masked Extended Attention' shows improvements by allowing the emergence of new content but still exhibits constraints, especially in regions where new and original content overlap.  Finally, the proposed 'Anchor Extended Attention' approach, which selectively applies dropout to the extended attention mechanism, achieves the optimal balance by focusing attention only on a few key regions, ensuring both the integrity of the original scene and natural integration of the new content.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2502.03621/x3.png", "caption": "Figure 3. DynVFX pipeline. Top row: Given an input video \ud835\udcb1origsubscript\ud835\udcb1orig\\mathcal{V}_{\\text{orig}}caligraphic_V start_POSTSUBSCRIPT orig end_POSTSUBSCRIPT, we apply DDIM inversion (see Sec. 3) and extract spatiotemporal keys and values [\ud835\udc0aorig,\ud835\udc15orig]subscript\ud835\udc0aorigsubscript\ud835\udc15orig[\\mathbf{K}_{\\text{orig}},\\mathbf{V}_{\\text{orig}}][ bold_K start_POSTSUBSCRIPT orig end_POSTSUBSCRIPT , bold_V start_POSTSUBSCRIPT orig end_POSTSUBSCRIPT ] from the original noisy latents. Given the user instruction \ud835\udcabVFXsubscript\ud835\udcabVFX\\mathcal{P}_{\\text{VFX}}caligraphic_P start_POSTSUBSCRIPT VFX end_POSTSUBSCRIPT we instruct the VLM to envision the augmented scene and output the text edit prompt \ud835\udcabcompsubscript\ud835\udcabcomp\\mathcal{P}_{\\text{comp}}caligraphic_P start_POSTSUBSCRIPT comp end_POSTSUBSCRIPT, prominent object descriptions \ud835\udcaaorigsubscript\ud835\udcaaorig\\mathcal{O}_{\\text{orig}}caligraphic_O start_POSTSUBSCRIPT orig end_POSTSUBSCRIPT that are used to mask out the extracted keys and values and target object descriptions \ud835\udcaaeditsubscript\ud835\udcaaedit\\mathcal{O}_{\\text{edit}}caligraphic_O start_POSTSUBSCRIPT edit end_POSTSUBSCRIPT. Bottom row: We estimate a residual \ud835\udc99ressubscript\ud835\udc99res\\bm{x}_{\\text{res}}bold_italic_x start_POSTSUBSCRIPT res end_POSTSUBSCRIPT to the original video latent (\ud835\udc99origsubscript\ud835\udc99orig\\bm{x}_{\\text{orig}}bold_italic_x start_POSTSUBSCRIPT orig end_POSTSUBSCRIPT). This is done by iteratively applying SDEdit with our Anchor Extended Attention, segmenting the target objects (\ud835\udcaaeditsubscript\ud835\udcaaedit\\mathcal{O}_{\\text{edit}}caligraphic_O start_POSTSUBSCRIPT edit end_POSTSUBSCRIPT) from the clean result, and updating \ud835\udc99ressubscript\ud835\udc99res\\bm{x}_{\\text{res}}bold_italic_x start_POSTSUBSCRIPT res end_POSTSUBSCRIPT accordingly.", "description": "This figure illustrates the DynVFX pipeline, which consists of two main stages. In the first stage (top row), an input video is processed using DDIM inversion to extract spatiotemporal keys and values from its noisy latents.  A Vision-Language Model (VLM) then receives a user-provided text instruction describing the desired video edit and generates a more detailed text prompt for the edit, along with a description of prominent objects in both the original and the modified video.  The original objects are then used to mask out parts of the extracted keys and values. The second stage (bottom row) involves iteratively using SDEdit with Anchor Extended Attention to estimate a residual to the original video latent. This residual, when added to the original latent, will produce an edited video containing the desired changes. The target objects are segmented from the refined video to further enhance the integration of the new content and to update the residual, refining the edit in subsequent iterations.", "section": "4 Method"}, {"figure_path": "https://arxiv.org/html/2502.03621/x4.png", "caption": "Figure 4. Ablations. (b) Excluding both AnchorExtAttn and the Iterative refinement process results in significant misalignment with the original scene and poor harmonization (e.g., the size of the puppy relative to the scene and boundary artifacts). (c) Omitting AnchorExtAttn leads to incorrect positioning of the new content. (d) Removing iterative refinement results in poor harmonization. Our full method (e) exhibits good localization and harmonization of the edit", "description": "Figure 4 presents an ablation study analyzing the impact of key components in the DynVFX model on video editing quality.  The study compares the results of five scenarios: (a) the original video, (b) the model without Anchor Extended Attention and iterative refinement, (c) the model without Anchor Extended Attention, (d) the model without iterative refinement, and (e) the full DynVFX model. The ablation results demonstrate that both Anchor Extended Attention and iterative refinement are critical for achieving accurate object placement and seamless integration of new content into the existing scene.  Removing either component results in misalignment and poor harmonization of the added content, as exemplified by the disproportionate size of the puppy and boundary artifacts in (b). Incorrect placement of the new content is observed when Anchor Extended Attention is omitted (c), while poor harmonization is seen when iterative refinement is absent (d). Only the full method (e) successfully integrates new content accurately and naturally.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2502.03621/x5.png", "caption": "Figure 5. Sample results of our method. See SM for full vide results.", "description": "This figure showcases several examples of videos enhanced using the DynVFX method.  Each example presents an input video and the corresponding output after applying a text prompt to add dynamic content.  The added content ranges from simple objects like a dog appearing from behind a box to more complex scenarios such as a whale seamlessly incorporated into an ocean scene. The examples demonstrate the system\u2019s capability to integrate new content realistically into existing videos by accounting for factors like perspective, lighting, and object interactions.  The full videos are available in the supplementary materials.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2502.03621/x6.png", "caption": "Figure 6. Qualitative comparison. Sample results of our method, SDEdit (Meng et\u00a0al., 2022), DDIM inversion (Song et\u00a0al., 2020), Lora fine-tuning (Hu et\u00a0al., 2021), and Gen-3 (R Team, Runway, [n.\u2009d.]). See SM for videos.", "description": "Figure 6 presents a qualitative comparison of video editing results obtained using five different methods: the proposed DynVFX approach, SDEdit, DDIM inversion, LoRA fine-tuning, and Gen-3.  Each method was tasked with integrating new dynamic content into existing videos.  The figure shows sample results for three distinct video editing tasks, each involving the addition of a different object or effect.  Visual inspection of the results reveals differences in how effectively each method handles aspects such as object localization, harmonization with the original scene, and overall visual quality.  For a complete comparison including videos, refer to the supplementary material (SM).", "section": "5 Results"}, {"figure_path": "https://arxiv.org/html/2502.03621/x7.png", "caption": "Figure 7. Metrics. We measure CLIP Directional score (higher is better) and masked SSIM (higher is better). Our method demonstrates a better balance between these two metrics.", "description": "Figure 7 presents a comparison of two metrics: CLIP Directional score and masked Structural Similarity Index (SSIM).  A higher score indicates better performance for both metrics.  The CLIP Directional score measures how well the edited video aligns with the intended edit (as described by the text prompt). Masked SSIM assesses the quality of the original video content that remains untouched by the edit. The graph visually shows the performance of different methods.  Our method aims for a balance between these two, and the figure shows it achieves a superior balance compared to other techniques, indicated by its placement on the graph, suggesting a better trade-off between preserving the original scene\u2019s quality while successfully incorporating the desired changes.", "section": "5 Results"}, {"figure_path": "https://arxiv.org/html/2502.03621/x8.png", "caption": "Figure 8. Limitations. In some cases, the T2V diffusion model struggles to precisely follow the edit prompt", "description": "Figure 8 shows examples where the text-to-video (T2V) diffusion model used in the DynVFX system struggles to precisely integrate the user-requested edits into the original video.  Despite the system's attempts at content harmonization and accurate localization, the generated content sometimes doesn't perfectly match the user's instructions, illustrating limitations in the underlying T2V model's ability to precisely control video generation in complex scenes.", "section": "Limitations"}, {"figure_path": "https://arxiv.org/html/2502.03621/x9.png", "caption": "Figure 9. Additional examples for Ablations.", "description": "This figure displays ablation studies, demonstrating the impact of removing key components of the DynVFX method.  The results show the effect of removing Anchor Extended Attention and Iterative Refinement separately and together.  By comparing the output videos across the different ablation experiments (removing Anchor Extended Attention, removing iterative refinement, and removing both), the figure visually demonstrates the contribution of each component to the final result. The comparison highlights the significance of each component in achieving accurate localization, natural content integration, and high-fidelity results in real-world video editing. ", "section": "5 Results"}, {"figure_path": "https://arxiv.org/html/2502.03621/x10.png", "caption": "Figure 10. Comparison to MagicVFX. The result of MagicVFX the output differs significantly from the original video.", "description": "Figure 10 presents a qualitative comparison between the DynVFX method and MagicVFX, focusing on two example edits: adding colorful bubbles and a car on fire. The original video frames are shown alongside the results produced by both methods.  The comparison highlights a key difference: MagicVFX significantly alters the original video's appearance, whereas DynVFX maintains visual fidelity to the source material while seamlessly integrating the generated content. This demonstrates DynVFX's superior ability to preserve the integrity of the original video during the augmentation process.", "section": "5 Results"}, {"figure_path": "https://arxiv.org/html/2502.03621/x11.png", "caption": "Figure 11. Output example for protocol", "description": "This figure shows an example of the DynVFX pipeline's output.  The input is a video of a forest scene.  The user prompt is to add a majestic elephant with large tusks and flapping ears. The system prompt, generated by the Vision Language Model (VLM), provides a detailed description of how the elephant should be integrated into the scene, ensuring natural integration with the existing trees, sunlight, and shadows.  The resulting output video shows the seamlessly integrated elephant, demonstrating the method's ability to generate realistic and harmonious edits.", "section": "4 Method"}, {"figure_path": "https://arxiv.org/html/2502.03621/x12.png", "caption": "Figure 12. VLM instructions used for generating the textual descriptions.", "description": "This figure displays the detailed instructions given to the Vision-Language Model (VLM) for generating the textual descriptions used in the DynVFX pipeline.  These instructions guide the VLM in creating three types of captions: a source scene caption describing the input video without the added content, a VFX conversation simulating a discussion between the VLM and a VFX artist on integrating the new content, and a final composited scene caption describing the video with the added content seamlessly integrated.  The instructions emphasize maintaining the original scene's mood and dynamics while ensuring the new content is realistically integrated into the scene.", "section": "4.1 VLM as a VFX assistant"}, {"figure_path": "https://arxiv.org/html/2502.03621/x13.png", "caption": "Figure 13. VLM evaluation protocol", "description": "This figure details the protocol used for evaluating the perceptual quality of video editing methods using a Vision-Language Model (VLM).  The evaluation involves presenting the VLM with four grids, each containing three frames from the original video and three frames each from different video editing methods. Each method attempts to integrate new content into the source video according to a specified edit prompt. The VLM then assesses four criteria: alignment with the edit prompt, visual quality, content harmonization, and dynamics. For each criterion, the VLM provides a score (0-1), along with a description summarizing the method's perceptual quality. The criteria are defined as follows: 1) Alignment with the edit prompt assesses how well the method integrates the desired content as per the instructions. 2) Visual quality evaluates realism, presence of artifacts, and the coherence of lighting and colors. 3) Content harmonization assesses the natural integration of the added content, with considerations for proportions, depth, perspective, and occlusion consistency with the scene. 4) Dynamics assesses how naturally the added objects move in relation to the original video's camera motion.", "section": "5.2 Quantitative Evaluation"}]