[{"figure_path": "https://arxiv.org/html/2502.20082/extracted/6233605/ruler.png", "caption": "Figure 1: LongRoPE2-extended LLaMA3-8B achieves the best performance at a 128k context length among \u223csimilar-to\\sim\u223c10B models.", "description": "The figure is a graph showing the performance of various large language models (LLMs) on a task requiring long-context understanding.  The x-axis represents the effective context window length (in tokens), and the y-axis represents the accuracy or a similar performance metric on the task.  The graph demonstrates that the LongRoPE2-enhanced LLaMA3-8B model achieves the highest accuracy when using an effective context window of 128k tokens.  Importantly, this performance is achieved while maintaining a model size (around 10B parameters) competitive with other models included in the comparison, showcasing the efficiency of the LongRoPE2 approach.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2502.20082/x1.png", "caption": "Figure 2: (a) RoPE OOD (red area) when extending context length from 2k to 4k. (b) Per-dimensional RoPE rescaling factor from different approaches for extending Phi3-mini from 2k to 128k, all aligning with RoPE OOD theory.\n(c) Performance of Phi3-mini-128k after fine-tuning. Existing methods fail to achieve an effective 128k context length and show noticeable short-context performance drop.", "description": "This figure demonstrates the challenges of extending context windows in LLMs. Panel (a) visually shows the out-of-distribution (OOD) problem of Rotary Positional Embeddings (RoPE) when extending the context length from 2k to 4k tokens.  The red area highlights the OOD regions, which are problematic for model performance. Panel (b) compares the per-dimensional RoPE rescaling factors used by different methods (YaRN, NTK, LongRoPE) to extend the context length of the Phi3-mini model from 2k to 128k tokens.  All methods attempt to align with the RoPE OOD theory to mitigate the OOD issue. Panel (c) shows the performance of the Phi3-mini model extended to 128k tokens using different methods, revealing that existing approaches fail to reach an effective 128k context length while maintaining short-context performance.  There is a clear performance drop in short-context tasks after extending the context length.", "section": "2 Context Window Extension and Challenges"}, {"figure_path": "https://arxiv.org/html/2502.20082/x2.png", "caption": "Figure 3: Sequence length required to span the theoretical period during Phi3-mini pre-training for different RoPE dimensions. Insufficient training in higher RoPE dimensions leads to shorter effective RoPE ranges and longer actual periods.", "description": "This figure visualizes the relationship between RoPE (Rotary Positional Embedding) dimension and the length of sequences needed to complete one full cycle of RoPE's periodic function during the Phi3-mini model's pre-training.  Lower dimensions require shorter sequences to complete a full cycle, indicating sufficient training across multiple cycles within the training data. Conversely, higher dimensions necessitate much longer sequences, exceeding the pre-training context window length. This demonstrates insufficient training in these higher dimensions, leading to incomplete rotation periods within the original training context and shorter effective RoPE ranges than theoretically expected. The discrepancy highlights a critical factor contributing to out-of-distribution issues when extrapolating context windows.", "section": "2.2 ROPE Rescaling Theory"}, {"figure_path": "https://arxiv.org/html/2502.20082/x3.png", "caption": "Figure 4: Scale factors across different RoPE rescaling approaches.", "description": "This figure compares the per-dimension scaling factors applied to the Rotary Position Embeddings (RoPE) in different context window extension methods. The x-axis represents the RoPE dimension index, and the y-axis shows the scaling factor applied to each dimension.  It highlights how LongRoPE2's scaling factors differ from those of YaRN, NTK, and LongRoPE, particularly in the higher dimensions. This difference reflects the unique approach of LongRoPE2 to address the out-of-distribution (OOD) problem in RoPE by adjusting the scaling factor based on the critical dimension, which is dynamically determined through an evolutionary search guided by needle-driven perplexity.", "section": "3 LongRoPE2 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.20082/x4.png", "caption": "Figure 5: Mixed context window training to improve both short and long context capabilities.", "description": "This figure illustrates the mixed context window training approach used to enhance both short and long context capabilities in LLMs.  The left panel (a) shows the training process for shorter context windows, where the original ROPE (Rotary Position Embeddings) is used and attention is masked to prevent cross-document attention. The right panel (b) shows the training process for longer context windows, where the rescaled ROPE is used for full attention across the entire window. This dual training strategy helps maintain performance on short contexts while improving the model's ability to process longer sequences.", "section": "3 LongRoPE2 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.20082/x5.png", "caption": "Figure 6: LongRoPE2 (right) delivers near-perfect lossless performance in the \u201dNeedle in a Haystack\u201d pressure test.", "description": "The Needle in a Haystack pressure test evaluates the ability of a language model to accurately retrieve specific pieces of text (needles) from long documents, assessing its long-context understanding.  This figure compares the performance of LongRoPE2 against another model on this task, showing that LongRoPE2 achieves near-perfect accuracy across various retrieval depths within a 128k context window, demonstrating its ability to maintain performance in long-context scenarios.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2502.20082/x6.png", "caption": "Figure 7: Needle in a Haystack full results for Phi3-mini (3.8B)-128k.", "description": "This figure displays the complete results of the 'Needle in a Haystack' experiment for the Phi3-mini (3.8B) language model with an extended context window of 128K tokens.  The Needle in a Haystack task assesses a language model's ability to locate specific pieces of text (the 'needles') within very long documents, testing its long-context understanding capabilities. The visualization likely shows the accuracy or success rate of the model in retrieving the needles at various depths within the extended document, which indicates how effectively it processes information over long contextual spans.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.20082/x7.png", "caption": "Figure 8: Needle in a Haystack full results for LLaMA3-8B-128k.", "description": "This figure displays the complete results of the \"Needle in a Haystack\" test for the LLaMA3-8B model with its context window extended to 128k tokens using different methods.  The test evaluates the model's ability to accurately retrieve a specific piece of text (\"needle\") from long documents at different depths. Each subplot shows the success rate at various depths (percentage of the total context length) and context lengths using different ROPE methods (YaRN, NTK, LongRoPE, and LongRoPE2).  The color intensity represents the accuracy rate, showing how well each model performs in retrieving the needles across various positions within the long sequence.  The comparison helps to illustrate the effectiveness and limitations of each ROPE method in handling long context windows.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.20082/extracted/6233605/yarn-ntk.png", "caption": "Figure 9: The RoPE rescaling factor distributions of NTK/YaRN adjusted based on the real critical dimension (i.e., YaRN-rcd, NTK-rcd).", "description": "This figure compares the per-dimensional RoPE scaling factors from NTK and YaRN methods, both before and after adjusting for the real critical dimension.  The original NTK and YaRN methods use different scaling strategies across various RoPE dimensions. By incorporating the \"real critical dimension\" (determined empirically), the adjusted methods (YaRN-rcd and NTK-rcd) demonstrate how the recalibration impacts the scaling factor distribution, showing differences in how each method adapts scaling across different dimension ranges.", "section": "2.4 Challenges"}, {"figure_path": "https://arxiv.org/html/2502.20082/x8.png", "caption": "Figure 10: The pseudocode for mixed context window training and inference.", "description": "This figure shows the pseudocode for LongRoPE2's mixed context window training and inference.  The code demonstrates how the model uses different positional embeddings (original RoPE and rescaled RoPE) depending on whether the input sequence length is within the original pre-trained context window or exceeds it. If the input length exceeds the original context window, rescaled RoPE is utilized; otherwise, the original ROPE is used.  The pseudocode also highlights the use of FlashAttention-2 for efficient attention calculation during both training and inference, showing how it handles different sequence lengths using functions like `flash_attn_func` and `flash_attn_varlen_func`.", "section": "3 LongRoPE2 Methodology"}]