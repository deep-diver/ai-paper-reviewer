{"importance": "This paper introduces **a novel meta-RL framework (MRT) for optimizing test-time compute in LLMs**, achieving superior performance & token efficiency in math reasoning. It offers new research directions of exploring reward structures for more efficient LLMs.", "summary": "LLMs can now reason more efficiently!", "takeaways": ["Standard outcome-reward RL fails to optimize test-time compute effectively.", "Meta Reinforcement Fine-Tuning (MRT) significantly improves both performance and token efficiency.", "Iteratively scaling test-time budgets implicitly maximizes progress and minimizes regret."], "tldr": "LLMs improve reasoning using more test-time computation, but **existing fine-tuning methods don't effectively utilize this computation.** Standard outcome-reward RL optimizes for the final result, neglecting intermediate progress, leading to inefficient token use & inability to solve hard problems. This causes unnecessarily long outputs & struggles in making steady progress.\n\nTo address these issues, this paper presents Meta Reinforcement Fine-Tuning (MRT). **MRT minimizes cumulative regret by optimizing a dense reward function** that values progress at each step, alongside the final outcome. This leads to a 2-3x performance gain & 1.5x token efficiency for math reasoning compared to standard RL, offering a new avenue for future explorations.", "affiliation": "Carnegie Mellon University", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "2503.07572/podcast.wav"}