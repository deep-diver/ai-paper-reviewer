{"importance": "This paper is important for researchers, because **it introduces a new open-source multilingual LLM, addressing the gap in language coverage**, and sets a strong foundation for future research in multilingual language modeling. It also sets a new standard for open multilingual LLMs.", "summary": "Babel: An open multilingual LLM supports over 90% of global speakers, filling the language coverage gap and setting new performance standards.", "takeaways": ["Babel, an open multilingual LLM, covers the top 25 languages by speakers, supporting over 90% of the global population.", "Babel enhances performance through layer extension, increasing parameter count rather than continue pretraining.", "Babel achieves superior performance compared to other open LLMs and reaches commercial model levels using supervised fine-tuning."], "tldr": "**Large language models (LLMs) have revolutionized NLP, but open-source multilingual LLMs are scarce, limiting language coverage**. Existing models prioritize well-resourced languages, overlooking widely spoken but under-resourced ones. To bridge this gap and enhance global accessibility, this paper introduces a new open-source multilingual LLM that aims to serve over 90% of speakers worldwide. It focuses on the top 25 languages by speaker numbers, including many languages neglected by existing open-source multilingual LLMs. Additionally, given the limited high-quality training data for many languages, the paper emphasizes optimizing the data-cleaning pipeline to ensure the highest possible data quality. \n\nThis paper **introduces Babel**, which enhances performance through a layer extension technique, increasing its parameter space instead of traditional continue pretraining. Two variants are presented: one designed for efficient inference and fine-tuning, and another setting a new standard for open multilingual LLMs. Evaluations on multilingual tasks show its superior performance compared to open LLMs of comparable size. Moreover, using open-source supervised fine-tuning datasets, Babel achieves remarkable performance, with one variant leading among 10B-sized LLMs and the other setting a new standard for multilingual tasks, rivaling commercial models.", "affiliation": "DAMO Academy, Alibaba Group", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.00865/podcast.wav"}