[{"figure_path": "https://arxiv.org/html/2503.00865/x2.png", "caption": "Figure 1: Layer extension for Babel.", "description": "This figure illustrates the layer extension method used in the Babel model.  The original model's layers are shown on the left.  The layer extension technique adds new layers (shown inserted and appended in the middle), which are identical in structure to the original model's layers. This approach increases the model's parameter count, enhancing its performance without significantly altering its core architecture.  The figure highlights that only layers in the latter half of the model are extended to minimize disruption of the existing layers.", "section": "4 Model Description"}, {"figure_path": "https://arxiv.org/html/2503.00865/x3.png", "caption": "(a) MMMLU", "description": "This figure presents a bar chart comparing the performance of three different 10B-parameter language models (Qwen2.5-7B, Gemma2-9B, and Babel-9B) on the MMMLU benchmark.  The chart displays the average performance across all languages, as well as the performance on high-resource and low-resource languages separately.  It visually demonstrates Babel-9B's superior performance, particularly on low-resource languages, showcasing its strength in multilingual understanding.", "section": "6.1 Experiment Setup"}, {"figure_path": "https://arxiv.org/html/2503.00865/x4.png", "caption": "(b) XNLI", "description": "The figure shows the performance comparison of Babel-9B-Base with other multilingual models on the XNLI (Cross-lingual Natural Language Inference) dataset.  It displays the average performance across high and low resource languages and highlights Babel's improved performance, especially on the low-resource languages, compared to other models such as Qwen2.5-7B and Gemma2-9B.", "section": "7 Further Analysis"}, {"figure_path": "https://arxiv.org/html/2503.00865/x5.png", "caption": "(c) MGSM", "description": "This figure displays the performance comparison of Babel-9B-Base against other multilingual LLMs across the MGSM (Multi-Genre Semantic Matching) dataset.  The chart likely shows performance metrics such as accuracy or F1-score, comparing Babel-9B-Base to models like Qwen2.5-7B and Gemma2-9B.  It specifically highlights how Babel performs across high and low-resource languages within the MGSM dataset.", "section": "6 Evaluations"}, {"figure_path": "https://arxiv.org/html/2503.00865/x6.png", "caption": "Figure 2: Performance of Babel-9B-Base comparison across languages.", "description": "This figure compares the performance of Babel-9B-Base, Qwen2.5-7B, and Gemma2-9B across different languages, categorized into high-resource and low-resource groups.  For each model and language group, the average performance is shown across three benchmark datasets: MMMLU, XNLI, and MGSM, representing world knowledge, natural language inference, and common sense reasoning, respectively. The figure visually demonstrates Babel-9B-Base's performance improvement over other models, particularly with low-resource languages, highlighting its multilingual capabilities.", "section": "7 Further Analysis"}]