{"references": [{" publication_date": "2023", "fullname_first_author": "Ashkenazi, M.", "paper_title": "Nern: Learning neural representations for neural networks", "reason": "This paper is highly relevant because it explores the use of neural representations for neural networks, a concept central to NeuMeta's approach of learning a continuous weight manifold.  Understanding how to effectively represent and learn these representations is crucial for the success of NeuMeta.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Cai, H.", "paper_title": "Once for all: Train one network and specialize it for efficient deployment", "reason": "This paper tackles efficient deployment of neural networks, a key motivation for NeuMeta.  Its exploration of training a single network and specializing it for various configurations is related to NeuMeta's goal of creating adaptable networks without retraining.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Chavan, A.", "paper_title": "Vision transformer slimming: Multi-dimension searching in continuous optimization space", "reason": "This paper focuses on optimizing the efficiency of vision transformers, which are closely related to the general topic of creating efficient and adaptable neural networks. NeuMeta aims to achieve similar goals but uses a different method for weight manifold learning.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Chen, R.T.", "paper_title": "Neural ordinary differential equations", "reason": "This paper introduces neural ordinary differential equations (NODEs), a concept relevant to continuous neural networks, which are closely related to the concept of a continuous weight manifold explored in NeuMeta. NODEs and continuous weight manifolds both represent the concept of continuous learning in neural networks.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "Chen, T.", "paper_title": "Net2net: Accelerating learning via knowledge transfer", "reason": "This work addresses knowledge transfer in neural networks, an area relevant to NeuMeta because NeuMeta aims to learn a weight manifold that allows transferring knowledge between networks of different architectures and sizes. The knowledge transfer mechanism used in this paper is different, but the general concept is relevant and provides some context.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "De Luigi, L.", "paper_title": "Deep learning on implicit neural representations of shapes", "reason": "This work leverages implicit neural representations (INRs) for shape representation and deep learning, a critical component of the proposed NeuMeta framework.  Understanding the strengths and limitations of INRs in this context is crucial for assessing the novelty and feasibility of NeuMeta's INR-based approach.", "section_number": 3}, {" publication_date": "2009", "fullname_first_author": "Deng, J.", "paper_title": "Imagenet: A large-scale hierarchical image database", "reason": "This paper introduces ImageNet, a large-scale dataset used to benchmark the performance of NeuMeta in image classification tasks. ImageNet is a critical component of the experimental validation of NeuMeta, and its properties impact the significance of the experimental results.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Devvrit, F.", "paper_title": "Matformer: Nested transformer for elastic inference", "reason": "This work relates to NeuMeta because it examines elastic inference in neural networks, a concept closely tied to the adaptability and efficiency goals of NeuMeta.  Understanding similar approaches for achieving flexible model sizing is helpful in context.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Ding, X.", "paper_title": "Repvgg: Making vgg-style convnets great again", "reason": "This paper introduces RepVGG, a type of convolutional neural network (CNN) that can be used in image classification tasks.  The architecture of RepVGG is relevant to NeuMeta because the efficiency and flexibility of NeuMeta may be further enhanced by the RepVGG architecture.", "section_number": 4}, {" publication_date": "2010", "fullname_first_author": "Everingham, M.", "paper_title": "The pascal visual object classes (voc) challenge", "reason": "This paper describes the Pascal VOC challenge, a dataset used to evaluate the performance of NeuMeta in semantic segmentation.  The dataset's characteristics and the metrics used to evaluate performance on this dataset are relevant to the experimental section of the paper.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Frankle, J.", "paper_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks", "reason": "This paper introduces the lottery ticket hypothesis, which relates to the idea of finding efficient subnetworks within larger networks.  This concept is related to NeuMeta's approach of learning a weight manifold that can be used to generate efficient subnetworks without retraining.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Glorot, X.", "paper_title": "Understanding the difficulty of training deep feedforward neural networks", "reason": "This work focuses on the fundamental challenges of training deep neural networks, which provides some background for the work of NeuMeta and its approach to creating a continuous weight manifold that helps to address the challenge of training large neural networks.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Grimaldi, M.", "paper_title": "Dynamic convnets on tiny devices via nested sparsity", "reason": "This work focuses on the problem of efficiently running convolutional networks on resource-constrained devices. This directly relates to one of the motivations of NeuMeta, which aims to increase efficiency in model deployment and adaptability to various hardware resources.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "Ha, D.", "paper_title": "Hypernetworks", "reason": "This work introduces hypernetworks, a concept central to NeuMeta.  Hypernetworks are used in NeuMeta to generate weights for different network architectures. Understanding how hypernetworks work is essential to fully grasp NeuMeta's approach.", "section_number": 3}, {" publication_date": "2011", "fullname_first_author": "Hariharan, B.", "paper_title": "Semantic contours from inverse detectors", "reason": "This paper introduces a method for semantic contour detection that is relevant to NeuMeta's semantic segmentation experiments.  The techniques used in this paper and the dataset on which it is evaluated provide context for evaluating NeuMeta's performance in semantic segmentation.", "section_number": 4}, {" publication_date": "2016", "fullname_first_author": "He, K.", "paper_title": "Deep residual learning for image recognition", "reason": "This paper introduces ResNet, a widely used architecture for image classification tasks.  NeuMeta's experiments utilize ResNet, so the properties of ResNet are critical for interpreting and evaluating the results presented in the paper.", "section_number": 4}, {" publication_date": "2015", "fullname_first_author": "He, K.", "paper_title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "reason": "This paper provides some background on image classification architectures that are used in the evaluation of NeuMeta.  Understanding the strengths and weaknesses of various architectures such as ResNets provides context for evaluating NeuMeta's performance.", "section_number": 4}, {" publication_date": "2015", "fullname_first_author": "Hinton, G.", "paper_title": "Distilling the knowledge in a neural network", "reason": "This work addresses knowledge distillation, which is directly relevant to NeuMeta. Knowledge distillation is a method for transferring knowledge from a large model to a smaller model.  This is relevant because NeuMeta implicitly performs a form of knowledge distillation by learning a smooth weight manifold.", "section_number": 3}, {" publication_date": "2009", "fullname_first_author": "Krizhevsky, A.", "paper_title": "Learning multiple layers of features from tiny images", "reason": "This paper introduces the CIFAR datasets, which are used in the experimental evaluation of NeuMeta.  Understanding the properties of these datasets is crucial for interpreting the performance numbers reported in the paper.", "section_number": 4}, {" publication_date": "1991", "fullname_first_author": "Krogh, A.", "paper_title": "A simple weight decay can improve generalization", "reason": "This paper discusses weight decay regularization, a technique used in NeuMeta to improve the generalization performance of the model.  Understanding the theoretical basis and practical implications of weight decay is helpful in evaluating the success of NeuMeta.", "section_number": 4}]}