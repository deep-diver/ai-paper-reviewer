[{"figure_path": "2410.11878/figures/figures_6_0.png", "caption": "Fig. 2: Diagram of NeuMeta and our content organization.", "description": "The figure illustrates the pipeline of Neural Metamorphosis, showing the process of transforming a trained neural network into a smoothed network, training an implicit neural representation (INR), sampling weights from the INR, and generating weights for a target model.", "section": "3.2 Network Architecture"}, {"figure_path": "2410.11878/figures/figures_8_0.png", "caption": "Fig. 3: Intra-model smoothness via permutation equivalence. Our approach involves permuting weights to minimize total variance within each neural clique graph, thereby enhancing global smoothness.", "description": "The figure illustrates how intra-model smoothness is achieved by permuting weights within neural network cliques to minimize total variance, enhancing overall smoothness.", "section": "3.3 Maintaining Manifold Smoothness"}, {"figure_path": "2410.11878/figures/figures_9_0.png", "caption": "Fig. 4: Cross-model smoothness via coordinate perturbation. Unlike the predict weights in discrete grid (Left), our INR predicts weight as the expectation within a small neighborhood (Right).", "description": "Figure 4 illustrates the contrast between discrete weight prediction in a grid (left) versus the continuous weight prediction as an expectation over a small neighborhood by INR (right).", "section": "3.3 Maintaining Manifold Smoothness"}, {"figure_path": "2410.11878/figures/figures_12_0.png", "caption": "Fig. 5: Accuracy comparison of NeuMeta versus different structure pruning methods on MNIST, CIFAR10, CIFAR100 and ImageNet. Our method consistently outperforms pruning-based methods. R18 and R20 are short for ResNet18 and ResNet20.", "description": "Figure 5 shows the accuracy comparison of NeuMeta against various structure pruning methods across four datasets (MNIST, CIFAR10, CIFAR100, and ImageNet) at different compression ratios.", "section": "4.2 Enhancing Efficiency by Morphing the Networks"}, {"figure_path": "2410.11878/figures/figures_13_0.png", "caption": "Fig. 6: VAE Visualizations on MNIST and CelebA Datasets on the same compress rate. Lower NLL and MSE indicates better performance.", "description": "The figure shows the visualization results of generated images by VAE on MNIST and CelebA datasets with 25% compression rate using NeuMeta and L1 25% pruning methods, indicating NeuMeta's superior performance with lower MSE and NLL values.", "section": "4.2 Enhancing Efficiency by Morphing the Networks"}, {"figure_path": "2410.11878/figures/figures_13_1.png", "caption": "Fig. 6: VAE Visualizations on MNIST and CelebA Datasets on the same compress rate. Lower NLL and MSE indicates better performance.", "description": "The figure shows the visualization of generated images by VAE on MNIST and CelebA datasets using NeuMeta with 25% compression for MNIST and 50% for CelebA, comparing the results with L1 pruning-based method, showing superior image generation quality of NeuMeta with lower MSE and NLL values.", "section": "4.2 Enhancing Efficiency by Morphing the Networks"}]