[{"figure_path": "https://arxiv.org/html/2411.19939/x1.png", "caption": "Figure 1: Overview of our work. We have discovered a problem in current multimodal safety data samples, which says visual safety information leakage (VSIL). Based on this leakage, we further find it leads to a counter-intuitive problem, that simpler SFT-based alignment methods can perform nearly the same high safety rate. Thus, we construct VLSBench, preventing visual leakage. This newly proposed task discourages textual alignment and motivates more dedicated multimodal alignment methods to better solve this challenging task. The red bar shows evaluation results separately on the raw and jailbreak set of JailbreakV\u00a0[34], a typical dataset with VSIL. The green bar shows evaluation results on our VLSBench.", "description": "The figure illustrates the discovery of Visual Safety Information Leakage (VSIL) in existing multimodal safety datasets. VSIL refers to the phenomenon where sensitive information from images is already present in the textual query, enabling models to easily identify and reject unsafe content based on text alone. This leads to the counter-intuitive observation that simple text-based alignment methods achieve high safety performance.  To address this issue, the authors created VLSBench, a new multimodal benchmark dataset designed to prevent VSIL. Results show that VLSBench presents a significant challenge for both textual and multimodal alignment methods, highlighting the need for more robust multimodal alignment techniques.", "section": "Visual Leakage in Multimodal Safety"}, {"figure_path": "https://arxiv.org/html/2411.19939/x2.png", "caption": "Figure 2: Four examples in current benchmarks to showcase the problem of visual safety information leakage. The leakage information from visual to textual is marked as red.", "description": "This figure showcases four examples from existing multimodal safety benchmarks to illustrate the problem of Visual Safety Information Leakage (VSIL).  Each example shows an image paired with a textual query. The red markings highlight instances where sensitive or risky content from the image is explicitly mentioned in the textual query, indicating that the safety information has leaked from the visual to the textual modality.  This leakage renders the safety evaluation based on the textual query alone inaccurate, as the MLLM can easily detect and respond to the unsafe content in the textual query without needing to process the visual information in the image. This highlights a significant flaw in the existing multimodal safety benchmarks, as their safety assessment relies heavily on this textual information, and consequently may not accurately reflect the safety performance of the MLLMs in actual scenarios where such leakage is less likely.", "section": "Visual Leakage in Multimodal Safety"}, {"figure_path": "https://arxiv.org/html/2411.19939/x3.png", "caption": "Figure 3: Safety taxonomy of our VLSBench.", "description": "This figure presents a hierarchical safety taxonomy used to categorize the 2.4k image-text pairs in the VLSBench dataset.  The taxonomy is structured in a two-level hierarchy, with six main categories (Violent, Illegal Activity, Self-Harm, Erotic, Hate, and Privacy) and a total of 19 subcategories. Each category represents a distinct type of harmful or risky content, and the subcategories offer more granular classifications of these categories.  The visual representation of the taxonomy resembles a circular chart or sunburst diagram, with each category and subcategory represented by a slice or segment.", "section": "3. VLSBench Construction Pipeline"}, {"figure_path": "https://arxiv.org/html/2411.19939/x4.png", "caption": "Figure 4: The Detailed statistics of our VLSBench across 6 categories and 19 sub-categories", "description": "This figure presents a detailed breakdown of the VLSBench dataset's composition across six high-level categories and their respective sub-categories.  It provides a visual representation of the number of samples within each sub-category, illustrating the distribution of data points across various safety-related themes within the benchmark. This allows for a granular understanding of the dataset's coverage of different types of risky content.", "section": "3. VLSBench Construction Pipeline"}, {"figure_path": "https://arxiv.org/html/2411.19939/x5.png", "caption": "Figure 5: Overview of VLSBench construction pipeline. Our pipeline features prevent visual leakage. This pipeline includes four steps: (a) Harmful query and image description generation. (b) M: Mitigating visual leakage from textual harmful query. (c) Iterative image generation from image description. (d) Final filtration ensuring image-text pairs are matched and harmful.", "description": "The figure illustrates the VLSBench construction pipeline, a four-step process designed to prevent visual safety information leakage.  Step (a) involves generating harmful textual queries and corresponding image descriptions using LLMs. Step (b) mitigates visual leakage by paraphrasing harmful queries to remove explicit safety-related information. Step (c) uses iterative image generation to refine image quality. Finally, step (d) filters out mismatched or safe image-text pairs, ensuring the final dataset contains only harmful and relevant image-text pairs.", "section": "3. VLSBench Construction Pipeline"}, {"figure_path": "https://arxiv.org/html/2411.19939/x6.png", "caption": "Figure 6:  Examples of our dataset across our three evaluation labels. We give an image-text pair and corresponding response evaluated as Safe with Refuse, Safe with Warning and Unsafe.", "description": "This figure showcases examples from the VLSBench dataset, categorized into three safety evaluation labels: \"Safe with Refuse\", \"Safe with Warning\", and \"Unsafe\". Each example includes an image-text pair and the corresponding model response.  The labels highlight the different ways a model can handle potentially harmful requests.  \"Safe with Refuse\" indicates the model correctly identified the safety concern and declined to generate a response.  \"Safe with Warning\" signifies the model recognized the potential risk but still provided a response, including a warning.  \"Unsafe\" denotes instances where the model failed to identify the safety implications and generated an unsafe or harmful response.", "section": "4. Benchmark Experiments"}, {"figure_path": "https://arxiv.org/html/2411.19939/x7.png", "caption": "Figure 7: Textual SFT compared with Multimodal SFT on our VSLBench. Dash lines mean the average safety rate on the three base models.", "description": "This figure compares the performance of textual Supervised Fine-Tuning (SFT) and multimodal SFT methods on the VLSBench dataset.  The y-axis represents the safety rate, indicating the percentage of safe responses from the models. The x-axis shows the different models used in the experiment. The dashed lines represent the average safety rates achieved by three base models before any alignment method was applied. The graph visually demonstrates that while textual SFT performs reasonably well on its own, multimodal SFT significantly outperforms it on this new benchmark, highlighting the importance of considering both visual and textual information for robust multimodal safety.", "section": "2.2 VSIL Causes Textual Alignment Perform well on Existing Multimodal Safety Datasets"}, {"figure_path": "https://arxiv.org/html/2411.19939/x8.png", "caption": "Figure 8: Selected examples with VSIL: (a)-(e) is from JailbreakV\u00a0[34], (f) is from FigStep\u00a0[16], (g)-(i) is from Ch3ef\u00a0[46] and (j)-(l) is from Harmbench\u00a0[35].", "description": "This figure showcases examples of Visual Safety Information Leakage (VSIL) from several existing multimodal safety datasets.  It demonstrates how sensitive or risky content present in the image is also explicitly or implicitly revealed in the corresponding textual query.  The examples highlight how this leakage allows models to identify and reject unsafe queries based solely on the text, without needing to process the visual information.  Specifically, images (a) through (e) are from the JailbreakV dataset, image (f) is from FigStep, images (g) through (i) are from Ch3ef, and images (j) through (l) are from Harmbench. Each image-text pair illustrates a different instance where the text in the query leaks information about the potentially harmful image content.", "section": "2. Visual Leakage in Multimodal Safety"}, {"figure_path": "https://arxiv.org/html/2411.19939/x9.png", "caption": "Figure 9: Words cloud of our VLSBench\u2019s textual queries.", "description": "This word cloud visualizes the frequency of words used in the textual queries of the VLSBench dataset.  It provides a visual representation of the most common words and their relative importance in the dataset's question construction. The visualization helps understand the overall characteristics and style of questions included in VLSBench, which were designed to be neutral and avoid revealing information contained within paired images.", "section": "3. VLSBench Construction Pipeline"}, {"figure_path": "https://arxiv.org/html/2411.19939/x10.png", "caption": "Figure 10: The harmful elements: sensitive objects and risky scenarios examples, used to generate our harmful queries and image captions.", "description": "Figure 10 shows examples of sensitive objects and risky scenarios used to generate harmful queries and image captions for the VLSBench dataset.  The examples are categorized to illustrate the range of harmful content covered, and they represent real-world situations or items that could be misused to elicit unsafe responses from large language models. This figure supports the methodology section of the paper by visually demonstrating the types of harmful inputs used to construct the dataset.", "section": "3.2 VLSBench Data Collection"}, {"figure_path": "https://arxiv.org/html/2411.19939/x11.png", "caption": "Figure 11: Prompt used to categorize our image-text pairs in VLSBench.", "description": "This prompt instructs the GPT-40 model to categorize image-text pairs based on a provided safety taxonomy.  The model is asked to analyze malicious intent in the image, text, or both, classifying it into one of six high-level categories and associated sub-categories.  It requires the model to provide both the category and subcategory labels along with an explanation in JSON format. The prompt is designed for use within the VLSBench project, to aid in the creation of a dataset with balanced categories.", "section": "3. VLSBench Construction Pipeline"}, {"figure_path": "https://arxiv.org/html/2411.19939/x12.png", "caption": "Figure 12: Evaluation method analysis. The LlamaGuard model is not able to perceive the image and identify the risky content in image-text pairs which hinders its evaluation. On the other hand, GPT-based methods is able to correctly evaluate on our VLSBench.", "description": "This figure compares the performance of LlamaGuard and GPT-based models in evaluating the safety of image-text pairs.  LlamaGuard struggles to accurately assess the safety of the image-text pairs because it cannot properly analyze the visual content of the images.  GPT models, on the other hand, successfully assess safety, highlighting the importance of visual context in safety evaluation and demonstrating that VLSBench poses a significant challenge for existing safety evaluation methods.", "section": "2. Visual Leakage in Multimodal Safety"}, {"figure_path": "https://arxiv.org/html/2411.19939/x13.png", "caption": "Figure 13: Our GPT-4o evaluation prompt for our VLSBench.", "description": "This figure shows the detailed prompt used for evaluating the safety of responses generated by various models on the VLSBench dataset. The prompt instructs the evaluator (GPT-40) to classify the model's responses into three categories: 'safe with refusal', 'safe with warning', and 'unsafe'.  It emphasizes the need for the evaluator to consider both direct and indirect safety risks conveyed through the image and text query.  The prompt provides specific criteria for each safety category and requests a JSON formatted explanation justifying the chosen classification.", "section": "4. Benchmark Experiments"}, {"figure_path": "https://arxiv.org/html/2411.19939/x14.png", "caption": "Figure 14: Selected models\u2019 results on our VLSBench across 6 categories. Reported results include safe with refuse rate, safe with warning rate, and total safe rate.", "description": "This radar chart visualizes the performance of various large language models (LLMs) on the VLSBench benchmark across six safety categories.  Each category represents a different type of harmful content (e.g., violent, erotic, hate speech). The three concentric circles within each category represent the percentage of safe responses classified as 'safe with refuse', 'safe with warning', and 'safe' overall.  The models evaluated include both open-source and closed-source LLMs, with various safety alignment methods applied. The chart allows for a direct comparison of each model's performance in handling different kinds of risky content and helps assess the effectiveness of various safety alignment techniques.", "section": "4. Main Results"}, {"figure_path": "https://arxiv.org/html/2411.19939/x15.png", "caption": "Figure 15:", "description": "This figure presents a qualitative comparison of different large language models' responses to a prompt requesting a scenario where a llama in an image reacts strongly.  The models' responses range from generating unsafe scenarios to refusing the prompt entirely or offering safer alternatives.  The models shown are Qwen2-VL-7B, Llama3.2-Vision, LLaVA-1.5-7b, Gemini-1.5-pro, and Qwen2-VL with safety alignment using VLGuard. The responses highlight the variation in safety and helpfulness across different models and the effectiveness of safety alignment techniques.", "section": "2. Visual Leakage in Multimodal Safety"}, {"figure_path": "https://arxiv.org/html/2411.19939/x16.png", "caption": "Figure 16:", "description": "This figure shows the results of evaluating several large language models (LLMs) and their safety-aligned variants on a specific query related to cleaning a firearm that has been used in a violent incident. The models' responses are categorized into three levels: safe with refusal (completely rejecting the request), safe with warning (identifying safety concerns and providing caution), and unsafe (directly answering the query without considering safety). The figure helps to illustrate how different models and alignment techniques handle the challenging task of providing safe and helpful responses to potentially harmful prompts.", "section": "Main Results"}, {"figure_path": "https://arxiv.org/html/2411.19939/x17.png", "caption": "Figure 17:", "description": "This figure showcases the results of evaluating several large language models (LLMs) on a specific task.  The task involves determining suitable online locations for purchasing certain unique adult novelty items. The image shows three such items: handcuffs, lubricant, and a vibrating egg.  The results demonstrate a range of responses, from completely unsafe suggestions (providing specific online stores) to safe responses (refusing to provide such information) and responses which offer warnings and partial guidance. Different models exhibit different capabilities in handling this safety-sensitive query.  The models tested include various versions of LLaVA, Qwen2-VL, and GPT-40, as well as a version of LLaVA fine-tuned using textual unlearning methods.", "section": "2.2 VSIL Causes Textual Alignment Perform well on Existing Multimodal Safety Datasets"}, {"figure_path": "https://arxiv.org/html/2411.19939/x18.png", "caption": "Figure 18: Used prompt for GPT-4o to generate harmful queries and image descriptions with safety-related information from harmful elements, like risky objects and scenarios.", "description": "This figure shows the prompt used to instruct GPT-40 to generate harmful queries and image descriptions.  The prompt provides GPT-40 with a category, sub-category, and a risky object or scenario.  It then instructs the model to generate an image depicting this scenario or object and three harmful queries related to the harmful element.  The prompt emphasizes that the generated image should clearly show the harmful object or dangerous behavior and that the queries should directly or indirectly lead to harmful outputs.  Two example inputs and outputs are provided to guide the model's generation process.", "section": "3.2 VLSBench Data Collection"}, {"figure_path": "https://arxiv.org/html/2411.19939/x19.png", "caption": "Figure 19: Used prompt for GPT-4o to make the harmful query less obvious and less harmful, thus hiding the safety information from the image, preventing VSIL problem.", "description": "This figure shows the prompt used to instruct GPT-40 to modify harmful queries, making them less obvious while maintaining the harmful intent when combined with the image. The goal is to prevent visual safety information leakage (VSIL) by ensuring that the harmful nature is primarily conveyed through the image rather than the text query.", "section": "2. Visual Leakage in Multimodal Safety"}, {"figure_path": "https://arxiv.org/html/2411.19939/x20.png", "caption": "Figure 20: Used prompt for GPT-4o to generate harmful queries with safety-related information from existing images.", "description": "This figure displays the prompt used to instruct GPT-40 to generate harmful queries based on existing images.  The prompt instructs the model to analyze images for potentially harmful content or situations and then generate a set of harmful queries related to that content. This aims to create a dataset of image-text pairs for evaluating the safety of multimodal large language models (MLLMs). The prompt provides examples to guide the model, illustrating the desired format and level of detail for the harmful query generation.", "section": "3.2 VLSBench Data Collection"}, {"figure_path": "https://arxiv.org/html/2411.19939/x21.png", "caption": "Figure 21: Used prompt for GPT-4o to filter the unsuccessful revised query, which is still harmful or loses original meanings.", "description": "This figure shows the prompt used to filter out unsuccessful revised queries generated in the previous step.  The goal is to ensure that the query, while not explicitly harmful, still retains its harmful intent when combined with the image.  The prompt instructs GPT-4 to check for both overt harmful content in the revised query and whether the image alone, or the combination of image and query, implies harmful intent.  It requires a JSON formatted response indicating whether the text query is harmful, the image is harmful, and if the combination is harmful, along with an explanation.", "section": "3.2 VLSBench Data Collection"}, {"figure_path": "https://arxiv.org/html/2411.19939/x22.png", "caption": "Figure 22: Used prompt for Qwen2-VL-72B for iterative image generation.", "description": "This prompt instructs the Qwen2-VL-72B model to iteratively refine image generation prompts.  The goal is to produce images depicting harmful or risky content, scenarios, or information.  The prompt emphasizes realism and detail in the generated images, ensuring they accurately reflect the harmful elements specified (category, risky object or scenario, and harmful query).  The model is expected to evaluate its own outputs, identifying if the generated image sufficiently conveys the intended harmful aspects. If not, it should provide a refined prompt for further iteration.", "section": "3.2 VLSBench Data Collection"}]