[{"content": "| Dataset | R<sub>1</sub>(%) | R<sub>2</sub>(%) | R<sub>1</sub>-R<sub>2</sub>(%) |\n|---|---|---|---|\n| FigStep | 54.80 | 54.40 | 0.40 |\n| JailbreakV-raw | 65.36 | 65.00 | 0.36 |\n| Harmbench-mm | 30.91 | 30.00 | 0.91 |\n| VLSafe | 91.35 | 91.08 | 0.27 |", "caption": "Table 1: Harmful evaluation experiment on multimodal safety datasets, evaluated by LlamaGuard3-11B-Vision. The results are shown in percentages (%). For JailbreakV, we use its miniset and choose the raw query for evaluation.", "description": "This table presents the results of a harmful evaluation experiment conducted on four multimodal safety datasets: FigStep, JailbreakV, Harmbench-mm, and VLSafe.  The evaluation was performed using LlamaGuard3-11B-Vision, a safety judge model. The table shows the percentage of harmful image-text pairs in each dataset, calculated using two different metrics (R1 and R2). R1 represents the overall harmful rate, while R2 indicates the rate at which both the image and text are individually harmful.  The small difference between R1 and R2 suggests that the harmful content in the images is largely reflected in the textual queries. For the JailbreakV dataset, only the miniset and raw queries were used in the evaluation.", "section": "2. Visual Leakage in Multimodal Safety"}, {"content": "| Category | Samples | Ratio(%) |\n|---|---|---|\n| **Violent** | 590 | 26.33 |\n| \u2022 Weapon-Related Violence | 244 | 10.07 |\n| \u2022 Public Violence and Rioting | 186 | 8.30 |\n| \u2022 Abuse and Physical Altercations | 160 | 7.14 |\n| **Illegal Activity** | 539 | 24.05 |\n| \u2022 Cybercrime | 226 | 10.07 |\n| \u2022 Property Crimes | 167 | 7.54 |\n| \u2022 Drug-Related Offenses | 115 | 5.13 |\n| \u2022 Human Trafficking and Exploitation | 28 | 1.25 |\n| **Self-Harm** | 370 | 16.51 |\n| \u2022 Risky or Dangerous Behaviors | 125 | 5.58 |\n| \u2022 Physical Self-Injury | 124 | 5.53 |\n| \u2022 Substance Abuse and Poisoning | 91 | 4.06 |\n| \u2022 Psychological and Disordered Harm | 32 | 1.43 |\n| **Erotic** | 343 | 15.31 |\n| \u2022 Adult Content | 178 | 7.94 |\n| \u2022 Erotic Visual Content | 165 | 7.36 |\n| **Hate** | 269 | 12.00 |\n| \u2022 Racial and Ethnic Discrimination | 99 | 4.42 |\n| \u2022 Cultural Xenophobia | 62 | 2.77 |\n| \u2022 Religious Intolerance | 56 | 2.50 |\n| \u2022 Gender and Sexual Orientation Discrimination | 50 | 2.23 |\n| **Privacy** | 130 | 5.80 |\n| \u2022 Unauthorized Data Collection | 69 | 2.08 |\n| \u2022 Identity Theft and Impersonation | 64 | 2.86 |", "caption": "Table 2: Multimodal safety and general ability comparison experiment between textual SFT and multimodal alignment. We have conducted our experiment on three MLLMs, LLaVA-v1.5-7b/13b\u00a0[30] and Qwen2-VL-7B\u00a0[51]. blue background marks the textual method that we apply. We leverage safety rate (%) as our multimodal safety metrics. For general ability, we adopt the official metrics used. Per. is short for perception. Cog. is short for cognition. \u2191\u2191\\mathbf{\\uparrow}\u2191 indicates that the higher, the better.", "description": "This table presents a comparative analysis of the performance of textual Supervised Fine-Tuning (SFT) against multimodal alignment methods in both safety and general ability tasks. Three large language models (LLMs) were used: LLaVA-v1.5-7b, LLaVA-v1.5-13b, and Qwen2-VL-7B.  The table displays safety rates (%), along with metrics for various general abilities, such as perception and cognition, using officially recognized evaluation metrics.  Textual SFT methods are highlighted with a blue background. Higher values indicate better performance.", "section": "2.2 VSIL Causes Textual Alignment Perform well on Existing Multimodal Safety Datasets"}, {"content": "| Baselines | Training Method | Learning Rate | Epochs | Max Length |\n|---|---|---|---|---|\n| LLaVA-v1.5-7B + Textual SFT | full-parameter | 2e-5 | 2 | 2048 |\n| LLaVA-v1.5-7B + Textual Unlearning | full-parameter | 2e-5 | 3 | 2048 |\n| LLaVA-v1.5-13B + Textual SFT | full-parameter | 2e-5 | 2 | 2048 |\n| Qwen2-VL-7B + Textual SFT | full-parameter | 1e-5 | 2 | 1024 |\n| Qwen2-VL-7B + Multimodal SFT | full-parameter | 1e-5 | 3 | 1024 |", "caption": "Table 3: Results of our VLSBench, including open-sourced MLLMs, closed-sourced APIs, textual alignment, and multimodal alignment baselines. The results are evaluated with GPT-4o with three labels: safe with refuse, safe with warning, and unsafe. The final safety rate is the sum of the refusal rate and the warning rate.", "description": "Table 3 presents the safety performance evaluation results on the VLSBench dataset.  It compares various large language models (LLMs), including both open-source and closed-source models, and evaluates their performance using three different alignment methods: textual SFT, multimodal SFT, and a combined approach. The evaluation metric is the safety rate, calculated as the sum of 'safe with refuse' and 'safe with warning' rates, determined by the GPT-40 model, which assigns one of three labels to each model's response: 'safe with refuse,' 'safe with warning,' and 'unsafe'. The table helps to understand the relative effectiveness of different LLM architectures and alignment techniques in handling safety concerns, specifically within the context of the VLSBench, which is designed to avoid visual safety information leakage (VSIL).", "section": "4. Main Results"}, {"content": "| Models | Text-only | Stable-Diffusion | Typo | Stable-Diffusion+Typo | Average \u2191 |\n|---|---|---|---|---|---| \n| **Base Models** |  |  |  |  |  |\n| LLaVA-v1.5-7B | 46.25 | 45.24 | 19.70 | 20.11 | 32.82 |\n| LLaVA-v1.5-13B | 52.98 | 44.52 | 20.36 | 21.01 | 34.72 |\n| Qwen2-VL-7B | 61.79 | 49.46 | 27.80 | 22.86 | 40.48 |\n| **Safety Aligned** |  |  |  |  |  |\n| LLaVA-v1.5-7B + VLGaurd | 74.88 | 89.16 | 95.17 | 90.89 | 87.53 |\n|  + SPA-VL-DPO | 67.14 | 76.31 | 67.38 | 63.69 | 68.63 |\n|  + SPA-VL-DPO | 68.93 | 79.40 | 75.95 | 70.36 | 73.66 |\n|  + Textual-Unlearning | 59.29 | 65.77 | 56.90 | 45.77 | 56.93 |\n|  + Textual-SFT | 67.44 | 67.14 | 62.38 | 52.85 | 62.45 |\n| LLaVA-v1.5-13B + VLGaurd | 75.71 | 90.65 | 94.76 | 90.95 | 88.02 |\n|  + Textual-SFT | 71.67 | 64.82 | 55.18 | 52.14 | 60.95 |\n| Qwen2-VL-7B + VLGaurd | 97.02 | 91.72 | 95.95 | 93.27 | 94.49 |\n|  + Textual-SFT | 71.07 | 80.29 | 74.40 | 75.35 | 75.28 |", "caption": "Table 4: Detailed training settings of our textual SFT with SafeRLHF\u00a0[22] and Multimodal SFT with VLGuard\u00a0[67].", "description": "This table details the training hyperparameters used for both textual and multimodal safety alignment methods in the paper.  For the textual safety alignment, it shows the settings used when fine-tuning with the SafeRLHF dataset [22]. This includes the optimizer, learning rate, number of training epochs, and maximum sequence length.  Similarly, for the multimodal safety alignment, it presents the training hyperparameters employed when fine-tuning with the VLGuard dataset [67], again specifying the optimizer, learning rate, epochs, and maximum sequence length. This allows a comparison of training strategies used for each approach.", "section": "4. Experiment Setup"}, {"content": "| Type | Image Source | Nums. | Ratio(%) |\n|---|---|---|---| \n| Generated | Stable-Diffusion [13] | 1234 | 62.96% |\n| Existing | MULTITrust [63] | 175 | 8.93% |\n|  | MLLMGuard [18] | 178 | 9.08% |\n|  | Ch3ef [46] | 117 | 5.97% |\n|  | UnsafeBench [40] | 130 | 6.63% |\n|  | CoCo [28] | 126 | 6.43% |", "caption": "Table 5: Textual alignment compared with multimodal alignment on MMSafetyBench\u00a0[33].", "description": "This table presents a comparison of the performance of textual alignment methods (using only text for model alignment) versus multimodal alignment methods (using both text and images) on the MMSafetyBench dataset [33].  It shows the safety performance of different models across various aspects of the benchmark, including \"Text-only,\" \"Stable-Diffusion,\" \"Typo,\" and \"Stable-Diffusion+Typo.\" The results demonstrate the relative effectiveness of each alignment strategy in mitigating safety risks within multimodal language models.", "section": "2.2 VSIL Causes Textual Alignment Perform well on Existing Multimodal Safety Datasets"}, {"content": "| Category | Description |\n|---|---| \n| **Violent** | Behaviors involving physical force or threats with intent to harm, injure, or intimidate, including assaults, public violence, and the use of weapons. |\n| \u2022 **Weapon-Related Violence** | Use of firearms, knives, or improvised weapons in dangerous situations |\n| \u2022 **Public Violence and Rioting** | Street fights, violent protests, and aggressive acts at public events. |\n| \u2022 **Abuse and Physical Altercations** | Unprovoked attacks, group fights, intimate partner violence, child and elder abuse and intimidation through physical force. |\n| **Illegal Activity** | Behaviors that violate laws or regulations, including cybercrimes, drug-related offenses, human trafficking, and property crimes. |\n| \u2022 **Cybercrime** | Hacking, phishing, and unauthorized access to digital systems. |\n| \u2022 **Property Crimes** | Theft, vandalism, and destruction of property without consent. |\n| \u2022 **Drug-Related Offenses** | Production, trafficking, and possession of controlled substances. |\n| \u2022 **Human Trafficking and Exploitation** | Forced labor, sex trafficking, and illegal organ trade. |\n| **Self-Harm** | Behaviors involving intentional injury or neglect directed at oneself, including physical injury, substance misuse, and extreme risk-taking. |\n| \u2022 **Risky or Dangerous Behaviors** | Reckless driving, extreme sports without safety precautions, and self-neglect. |\n| \u2022 **Physical Self-Injury** | Cutting, burning, hitting oneself, or other forms of direct physical harm. |\n| \u2022 **Substance Abuse and Poisoning** | Overuse of medication, drug misuse, and intentional poisoning. |\n| \u2022 **Psychological and Disordered Harm** | Isolation from social support, restrictive eating, binge-purge cycles, and emotional suppression. |\n| **Erotic** | Content and interactions focused on explicit or suggestive sexual scenarios, including visual media and adult content. |\n| \u2022 **Adult Content** | Explicit sexual content including pornography and other adult material. |\n| \u2022 **Erotic Visual Content** | Content that entices and attracts through visual appeal, often using seductive elements. |\n| **Hate** | Hostile or discriminatory actions and language targeted at individuals or groups based on race, religion, gender, or ethnicity. |\n| \u2022 **Racial and Ethnic Discrimination** | Harassment, exclusion, and stereotyping based on race or ethnicity. |\n| \u2022 **Cultural Xenophobia** | Stereotyping, exclusion, or hostility toward specific cultural groups or national origins. |\n| \u2022 **Religious Intolerance** | Desecration of religious sites, mockery of practices, and exclusion due to religion. |\n| \u2022 **Gender and Sexual Orientation Discrimination** | Harassment, exclusion, and discrimination based on gender identity or sexual orientation. |\n| **Privacy** | Actions that involve unauthorized access, collection, or exposure of personal information, often with intent to exploit or misuse data. |\n| \u2022 **Unauthorized Data Collection** | Surveillance, GPS tracking, and data scraping without consent. |\n| \u2022 **Identity Theft and Impersonation** | Misuse of personal details to assume another\u2019s identity in fraudulent activities. |", "caption": "Table 6: Image source of our VLSBench including generated images and collected images from existing datasets.", "description": "Table 6 details the sources of images used in the VLSBench dataset.  It shows that a significant portion (62.96%) of the images were generated using Stable Diffusion, while the remaining images were sourced from four other existing datasets: MULTITrust, MLLMGuard, Ch3ef, and UnsafeBench, as well as CoCo.  The table indicates the number of images from each source and their corresponding percentage of the total dataset.", "section": "3. VLSBench Construction Pipeline"}]