<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information? &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information? &#183; HF Daily Paper Reviews by AI"><meta name=description content="AV-Odyssey Bench reveals that current multimodal LLMs struggle with basic audio-visual understanding, prompting the development of a comprehensive benchmark for more effective evaluation."><meta name=keywords content="Multimodal Learning,Multimodal Understanding,üè¢ CUHK MMLab,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02611/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02611/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?"><meta property="og:description" content="AV-Odyssey Bench reveals that current multimodal LLMs struggle with basic audio-visual understanding, prompting the development of a comprehensive benchmark for more effective evaluation."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-03T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-03T00:00:00+00:00"><meta property="article:tag" content="Multimodal Learning"><meta property="article:tag" content="Multimodal Understanding"><meta property="article:tag" content="üè¢ CUHK MMLab"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02611/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02611/cover.png"><meta name=twitter:title content="AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?"><meta name=twitter:description content="AV-Odyssey Bench reveals that current multimodal LLMs struggle with basic audio-visual understanding, prompting the development of a comprehensive benchmark for more effective evaluation."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?","headline":"AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?","abstract":"AV-Odyssey Bench reveals that current multimodal LLMs struggle with basic audio-visual understanding, prompting the development of a comprehensive benchmark for more effective evaluation.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.02611\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2024","dateCreated":"2024-12-03T00:00:00\u002b00:00","datePublished":"2024-12-03T00:00:00\u002b00:00","dateModified":"2024-12-03T00:00:00\u002b00:00","keywords":["Multimodal Learning","Multimodal Understanding","üè¢ CUHK MMLab"],"mainEntityOfPage":"true","wordCount":"5843"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-31</p></a><a href=/ai-paper-reviewer/2025-04-01/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-04-01</p></a><a href=/ai-paper-reviewer/2025-04-02/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-04-02</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-31</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-04-01/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-04-01</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-04-02/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-04-02</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.02611/cover_hu8990755592927101593.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.02611/>AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-03T00:00:00+00:00>3 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>5843 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">28 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.02611/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.02611/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-understanding/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Understanding
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-cuhk-mmlab/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ CUHK MMLab</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#multimodal-llm-limits>Multimodal LLM Limits</a></li><li><a href=#av-odyssey-bench>AV-Odyssey Bench</a></li><li><a href=#deaftest-results>DeafTest Results</a></li><li><a href=#audio-visual-int>Audio-Visual Int.</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#multimodal-llm-limits>Multimodal LLM Limits</a></li><li><a href=#av-odyssey-bench>AV-Odyssey Bench</a></li><li><a href=#deaftest-results>DeafTest Results</a></li><li><a href=#audio-visual-int>Audio-Visual Int.</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.02611</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Kaixiong Gong et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2024-12-04</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.02611 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.02611 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/av-odyssey-bench-can-your-multimodal-llms target=_self role=button>‚Üó Papers with Code</a></p><audio controls><source src=https://ai-paper-reviewer.com/2412.02611/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current multimodal large language models (MLLMs) show impressive performance in many areas but struggle with basic audio-visual understanding, as highlighted by a new test called DeafTest. This test revealed that even advanced MLLMs struggle with simple tasks such as identifying louder or higher-pitched sounds, revealing a critical gap in their audio processing capabilities.</p><p>To address this issue, the researchers introduced AV-Odyssey Bench, a comprehensive benchmark containing 4,555 carefully designed questions involving text, images, and audio. This benchmark challenges MLLMs to integrate information from all three modalities to accurately answer questions. The results from AV-Odyssey show that even state-of-the-art models underperform significantly. This signifies a need for more advanced models and datasets focused on robust audio-visual integration.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-f2cacd244d279bdb0423d04614db78cf></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-f2cacd244d279bdb0423d04614db78cf",{strings:[" Multimodal LLMs often fail at simple audio-visual tasks humans find trivial. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-93d91d33d3d4ab320d68f45cc59b445b></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-93d91d33d3d4ab320d68f45cc59b445b",{strings:[" AV-Odyssey Bench provides a more thorough benchmark for evaluating multimodal LLMs' understanding of audio-visual information. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-0a3984935dcee9ee935cc0ee1ff4e0bb></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-0a3984935dcee9ee935cc0ee1ff4e0bb",{strings:[" DeafTest effectively highlights fundamental listening limitations in current LLMs. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial for <strong>multimodal LLM research</strong> because it reveals significant limitations in current models&rsquo; ability to understand audio-visual information. It introduces a novel benchmark, <strong>AV-Odyssey</strong>, for more comprehensive evaluation and paves the way for future dataset creation and model development that better integrate audio-visual cues. The <strong>DeafTest</strong>, used to evaluate basic listening abilities, also serves as a critical tool for highlighting fundamental limitations. This work addresses <strong>a crucial gap in the field</strong>, setting a new standard for evaluating multimodal models.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x1.png alt></figure></p><blockquote><p>üîº DeafTest is an evaluation benchmark consisting of four simple audio tasks designed to assess the fundamental audio understanding capabilities of multimodal large language models (MLLMs). Figure 1 showcases two of these tasks: loudness comparison and pitch comparison. In the loudness comparison task, the MLLM is presented with two audio clips and asked to identify which is louder. The pitch comparison task involves determining which of two audio clips has a higher pitch. These tasks assess the basic audio processing abilities of MLLMs before more complex reasoning is required, helping to determine if the model can truly &lsquo;hear&rsquo; and interpret simple auditory information.</p><details><summary>read the caption</summary>Figure 1: Illustration of two out of four DeafTest tasks. Loudness comparison is used to determine the louder sound of two given sounds. Pitch comparison is to determine which sound has the higher pitch.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>Sound Counting</th><th>Loudness Comparison</th><th>Pitch Comparison</th><th>Duration Comparison</th></tr></thead><tbody><tr><td>Random</td><td>50.0</td><td>50.0</td><td>50.0</td><td>50.0</td></tr><tr><td>Gemini 1.5 Flash [70]</td><td>55.0</td><td>62.0</td><td>54.0</td><td>89.0</td></tr><tr><td>Gemini 1.5 Flash-8B [70]</td><td>49.0</td><td>55.0</td><td>51.0</td><td>51.0</td></tr><tr><td>Gemini 1.5 Pro [70]</td><td>81.0</td><td>60.0</td><td>52.0</td><td>84.0</td></tr><tr><td>Reka Core [71]</td><td>54.0</td><td>43.0</td><td>42.0</td><td>40.0</td></tr><tr><td>Reka Flash [71]</td><td>48.0</td><td>58.0</td><td>51.0</td><td>44.0</td></tr><tr><td>Reka Edge [71]</td><td>47.0</td><td>56.0</td><td>50.0</td><td>44.0</td></tr><tr><td>GPT-4o audio-preview [27]</td><td>50.0</td><td>58.0</td><td>58.0</td><td>57.0</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of four basic auditory tasks from the DeafTest, designed to evaluate the fundamental listening abilities of multimodal large language models (MLLMs). The tasks assess the models&rsquo; performance on simple auditory discriminations, including sound counting, loudness comparison, pitch comparison, and duration comparison. Each task is a two-choice question, meaning the model must select one of two options. The random baseline performance for these two-choice questions is 50%, providing a context for evaluating the model&rsquo;s actual performance. The table shows the performance of several MLLMs (Gemini 1.5 Flash, Gemini 1.5 Flash-8B, Gemini 1.5 Pro, Reka Core, Reka Flash, Reka Edge, and GPT-40 Audio-preview) on each of the four tasks, expressed as percentages. This allows for a direct comparison of how well these models perform on these basic audio processing tasks compared to random chance.</p><details><summary>read the caption</summary>Table 1: Results on four basic auditory tasks (DeafTest). The questions are designed as two-choice questions. The random baseline performance is 50%.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Multimodal LLM Limits<div id=multimodal-llm-limits class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multimodal-llm-limits aria-label=Anchor>#</a></span></h4><p>Multimodal LLMs, while showing promise, reveal significant limitations in truly understanding audio-visual information. <strong>DeafTest</strong>, a benchmark focusing on fundamental auditory tasks, highlights these models&rsquo; struggles with simple sound discrimination (loudness, pitch, duration), suggesting a core deficiency in basic audio processing. This is further supported by the <strong>AV-Odyssey Benchmark</strong>, which shows that even complex, multi-modal tasks are not accurately solved. The results indicate a shallow understanding of audio-visual relationships. Models often fail to correctly integrate audio cues, even in scenarios where visual information is abundant. Therefore, <strong>current multimodal LLMs primarily demonstrate surface-level pattern recognition rather than deep semantic understanding</strong> of audio-visual content. Further research and improved datasets are crucial to bridge this gap and develop models with improved audio-visual reasoning abilities.</p><h4 class="relative group">AV-Odyssey Bench<div id=av-odyssey-bench class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#av-odyssey-bench aria-label=Anchor>#</a></span></h4><p>The AV-Odyssey Bench is a <strong>comprehensive benchmark</strong> designed to rigorously evaluate the true audio-visual understanding capabilities of Multimodal Large Language Models (MLLMs). It addresses limitations of existing benchmarks by incorporating <strong>diverse audio attributes</strong>, <strong>extensive domains</strong>, and <strong>interleaved audio-visual inputs</strong>. The benchmark&rsquo;s design goes beyond simple pattern recognition and necessitates the models to truly integrate clues from both visual and audio streams for accurate inference. This focus makes the AV-Odyssey Bench a <strong>critical tool</strong> for evaluating progress in MLLM development, providing valuable insights for dataset creation and model improvement by focusing on the often-overlooked aspects of fundamental audio-visual processing.</p><h4 class="relative group">DeafTest Results<div id=deaftest-results class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#deaftest-results aria-label=Anchor>#</a></span></h4><p>A hypothetical &lsquo;DeafTest Results&rsquo; section would present a crucial analysis of basic audio comprehension in multimodal large language models (MLLMs). The results would likely reveal significant shortcomings, demonstrating that <strong>even simple auditory tasks, such as distinguishing loudness or pitch, pose considerable challenges</strong> for these advanced models. This finding would be particularly insightful because it highlights a foundational weakness: while MLLMs may excel at complex reasoning, their <strong>ability to process fundamental audio features is unexpectedly weak.</strong> The low accuracy rates across various tasks would underscore the need for improved training data and model architectures that better integrate and utilize low-level auditory information. A detailed breakdown by task, model, and metric would further enhance understanding of where these models currently fall short, and <strong>suggest specific areas of development for future model improvement.</strong> The contrast between human performance (near-perfect) and MLLM performance (significantly lower) would strongly emphasize the need for more robust evaluation benchmarks. The section would also, therefore, suggest avenues for future research to bridge this gap in audio understanding.</p><h4 class="relative group">Audio-Visual Int.<div id=audio-visual-int class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#audio-visual-int aria-label=Anchor>#</a></span></h4><p>The heading &lsquo;Audio-Visual Int.&rsquo; likely refers to the integration and interplay of audio and visual information within a multimodal model. A thoughtful exploration would examine how these modalities are <strong>fused</strong>, the challenges of <strong>multimodal alignment</strong> (matching audio events to visual elements), and the potential for <strong>emergent capabilities</strong> arising from this interaction. <strong>Data limitations</strong> and the biases introduced by the training datasets would also be critical areas to investigate. Crucially, an in-depth analysis needs to consider whether the model truly understands the combined meaning or just performs pattern recognition; hence, the effectiveness of its <strong>reasoning abilities</strong> in audio-visual scenarios becomes central to the discussion. It&rsquo;s important to address whether the <strong>basic listening skills</strong> are sufficient to underpin high-level audio-visual understanding.</p><h4 class="relative group">Future Work<div id=future-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-work aria-label=Anchor>#</a></span></h4><p>Future work should prioritize <strong>improving the foundational audio understanding capabilities of MLLMs</strong>. Addressing the limitations revealed by DeafTest is crucial before tackling more complex audio-visual reasoning tasks. This involves exploring new training methodologies that emphasize low-level auditory feature extraction and integration. <strong>Developing more comprehensive and nuanced audio-visual datasets</strong> is also essential, particularly focusing on diverse audio attributes and scenarios to improve generalizability and robustness. Research into effective methods for <strong>multi-modal information fusion</strong> is critical, investigating novel architectures and training strategies that facilitate seamless interaction and mutual enhancement between audio and visual streams. Finally, <strong>more rigorous benchmark evaluation</strong> methods are needed, potentially incorporating human evaluation to ground the assessment in human perception and understanding. This multi-pronged approach will advance MLLM capabilities towards true audio-visual comprehension.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x2.png alt></figure></p><blockquote><p>üîº Figure 2 illustrates the AV-Odyssey Benchmark, a comprehensive evaluation suite for multimodal large language models (MLLMs). The figure highlights three key aspects of the benchmark: 1) <strong>Comprehensive Audio Attributes</strong>: It assesses MLLMs&rsquo; understanding of various sound characteristics, including timbre, tone, space, melody, hallucination, time, and intricacy. 2) <strong>Extensive Domains</strong>: The benchmark covers a wide range of audio-visual scenarios from daily life to more specialized domains like music, making it robust and generalizable. 3) <strong>Interleaved Text, Audio, and Images</strong>: The benchmark presents problems that require models to integrate information from text, audio, and visual inputs simultaneously, mirroring real-world complexities. This design ensures that the MLLMs truly understand audio-visual information, and doesn&rsquo;t just rely on superficial pattern recognition.</p><details><summary>read the caption</summary>Figure 2: Overview of AV-Odyssey Benchmark. AV-Odyssey Bench demonstrates three major features: 1. Comprehensive Audio Attributes; 2. Extensive Domains; 3. Interleaved Text, Audio, and Images.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x3.png alt></figure></p><blockquote><p>üîº This figure provides a visual overview of the 26 evaluation tasks included in the AV-Odyssey benchmark. These tasks are categorized into seven main classes based on the prominent audio attributes they assess: Timbre, Tone, Melody, Space, Time, Intricacy, and Hallucination. The figure uses a circular layout to display the various tasks within each category, making it easy to see the breadth and depth of the benchmark&rsquo;s coverage of different audio-visual scenarios. Each task assesses a unique aspect of multimodal understanding, requiring models to integrate information from both audio and visual modalities in order to arrive at the correct answer.</p><details><summary>read the caption</summary>Figure 3: Overview of 26 evaluation tasks of AV-Odyssey Benchmark. We mainly categorize these tasks with the sound attributed into 7 classes.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x4.png alt></figure></p><blockquote><p>üîº Figure 4 presents example questions from the AV-Odyssey benchmark dataset. Each example showcases a different task from the benchmark, highlighting its multi-modal nature (text, image/video, and audio). The questions require models to integrate information from all modalities to provide a correct answer. This figure illustrates the diversity of tasks and complexity present in the AV-Odyssey benchmark, which tests multimodal large language models&rsquo; ability to understand and reason using audio-visual information.</p><details><summary>read the caption</summary>Figure 4: Sampled examples from our AV-Odyssey Benchmark.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x5.png alt></figure></p><blockquote><p>üîº This figure shows a pie chart that breaks down the 104 human-annotated errors made by Gemini 1.5 Pro on the AV-Odyssey benchmark. The errors are categorized into four main types: Audio Understanding (63%), Vision Understanding (10%), Text Understanding (8%), and Reasoning (13%). The remaining 6% of errors fall into the &lsquo;Other&rsquo; category.</p><details><summary>read the caption</summary>Figure 5: Distribution of 104 human-annotated errors in the Gemini 1.5 Pro.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x6.png alt></figure></p><blockquote><p>üîº The figure shows an example where a model misidentifies the audio content. Specifically, the model incorrectly labels a lion&rsquo;s roar as an elephant trumpeting sound. This highlights the model&rsquo;s limitations in accurately understanding and classifying audio information, demonstrating an audio understanding error.</p><details><summary>read the caption</summary>Figure 6: An example of audio understanding error. More examples are provided in the Appendix.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x7.png alt></figure></p><blockquote><p>üîº The figure shows a multiple-choice question where the model is asked to identify which instrument best matches an audio clip of keyboard music. The correct answer is the keyboard (C), but the model incorrectly chose the vibraphone (D), demonstrating a failure in audio understanding. The model focused on the timbre and resonance, incorrectly identifying them with a vibraphone instead of the keyboard.</p><details><summary>read the caption</summary>Figure 7: A sampled error case in the instrument recognition task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x8.png alt></figure></p><blockquote><p>üîº This figure shows a sample error from the singer recognition task in the AV-Odyssey benchmark. The task required the model to identify the singer based on their vocal timbre in an audio clip and choose from four images of different singers. The model incorrectly identified the singer in the audio as Billie Eilish, when it was actually Rihanna. This highlights the model&rsquo;s limitation in accurately identifying singers based solely on vocal timbre, even in simple scenarios. The image provides the audio clip, the options to choose from, the model&rsquo;s incorrect response and the correct answer.</p><details><summary>read the caption</summary>Figure 8: A sampled error case in the singer recognition task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x9.png alt></figure></p><blockquote><p>üîº The figure shows a multiple choice question in which the model is asked to identify which image best corresponds to the sound of gunfire. The correct answer is an image depicting a soldier firing a gun, while the model incorrectly chooses an image of a machine gun. This highlights the model&rsquo;s difficulty distinguishing between the sound of different types of gunfire, emphasizing the complexity of audio-visual tasks.</p><details><summary>read the caption</summary>Figure 9: A sampled error case in the gunshot recognition task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x10.png alt></figure></p><blockquote><p>üîº The figure showcases a sample error from the bird recognition task within the AV-Odyssey benchmark. It highlights a multimodal large language model&rsquo;s (MLLM) failure to correctly identify both the visual (bird species) and audio (bird sounds) components. The model incorrectly identifies a common grackle as a Brewer&rsquo;s Blackbird and subsequently mismatches the bird sound, illustrating the challenges faced by MLLMs in accurately integrating audio-visual information for complex tasks.</p><details><summary>read the caption</summary>Figure 10: A sampled error case in the bird recognition task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x11.png alt></figure></p><blockquote><p>üîº This figure shows an example where the model incorrectly identifies the sound of a frog as a cat&rsquo;s meow while correctly identifying the image as a cat. This highlights the model&rsquo;s struggles in accurately associating audio with the correct visual element and demonstrates a failure in audio recognition.</p><details><summary>read the caption</summary>Figure 11: A sampled error case in the animal recognition task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x12.png alt></figure></p><blockquote><p>üîº This figure shows a sample error case from the transportation recognition task within the AV-Odyssey benchmark. The model incorrectly identified the sound of an airplane as a motorcycle sound, despite correctly identifying the image of a motorcycle. This highlights a failure in audio understanding, where the model misinterprets the audio despite accurate visual recognition.</p><details><summary>read the caption</summary>Figure 12: A sampled error case in the transportation recognition task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x13.png alt></figure></p><blockquote><p>üîº This figure shows a multiple-choice question from the AV-Odyssey benchmark&rsquo;s material recognition task. The question asks the model to identify which of four materials (shown in images) is most likely to produce the sound of someone stepping or hitting on fallen leaves (played in an audio clip). The model incorrectly answers, highlighting a potential text understanding error. The model&rsquo;s response suggests it misunderstood the question, focusing on identifying the source image of the audio rather than identifying the correct material based on the audio. The correct answer is an image depicting a leaf-littered path.</p><details><summary>read the caption</summary>Figure 13: A sampled error case in the material recognition task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x14.png alt></figure></p><blockquote><p>üîº The figure shows an example where Gemini 1.5 Pro misidentified the sound of traffic as that of a subway train. The model correctly identified the image content showing a street scene but failed to accurately understand the audio. This highlights the model&rsquo;s difficulty in accurately associating sounds with visual scenes, a key challenge in audio-visual comprehension tasks.</p><details><summary>read the caption</summary>Figure 14: A sampled error case in the scene recognition task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x15.png alt></figure></p><blockquote><p>üîº The figure showcases a sample error from the hazard recognition task within the AV-Odyssey benchmark. It visually presents the question, the model&rsquo;s incorrect answer, the correct answer, and a detailed breakdown of the error&rsquo;s cause. The question involves identifying the image depicting a hazard that aligns with the audio clip of a fire. The model misinterprets the sound of fire burning as the sound of boiling water, illustrating a flaw in its audio understanding capabilities and highlights the complexity of audio-visual comprehension tasks.</p><details><summary>read the caption</summary>Figure 15: A sampled error case in the hazard recognition task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x16.png alt></figure></p><blockquote><p>üîº The figure shows an example where a multimodal large language model (MLLM) fails to correctly identify the action in a video based on the corresponding audio. The model incorrectly identifies the audio of someone running on a treadmill as the sound of playing basketball.</p><details><summary>read the caption</summary>Figure 16: A sampled error case in the action recognition task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x17.png alt></figure></p><blockquote><p>üîº The figure shows an example where the Gemini 1.5 Pro model misidentifies the sound of eating juicy grapes as the sound of eating crispy chips. The model correctly identifies the image (grapes), but incorrectly identifies the audio. This highlights a limitation in audio understanding within the model, specifically in distinguishing between similar sounds with different textures.</p><details><summary>read the caption</summary>Figure 17: A sampled error case in the eating sound recognition task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x18.png alt></figure></p><blockquote><p>üîº This figure shows a case where the model incorrectly identifies the emotion conveyed in an audio clip. The task is to match the audio (an angry voice) to one of four images representing different emotions. The model incorrectly selects an image depicting disgust, demonstrating a failure in accurately interpreting audio-based emotional cues. The image shows four options; a woman showing disgust, a man showing surprise, an eggplant emoji showing anger, and a sad face emoji showing sadness. The model chose the image of a woman with a disgusted face, even though the audio was of an angry voice.</p><details><summary>read the caption</summary>Figure 18: A sampled error case in the speech sentiment analysis task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x19.png alt></figure></p><blockquote><p>üîº The figure shows an example where the model (Gemini 1.5 Pro) failed to answer a question about a meme because the content was mistakenly flagged for security reasons. The question asked about the humor in a meme given an audio clip and a sequence of images. Gemini 1.5 Pro was unable to provide any answer. The correct answer involved the contrast between the calm audio and the cat&rsquo;s expressionless face in the meme images. This highlights the model&rsquo;s limitations in handling sensitive content and its inability to fully understand the nuances of humor in multimodal contexts.</p><details><summary>read the caption</summary>Figure 19: A sampled error case in the meme understanding task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x20.png alt></figure></p><blockquote><p>üîº This figure shows a case where the model incorrectly identifies the sentiment of cheerful music as sad. The model correctly identified the visual content of the image (a crying emoji face), but failed in audio recognition, highlighting its limitations in accurately understanding musical emotions.</p><details><summary>read the caption</summary>Figure 20: A sampled error case in the music sentiment analysis task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x21.png alt></figure></p><blockquote><p>üîº Gemini 1.5 Pro incorrectly classified the audio as country music instead of classical music, despite accurately identifying the image content. This highlights the model&rsquo;s limitations in audio understanding and genre classification.</p><details><summary>read the caption</summary>Figure 21: A sampled error case in the music genre classification task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x22.png alt></figure></p><blockquote><p>üîº This figure shows a case where the Gemini 1.5 Pro model failed to correctly identify the audio that best matches the dance in a video. The task was to select the audio clip that most accurately corresponds to the style and rhythm of the dance shown. The model failed to answer, likely due to limitations in the model&rsquo;s ability to integrate visual and audio cues to make complex decisions about audio-visual synchronicity. The model&rsquo;s failure to answer highlights the challenges of multimodal understanding, even in relatively simple tasks.</p><details><summary>read the caption</summary>Figure 22: A sampled error case in the dance and music matching task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x23.png alt></figure></p><blockquote><p>üîº The figure shows an example where the Gemini 1.5 Pro model incorrectly matches a fast-paced, cheerful music clip with a scene from an action movie. The model fails to recognize that the humorous tone of the audio, indicated by comical screams, is more characteristic of a comedy than an action film.</p><details><summary>read the caption</summary>Figure 23: A sampled error case in the film and music matching task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x24.png alt></figure></p><blockquote><p>üîº This figure showcases a case where Gemini 1.5 Pro misidentifies a music score. The audio features slow-paced music with a sustained vocal at the end. The model incorrectly identifies the audio as moderately paced with a swing feel and syncopated rhythm, leading to a mismatched score selection. The error highlights the model&rsquo;s limitations in accurately interpreting tempo, articulation, and the interplay of rhythmic and melodic elements in music.</p><details><summary>read the caption</summary>Figure 24: A sampled error case in the music score matching task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x25.png alt></figure></p><blockquote><p>üîº This figure shows a sample error case in the audio 3D angle estimation task of the AV-Odyssey benchmark. The task involves estimating the azimuth and elevation angles of a sound source relative to a person in an image. The model incorrectly identifies the person and misinterprets spatial audio cues, leading to an inaccurate angle estimation. The correct and predicted answers are shown, highlighting the model&rsquo;s inability to properly integrate visual and audio information for spatial reasoning.</p><details><summary>read the caption</summary>Figure 25: A sampled error case in the audio 3D angle estimation task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x26.png alt></figure></p><blockquote><p>üîº The figure shows an example where the model fails to accurately estimate the distance of a sound source using audio and visual cues. The model correctly identifies the visual elements but fails to integrate the spatial audio information from the 4-channel spatial audio recording, leading to an inaccurate distance estimation. This highlights the model&rsquo;s limitations in multi-modal reasoning and its reliance on visual cues over more precise spatial audio information.</p><details><summary>read the caption</summary>Figure 26: A sampled error case in the audio distance estimation task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x27.png alt></figure></p><blockquote><p>üîº The figure shows a sample error from the audio time estimation task of the AV-Odyssey benchmark. The task requires identifying the start and end times of an action in a video based solely on an accompanying audio clip. The example highlights a model&rsquo;s misidentification of the correct timeframe for a specific action (putting utensils in a drawer). The model incorrectly identified the timeframe based on the audio, demonstrating limitations in precise temporal alignment between audio and visual inputs.</p><details><summary>read the caption</summary>Figure 27: A sampled error case in the audio time estimation task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x28.png alt></figure></p><blockquote><p>üîº The figure shows an example where a multimodal large language model (MLLM) fails to accurately synchronize audio and video. The task was to identify which audio clip best matches a given video. The model incorrectly chose an audio clip with random offsets, speed-ups, and slow-downs, demonstrating a lack of understanding in aligning events across different modalities.</p><details><summary>read the caption</summary>Figure 28: A sampled error case in the audio-visual synchronization task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/ alt></figure></p><blockquote><p>üîº This figure shows a sample error case from the AV-Odyssey benchmark&rsquo;s action sequencing task. Gemini 1.5 Pro incorrectly identified the order of actions based on the audio cues, indicating issues with both audio understanding and reasoning capabilities. The correct sequence is shown for comparison, highlighting the model&rsquo;s inability to accurately interpret temporal relationships between actions.</p><details><summary>read the caption</summary>Figure 29: A sampled error case in the action sequencing task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x30.png alt></figure></p><blockquote><p>üîº The figure showcases a common mistake made by the Gemini 1.5 Pro model during the hallucination evaluation task. The model incorrectly identifies a sitar as being present in an audio clip that actually only contains drums. This highlights the model&rsquo;s tendency to hallucinate or falsely perceive elements not present in the input audio, demonstrating limitations in its audio understanding capabilities.</p><details><summary>read the caption</summary>Figure 30: A sampled error case in the hallucination evaluation task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x31.png alt></figure></p><blockquote><p>üîº The figure displays an example where a multimodal large language model (MLLM) incorrectly predicts the action a person is performing. The model is presented with an image of a person standing near a coffee container and an audio clip of sounds associated with the action. The MLLM incorrectly identifies the action as &lsquo;wrapping up coffee&rsquo; due to errors in understanding the temporal relationship between the visual input and the audio clip.</p><details><summary>read the caption</summary>Figure 31: A sampled error case in the action prediction task.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.02611/x32.png alt></figure></p><blockquote><p>üîº This figure shows a case where the model incorrectly identifies the action being performed in a video clip. The task is to determine what the person in the video is doing based on the audio and visual information. The image shows a person near a countertop holding a rag. The model incorrectly determines that the person is wiping the counter with the rag. However, the correct answer is that the person is rinsing the chopping board.</p><details><summary>read the caption</summary>Figure 32: A sampled error case in the action tracing task.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Benchmark / Dataset</th><th>Modality</th><th>Questions</th><th>Answer Type</th><th>Customized Question</th><th>Timbre</th><th>Tone</th><th>Melody</th><th>Space</th><th>Time</th><th>Hallucination</th><th>Intricacy</th><th>Multiple Domains</th><th>Interleaved</th><th></th></tr></thead><tbody><tr><td>MME Bench [21]</td><td>Image</td><td>2194</td><td>Y/N</td><td>‚úì</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>‚úì</td><td>‚úó</td><td></td></tr><tr><td>MMBench [42]</td><td>Image(s)</td><td>2974</td><td>A/B/C/D</td><td>‚úì</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>‚úì</td><td>‚úó</td><td></td></tr><tr><td>SEED-Bench-2 [32]</td><td>Image(s) & Video</td><td>24371</td><td>A/B/C/D</td><td>‚úì</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>‚úì</td><td>‚úì</td><td></td></tr><tr><td>AVQA Dataset [81]</td><td>Video & Audio</td><td>57335</td><td>A/B/C/D</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>‚úì</td><td>‚úó</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td></td></tr><tr><td>Pano-AVQA Dataset [88]</td><td>Video & Audio</td><td>51700</td><td>defined words & bbox</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td>‚úì</td><td>‚úó</td><td>‚úó</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td></td></tr><tr><td>Music-AVQA Dataset [33]</td><td>Video & Audio</td><td>45867</td><td>defined words</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td>‚úó</td><td></td></tr><tr><td>SAVE Bench [68]</td><td>Image & Video & Audio</td><td>4350</td><td>free-form</td><td>‚úó</td><td>‚úì</td><td>‚úó</td><td>‚úó</td><td>‚úì</td><td>‚úó</td><td>‚úó</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td></td></tr><tr><td>OmniBench [37]</td><td>Image & Audio</td><td>1142</td><td>A/B/C/D</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>‚úì</td><td>‚úó</td><td>‚úó</td><td>‚úì</td><td>‚úó</td><td></td></tr><tr><td>AV-Odyssey Bench (ours)</td><td>Image(s) & Video & Audio(s)</td><td>4555</td><td>A/B/C/D</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares various multimodal large language model (MLLM) benchmarks and datasets, highlighting their differences in terms of modality (e.g., image, video, audio), number of questions, answer type (e.g., Yes/No, multiple choice), and the specific audio attributes considered (e.g., timbre, tone, melody). The table helps to illustrate the limitations of existing benchmarks in terms of their scope and ability to fully assess the audio-visual capabilities of MLLMs, motivating the need for a more comprehensive benchmark.</p><details><summary>read the caption</summary>Table 2: Comparisons between MLLM benchmarks / datasets.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Statistics</th><th>Number</th></tr></thead><tbody><tr><td>Total Questions</td><td>4555</td></tr><tr><td>Total Tasks</td><td>26</td></tr><tr><td>Domains</td><td>10</td></tr><tr><td>Questions with Multiple Images, Singe Audio</td><td>2610</td></tr><tr><td>Questions with Single Image, Multiple Audios</td><td>891</td></tr><tr><td>Questions with Singe Image, Singe Audio</td><td>434</td></tr><tr><td>Questions with Singe Video, Singe Audio</td><td>220</td></tr><tr><td>Questions with Single Video, Multiple Audios</td><td>400</td></tr><tr><td>Correct Option Distribution (A:B:C:D)</td><td>1167:1153:1119:1116</td></tr><tr><td>Average Audio Time</td><td>16.32 seconds</td></tr><tr><td>Average Image Resolution</td><td>1267.72 √ó 891.40</td></tr><tr><td>Average Video Resolution</td><td>1678.69 √ó 948.56</td></tr><tr><td>Average Video Time</td><td>15.58 seconds</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 3 presents a detailed statistical overview of the AV-Odyssey Benchmark dataset. It provides the total number of questions and tasks included, the number of domains covered, and a breakdown of question types based on the combination of input modalities (single image, multiple images, single audio, multiple audios, single video, multiple videos). Furthermore, it shows the distribution of correct answers across the four answer choices (A, B, C, and D), along with the average duration of audio clips, and the average resolutions and duration of image and video data used in the benchmark.</p><details><summary>read the caption</summary>Table 3: Detailed statistics of AV-Odyssey Benchmark.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption>Model|LLM Size|Timbre|Timbre R<sub>TÃÑ</sub>|Tone|Tone R<sub>TÃÑ</sub>|Melody|Melody R<sub>TÃÑ</sub>|Space|Space R<sub>TÃÑ</sub>|Time|Time R<sub>TÃÑ</sub>|Hallucination|Hallucination R<sub>TÃÑ</sub>|Intricacy|Intricacy R<sub>TÃÑ</sub>|All Avg.|All Avg. R<sub>TÃÑ</sub>
&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;
Random|-|25.0|-|25.0|-|25.0|-|25.0|-|25.0|-|25.0|-|25.0|-|25.0|-
Open Source|Unified-IO-2 L [47]|1B|23.8|16|24.1|11|28.8|6|15.0|18|26.8|9|30.0|5|30.4|11|26.0|16
|Unified-IO-2 XL [47]|3B|24.3|12|23.2|13|27.8|7|22.5|14|25.3|16|31.5|2|34.8|4|26.3|12
|Unified-IO-2 XXL [47]|7B|26.3|6|22.7|15|26.4|12|32.5|4|26.8|9|24.5|14|33.8|7|27.2|6
|OneLLM [23]|7B|25.0|10|25.5|6|21.5|18|37.5|2|29.3|1|25.5|11|38.4|1|27.4|5
|PandaGPT [67]|7B|23.5|17|23.2|13|27.6|10|45.0|1|23.8|18|28.0|10|23.9|17|26.7|10
|Video-llama [90]|7B|25.5|7|22.3|16|24.4|17|30.0|6|26.2|13|25.0|12|30.7|10|26.1|14
|VideoLLaMA2 [15]|7B|24.1|13|25.5|6|26.4|14|30.0|6|27.2|8|33.0|1|34.5|5|26.8|9
|AnyGPT [89]|7B|24.6|11|25.0|8|26.4|15|27.5|11|29.2|2|29.0|6|25.7|15|26.1|15
|NExT-GPT [77]|7B|23.2|18|20.9|17|27.8|9|30.0|6|28.8|3|28.5|8|23.6|18|25.5|17
Closed Source|VITA [22]|8x7B|24.1|14|26.4|5|27.8|7|22.5|14|26.3|12|31.0|4|36.8|2|26.4|11
|Gemini 1.5 Flash [70]| -|27.2|4|25.0|8|28.8|5|30.0|6|25.3|16|28.5|8|31.2|9|27.8|4
|Gemini 1.5 Flash-8B [70]| -|25.1|9|24.5|10|28.9|4|27.5|11|27.5|5|29.0|6|30.2|12|26.8|8
|Gemini 1.5 Pro [70]| -|30.8|3|31.4|2|31.3|3|37.5|2|27.7|4|20.5|18|33.0|8|30.8|3
|Reka Core [71]|67B|26.7|5|27.7|4|26.4|13|22.5|14|26.5|11|24.0|15|34.3|6|26.9|7
|Reka Flash [71]|21B|25.5|8|24.1|11|27.2|11|30.0|6|27.5|5|31.5|2|24.1|16|26.3|13
|Reka Edge [71]|7B|23.8|15|20.5|18|26.3|16|22.5|14|25.5|14|22.5|17|36.8|3|25.0|18
|GPT-4o visual caption [27]| -|37.4|2|28.6|3|32.3|2|27.5|11|25.5|14|23.0|16|28.9|13|32.3|2
|GPT-4o audio caption [27]| -|38.6|1|31.8|1|33.6|1|32.5|4|27.5|5|25.0|12|26.1|14|34.5|1</table></figure><blockquote><p>üîº Table 4 presents a comprehensive evaluation of various Multimodal Large Language Models (MLLMs) on the AV-Odyssey benchmark. The benchmark is divided into several sub-sections representing different audio-visual attributes. For each MLLM, the table shows the model size, the average accuracy (T) across all sub-sections, the ranking (R<sub>T</sub>) based on this average accuracy, and then individual average accuracies for each sub-section. The highest accuracy in each column is bolded, and the second highest is underlined. Finally, the table provides the overall average accuracy across all questions in the entire AV-Odyssey benchmark.</p><details><summary>read the caption</summary>Table 4: Evaluation results of various MLLMs in different parts of AV-Odyssey Bench. The highest performance is highlighted in bold, while the second highest is underlined. T¬Ø¬Øùëá\bar{T}over¬Ø start_ARG italic_T end_ARG is the averaged accuracy across corresponding dimensions, and RT¬ØsubscriptùëÖ¬ØùëáR_{\bar{T}}italic_R start_POSTSUBSCRIPT over¬Ø start_ARG italic_T end_ARG end_POSTSUBSCRIPT is the rank based on the the averaged accuracy. ‚ÄúAll Avg.‚Äù represents the averaged accuracy over all questions in our AV-Odyssey Bench.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Task ID</th><th>Task Name</th><th>Task Category</th><th>Class</th><th>Number</th></tr></thead><tbody><tr><td>1</td><td>Instrument Recognition</td><td>Timbre</td><td>28</td><td>200</td></tr><tr><td>2</td><td>Singer Recognition</td><td>Timbre</td><td>20</td><td>200</td></tr><tr><td>3</td><td>Gunshot Recognition</td><td>Timbre</td><td>13</td><td>200</td></tr><tr><td>4</td><td>Bird Recognition</td><td>Timbre</td><td>39</td><td>200</td></tr><tr><td>5</td><td>Animal Recognition</td><td>Timbre</td><td>13</td><td>200</td></tr><tr><td>6</td><td>Transportation Recognition</td><td>Timbre</td><td>8</td><td>200</td></tr><tr><td>7</td><td>Material Recognition</td><td>Timbre</td><td>10</td><td>200</td></tr><tr><td>8</td><td>Scene Recognition</td><td>Timbre</td><td>8</td><td>200</td></tr><tr><td>9</td><td>Hazard Recognition</td><td>Timbre</td><td>8</td><td>108</td></tr><tr><td>10</td><td>Action Recognition</td><td>Timbre</td><td>20</td><td>196</td></tr><tr><td>11</td><td>Eating Sound Recognition</td><td>Timbre</td><td>20</td><td>200</td></tr><tr><td>12</td><td>Speech Sentiment Analysis</td><td>Tone</td><td>7</td><td>200</td></tr><tr><td>13</td><td>Meme Understanding</td><td>Tone</td><td>N/A</td><td>20</td></tr><tr><td>14</td><td>Music Sentiment Analysis</td><td>Melody</td><td>7</td><td>197</td></tr><tr><td>15</td><td>Music Genre Classification</td><td>Melody</td><td>8</td><td>200</td></tr><tr><td>16</td><td>Dance and Music Matching</td><td>Melody</td><td>10</td><td>200</td></tr><tr><td>17</td><td>Film and Music Matching</td><td>Melody</td><td>5</td><td>200</td></tr><tr><td>18</td><td>Music Score Matching</td><td>Melody</td><td>N/A</td><td>200</td></tr><tr><td>19</td><td>Audio 3D Angle Estimation</td><td>Space</td><td>N/A</td><td>20</td></tr><tr><td>20</td><td>Audio Distance Estimation</td><td>Space</td><td>N/A</td><td>20</td></tr><tr><td>21</td><td>Audio Time Estimation</td><td>Time</td><td>N/A</td><td>200</td></tr><tr><td>22</td><td>Audio-Visual Synchronization</td><td>Time</td><td>N/A</td><td>200</td></tr><tr><td>23</td><td>Action Sequencing</td><td>Time</td><td>N/A</td><td>200</td></tr><tr><td>24</td><td>Hallucination Evaluation</td><td>Hallucination</td><td>19</td><td>200</td></tr><tr><td>25</td><td>Action Prediction</td><td>Intricacy</td><td>N/A</td><td>199</td></tr><tr><td>26</td><td>Action Tracing</td><td>Intricacy</td><td>N/A</td><td>195</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 5 presents a detailed breakdown of the tasks included in the AV-Odyssey benchmark. It lists each task&rsquo;s name, its category (e.g., Timbre, Tone, Melody), and the number of classes and questions associated with that task. This provides a comprehensive overview of the benchmark&rsquo;s structure and the distribution of different audio-visual challenges it presents.</p><details><summary>read the caption</summary>Table 5: Detailed task statistics in AV-Odyssey Bench.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>LLM Size</th><th>Instrument Recognition</th><th>Singer Recognition</th><th>Gunshot Recognition</th><th>Bird Recognition</th><th>Animal Recognition</th><th>Transportation Recognition</th><th>Material Recognition</th><th>Scene Recognition</th><th>Hazard Recognition</th><th>Action Recognition</th><th>Eating Sound Recognition</th></tr></thead><tbody><tr><td>Open Source</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Unified-IO-2 L [47]</td><td>1B</td><td>20.5</td><td>22.5</td><td>25.5</td><td>18.5</td><td>27.0</td><td>26.5</td><td>23.0</td><td>28.0</td><td>21.3</td><td>20.9</td><td>26.5</td></tr><tr><td>Unified-IO-2 XL [47]</td><td>3B</td><td>20.0</td><td>23.5</td><td>24.0</td><td>20.5</td><td>27.5</td><td>26.0</td><td>27.5</td><td>30.0</td><td>19.4</td><td>19.9</td><td>26.5</td></tr><tr><td>Unified-IO-2 XXL [47]</td><td>7B</td><td>29.5</td><td>24.0</td><td>23.5</td><td>29.0</td><td>23.5</td><td>25.5</td><td>30.5</td><td>26.5</td><td>23.1</td><td>27.0</td><td>25.5</td></tr><tr><td>OneLLM [23]</td><td>7B</td><td>26.0</td><td>21.5</td><td>27.0</td><td>26.0</td><td>22.0</td><td>20.0</td><td>29.5</td><td>24.5</td><td>26.9</td><td>23.0</td><td>29.5</td></tr><tr><td>PandaGPT [67]</td><td>7B</td><td>20.0</td><td>21.5</td><td>23.0</td><td>17.5</td><td>26.0</td><td>26.5</td><td>28.0</td><td>27.0</td><td>23.1</td><td>21.4</td><td>24.5</td></tr><tr><td>Video-llama [90]</td><td>7B</td><td>22.5</td><td>24.5</td><td>27.0</td><td>26.5</td><td>27.0</td><td>23.5</td><td>28.0</td><td>25.0</td><td>25.0</td><td>26.0</td><td>25.5</td></tr><tr><td>VideoLLaMA2 [15]</td><td>7B</td><td>22.5</td><td>24.0</td><td>27.0</td><td>17.0</td><td>23.5</td><td>27.5</td><td>26.5</td><td>26.5</td><td>19.4</td><td>23.0</td><td>25.5</td></tr><tr><td>AnyGPT [89]</td><td>7B</td><td>22.5</td><td>28.5</td><td>28.0</td><td>17.5</td><td>24.0</td><td>25.5</td><td>23.0</td><td>28.0</td><td>25.9</td><td>20.4</td><td>27.5</td></tr><tr><td>NExT-GPT [77]</td><td>7B</td><td>21.0</td><td>23.5</td><td>25.5</td><td>21.5</td><td>25.5</td><td>25.5</td><td>21.0</td><td>24.0</td><td>19.4</td><td>23.0</td><td>24.0</td></tr><tr><td>VITA [22]</td><td>8 √ó 7B</td><td>22.0</td><td>20.5</td><td>24.5</td><td>21.5</td><td>27.5</td><td>25.0</td><td>23.5</td><td>28.5</td><td>21.3</td><td>19.4</td><td>29.5</td></tr><tr><td>Closed Source</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Gemini 1.5 Flash [70]</td><td>-</td><td>24.5</td><td>24.0</td><td>23.5</td><td>17.0</td><td>32.5</td><td>26.0</td><td>22.5</td><td>29.5</td><td>34.3</td><td>48.0</td><td>21.5</td></tr><tr><td>Gemini 1.5 Flash-8B [70]</td><td>-</td><td>16.5</td><td>22.5</td><td>24.0</td><td>19.0</td><td>28.0</td><td>26.5</td><td>27.0</td><td>29.0</td><td>26.9</td><td>32.7</td><td>24.5</td></tr><tr><td>Gemini 1.5 Pro [70]</td><td>-</td><td>33.0</td><td>26.0</td><td>29.0</td><td>25.0</td><td>25.5</td><td>26.0</td><td>29.5</td><td>30.0</td><td>38.0</td><td>57.7</td><td>22.5</td></tr><tr><td>Reka Core [71]</td><td>67B</td><td>32.5</td><td>20.0</td><td>26.5</td><td>25.0</td><td>24.0</td><td>27.0</td><td>30.0</td><td>27.0</td><td>25.0</td><td>34.2</td><td>21.5</td></tr><tr><td>Reka Flash [71]</td><td>21B</td><td>20.0</td><td>22.5</td><td>26.5</td><td>26.0</td><td>28.5</td><td>26.5</td><td>26.5</td><td>29.0</td><td>28.7</td><td>22.4</td><td>25.0</td></tr><tr><td>Reka Edge [71]</td><td>7B</td><td>21.5</td><td>24.0</td><td>30.5</td><td>20.0</td><td>19.5</td><td>22.5</td><td>20.5</td><td>25.5</td><td>25.9</td><td>23.5</td><td>29.0</td></tr><tr><td>GPT-4o visual caption [27]</td><td>-</td><td>33.0</td><td>30.5</td><td>24.0</td><td>26.5</td><td>43.0</td><td>42.0</td><td>32.5</td><td>39.0</td><td>49.1</td><td>67.3</td><td>30.5</td></tr><tr><td>GPT-4o audio caption [27]</td><td>-</td><td>40.0</td><td>38.0</td><td>27.5</td><td>26.5</td><td>45.0</td><td>42.0</td><td>27.0</td><td>41.0</td><td>42.6</td><td>62.2</td><td>35.5</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 6 presents the performance of various multimodal large language models (MLLMs) on the &lsquo;Timbre&rsquo; portion of the AV-Odyssey benchmark. The benchmark assesses the models&rsquo; ability to understand and reason using audio-visual information focusing on timbre, a key attribute of sound. The table shows each model&rsquo;s accuracy (percentage correct) on several tasks related to timbre, including instrument, singer, gunshot, bird, animal, transportation, material, scene, hazard, and action recognition, as well as eating sound recognition. The best and second-best performing model for each task is highlighted in bold and underlined, respectively. Parenthetical values after each task name denote the number of questions associated with that task.</p><details><summary>read the caption</summary>Table 6: Evaluation results of various MLLMs in ‚ÄòTimbre‚Äô part of AV-Odyssey Bench. The best (second best) is in bold (underline). The corresponding brackets for each task indicate the number of associated questions.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>LLM</th><th>Size</th><th>Tone</th><th>Melody</th><th>Melody</th><th>Melody</th><th>Melody</th><th>Melody</th><th>Space</th><th>Space</th><th>Time</th><th>Time</th><th>Time</th><th>Hallucination</th><th>Intricacy</th><th>Intricacy</th></tr></thead><tbody><tr><td>Open Source</td><td>Speech Sentiment Analysis</td><td>Meme Understanding</td><td>Music Sentiment Analysis</td><td>Music Genre Classification</td><td>Dance and Music Matching</td><td>Film and Music Matching</td><td>Music Score Matching</td><td>Audio 3D Angle Estimation</td><td>Audio Distance Estimation</td><td>Audio Time Estimation</td><td>Audio-Visual Synchronization</td><td>Action Sequencing</td><td>Hallucination Evaluation</td><td>Action Prediction</td><td>Action Tracing</td><td></td></tr><tr><td>Open Source</td><td>200</td><td>20</td><td>97</td><td>200</td><td>200</td><td>200</td><td>200</td><td>20</td><td>20</td><td>200</td><td>200</td><td>200</td><td>200</td><td>199</td><td>195</td><td></td></tr><tr><td>Unified-IO-2 L [47]</td><td>1B</td><td>24.5</td><td>20.0</td><td>27.9</td><td>31.0</td><td>27.5</td><td>32.5</td><td>24.5</td><td>15.0</td><td>15.0</td><td>28.0</td><td>25.5</td><td>27.0</td><td>30.0</td><td>27.1</td><td>33.8</td></tr><tr><td>Unified-IO-2 XL [47]</td><td>3B</td><td>23.0</td><td>25.0</td><td>26.9</td><td>30.5</td><td>27.0</td><td>31.5</td><td>22.5</td><td>30.0</td><td>15.0</td><td>26.5</td><td>25.5</td><td>24.0</td><td>31.5</td><td>35.7</td><td>33.8</td></tr><tr><td>Unified-IO-2 XXL [47]</td><td>7B</td><td>23.0</td><td>20.0</td><td>23.9</td><td>31.5</td><td>27.5</td><td>24.5</td><td>23.5</td><td>50.0</td><td>15.0</td><td>28.0</td><td>25.0</td><td>27.5</td><td>24.5</td><td>33.2</td><td>34.4</td></tr><tr><td>OneLLM [23]</td><td>7B</td><td>26.0</td><td>20.0</td><td>20.8</td><td>23.5</td><td>26.5</td><td>18.5</td><td>18.0</td><td>45.0</td><td>30.0</td><td>31.5</td><td>29.5</td><td>27.0</td><td>25.5</td><td>41.7</td><td>34.9</td></tr><tr><td>PandaGPT [67]</td><td>7B</td><td>23.5</td><td>20.0</td><td>21.6</td><td>28.0</td><td>27.0</td><td>32.5</td><td>26.0</td><td>45.0</td><td>45.0</td><td>18.5</td><td>26.0</td><td>27.0</td><td>28.0</td><td>19.6</td><td>28.2</td></tr><tr><td>Video-llama [90]</td><td>7B</td><td>23.0</td><td>15.0</td><td>25.8</td><td>24.0</td><td>20.0</td><td>25.0</td><td>28.0</td><td>45.0</td><td>15.0</td><td>28.5</td><td>23.5</td><td>26.5</td><td>25.0</td><td>28.6</td><td>32.8</td></tr><tr><td>VideoLLaMA2 [15]</td><td>7B</td><td>26.0</td><td>20.0</td><td>26.8</td><td>29.0</td><td>25.5</td><td>30.5</td><td>20.5</td><td>45.0</td><td>15.0</td><td>28.5</td><td>26.5</td><td>26.5</td><td>33.0</td><td>28.6</td><td>40.5</td></tr><tr><td>AnyGPT [89]</td><td>7B</td><td>25.5</td><td>20.0</td><td>23.4</td><td>29.5</td><td>25.5</td><td>26.0</td><td>26.0</td><td>40.0</td><td>15.0</td><td>30.5</td><td>28.0</td><td>29.0</td><td>29.0</td><td>21.1</td><td>30.3</td></tr><tr><td>NExT-GPT [77]</td><td>7B</td><td>21.5</td><td>15.0</td><td>23.7</td><td>26.0</td><td>28.0</td><td>31.0</td><td>28.0</td><td>45.0</td><td>15.0</td><td>31.5</td><td>24.0</td><td>31.0</td><td>28.5</td><td>20.6</td><td>26.7</td></tr><tr><td>VITA [22]</td><td>8 √ó 7B</td><td>24.5</td><td>45.0</td><td>26.8</td><td>26.0</td><td>27.5</td><td>33.5</td><td>24.5</td><td>25.0</td><td>20.0</td><td>26.5</td><td>25.5</td><td>27.0</td><td>31.0</td><td>34.2</td><td>39.5</td></tr><tr><td>Closed Source</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td></td></tr><tr><td>Gemini 1.5 Flash [70]</td><td>-</td><td>23.5</td><td>40.0</td><td>21.3</td><td>31.0</td><td>27.5</td><td>32.5</td><td>28.0</td><td>30.0</td><td>30.0</td><td>27.5</td><td>23.5</td><td>25.0</td><td>28.5</td><td>27.6</td><td>34.9</td></tr><tr><td>Gemini 1.5 Flash-8B [70]</td><td>-</td><td>24.5</td><td>25.0</td><td>25.9</td><td>33.0</td><td>27.5</td><td>32.0</td><td>24.5</td><td>40.0</td><td>15.0</td><td>31.0</td><td>25.5</td><td>26.0</td><td>29.0</td><td>25.6</td><td>34.9</td></tr><tr><td>Gemini 1.5 Pro [70]</td><td>-</td><td>29.5</td><td>50.0</td><td>25.4</td><td>42.5</td><td>28.0</td><td>28.5</td><td>29.0</td><td>35.0</td><td>40.0</td><td>30.0</td><td>24.5</td><td>28.5</td><td>20.5</td><td>32.2</td><td>33.8</td></tr><tr><td>Reka Core [71]</td><td>67B</td><td>28.5</td><td>20.0</td><td>22.8</td><td>24.5</td><td>27.5</td><td>30.0</td><td>25.5</td><td>25.0</td><td>20.0</td><td>30.0</td><td>25.5</td><td>24.0</td><td>24.0</td><td>33.7</td><td>34.9</td></tr><tr><td>Reka Flash [71]</td><td>21B</td><td>24.5</td><td>20.0</td><td>30.5</td><td>29.5</td><td>27.5</td><td>25.5</td><td>24.5</td><td>45.0</td><td>15.0</td><td>30.0</td><td>25.5</td><td>27.0</td><td>31.5</td><td>19.1</td><td>29.2</td></tr><tr><td>Reka Edge [71]</td><td>7B</td><td>20.5</td><td>20.0</td><td>24.9</td><td>24.5</td><td>27.5</td><td>30.0</td><td>24.0</td><td>30.0</td><td>15.0</td><td>30.0</td><td>25.5</td><td>21.0</td><td>22.5</td><td>38.2</td><td>35.4</td></tr><tr><td>GPT-4o visual caption [27]</td><td>-</td><td>26.0</td><td>55.0</td><td>24.4</td><td>48.0</td><td>27.0</td><td>34.5</td><td>23.5</td><td>25.0</td><td>30.0</td><td>21.5</td><td>22.5</td><td>32.5</td><td>23.0</td><td>32.2</td><td>25.6</td></tr><tr><td>GPT-4o audio caption [27]</td><td>-</td><td>28.0</td><td>70.0</td><td>24.4</td><td>56.5</td><td>27.5</td><td>32.5</td><td>22.5</td><td>30.0</td><td>35.0</td><td>23.5</td><td>25.5</td><td>33.5</td><td>25.0</td><td>30.2</td><td>22.0</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 7 presents a comprehensive evaluation of various Multimodal Large Language Models (MLLMs) across six key aspects within the AV-Odyssey benchmark: Time, Melody, Space, Hallucination, and Intricacy. Each aspect represents a set of tasks designed to assess different audio-visual comprehension abilities. The table details the performance of both closed-source and open-source MLLMs, showing their accuracy (percentage) for each task. The best-performing model for each task is highlighted in bold, while the second-best is underlined. The number of questions associated with each task is also indicated in parentheses for context.</p><details><summary>read the caption</summary>Table 7: Evaluation results of various MLLMs in ‚ÄòTime‚Äô, ‚ÄòMelody‚Äô, ‚ÄòSpace‚Äô. ‚ÄòTime‚Äô, ‚ÄòHallucination‚Äô, and ‚ÄòIntricacy‚Äô parts of AV-Odyssey Bench. The best (second best) is in bold (underline). The corresponding brackets for each task indicate the number of associated questions.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-bd412145bf1c62f4808039af9bbd16ea class=gallery><img src=https://ai-paper-reviewer.com/2412.02611/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.02611/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02611/&amp;title=AV-Odyssey%20Bench:%20Can%20Your%20Multimodal%20LLMs%20Really%20Understand%20Audio-Visual%20Information?" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02611/&amp;text=AV-Odyssey%20Bench:%20Can%20Your%20Multimodal%20LLMs%20Really%20Understand%20Audio-Visual%20Information?" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02611/&amp;subject=AV-Odyssey%20Bench:%20Can%20Your%20Multimodal%20LLMs%20Really%20Understand%20Audio-Visual%20Information?" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.02611/index.md",oid_likes="likes_paper-reviews/2412.02611/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.02592/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-03T00:00:00+00:00>3 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.03187/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Weighted-Reward Preference Optimization for Implicit Model Fusion</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-04T00:00:00+00:00>4 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>