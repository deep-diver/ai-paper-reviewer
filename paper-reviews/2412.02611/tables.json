[{"content": "| Method | Sound Counting | Loudness Comparison | Pitch Comparison | Duration Comparison |\n|---|---|---|---|---|\n| Random | 50.0 | 50.0 | 50.0 | 50.0 |\n| Gemini 1.5 Flash [70] | 55.0 | 62.0 | 54.0 | 89.0 |\n| Gemini 1.5 Flash-8B [70] | 49.0 | 55.0 | 51.0 | 51.0 |\n| Gemini 1.5 Pro [70] | 81.0 | 60.0 | 52.0 | 84.0 |\n| Reka Core [71] | 54.0 | 43.0 | 42.0 | 40.0 |\n| Reka Flash [71] | 48.0 | 58.0 | 51.0 | 44.0 |\n| Reka Edge [71] | 47.0 | 56.0 | 50.0 | 44.0 |\n| GPT-4o audio-preview [27] | 50.0 | 58.0 | 58.0 | 57.0 |", "caption": "Table 1: Results on four basic auditory tasks (DeafTest). The questions are designed as two-choice questions. The random baseline performance is 50%.", "description": "This table presents the results of four basic auditory tasks from the DeafTest, designed to evaluate the fundamental listening abilities of multimodal large language models (MLLMs).  The tasks assess the models' performance on simple auditory discriminations, including sound counting, loudness comparison, pitch comparison, and duration comparison. Each task is a two-choice question, meaning the model must select one of two options.  The random baseline performance for these two-choice questions is 50%, providing a context for evaluating the model's actual performance.  The table shows the performance of several MLLMs (Gemini 1.5 Flash, Gemini 1.5 Flash-8B, Gemini 1.5 Pro, Reka Core, Reka Flash, Reka Edge, and GPT-40 Audio-preview) on each of the four tasks, expressed as percentages. This allows for a direct comparison of how well these models perform on these basic audio processing tasks compared to random chance.", "section": "3.1. Deaf Test Tasks"}, {"content": "| Benchmark / Dataset | Modality | Questions | Answer Type | Customized Question | Timbre | Tone | Melody | Space | Time | Hallucination | Intricacy | Multiple Domains | Interleaved |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| MME Bench [21] | Image | 2194 | Y/N | \u2713 | - | - | - | - | - | - | - | \u2713 | \u2717 |\n| MMBench [42] | Image(s) | 2974 | A/B/C/D | \u2713 | - | - | - | - | - | - | - | \u2713 | \u2717 |\n| SEED-Bench-2 [32] | Image(s) & Video | 24371 | A/B/C/D | \u2713 | - | - | - | - | - | - | - | \u2713 | \u2713 |\n| AVQA Dataset [81] | Video & Audio | 57335 | A/B/C/D | \u2713 | \u2713 | \u2717 | \u2717 | \u2717 | \u2713 | \u2717 | \u2713 | \u2713 | \u2717 |\n| Pano-AVQA Dataset [88] | Video & Audio | 51700 | defined words & bbox | \u2713 | \u2713 | \u2713 | \u2717 | \u2713 | \u2717 | \u2717 | \u2713 | \u2713 | \u2717 |\n| Music-AVQA Dataset [33] | Video & Audio | 45867 | defined words | \u2713 | \u2713 | \u2717 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2717 | \u2717 |\n| SAVE Bench [68] | Image & Video & Audio | 4350 | free-form | \u2717 | \u2713 | \u2717 | \u2717 | \u2713 | \u2717 | \u2717 | \u2713 | \u2713 | \u2717 |\n| OmniBench [37] | Image & Audio | 1142 | A/B/C/D | \u2713 | \u2713 | \u2717 | \u2717 | \u2717 | \u2713 | \u2717 | \u2717 | \u2713 | \u2717 |\n| AV-Odyssey Bench (ours) | Image(s) & Video & Audio(s) | 4555 | A/B/C/D | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 |", "caption": "Table 2: Comparisons between MLLM benchmarks / datasets.", "description": "This table compares various multimodal large language model (MLLM) benchmarks and datasets, highlighting their differences in terms of modality (e.g., image, video, audio), number of questions, answer type (e.g., Yes/No, multiple choice), and the specific audio attributes considered (e.g., timbre, tone, melody).  The table helps to illustrate the limitations of existing benchmarks in terms of their scope and ability to fully assess the audio-visual capabilities of MLLMs, motivating the need for a more comprehensive benchmark.", "section": "2. Related Work"}, {"content": "| Statistics | Number |\n|---|---| \n| Total Questions | 4555 |\n| Total Tasks | 26 |\n| Domains | 10 |\n| Questions with Multiple Images, Singe Audio | 2610 |\n| Questions with Single Image, Multiple Audios | 891 |\n| Questions with Singe Image, Singe Audio | 434 |\n| Questions with Singe Video, Singe Audio | 220 |\n| Questions with Single Video, Multiple Audios | 400 |\n| Correct Option Distribution (A:B:C:D) | 1167:1153:1119:1116 |\n| Average Audio Time | 16.32 seconds |\n| Average Image Resolution | 1267.72 \u00d7 891.40 |\n| Average Video Resolution | 1678.69 \u00d7 948.56 |\n| Average Video Time | 15.58 seconds |", "caption": "Table 3: Detailed statistics of AV-Odyssey Benchmark.", "description": "Table 3 presents a detailed statistical overview of the AV-Odyssey Benchmark dataset.  It provides the total number of questions and tasks included, the number of domains covered, and a breakdown of question types based on the combination of input modalities (single image, multiple images, single audio, multiple audios, single video, multiple videos).  Furthermore, it shows the distribution of correct answers across the four answer choices (A, B, C, and D), along with the average duration of audio clips, and the average resolutions and duration of image and video data used in the benchmark.", "section": "3. Method"}, {"content": "Model|LLM Size|Timbre|Timbre R<sub>T\u0304</sub>|Tone|Tone R<sub>T\u0304</sub>|Melody|Melody R<sub>T\u0304</sub>|Space|Space R<sub>T\u0304</sub>|Time|Time R<sub>T\u0304</sub>|Hallucination|Hallucination R<sub>T\u0304</sub>|Intricacy|Intricacy R<sub>T\u0304</sub>|All Avg.|All Avg. R<sub>T\u0304</sub>\n---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---\nRandom|-|25.0|-|25.0|-|25.0|-|25.0|-|25.0|-|25.0|-|25.0|-|25.0|-\nOpen Source|Unified-IO-2 L [47]|1B|23.8|16|24.1|11|28.8|6|15.0|18|26.8|9|30.0|5|30.4|11|26.0|16\n|Unified-IO-2 XL [47]|3B|24.3|12|23.2|13|27.8|7|22.5|14|25.3|16|31.5|2|34.8|4|26.3|12\n|Unified-IO-2 XXL [47]|7B|26.3|6|22.7|15|26.4|12|32.5|4|26.8|9|24.5|14|33.8|7|27.2|6\n|OneLLM [23]|7B|25.0|10|25.5|6|21.5|18|37.5|2|29.3|1|25.5|11|38.4|1|27.4|5\n|PandaGPT [67]|7B|23.5|17|23.2|13|27.6|10|45.0|1|23.8|18|28.0|10|23.9|17|26.7|10\n|Video-llama [90]|7B|25.5|7|22.3|16|24.4|17|30.0|6|26.2|13|25.0|12|30.7|10|26.1|14\n|VideoLLaMA2 [15]|7B|24.1|13|25.5|6|26.4|14|30.0|6|27.2|8|33.0|1|34.5|5|26.8|9\n|AnyGPT [89]|7B|24.6|11|25.0|8|26.4|15|27.5|11|29.2|2|29.0|6|25.7|15|26.1|15\n|NExT-GPT [77]|7B|23.2|18|20.9|17|27.8|9|30.0|6|28.8|3|28.5|8|23.6|18|25.5|17\nClosed Source|VITA [22]|8x7B|24.1|14|26.4|5|27.8|7|22.5|14|26.3|12|31.0|4|36.8|2|26.4|11\n|Gemini 1.5 Flash [70]| -|27.2|4|25.0|8|28.8|5|30.0|6|25.3|16|28.5|8|31.2|9|27.8|4\n|Gemini 1.5 Flash-8B [70]| -|25.1|9|24.5|10|28.9|4|27.5|11|27.5|5|29.0|6|30.2|12|26.8|8\n|Gemini 1.5 Pro [70]| -|30.8|3|31.4|2|31.3|3|37.5|2|27.7|4|20.5|18|33.0|8|30.8|3\n|Reka Core [71]|67B|26.7|5|27.7|4|26.4|13|22.5|14|26.5|11|24.0|15|34.3|6|26.9|7\n|Reka Flash [71]|21B|25.5|8|24.1|11|27.2|11|30.0|6|27.5|5|31.5|2|24.1|16|26.3|13\n|Reka Edge [71]|7B|23.8|15|20.5|18|26.3|16|22.5|14|25.5|14|22.5|17|36.8|3|25.0|18\n|GPT-4o visual caption [27]| -|37.4|2|28.6|3|32.3|2|27.5|11|25.5|14|23.0|16|28.9|13|32.3|2\n|GPT-4o audio caption [27]| -|38.6|1|31.8|1|33.6|1|32.5|4|27.5|5|25.0|12|26.1|14|34.5|1", "caption": "Table 4: Evaluation results of various MLLMs in different parts of AV-Odyssey Bench. The highest performance is highlighted in bold, while the second highest is underlined. T\u00af\u00af\ud835\udc47\\bar{T}over\u00af start_ARG italic_T end_ARG is the averaged accuracy across corresponding dimensions, and RT\u00afsubscript\ud835\udc45\u00af\ud835\udc47R_{\\bar{T}}italic_R start_POSTSUBSCRIPT over\u00af start_ARG italic_T end_ARG end_POSTSUBSCRIPT is the rank based on the the averaged accuracy. \u201cAll Avg.\u201d represents the averaged accuracy over all questions in our AV-Odyssey Bench.", "description": "Table 4 presents a comprehensive evaluation of various Multimodal Large Language Models (MLLMs) on the AV-Odyssey benchmark.  The benchmark is divided into several sub-sections representing different audio-visual attributes. For each MLLM, the table shows the model size, the average accuracy (T) across all sub-sections, the ranking (R<sub>T</sub>) based on this average accuracy, and then individual average accuracies for each sub-section.  The highest accuracy in each column is bolded, and the second highest is underlined.  Finally, the table provides the overall average accuracy across all questions in the entire AV-Odyssey benchmark.", "section": "4. Experiment"}, {"content": "| Task ID | Task Name | Task Category | Class | Number |\n|---|---|---|---|---|\n| 1 | Instrument Recognition | Timbre | 28 | 200 |\n| 2 | Singer Recognition | Timbre | 20 | 200 |\n| 3 | Gunshot Recognition | Timbre | 13 | 200 |\n| 4 | Bird Recognition | Timbre | 39 | 200 |\n| 5 | Animal Recognition | Timbre | 13 | 200 |\n| 6 | Transportation Recognition | Timbre | 8 | 200 |\n| 7 | Material Recognition | Timbre | 10 | 200 |\n| 8 | Scene Recognition | Timbre | 8 | 200 |\n| 9 | Hazard Recognition | Timbre | 8 | 108 |\n| 10 | Action Recognition | Timbre | 20 | 196 |\n| 11 | Eating Sound Recognition | Timbre | 20 | 200 |\n| 12 | Speech Sentiment Analysis | Tone | 7 | 200 |\n| 13 | Meme Understanding | Tone | N/A | 20 |\n| 14 | Music Sentiment Analysis | Melody | 7 | 197 |\n| 15 | Music Genre Classification | Melody | 8 | 200 |\n| 16 | Dance and Music Matching | Melody | 10 | 200 |\n| 17 | Film and Music Matching | Melody | 5 | 200 |\n| 18 | Music Score Matching | Melody | N/A | 200 |\n| 19 | Audio 3D Angle Estimation | Space | N/A | 20 |\n| 20 | Audio Distance Estimation | Space | N/A | 20 |\n| 21 | Audio Time Estimation | Time | N/A | 200 |\n| 22 | Audio-Visual Synchronization | Time | N/A | 200 |\n| 23 | Action Sequencing | Time | N/A | 200 |\n| 24 | Hallucination Evaluation | Hallucination | 19 | 200 |\n| 25 | Action Prediction | Intricacy | N/A | 199 |\n| 26 | Action Tracing | Intricacy | N/A | 195 |", "caption": "Table 5: Detailed task statistics in AV-Odyssey Bench.", "description": "Table 5 presents a detailed breakdown of the tasks included in the AV-Odyssey benchmark.  It lists each task's name, its category (e.g., Timbre, Tone, Melody), and the number of classes and questions associated with that task. This provides a comprehensive overview of the benchmark's structure and the distribution of different audio-visual challenges it presents.", "section": "3.3 Data Curation Process"}, {"content": "| Model | LLM Size | Instrument Recognition | Singer Recognition | Gunshot Recognition | Bird Recognition | Animal Recognition | Transportation Recognition | Material Recognition | Scene Recognition | Hazard Recognition | Action Recognition | Eating Sound Recognition |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Open Source |  |  |  |  |  |  |  |  |  |  |  |  |\n| Unified-IO-2 L [47] | 1B | 20.5 | 22.5 | 25.5 | 18.5 | 27.0 | 26.5 | 23.0 | 28.0 | 21.3 | 20.9 | 26.5 |\n| Unified-IO-2 XL [47] | 3B | 20.0 | 23.5 | 24.0 | 20.5 | 27.5 | 26.0 | 27.5 | 30.0 | 19.4 | 19.9 | 26.5 |\n| Unified-IO-2 XXL [47] | 7B | 29.5 | 24.0 | 23.5 | 29.0 | 23.5 | 25.5 | 30.5 | 26.5 | 23.1 | 27.0 | 25.5 |\n| OneLLM [23] | 7B | 26.0 | 21.5 | 27.0 | 26.0 | 22.0 | 20.0 | 29.5 | 24.5 | 26.9 | 23.0 | 29.5 |\n| PandaGPT [67] | 7B | 20.0 | 21.5 | 23.0 | 17.5 | 26.0 | 26.5 | 28.0 | 27.0 | 23.1 | 21.4 | 24.5 |\n| Video-llama [90] | 7B | 22.5 | 24.5 | 27.0 | 26.5 | 27.0 | 23.5 | 28.0 | 25.0 | 25.0 | 26.0 | 25.5 |\n| VideoLLaMA2 [15] | 7B | 22.5 | 24.0 | 27.0 | 17.0 | 23.5 | 27.5 | 26.5 | 26.5 | 19.4 | 23.0 | 25.5 |\n| AnyGPT [89] | 7B | 22.5 | 28.5 | 28.0 | 17.5 | 24.0 | 25.5 | 23.0 | 28.0 | 25.9 | 20.4 | 27.5 |\n| NExT-GPT [77] | 7B | 21.0 | 23.5 | 25.5 | 21.5 | 25.5 | 25.5 | 21.0 | 24.0 | 19.4 | 23.0 | 24.0 |\n| VITA [22] | 8 \u00d7 7B | 22.0 | 20.5 | 24.5 | 21.5 | 27.5 | 25.0 | 23.5 | 28.5 | 21.3 | 19.4 | 29.5 |\n| Closed Source |  |  |  |  |  |  |  |  |  |  |  |  |\n| Gemini 1.5 Flash [70] | - | 24.5 | 24.0 | 23.5 | 17.0 | 32.5 | 26.0 | 22.5 | 29.5 | 34.3 | 48.0 | 21.5 |\n| Gemini 1.5 Flash-8B [70] | - | 16.5 | 22.5 | 24.0 | 19.0 | 28.0 | 26.5 | 27.0 | 29.0 | 26.9 | 32.7 | 24.5 |\n| Gemini 1.5 Pro [70] | - | 33.0 | 26.0 | 29.0 | 25.0 | 25.5 | 26.0 | 29.5 | 30.0 | 38.0 | 57.7 | 22.5 |\n| Reka Core [71] | 67B | 32.5 | 20.0 | 26.5 | 25.0 | 24.0 | 27.0 | 30.0 | 27.0 | 25.0 | 34.2 | 21.5 |\n| Reka Flash [71] | 21B | 20.0 | 22.5 | 26.5 | 26.0 | 28.5 | 26.5 | 26.5 | 29.0 | 28.7 | 22.4 | 25.0 |\n| Reka Edge [71] | 7B | 21.5 | 24.0 | 30.5 | 20.0 | 19.5 | 22.5 | 20.5 | 25.5 | 25.9 | 23.5 | 29.0 |\n| GPT-4o visual caption [27] | - | 33.0 | 30.5 | 24.0 | 26.5 | 43.0 | 42.0 | 32.5 | 39.0 | 49.1 | 67.3 | 30.5 |\n| GPT-4o audio caption [27] | - | 40.0 | 38.0 | 27.5 | 26.5 | 45.0 | 42.0 | 27.0 | 41.0 | 42.6 | 62.2 | 35.5 |", "caption": "Table 6: Evaluation results of various MLLMs in \u2018Timbre\u2019 part of AV-Odyssey Bench. The best (second best) is in bold (underline). The corresponding brackets for each task indicate the number of associated questions.", "description": "Table 6 presents the performance of various multimodal large language models (MLLMs) on the 'Timbre' portion of the AV-Odyssey benchmark.  The benchmark assesses the models' ability to understand and reason using audio-visual information focusing on timbre, a key attribute of sound. The table shows each model's accuracy (percentage correct) on several tasks related to timbre,  including instrument, singer, gunshot, bird, animal, transportation, material, scene, hazard, and action recognition, as well as eating sound recognition.  The best and second-best performing model for each task is highlighted in bold and underlined, respectively.  Parenthetical values after each task name denote the number of questions associated with that task.", "section": "4. Experiment"}, {"content": "Model|LLM|Size|Tone|Melody|Melody|Melody|Melody|Melody|Space|Space|Time|Time|Time|Hallucination|Intricacy|Intricacy\n---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---\nOpen Source|Speech Sentiment Analysis|Meme Understanding|Music Sentiment Analysis|Music Genre Classification|Dance and Music Matching|Film and Music Matching|Music Score Matching|Audio 3D Angle Estimation|Audio Distance Estimation|Audio Time Estimation|Audio-Visual Synchronization|Action Sequencing|Hallucination Evaluation|Action Prediction|Action Tracing\nOpen Source|200|20|97|200|200|200|200|20|20|200|200|200|200|199|195\nUnified-IO-2 L [47]|1B|24.5|20.0|27.9|31.0|27.5|32.5|24.5|15.0|15.0|28.0|25.5|27.0|30.0|27.1|33.8\nUnified-IO-2 XL [47]|3B|23.0|25.0|26.9|30.5|27.0|31.5|22.5|30.0|15.0|26.5|25.5|24.0|31.5|35.7|33.8\nUnified-IO-2 XXL [47]|7B|23.0|20.0|23.9|31.5|27.5|24.5|23.5|50.0|15.0|28.0|25.0|27.5|24.5|33.2|34.4\nOneLLM [23]|7B|26.0|20.0|20.8|23.5|26.5|18.5|18.0|45.0|30.0|31.5|29.5|27.0|25.5|41.7|34.9\nPandaGPT [67]|7B|23.5|20.0|21.6|28.0|27.0|32.5|26.0|45.0|45.0|18.5|26.0|27.0|28.0|19.6|28.2\nVideo-llama [90]|7B|23.0|15.0|25.8|24.0|20.0|25.0|28.0|45.0|15.0|28.5|23.5|26.5|25.0|28.6|32.8\nVideoLLaMA2 [15]|7B|26.0|20.0|26.8|29.0|25.5|30.5|20.5|45.0|15.0|28.5|26.5|26.5|33.0|28.6|40.5\nAnyGPT [89]|7B|25.5|20.0|23.4|29.5|25.5|26.0|26.0|40.0|15.0|30.5|28.0|29.0|29.0|21.1|30.3\nNExT-GPT [77]|7B|21.5|15.0|23.7|26.0|28.0|31.0|28.0|45.0|15.0|31.5|24.0|31.0|28.5|20.6|26.7\nVITA [22]|8 \u00d7 7B|24.5|45.0|26.8|26.0|27.5|33.5|24.5|25.0|20.0|26.5|25.5|27.0|31.0|34.2|39.5\nClosed Source|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---\nGemini 1.5 Flash [70]| - |23.5|40.0|21.3|31.0|27.5|32.5|28.0|30.0|30.0|27.5|23.5|25.0|28.5|27.6|34.9\nGemini 1.5 Flash-8B [70]| - |24.5|25.0|25.9|33.0|27.5|32.0|24.5|40.0|15.0|31.0|25.5|26.0|29.0|25.6|34.9\nGemini 1.5 Pro [70]| - |29.5|50.0|25.4|42.5|28.0|28.5|29.0|35.0|40.0|30.0|24.5|28.5|20.5|32.2|33.8\nReka Core [71]|67B|28.5|20.0|22.8|24.5|27.5|30.0|25.5|25.0|20.0|30.0|25.5|24.0|24.0|33.7|34.9\nReka Flash [71]|21B|24.5|20.0|30.5|29.5|27.5|25.5|24.5|45.0|15.0|30.0|25.5|27.0|31.5|19.1|29.2\nReka Edge [71]|7B|20.5|20.0|24.9|24.5|27.5|30.0|24.0|30.0|15.0|30.0|25.5|21.0|22.5|38.2|35.4\nGPT-4o visual caption [27]| - |26.0|55.0|24.4|48.0|27.0|34.5|23.5|25.0|30.0|21.5|22.5|32.5|23.0|32.2|25.6\nGPT-4o audio caption [27]| - |28.0|70.0|24.4|56.5|27.5|32.5|22.5|30.0|35.0|23.5|25.5|33.5|25.0|30.2|22.0", "caption": "Table 7: Evaluation results of various MLLMs in \u2018Time\u2019, \u2018Melody\u2019, \u2018Space\u2019. \u2018Time\u2019, \u2018Hallucination\u2019, and \u2018Intricacy\u2019 parts of AV-Odyssey Bench. The best (second best) is in bold (underline). The corresponding brackets for each task indicate the number of associated questions.", "description": "Table 7 presents a comprehensive evaluation of various Multimodal Large Language Models (MLLMs) across six key aspects within the AV-Odyssey benchmark: Time, Melody, Space, Hallucination, and Intricacy.  Each aspect represents a set of tasks designed to assess different audio-visual comprehension abilities.  The table details the performance of both closed-source and open-source MLLMs, showing their accuracy (percentage) for each task. The best-performing model for each task is highlighted in bold, while the second-best is underlined.  The number of questions associated with each task is also indicated in parentheses for context.", "section": "4. Experiment"}]