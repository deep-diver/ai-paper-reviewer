[{"figure_path": "https://arxiv.org/html/2412.02611/x1.png", "caption": "Figure 1: Illustration of two out of four DeafTest tasks. Loudness comparison is used to determine the louder sound of two given sounds. Pitch comparison is to determine which sound has the higher pitch.", "description": "DeafTest is an evaluation benchmark consisting of four simple audio tasks designed to assess the fundamental audio understanding capabilities of multimodal large language models (MLLMs).  Figure 1 showcases two of these tasks: loudness comparison and pitch comparison.  In the loudness comparison task, the MLLM is presented with two audio clips and asked to identify which is louder. The pitch comparison task involves determining which of two audio clips has a higher pitch. These tasks assess the basic audio processing abilities of MLLMs before more complex reasoning is required, helping to determine if the model can truly \"hear\" and interpret simple auditory information.", "section": "3.1. Deaf Test Tasks"}, {"figure_path": "https://arxiv.org/html/2412.02611/x2.png", "caption": "Figure 2: Overview of AV-Odyssey Benchmark. AV-Odyssey Bench demonstrates three major features: 1. Comprehensive Audio Attributes; 2. Extensive Domains; 3. Interleaved Text, Audio, and Images.", "description": "Figure 2 illustrates the AV-Odyssey Benchmark, a comprehensive evaluation suite for multimodal large language models (MLLMs).  The figure highlights three key aspects of the benchmark: 1) **Comprehensive Audio Attributes**: It assesses MLLMs' understanding of various sound characteristics, including timbre, tone, space, melody, hallucination, time, and intricacy. 2) **Extensive Domains**:  The benchmark covers a wide range of audio-visual scenarios from daily life to more specialized domains like music, making it robust and generalizable. 3) **Interleaved Text, Audio, and Images**: The benchmark presents problems that require models to integrate information from text, audio, and visual inputs simultaneously, mirroring real-world complexities. This design ensures that the MLLMs truly understand audio-visual information, and doesn't just rely on superficial pattern recognition.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.02611/x3.png", "caption": "Figure 3: Overview of 26 evaluation tasks of AV-Odyssey Benchmark. We mainly categorize these tasks with the sound attributed into 7 classes.", "description": "This figure provides a visual overview of the 26 evaluation tasks included in the AV-Odyssey benchmark.  These tasks are categorized into seven main classes based on the prominent audio attributes they assess: Timbre, Tone, Melody, Space, Time, Intricacy, and Hallucination. The figure uses a circular layout to display the various tasks within each category, making it easy to see the breadth and depth of the benchmark's coverage of different audio-visual scenarios.  Each task assesses a unique aspect of multimodal understanding, requiring models to integrate information from both audio and visual modalities in order to arrive at the correct answer.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.02611/x4.png", "caption": "Figure 4: Sampled examples from our AV-Odyssey Benchmark.", "description": "Figure 4 presents example questions from the AV-Odyssey benchmark dataset.  Each example showcases a different task from the benchmark, highlighting its multi-modal nature (text, image/video, and audio). The questions require models to integrate information from all modalities to provide a correct answer. This figure illustrates the diversity of tasks and complexity present in the AV-Odyssey benchmark, which tests multimodal large language models' ability to understand and reason using audio-visual information.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.02611/x5.png", "caption": "Figure 5: Distribution of 104 human-annotated errors in the Gemini 1.5 Pro.", "description": "This figure shows a pie chart that breaks down the 104 human-annotated errors made by Gemini 1.5 Pro on the AV-Odyssey benchmark. The errors are categorized into four main types: Audio Understanding (63%), Vision Understanding (10%), Text Understanding (8%), and Reasoning (13%). The remaining 6% of errors fall into the 'Other' category.", "section": "4.3 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2412.02611/x6.png", "caption": "Figure 6: An example of audio understanding error. More examples are provided in the Appendix.", "description": "The figure shows an example where a model misidentifies the audio content.  Specifically, the model incorrectly labels a lion's roar as an elephant trumpeting sound. This highlights the model's limitations in accurately understanding and classifying audio information, demonstrating an audio understanding error.", "section": "4.3 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2412.02611/x7.png", "caption": "Figure 7: A sampled error case in the instrument recognition task.", "description": "The figure shows a multiple-choice question where the model is asked to identify which instrument best matches an audio clip of keyboard music.  The correct answer is the keyboard (C), but the model incorrectly chose the vibraphone (D), demonstrating a failure in audio understanding. The model focused on the timbre and resonance, incorrectly identifying them with a vibraphone instead of the keyboard.", "section": "3.1. Deaf Test Tasks"}, {"figure_path": "https://arxiv.org/html/2412.02611/x8.png", "caption": "Figure 8: A sampled error case in the singer recognition task.", "description": "This figure shows a sample error from the singer recognition task in the AV-Odyssey benchmark. The task required the model to identify the singer based on their vocal timbre in an audio clip and choose from four images of different singers. The model incorrectly identified the singer in the audio as Billie Eilish, when it was actually Rihanna. This highlights the model's limitation in accurately identifying singers based solely on vocal timbre, even in simple scenarios. The image provides the audio clip, the options to choose from, the model's incorrect response and the correct answer.", "section": "3.1 Deaf Test Tasks"}, {"figure_path": "https://arxiv.org/html/2412.02611/x9.png", "caption": "Figure 9: A sampled error case in the gunshot recognition task.", "description": "The figure shows a multiple choice question in which the model is asked to identify which image best corresponds to the sound of gunfire. The correct answer is an image depicting a soldier firing a gun, while the model incorrectly chooses an image of a machine gun. This highlights the model's difficulty distinguishing between the sound of different types of gunfire, emphasizing the complexity of audio-visual tasks.", "section": "3.1 Deaf Test Tasks"}, {"figure_path": "https://arxiv.org/html/2412.02611/x10.png", "caption": "Figure 10: A sampled error case in the bird recognition task.", "description": "The figure showcases a sample error from the bird recognition task within the AV-Odyssey benchmark.  It highlights a multimodal large language model's (MLLM) failure to correctly identify both the visual (bird species) and audio (bird sounds) components. The model incorrectly identifies a common grackle as a Brewer's Blackbird and subsequently mismatches the bird sound, illustrating the challenges faced by MLLMs in accurately integrating audio-visual information for complex tasks.", "section": "3.1 Deaf Test Tasks"}, {"figure_path": "https://arxiv.org/html/2412.02611/x11.png", "caption": "Figure 11: A sampled error case in the animal recognition task.", "description": "This figure shows an example where the model incorrectly identifies the sound of a frog as a cat's meow while correctly identifying the image as a cat.  This highlights the model's struggles in accurately associating audio with the correct visual element and demonstrates a failure in audio recognition.", "section": "3.1. Deaf Test Tasks"}, {"figure_path": "https://arxiv.org/html/2412.02611/x12.png", "caption": "Figure 12: A sampled error case in the transportation recognition task.", "description": "This figure shows a sample error case from the transportation recognition task within the AV-Odyssey benchmark.  The model incorrectly identified the sound of an airplane as a motorcycle sound, despite correctly identifying the image of a motorcycle. This highlights a failure in audio understanding, where the model misinterprets the audio despite accurate visual recognition.", "section": "3.1 Deaf Test Tasks"}, {"figure_path": "https://arxiv.org/html/2412.02611/x13.png", "caption": "Figure 13: A sampled error case in the material recognition task.", "description": "This figure shows a multiple-choice question from the AV-Odyssey benchmark's material recognition task.  The question asks the model to identify which of four materials (shown in images) is most likely to produce the sound of someone stepping or hitting on fallen leaves (played in an audio clip). The model incorrectly answers, highlighting a potential text understanding error. The model's response suggests it misunderstood the question, focusing on identifying the source image of the audio rather than identifying the correct material based on the audio. The correct answer is an image depicting a leaf-littered path.", "section": "3.1 Deaf Test Tasks"}, {"figure_path": "https://arxiv.org/html/2412.02611/x14.png", "caption": "Figure 14: A sampled error case in the scene recognition task.", "description": "The figure shows an example where Gemini 1.5 Pro misidentified the sound of traffic as that of a subway train.  The model correctly identified the image content showing a street scene but failed to accurately understand the audio. This highlights the model's difficulty in accurately associating sounds with visual scenes, a key challenge in audio-visual comprehension tasks.", "section": "Timbre: Scene Recognition"}, {"figure_path": "https://arxiv.org/html/2412.02611/x15.png", "caption": "Figure 15: A sampled error case in the hazard recognition task.", "description": "The figure showcases a sample error from the hazard recognition task within the AV-Odyssey benchmark. It visually presents the question, the model's incorrect answer, the correct answer, and a detailed breakdown of the error's cause. The question involves identifying the image depicting a hazard that aligns with the audio clip of a fire. The model misinterprets the sound of fire burning as the sound of boiling water, illustrating a flaw in its audio understanding capabilities and highlights the complexity of audio-visual comprehension tasks.", "section": "4.3. Error Analysis"}, {"figure_path": "https://arxiv.org/html/2412.02611/x16.png", "caption": "Figure 16: A sampled error case in the action recognition task.", "description": "The figure shows an example where a multimodal large language model (MLLM) fails to correctly identify the action in a video based on the corresponding audio. The model incorrectly identifies the audio of someone running on a treadmill as the sound of playing basketball.", "section": "3.1 Deaf Test Tasks"}, {"figure_path": "https://arxiv.org/html/2412.02611/x17.png", "caption": "Figure 17: A sampled error case in the eating sound recognition task.", "description": "The figure shows an example where the Gemini 1.5 Pro model misidentifies the sound of eating juicy grapes as the sound of eating crispy chips.  The model correctly identifies the image (grapes), but incorrectly identifies the audio. This highlights a limitation in audio understanding within the model, specifically in distinguishing between similar sounds with different textures.", "section": "3.3 Data Curation Process"}, {"figure_path": "https://arxiv.org/html/2412.02611/x18.png", "caption": "Figure 18: A sampled error case in the speech sentiment analysis task.", "description": "This figure shows a case where the model incorrectly identifies the emotion conveyed in an audio clip.  The task is to match the audio (an angry voice) to one of four images representing different emotions. The model incorrectly selects an image depicting disgust, demonstrating a failure in accurately interpreting audio-based emotional cues. The image shows four options; a woman showing disgust, a man showing surprise, an eggplant emoji showing anger, and a sad face emoji showing sadness. The model chose the image of a woman with a disgusted face, even though the audio was of an angry voice.", "section": "4.3 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2412.02611/x19.png", "caption": "Figure 19: A sampled error case in the meme understanding task.", "description": "The figure shows an example where the model (Gemini 1.5 Pro) failed to answer a question about a meme because the content was mistakenly flagged for security reasons.  The question asked about the humor in a meme given an audio clip and a sequence of images. Gemini 1.5 Pro was unable to provide any answer. The correct answer involved the contrast between the calm audio and the cat's expressionless face in the meme images. This highlights the model's limitations in handling sensitive content and its inability to fully understand the nuances of humor in multimodal contexts.", "section": "Tone: Meme Understanding"}, {"figure_path": "https://arxiv.org/html/2412.02611/x20.png", "caption": "Figure 20: A sampled error case in the music sentiment analysis task.", "description": "This figure shows a case where the model incorrectly identifies the sentiment of cheerful music as sad. The model correctly identified the visual content of the image (a crying emoji face), but failed in audio recognition, highlighting its limitations in accurately understanding musical emotions.", "section": "4.3 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2412.02611/x21.png", "caption": "Figure 21: A sampled error case in the music genre classification task.", "description": "Gemini 1.5 Pro incorrectly classified the audio as country music instead of classical music, despite accurately identifying the image content.  This highlights the model's limitations in audio understanding and genre classification.", "section": "4.3 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2412.02611/x22.png", "caption": "Figure 22: A sampled error case in the dance and music matching task.", "description": "This figure shows a case where the Gemini 1.5 Pro model failed to correctly identify the audio that best matches the dance in a video.  The task was to select the audio clip that most accurately corresponds to the style and rhythm of the dance shown.  The model failed to answer, likely due to limitations in the model's ability to integrate visual and audio cues to make complex decisions about audio-visual synchronicity. The model's failure to answer highlights the challenges of multimodal understanding, even in relatively simple tasks. ", "section": "3. Case Study"}, {"figure_path": "https://arxiv.org/html/2412.02611/x23.png", "caption": "Figure 23: A sampled error case in the film and music matching task.", "description": "The figure shows an example where the Gemini 1.5 Pro model incorrectly matches a fast-paced, cheerful music clip with a scene from an action movie.  The model fails to recognize that the humorous tone of the audio, indicated by comical screams, is more characteristic of a comedy than an action film.", "section": "4.3 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2412.02611/x24.png", "caption": "Figure 24: A sampled error case in the music score matching task.", "description": "This figure showcases a case where Gemini 1.5 Pro misidentifies a music score.  The audio features slow-paced music with a sustained vocal at the end.  The model incorrectly identifies the audio as moderately paced with a swing feel and syncopated rhythm, leading to a mismatched score selection. The error highlights the model's limitations in accurately interpreting tempo, articulation, and the interplay of rhythmic and melodic elements in music.", "section": "3.3 Data Curation Process"}, {"figure_path": "https://arxiv.org/html/2412.02611/x25.png", "caption": "Figure 25: A sampled error case in the audio 3D angle estimation task.", "description": "This figure shows a sample error case in the audio 3D angle estimation task of the AV-Odyssey benchmark.  The task involves estimating the azimuth and elevation angles of a sound source relative to a person in an image.  The model incorrectly identifies the person and misinterprets spatial audio cues, leading to an inaccurate angle estimation. The correct and predicted answers are shown, highlighting the model's inability to properly integrate visual and audio information for spatial reasoning.", "section": "4.3 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2412.02611/x26.png", "caption": "Figure 26: A sampled error case in the audio distance estimation task.", "description": "The figure shows an example where the model fails to accurately estimate the distance of a sound source using audio and visual cues.  The model correctly identifies the visual elements but fails to integrate the spatial audio information from the 4-channel spatial audio recording, leading to an inaccurate distance estimation.  This highlights the model's limitations in multi-modal reasoning and its reliance on visual cues over more precise spatial audio information.", "section": "4.3 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2412.02611/x27.png", "caption": "Figure 27: A sampled error case in the audio time estimation task.", "description": "The figure shows a sample error from the audio time estimation task of the AV-Odyssey benchmark.  The task requires identifying the start and end times of an action in a video based solely on an accompanying audio clip. The example highlights a model's misidentification of the correct timeframe for a specific action (putting utensils in a drawer). The model incorrectly identified the timeframe based on the audio, demonstrating limitations in precise temporal alignment between audio and visual inputs.", "section": "3.1 Deaf Test Tasks"}, {"figure_path": "https://arxiv.org/html/2412.02611/x28.png", "caption": "Figure 28: A sampled error case in the audio-visual synchronization task.", "description": "The figure shows an example where a multimodal large language model (MLLM) fails to accurately synchronize audio and video. The task was to identify which audio clip best matches a given video. The model incorrectly chose an audio clip with random offsets, speed-ups, and slow-downs, demonstrating a lack of understanding in aligning events across different modalities.", "section": "4.3 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2412.02611/", "caption": "Figure 29: A sampled error case in the action sequencing task.", "description": "This figure shows a sample error case from the AV-Odyssey benchmark's action sequencing task.  Gemini 1.5 Pro incorrectly identified the order of actions based on the audio cues, indicating issues with both audio understanding and reasoning capabilities. The correct sequence is shown for comparison, highlighting the model's inability to accurately interpret temporal relationships between actions.", "section": "Time Tasks"}, {"figure_path": "https://arxiv.org/html/2412.02611/x30.png", "caption": "Figure 30: A sampled error case in the hallucination evaluation task.", "description": "The figure showcases a common mistake made by the Gemini 1.5 Pro model during the hallucination evaluation task.  The model incorrectly identifies a sitar as being present in an audio clip that actually only contains drums.  This highlights the model's tendency to hallucinate or falsely perceive elements not present in the input audio, demonstrating limitations in its audio understanding capabilities.", "section": "4.3. Error Analysis"}, {"figure_path": "https://arxiv.org/html/2412.02611/x31.png", "caption": "Figure 31: A sampled error case in the action prediction task.", "description": "The figure displays an example where a multimodal large language model (MLLM) incorrectly predicts the action a person is performing. The model is presented with an image of a person standing near a coffee container and an audio clip of sounds associated with the action. The MLLM incorrectly identifies the action as 'wrapping up coffee' due to errors in understanding the temporal relationship between the visual input and the audio clip.", "section": "Intricacy: Action Prediction"}, {"figure_path": "https://arxiv.org/html/2412.02611/x32.png", "caption": "Figure 32: A sampled error case in the action tracing task.", "description": "This figure shows a case where the model incorrectly identifies the action being performed in a video clip. The task is to determine what the person in the video is doing based on the audio and visual information. The image shows a person near a countertop holding a rag. The model incorrectly determines that the person is wiping the counter with the rag. However, the correct answer is that the person is rinsing the chopping board.", "section": "Intricacy: Action Tracing"}]