{"importance": "This paper reveals **LLMs' cultural biases** in math reasoning, highlighting the need for diverse training data and culturally nuanced evaluation. It prompts future research into **fairer, more robust AI** across cultures.", "summary": "LLMs falter on culturally adapted math problems, revealing a critical cultural bias.", "takeaways": ["LLMs struggle with math problems when cultural references are changed, even if the underlying mathematical structure remains constant.", "Smaller models exhibit greater performance drops compared to larger models when cultural references change.", "Cultural familiarity can enhance mathematical reasoning."], "tldr": "Large Language Models(LLMs) excel in many areas, but do they truly understand different cultures, or do they just reflect the cultural biases present in their training data? This paper explores whether LLMs can solve math problems when those problems are adapted to different cultural contexts. The researchers question whether LLMs can perform math reasoning when presented with math word problems that are adapted in different cultures. They changed cultural elements in problems and assessed how well LLMs could solve them. This paper finds that **cultural context greatly impacts LLMs' math abilities**. \n\nTo tackle this question, researchers created six synthetic cultural datasets based on the GSM8K benchmark, a standard test for LLMs' math skills. While keeping the math the same, they altered cultural details such as names, foods, and places to reflect different regions. Then, they tested 14 LLMs on these datasets to see how well they performed.The results showed that **LLMs struggle when math problems include unfamiliar cultural references**, even when the underlying math is unchanged. Smaller models had even more trouble than larger ones. Interestingly, exposure to relevant cultural contexts can improve mathematical reasoning.", "affiliation": "155mv Research Lab", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.18018/podcast.wav"}