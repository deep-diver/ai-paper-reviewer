[{"figure_path": "https://arxiv.org/html/2411.04999/x1.png", "caption": "Figure 1: An illustration of how our online dynamic spatio-semantic memory DynaMem responds to open vocabulary queries in a dynamic environment. During operation and exploration,\u00a0DynaMem keeps updating its semantic map in memory. DynaMem maintains a voxelized pointcloud representation of the environment, and updates with dynamic changes in the environment by adding and removing points.", "description": "Figure 1 illustrates DynaMem, an online dynamic spatio-semantic memory system.  It shows how DynaMem responds to open-vocabulary queries in a dynamic environment by continuously updating its internal 3D representation.  This representation is a voxelized point cloud that dynamically adapts to changes in the environment, such as objects moving, appearing, or disappearing. The figure shows a sequence of snapshots illustrating how the system updates its map, searches for objects, and performs actions based on natural language commands.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.04999/x2.png", "caption": "Figure 2: (Left) DynaMem keeps its memory stored in a sparse voxel grid with associated information at each voxel. (Right) Updating\u00a0DynaMem by adding new points to it, alongside the rules used to update the stored information.", "description": "Figure 2 is a two-part illustration detailing DynaMem's architecture and update mechanism.  The left panel shows DynaMem's core structure: a sparse 3D voxel grid. Each occupied voxel stores multiple pieces of information including its 3D coordinates, a count of observations, the ID of the source image, a semantic feature vector (from a Vision-Language Model like CLIP), and the time of its last observation. The right panel illustrates how DynaMem updates when new points are detected.  It shows the addition of new points to the voxel grid and the rules governing updates to the existing data within each voxel, including recalculating the feature vector and timestamp.", "section": "3.2. Dynamic 3D Voxel Map"}, {"figure_path": "https://arxiv.org/html/2411.04999/x3.png", "caption": "Figure 3: A high-level, 2D depiction of how adding and removing voxels from the voxel map works. New voxels are included which are in the RGB-D cameras view frustum, and old voxels that should block the view frustum but does not are removed from the map.", "description": "Figure 3 illustrates the dynamic update process of the 3D voxel map used in DynaMem.  It shows how new voxels representing observed objects are added to the map, but only if they fall within the camera's view frustum.  Conversely, old voxels that are no longer observed, or that are obstructed by more recently observed objects which should block them from the view, are removed, reflecting changes in the environment over time. This dynamic update ensures that the map always reflects the current state of the environment, dealing effectively with object movement, occlusion and removal.", "section": "3.2. Dynamic 3D Voxel Map"}, {"figure_path": "https://arxiv.org/html/2411.04999/x4.png", "caption": "Figure 4: Querying\u00a0DynaMem with a natural language query. First, we find the voxel with the highest alighnment to the query. Next, we find the latest image of that voxel, and query with an open-vocabulary object detector to confirm the object location or abstain.", "description": "This figure illustrates the process of querying DynaMem, a dynamic spatio-semantic memory, using a natural language query.  The system first identifies the voxel in its 3D voxel grid that best matches the query. Then, it retrieves the most recent image associated with that voxel. Finally, an open-vocabulary object detector is used on that image to verify the presence of the queried object and provide its 3D coordinates or abstain if the object isn't found.  This process demonstrates how DynaMem handles object localization requests in a dynamic environment by combining voxel-based spatial information with image-based object detection.", "section": "3.3. Querying DynaMem for Object Localization"}, {"figure_path": "https://arxiv.org/html/2411.04999/x5.png", "caption": "Figure 5: The prompting system for querying multimodal LLMs such as GPT-4o or Gemini-1.5 for the image index for an object query.", "description": "Figure 5 illustrates the process of querying a multimodal large language model (LLM), such as GPT-4 or Gemini-1.5, to identify the index of the image containing a target object.  The prompt carefully instructs the LLM to focus solely on providing the image index without adding any extraneous information or context.  If the model cannot locate the object in any of the provided images, it is prompted to return only the object name and the word \"None\" for the image index.  The figure shows an example prompt and response, emphasizing the structure and clarity needed for effective LLM interaction in this specific task of visual grounding.", "section": "3.3. Querying DynaMem for Object Localization"}, {"figure_path": "https://arxiv.org/html/2411.04999/x6.png", "caption": "Figure 6: Real robot experiments in three different environments: kitchen, game room, and meeting room. In each environment, we modify the environment thrice and run 10 pick-and-drop queries.", "description": "Figure 6 shows three real-world environments used for robotic manipulation experiments: a kitchen, a game room, and a meeting room.  In each setting, the researchers altered the environment's arrangement three times, and during each alteration, they conducted ten object pick-and-drop tasks using the robot. This setup allowed them to evaluate the robot's ability to perform manipulation tasks in dynamic environments.  The image provides a panoramic view of each environment.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.04999/x7.png", "caption": "Figure 7: Statistics of failure, broken down by failure modes, in our real robot experiments in the lab and in home environments. Statistics are collected over three environments and 30 open-vocabulary pick-and-drop queries for the lab experiments, and two environments and 17 pick-and-drop queries for the home environments, on objects whose locations change over time.", "description": "Figure 7 presents a detailed breakdown of the failure modes encountered during real-world experiments using the DynaMem system for open-vocabulary mobile manipulation.  The experiments were conducted in both lab and home environments. The lab experiments involved three different environments and 30 open-vocabulary pick-and-drop queries, while the home experiments used two environments and 17 queries.  Crucially, all experiments tested the system's ability to handle objects whose locations changed over time.  The figure visually represents the frequency of each failure type, offering insights into the system's weaknesses and areas for potential improvement.  This allows for a precise quantitative analysis of the system's reliability and robustness in dynamic real-world settings.", "section": "4. Experiments"}]