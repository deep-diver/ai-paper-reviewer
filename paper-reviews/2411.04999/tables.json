[{"content": "| Query type | Variant | Success rate |\n|---|---|---|\n| Human | (average over five participants) | **81.9%** |\n| VLM-feature | default (adding and removing points) | **70.6%** |\n|  | only adding points | 67.8% |\n|  | no OWL-v2 cross-check | 59.2% |\n|  | no similarity thresholding | 66.8% |\n| mLLM-QA | default (Gemini Pro 1.5) | **67.3%** |\n|  | Gemini Pro 1.5, no voxelmap filtering | 66.8% |\n|  | Gemini Flash 1.5 | 63.5% |\n| Hybrid | VLM-feature \u2192 mLLM (k=3) | **74.5%** |", "caption": "Table 1: Ablating the design choices for our query methods for DynaMem on the offline DynaBench benchmark. We also present results from five human participants to ground the performances.", "description": "This table presents an ablation study on the design choices of DynaMem's query methods, evaluated on the offline DynaBench benchmark.  It compares the success rates of different variations of the VLM-feature and mLLM-QA based query methods, including variations in the point-adding/removing strategies, cross-checking with an object detector, and use of voxelmap filtering. For comparison, it also includes the average success rate achieved by five human participants on the same task. This allows for a quantitative analysis of the impact of each design choice on the overall performance of DynaMem.", "section": "4.3. Ablations on an Offline Benchmark"}]