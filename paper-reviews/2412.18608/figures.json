[{"figure_path": "https://arxiv.org/html/2412.18608/x2.png", "caption": "Figure 1: \nWe introduce PartGen, a pipeline that generates compositional 3D objects similar to a human artist.\nIt can start from text, an image, or an existing, unstructured 3D object.\nIt consists of a multi-view diffusion model that identifies plausible parts automatically and another that completes and reconstructs them in 3D, accounting for their context, i.e., the other parts, to ensure that they fit together correctly. Additionally, PartGen enables 3D part editing based on text instructions, enhancing flexibility and control in 3D object creation.", "description": "PartGen is a system for creating compositional 3D objects.  It starts with text, an image, or a pre-existing 3D model.  A multi-view diffusion model automatically identifies and segments the object into its constituent parts. A second diffusion model then completes and reconstructs each part in 3D, considering the context of the other parts for accurate assembly. The system also allows for text-based 3D part editing, offering greater control over the final object's creation.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.18608/x3.png", "caption": "Figure 2: Overview of PartGen. Our method begins with text, single images, or existing 3D objects to obtain an initial grid view of the object. This view is then processed by a diffusion-based segmentation network to achieve multi-view consistent part segmentation. Next, the segmented parts, along with contextual information, are input into a multi-view part completion network to generate a fully completed view of each part. Finally, a pre-trained reconstruction model generates the 3D parts.", "description": "PartGen starts with text, an image, or a 3D model as input.  It first generates a multi-view grid image of the object. A diffusion-based segmentation network then identifies the object's constituent parts, ensuring consistency across multiple views. Each segmented part, along with its context within the entire object, is fed into a multi-view part completion network. This network completes each part's view, handling occlusions and generating missing information. Finally, a pre-trained 3D reconstruction model uses these completed views to generate the final 3D parts that make up the object.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.18608/x4.png", "caption": "Figure 3: Training data.\nWe obtain a dataset of 3D objects decomposed into parts from assets created by artists.\nThese come \u2018naturally\u2019 decomposed into parts according to the artist\u2019s design.", "description": "The figure displays examples from the dataset used to train the PartGen model.  The dataset consists of 3D objects created by artists and already decomposed into meaningful parts.  The decomposition is natural, reflecting how a human artist would separate the object into its constituent components.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.18608/x5.png", "caption": "Figure 4: Examples of automatic multi-view part segmentations.\nBy running our method several times, we obtain different segmentations, covering the space of artist intents.", "description": "This figure displays multiple segmentations of the same 3D object generated by the PartGen model.  Each segmentation shows a different plausible decomposition of the object into meaningful parts, demonstrating the model's ability to capture the variability inherent in how a human artist might segment a similar object. Running the PartGen model multiple times produces a range of segmentations, illustrating the model's inherent stochasticity and its capacity to generate various interpretations of an object's constituent parts.", "section": "3.2. Multi-view part segmentation"}, {"figure_path": "https://arxiv.org/html/2412.18608/x6.png", "caption": "Figure 5: Qualitative results of part completion. The images with blue borders are the inputs. Our algorithm produces various plausible outputs across different runs. Even if given an empty part, PartGen attempts to generate internal structures inside the object, such as sand or inner wheels.", "description": "Figure 5 presents qualitative results demonstrating PartGen's part completion capabilities.  The input images (with blue borders) show partially visible or occluded parts of 3D objects. PartGen processes these incomplete parts and generates various plausible completed versions. Notably, even when the input is an 'empty' part, PartGen intelligently hallucinates internal structures such as sand or the inner workings of a wheel, showcasing its ability to fill in missing information contextually.", "section": "4.2 Part Completion and Reconstruction"}, {"figure_path": "https://arxiv.org/html/2412.18608/x7.png", "caption": "Figure 6: Examples of applications. PartGen can effectively generate or reconstruct 3D objects with meaningful and realistic parts in different scenarios: a) Part-aware text-to-3D generation; b) Part-aware image-to-3D generation; c) 3D decomposition.", "description": "Figure 6 showcases PartGen's versatility across various applications.  Panel (a) demonstrates part-aware text-to-3D generation, where textual descriptions are used to create 3D objects composed of distinct, meaningful parts. Panel (b) shows part-aware image-to-3D generation, reconstructing 3D objects from input images while preserving and enhancing their component parts. Finally, panel (c) illustrates 3D decomposition, where an existing unstructured 3D model is intelligently broken down into its constituent parts. Each panel provides examples of the input and the resulting 3D models, highlighting the realistic and meaningful part compositions achieved by PartGen.", "section": "4. Applications"}, {"figure_path": "https://arxiv.org/html/2412.18608/x8.png", "caption": "Figure 7: 3D part editing. We can edit the appearance and shape of the 3D objects with text prompt.", "description": "Figure 7 showcases the application of PartGen for 3D part editing.  It demonstrates the model's ability to modify the appearance and shape of individual parts within a 3D object using only text-based instructions. Three examples are shown, highlighting how different parts (e.g., a t-shirt, hat, cup) can be altered. This illustrates the power of PartGen's part-level control to enable fine-grained manipulation of 3D assets.", "section": "4. Applications"}, {"figure_path": "https://arxiv.org/html/2412.18608/x9.png", "caption": "Figure 8: 3D part editing and captioning examples. The top section illustrates training examples for the editing network, where a mask, a masked image, and text instructions are provided as conditioning to the diffusion network, which fills in the part based on the given textual input. The bottom section demonstrates the input for the part captioning pipeline. Here, a red circle and highlights are used to help the large vision-language model (LVLM) identify and annotate the specific part.", "description": "Figure 8 demonstrates 3D part editing using a diffusion model. The top half shows examples of training data for this model.  Each example includes a 3D object, a mask indicating the part to be edited, the masked image (showing only the part to be edited), and text instructions describing the desired changes.  The model learns to fill in the masked area based on the text and the visible context. The bottom half shows how the part captioning pipeline works. A red circle highlights the part for which a caption is needed, guiding a Large Vision-Language Model (LVLM) to generate a suitable caption for that specific part.", "section": "A. Implementation Details"}, {"figure_path": "https://arxiv.org/html/2412.18608/x10.png", "caption": "Figure 9: Recall curve of different methods. Our method achieve better performance comparing with SAM2 and its variants.", "description": "This figure compares the performance of PartGen against several baselines on the task of 3D part segmentation.  The y-axis shows the recall, which is the percentage of correctly identified parts. The x-axis represents the number of top predictions considered (i.e. ranking). The curves show that PartGen (with different sampling strategies: 1, 5, and 10 samples) significantly outperforms the SAM2 baseline and its variants, demonstrating its ability to accurately segment 3D objects into meaningful parts.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.18608/x11.png", "caption": "Figure 10: More examples. Additional examples illustrate that PartGen can process various modalities and effectively generate or reconstruct 3D objects with distinct parts.", "description": "Figure 10 presents supplementary examples showcasing PartGen's capabilities.  It demonstrates the model's ability to process diverse input types (text, images, or existing 3D scans) and generate or reconstruct high-quality 3D objects composed of distinct, meaningful parts. The examples highlight the model's versatility and effectiveness in handling complex 3D structures.", "section": "Additional Examples"}, {"figure_path": "https://arxiv.org/html/2412.18608/x12.png", "caption": "Figure 11: Iteratively adding parts. We show that users can iteratively add parts and combine the results of PartGen pipeline.", "description": "Figure 11 demonstrates the iterative aspect of PartGen.  It shows how users can incrementally add new parts to an existing 3D object. The figure showcases a sequence of images, beginning with a simple base object and gradually adding more complex parts. Each step displays the newly added part individually, along with the composite object containing all previously generated parts. This showcases PartGen's ability to combine results from multiple runs of the pipeline, allowing for flexible and layered 3D model creation.", "section": "4. Applications"}, {"figure_path": "https://arxiv.org/html/2412.18608/x13.png", "caption": "Figure 12: Failure Cases. (a) Multi-view grid generation failure, where the generated views lack 3D consistency. (b) Segmentation failure, where semantically distinct parts are incorrectly grouped together. (c) Reconstruction model failure, where the complex geometry of the input leads to inaccuracies in the depth map.", "description": "Figure 12 illustrates three failure scenarios encountered during the PartGen pipeline. (a) shows a failure in the multi-view grid generation stage, where inconsistencies in the generated views hinder accurate 3D reconstruction.  The generated views of the object lack 3D consistency, resulting in an inaccurate representation. (b) demonstrates a segmentation failure where semantically distinct parts of the object are incorrectly grouped together. The model fails to accurately segment the object into meaningful and separate parts. (c) presents a reconstruction model failure. In this case, the complexity of the object's geometry leads to inaccuracies in the depth map, impacting the quality of the final 3D reconstruction. The reconstruction model struggles to accurately represent the intricate details of the object due to its complexity, resulting in errors in the generated depth map.", "section": "Failure Case"}]