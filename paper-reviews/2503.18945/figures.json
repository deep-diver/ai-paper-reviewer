[{"figure_path": "https://arxiv.org/html/2503.18945/x2.png", "caption": "Figure 1: An overview of Aether, trained entirely on synthetic data. The figure highlights its three key capabilities: 4D reconstruction, action-conditioned 4D prediction, and visual planning, all demonstrated on unseen real-world data. The 4D reconstruction examples are derived from MovieGen\u00a0[48] and Veo 2\u00a0[62] generated videos, while the action-conditioned prediction uses an observation image from a university classroom. The visual planning example utilizes observation and goal images from an office building. Better viewed when zoomed in. Additional visualizations can be found in our website.", "description": "Figure 1 provides a visual overview of the Aether model, showcasing its core functionalities trained exclusively on synthetic data.  It demonstrates the model's ability to perform 4D reconstruction (using data from MovieGen [48] and Veo 2 [62]), action-conditioned 4D prediction (with input from a university classroom), and goal-conditioned visual planning (with data from an office environment). Notably, these capabilities are shown applied to real-world data never seen during the training process, highlighting the model's impressive generalization abilities. The image is best viewed at a larger scale for better detail.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.18945/x3.png", "caption": "Figure 2: Some visualization results of data annotated through our pipeline. Better viewed when zoomed in.", "description": "Figure 2 presents visualization results from the automatic camera annotation pipeline.  The pipeline processes synthetic RGB-D videos to generate accurate camera pose annotations.  The images show examples of various scenes (indoor/outdoor, static/dynamic) and demonstrate the pipeline's ability to accurately annotate camera parameters and dynamic masks, even in challenging conditions.  Zooming in is recommended for a clearer view of the details.", "section": "2. 4D Synthetic Data Annotation Pipeline"}, {"figure_path": "https://arxiv.org/html/2503.18945/x4.png", "caption": "Figure 3: Our robust automatic camera annotation pipeline.", "description": "This figure illustrates the four-stage pipeline used for automatically annotating camera parameters (both intrinsic and extrinsic) from synthetic RGB-D videos.  Stage 1, Object-Level Dynamic Masking, utilizes semantic segmentation to identify and separate dynamic regions from static ones, crucial for accurate camera estimation. This is followed by Video Slicing (Stage 2), which segments long videos into shorter, temporally consistent clips to improve efficiency and robustness. Stage 3, Coarse Camera Estimation, employs DroidCalib to provide an initial estimation of camera parameters. Finally, Stage 4, Tracking-Based Camera Refinement with Bundle Adjustment, refines the initial estimate using CoTracker3 for long-term correspondence and bundle adjustment techniques to minimize reprojection errors.  The resulting output is a fully annotated dataset with precise camera parameters for each frame.", "section": "2. 4D Synthetic Data Annotation Pipeline"}]