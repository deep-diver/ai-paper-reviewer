[{"content": "| Method | Flickr30K R@1 | Flickr30K R@10 | COCO R@1 | COCO R@10 | nocaps R@1 | nocaps R@10 | Flickr30K R@1 | Flickr30K R@10 | COCO R@1 | COCO R@10 | nocaps R@1 | nocaps R@10 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| CLIP (ViT-B) [36] | 58.8 | 89.8 | 30.5 | 66.8 | 46.8 | 85.1 | 77.8 | 98.2 | 51.0 | 83.5 | 67.3 | 95.6 |\n| CLIP (ViT-L) [36] | 67.3 | 93.3 | 37.0 | 71.5 | 48.6 | 85.7 | 87.2 | 99.4 | 58.1 | 87.8 | 70.0 | 96.2 |\n| BLIP (ViT-L) [28] | 70.0 | 95.2 | 48.4 | 83.2 | 62.3 | 93.4 | 75.5 | 97.7 | 63.5 | 92.5 | 72.1 | 97.7 |\n| BLIP2 (ViT-L) [29] | 74.5 | 97.2 | 50.0 | 86.1 | 63.0 | 93.8 | 86.1 | 99.4 | 63.0 | 93.1 | 74.4 | 98.3 |\n| OpenCLIP (ViT-G/14) [37] | 77.8 | 96.9 | 48.8 | 81.5 | 63.7 | 93.2 | 91.5 | 99.6 | 66.3 | 91.8 | 81.0 | 98.7 |\n| OpenCLIP (ViT-BigG/14) [37] | 79.5 | 97.1 | 51.3 | 83.0 | 65.1 | 93.5 | 92.9 | 97.1 | 67.3 | 92.6 | 82.3 | 98.8 |\n| EVA-02-CLIP (ViT-E/14+) [38] | 78.8 | 96.8 | 51.1 | 82.7 | 64.5 | 92.9 | 93.9 | 99.8 | 68.8 | 92.8 | 83.0 | 98.9 |\n| EVA-CLIP (8B) [39] | 80.3 | 97.2 | 52.0 | 82.9 | 65.3 | 93.2 | 94.5 | 99.7 | 70.1 | 93.1 | 83.5 | 98.6 |\n| EVA-CLIP (18B) [39] | 83.3 | 97.9 | 55.6 | 85.2 | 69.3 | 94.8 | 95.3 | 99.8 | 72.8 | 94.2 | 85.6 | 99.2 |\n| LLaVA-1.5-7B [34] | 59.6 | 89.3 | 34.4 | 69.6 | 46.9 | 83.3 | 65.6 | 92.3 | 35.6 | 70.5 | 52.1 | 88.1 |\n| E5-V (LLaVA-Next-8B) [23] | 79.5 | 97.6 | 52.0 | 84.7 | 65.9 | 94.3 | 88.2 | 99.4 | 62.0 | 89.7 | 74.9 | 98.3 |\n| E5-V (LLaVA-1.5-7B) [23] | 76.7 | 96.9 | 48.2 | 82.1 | 62.0 | 93.0 | 86.6 | 99.0 | 57.4 | 88.4 | 71.9 | 97.0 |\n| VladVA (Ours) (LLaVA-1.5-7B) | 85.0 | 98.5 | 59.0 | 88.6 | 72.3 | 96.5 | 94.3 | 99.9 | 72.9 | 94.4 | 85.7 | 99.5 |", "caption": "Table 1: Zero-shot text-image retrieval accuracy on Flickr30K, COCO and nocaps.", "description": "This table presents the zero-shot performance of different vision-language models on three image-text retrieval benchmarks: Flickr30K, COCO, and nocaps.  Zero-shot indicates that the models were evaluated without any additional fine-tuning specific to these datasets. The results are reported as Recall@1 and Recall@10, which represent the percentage of times the correct image/text was among the top 1 and top 10 retrieval results, respectively.  The table allows comparison of various models' capabilities in image-to-text and text-to-image retrieval tasks.", "section": "4. Experiments"}, {"content": "| Method | Params (B) | Replace Object | Replace Attribute | Replace Relation | Replace Object | Swap Attribute | Swap Object | Add Attribute | Add Object |\n|---|---|---|---|---|---|---|---|---|---|\n| Human | \u2013 | 100 | 99 | 97 | 99 | 100 | 99 | 99 |  |\n| CLIP (ViT-B) [36] | 0.15 | 90.9 | 80.1 | 69.2 | 61.4 | 64.0 | 77.2 | 68.8 |  |\n| CLIP (ViT-L) [36] | 0.43 | 94.1 | 79.2 | 65.2 | 60.2 | 62.3 | 78.3 | 71.5 |  |\n| BLIP (ViT-L) [28] | 0.23 | 96.5 | 81.7 | 69.1 | 66.6 | 76.8 | 92.0 | 85.1 |  |\n| BLIP2 (ViT-L) [29] | 1.17 | 97.6 | 81.7 | 77.8 | 62.1 | 65.5 | 92.4 | 87.4 |  |\n| OpenCLIP (ViT-G/14) [37] | 1.37 | 95.8 | 85.0 | 72.4 | 63.0 | 71.2 | 91.5 | 82.1 |  |\n| OpenCLIP (ViT-BigG/14) [37] | 2.54 | 96.6 | 87.9 | 74.9 | 62.5 | 75.2 | 92.2 | 84.5 |  |\n| EVA-02-CLIP (ViT-E/14+) [38] | 5.04 | 97.1 | 88.5 | 74.2 | 67.3 | 74.1 | 91.8 | 83.9 |  |\n| EVA-CLIP (8B) [39] | 8.22 | 96.4 | 86.6 | 74.8 | 66.1 | 74.6 | 91.3 | 82.0 |  |\n| EVA-CLIP (18B) [39] | 18.3 | 97.5 | 88.8 | 76.1 | 65.3 | 76.0 | 92.4 | 85.0 |  |\n| NegCLIP [47] | 0.15 | 92.7 | 85.9 | 76.5 | 75.2 | 75.4 | 88.8 | 82.8 |  |\n| LLaVA-1.5-7B [34] | 7.06 | 88.0 | 81.6 | 76.1 | 60.9 | 58.8 | 67.0 | 62.4 |  |\n| E5-V (LLaVA-Next-8B) [23] | 8.36 | 96.7 | 89.5 | 85.3 | 75.0 | 70.1 | 89.2 | 83.5 |  |\n| E5-V (LLaVA-1.5-7B) [23] | 7.06 | 95.8 | 86.6 | 81.6 | 62.9 | 64.0 | 93.5 | 88.0 |  |\n| VladVA (Ours) (LLaVA-1.5-7B) | 7.06 | **98.1** | **92.1** | **86.8** | **79.0** | **82.9** | **95.2** | **95.8** |  |", "caption": "Table 2: Comparison with state-of-the-art on the SugarCrepe compositionality benchmark.", "description": "This table presents a comparison of the performance of various state-of-the-art vision-language models on the SugarCrepe compositionality benchmark.  The benchmark evaluates how well models understand the compositional nature of language and vision, by testing their ability to reason about relationships between objects and attributes in images.  The table shows the number of parameters (in billions) for each model, as well as their performance across three specific compositionality tests: object replacement, attribute replacement, and relation replacement.  Performance is expressed in terms of accuracy.  The purpose is to show how well the proposed VladVA model performs relative to the existing leading models in the field.", "section": "4. Experiments"}, {"content": "| Method | Params | Swap Object |  | Swap Attribute |  | Replace Object |  | Replace Attribute |  | Replace Relation |  |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| (B) | ITT | TOT | ITT | TOT | ITT | TOT | ITT | TOT | ITT | TOT |\n| Human | \u2013 | 100.00 | 96.7 | 96.7 | 93.3 | 100.00 | 97.00 | 100.00 | 98.3 | 100.00 | 96.7 |\n| CLIP (ViT-B) [36] | 0.15 | 45.2 | 19.7 | 45.2 | 33.0 | 86.8 | 83.7 | 65.6 | 59.1 | 56.3 | 38.62 |\n| CLIP (ViT-L) [36] | 0.43 | 46.0 | 14.5 | 44.5 | 28.7 | 92.0 | 81.3 | 68.8 | 56.3 | 53.4 | 39.1 |\n| BLIP (ViT-L) [28] | 0.23 | 46.8 | 29.8 | 60.1 | 52.5 | 92.6 | 89.1 | 71.7 | 75.0 | 56.8 | 57.7 |\n| BLIP2 (ViT-L) [29] | 1.17 | 37.9 | 39.5 | 51.9 | 55.4 | 94.8 | 96.9 | 73.2 | 86.5 | 65.1 | 69.6 |\n| OpenCLIP (ViT-G/14) [37] | 1.37 | 40.7 | 27.4 | 54.2 | 49.6 | 93.1 | 89.4 | 72.5 | 73.1 | 57.6 | 51.4 |\n| OpenCLIP (ViT-BigG/14) [37] | 2.54 | 48.8 | 28.2 | 57.7 | 52.4 | 94.2 | 90.5 | 76.4 | 72.6 | 59.4 | 53.6 |\n| EVA-02-CLIP (ViT-E/14+) [38] | 5.04 | 48.4 | 28.2 | 56.3 | 49.4 | 94.5 | 88.9 | 76.3 | 70.6 | 59.4 | 49.4 |\n| EVA-CLIP (8B) [39] | 8.22 | 43.6 | 25.4 | 55.2 | 46.9 | 93.7 | 85.8 | 73.4 | 67.9 | 59.7 | 49.2 |\n| EVA-CLIP (18B) [39] | 18.3 | 45.2 | 25.4 | 55.5 | 47.6 | 94.1 | 85.1 | 77.0 | 69.8 | 60.4 | 47.8 |\n| NegCLIP [47] | 0.15 | 55.3 | 34.7 | 58.0 | 56.5 | 89.5 | 94.5 | 69.4 | 76.3 | 52.3 | 51.6 |\n| CLIP-SVLC [14] | 0.15 | 43.0 | 18.9 | 48.4 | 34.6 | 80.9 | 91.6 | 57.0 | 66.9 | 47.3 | 51.3 |\n| BLIP-SGVL [18] | 0.15 | 13.2 | \u2013 | 38.8 | \u2013 | 53.8 | \u2013 | 34.4 | \u2013 | 30.7 | \u2013 |\n| LLaVA-1.5-7B [34] | 7.06 | 23.8 | 30.7 | 28.0 | 29.5 | 58.1 | 63.0 | 46.8 | 58.1 | 52.3 | 63.4 |\n| E5-V (LLaVA-Next-8B) [23] | 8.36 | 50.8 | 48.4 | 49.7 | 56.9 | 93.1 | 97.6 | 76.1 | 87.1 | 74.7 | 84.4 |\n| E5-V (LLaVA-1.5-7B) [23] | 7.06 | 39.5 | 42.3 | 40.7 | 48.5 | 89.7 | 94.6 | 71.7 | 86.4 | 72.0 | 81.5 |\n| VladVA (Ours) (LLaVA-1.5-7B) | 7.06 | 56.1 | 36.7 | 63.0 | 62.5 | 95.0 | 93.0 | 78.2 | 82.3 | 71.1 | 66.3 |", "caption": "Table 3: Comparison with state-of-the-art on the SugarCrepe++ compositionality benchmark.", "description": "This table presents a comparison of various state-of-the-art models on the SugarCrepe++ benchmark, which assesses the compositionality of vision-language models.  It shows the performance of different models across several sub-tasks, specifically those involving object and attribute replacement and swapping, and relation replacement.  The results highlight the strengths and weaknesses of each model in handling different types of compositional challenges within image-text understanding.", "section": "4. Experiments"}, {"content": "| Method | AR | SugarCrepe loss | SugarCrepe Replace | SugarCrepe Swap | SugarCrepe Add | Flickr30k T2I | Flickr30k I2T |\n|---|---|---|---|---|---|---|---| \n| LLaVA-1.5-7B | \u2717 | 81.9 | 59.8 | 64.7 | 59.6 | 65.6 |\n| + soft-prom. | \u2717 | 86.4 | 66.9 | 89.3 | 76.7 | 91.7 |\n| + adapter-ft. | \u2717 | 87.0 | 69.8 | 88.8 | 79.1 | 91.4 |\n| + adapter-ft. + soft-prom. | \u2717 | 87.1 | 72.0 | 88.6 | 79.6 | **92.9** |\n| + adapter-ft. + soft-prom. | \u2713 | **89.5** | **75.5** | **89.5** | **80.6** | 91.8 |", "caption": "Table 4: Impact of adaptation components and AR loss. All models are trained on 1M samples.", "description": "This table presents an ablation study analyzing the impact of different components in the VladVA model, trained on a smaller subset of 1 million samples.  It shows the results for various configurations, such as using soft prompting, LoRA adapters, and/or the autoregressive (AR) loss, on two image-text retrieval benchmarks: SugarCrepe and Flickr30k. The purpose is to determine the contribution of each component and whether the autoregressive loss helps improve the model's discriminative capabilities.", "section": "3.2 Discriminative fine-tuning of LVLMs: from generation to discrimination"}, {"content": "| Method | Flickr30K R@1 | Flickr30K R@10 | COCO R@1 | COCO R@10 | nocaps R@1 | nocaps R@10 | Flickr30K R@1 | Flickr30K R@10 | COCO R@1 | COCO R@10 | nocaps R@1 | nocaps R@10 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **image retrieval** |  |  |  |  |  |  |  |  |  |  |  |  |\n| **text retrieval** |  |  |  |  |  |  |  |  |  |  |  |  |\n| Qwen2-VL-2B [43] | 54.1 | 86.0 | 32.4 | 68.2 | 41.2 | 80.1 | 59.6 | 89.2 | 35.3 | 71.8 | 54.0 | 90.3 |\n| VladVA (Ours) (<span class=\"ltx_text ltx_font_typewriter\">Qwen2-VL-2B</span>) | **80.4** | **97.3** | **52.5** | **84.4** | **68.3** | **94.9** | **93.7** | **99.9** | **71.9** | **93.9** | **86.0** | **99.4** |\n| LLaVA-1.5-7B [34] | 59.6 | 89.3 | 34.4 | 69.6 | 46.9 | 83.3 | 65.6 | 92.3 | 35.6 | 70.5 | 52.1 | 88.1 |\n| VladVA (Ours) (<span class=\"ltx_text ltx_font_typewriter\">LLaVA-1.5-7B</span>) | **85.0** | **98.5** | **59.0** | **88.6** | **72.3** | **96.5** | **94.3** | **99.9** | **72.9** | **94.4** | **85.7** | **99.5** |\n| LLaVA-1.5-13B [34] | 61.7 | 90.4 | 37.9 | 74.1 | 48.4 | 85.0 | 66.9 | 93.6 | 35.3 | 71.0 | 48.0 | 87.9 |\n| VladVA (Ours) (<span class=\"ltx_text ltx_font_typewriter\">LLaVA-1.5-13B</span>) | **85.6** | **98.6** | **58.2** | **88.4** | **74.0** | **96.6** | **94.5** | **99.8** | **75.0** | **95.6** | **85.4** | **99.6** |", "caption": "Table 5: Zero-shot text-image retrieval accuracy on Flickr30K, COCO and nocaps.", "description": "This table presents the zero-shot performance of various models on three benchmark datasets: Flickr30K, COCO, and nocaps.  It shows the recall@1 and recall@10 scores for both image retrieval (given a text query, how accurately the model retrieves the correct image) and text retrieval (given an image, how accurately the model retrieves the correct text description). The results illustrate the relative effectiveness of different models in zero-shot image-text matching tasks.", "section": "4. Experiments"}, {"content": "| Method | Params | Replace Object | Replace Attribute | Replace Relation | Swap Object | Swap Attribute | Add Object | Add Attribute |\n|---|---|---|---|---|---|---|---|---|\n| Qwen2-VL-2B [43] | 2.21 | 89.9 | 72.0 | 75.0 | 56.1 | 56.1 | 73.2 | 70.1 |\n| VladVA (Ours) (<span class=\"ltx_text ltx_font_typewriter\">Qwen2-VL-2B</span>) | 2.21 | **97.9** | **89.7** | **81.5** | **76.5** | **82.6** | **93.6** | **95.4** |\n| LLaVA-1.5-7B [34] | 7.06 | 88.0 | 81.6 | 76.1 | 60.9 | 58.8 | 67.0 | 62.4 |\n| VladVA (Ours) (<span class=\"ltx_text ltx_font_typewriter\">LLaVA-1.5-7B</span>) | 7.06 | **98.1** | **92.1** | **86.8** | **79.0** | **82.9** | **95.2** | **95.8** |\n| LLaVA-1.5-13B [34] | 13.35 | 90.0 | 80.6 | 76.3 | 71.8 | 61.9 | 69.3 | 59.1 |\n| VladVA (Ours) (<span class=\"ltx_text ltx_font_typewriter\">LLaVA-1.5-13B</span>) | 13.35 | **98.1** | **93.9** | **89.8** | **81.1** | **86.0** | **95.2** | **97.0** |", "caption": "Table 6: Zero-shot results on SugarCrepe compositionality benchmark.", "description": "This table presents the zero-shot performance of various vision-language models on the SugarCrepe compositionality benchmark.  SugarCrepe tests the models' ability to understand and generate captions correctly in scenarios with varying degrees of compositional complexity (e.g., replacing, swapping, or adding objects or attributes). The results are broken down by the type of compositionality challenge and measure the model's accuracy for both image-to-text (ITT) and text-to-text (TOT) tasks, illustrating the relative strengths and weaknesses of different models in handling compositional language.", "section": "4. Experiments"}, {"content": "| Method | Params | Swap Object |  | Swap Attribute |  | Replace Object |  | Replace Attribute |  | Replace Relation |  | \n|---|---|---|---|---|---|---|---|---|---|---|---| \n| Qwen2-VL-2B [43] | 2.21 | 32.7 | 27.8 | 30.5 | 25.3 | 73.6 | 65.9 | 46.8 | 43.0 | 57.6 | **58.3** | \n| VladVA (Ours) (Qwen2-VL-2B) | 2.21 | **50.8** | **33.5** | **60.4** | **48.2** | **93.7** | **93.8** | **74.8** | **77.5** | **63.6** | 57.4 | \n| LLaVA-1.5-7B [34] | 7.06 | 23.8 | 30.7 | 28.0 | 29.5 | 58.1 | 63.0 | 46.8 | 58.1 | 52.3 | 63.4 | \n| VladVA (Ours) (LLaVA-1.5-7B) | 7.06 | **56.1** | 36.7 | **63.0** | **62.5** | **95.0** | 93.0 | **78.2** | 82.3 | 71.1 | 66.3 | \n| LLaVA-1.5-13B [34] | 13.35 | 35.5 | 32.3 | 30.2 | 32.4 | 68.7 | 66.8 | 44.8 | 43.1 | 52.3 | 55.6 | \n| VladVA (Ours) (LLaVA-1.5-13B) | 13.35 | **55.2** | **38.3** | **65.6** | **60.6** | **94.5** | **92.5** | **80.7** | **81.1** | **73.2** | **66.4** | ", "caption": "Table 7: Zero-shot results on the SugarCrepe++ compositionality benchmark.", "description": "This table presents the zero-shot performance of various vision-language models on the SugarCrepe++ benchmark. SugarCrepe++ is a challenging dataset designed to evaluate the compositionality capabilities of these models by testing their ability to correctly interpret images and text with various compositional manipulations, including object, attribute, and relation replacements and swaps. The results show the accuracy of each model across different compositionality tasks, enabling a detailed comparison of their performance in handling complex language and vision interactions.", "section": "4. Experiments"}, {"content": "| Model | Image | Text | Group |\n|---|---|---|---|\n| CLIP (ViT-B) [36] | 10.5 | 25.0 | 7.3 |\n| CLIP (ViT-L) [36] | 12.3 | 27.5 | 8.3 |\n| BLIP (ViT-L) [28] | 10.0 | 30.5 | 7.8 |\n| BLIP2 (ViT-L) [29] | 10.5 | 29.5 | 8.5 |\n| OpenCLIP (ViT-G/14) [37] | 12.8 | 32.0 | 9.3 |\n| OpenCLIP (ViT-BigG/14) [37] | 15.5 | 35.5 | 12.0 |\n| EVA-02-CLIP (ViT-E/14+) [38] | 14.0 | 33.8 | 10.8 |\n| EVA-CLIP (8B) [39] | 14.8 | 36.5 | 10.3 |\n| EVA-CLIP (18B) [39] | 15.0 | 35.8 | 10.5 |\n| NegCLIP [47] | 10.5 | 29.5 | 8.0 |\n| LLaVA-1.5-7B [34] | 11.3 | 18.5 | 6.5 |\n| E5-V (LLaVA-Next-8B) [23] | 14.8 | 32.3 | 11.3 |\n| E5-V (LLaVA-1.5-7B) [23] | 17.4 | 31.3 | 10.5 |\n| VladVA (Ours) (LLaVA-1.5-7B) | **17.5** | **40.5** | **12.8** |", "caption": "Table 8: Comparison with state-of-the-art on the Winoground compositionality benchmark.", "description": "This table presents a comparison of the performance of various state-of-the-art vision-language models on the Winoground benchmark.  Winoground is a challenging dataset designed to evaluate the compositional abilities of these models, specifically focusing on how well they understand the relationship between objects, attributes, and relations in images. The table shows the results for each model's performance on image, text, and a combined image and text evaluation, providing a comprehensive assessment of their ability to correctly interpret compositional aspects of visual scenes.", "section": "4. Experiments"}, {"content": "| Model | Data size | Top-1 | Top-5 |\n|---|---|---|---| \n| CLIP (ViT-B) [36] | 400M | 68.4 | 91.9 |\n| CLIP (ViT-L) [36] | 400M | 74.0 | 94.0 |\n| EVA-CLIP (18B) [39] | 2.7B | 83.5 | 97.2 |\n| CLIP (ViT-B) [36] | 15M | 32.8 | - |\n| HiDeCLIP (ViT-B) [36] | 15M | 45.9 | - |\n| FFF (ViT-B) [7] | 15M | 51.1 | - |\n| BLIP (ViT-L) [28] | 129M | 54.2 | 81.5 |\n| BLIP2 (ViT-L) [29] | 129M | 46.7 | 74.2 |\n| LLaVA-Next-8B [26] | 0M | 45.8 | 74.6 |\n| E5-V [23] (LLaVA-Next-8B) | 0M | 48.2 | 76.6 |\n| LLaVA-1.5-7B [34] | 0M | 42.0 | 74.6 |\n| VladVA (Ours) (LLaVA-1.5-7B) | 8.1M | 63.7 | 88.3 |\n| Qwen2-VL-2B [43] | 0M | 54.7 | 79.4 |\n| VladVA (Ours) (Qwen2-VL-2B) | 8.1M | 70.6 | 91.1 |", "caption": "Table 9: Zero-shot image recognition results on ImageNet dataset in terms of Top-1 and Top-5 (%) accuracy.", "description": "This table presents the zero-shot image recognition performance of various models on the ImageNet dataset.  The results are evaluated using two metrics: Top-1 accuracy (the percentage of images correctly classified as the most likely class) and Top-5 accuracy (the percentage of images correctly classified among the five most likely classes).  The table allows comparison of different models' ability to generalize to unseen ImageNet images without any specific training on this dataset.", "section": "Experiments"}]