{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduced CLIP, a foundational contrastively-trained vision-language model that the current research builds upon and aims to improve."}, {"fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-05-22", "reason": "This paper demonstrated the capabilities of large language models (LLMs) as few-shot learners, inspiring the use of LLMs in combination with vision encoders for the current research."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "publication_date": "2022-07-01", "reason": "This paper introduced BLIP, a significant vision-language model that the current research compares against and aims to surpass in performance."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-07-01", "reason": "This paper introduced BLIP-2, an improved version of BLIP, providing a strong benchmark for the current research's performance evaluation."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Improved baselines with visual instruction tuning", "publication_date": "2024-07-01", "reason": "This paper introduced LLaVA, a large vision-language model that is directly adapted and improved by the current research, serving as the primary basis for the new method."}]}