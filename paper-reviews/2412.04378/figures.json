[{"figure_path": "https://arxiv.org/html/2412.04378/x1.png", "caption": "Figure 1: Overall VladVA framework: a generative LVLM is adapted into a discriminative model. At test time, the vision features (\ud835\udc1fvsubscript\ud835\udc1f\ud835\udc63\\mathbf{f}_{v}bold_f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT) are produced by passing the image \ud835\udc31vsubscript\ud835\udc31\ud835\udc63\\mathbf{x}_{v}bold_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT alongside the handcrafted and/or soft prompt \ud835\udc31pvsubscriptsuperscript\ud835\udc31\ud835\udc63\ud835\udc5d\\mathbf{x}^{v}_{p}bold_x start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT to the LVLM. The last token of this sequence will contain the summarized representation. Analogously, the textual features (\ud835\udc1ftsubscript\ud835\udc1f\ud835\udc61\\mathbf{f}_{t}bold_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) are produced by passing the prompt corresponding to the language modality (\ud835\udc31ptsuperscriptsubscript\ud835\udc31\ud835\udc5d\ud835\udc61\\mathbf{x}_{p}^{t}bold_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT) through the LLM of the LVLM (i.e. the weights of the LLM are fully shared) alongside the short caption (\ud835\udc31qs\u2062h\u2062o\u2062r\u2062tsuperscriptsubscript\ud835\udc31\ud835\udc5e\ud835\udc60\u210e\ud835\udc5c\ud835\udc5f\ud835\udc61\\mathbf{x}_{q}^{short}bold_x start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s italic_h italic_o italic_r italic_t end_POSTSUPERSCRIPT). During training, the LVLM will also take as input a long, detailed caption \ud835\udc31\ud835\udc2a\ud835\udc25\ud835\udc28\ud835\udc27\ud835\udc20superscriptsubscript\ud835\udc31\ud835\udc2a\ud835\udc25\ud835\udc28\ud835\udc27\ud835\udc20\\mathbf{x_{q}^{long}}bold_x start_POSTSUBSCRIPT bold_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_long end_POSTSUPERSCRIPT.", "description": "The VladVA framework adapts a generative Large Vision-Language Model (LVLM) for discriminative tasks.  At test time, image features are generated by passing an image and a prompt (handcrafted or soft) through the LVLM; the final token represents a summarized image embedding. Similarly, text features are created by processing a text prompt and short caption through the LVLM's Language Model (LLM). During training, a longer, detailed caption is also used.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.04378/x2.png", "caption": "Figure 2: Entropy of the output probability distribution at the next-to-be-predicted token location using a LLaVA-1.5-7B\nfor a set of 50 prompts for both images and captions.", "description": "This figure visualizes the entropy of the probability distribution for the next predicted token.  It uses the LLaVA-1.5-7B model and evaluates 50 different prompts for both image and text inputs. The x-axis represents the prompt index, and the y-axis represents the entropy.  Separate lines show entropy values for image prompts and text prompts. The goal is to show the effect of different prompt types on the model's output uncertainty and information content. Higher entropy indicates greater uncertainty.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.04378/x3.png", "caption": "Figure 3: Cumulative variance of the image and text embedding matrices\nover a set of 50 prompts on Flickr30k.\nEmbeddings that capture more information about the input translate\ninto a cumulative variance that requires more principal components to be explained, i.e. a higher rank embedding matrix.", "description": "Figure 3 visualizes the cumulative variance of embedding matrices generated from 50 different prompts applied to image and text data from the Flickr30k dataset.  The x-axis represents the number of principal components used in the principal component analysis (PCA), while the y-axis shows the cumulative variance explained.  The key insight is that richer embeddings, carrying more information about the input image or text, exhibit higher cumulative variance. This means a larger number of principal components are needed to capture the significant portion of the variance in those richer embeddings.  In simpler terms, a higher cumulative variance signifies a higher-rank embedding matrix, indicating greater complexity and information density within the embedding.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.04378/x4.png", "caption": "Figure 4: Top-k next-to-be-predicted tokens before and after the proposed fine-tuning approach. On the right, we show the output probability distribution for each case.\nWe observe that, when using the optimal prompt, the representations of the next token can encode diverse\nand more discriminative information, making potentially better-quality embeddings. This behavior\nis further improved by the proposed fine-tuning method.", "description": "This figure shows the top-k predicted tokens (words) before and after fine-tuning a vision-language model.  The left side displays the predicted words and their probabilities for various prompts. The right side shows the probability distribution of these predictions.  The results demonstrate that using an optimized prompt and the proposed fine-tuning method leads to more diverse and discriminative information encoded in the predicted tokens. This improved representation, in turn, yields potentially higher-quality embeddings for image and text data.", "section": "3.2 Discriminative fine-tuning of LVLMs: from generation to discrimination"}, {"figure_path": "https://arxiv.org/html/2412.04378/x5.png", "caption": "Figure 5: Image and text retrieval score on Flickr30k over a\nset of 50 image-text\nprompts ordered by their entropy scores (Fig.\u00a02).\nWe can observe that prompts with high average entropy scores\ncorrelate positively with the zero-shot retrieval performance.", "description": "This figure displays the relationship between prompt entropy and zero-shot retrieval performance on the Flickr30k dataset.  Fifty image-text prompt pairs were ranked by their average entropy (as calculated and shown in Figure 2).  The graph plots recall@1 for both image and text retrieval against the sorted prompt indices. The results show a positive correlation: prompts with higher entropy tend to yield better zero-shot retrieval performance.", "section": "3.2. Discriminative fine-tuning of LVLMs: from generation to discrimination"}, {"figure_path": "https://arxiv.org/html/2412.04378/x6.png", "caption": "Figure 6: Attention map between the summary and vision tokens shown for a set of heads. Notice that post-training, the attention maps densify. This behavioral change can be interpreted as follows: For generative tasks, at every step in the generation process, the model has the chance to look back at the vision tokens, selectively attending to the regions of interest at the current step. In contrast, in a discriminative setting, the model must compress all information present in the image within the summary token.", "description": "Figure 6 visualizes the attention maps between the summary token and vision tokens across various attention heads.  Before fine-tuning (pre-training), the attention is sparsely distributed across the image, reflecting the model's generative approach where it focuses on specific regions at each step of the generation process.  After fine-tuning, the attention maps become denser and more uniformly distributed across the image. This shift indicates the model's transition to a discriminative behavior where it needs to comprehensively integrate all image information into a single, compact summary token.", "section": "3.4 How does the model's behavior change?"}, {"figure_path": "https://arxiv.org/html/2412.04378/x7.png", "caption": "Figure 7: Qualitative comparison on image captioning of the base LLaVA-1.5-7B model and its fine-tuned versions using both E5-V\u00a0[23] and our proposed method. We show that with our method, the LLaVA-1.5-7B better retains its captioning capabilities, while E5-V fine-tuning appears to result in less informative captions.", "description": "This figure presents a qualitative comparison of image captioning results generated by three different models: the original LLaVA-1.5-7B model and the same model after fine-tuning using two different methods \u2013 E5-V and the authors' proposed method. The comparison highlights how well each method preserves the original model's captioning capabilities while enhancing discriminative image-text capabilities.  The figure shows that the proposed method better retains the LLaVA model's ability to produce detailed and informative captions, unlike the E5-V method which generates less descriptive and shorter captions.", "section": "Qualitative text generation examples post discriminative adaptation"}]