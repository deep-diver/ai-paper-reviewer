[{"figure_path": "https://arxiv.org/html/2502.09042/extracted/6200630/figures/overview.png", "caption": "Figure 1: Top: The transformation-and-refinement pipeline used for long-thinking data generation described in Sections\u00a02.2.1 and 2.2.2. Bottom-Left: The structured long-thinking (the best thinking format) training pipeline for Typhoon T, as described in Section\u00a03.1. Bottom-Right: The bilingual English-Thai Typhoon T1 model training pipeline detailed in Section\u00a03.4.", "description": "This figure illustrates the data processing pipeline for creating a Thai reasoning model.  The top panel shows the transformation and refinement stages used to convert existing datasets into the \"long-thinking\" format required for training.  The bottom-left panel details the training process for the Typhoon T model, which uses the structured long-thinking format.  The bottom-right panel shows how the bilingual Typhoon T1 model is trained using the structured data generated and both English and Thai languages.", "section": "2 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.09042/extracted/6200630/figures/thinking_format.png", "caption": "Figure 2: Differences between three thinking formats: (a) Unstructured thinking, where no XML structural tags are included; (b) Semi-structured thinking, which is similar to unstructured thinking but adds <thoughts> and <response> tags to separate thoughts and responses; (c) Structured thinking, which introduces additional XML tags for structural purposes in the thoughts section.", "description": "This figure illustrates three different formats for representing the reasoning process of a large language model (LLM).  Panel (a) shows unstructured thinking, where the LLM's reasoning and final answer are presented as a single, continuous text stream without any explicit separation or structure.  Panel (b) demonstrates semi-structured thinking, which adds simple XML tags such as <thoughts> and <response> to delineate the reasoning steps from the final answer. Panel (c) shows structured thinking, a more advanced format that utilizes additional XML tags within the <thoughts> section to further organize and structure the LLM's reasoning process, helping to clarify the intermediate steps and sub-goals involved in arriving at the final answer. This structured format is designed to improve the clarity and organization of the LLM's thinking.", "section": "2 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.09042/extracted/6200630/figures/training_percentage.png", "caption": "Figure 3: Increasing the proportion of the training set beyond 75% results in performance degradation for some datasets, while GSM8K generally shows a trend of performance improvement with the proportion.", "description": "This figure presents the results of an experiment assessing the impact of training data size on the performance of a reasoning model.  Multiple datasets were used, and varying percentages of the training data (5%, 10%, 25%, 50%, 75%, and 100%) were used to train separate models. The results show that for most datasets, using more than 75% of the training data did not significantly improve performance, and in some cases even led to performance degradation. The exception is the GSM8K dataset, which demonstrated a consistent performance improvement as more training data was used.", "section": "3.2 BALANCING DATA QUANTITY: THE KEY TO OPTIMAL REASONING MODEL PERFORMANCE"}, {"figure_path": "https://arxiv.org/html/2502.09042/extracted/6200630/figures/final_comparison.png", "caption": "Figure 4: Final performance comparison of Typhoon T1-EN and Typhoon T1 against the baseline Typhoon T1 3B Instruct model across six evaluation benchmarks.", "description": "Figure 4 presents a comparative analysis of the performance of three different models: Typhoon T1-EN, Typhoon T1, and the baseline Typhoon 2 3B Instruct model, across six distinct evaluation benchmarks.  It visually represents the relative performance of each model on each benchmark, enabling easy comparison of their strengths and weaknesses.  The benchmarks likely represent a variety of tasks to assess the models' reasoning capabilities comprehensively.", "section": "3.6 SUMMARY: TYPHOON T1-EN AND TYPHOON T1"}, {"figure_path": "https://arxiv.org/html/2502.09042/extracted/6200630/figures/domain_distribution.png", "caption": "Figure 5: This figures show domain distribution of the training set for the experiments.", "description": "This pie chart visualizes the distribution of the training data across five different domains used in the experiments described in the paper.  Each slice of the pie represents a domain, and its size corresponds to the proportion of data samples from that domain within the overall training dataset.  The domains are: Mathematics, Instruction Following, Coding, Safety, and Finance.", "section": "2.2 Data Preparation"}, {"figure_path": "https://arxiv.org/html/2502.09042/extracted/6200630/figures/t1_reasoning.png", "caption": "Figure 6: This figure shows Typhoon T1\u2019s Thai thinking trace and Typhoon T1-EN\u2019s English thinking trace.", "description": "This figure presents a side-by-side comparison of the reasoning traces generated by two models: Typhoon T1 and Typhoon T1-EN.  Typhoon T1, trained with Thai-translated data, produces a reasoning trace in Thai, showcasing its ability to generate reasoning steps in a low-resource language. In contrast, Typhoon T1-EN, trained without Thai data, provides a reasoning trace in English.  The comparison highlights the impact of training data on the model's ability to perform reasoning and generate traces in different languages. Both models answer the same question, allowing for a direct comparison of their respective reasoning processes and language usage.", "section": "3 Experiments"}]