{"references": [{" publication_date": "2024", "fullname_first_author": "Yann Dubois", "paper_title": "Alpacafarm: A simulation framework for methods that learn from human feedback", "reason": "This paper introduces AlpacaFarm, a simulation framework for evaluating methods that learn from human feedback.  This is highly relevant to the current work because it provides a structured way to assess the quality and reliability of LLM-based judges, which are fundamentally methods that learn from human-provided feedback (or a simulation of it).  Understanding the strengths and limitations of such methods is essential to building trust and confidence in LLM-based judgments, a key theme explored in the target paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zheng Cai", "paper_title": "Internlm2 technical report", "reason": "This technical report provides crucial details about the InternLM2 language model, which is a large language model used in the evaluation of JudgeBench.  Understanding the architecture, training data, and performance characteristics of InternLM2 is vital for interpreting the results and understanding the limitations of the benchmark. The model's capabilities influence the reliability of the LLM-based judges which is a key issue addressed by the target paper.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "reason": "This foundational paper introduces reinforcement learning from human preferences (RLHF), a core technique used in training reward models.  Given the close relationship between reward models and LLM-based judges, this paper is central to the context.  Understanding RLHF is crucial for grasping how reward models work and for evaluating their performance as judges, a key point highlighted in the related work section of the target paper.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "reason": "This paper introduces the concept of training 'verifiers', which are LLMs trained to assess the correctness of responses to challenging problems.  This is directly relevant to the evaluation of LLM-based judges as the judges themselves act as verifiers, distinguishing between correct and incorrect responses.  Understanding the methods and challenges involved in training such verifiers is critical for evaluating the effectiveness of LLM-based judges.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Xuechen Li", "paper_title": "Alpacaeval: An automatic evaluator of instruction-following models", "reason": "This work provides an automatic evaluator specifically for instruction-following LLMs.  The focus on instruction-following is closely aligned with one of the three principles underpinning JudgeBench's evaluation framework.  Understanding the methods used in this evaluator, including prompt engineering and response analysis techniques, is essential for evaluating the reliability of instruction-following aspects of LLM-based judges.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Haipeng Luo", "paper_title": "Arena learning: Build data flywheel for llms post-training via simulated chatbot arena", "reason": "This paper introduces Arena learning, a method for aligning LLMs with human preferences using a simulated competitive environment. This is closely related to the concept of LLM-based judges that are used as evaluators.  Understanding the strengths and limitations of Arena learning is essential for evaluating the general effectiveness of LLM-based judges, especially in the context of assessing complex reasoning skills.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "reason": "This paper introduces the MMLU benchmark, a large-scale multitask language understanding dataset used as a data source for JudgeBench. Understanding the design, characteristics, and challenges of MMLU helps to interpret the results from JudgeBench, especially in understanding how well LLM-based judges perform on various tasks.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Colin White", "paper_title": "Livebench: A challenging, contamination-free llm benchmark", "reason": "LiveBench is a dataset used in JudgeBench. Understanding LiveBench's design, composition, and challenges will help to interpret the results of JudgeBench. This paper provides vital context on the difficulty of tasks in LiveBench which are a subset of those used in JudgeBench.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Naman Jain", "paper_title": "LiveCodeBench: Holistic and contamination free evaluation of large language models for code", "reason": "LiveCodeBench is a dataset used in JudgeBench, providing coding-related tasks. The understanding of its characteristics and the challenges it presents for LLMs is critical to interpreting the results of JudgeBench. This paper provides this critical context.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Nathan Lambert", "paper_title": "RewardBench: Evaluating reward models", "reason": "RewardBench is a benchmark for reward models and is directly compared to JudgeBench in the target paper.  Understanding its design, scope, and limitations is crucial for interpreting and contextualizing the performance of JudgeBench and the relative strengths and weaknesses of different evaluation approaches for LLMs. The comparison highlights the differing levels of difficulty and saturation between the datasets.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Lianmin Zheng", "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena", "reason": "This paper examines LLMs as judges themselves and the benchmark MT-Bench is directly compared to JudgeBench in the target paper.  Understanding MT-Bench helps to frame the relative strengths and weaknesses of JudgeBench, providing insights into its improvements over existing benchmarks.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Bradley Brown", "paper_title": "Large language monkeys: Scaling inference compute with repeated sampling", "reason": "This work explores the impact of scaling inference compute on LLM performance, particularly the technique of repeated sampling.  This is highly relevant to JudgeBench, as scaling compute affects the performance of LLM-based judges which are also LLMs.  Understanding how repeated sampling impacts the reliability of LLMs directly affects the analysis and interpretation of JudgeBench results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yann Dubois", "paper_title": "Alpacafarm: A simulation framework for methods that learn from human feedback", "reason": "This paper introduces AlpacaFarm, a simulation framework for evaluating methods that learn from human feedback.  Understanding AlpacaFarm's methodology and its strengths and limitations are valuable for analyzing the results of evaluating LLM-based judges on JudgeBench.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Hunter Lightman", "paper_title": "Let's verify step by step", "reason": "This work introduces a method for step-by-step verification of solutions, a crucial aspect of evaluating LLMs on complex tasks.  This approach is related to the methodology used in constructing JudgeBench, which also focuses on evaluating the logical correctness of answers.   Understanding this verification approach is crucial for evaluating the methodology of JudgeBench.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Seungone Kim", "paper_title": "Prometheus: Inducing fine-grained evaluation capability in language models", "reason": "This paper presents Prometheus, a fine-tuned LLM-based judge.  Analyzing Prometheus's performance and methodology on JudgeBench helps to understand the general capabilities and limitations of fine-tuned judges and contributes to evaluating different approaches for building robust LLM-based judges.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Tu Shiwen", "paper_title": "Skywork critic model series", "reason": "This paper introduces Skywork's LLM-based judges, which are evaluated in the experiments of JudgeBench.  Understanding Skywork's approach to building LLM-based judges, including their training data and methods, is crucial for interpreting the results and assessing their relative performance compared to other LLM-based judges.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Zhiyuan Zeng", "paper_title": "Evaluating large language models at evaluating instruction following", "reason": "This paper explores the challenges of evaluating LLMs' ability to follow instructions, a crucial aspect of JudgeBench's hierarchical evaluation framework.  Understanding the methods used in this work helps to contextualize and evaluate JudgeBench's focus on instruction following and its implications for overall LLM-based judge performance.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Amanda Askell", "paper_title": "A general language assistant as a laboratory for alignment", "reason": "This foundational paper explores the concept of alignment in LLMs. The alignment of LLMs with human intentions is crucial for building reliable LLM-based judges.   This paper helps in understanding the general challenges in achieving such alignment and the need for sophisticated evaluation methodologies like the ones proposed by the target paper.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yidong Wang", "paper_title": "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization", "reason": "PandaLM is a fine-tuned LLM-based judge used in JudgeBench's evaluation.  Understanding its training data, methodology, and performance characteristics is vital for interpreting the results and assessing its suitability as an evaluator of LLMs.  This paper offers a deep dive into the specific design choices made in creating PandaLM.", "section_number": 4}]}