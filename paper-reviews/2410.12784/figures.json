[{"figure_path": "2410.12784/figures/figures_4_0.png", "caption": "Figure 2: Overview of JudgeBench Pipeline. Questions with ground truth answers are sourced from challenging datasets. We sample k responses to each question using a strong LLM (e.g., GPT-40) and grade each response for correctness. Response pairs are constructed from correct and incorrect responses. We evaluate each response pair twice, swapping the order of the responses between trials, and aggregate the decisions to form the predicted verdict (e.g., B > A).", "description": "The figure illustrates the JudgeBench pipeline, which transforms datasets with ground truth labels into response pairs for evaluating LLM-based judges.", "section": "3 JUDGEBENCH"}]