[{"figure_path": "2410.12784/tables/table_2_0.html", "caption": "Table 1: Evaluating LLM-based judges on JudgeBench.", "description": "The table presents the overall performance of different LLM-based judges (prompted, fine-tuned, and multi-agent) across four categories (knowledge, reasoning, math, and coding) in the JudgeBench benchmark.", "section": "4.1 EVALUATING LLM-BASED JUDGES ON JUDGEBENCH"}, {"figure_path": "2410.12784/tables/table_7_0.html", "caption": "Table 1: Evaluating LLM-based judges on JudgeBench.", "description": "Table 1 presents the overall performance of different LLM-based judges across four categories (knowledge, reasoning, math, and coding) using the JudgeBench benchmark.", "section": "4.1 EVALUATING LLM-BASED JUDGES ON JUDGEBENCH"}, {"figure_path": "2410.12784/tables/table_7_1.html", "caption": "Table 2: Evaluating the Arena-Hard Judge on JudgeBench, with different underlying models.", "description": "The table presents the performance of the Arena-Hard Judge using different underlying language models on the JudgeBench benchmark, categorized by knowledge, reasoning, math, coding, and overall performance.", "section": "4.1 EVALUATING LLM-BASED JUDGES ON JUDGEBENCH"}, {"figure_path": "2410.12784/tables/table_8_0.html", "caption": "Table 3: Evaluating reward models on JudgeBench.", "description": "This table presents the performance of five reward models on the JudgeBench benchmark, categorized by knowledge, reasoning, math, coding, and overall accuracy.", "section": "4.1 Evaluating LLM-based judges on JudgeBench"}, {"figure_path": "2410.12784/tables/table_10_0.html", "caption": "Table 4: Evaluating the LLM's ability to solve the problems.", "description": "This table compares the performance of different LLMs in solving problems and their corresponding judges' performance in evaluating the solutions, highlighting the correlation between solving and verifying abilities.", "section": "4.1 EVALUATING LLM-BASED JUDGES ON JUDGEBENCH"}, {"figure_path": "2410.12784/tables/table_18_0.html", "caption": "Table 1: Evaluating LLM-based judges on JudgeBench.", "description": "Table 1 presents the overall performance of different LLM-based judges across four categories (Knowledge, Reasoning, Math, and Coding) on the JudgeBench benchmark.", "section": "4.1 EVALUATING LLM-BASED JUDGES ON JUDGEBENCH."}, {"figure_path": "2410.12784/tables/table_18_1.html", "caption": "Table 1: Evaluating LLM-based judges on JudgeBench.", "description": "The table presents the overall performance of various LLM-based judges across different categories (Knowledge, Reasoning, Math, Coding) on the JudgeBench benchmark.", "section": "4.1 EVALUATING LLM-BASED JUDGES ON JUDGEBENCH"}, {"figure_path": "2410.12784/tables/table_18_2.html", "caption": "Table 1: Evaluating LLM-based judges on JudgeBench.", "description": "The table presents the overall performance of various LLM-based judges across different categories (knowledge, reasoning, math, coding) on the JudgeBench benchmark.", "section": "4.1 EVALUATING LLM-BASED JUDGES ON JUDGEBENCH."}]