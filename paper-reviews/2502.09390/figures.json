[{"figure_path": "https://arxiv.org/html/2502.09390/x1.png", "caption": "Figure 1: The SQuARE methods prompts the model to generate N\ud835\udc41Nitalic_N question-answer pairs about the topic and then respond to the original query, having established additional context.", "description": "The figure illustrates the SQUARE method's operation.  It shows how SQUARE prompts a language model to generate multiple question-answer pairs (N) related to the main query before answering it. This approach builds context and enables the model to explore different facets of the topic more thoroughly, enhancing its ability to answer complex questions accurately.", "section": "2 SQUARE"}, {"figure_path": "https://arxiv.org/html/2502.09390/x2.png", "caption": "Figure 2: Ablation study illustrating how few-shot examples influence performance metrics for the CoT, RaR, and SQuARE approaches, using the Llama-3.1 8B model.", "description": "This ablation study uses the Llama-3.1 8B model to evaluate the effect of few-shot examples on the performance of three different prompting techniques: Chain-of-Thought (CoT), Rephrase-and-Respond (RaR), and SQUARE.  The figure visually represents the performance metrics (likely accuracy or a similar measure) for each method across various numbers of few-shot examples.  It allows for a comparison of how each approach's performance changes with different amounts of training data presented as examples.", "section": "3 Experiments"}]