[{"figure_path": "2410.18889/charts/charts_6_0.png", "caption": "Figure 2: When LLMs disagrees with original labels who is correct?. As the LLM's confidence grows, so does the precision of identifying an error in the original labels.", "description": "The chart visualizes the precision of LLMs in detecting label errors across various confidence levels, showing improved precision with higher confidence.", "section": "4.2 LLMS FOR LABEL ERROR DETECTION"}, {"figure_path": "2410.18889/charts/charts_7_0.png", "caption": "Figure 3: LLM Ensemble of different sizes (random subsets). (Left) presents the performance of the ensemble in terms of ROC AUC compared to the gold labels. (Right) presents the increasing ability to detect label errors. F1 is computed over Error / Not Error predictions.", "description": "The chart shows that as the number of models in an LLM ensemble increases, both its performance on gold labels (ROC AUC) and its ability to detect label errors (F1 score) improve.", "section": "4.2.2 THE POWER OF ENSEMBLE"}, {"figure_path": "2410.18889/charts/charts_8_0.png", "caption": "Figure 5: Comparison between all annotation methods, measured by the weighted-F1-score. Rows represent the \\\"true\\\" label and columns represent the \\\"prediction\\\". For instance, the score of LLMs compared to the Original label is 0.72.", "description": "The chart displays the weighted F1-score between pairs of annotation methods (Original labels, LLM-binary, MTurk-Strict, MTurk-Majority, and Gold labels), showing the agreement level between different annotation approaches.", "section": "5 COMPARING ANNOTATION APPROACHES"}, {"figure_path": "2410.18889/charts/charts_8_1.png", "caption": "Figure 6: (x-axis) at list x annotations per annotator. (Right y-axis) The number of annotators with at least x annotations (bins). (Left y-axis) the average F1-score or accuracy for all user annotations with at least x annotations.", "description": "The chart shows that as the number of annotations per annotator increases, the quality of crowd-sourced annotations improves, as measured by accuracy and F1-score.", "section": "5 COMPARING ANNOTATION APPROACHES"}, {"figure_path": "2410.18889/charts/charts_10_0.png", "caption": "Figure 7: Fine-tuning a model on a transformed dataset. The gray bar is the original dataset - without any changes. The green bars present results for label flipping for a subset of examples, determined by LLMs-confidence (plain), or at random (dotted). The blue bars represent filtering of these examples.", "description": "The chart displays the effect of handling label errors (flipping or filtering) on model performance (ROC AUC) when training on different subsets of data with varying confidence levels.", "section": "6.1 Training on Mislabeled Data"}, {"figure_path": "2410.18889/charts/charts_21_0.png", "caption": "Figure 5: Comparison between all annotation methods, measured by the weighted-F1-score. Rows represent the \"true\" label and columns represent the \"prediction\". For instance, the score of LLMs compared to the Original label is 0.72.", "description": "The chart displays the weighted F1-score of agreement between different annotation methods (Original, LLM, Crowd-sourced, Gold) using a heatmap.", "section": "5 COMPARING ANNOTATION APPROACHES"}, {"figure_path": "2410.18889/charts/charts_21_1.png", "caption": "Figure 6: (x-axis) at list x annotations per annotator. (Right y-axis) The number of annotators with at least x annotations (bins). (Left y-axis) the average F1-score or accuracy for all user annotations with at least x annotations.", "description": "The chart displays the relationship between the number of annotations per annotator and their annotation quality, revealing that annotators with more annotations tend to achieve higher accuracy and F1-scores.", "section": "5.2 CONSISTENCY"}, {"figure_path": "2410.18889/charts/charts_25_0.png", "caption": "Figure 2: When LLMs disagrees with original labels who is correct?. As the LLM's confidence grows, so does the precision of identifying an error in the original labels.", "description": "The chart shows that as LLM confidence in disagreeing with original labels increases, the precision of identifying label errors also increases, surpassing original label agreement with expert re-labeling at the highest confidence levels.", "section": "4.2 LLMS FOR LABEL ERROR DETECTION"}, {"figure_path": "2410.18889/charts/charts_25_1.png", "caption": "Figure 7: Fine-tuning a model on a transformed dataset. The gray bar is the original dataset - without any changes. The green bars present results for label flipping for a subset of examples, determined by LLMs-confidence (plain), or at random (dotted). The blue bars represent filtering of these examples.", "description": "The chart displays the impact of handling label errors (filtering or flipping) based on LLM confidence on model fine-tuning performance, comparing different approaches and random manipulations.", "section": "6.1 Training on Mislabeled Data"}]