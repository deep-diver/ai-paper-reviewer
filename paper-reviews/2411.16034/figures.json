[{"figure_path": "https://arxiv.org/html/2411.16034/x1.png", "caption": "Figure 1: VisualLens leverages a user\u2019s task-agnostic visual history to provide personalized recommendations. Our method outperforms GPT-4o by 1.6%\u223csimilar-to\\sim\u223c4.6% on Hit@3.", "description": "This figure demonstrates the VisualLens approach, which utilizes a user's task-agnostic visual history (images reflecting daily life) to generate personalized recommendations.  The chart compares the Hit@3 performance of VisualLens against several baselines, including UniMP and GPT-40.  VisualLens shows a consistent improvement in recommendation accuracy over these baselines, specifically outperforming GPT-40 by a margin ranging from 1.6% to 4.6% in Hit@3.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2411.16034/x2.png", "caption": "Figure 2: VisualLens inference pipeline: the offline process augments images in the visual history with captions and aspect words; the runtime recommendation process retrieves relevant images, generate user profile accordingly, and then predict candidate preferences.", "description": "This figure illustrates the two-stage process of VisualLens. The offline stage involves augmenting each image in a user's visual history with automatically generated captions and aspect words.  The runtime stage consists of three steps: 1) History Retrieval: Selects images from the visual history most relevant to a given recommendation query. 2) Preference Profiling: Generates a user preference profile based on the retrieved images, captions, and aspect words.  This often involves aggregating multiple images into a grid. 3) Candidate Matching: Matches the user profile against potential recommendation candidates, ranking them based on a calculated probability.", "section": "4 Recommendation Method"}, {"figure_path": "https://arxiv.org/html/2411.16034/x3.png", "caption": "(a)", "description": "The figure shows the MRR distribution over the number of candidates.  The MRR (Mean Reciprocal Rank) is a metric used to evaluate the ranking quality of recommendations, where a higher MRR indicates better performance.  The x-axis represents the number of candidates and the y-axis represents the MRR. The plot shows that the MRR initially increases as the number of candidates increases but eventually plateaus, indicating that increasing the number of candidates beyond a certain point doesn't significantly improve recommendation quality. This suggests there's a point of diminishing returns when adding more candidates.", "section": "7 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2411.16034/x4.png", "caption": "(b)", "description": "The figure shows the MRR distribution over the number of images in the visual history.  The x-axis represents the number of images, and the y-axis represents the Mean Reciprocal Rank (MRR). The plot demonstrates that as the number of images increases, the MRR initially increases and then plateaus after reaching approximately 100 images. This indicates that while a larger visual history provides more information, there's a point of diminishing returns beyond which additional images don't significantly improve the recommendation accuracy.  The flattening of the curve suggests the model's robustness to noise in the visual history; the system can effectively filter out irrelevant images.", "section": "7 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2411.16034/x5.png", "caption": "Figure 3: (a) MRR distribution over number of candidates, (b) MRR distribution over number of images. Both are on the User ID test set. We find (1) MRR converges when number of candidates exceeds 50; (2) MRR increases and flattens after reaching \u223csimilar-to\\sim\u223c100 images.", "description": "This figure presents two bar charts visualizing the performance of the VisualLens model. Chart (a) displays the Mean Reciprocal Rank (MRR) across varying numbers of candidate items for a recommendation task, revealing MRR convergence beyond 50 candidates. Chart (b) shows MRR performance with different numbers of images in the user's visual history, demonstrating that MRR increases and plateaus around 100 images. Both charts are based on User ID test data, meaning each user's history and recommendations are treated separately, and no user data is shared between train and test data.", "section": "7 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2411.16034/x6.png", "caption": "(a)", "description": "The figure shows the MRR distribution based on the number of candidates.  The left graph shows that the MRR converges when the number of candidates exceeds 50. The right graph shows that the MRR increases and flattens after reaching around 100 images.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2411.16034/x7.png", "caption": "(b)", "description": "The figure shows the MRR distribution over the number of images in a user's visual history.  The x-axis represents the number of images, and the y-axis represents the Mean Reciprocal Rank (MRR). The graph illustrates how the MRR changes as the amount of visual data increases.  It shows that initially, MRR improves as more images are available because the model has more data to learn user preferences from. However, after a certain point (~100 images), the MRR plateaus or even slightly decreases. This indicates that including excessively large amounts of visual data may not necessarily improve the quality of the recommendations and can introduce noise.", "section": "7 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2411.16034/x8.png", "caption": "Figure 4: (a) MRR distribution over categories on Google Review-V, (b) MRR distribution over categories on Yelp-V. We find (1) the performance per category is loosely correlated with number of training data; (2) when a category is more general and less ambiguous, the performance on the category is better.", "description": "This figure displays the mean reciprocal rank (MRR) distribution across various categories for both Google Review-V and Yelp-V datasets.  The left panel (a) shows the distribution for Google Review-V, while the right panel (b) shows the distribution for Yelp-V.  The key observations are a loose correlation between the number of training examples per category and the MRR, and a tendency for more general, less ambiguous categories to achieve higher MRR scores.", "section": "7 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2411.16034/x9.png", "caption": "Figure 5: The Google Review-Vision (Google Review-V) training data consists of 66 categories.", "description": "This bar chart visualizes the distribution of the 66 categories within the Google Review-Vision (GR-V) dataset used for training.  The x-axis represents the different categories, and the y-axis shows the number of data points or instances belonging to each category.  The chart provides insight into the class imbalance of the dataset, revealing which categories are more prevalent than others in the training data.  This information is useful in understanding potential biases in the model's training.", "section": "6.1 Benchmark creation"}]