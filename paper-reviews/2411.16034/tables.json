[{"content": "| Hyperparameters for training on PaliGemma |\n|---|---| \n| Parameter Size | 3B |\n| Image Resolution | 896 \u00d7 896 |\n| Number of Image Tokens | 4096 |\n| Hidden Dimension Size | 2048 |\n| LoRA Rank | 16 |\n| LoRA \u03b1 | 16 |\n| LoRA dropout | 0.1 |\n| GPU | 8 \u00d7 NVIDIA H100 |\n| Batch Size | 8 |\n| Gradient Accumulation Steps | 8 |\n| Warmup Steps | 200 |\n| Learning Rate | 1e-3 |", "caption": "Table 1: Dataset statistics of Google Review-Vision (GR-V) and Yelp-Vision (Yelp-V).", "description": "This table presents a summary of the statistics for two datasets: Google Review-Vision (GR-V) and Yelp-Vision (Yelp-V), which were created for evaluating personalized recommendations using visual history.  For each dataset, it provides the size of the train, dev, and test splits, the number of categories included, the average number of images per category, the average number of ground truth items, and the average number of candidate items. These statistics offer insights into the scale and characteristics of the datasets used in the study.", "section": "6 Benchmarks and Experiments Setups"}, {"content": "| Hyperparameters for training on MiniCPM-V2.5 |\n|---|---| \n| Parameter Size | 8B |\n| Image Resolution | 980 \u00d7 980 |\n| Number of Image Tokens | 96 |\n| Hidden Dimension Size | 4096 |\n| LoRA Rank | 64 |\n| LoRA \u03b1 | 64 |\n| LoRA dropout | 0.1 |\n| GPU | 8 \u00d7 NVIDIA H100 |\n| Batch Size | 8 |\n| Gradient Accumulation Steps | 8 |\n| Warmup Steps | 200 |\n| Learning Rate | 1e-3 |", "caption": "Table 2: Hit rates and MRRs of VisualLens vs. multiple baselines on Google Review-V and Yelp-V. The result shows (a) VisualLens outperforms other baselines, though has a gap with the human oracle; (b) model size greatly affects the performance; (c) simply rank by rating is a worse design than the random baseline.", "description": "This table presents a comprehensive comparison of VisualLens's performance against various baselines and state-of-the-art models on two benchmark datasets: Google Review-V and Yelp-V.  The metrics used for comparison are Hit@1, Hit@3, Hit@10, and Mean Reciprocal Rank (MRR), which assess the accuracy of recommendations. The table reveals VisualLens's superior performance compared to other methods across all metrics on both datasets, highlighting the impact of model size and the inadequacy of simpler baselines such as ranking by rating.  It showcases that VisualLens significantly reduces the gap between automated recommendations and human-level performance.", "section": "6 Benchmarks and Experiments Setups"}]