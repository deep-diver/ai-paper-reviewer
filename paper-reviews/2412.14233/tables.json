[{"content": "| Attributes | Visual Specialists | Detailed Process |\n|---|---|---|\n| *Object* |  |  |\n| Size | Detection model | Using **the area of the bounding box** to measure the size of the instance. |\n| Depth | Depth & Detection model | **Average the depth map values within the bounding box region** to obtain the depth information. |\n| Emotion | Emotion model | If the detected region is labeled as **\u201dperson\u201d**, an emotion model is used **to extract an emotion label**. |\n| OCR | OCR Model | Using an OCR model to **extract the text content and bounding box** from the region. |\n| Animal | Fine Grained model | A fine-grained recognition model to identify **specific species of the animal**. |\n| Plants |  | A fine-grained recognition model to identify **specific species of the plants**. |\n| Aircrafts |  | A fine-grained recognition model to identify **specific model of the aircraft**. |\n| Logo |  | A fine-grained recognition model to **identify logos** in the region. |\n| Landmark |  | A fine-grained recognition model to **identify landmarks** within the region. |\n| Food |  | A fine-grained recognition model to identify **specific species of the food**. |\n| Celebrity |  | Using a fine-grained recognition model to **identify celebrity** within the region. |\n| *Relation* |  |  |\n| P2O relation | HOI Model | Using an HOI model to **determine the relationship between the person and the object**, while the bounding boxes of both the person and the object define their respective regions. |\n| Count | Detection model | **Counting the number of all objects** in the image based on the detection results. |\n| 2D Absolute Location | Detection model | Using the bounding box to **determine the instance\u2019s position within the image**, including regions such as **left**, **right**, **top**, **bottom**, **center**, **top-left**, **bottom-left**, **top-right**, and **bottom-right**. |\n| 2D Relative Location | Detection model | Using the bounding box to **determine the relative position among multiple objects within** the image, including regions such as **left**, **right**, **near**, **next to**, **close by**, and so on. |\n| 3D Relative Location | Detection & Depth model | Using the depth attributes of different instances to **capture the 3D spatial relationships of objects** **relative to the camera**, such as \u201dInstance_A is **in front of** Instance_B\u201d or \u201dInstance_A is **behind of** Instance_B\u201d relative to the camera. |", "caption": "Table 1: Summary of attributes our approach extracts through visual specialists. It includes the specific attribute names, the models used, and the extraction process for each.", "description": "Table 1 details the visual attributes extracted by the DCE pipeline's visual specialists.  For each attribute (e.g., size, depth, emotion, OCR text, type of animal, plant, aircraft, logo, landmark, food, or celebrity), the table specifies the visual specialist model used for extraction and a concise description of the extraction process. This provides a comprehensive overview of the visual feature extraction methods used in the DCE pipeline.", "section": "3. Approach"}, {"content": "| Using an HOI model to | determine the relationship between the person and the object, while | \n| the bounding boxes of both the person and the object define their respective regions. |", "caption": "Table 2: Human evaluation of attribute richness, conducted on 100 validation samples with 10 volunteers.", "description": "This table presents the results of a human evaluation assessing the richness of attributes in image captions generated by different methods.  10 volunteers evaluated 100 validation samples, rating the presence of various attributes, such as spatial relationships, human-object interactions (HOI), fine-grained details, Optical Character Recognition (OCR) information, and emotions.  The scores for each attribute type (spatial relationship, HOI, fine-grained attributes, OCR, emotion)  for three methods (InternVL2, LLaVA-NeXT, and DCE) are presented, providing a comparison of attribute completeness in generated image captions.", "section": "3.2 Object Relation"}, {"content": "| Using the bounding box to **determine the instance\u2019s position within the image**, including regions|\n|---|---| \n| such as **left**, **right**, **top**, **bottom**, **center**, **top-left**, **bottom-left**, **top-right**, and **bottom-right**.|", "caption": "Table 3: Performance on seven General Visual Question Answering benchmarks. The red and blue colors respectively represent the optimal and suboptimal results on each benchmark. \u2217*\u2217 indicates the use of LLaVA-NeXT\u2019s open-source SFT data, with certain private data excluded.", "description": "Table 3 presents the performance of various models on seven general visual question answering (VQA) benchmark datasets.  The table compares different models' accuracy across these datasets, highlighting the best (red) and worst (blue) performing models for each benchmark. The asterisk (*) indicates that some models utilized LLaVA-NeXT's open-source supervised fine-tuning (SFT) data, with certain private data excluded from training. This detail is important because the availability of data can significantly influence a model's performance.", "section": "4.2. Main Results"}, {"content": "| Using the bounding box to **determine the relative position among multiple objects within**\n| **the image**, including regions such as **left**, **right**, **near**, **next to**, **close by**, and so on.", "caption": "Table 4: Performance on seven Large Multi-Modal benchmarks. The red and blue colors respectively represent the optimal and suboptimal results on each benchmark. \u2217*\u2217 indicates the use of LLaVA-NeXT\u2019s open-source SFT data, with certain private data excluded.", "description": "Table 4 presents the performance comparison of various Large Multimodal Models (LMMs) across seven benchmark datasets.  These benchmarks assess different aspects of multimodal understanding.  The models are evaluated on their ability to perform tasks requiring integrated vision and language capabilities.  The table highlights the top-performing model for each benchmark, indicated by red coloring, and the worst-performing model, indicated by blue. The asterisk (*) denotes the usage of LLaVA-NeXT's openly available instruction-tuning data, while noting that some proprietary data was excluded for those experiments.", "section": "4. Experiments"}, {"content": "| Using the depth attributes of different instances to **capture the 3D spatial relationships of objects**|\n| **relative to the camera**, such as \u201dInstance_A is **in front of** Instance_B\u201d or \u201dInstance_A is **behind of** Instance_B\u201d relative to the camera. |", "caption": "Table 5: Comparison of Different Image Captioning Annotation Methods.", "description": "Table 5 presents a comparative analysis of various image captioning annotation methods' impact on downstream tasks.  It contrasts the performance of models (LLaVA-v1.5 and LLaVA-NeXT) trained on different caption datasets: human-annotated captions, captions generated by InternVL2-26B and LLaVA-NeXT-34B, and captions generated by the proposed DCE method.  The table shows the performance on several key benchmark datasets (OKVQA, GQA, ScienceQA, TextVQA, MMBench, MM-Vet, and SEED-Bench), allowing for a direct comparison of the effectiveness of each annotation approach in enhancing the model's understanding and reasoning abilities.", "section": "4.3 Ablation Study"}]