{"importance": "This paper is important because it introduces a novel approach to enhance image captions by leveraging off-the-shelf visual specialists. This method addresses the limitations of existing captioning techniques that rely solely on Large Multimodal Models (LMMs) or human annotations, which are often costly and lack scalability. The proposed method, DCE, offers a cost-effective and highly scalable solution by integrating readily available visual experts into the caption generation pipeline. This allows for the creation of high-quality image captions with significantly enhanced details and comprehensiveness, which will benefit a wide range of vision-language tasks. The approach is generalizable and the code is open-source, enabling wider adoption and contributing to advancement in multimodal learning. ", "summary": "Enhance image captions significantly with DCE, a novel engine leveraging visual specialists to generate comprehensive, detailed descriptions surpassing LMM and human-annotated captions.", "takeaways": ["The Descriptive Caption Enhancement Engine (DCE) significantly improves image caption quality by integrating visual specialists.", "DCE generates superior image captions compared to existing LMMs and human annotations, enriching the descriptions with fine-grained details and comprehensive information.", "The proposed approach is cost-effective and scalable, using off-the-shelf visual specialists and open-source LLM, facilitating wider adoption and future research."], "tldr": "Current methods of generating image captions, either through manual annotation or using Large Multimodal Models (LMMs), face challenges in terms of cost, scalability, and accuracy. LMM-generated captions often lack detail and completeness, while manual annotation is expensive and time-consuming. This research addresses these issues by introducing a novel Descriptive Caption Enhancement Engine (DCE). \nDCE enhances image captions by incorporating off-the-shelf visual specialists to extract both instance-level (e.g., object attributes) and relational-level (e.g., spatial relationships) information. This detailed visual information is then combined with Large Language Models (LLMs) to generate richer and more accurate captions. Experiments on a large-scale dataset demonstrates that DCE-generated captions significantly improve the performance of both LLaVA-v1.5 and LLaVA-NeXT models across several benchmarks.  **The key contribution is the development of a scalable and efficient image captioning method using existing visual specialists and LLMs, eliminating the need for expensive manual annotations or the limitations of LMMs.**", "affiliation": "Hong Kong University of Science and Technology", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2412.14233/podcast.wav"}