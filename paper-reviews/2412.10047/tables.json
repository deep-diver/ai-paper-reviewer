[{"content": "| Model | Data Type | Data Source | Input &#x2192; Output Format | Data Size |\n|---|---|---|---|---| \n| LAM\u00b9 | Task-Plan Pairs | Application documentation, WikiHow,\nhistorical search queries, evolved data | t<sub>i</sub> &#x2192; P<sub>i</sub> | 76,672 tasks |\n| LAM\u00b2 | Task-Action Trajectories | GPT-4o | s<sub>t</sub> &#x2192; a<sub>t</sub> | 2,192 trajectories |\n| LAM\u00b3 | Task-Action Trajectories | LAM\u00b2 + GPT-4o | s<sub>t</sub> &#x2192; a<sub>t</sub> | 2,688 trajectories |\n| LAM\u2074 | Task-Action-Reward Trajectories | RM + LAM\u00b3 | (s<sub>t</sub>,r<sub>t</sub>) &#x2192; a<sub>t</sub> | 1,788 trajectories |\n| Reward Model | Task-Action-Reward Trajectories | GPT-4o + LAM\u00b3 | (s<sub>t</sub>,a<sub>t</sub>) &#x2192; r<sub>t</sub> | 4,476 trajectories |", "caption": "Table 1. Training data summary for each phase of LAM training.", "description": "This table summarizes the training datasets used in each of the four phases of LAM training.  It details the data type, source of the data, the input/output format used for training, and the size of each dataset.  Specifically, it breaks down the data used for training LAM\u00b9, LAM\u00b2, LAM\u00b3, LAM\u2074, and the Reward Model.  This table helps to illustrate the progression of training data complexity and how each phase builds upon the previous one.", "section": "4 MODEL TRAINING"}, {"content": "| Model | TSR (%) | Step Precision (%) | Step Recall (%) |\n|---|---|---|---| \n| LAM\u00b9 | 82.2 | 54.7 | 55.7 |\n| GPT-4o | 84.5 | 28.2 | 66.1 |\n| Mistral-7B | 0.0 | 0.1 | 0.5 |", "caption": "Table 2. Performance (%) comparison of different models on planning.", "description": "This table presents a comparison of the performance of different models, including LAM\u00b9, GPT-40, and Mistral-7B, on planning tasks.  It uses three metrics: Task Success Rate (TSR), Step Precision, and Step Recall.  The table demonstrates that LAM\u00b9 achieves comparable TSR to GPT-40 but with significantly higher Step Precision, indicating more efficient planning.  The baseline Mistral-7B model performs poorly without fine-tuning, highlighting the importance of the training approach used for LAM\u00b9.", "section": "5.2 Task-Plan Pretraining Results (Phase 1)"}, {"content": "| Metric | **LAM**<sup>1</sup> | **LAM**<sup>2</sup> | **LAM**<sup>3</sup> | **LAM**<sup>4</sup> | GPT-4o (Text-only) | GPT-4o Mini (Text-only) |\n|---|---|---|---|---|---|---| \n| Object Acc (%) | 39.4 | 85.6 | 87.4 | **87.8** | 73.2 | 74.6 |\n| Operation Acc (%) | 59.9 | 97.3 | 97.7 | **97.7** | 94.2 | 91.5 |\n| Status Acc (%) | 32.7 | 97.8 | 98.2 | **99.0** | 52.1 | 67.4 |\n| Step Success Rate (SSR) (%) | 33.0 | 83.6 | 85.9 | **86.2** | 68.8 | 73.4 |\n| Task Success Rate (TSR) (%) | 35.6 | 76.8 | 79.3 | **81.2** | 67.2 | 62.3 |", "caption": "Table 3. Offline performance comparison across different models and metrics on decision making.", "description": "This table presents a comparison of offline performance across various models, including LAM variants (LAM\u00b9, LAM\u00b2, LAM\u00b3, LAM\u2074) and GPT-40 (both text-only and Mini versions), using five key metrics: Object Accuracy, Operation Accuracy, Status Accuracy, Step Success Rate, and Task Success Rate.  The comparison highlights the performance differences in decision-making capabilities between the Large Action Models (LAMs) and the baseline GPT models.", "section": "5.3 Task-Action Results (Phases 2-4)"}, {"content": "| Metric | Text-only | | | Text + Visual | |\n|---|---|---|---|---|---|\n| | LAM | GPT-4o | GPT-4o Mini | GPT-4o | GPT-4o Mini |\n| Task Success Rate (%) | 71.0 | 63.0 | 57.8 | 75.5 | 66.7 |\n| Task Completion Time (s) | 30.42 | 86.42 | 35.24 | 96.48 | 46.21 |\n| Task Completion Steps | 5.62 | 6.73 | 5.99 | 4.98 | 6.34 |\n| Average Step Latency (s) | 5.41 | 12.84 | 5.88 | 19.36 | 7.29 |", "caption": "Table 4. Performance comparison of LAM and baseline models across metrics.", "description": "This table presents a comparison of the Large Action Model (LAM) against baseline models, GPT-40 and GPT-40 Mini, across several performance metrics.  The metrics include Task Success Rate (TSR), Task Completion Time, Task Completion Steps, and Average Step Latency. The comparison is further broken down by input modality, showing results for both text-only and text+visual inputs. This comprehensive evaluation aims to demonstrate the effectiveness and efficiency of the LAM, particularly highlighting its advantages as a text-only model in achieving high task success rates and low latency.", "section": "7 ONLINE EVALUATIONS"}]