[{"content": "## Table 1:  Performance Comparison on VQA tasks\n\n| Methods | LLM | Res. | #Vision | Tokens | VQA<sup>v2</sup> | GQA | VisWiz | SciQA | VQA<sup>T</sup> | POPE | MME | MMB | SEED | LLaVA<sup>W</sup> | MM-Vet | Avg. (%) | \n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| **BLIP-2** | Vicuna-13B | 224 | 32 | 65.0 | 41.0 | 19.6 | 61.0 | 42.5 | 85.3 | 1293.8 | - | 46.4 | 38.1 | 22.4 | - |\n| **InstructBLIP** | Vicuna-7B | 224 | 32 | - | 49.2 | 34.5 | 60.5 | 50.1 | - | - | 36.0 | 53.4 | 60.9 | 26.2 | - |\n| **IDEFICS-9B** | LLaMA-7B | 224 | 64 | 50.9 | 38.4 | 35.5 | - | 25.9 | - | - | - | 48.2 | - | - | - |\n| **IDEFICS-80B** | LLaMA-65B | 224 | 64 | 60.0 | 45.2 | 36.0 | - | 30.9 | - | - | - | 54.5 | - | - | - |\n| **Qwen-VL** | Qwen-7B | 448 | 256 | 78.8 | 59.3 | 35.2 | 67.1 | 63.8 | - | - | - | 38.2 | 56.3 | - | - |\n| **Qwen-VL-Chat** | Qwen-7B | 448 | 256 | 78.2 | 57.5 | 38.9 | 68.2 | 61.5 | - | 1487.5 | 60.6 | 58.2 | - | - | - |\n| **SPHINX** | LLaMA-13B | 224 | 289 | 78.1 | 62.6 | 39.9 | 69.3 | 51.6 | 80.7 | 1476.1 | 66.9 | 56.2 | 73.5 | 36.0 | 56.0 |\n| **SPHINX-2k** | LLaMA-13B | 762 | 2890 | 80.7 | 63.1 | 44.9 | 70.6 | 61.2 | 87.2 | 1470.6 | 65.9 | 57.9 | 76.9 | 40.2 | 59.0 |\n| **mPLUG-Owl2** | LLaMA-7B | 448 | 1024 | 79.4 | 56.1 | 54.5 | 68.7 | 54.3 | - | 1450.2 | 64.5 | 57.8 | - | 36.2 | - |\n| **Video-LLaVA** | Vicuna-7B | 224 | 256 | 74.7 | 60.3 | 48.1 | 66.4 | 51.8 | 84.4 | - | 60.9 | - | 73.1 | 32.0 | - |\n| **LLaVA-v1.5** | Vicuna-7B | 336 | 576 | 78.5 | 62.0 | 50.0 | 66.8 | 58.2 | 85.9 | 1510.7 | 64.3 | 58.6 | 63.4 | 30.5 | 56.3 |\n| <span style=\"font-style:italic;\">**LMMs with fewer vision tokens**</span> |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n| **MQT-LLaVA** | Vicuna-7B | 336 | 2 | 61.0 | 50.8 | 48.5 | 65.0 | - | 74.5 | 1144.0 | 54.4 | - | 41.7 | 19.5 | - |\n| **MQT-LLaVA** | Vicuna-7B | 336 | 36 | 73.7 | 58.8 | 51.0 | 66.8 | - | 81.9 | 1416.3 | 63.4 | - | 59.6 | 27.8 | - |\n| **MQT-LLaVA** | Vicuna-7B | 336 | 256 | 76.8 | 61.6 | 53.1 | 67.6 | - | 84.4 | 1434.5 | 64.3 | - | 64.6 | 29.8 | - |\n| **PruMerge** | Vicuna-7B | 336 | 32 | 72.0 | - | - | 68.5 | 56.0 | 76.3 | 1350.3 | 60.9 | - | - | - | - |\n| **PruMerge++** | Vicuna-7B | 336 | 144 | 76.8 | - | - | 68.3 | 57.1 | 84.0 | 1462.4 | 64.9 | - | - | - | - |\n| **LLaMA-VID** | Vicuna-7B | 336 | 2 | - | 55.5 | - | 68.8 | 49.0 | 83.1 | - | - | - | - | - | - |\n| **VoCo-LLaMA** | Vicuna-7B | 336 | 1 | 72.3 | 57.0 | - | 65.4 | - | 81.4 | 1323.3 | 58.8 | 53.7 | - | - | - |\n| **TokenPacker** | Vicuna-7B | 336 | 36 | 75.0 | 59.6 | 50.2 | - | - | 86.2 | - | 62.8 | - | - | 29.6 | - |\n| <span style=\"font-style:italic;\">**Ours**</span> |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n| **LLaVA-Mini** | Vicuna-7B | 336 | 1 | 77.6 | 60.9 | 56.2 | 70.4 | 57.0 | 84.4 | 1466.0 | 65.6 | 58.5 | 68.9 | 36.6 | 57.9 |\n| \u0394 <span style=\"font-style:italic;\">compare to LLaVA-v1.5</span> |  |  0.17% |  | -0.9 | -1.1 | +6.1 | +3.6 | -1.3 | -1.5 | -44.7 | +1.3 | -0.1 | +5.5 | +6.1 | +1.6 |\n| **LLaVA-Mini-HD** | Vicuna-7B | 672 | 64 | 78.9 | 61.8 | 58.5 | 69.7 | 59.1 | 85.3 | 1476.8 | 67.5 | 60.2 | 69.3 | 33.9 | 58.6 |\n| \u0394 <span style=\"font-style:italic;\">compare to LLaVA-v1.5</span> |  | 11.1% |  | +0.4 | -0.2 | +8.5 | +2.9 | +0.9 | -0.6 | -33.9 | +3.2 | +1.6 | +5.9 | +3.4 | +2.4 |\n| **LLaVA-Mini*** (Image & Video) | LLaMA-3.1-8B-Instruct | 336 | 1 | 79.0 | 61.3 | 57.4 | 83.1 | 58.5 | 85.3 | 1522.7 | 71.6 | 63.0 | 70.2 | 37.2 | 60.7 |\n", "caption": "Table 1: Performance on 11 image-based benchmarks. \u2018Res.\u2019 is resolution and \u2018#Vision Tokens\u2019 is the number of vision tokens fed to LLM backbone. \u2018*\u2019 indicates that involving extra training data.", "description": "This table presents a comparison of different large multimodal models (LMMs) on eleven image-based benchmarks.  The benchmarks evaluate various aspects of visual understanding.  Key metrics include the model's accuracy on each benchmark, the resolution of images used ('Res.'), and the number of vision tokens processed by the large language model (LLM) backbone ('#Vision Tokens').  The asterisk (*) indicates that additional training data was used for that specific model.  The table allows for an assessment of the trade-off between model performance and computational efficiency in the context of different LLM architectures.", "section": "5.1 Experimental Setting"}, {"content": "| #Vision |\n|---|---| \n| Tokens |", "caption": "Table 2: Performance on video-based open-ended generative benchmarks. We report accuracy (%) for question-answer, and scores (1-5, higher is better) for question-answer and generative performance. Results marked with bold and underlined indicate the best and second best, respectively.", "description": "This table presents the performance of various large multimodal models (LMMs) on video-based open-ended generative benchmarks.  It evaluates both question-answering accuracy (percentage) and a 1-5 rating for both question-answering quality and overall generative performance.  The best and second-best results for each metric are highlighted in bold and underlined.", "section": "5. Experimental Setting"}, {"content": "| Methods | #Frames | #VisionTokensper Frame | Video-based Question-Answer | Video-based Question-Answer | Video-based Question-Answer | Video-based Generative Performance | Video-based Generative Performance | Video-based Generative Performance | Video-based Generative Performance | Video-based Generative Performance | Avg. |  |  |  |  |  | \n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| **LLaMA Adapter** | 5 | 256 | 54.9 | 3.1 | 43.8 | 2.7 | 34.2 | 2.7 | 2.03 | 2.32 | 2.30 | 1.98 | 2.15 | 2.19 |  |  |  | \n| **VideoChat** | 16 | 32 | 56.3 | 2.8 | 45.0 | 2.5 | 26.5 | 2.2 | 2.23 | 2.50 | 2.53 | 1.94 | 2.24 | 2.30 |  |  |  | \n| **Video-LLaMA** | 16 | 64 | 51.6 | 2.5 | 29.6 | 1.8 | 12.4 | 1.1 | 1.96 | 2.18 | 2.16 | 1.82 | 1.79 | 1.99 |  |  |  | \n| **Video-ChatGPT** | 100 | ~3.6 | 64.9 | 3.3 | 49.3 | 2.8 | 35.2 | 2.7 | 2.40 | 2.52 | 2.62 | 1.98 | 2.37 | 2.37 |  |  |  | \n| **BT-Adapter** | 100 | ~2.6 | 67.5 | 3.7 | 57.0 | 3.2 | 45.7 | 3.2 | 2.68 | 2.69 | 3.27 | 2.34 | 2.46 | 2.69 |  |  |  | \n| **MovieChat** | 2048 | 32 | **75.2** | 3.8 | 52.7 | 2.6 | 45.7 | **3.4** | 2.76 | 2.93 | 3.01 | 2.24 | 2.42 | 2.65 |  |  |  | \n| **LLaMA-VID** | 1fps | 2 | 69.7 | 3.7 | 57.7 | 3.2 | **47.4** | 3.3 | **2.96** | **3.00** | **3.53** | **2.46** | **2.51** | **2.88** |  |  |  | \n| **Video-LLaVA** | 8 | 256 | 70.7 | **3.9** | **59.2** | **3.5** | 45.3 | 3.3 | 2.87 | 2.94 | 3.44 | 2.45 | **2.51** | 2.84 |  |  |  | \n| **LLaVA-Mini** | 1fps | **1** | **70.9** | 4.0 | **59.5** | **3.6** | **53.5** | **3.5** | **2.97** | **2.99** | **3.61** | **2.48** | **2.67** | **2.94** |  |  |  | ", "caption": "Table 3: Performance on MVBench (accuracy). Detailed scores are reported in Appendix H.", "description": "This table presents the performance of different large multimodal models (LLMs) on the MVBench benchmark, which evaluates various aspects of video understanding capabilities. The accuracy of each model is shown across 20 sub-tasks of MVBench, including action recognition, object detection, scene understanding, and others.  The table helps compare the overall performance of the models on video understanding and analyze their strengths and weaknesses in different sub-tasks. Detailed scores are provided in Appendix H of the paper.", "section": "5.1 Experimental Setting"}, {"content": "| Methods | Action | Object | Position | Scene | Count | Attribute | Pose | Character | Cognition | Avg. | \n|---|---|---|---|---|---|---|---|---|---|---|\n| **mPLUG-Owl** | 28.4 | 33.0 | 25.0 | 29.0 | 29.3 | 42.0 | 24.0 | 31.0 | 25.3 | 29.7 | \n| **Video-ChatGPT** | 32.1 | 40.7 | 21.5 | 31.0 | 28.0 | 44.0 | 29.0 | 33.0 | 30.3 | 32.7 | \n| **Video-LLaMA** | 34.4 | 42.2 | 22.5 | 43.0 | 28.3 | 39.0 | 32.5 | 40.0 | 29.3 | 34.1 | \n| **VideoChat** | 38.0 | 41.2 | 26.3 | 48.5 | 27.8 | 44.3 | 26.5 | 41.0 | 27.7 | 35.5 | \n| **LLaMA-VID** | 43.4 | 36.7 | 39.8 | 22.0 | 36.5 | 37.3 | 37.5 | 34.0 | 60.5 | 41.4 | \n| **Video-LLaVA** | 48.0 | 46.5 | 27.8 | 84.5 | 35.5 | 45.8 | 34.0 | 42.5 | 34.2 | 43.1 | \n| **LLaVA-Mini** | 52.1 | 43.2 | 31.8 | 85.5 | 37.5 | 44.5 | 29.5 | 52.0 | 35.0 | 44.5 |", "caption": "Table 8: Effect of query-based compression.", "description": "This table compares the performance of LLaVA-Mini using query-based compression against average pooling for compressing vision tokens.  It shows the impact of different compression methods on visual question answering tasks using the VQA, GQA, and MMBench datasets.  The results highlight the effectiveness of query-based compression in balancing performance and computational cost.", "section": "6.2 EFFECT OF COMPRESSION"}, {"content": "| Methods | #Frames | Holistic TR | Holistic AR | Single Detail NQA | Single Detail ER | Single Detail PQA | Multi Detail AO | Multi Detail AC | Avg. | Avg. Video Duration (minute) | 7 | 10 | 14 | 10 | 8 | 16 | 13 | 11 | Max Video Duration (minute) | 20 | 543 | 139 | 20 | 13 | 137 | 130 | 143 | Video-ChatGPT | 100 | 26.9 | 24.0 | 40.3 | 42.0 | 29.9 | 25.1 | 31.1 | 31.3 | MovieChat | 2048 | 29.5 | 25.0 | 24.2 | 24.7 | 25.8 | 28.6 | 22.8 | 25.8 | Movie-LLM | 1fps | 30.0 | 29.0 | 29.6 | 24.7 | 24.1 | 20.5 | 24.8 | 26.1 | TimeChat | 96 | 23.1 | 27.0 | 24.5 | 28.4 | 25.8 | 24.7 | 32.0 | 30.9 | LLaMA-VID | 1fps | 50.8 | 34.5 | 30.1 | 32.7 | 32.5 | 23.9 | 27.8 | 33.2 | MA-LMM | 1000 | 51.9 | 35.5 | 43.1 | 38.9 | 35.8 | 25.1 | 24.3 | 36.4 | LLaVA-Mini | 1fps | 76.0 | 50.0 | 44.5 | 37.5 | 49.0 | 24.3 | 18.4 | 42.8 |", "caption": "Table 9: Training details of LLaVA-Mini.", "description": "This table details the training hyperparameters and settings used in two stages of training the LLaVA-Mini model. Stage 1 focuses on vision-language pretraining, while Stage 2 involves instruction tuning.  The table specifies which modules (vision encoder, projection, large language model, compression, and modality pre-fusion) were trained or frozen during each stage, along with details like batch size, learning rate, and the optimizer used. It provides a comprehensive overview of the training configuration for LLaVA-Mini.", "section": "4 LLaVA-MINI"}, {"content": "| Methods | #Frames | EgoSchema |\n|---|---|---:|\n| **Random** | - | 20 |\n| **mPLUG-Owl** | 16 | 31.1 |\n| **InternVideo** | 16 | 32.1 |\n| **Video-ChatGPT** | 100 | 36.2 |\n| **VideoChat** | 16 | **42.2** |\n| **TimeChat** | 96 | 33.0 |\n| **LLaMA-VID** | 1fps | 38.5 |\n| **Video-LLaVA** | 8 | 38.4 |\n| **LLaVA-Mini** | 1fps | **51.2** |", "caption": "Table 10: Comparison of LLaVA-Mini with previous token merging methods.", "description": "This table compares the performance of LLaVA-Mini against other methods that also reduce the number of vision tokens used in LLMs.  It shows the performance of several models (including variations of MQT-LLaVA and PruMerge) on three benchmark metrics (VQAV2, GQA, and MMB), each using a different number of vision tokens. The goal is to demonstrate LLaVA-Mini's effectiveness at achieving strong performance even with a drastically reduced number of vision tokens.", "section": "5.2 MAIN RESULTS"}, {"content": "| Methods | Pre-fusion#Layers | #VisionTokens | FLOPs(T) | VQA<sup>v2</sup> | GQA | MMB |\n|---|---|---|---|---|---|---|\n| **LLaVA-v1.5** | - | 576 | 8.55 | 78.5 | 62.0 | 64.3 |\n| **LLaVA-Mini (w/o pre-fusion)** | 0 | 1 | 0.96 | 72.4 | 54.2 | 57.7 |\n|  | 0 | 16 | 1.16 | 74.1 | 55.4 | 59.2 |\n|  | 0 | 64 | 1.79 | 75.3 | 56.7 | 62.1 |\n|  | 0 | 144 | 2.85 | 76.9 | 58.9 | 64.9 |\n| **LLaVA-Mini (w/ pre-fusion)** | 1 | 1 | 1.21 | 74.8 | 55.5 | 60.4 |\n|  | 2 | 1 | 1.46 | 76.0 | 57.6 | 63.1 |\n|  | 3 | 1 | 1.81 | 76.9 | 59.1 | 64.9 |\n|  | 4 | 1 | 1.96 | 77.6 | 60.9 | 65.6 |", "caption": "Table 11: Performance of LLaVA-Mini when using only pre-fusion module without compression.", "description": "This table presents a comparison of the performance of LLaVA-Mini with and without its compression module, while keeping the pre-fusion module.  The performance metrics used are VQA-v2, GQA, and MMB accuracy, and the number of vision tokens used is specified.  This demonstrates the impact of the pre-fusion module on the model's performance.", "section": "E.2 EFFECT OF MODALITY PRE-FUSION"}, {"content": "| Methods | Res. | #Vision Tokens | VQA<sup>v2</sup> | GQA | MMB |\n|---|---|---|---|---|---| \n| **LLaVA-v1.5** | 336 | 576 | 78.5 | 62.0 | 64.3 |\n| **LLaVA-Mini** | 336 | 1 | 77.6 | 60.9 | 65.6 |\n|  | 336 | 4 | 77.7 | 60.9 | 66.7 |\n|  | 336 | 16 | 78.1 | 61.3 | 66.6 |\n|  | 336 | 64 | **78.5** | **61.6** | **67.5** |\n|  | 672 | 16 | 78.5 | 61.5 | 67.4 |\n|  | 672 | 64 | 78.9 | 61.8 | 67.5 |\n|  | 672 | 144 | 79.3 | 62.3 | 67.9 |\n|  | 672 | 576 | **80.0** | **62.9** | **68.1** |", "caption": "Table 12: Comparison of performing compression and pre-fusion outside or within LLM backbone.", "description": "This table compares the performance of LLaVA-Mini when the compression and modality pre-fusion modules are placed outside the LLM backbone versus inside.  It shows that placing these modules outside the LLM yields better results (higher performance in VQAv2, GQA, and MMB) with a comparable FLOPs count.  This highlights the advantage of the LLaVA-Mini architecture for efficiency and performance.", "section": "E.3 WHY PREFORMING COMPRESSION AND PRE-FUSION OUTSIDE LLM BACKBONE?"}, {"content": "| Compression | #VisionTokens | FLOPs | VQA<sup>v2</sup> | GQA | MMB |\n|---|---|---|---|---|---| \n| **Average Pooling** | 1 | 1.96T | 76.1 | 59.8 | 64.0 |\n| **Query-based** |  | +2.42G | **77.6** | **60.9** | **65.6** |\n| **Average Pooling** | 4 | 2.01T | 76.9 | 60.3 | 65.1 |\n| **Query-based** |  | +2.44G | **77.7** | **60.9** | **66.7** |", "caption": "Table 13: Inference latency (millisecond) of LLaVA-Mini on various hardware platforms.", "description": "This table presents a comparison of inference latency (in milliseconds) for the LLaVA-Mini model across three different hardware platforms: RTX 3090 (with 24GB of memory), A100 (with 40GB of memory), and A800 (with 80GB of memory).  The latency is measured for varying numbers of vision tokens used in the model (1, 4, 16, and 64).  This allows for an assessment of the model's efficiency and scalability across different hardware configurations and varying model complexities.  The comparison also includes latency for the LLaVA-v1.5 model (using 576 vision tokens) as a baseline for evaluating the performance gains of LLaVA-Mini.", "section": "5.3 Efficiency"}, {"content": "| Settings | Stage1 | Stage2 |\n|---|---|---|\n| **Vision-Language Pretraining** | Vision-Language Pretraining | Instruction Turning |\n| **Modules** |  |  |\n| Vision Encoder | Frozen | Frozen |\n| Projection | Trainable | Trainable |\n| Large Language Model | Frozen | Trainable |\n| Compression | N/A | Trainable |\n| Modality Pre-fusion | N/A | Trainable |\n| **Hyperparameters** |  |  |\n| Batch Size | 256 | 256 |\n| Learning Rate | - | 1e-4 |\n| MM Learning Rate | 1e-3 | 1e-5 |\n| Schedule | Cosine decay | Cosine decay |\n| Warmup Ratio | 0.03 | 0.03 |\n| Optimizer | AdamW | AdamW |\n| Epoch | 1 | 2 |", "caption": "Table 14: Computational overhead (FLOPs) of each component in LLaVA-Mini.", "description": "This table breaks down the computational cost (measured in FLOPs) for each component of the LLaVA-Mini model. It compares LLaVA-Mini's computational efficiency against the original LLaVA-v1.5 model at two different resolutions (336x336 and 672x672 pixels).  The components analyzed include the Vision Encoder, Projection, Compression, Pre-fusion, and LLM. The total FLOPs for each model at each resolution is also shown, highlighting the significant reduction in computational cost achieved by LLaVA-Mini.", "section": "5.3 Efficiency"}, {"content": "| Methods | #Vision Tokens | VQA<sup>v2</sup> | GQA | MMB |\n|---|---|---|---|---|\n| **MQT-LLaVA** | 2 | 61.0 | 50.8 | 54.4 |\n| **MQT-LLaVA** | 36 | 73.7 | 58.8 | 63.4 |\n| **MQT-LLaVA** | 256 | 76.8 | 61.6 | 64.3 |\n| **PruMerge** | 32 | 72.0 | - | 60.9 |\n| **PruMerge++** | 144 | 76.8 | - | 64.9 |\n| **LLaVA-Mini** | 1 | 72.4 | 54.2 | 57.7 |\n| **LLaVA-Mini** | 16 | 74.1 | 55.4 | 59.2 |\n| **LLaVA-Mini** | 64 | 75.3 | 56.7 | 62.1 |\n| **LLaVA-Mini** | 144 | 76.9 | 58.9 | 64.9 |", "caption": "Table 15: Detailed results on 20 subsets of MVBench.", "description": "This table presents a detailed breakdown of the performance results for LLaVA-Mini on the MVBench benchmark. MVBench is a comprehensive evaluation benchmark for multimodal video understanding, consisting of 20 challenging tasks across different aspects of video understanding, including action, object, position, scene, count, attribute, pose, character, and cognition. The table shows LLaVA-Mini's performance on each of these 20 individual sub-tasks, allowing for a granular analysis of its strengths and weaknesses in various video understanding capabilities.  The results are presented in terms of accuracy for each sub-task, providing a precise and comprehensive view of LLaVA-Mini's overall performance on the benchmark.", "section": "5. EXPERIMENTAL RESULTS"}]