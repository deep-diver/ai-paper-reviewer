[{"figure_path": "https://arxiv.org/html/2411.18412/x1.png", "caption": "Figure 1: Our model significantly outperforms state-of-the-art all-in-one image restoration (IR) methods, Restormer\u00a0[63], PromptIR\u00a0[40], and DiffUIR\u00a0[66], across five known IR tasks, three unseen tasks, and three mixed degradation scenarios. The plot is normalized along each axis, with the lowest value positioned on the second circle and the highest value on the outermost circle.", "description": "This figure compares the performance of the proposed Adaptive Blind All-in-One Image Restoration (ABAIR) model against three state-of-the-art all-in-one image restoration methods: Restormer, PromptIR, and DiffUIR.  The comparison is done across a total of 11 different image restoration tasks. These tasks are categorized into three groups: 5 known tasks (common image restoration problems), 3 unseen tasks (representing generalization to previously unknown restoration problems), and 3 mixed degradation scenarios (combining multiple types of image degradations to simulate real-world conditions).  The radial plot visually represents the performance of each model on each task, with the outermost circle representing the best performance and the innermost circle representing the worst.  Each axis of the plot represents one of the three task categories (known, unseen, and mixed), making it easy to compare the models' overall performance across diverse image restoration scenarios.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.18412/x2.png", "caption": "Figure 2: General schema of our proposed method. Our method is divided into three phases. In Phase I we pre-train our baseline model with synthetic degradations of high-fidelity images. Each image contains different degradations in different regions, and a segmentation head learns to predict them, while a restoration loss aims at restoring the image. In this way, the model is able to distinguish and generalize well to multiple degradations. In Phase II, we learn degradation-specific adaptors using standard image restoration datasets. In Phase III, we learn a lightweight degradation estimator to adaptively blend the adapters based on the degradation profile of the input image. This 3-phase methodology makes our method flexible to deal with images containing multiple distortions and easy to update for new ones as it only requires training an adapter for the new distortion and retraining the degradation estimator.", "description": "This figure illustrates the three-phase training process of the proposed Adaptive Blind All-in-One Image Restoration (ABAIR) model. Phase I involves pre-training a baseline model on a large dataset of high-fidelity images with synthetically generated composite degradations (e.g., combined noise, blur, rain, etc.). A segmentation head is trained simultaneously to predict the type of degradation in each image region. Phase II focuses on adapting the pre-trained model to specific degradation types by training independent low-rank adapters (LoRAs) on standard image restoration datasets. Finally, in Phase III, a lightweight degradation estimator is trained to adaptively select or blend the appropriate adapters based on the input image's degradation characteristics. This flexible three-phase training approach enables the model to handle unseen degradations and composite distortions efficiently and allows for easy updates with new degradation types by only training additional adapters and the estimator.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.18412/extracted/6029195/images/synthetic_examples/fig3_rainy.jpg", "caption": "(a) Rain", "description": "This figure shows an example of synthetic rain degradation generated for pre-training the image restoration model.  It visually demonstrates how the model's input images are augmented with various levels of simulated rain, allowing the model to learn how to remove this type of degradation from real-world images. The specific parameters used to generate this particular rain effect are not explicitly given in the caption but are discussed in the supplementary material of the paper.", "section": "Phase I: Pre-training with synthetic degradations"}, {"figure_path": "https://arxiv.org/html/2411.18412/extracted/6029195/images/synthetic_examples/fig3_hazy.jpg", "caption": "(b) Haze", "description": "This image shows an example of synthetically generated haze for image restoration. The image demonstrates the effect of haze, obscuring details and reducing visibility. It is part of a dataset used for training a model capable of removing this type of degradation, allowing the model to effectively enhance image quality.", "section": "Phase I: Pre-training with synthetic degradations"}, {"figure_path": "https://arxiv.org/html/2411.18412/extracted/6029195/images/synthetic_examples/fig3_noise.jpg", "caption": "(c) Noise", "description": "This image shows examples of synthetically generated noise degradation for image restoration experiments. Different noise levels are shown to demonstrate the range of noise intensities that the model was trained on.", "section": "Phase I: Pre-training with synthetic degradations"}, {"figure_path": "https://arxiv.org/html/2411.18412/extracted/6029195/images/synthetic_examples/fig3_blurry.jpg", "caption": "(d) Blur", "description": "This figure shows examples of synthetically generated blur degradation.  Different levels of blur are simulated, demonstrating the range of degradation levels achievable through this process in the dataset. The blur is created programmatically, not by applying a real-world blur effect to images. This is part of a larger pipeline to produce a dataset of training images with various synthetic degradations, which includes rain, haze, noise, and low-light, in addition to the blur shown in this figure.", "section": "Phase I: Pre-training with synthetic degradations"}, {"figure_path": "https://arxiv.org/html/2411.18412/extracted/6029195/images/synthetic_examples/fig3_lol.jpg", "caption": "(e) Low-light", "description": "This figure shows a sample image with low-light degradation.  It is one of five examples in a series illustrating different types of synthetic degradation used to pre-train the ABAIR model. The goal is to train the model on a wide variety of synthetically degraded images to improve generalization and robustness for unseen degradations during real-world applications.", "section": "Phase I: Pre-training with synthetic degradations"}, {"figure_path": "https://arxiv.org/html/2411.18412/x28.png", "caption": "Figure 3: Examples of our synthetic degradation generation for five traditional distortions.", "description": "This figure displays examples of synthetically generated image degradations used in the training process of the proposed model.  It showcases five common image distortions: rain, haze, noise, blur, and low-light. Each example demonstrates the type and approximate severity of each distortion.  The purpose of this figure is to illustrate the variety of artificial degradations used to create a robust pre-training dataset, helping the model generalize better to real-world scenarios.", "section": "Phase I: Pre-training with synthetic degradations"}, {"figure_path": "https://arxiv.org/html/2411.18412/x29.png", "caption": "Figure 4: Qualitative results for single degradation removal, including deblurring on the GoPro\u00a0[35] dataset, denoising on the LoLv1\u00a0[52] dataset, and deraining on the Rain100H\u00a0[56] dataset.", "description": "This figure showcases the qualitative results of the proposed ABAIR model on single degradation removal tasks.  It presents visual comparisons of the input images with the outputs generated by the ABAIR model, Restormer [63], and PromptIR [40]. The results are shown for three specific degradation types: deblurring (using the GoPro [35] dataset), denoising (using the LoLv1 [52] dataset), and deraining (using the Rain100H [56] dataset).  This allows for a direct visual assessment of the model's performance in restoring image quality across various types of single degradations.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.18412/x30.png", "caption": "Figure 5: Qualitative results for unseen IR tasks, including JPEG artifact removal and 4-to-8 bit reconstruction. PromptIR\u00a0[40] and Ours are not trained for this task, while Ours retrained has a specified LoRA in an 8-degradation setup.", "description": "This figure demonstrates the model's generalization ability to unseen image degradation types.  It presents qualitative results comparing the performance of the proposed model, a state-of-the-art model (PromptIR), and a version of the proposed model that has been retrained to include these unseen degradation types. The unseen tasks highlighted are JPEG artifact removal and 4-to-8-bit image reconstruction.  The results show that while neither the proposed model nor PromptIR were trained on these specific tasks, the retrained version of the proposed model achieves significantly better performance.", "section": "4. Experiments"}]