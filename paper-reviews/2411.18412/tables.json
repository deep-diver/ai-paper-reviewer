[{"content": "| PSNR/SSIM | Deraining |  | Dehazing |  | Denoising |  | Deblurring |  | Low-Light |  | Average |  | Param. | \n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  | Rain100L |  | SOTS (Out) |  | BSD68<sub>\u03c3=25</sub> |  | GoPro |  | LoLv1 |  |  |  |  |\n| AirNet [27] | 32.98 | 0.951 | 21.04 | 0.884 | 30.91 | 0.882 | 24.35 | 0.781 | 18.18 | 0.735 | 25.49 | 0.847 | 9M |\n| Uformer [51] | 35.48 | 0.967 | 27.20 | 0.958 | 30.59 | 0.869 | 26.41 | 0.809 | 21.40 | 0.808 | 28.21 | 0.882 | 52M |\n| IDR [64] | 35.63 | 0.965 | 25.24 | 0.943 | 31.60 | 0.887 | 27.87 | 0.846 | 21.34 | 0.826 | 28.34 | 0.893 | 15M |\n| X-Restormer [8] | 35.42 | 0.968 | 27.58 | 0.959 | 30.92 | 0.880 | 27.54 | 0.835 | 20.88 | 0.817 | 28.47 | 0.891 | 26M |\n| DA-CLIP [30] | 35.49 | 0.970 | 28.10 | 0.962 | 30.42 | 0.859 | 26.50 | 0.807 | 21.94 | 0.817 | 28.49 | 0.880 | 174M |\n| DiffUIR [66] | 35.52 | 0.969 | 28.17 | 0.964 | 30.92 | 0.879 | 26.99 | 0.821 | 20.92 | 0.789 | 28.50 | 0.880 | 36M |\n| Restormer [63] | 35.56 | 0.970 | 27.94 | 0.962 | 30.74 | 0.875 | 26.84 | 0.818 | 21.74 | 0.815 | 28.56 | 0.888 | 26M |\n| PromptIR [40] | 35.40 | 0.967 | 28.26 | 0.965 | 30.89 | 0.872 | 26.55 | 0.808 | 21.80 | 0.815 | 28.58 | 0.885 | 36M |\n| Ours OH | 37.73 | 0.978 | 33.46 | 0.983 | 31.38 | 0.898 | 29.00 | 0.878 | 24.20 | 0.865 | 31.15 | 0.920 | 59M |\n| Ours SW | 37.79 | 0.979 | 33.48 | 0.984 | 31.38 | 0.898 | 29.00 | 0.878 | 24.19 | 0.865 | 31.17 | 0.921 | 59M |\n| Ours (Oracle) | 39.09 | 0.981 | 33.54 | 0.984 | 31.40 | 0.901 | 29.10 | 0.879 | 24.45 | 0.866 | 31.39 | 0.922 | 59M |", "caption": "Table 1: 5-degradations setup. Quantitative results on five IR datasets comparing the state-of-the-art all-in-one methods and our approach. Ours (Oracle) is an upper bound for our approach: it computes the best reachable value in case our estimator always chooses the correct degradation.", "description": "This table presents a quantitative comparison of different all-in-one image restoration (IR) methods on five standard IR datasets, each focusing on a specific type of image degradation (deraining, dehazing, denoising, deblurring, and low-light enhancement).  The performance of state-of-the-art all-in-one models is compared to the proposed approach, ABAIR.  The results are evaluated using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) metrics.  An additional row, 'Ours (Oracle)', shows the best possible performance of the ABAIR model assuming perfect degradation type estimation, providing an upper bound for the model's potential.", "section": "4. Experiments"}, {"content": "| PSNR/SSIM | Deraining |  | Dehazing |  | Denoising |  |  |  | Average |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|  | Rain100L |  | SOTS (Out) |  | BSD68<sub>\u03c3=15</sub> |  | BSD68<sub>\u03c3=25</sub> |  | BSD68<sub>\u03c3=50</sub> |  |  |\n| DL [14] | 32.62 | 0.931 | 26.92 | 0.931 | 33.05 | 0.914 | 30.41 | 0.861 | 26.90 | 0.740 | 29.98 | 0.875 |\n| MPRNet [62] | 33.57 | 0.954 | 25.28 | 0.954 | 33.54 | 0.927 | 30.89 | 0.880 | 27.56 | 0.779 | 30.17 | 0.899 |\n| AirNet [27] | 34.90 | 0.967 | 27.94 | 0.962 | 33.92 | 0.933 | 31.26 | 0.888 | 28.00 | 0.797 | 31.20 | 0.909 |\n| Restormer [63] | 35.56 | 0.969 | 29.92 | 0.970 | 33.86 | 0.933 | 31.20 | 0.888 | 27.90 | 0.794 | 31.69 | 0.911 |\n| PromptIR [40] | 36.37 | 0.972 | 30.58 | 0.974 | 33.98 | 0.933 | 31.31 | 0.888 | 28.06 | 0.799 | 32.06 | 0.913 |\n| Ours OH | 38.58 | 0.981 | 33.71 | 0.985 | 33.95 | 0.934 | 31.29 | 0.889 | 28.04 | 0.798 | 33.11 | 0.918 |\n| Ours SW | 38.52 | 0.980 | 33.62 | 0.984 | 33.95 | 0.933 | 31.24 | 0.889 | 28.01 | 0.796 | 33.07 | 0.916 |", "caption": "Table 2: 3-degradations setup. Quantitative results on three IR datasets comparing the state-of-the-art all-in-one methods and our approach.", "description": "This table presents a quantitative comparison of different all-in-one image restoration (IR) methods on three image degradation types: deraining, dehazing, and denoising.  The performance of several state-of-the-art methods is compared against the proposed ABAIR method, evaluating both PSNR and SSIM metrics across three benchmark datasets (Rain100L, SOTS (Out), and BSD68). The results highlight the effectiveness of the ABAIR model in handling multiple degradation types simultaneously.", "section": "4. Experiments"}, {"content": "| PSNR/SSIM | Deraining (Rain100H) |  | Deblurring (HIDE) |  | Low-Light (Lolv2-Real) |  |\n|---|---|---|---|---|---|---|\n| IDR [64] | 11.32 | .397 | 16.83 | .621 | 17.61 | .697 |\n| X-Restormer [8] | 14.08 | .437 | 25.40 | .801 | 25.42 | .876 |\n| DiffUIR [66] | 14.78 | .487 | 23.98 | .739 | 26.12 | .861 |\n| Restormer [63] | 14.50 | .464 | 24.42 | .781 | 27.12 | .877 |\n| PromptIR [40] | 14.28 | .444 | 24.49 | .762 | 27.70 | .870 |\n| Ours OH | **21.69** | **.692** | 27.04 | **.850** | **28.09** | **.907** |\n| Ours SW | 19.37 | .594 | **27.05** | **.850** | **28.09** | .906 |", "caption": "Table 3: Quantitative results on additional test datasets with the learned degradations.", "description": "This table presents quantitative results, specifically PSNR and SSIM scores, comparing different image restoration methods on three additional datasets. These datasets were not used during the training process of the models.  The purpose is to evaluate the generalization capability of the models to unseen data, thereby assessing their robustness and applicability in real-world scenarios where diverse degradation types are common.", "section": "4. Experiments"}, {"content": "| PSNR/SSIM | 4-to-8 bits |  | JPEG Q20 |  | Desnowing |  | \n|---|---|---|---|---|---|---|---| \n| IDR [64] | 24.02 | 0.738 | 26.51 | 0.913 | 18.00 | 0.649 |  | \n| X-Restormer [8] | 24.73 | 0.745 | 26.86 | 0.922 | 18.51 | 0.681 |  | \n| DiffUIR [66] | 24.68 | 0.743 | 26.88 | 0.921 | 18.39 | 0.671 |  | \n| Restormer [63] | 24.64 | 0.743 | 26.90 | 0.929 | 18.14 | 0.655 |  | \n| PromptIR [40] | 24.70 | 0.740 | 26.60 | 0.920 | 18.49 | 0.673 |  | \n| Ours OH | 25.25 | 0.742 | 29.20 | 0.931 | 18.71 | 0.684 |  | \n| Ours SW | 25.32 | 0.743 | 29.35 | 0.926 | 18.67 | 0.683 |  | \n| Ours OH* | 29.14 | 0.826 | 30.82 | 0.943 | 24.19 | 0.797 |  | \n| Ours SW* | 29.03 | 0.810 | 30.71 | 0.939 | 24.02 | 0.779 |  | ", "caption": "Table 4: Quantitative results for unseen IR tasks. Note that the models have not been trained for these degradations. Ours* shows results for the lightweight re-training scenario. New adapters are trained for the new tasks and the estimator is retrained with 8 tasks (5-IR case + 3 new ones; only 8M training parameters).", "description": "This table presents a quantitative comparison of different image restoration (IR) models on three unseen IR tasks: 4-to-8 bits JPEG compression artifact removal, JPEG compression artifact removal, and snow removal.  The models were not trained on these specific degradation types. The results for the proposed method ('Ours') are shown both without ('Ours') and with ('Ours*') a lightweight retraining approach.  The lightweight retraining involved training new adapters for these three tasks and retraining the degradation estimator with all eight tasks (the original five plus the three new ones), resulting in a model with only 8M parameters. The table allows readers to assess how well different IR models generalize to unseen degradation types and the effectiveness of the lightweight retraining strategy.", "section": "4. Experiments"}, {"content": "| PSNR/SSIM | Blur&Noise |  | Blur&JPEG |  | Haze&Snow |  |\n|---|---|---|---|---|---|---|\n| IDR [64] | 21.98 | .683 | 23.02 | .681 | 20.51 | .789 |\n| X-Restormer [8] | 22.67 | .669 | 23.98 | .710 | 20.76 | .805 |\n| DiffUIR [66] | 22.71 | .670 | 24.00 | .711 | 20.86 | .802 |\n| Restormer [63] | 22.35 | .662 | 23.24 | .698 | 20.76 | .800 |\n| PromptIR [40] | 22.89 | .671 | 23.92 | .705 | 20.94 | .803 |\n| X-Restormer [8] | 22.67 | .669 | 23.98 | .710 | 20.76 | .805 |\n| Ours OH | 24.30 | .743 | 24.81 | .717 | 21.48 | .834 |\n| Ours SW | 25.14 | .750 | 24.97 | .719 | 22.09 | .839 |", "caption": "Table 5: Quantitative results on datasets with mixed degradations.", "description": "This table presents a quantitative comparison of different all-in-one image restoration (IR) methods on datasets containing images with mixed degradations (multiple types of image distortions combined).  The methods are evaluated using PSNR and SSIM metrics, which assess the peak signal-to-noise ratio and structural similarity between the restored images and their ground truth counterparts. The table helps demonstrate the ability of each method to handle complex real-world image degradations.", "section": "4. Experiments"}, {"content": "| Pre-training | PSNR | SSIM |\n|---|---|---|\n| IR datasets | 28.50 | .892 |\n| GLD+synth. | 30.63 | .913 |\n| + CutMix | 31.09 | .920 |\n| + Aux. segm. | 31.17 | .921 |", "caption": "Table 6: Ablation studies on types of pre-training for Phase I, and the rank of LoRA\u00a0[19] for Phase II and III.", "description": "This table presents the results of ablation studies conducted to analyze the impact of different pre-training strategies and LoRA rank settings on the performance of the proposed Adaptive Blind All-in-One Image Restoration (ABAIR) model.  Phase I pre-training involves training the model on various datasets and with different synthetic degradations.  Phase II and III involve fine-tuning with LoRA adapters of different ranks. The table shows the impact of these choices on PSNR and SSIM metrics.", "section": "3. Method"}, {"content": "| Rank | PSNR | SSIM | Params |\n|---|---|---|---| \n| 4 | 31.17 | .921 | 3.6M |\n| 8 | 31.14 | .920 | 7.2M |\n| 16 | 30.97 | .916 | 14.3M |", "caption": "Table 7: Ablation study on different low-rank adapters and their rank. Results are the mean for all images. LoRA outperforms both VeRA and Conv-LORA. Lower ranks perform better.", "description": "This table presents an ablation study comparing the performance of three different low-rank adapter methods (LoRA, VeRA, and Conv-LORA) used for parameter-efficient fine-tuning of an image restoration model.  It shows the impact of varying the rank (a hyperparameter controlling the number of parameters updated) of these adapters on image restoration quality, measured by PSNR and SSIM metrics, across various image degradation types (Rain100L, SOTS, BSD68, GoPro, LoLv1). The results demonstrate that LoRA consistently outperforms the other methods, and lower-rank adapters generally achieve better performance with fewer parameters.", "section": "3. Method"}, {"content": "| PSNR/SSIM | Deraining | Dehazing | Denoising | Deblurring | Low-Light | Average | Adapter Param. |\n|---|---|---|---|---|---|---|---|---|\n| Method | Rank | Rain100L | SOTS (Out) | BSD68<sub>\u03c3=25</sub> | GoPro | LoLv1 |  |  |\n| LoRA [19] | 4 | 37.79 | .979 | 33.48 | .984 | 31.38 | .898 | 29.00 | .878 | 24.19 | .865 | 31.17 | .921 | 3.6M |\n|  | 8 | 37.75 | .978 | 33.4 | .982 | 31.39 | .898 | 29.02 | .878 | 24.18 | .865 | 31.15 | .920 | 7.2M |\n|  | 16 | 37.61 | .972 | 33.21 | .977 | 31.31 | .896 | 28.77 | .875 | 23.96 | .862 | 30.97 | .916 | 14.3M |\n| VeRA [24] | 4 | 37.02 | .971 | 32.67 | .972 | 31.32 | .896 | 28.61 | .872 | 23.78 | .580 | 30.68 | .858 | 460K |\n|  | 8 | 37.09 | .971 | 32.69 | .972 | 31.32 | .896 | 28.64 | .873 | 23.79 | .580 | 30.71 | .858 | 468K |\n|  | 16 | 37.04 | .970 | 32.62 | .970 | 31.33 | .896 | 28.62 | .872 | 23.84 | .581 | 30.69 | .858 | 476K |\n| Conv-LoRA [67] | 4 | 37.00 | .969 | 32.55 | .971 | 31.32 | .896 | 28.54 | .870 | 23.70 | .576 | 30.62 | .856 | 3.9M |\n|  | 8 | 36.94 | .968 | 32.44 | .968 | 31.30 | .895 | 28.48 | .868 | 23.62 | .575 | 30.56 | .855 | 7.5M |", "caption": "Table 8: Ablation study on different methods for blending the five degradations task-specific LoRA\u00a0[19] adapters.", "description": "This table presents an ablation study comparing different methods for combining the predictions of five task-specific LoRA (Low-Rank Adaptation) adapters. Each adapter is trained to handle a specific type of image degradation (rain, haze, noise, blur, and low-light). The methods compared are: summing the outputs of all adapters, averaging the outputs, and using the proposed method in the paper which uses a lightweight estimator to predict the probability of each degradation type and weights the adapter outputs accordingly.  The table shows the performance of each method in terms of PSNR and SSIM on five image restoration datasets, for each of the five degradation types, and gives the average performance across all five types.", "section": "4. Experiments"}]