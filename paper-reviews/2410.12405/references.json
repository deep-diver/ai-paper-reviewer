{"references": [{" publication_date": "2021", "fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "reason": "This paper is highly relevant because it explores the capabilities of LLMs in program synthesis, a complex task highly susceptible to prompt variations. Understanding how prompt engineering affects performance in such a demanding task is crucial for the overall research on prompt sensitivity and for improving LLMs' robustness.  This makes it highly relevant to the evaluation framework proposed in the ProSA paper. The method and findings described are valuable for comparison and establishing a baseline for the performance of LLMs when confronted with different prompts.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "reason": "This is significant because it provides details about the Qwen series of large language models, which are directly used in the ProSA experiments, and which represent a set of state-of-the-art models.  Understanding the architectures and training data used in the Qwen models helps in interpreting the results of the prompt sensitivity analysis and improves the overall reliability and relevance of the ProSA research findings.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is highly influential because it demonstrates the ability of LLMs to perform well on various tasks with only a few examples, suggesting that the method of providing few-shot examples could alleviate prompt sensitivity. The findings reported in this paper are directly relevant to the discussion of few-shot learning and its impact on reducing prompt sensitivity, which is a key focus in the ProSA study.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Zheng Cai", "paper_title": "Internlm2 technical report", "reason": "This paper provides details about the InternLM series of language models, which are used in the ProSA experiments.  Understanding the architecture and training data of these LLMs is important for understanding the results and improving the robustness and generalizability of the proposed ProSA methodology.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "reason": "This paper is critical to the ProSA research because it focuses on evaluating LLMs on code generation tasks, highlighting the challenges of prompt engineering in a demanding domain.   The study includes human evaluation, providing a strong contrast and complement to the objective metrics used in the ProSA research.  The findings of the study will help to determine if the subjective evaluations show similar prompt sensitivity compared to the objective evaluations used in ProSA.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Peter Clark", "paper_title": "Think you have solved question answering? try arc, the ai2 reasoning challenge", "reason": "This paper is significant because the ARC-Challenge dataset, introduced in this paper, is used in the ProSA experiments.  The ARC-Challenge dataset consists of multiple-choice questions that assess common-sense reasoning, a task where prompt variations could significantly affect model performance.  Using the data from this paper allows ProSA to make more relevant conclusions regarding prompt sensitivity.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "reason": "This paper focuses on a specific task of solving mathematical word problems, which are also investigated in the MATH dataset used in the ProSA study. The methods and findings in this paper are valuable for comparison with the results of ProSA, particularly in the context of how different types of problems and prompt variations affect the performance of LLMs and evaluating the robustness of LLMs.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yann Dubois", "paper_title": "Length-controlled alpacaeval: A simple way to debias automatic evaluators", "reason": "This paper is very important to the ProSA research because it introduces a new metric called LC AlpacaEval 2.0 for evaluating LLMs that specifically controls the length of generated responses to mitigate biases present in the previous generation of the metric called AlpacaEval. Using the data from this paper allows for a better comparative analysis of model responses.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the MATH dataset", "reason": "This paper introduces the MATH dataset, which is used in the ProSA experiments. The MATH dataset contains a wide variety of math problems, and these problems offer a good testing ground for evaluating the effects of different prompts on LLMs' problem-solving capabilities. The results of ProSA are directly applicable to the dataset, offering a valuable means of comparing and contrasting performance. Using the dataset from this paper offers an important means of confirming and contrasting results.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "This paper introduces the Mistral-7B model, which is directly used in the ProSA experiments.  Understanding the model's architecture and training data is important for interpreting the results of the prompt sensitivity analysis and improving the reliability of the proposed ProSA methodology.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Pouya Pezeshkpour", "paper_title": "Large language models sensitivity to the order of options in multiple-choice questions", "reason": "This study directly addresses the prompt sensitivity issue, particularly focusing on the impact of option order in multiple-choice questions, a key component of some of the datasets used in the ProSA evaluation.  Their analysis provides a valuable context and comparison for the findings of the ProSA research.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Melanie Sclar", "paper_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting", "reason": "This is extremely relevant to the topic of prompt sensitivity, directly investigating the impact of formatting and design choices within prompts on the accuracy of LLMs.  The insights gained here are important for understanding various aspects of prompt engineering, such as how to design and present prompts to optimize LLM performance. This study offers an important comparative analysis for the proposed ProSA framework.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Keisuke Sakaguchi", "paper_title": "Winogrande: An adversarial winograd schema challenge at scale", "reason": "This paper introduces the Winogrande dataset, which, while not directly used in ProSA, represents a similar type of multiple-choice question answering task.  Comparing the findings of ProSA with results on this dataset could reveal common trends or differences in prompt sensitivity across different common-sense reasoning datasets, helping to understand the generality of the observed phenomena.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Tal Schuster", "paper_title": "Confident adaptive language modeling", "reason": "This paper introduces the concept of decoding confidence as a measure of model certainty in its predictions.  This is directly relevant to the ProSA study, which explores the relationship between decoding confidence and prompt robustness, and which uses decoding confidence as a key factor in explaining LLM behavior. The use of this concept adds to the overall understanding of the reasons behind the sensitivity of LLMs.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Wayne Xin Zhao", "paper_title": "A survey of large language models", "reason": "This paper is a comprehensive survey of large language models, serving as a valuable background resource for the ProSA paper.  It provides a detailed overview of the current state of LLM research and offers a solid foundation for understanding the context and significance of the ProSA study, which addresses a specific challenge in evaluating and improving LLMs.  This offers a broad overview of the space and serves as an important comparison point.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Kaijie Zhu", "paper_title": "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts", "reason": "This paper directly tackles the issue of LLM robustness to various types of prompts, particularly adversarial ones. This offers a significant parallel to the research conducted in ProSA, though focusing on adversarial prompt variations. The findings of this study provides a valuable comparison and contrast to the ProSA method and findings.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Lianmin Zheng", "paper_title": "Lmsys-chat-1m: A large-scale real-world llm conversation dataset", "reason": "The introduction of the LMSYS-CHAT-1M dataset, while not directly used in the ProSA experiments, offers a comparable large-scale dataset of human-LLM conversations. Comparing results from ProSA's analysis with results on LMSYS-CHAT-1M could help to understand the generalizability of the findings and evaluate the impact of different data sources on prompt sensitivity.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Kun Zhou", "paper_title": "Don't make your llm an evaluation benchmark cheater", "reason": "This study highlights the potential biases and issues in existing LLM evaluation methodologies, directly relating to the challenges of prompt sensitivity. It underscores the importance of developing rigorous evaluation techniques, which is precisely the goal of the ProSA framework.  The findings of this paper directly support the need for more robust evaluation methods to better assess LLM performance.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Kaijie Zhu", "paper_title": "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts", "reason": "This paper is highly relevant as it also focuses on prompt sensitivity and robustness, which are the main topics of the ProSA paper.  Comparing the methods and results from PromptBench and ProSA provides a comprehensive understanding of the problem and various approaches for addressing it. This offers an excellent comparative analysis.", "section_number": 1}]}