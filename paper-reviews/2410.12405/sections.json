[{"page_end_idx": 2, "page_start_idx": 1, "section_number": 1, "section_title": "Introduction", "details": {"details": "Large language models (LLMs) have shown remarkable progress in various tasks, but their performance heavily relies on the input prompts.  This prompt sensitivity creates challenges for both accurate evaluation and consistent user experience. Current research often overlooks the impact of individual prompt variations, focusing instead on broader dataset-level analyses.  This introductory section highlights the significance of this overlooked instance-level prompt variation and sets the stage for the introduction of ProSA, a framework designed to address these limitations by focusing on a novel sensitivity metric and exploring the underlying mechanisms contributing to prompt sensitivity.", "first_cons": "The introduction lacks specific examples of prompt variations that significantly affect LLM performance.  While it mentions the problem, illustrating concrete instances would strengthen the argument and make the issue more relatable to readers.", "first_pros": "The introduction effectively establishes the context and motivation for the research by clearly highlighting the limitations of existing approaches to evaluating LLMs. The problem of prompt sensitivity is well-defined and the need for a new evaluation framework is convincingly presented.", "keypoints": ["LLM performance is highly sensitive to prompt variations.", "Current research frequently ignores instance-level prompt variations.", "ProSA is introduced as a framework to evaluate and understand prompt sensitivity in LLMs.", "The study will incorporate a novel sensitivity metric and investigate underlying mechanisms."], "second_cons": "The introduction could benefit from a more precise definition of \"prompt sensitivity.\" The current description is somewhat general, and a more concrete definition would help the reader better understand the scope of the problem.", "second_pros": "The introduction is concise and well-written, effectively conveying the key issues and setting the stage for the subsequent sections of the paper. The language is clear and the flow of ideas is logical.", "summary": "The introduction to the ProSA paper highlights the critical issue of prompt sensitivity in large language models (LLMs).  It emphasizes that current research methodologies often neglect instance-level variations in prompts, leading to inaccurate assessments and inconsistent user experiences.  The paper introduces ProSA as a novel framework for evaluating and understanding prompt sensitivity in LLMs, incorporating a new sensitivity metric and analyzing underlying mechanisms to improve LLM robustness and user experience."}}, {"page_end_idx": 3, "page_start_idx": 2, "section_number": 2, "section_title": "Instance Level Prompt Sensitivity", "details": {"details": "This section delves into the concept of instance-level prompt sensitivity in LLMs.  It argues that existing research often overlooks the variations in responses from LLMs based on subtle differences in prompts for the *same* task or instance.  Instead of focusing on aggregate metrics across datasets, ProSA focuses on analyzing responses to different prompt variations within individual instances.  This focus allows for a more granular understanding of the model's robustness to different phrasings of the same instruction.  The section introduces PromptSensiScore (PSS) as a novel metric to quantify this instance-level sensitivity. PSS measures the average discrepancy in LLM responses to semantically similar prompt variations. It's calculated using the absolute difference in performance metrics (correctness or quality scores) between the various prompts for the same instance, averaged across multiple instances. The section highlights the need for instance-level analysis because aggregate dataset-level analysis can mask significant differences in the model's behavior at the instance level. The authors' findings from objective evaluations across 8 LLMs and 4 datasets suggest that prompt sensitivity varies across models and datasets, with larger models demonstrating better robustness. The use of few-shot examples helps mitigate prompt sensitivity. Moreover, higher model confidence correlates with increased prompt robustness, suggesting that decoding confidence serves as a key mechanism in understanding prompt sensitivity.", "first_cons": "The explanation of the PromptSensiScore (PSS) calculation could be more intuitive and easier to understand for readers with less mathematical background.", "first_pros": "The introduction of instance-level analysis of prompt sensitivity provides a more nuanced perspective on LLM robustness than previous dataset-level analyses.", "keypoints": ["Focuses on instance-level prompt sensitivity, rather than dataset-level, revealing nuances missed by aggregate metrics.", "Introduces PromptSensiScore (PSS) as a novel metric to quantify instance-level prompt sensitivity.", "Findings suggest prompt sensitivity varies across models and datasets, with larger models demonstrating better robustness.", "Incorporating few-shot examples alleviates prompt sensitivity; larger LLMs benefit more from this.", "Higher model confidence correlates with increased prompt robustness, indicating decoding confidence as a key mechanism in understanding prompt sensitivity.", "The study uses objective evaluations across 8 LLMs and 4 datasets, enhancing the reliability of its findings and providing a comprehensive analysis of prompt sensitivity across different contexts."], "second_cons": "The study primarily relies on objective evaluations, potentially neglecting the subjective aspects of prompt sensitivity that are important to real-world users.", "second_pros": "The paper presents a novel metric, PSS, to quantify prompt sensitivity and provides comprehensive experimental results using several different LLMs and datasets, demonstrating the robustness and reliability of their method.  It's valuable for both researchers and practitioners looking to better understand and evaluate the capabilities of different LLMs.", "summary": "This section introduces the concept of instance-level prompt sensitivity in Large Language Models (LLMs), arguing that existing research often overlooks the variability of LLM responses to subtle prompt changes within the same task instance. It proposes PromptSensiScore (PSS), a new metric to quantify this sensitivity, and presents experimental results showing that it varies across LLMs and datasets, with larger models generally showing higher robustness.  The use of few-shot examples is shown to improve robustness, and there is a correlation between high confidence and high robustness, implying that decoding confidence is a key to understanding prompt sensitivity. This section emphasizes that focusing on instance-level analysis rather than dataset-level analysis provides more accurate insights into LLM behaviors and capabilities.  This allows for a more reliable and nuanced understanding of LLMs performance, which is crucial for both researchers and developers aiming to improve LLM robustness and user experience.  The use of a new metric, PSS, offers a quantifiable means for analyzing this important aspect of LLM behavior.  Furthermore, the correlation between confidence and prompt robustness suggests a potential mechanism for improving the robustness of future LLMs.  The empirical studies show a strong link between confidence and robustness, suggesting ways to mitigate the prompt sensitivity issue by improving the model's confidence.  The focus on instance-level analysis, rather than overall dataset performance, provides a more granular understanding of the influence of prompt variations on model performance.  Ultimately, this section provides an improved approach to evaluating and understanding the nuances of LLM performance and how prompt engineering can affect the results obtained."}}, {"page_end_idx": 5, "page_start_idx": 3, "section_number": 3, "section_title": "Prompt Sensitivity on the Objective Evaluation", "details": {"details": "This section evaluates the prompt sensitivity of various LLMs on objective evaluation tasks using a novel metric, PromptSensiScore (PSS).  The experiments involved 8 LLMs across 4 diverse datasets (CommonsenseQA, ARC-Challenge, MATH, HumanEval), each model tested with 12 prompt variations. The results show that prompt sensitivity varies across datasets and models; larger models demonstrate higher robustness, with Llama3-70B-Instruct exhibiting the highest robustness. Few-shot examples significantly improve prompt robustness, especially for larger models.  Analysis using decoding confidence reveals a strong correlation between model confidence and prompt robustness; higher confidence correlates with increased robustness.  The findings highlight instance-level variations in prompt sensitivity, underscoring the limitations of dataset-level analyses prevalent in previous research.", "first_cons": "The study focuses on specific LLMs and datasets, limiting the generalizability of the findings to other models or tasks.  More extensive testing would strengthen the conclusions.", "first_pros": "Introduction of the novel PromptSensiScore (PSS) metric offers a more granular assessment of prompt sensitivity, going beyond dataset-level analysis.", "keypoints": ["Prompt sensitivity varies across datasets and models, with larger models generally exhibiting greater robustness.", "Llama3-70B-Instruct demonstrates the highest robustness among the tested models.", "Few-shot examples significantly mitigate prompt sensitivity, especially for larger models.  The transition from zero-shot to one-shot often produces the most significant improvement.", "Higher model confidence strongly correlates with increased prompt robustness.", "The study highlights the importance of instance-level analysis over dataset-level analysis to fully understand prompt sensitivity. ", "Across 4 datasets and 8 LLMs, the study reveals the diverse nature of prompt sensitivity, emphasizing the need for instance-level analysis and more nuanced evaluation methods."], "second_cons": "The objective evaluation may not fully capture the nuanced aspects of LLM performance, especially for subjective tasks.  The reliance solely on objective metrics could limit the understanding of user experience.", "second_pros": "The use of multiple datasets spanning diverse tasks provides a more comprehensive evaluation of prompt sensitivity, strengthening the validity and generalizability of the results.", "summary": "This study investigates prompt sensitivity in LLMs using objective evaluation metrics across multiple datasets and models, introducing a novel metric called PromptSensiScore. The results indicate that prompt sensitivity varies significantly across models and datasets, with larger models showing higher robustness. Few-shot learning substantially improves model robustness, particularly for larger models. A strong positive correlation exists between decoding confidence and prompt robustness.  This work underscores the need for granular instance-level analysis of prompt sensitivity."}}, {"page_end_idx": 8, "page_start_idx": 5, "section_number": 4, "section_title": "Prompt Sensitivity on the Subjective Evaluation", "details": {"details": "This section explores prompt sensitivity through subjective evaluation, focusing on how well LLMs align with human expectations when presented with varied prompts for the same tasks.  Unlike objective evaluations with definitive right/wrong answers, subjective evaluations assess the quality of LLM responses, reflecting real-world user experiences. Two established benchmarks, LC AlpacaEval 2.0 and Arena Hard Auto, were used to assess five advanced LLMs. LC AlpacaEval 2.0, which controls response length and uses GPT-4 as an evaluator, yielded lower prompt sensitivity scores, suggesting model robustness on simpler tasks.  Conversely, Arena Hard Auto, a more challenging benchmark that compares responses pairwise, revealed greater prompt sensitivity, especially on complex questions. The study further investigates the relationship between prompt sensitivity and task categories in Arena Hard Auto, demonstrating higher sensitivity in tasks demanding creativity and coding compared to knowledge-based queries. The results are analyzed and discussed in detail.", "first_cons": "The subjective evaluation relies heavily on the quality assessments of the GPT-4 model; this raises concerns about potential bias and limitations of using another LLM as the sole judge of quality. It also limits the generalizability of the findings to a specific large language model.", "first_pros": "The section utilizes two widely recognized and respected subjective evaluation benchmarks (LC AlpacaEval 2.0 and Arena Hard Auto), thereby adding to the credibility and reliability of the findings.  This approach allows for a more nuanced and realistic assessment of LLM performance than objective evaluations alone.", "keypoints": ["Subjective evaluation benchmarks (LC AlpacaEval 2.0 and Arena Hard Auto) used to assess 5 advanced LLMs.", "LC AlpacaEval 2.0 showed lower prompt sensitivity (more robustness) compared to Arena Hard Auto.", "Arena Hard Auto showed higher sensitivity, particularly on complex tasks.", "Analysis of prompt sensitivity across task categories (Arena Hard Auto) highlights differences; creativity and coding tasks showed higher sensitivity.", "The study emphasizes the importance of instance-level analysis in evaluating prompt sensitivity, moving beyond dataset-level assessments."], "second_cons": "The reliance on only five advanced LLMs may limit the generalizability of the findings. A more comprehensive analysis using a wider range of models, including smaller and less powerful ones, would provide a more complete understanding of prompt sensitivity across different LLM capabilities.", "second_pros": "The study introduces an important perspective by exploring the underlying reasons for prompt sensitivity by examining the relationship between decoding confidence and prompt robustness. This insightful investigation adds a new dimension to understanding how LLMs perform under varying prompt conditions.", "summary": "This section investigates prompt sensitivity through subjective evaluations using two established benchmarks, LC AlpacaEval 2.0 and Arena Hard Auto.  Five advanced LLMs were assessed, showing lower prompt sensitivity on LC AlpacaEval 2.0 (simpler tasks) and higher sensitivity on Arena Hard Auto (complex tasks).  An analysis of task categories within Arena Hard Auto highlights increased sensitivity for creative/coding tasks.  The study also links decoding confidence with prompt robustness."}}, {"page_end_idx": 9, "page_start_idx": 8, "section_number": 5, "section_title": "Why LLMs are Sensitive to Prompts", "details": {"details": "This section investigates the reasons behind LLMs' sensitivity to prompts.  The authors challenge the prevailing focus on dataset-level analysis and instead emphasize instance-level analysis. They introduce the concept of decoding confidence, arguing that a model's confidence in its answer is directly related to its robustness against prompt variations. Experiments on CommonsenseQA with three different LLMs (Mistral-7B-Instruct, InternLM2-7B-Chat, and InternLM2-20B-Chat) showed a strong correlation between high decoding confidence and low prompt sensitivity scores (PSS).  When a model is confident in its answer, it demonstrates greater resilience to prompt changes. This suggests that prompt sensitivity is largely a reflection of the model's internal confidence, rather than an inherent limitation of LLMs themselves. The work implies that improving model confidence could be a key strategy for addressing prompt sensitivity.", "first_cons": "The study's focus on only three LLMs and one dataset (CommonsenseQA) limits the generalizability of the findings. More comprehensive evaluation across diverse datasets and a broader range of models is needed to confirm these observations.", "first_pros": "The study shifts the focus from dataset-level to instance-level analysis, providing a more nuanced and insightful perspective on prompt sensitivity. This granular approach reveals important details that aggregate metrics might obscure.", "keypoints": ["Instance-level analysis is crucial for understanding prompt sensitivity, as opposed to relying solely on dataset-level observations.", "Decoding confidence is strongly correlated with prompt robustness.  High confidence correlates with low prompt sensitivity scores (PSS).", "Experiments on CommonsenseQA with three LLMs demonstrated this correlation between confidence and robustness."], "second_cons": "While the correlation between confidence and robustness is observed, the underlying causal mechanisms are not fully explored. The research does not delve into the internal workings of the models to explain this correlation.", "second_pros": "The introduction of decoding confidence as a metric for understanding prompt sensitivity provides a novel and potentially valuable tool for researchers and developers working to improve LLM robustness.", "summary": "This section explores why LLMs are sensitive to prompts, arguing that prompt sensitivity is strongly linked to the model's confidence in its response. Instance-level analysis reveals a direct correlation between high decoding confidence and low prompt sensitivity scores (PSS), suggesting that improving model confidence is a key to enhancing prompt robustness. Experiments using three LLMs on the CommonsenseQA dataset support this hypothesis."}}]