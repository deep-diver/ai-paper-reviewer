[{"figure_path": "2410.12405/charts/charts_3_0.png", "caption": "Figure 2: A Comparision of Evaluating LLMs' Prompt Sensitivity. \u2713 and X indicate the accuracy of the LLM's responses. In this example, LLMs appear robust at the dataset level evaluation (calculated from the variance of different templates), but this overlooks the sensitivity of LLMs to different templates within the same instance.", "description": "The chart compares dataset-level and instance-level evaluations of LLMs' prompt sensitivity, highlighting that dataset-level analysis overlooks the model's sensitivity to different prompts within the same instance.", "section": "2 Instance Level Prompt Sensitivity"}, {"figure_path": "2410.12405/charts/charts_4_0.png", "caption": "Figure 3: Main Results of Prompt Sensitivity. The scatter represents the average performance score of 12 prompts and the PSS under different datasets.", "description": "The chart displays the average performance and prompt sensitivity score (PSS) across four datasets for several LLMs, revealing variations in prompt sensitivity among models and datasets.", "section": "3 Prompt Sensitivity on the Objective Evaluation"}, {"figure_path": "2410.12405/charts/charts_5_0.png", "caption": "Figure 4: Prompt Sensitivity vs. Model Size. The comparative charts display the relationship between the size of the model's parameters and prompts sensitivity. PSS refers to the average PSS of four datasets.", "description": "The chart displays the relationship between the size of a language model (in billions of parameters) and its prompt sensitivity, as measured by the average PromptSensiScore (PSS) across four datasets.", "section": "3.3 Prompt Sensitivity and Model Size"}, {"figure_path": "2410.12405/charts/charts_6_0.png", "caption": "Figure 5: Impact of Few-shot on the Performance and Sensitivity. Conduct experiments on the CommonsenseQA and ARC-Challenge datasets using five few-shot settings and four models from the Qwen series. The blue line represents the changes in the scores of LLMs (using the left scale). The orange line represents the changes in the PSS of LLMs (using the right scale).", "description": "The chart shows the impact of few-shot examples on the performance and prompt sensitivity of four different sized language models on two datasets, CommonsenseQA and ARC-Challenge.", "section": "3.4 Few-shot Enhances Prompt Robustness"}, {"figure_path": "2410.12405/charts/charts_8_0.png", "caption": "Figure 6: Prompt Sensitivity of Different Categories on Arena Hard Auto. We separately presented the five most sensitive and the five least sensitive categories on Arena Hard Auto. The PSS for a particular category refers to the average of the PSS of five LLMs in that category.", "description": "The bar chart visualizes the average prompt sensitivity scores (PSS) across different task categories, revealing variations in LLM robustness across various tasks.", "section": "4.3 Prompt Sensitivity and Categories"}, {"figure_path": "2410.12405/charts/charts_8_1.png", "caption": "Figure 7: The Relationship between Model Confidence and Prompt Sensitivity on CommonsenseQA. Each bar represents the model\u2019s average confidence when its PPS falls within that interval. Note that due to variations in model and vocabulary size, cross-model confidence comparisons are not meaningful.", "description": "The chart displays the relationship between model confidence and prompt sensitivity across three different LLMs, categorized by their prompt sensitivity scores.", "section": "5.2 Experiments and Analysis"}]