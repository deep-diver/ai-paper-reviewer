[{"figure_path": "2410.12405/tables/table_7_0.html", "caption": "Table 1: Verifications for Rewritten Prompts. Here, BS stands for BERTScore, and HS stands for Human-labeled Similarity.", "description": "Table 1 presents the results of two quality verifications on the rewritten prompts using BERTScore and human evaluation, demonstrating high semantic similarity between original and rewritten prompts.", "section": "4.1.3 Prompt Rewriting"}, {"figure_path": "2410.12405/tables/table_7_1.html", "caption": "Table 2: PSS on LC AlpacaEval 2.0 and Arena Hard Auto. Reference refers to the average quality difference of responses generated by Llama3-8b-Instruct and Llama3-70b-Instruct. The others represent the PSS of LLMs under the three prompt versions (One original and two generated). Due to the different default comparison models, the PSS of LC AlpacaEval 2.0 and Arena Hard cannot be directly compared.", "description": "Table 2 presents the Prompt Sensitivity Score (PSS) for several LLMs across two subjective evaluation benchmarks, showing their robustness to prompt variations.", "section": "4.2 Main Results and Analysis"}, {"figure_path": "2410.12405/tables/table_11_0.html", "caption": "Table 3: Examples of Model Responses and PSS. This table provides three examples of what the PSS values are for given responses.", "description": "Table 3 shows examples of model responses to different prompts for three instances, illustrating the variation in correctness and the calculation of the PromptSensiScore (PSS).", "section": "A Examples of Model Responses and PSS"}, {"figure_path": "2410.12405/tables/table_11_1.html", "caption": "Table 4: Results about Several Models on Humaneval.", "description": "The table presents the prompt sensitivity (PSS) and average accuracy of several LLMs on the HumanEval benchmark.", "section": "4.1.2 LLMs Selection"}]