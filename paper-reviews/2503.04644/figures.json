[{"figure_path": "https://arxiv.org/html/2503.04644/x3.png", "caption": "Figure 1: \n(Top): An illustration of instruction-following IR scenarios explored in this study. The example simulates a legal case search, where the user provides detailed instructions to retrieve relevant legal cases. Current IR systems struggle to handle such complex queries. (Bottom left): As a result, users have to break down their information needs into simpler, iterative search queries and manually filter the retrieved cases, resulting in a time-consuming and inefficient process.\n(Bottom right): This study focuses on evaluating the progress and limitations of current end-to-end retrieval systems in expert-domain instruction-following IR.", "description": "This figure illustrates instruction-following in information retrieval (IR). The top panel shows an example of a complex query for legal cases, highlighting the challenge for current IR systems. The bottom-left panel demonstrates how users typically work around this limitation by simplifying their queries and manually filtering results, a time-consuming process.  The bottom-right panel emphasizes the study's focus: evaluating the capabilities and limitations of modern end-to-end retrieval systems to handle complex instruction-following tasks in specialized domains.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.04644/x4.png", "caption": "Figure 2: Dataset Construction Pipeline: We derive a specific task according to the dataset, which then guides the generation of instructions based on the original query and task conditions. An LLM is used to assess whether the corpora are relevant to these instructions. As illustrated in the figure, different colors in the \u201cTask\u201d section correspond to the conditions outlined in the \u201cInstruction\u201d section.", "description": "This figure illustrates the pipeline used to create the IFIR benchmark dataset.  First, a domain-specific task is selected based on the source dataset.  Then, instructions for that task are generated, mirroring real-world user queries. These instructions vary in complexity. An LLM (Large Language Model) then assesses the relevance of existing corpora to these newly generated instructions. The color-coding in the \"Task\" section visually links instruction criteria to the relevant passages in the corpus. This process ensures that the dataset contains relevant passages that accurately reflect the complexity and nuance of real-world instruction-following scenarios in expert domains.", "section": "3 IFIR Benchmark"}, {"figure_path": "https://arxiv.org/html/2503.04644/x5.png", "caption": "Figure 3: The nDCG improvement when the model is provided with detailed instructions for retrieval.", "description": "This figure displays the improvement in nDCG scores achieved by incorporating detailed instructions into retrieval queries.  It compares the performance of different models with and without the use of these detailed instructions, providing insights into the effectiveness of instruction-following in retrieval tasks.  The chart likely shows the percentage change in nDCG for each model when using detailed instructions, highlighting the impact of instruction detail on retrieval performance.  Higher bars indicate a greater performance boost from using more detailed instructions.", "section": "5.1 Main Results"}, {"figure_path": "https://arxiv.org/html/2503.04644/x6.png", "caption": "Figure 4: Average nDCG@20 performance on different levels of instructions in different domains.", "description": "This figure displays the average normalized discounted cumulative gain (nDCG@20) scores achieved by various information retrieval models across three different levels of instruction complexity within four specialized domains: finance, scientific literature, law, and healthcare.  Each bar represents the average nDCG@20 score for a given domain and instruction level.  The x-axis represents the domain and the three instruction levels within each domain (Level 1, Level 2, and Level 3, with increasing complexity).  The y-axis shows the nDCG@20 score, indicating the effectiveness of the models in retrieving relevant information according to the instructions provided. This visualization allows for a comparison of model performance across domains and instruction complexity levels, revealing insights into the models' capabilities to handle instruction-following information retrieval tasks of varying difficulty.", "section": "5.1 Main Results"}]