{"importance": "This paper is crucial for researchers in Arabic NLP because it **introduces Swan, a family of dialect-aware Arabic embedding models**, and **ArabicMTEB, a comprehensive benchmark** for evaluating Arabic text embeddings across diverse tasks.  This work **addresses the scarcity of high-quality Arabic resources** and provides valuable tools and datasets for advancing research in this important area. Its findings on the effectiveness of dialect-aware models and the establishment of a robust benchmark will significantly impact future research. The public availability of the models and benchmark further enhances its significance for the research community.", "summary": "Swan & ArabicMTEB: New dialect-aware Arabic embedding models and benchmark achieve state-of-the-art performance, addressing limitations of existing multilingual models.", "takeaways": ["Swan models (Swan-Small and Swan-Large) achieve state-of-the-art results on Arabic tasks, outperforming existing multilingual models.", "ArabicMTEB, a new benchmark suite, offers a comprehensive evaluation of Arabic text embeddings across diverse tasks and datasets.", "Swan models demonstrate dialectal and cultural awareness, excelling across various Arabic domains while being more cost-effective."], "tldr": "Current multilingual embedding models often underperform on Arabic NLP tasks due to the language's unique morphology, diverse dialects, and cultural nuances.  Existing benchmarks also lack sufficient coverage of these aspects. This necessitates the development of Arabic-specific embedding models and a comprehensive evaluation framework. \n\nThis paper introduces Swan, a family of Arabic-centric embedding models, focusing on both small and large scale applications. It also proposes ArabicMTEB, a benchmark that evaluates cross-lingual, multi-dialectal, and multi-cultural performance on eight diverse tasks. Swan-Large achieves state-of-the-art results, while Swan-Small surpasses Multilingual-E5-base. The research demonstrates that Swan models are dialectally and culturally aware and provide valuable resources for future NLP research.", "affiliation": "University of British Columbia", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}}