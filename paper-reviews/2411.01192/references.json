{"references": [{"fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-11", "reason": "This paper introduced BERT, a foundational model for many subsequent advancements in NLP, including the models presented in this paper."}, {"fullname_first_author": "Mikel Artetxe", "paper_title": "Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond", "publication_date": "2019-07-01", "reason": "This paper introduced LASER, one of the first successful massively multilingual sentence embedding models, highly relevant to the cross-lingual aspects of this work."}, {"fullname_first_author": "Nils Reimers", "paper_title": "Sentence-BERT: Sentence embeddings using siamese BERT-networks", "publication_date": "2019-08-01", "reason": "Sentence-BERT is a highly influential method for generating sentence embeddings used in many downstream tasks.  This is directly relevant as sentence embeddings are the core of this paper's approach."}, {"fullname_first_author": "Liang Wang", "paper_title": "Multilingual E5 text embeddings: A technical report", "publication_date": "2024-02-01", "reason": "This paper introduced Multilingual-E5, a strong baseline for multilingual text embeddings which this paper directly compares against and improves upon for Arabic."}, {"fullname_first_author": "AbdelRahim Elmadany", "paper_title": "ORCA: A challenging benchmark for Arabic language understanding", "publication_date": "2023-01-01", "reason": "This paper introduced ORCA, a benchmark for Arabic NLP, which served as a foundation for the ArabicMTEB benchmark presented in this paper."}]}