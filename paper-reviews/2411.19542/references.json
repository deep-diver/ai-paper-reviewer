{"references": [{"fullname_first_author": "Amey Agrawal", "paper_title": "Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills", "publication_date": "2023-XX-XX", "reason": "This paper is highly relevant because it directly addresses LLM inference optimization, a central theme of the target paper, proposing a method to improve efficiency by combining decode and prefill phases."}, {"fullname_first_author": "Tim Dettmers", "paper_title": "QLoRA: Efficient Finetuning of Quantized LLMs", "publication_date": "2023-XX-XX", "reason": "This paper focuses on efficient fine-tuning of quantized LLMs which is directly relevant to the work in the target paper, as it explores techniques for optimizing LLM performance through quantization."}, {"fullname_first_author": "Elias Frantar", "paper_title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers", "publication_date": "2023-XX-XX", "reason": "This paper presents GPTQ, a method for accurate post-training quantization of LLMs, a technique directly relevant to the target paper's exploration of efficient LLM inference through quantization."}, {"fullname_first_author": "Georgi Gerganov", "paper_title": "llama.cpp", "publication_date": "2023-XX-XX", "reason": "This is a foundational framework for LLM inference on CPUs that the target paper builds upon; optimizing its performance on hybrid CPUs is a key objective of the target paper."}, {"fullname_first_author": "Pujiang He", "paper_title": "Inference Performance Optimization for Large Language Models on CPUs", "publication_date": "2024-XX-XX", "reason": "This paper shares the same goal of optimizing LLM inference performance on CPUs, making it a relevant and comparable work for evaluating the significance of the proposed method in the target paper."}]}