[{"figure_path": "https://arxiv.org/html/2411.19542/extracted/6031307/dynamic_process.png", "caption": "Figure 1: The dynamic LLM inference process", "description": "This figure illustrates the dynamic LLM inference process using a hybrid CPU.  The process begins with the CPU runtime recording and managing CPU status, including execution times for each thread. A thread scheduler then uses this information to dynamically divide kernel tasks into subtasks based on each core's performance.  The performance of each core is constantly monitored and updated, allowing for optimal task assignment to maximize inference performance. The dynamic adjustment of workloads ensures efficient utilization of the hybrid CPU's resources, overcoming limitations imposed by traditional static task allocation methods.", "section": "2 Approach"}, {"figure_path": "https://arxiv.org/html/2411.19542/extracted/6031307/TOPs.png", "caption": "Figure 2: The latency and bandwidth of GEMM in different parallel methods", "description": "This figure compares the latency and memory bandwidth of INT8 GEMM (General Matrix Multiply) and INT4 GEMV (General Matrix-Vector Multiply) operations across three different parallel methods: the authors' dynamic parallel method, OpenMP, and MLC (Intel Memory Latency Checker).  The comparison is performed on two different Intel hybrid CPUs: Core i9-12900K and Intel Ultra-125H.  The results highlight the performance improvements achieved by the authors' dynamic parallel method in terms of reduced latency and increased memory bandwidth utilization.", "section": "3 Results"}, {"figure_path": "https://arxiv.org/html/2411.19542/extracted/6031307/bandwidth.png", "caption": "Figure 3: The latency of the prefill phase and the decode phase in Neural Speed (OpenMP and our method) and l\u2062l\u2062a\u2062m\u2062a.c\u2062p\u2062pformulae-sequence\ud835\udc59\ud835\udc59\ud835\udc4e\ud835\udc5a\ud835\udc4e\ud835\udc50\ud835\udc5d\ud835\udc5dllama.cppitalic_l italic_l italic_a italic_m italic_a . italic_c italic_p italic_p", "description": "Figure 3 presents a comparison of the latency (time taken) for the prefill and decode phases of Large Language Model (LLM) inference across three different methods: Neural Speed using OpenMP (a standard parallel programming model), Neural Speed enhanced with the authors' new dynamic parallel method, and llama.cpp.  The prefill phase processes the input prompt, while the decode phase generates the output tokens.  The figure shows that the dynamic parallel method significantly reduces latency in both phases compared to the other two methods, particularly in the prefill phase where it exhibits a much more substantial improvement. This highlights the efficiency gains achieved by the proposed dynamic workload balancing technique.", "section": "3 Results"}, {"figure_path": "https://arxiv.org/html/2411.19542/extracted/6031307/first_token.png", "caption": "Figure 4: The performance ratio of one P-core in the prefill phase and the decode phase", "description": "This figure illustrates the dynamic changes in the performance ratio of a single P-core (performance core) within a hybrid CPU during the two main phases of LLM inference: the prefill phase and the decode phase.  The performance ratio reflects the relative speed at which this specific core can complete its tasks. The graph shows that the ratio fluctuates, especially during the transition between phases, indicating a change in computational bottlenecks.  The initial high ratio settles to a more stable range, and then a second change occurs at the boundary between the prefill and decode phases due to different computational demands of each phase.  These variations highlight the need for dynamic workload balancing in hybrid CPUs and suggest the effectiveness of the proposed dynamic parallel approach in adapting to fluctuating performance.", "section": "3.2 Experiment Result"}]