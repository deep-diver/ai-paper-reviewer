[{"figure_path": "2410.21220/tables/table_6_0.html", "caption": "Table 1. Closed-Set Evaluation on the LLaVA-W benchmark. We use GPT-40 (0806) for evaluation. Naive search here denotes the VLM with Google image search.", "description": "Table 1 presents a closed-set evaluation of the Vision Search Assistant and baseline models on the LLaVA-W benchmark, showing improvements in conversation, detail, reasoning, and overall performance.", "section": "4.2. Closed-Set Evaluation"}, {"figure_path": "2410.21220/tables/table_6_1.html", "caption": "Table 1. Closed-Set Evaluation on the LLaVA-W benchmark. We use GPT-40 (0806) for evaluation. Naive search here denotes the VLM with Google image search.", "description": "Table 1 presents a closed-set evaluation of the Vision Search Assistant and baseline models on the LLaVA-W benchmark, showing significant performance gains in conversation, detail, reasoning, and overall accuracy.", "section": "4.2. Closed-Set Evaluation"}, {"figure_path": "2410.21220/tables/table_7_0.html", "caption": "Table 1. Closed-Set Evaluation on the LLaVA-W benchmark. We use GPT-40 (0806) for evaluation. Naive search here denotes the VLM with Google image search.", "description": "Table 1 presents a closed-set evaluation of the LLaVA-W benchmark, comparing the performance of different models, including a baseline, naive search, an improved model using the proposed method, and the Vision Search Assistant.", "section": "4.2. Closed-Set Evaluation"}]