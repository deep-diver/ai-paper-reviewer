[{"figure_path": "2410.21220/figures/figures_2_1.png", "caption": "For Novel Images & Events: VLMs show very limited generalization ability. Figure 1. Vision Search Assistant acquires unknown visual knowledge through web search. An intuitive comparison of answering the user's question with an unseen image. The proposed Vision Search Assistant is developed based on LLaVA-1.6-7B, and its ability to answer the question on unseen images outperforms the state-of-the-art models including LLava-1.6-34B [29], Qwen2-VL-72B [5], and InternVL2-76B [11].", "description": "Figure 1 shows a comparison of four different models' abilities to answer a question about a game image that none of the models were trained on, highlighting the superior performance of the proposed Vision Search Assistant.", "section": "1. Introduction"}, {"figure_path": "2410.21220/figures/figures_2_3.png", "caption": "Figure 6. Comparisons among Qwen2-VL-72B, InternVL2-76B, and Vision Search Assistant. We compare the open-set QA results on both novel events (the first two rows) and images (the last two rows). Vision Search Assistant excels in generating accurate and detailed results.", "description": "The figure compares the performance of Vision Search Assistant against Qwen2-VL-72B and InternVL2-76B on open-set question answering tasks using both novel images and events.", "section": "4. Experiments"}, {"figure_path": "2410.21220/figures/figures_2_4.png", "caption": "Figure 2. Comparsion with Closed-Source Models including GPT-40 [34], Gemini [37], Claude 3.5 Sonnet [3] with Vision Search Assistant shows that Vision Search Assistant satisfies users' needs better even if the image is novel.", "description": "The figure shows a comparison of Vision Search Assistant with other closed-source models on novel image questions, demonstrating Vision Search Assistant's superior performance.", "section": "Experiments"}, {"figure_path": "2410.21220/figures/figures_2_5.png", "caption": "For Novel Images & Events: VLMs show very limited generalization ability. Figure 1. Vision Search Assistant acquires unknown visual knowledge through web search. An intuitive comparison of answering the user's question with an unseen image. The proposed Vision Search Assistant is developed based on LLaVA-1.6-7B, and its ability to answer the question on unseen images outperforms the state-of-the-art models including LLava-1.6-34B [29], Qwen2-VL-72B [5], and InternVL2-76B [11].", "description": "Figure 1 shows a comparison of four different models' responses to a user query about an unseen image, highlighting the superior performance of the proposed Vision Search Assistant.", "section": "1. Introduction"}, {"figure_path": "2410.21220/figures/figures_3_1.png", "caption": "Figure 2. Comparsion with Closed-Source Models including GPT-40 [34], Gemini [37], Claude 3.5 Sonnet [3] with Vision Search Assistant shows that Vision Search Assistant satisfies users' needs better even if the image is novel.", "description": "The figure compares the responses of Vision Search Assistant with those of GPT-40, Gemini, and Claude 3.5 Sonnet on a novel image, demonstrating Vision Search Assistant's superior performance.", "section": "Experiments"}, {"figure_path": "2410.21220/figures/figures_4_0.png", "caption": "Figure 3. Overview of Vision Search Assistant. We first identify the critical objects and generate their descriptions considering their correlations, named Correlated Formulation, using the Vision Language Model (VLM). We then use the LLM to generate sub-questions that leads to the final answer, which is referred to as the Planning Agent. The web pages returned from the search engine are analyzed, selected, and summarized by the same LLM, which is referred to as the Searching Agent. We use the original image, the user\u2019s prompt, the Correlated Formulation together with the obtained web knowledge to generate the final answer. Vision Search Assistant produces reliable answers, even for novel images, by leveraging the collaboration between VLM and web agents to gather visual information from the web effectively.", "description": "The figure illustrates the Vision Search Assistant framework, detailing the three main steps: visual content formulation, web knowledge search, and collaborative generation, showing how it leverages VLMs and web agents for accurate answers to visual questions.", "section": "3. Vision Search Assistant"}, {"figure_path": "2410.21220/figures/figures_8_0.png", "caption": "Figure 10. A series of demos of Vision Search Assistant on novel images, events, and in-the-wild scenarios. Vision Search Assistant delivers promising potential as a powerful multimodal engine.", "description": "Figure 10 shows various examples of Vision Search Assistant successfully handling diverse inputs, demonstrating its capabilities in different scenarios.", "section": "Experiments"}, {"figure_path": "2410.21220/figures/figures_11_0.png", "caption": "Figure 1. We model a generative image-space prior on scene motion. From a single RGB image, our method generates a spectral volume [23] \u2013 a motion representation that models dense, long-term pixel trajectories in the frequency domain. Our learned motion priors can be used to animate pictures realistically. We visualize carpet videos as interactive simulation dynamics that respond to user inputs like dragging individual points. On the right, we see looping carpet videos in space-time. At t = 0, we align the input scanline shown on the left.", "description": "Figure 1 illustrates the generative image-space prior on scene motion, showing how a single RGB image is used to generate a spectral volume representing long-term pixel trajectories, enabling realistic animation of pictures and interactive simulations.", "section": "3. Vision Search Assistant"}, {"figure_path": "2410.21220/figures/figures_11_1.png", "caption": "Figure 3. Overview of Vision Search Assistant. We first identify the critical objects and generate their descriptions considering their correlations, named Correlated Formulation, using the Vision Language Model (VLM). We then use the LLM to generate sub-questions that leads to the final answer, which is referred to as the Planning Agent. The web pages returned from the search engine are analyzed, selected, and summarized by the same LLM, which is referred to as the Searching Agent. We use the original image, the user's prompt, the Correlated Formulation together with the obtained web knowledge to generate the final answer. Vision Search Assistant produces reliable answers, even for novel images, by leveraging the collaboration between VLM and web agents to gather visual information from the web effectively.", "description": "The figure illustrates the workflow of Vision Search Assistant, which leverages the collaboration between Vision Language Models and web agents to answer questions about images, even novel ones, by gathering visual information from the web.", "section": "3. Vision Search Assistant"}, {"figure_path": "2410.21220/figures/figures_11_3.png", "caption": "Figure 10. A series of demos of Vision Search Assistant on novel images, events, and in-the-wild scenarios. Vision Search Assistant delivers promising potential as a powerful multimodal engine.", "description": "Figure 10 shows examples of Vision Search Assistant handling various scenarios including novel images, events, and in-the-wild situations, demonstrating its potential as a powerful multimodal engine.", "section": "Experiments"}, {"figure_path": "2410.21220/figures/figures_11_4.png", "caption": "Figure 7. Ablation Study on What to Search. We use the object description to avoid the visual redundancy of the image.", "description": "The figure shows an ablation study comparing the use of image-based captions versus object-level descriptions for visual search tasks, highlighting the improved precision of the latter approach.", "section": "4.3. Ablation Study"}, {"figure_path": "2410.21220/figures/figures_11_5.png", "caption": "Figure 10. A series of demos of Vision Search Assistant on novel images, events, and in-the-wild scenarios. Vision Search Assistant delivers promising potential as a powerful multimodal engine.", "description": "Figure 10 shows several example uses of the Vision Search Assistant on various inputs, demonstrating its ability to answer questions about images, events, and general knowledge.", "section": "Experiments"}]