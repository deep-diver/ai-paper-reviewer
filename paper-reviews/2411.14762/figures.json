[{"figure_path": "https://arxiv.org/html/2411.14762/x1.png", "caption": "(a) Maximum batch-size when training video tokenizers on 128\u00d7128128128128\\times 128128 \u00d7 128 resolution videos with varying lengths, measured with a single NVIDIA 4090 24GB GPU.", "description": "The figure shows a graph illustrating the maximum batch size achievable when training different video tokenizers on videos of varying lengths.  The training was performed on videos with a resolution of 128x128 pixels using a single NVIDIA 4090 24GB GPU.  The x-axis represents the video length (number of frames), and the y-axis represents the maximum batch size that could be processed without running out of GPU memory.  Different video tokenizers (TATS-AE, LARP, PVDM-AE, and CoordTok) are compared, demonstrating the impact of video length on training capacity.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.14762/x2.png", "caption": "(b) Inter-clip reconstruction consistency of video tokenizers. Existing video tokenizers [9, 64, 50] show the pixel-value inconsistency between short clips (16 frames). In contrast, Our tokenizer shows the temporally consistent reconstruction.", "description": "The figure (1b) demonstrates the temporal consistency of video reconstruction between short clips using CoordTok, in contrast to existing methods (PVDM, LARP) that exhibit pixel-value inconsistencies.  This highlights CoordTok's ability to leverage temporal coherence in videos, even when trained on longer sequences. The visualization shows reconstructed frames for a short video clip, comparing the reconstruction quality of CoordTok against other methods. Each method's reconstruction is shown for four representative frames from the video sequence, illustrating a smoother and more consistent reconstruction from CoordTok in comparison to the other tokenizers.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.14762/x3.png", "caption": "Figure 1: Limitation of existing video tokenizers.\n(a) Existing video tokenizers [9, 64, 50] are often not scalable to long videos because of excessive memory and computational demands.\nThis is because they are trained to reconstruct all video frames at once, i.e., a giant 3D array of pixels, which incurs a huge computation and memory burden in training especially when trained on long videos.\nFor instance, PVDM-AE [64] becomes out-of-memory when trained to encode 128-frame videos when using a single NVIDIA 4090 24GB GPU.\n(b) As a result, existing tokenizers are typically trained to encode up to 16-frame videos and struggle to capture the temporal coherence of videos.", "description": "Figure 1 illustrates the limitations of existing video tokenizers in handling long videos.  Part (a) shows that training these models on long videos is computationally expensive and memory-intensive because they reconstruct all frames simultaneously. This is exemplified by PVDM-AE, which runs out of memory (on a single NVIDIA 4090 24GB GPU) when training on 128-frame videos.  Part (b) demonstrates that this limitation restricts the training to shorter videos (up to 16 frames), hindering their ability to leverage the temporal coherence present in longer videos.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.14762/x4.png", "caption": "Figure 2: Overview of CoordTok. We design our encoder to encode a video \ud835\udc31\ud835\udc31{\\mathbf{x}}bold_x into factorized triplane representations \ud835\udc33=[\ud835\udc33x\u2062y,\ud835\udc33y\u2062t,\ud835\udc33x\u2062t]\ud835\udc33superscript\ud835\udc33\ud835\udc65\ud835\udc66superscript\ud835\udc33\ud835\udc66\ud835\udc61superscript\ud835\udc33\ud835\udc65\ud835\udc61{\\mathbf{z}}=[{\\mathbf{z}}^{xy},{\\mathbf{z}}^{yt},{\\mathbf{z}}^{xt}]bold_z = [ bold_z start_POSTSUPERSCRIPT italic_x italic_y end_POSTSUPERSCRIPT , bold_z start_POSTSUPERSCRIPT italic_y italic_t end_POSTSUPERSCRIPT , bold_z start_POSTSUPERSCRIPT italic_x italic_t end_POSTSUPERSCRIPT ] which can efficiently represent the video with three 2D latent planes.\nGiven the triplane representations \ud835\udc33\ud835\udc33\\mathbf{z}bold_z, our decoder learns a mapping from (x,y,t)\ud835\udc65\ud835\udc66\ud835\udc61(x,y,t)( italic_x , italic_y , italic_t ) coordinates to RGB pixels within the corresponding patches.\nIn particular, we extract coordinate-based representations of N\ud835\udc41Nitalic_N sampled coordinates by querying the coordinates from triplane representations via bilinear interpolation.\nThen the decoder aggregates and fuses information from different coordinates with self-attention layers and project outputs into corresponding patches.\nThis design enables us to train tokenizers on long videos in a compute-efficient manner by avoiding reconstruction of entire frames at once.", "description": "CoordTok processes videos by first encoding them into a compact triplane representation using a transformer encoder. This representation uses three 2D planes (zxy, zyt, zxt) to capture spatial and temporal information efficiently.  The decoder then takes randomly sampled (x, y, t) coordinates as input and uses bilinear interpolation on the triplane representation to get feature vectors for these coordinates.  These features are then processed by self-attention layers in the transformer decoder, which aggregate information across different coordinates. Finally, the decoder reconstructs the corresponding image patches. This approach avoids reconstructing full frames at once, enabling efficient training on long videos.", "section": "2. Method"}, {"figure_path": "https://arxiv.org/html/2411.14762/x5.png", "caption": "Figure 3: 128-frame, 128\u00d7\\times\u00d7128 resolution video reconstruction results from CoordTok (Ours) and baselines [64, 50] trained on the UCF-101 dataset [40].\nFor each frame, we visualize the ground-truth (GT) and reconstructed pixels within the region highlighted in the red box, where CoordTok achieves noticeably better reconstruction quality than other baselines.", "description": "This figure compares the video reconstruction capabilities of CoordTok with two other state-of-the-art methods (PVDM-AE and LARP) on the UCF-101 dataset.  It focuses on a 128-frame, 128x128 resolution video. A close-up of a specific area is shown for each method: the ground truth and each model's reconstruction. This detailed comparison highlights CoordTok's superior reconstruction quality in comparison to the baselines.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.14762/x7.png", "caption": "Figure 4: CoordTok can efficiently encode long videos. rFVD scores of video tokenizers, evaluated on 128-frame videos, with respect to the token size. \u2193\u2193\\downarrow\u2193 indicates lower values are better.", "description": "Figure 4 illustrates the efficiency of CoordTok in encoding long videos.  It compares CoordTok to several baseline video tokenizers by measuring their reconstruction quality (rFVD) in relation to the number of tokens used to encode 128-frame videos. The graph shows that CoordTok requires significantly fewer tokens to achieve a comparable rFVD score, indicating superior compression efficiency for long videos.", "section": "3.2. Long video tokenization"}, {"figure_path": "https://arxiv.org/html/2411.14762/x8.png", "caption": "Figure 5: \nEfficient video tokenization improves video generation.\nWe report FVDs of SiT-L/2 models trained upon CoordTok with token sizes of 1280 and 3072. \u2193\u2193\\downarrow\u2193 indicates lower values are better.", "description": "This figure demonstrates the impact of efficient video tokenization on video generation quality.  Two SiT-L/2 models were trained using CoordTok, a novel video tokenizer. One model used 1280 tokens per video, while the other used 3072.  The Fr\u00e9chet Video Distance (FVD) metric, lower values indicating better generation quality, was used to assess the generated videos.  The results show that the model trained with fewer tokens (1280) achieved significantly better video generation quality, highlighting CoordTok's effectiveness in reducing computational requirements without sacrificing performance.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.14762/x9.png", "caption": "Figure 6: Unconditional 128-frame, 128\u00d7\\times\u00d7128 resolution video generation results from CoordTok-SiT-L/2 trained on 128-frame videos from the UCF-101 dataset [40].\nWe provide more examples of generated videos in Appendix\u00a0D.", "description": "This figure showcases the results of unconditional video generation using the CoordTok-SiT-L/2 model.  The model was trained on 128-frame video clips from the UCF-101 dataset.  The figure displays sample frames from a generated video, demonstrating the model's ability to produce coherent and visually plausible long video sequences. Additional examples are provided in Appendix D.", "section": "3.3. Long video generation"}, {"figure_path": "https://arxiv.org/html/2411.14762/x10.png", "caption": "(a) Effect of Model size", "description": "This figure demonstrates the impact of varying model sizes on the performance of CoordTok.  The x-axis represents different model sizes (small, base, large).  The y-axis displays two metrics: rFVD (reconstruction Fr\u00e9chet video distance, lower is better) and PSNR (peak signal-to-noise ratio, higher is better).  By comparing the rFVD and PSNR values across different model sizes, we can see how model capacity influences the quality of video reconstruction.", "section": "3.4 Analysis and ablation studies"}, {"figure_path": "https://arxiv.org/html/2411.14762/x11.png", "caption": "(b) Effect of Triplane size (spatial)", "description": "This figure shows the impact of altering the spatial dimensions of the triplane representations within the CoordTok model on the reconstruction quality of videos.  It assesses how changing the size of the spatial dimensions (e.g., 16x16, 32x32, 64x64 pixels) within the triplane representations (zxy, zyt, zxt) affects the resulting PSNR and rFVD (Fr\u00e9chet Video Distance) scores.  Larger spatial dimensions generally lead to better reconstruction quality but could potentially increase computational cost.", "section": "3.4 Analysis and ablation studies"}, {"figure_path": "https://arxiv.org/html/2411.14762/x12.png", "caption": "(c) Effect of Triplane size (temporal)", "description": "This figure shows the effect of varying the temporal dimension of the triplane representations in CoordTok on video reconstruction quality.  The x-axis represents different temporal sizes (e.g., 16x8, 16x16, 16x32), while the y-axis shows the reconstruction quality measured by rFVD (lower is better) and PSNR (higher is better). It demonstrates how the model's performance changes with different choices for temporal dimension, highlighting the optimal settings for balancing performance and efficiency.", "section": "3.4. Analysis and ablation studies"}, {"figure_path": "https://arxiv.org/html/2411.14762/x13.png", "caption": "Figure 7: Analysis on the effect of (a) model size, (b) spatial dimensions of triplane representations, and (c) temporal dimensions of triplane representations.\nFor our main experiments, we use CoordTok-L with triplane representations of 16\u00d7\\times\u00d716 spatial dimensions and 32 temporal dimensions.\n\u2193\u2193\\downarrow\u2193 and \u2191\u2191\\uparrow\u2191 denote whether lower or higher values are better, respectively.", "description": "Figure 7 presents a comprehensive analysis of the impact of different design choices on CoordTok's performance.  Specifically, it explores the effect of (a) varying model size (CoordTok-S, CoordTok-B, CoordTok-L), (b) adjusting the spatial dimensions of the triplane representations (16x16, 32x32, 64x64), and (c) modifying the temporal dimensions of the triplane representations (8, 16, 32).  Each subfigure displays the relationship between a performance metric (rFVD and PSNR) and the specific design choice, enabling visualization of how modifications impact reconstruction quality.  The main experiments in the paper utilized CoordTok-L with 16x16 spatial and 32 temporal dimensions.  Arrows indicate whether lower or higher values on the y-axis are preferable for each metric.", "section": "3.4 Analysis and ablation studies"}, {"figure_path": "https://arxiv.org/html/2411.14762/x14.png", "caption": "(a) Effect of triplane representations.", "description": "This figure shows the impact of the triplane representation on the quality of video reconstruction.  The x-axis represents a video dynamics metric which measures how much the video changes frame to frame.  The y-axis shows the PSNR (Peak Signal-to-Noise Ratio) of the video reconstruction.  The figure compares the correlation between dynamics and reconstruction quality for CoordTok and two baseline models (TATS-AE and MaskGIT-AE). A strong negative correlation indicates that the model is better at reconstructing less dynamic videos.  CoordTok shows a stronger negative correlation than the baselines, suggesting it handles dynamic videos better.", "section": "3.4. Analysis and ablation studies"}, {"figure_path": "https://arxiv.org/html/2411.14762/x15.png", "caption": "(b) Effect of coordinate-based representations", "description": "This figure shows the correlation between the reconstruction quality (PSNR) and the frequency of video details.  The frequency magnitude, calculated using a Sobel edge detection filter, represents the fineness of video details. A higher frequency magnitude indicates finer details. The figure demonstrates that CoordTok's reconstruction quality is less sensitive to the frequency magnitude compared to baselines. This suggests CoordTok is less affected by the level of detail in the video because it learns a mapping directly from coordinates to pixels, rather than relying on the overall features of the video.", "section": "3.4 Analysis and ablation studies"}]