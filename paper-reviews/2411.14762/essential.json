{"importance": "This paper is important because **it addresses the critical challenge of efficiently processing long videos in computer vision**, a limitation of many existing models.  By introducing a novel video tokenizer, **CoordTok**, it enables more efficient training of large models on longer video sequences, leading to improved performance on various downstream tasks. This opens up **new avenues of research in video understanding and generation**, particularly for applications involving long videos.", "summary": "CoordTok: a novel video tokenizer drastically reduces token count for long videos, enabling memory-efficient training of diffusion models for high-quality, long video generation.", "takeaways": ["CoordTok efficiently encodes long videos using significantly fewer tokens than existing methods.", "Efficient tokenization with CoordTok allows memory-efficient training of diffusion models for long video generation.", "CoordTok effectively leverages temporal coherence in videos, improving reconstruction and generation quality."], "tldr": "Current video tokenization methods struggle with long videos due to high computational costs and memory constraints.  Training on short clips limits the exploitation of temporal coherence, impacting model performance.  This leads to limitations in video understanding and generation tasks involving longer video sequences.\nThis paper introduces CoordTok, a novel video tokenizer which learns a mapping from coordinate-based representations to video patches. This enables training on long videos without excessive resources, significantly reducing token counts and improving model performance. CoordTok's efficient tokenization enables memory-efficient training of diffusion transformers for high-quality long video generation, showcasing the benefits of exploiting the temporal coherence in longer sequences.", "affiliation": "KAIST", "categories": {"main_category": "Computer Vision", "sub_category": "Video Understanding"}, "podcast_path": "2411.14762/podcast.wav"}