{"references": [{" publication_date": "2019", "fullname_first_author": "Nazmiye Ceren Abay", "paper_title": "Privacy preserving synthetic data release using deep learning", "reason": "This paper is highly relevant to LlamaDuo's core methodology, which involves generating synthetic data for fine-tuning smaller LLMs.  Understanding privacy-preserving techniques for synthetic data generation is crucial for responsible and ethical model development, particularly when dealing with sensitive information that may be used in the training process.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "GPT-4 is a leading service LLM used in LlamaDuo's experiments and analysis. Understanding the technical details of GPT-4 is crucial for evaluating and comparing the performance of LlamaDuo's fine-tuned models against this state-of-the-art service LLM.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Anthropic", "paper_title": "The Claude 3 Model Family: Opus, Sonnet, Haiku", "reason": "Claude is another leading service LLM used for experiments in LlamaDuo.  Understanding its capabilities and characteristics helps in assessing the effectiveness of LlamaDuo in migrating knowledge and abilities from this service LLM to smaller, locally managed models.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Weilin Cai", "paper_title": "A survey on mixture of experts", "reason": "Mixture of experts (MoE) models are important for improving the efficiency of large models. This paper is highly relevant to LlamaDuo since MoE is one of the techniques that can make LlamaDuo even more cost-effective and scalable. By utilizing MoE, LlamaDuo can further increase the efficiency in terms of computation, memory usage, and power consumption.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yupeng Chang", "paper_title": "A survey on evaluation of large language models", "reason": "This paper provides a comprehensive overview of different evaluation methods for large language models (LLMs). Understanding these evaluation metrics is essential for assessing the performance of LlamaDuo's fine-tuned models and comparing them to service LLMs.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality", "reason": "Vicuna is one of the open-source models that LlamaDuo uses in its experiments.  Understanding Vicuna and its capabilities are important in comparing different LLMs and showing the effectiveness of LlamaDuo.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Aakanksha Chowdhery", "paper_title": "Palm: Scaling language modeling with pathways", "reason": "This paper explores the scaling laws of language models.  Understanding these laws is essential for LlamaDuo since LlamaDuo focuses on migrating knowledge from large LLMs to smaller ones. The scaling laws provide theoretical guidance for LlamaDuo's methodology.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Hyung Won Chung", "paper_title": "Scaling instruction-finetuned language models", "reason": "Instruction tuning is crucial for aligning LLMs with user instructions.  This paper provides insights into effective instruction-tuning strategies. These insights are highly relevant to LlamaDuo because LlamaDuo fine-tunes smaller models using instruction-based synthetic data.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Tim Dettmers", "paper_title": "Qlora: Efficient finetuning of quantized llms", "reason": "QLORA is the specific fine-tuning technique used in LlamaDuo.  This paper provides in-depth knowledge of its mechanism and benefits, which are essential to understand why this technique was selected in LlamaDuo and what its advantages are.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Gemini Team", "paper_title": "Gemini: a family of highly capable multimodal models", "reason": "Gemini is one of the service LLMs used in LlamaDuo's experiments and analysis.  Understanding Gemini and its multimodal capabilities is important for assessing the effectiveness of LlamaDuo in migrating knowledge from this service LLM to smaller, locally managed models.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Gemma Team", "paper_title": "Gemma: Open models based on gemini research and technology", "reason": "Gemma is one of the open-source models that LlamaDuo uses in its experiments. Understanding Gemma and its capabilities are important in comparing different LLMs and showing the effectiveness of LlamaDuo.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Z Guo", "paper_title": "Improving small language models on pubmedqa via generative data augmentation", "reason": "This paper demonstrates the effectiveness of generative data augmentation for improving small language models.  This is highly relevant to LlamaDuo since LlamaDuo uses a similar approach using synthetic data generated by service LLMs to improve the performance of smaller, local models.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Muhammad Usman Hadi", "paper_title": "A survey on large language models: Applications, challenges, limitations, and practical usage", "reason": "This paper provides a comprehensive overview of large language models (LLMs), including their applications, challenges, and limitations.  This is highly relevant to LlamaDuo since LlamaDuo addresses the challenges associated with using cloud-based service LLMs and provides a practical solution by migrating to smaller, local models.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "reason": "This paper explores the scaling laws of large language models and investigates ways to train compute-optimal models.  This is highly relevant to LlamaDuo because LlamaDuo aims to make the migration from large service LLMs to smaller, local models more cost-effective.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "Mistral is one of the open-source models that LlamaDuo uses in its experiments. Understanding Mistral and its capabilities are important in comparing different LLMs and showing the effectiveness of LlamaDuo.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Juyong Jiang", "paper_title": "A survey on large language models for code generation", "reason": "This paper provides a review of large language models (LLMs) for code generation. This is relevant to LlamaDuo because it demonstrates the applications of LLMs in specific domains and LlamaDuo demonstrates the ability to migrate these capabilities to local models.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "reason": "This paper is foundational to LlamaDuo because it explores the scaling relationships between model size, dataset size, and performance. Understanding these scaling laws is essential for designing effective strategies for fine-tuning smaller models to match or exceed the capabilities of larger ones.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Ruibo Liu", "paper_title": "Best practices and lessons learned on synthetic data for language models", "reason": "This paper discusses best practices and lessons learned in using synthetic data for training language models, providing valuable insights into generating high-quality synthetic data for LlamaDuo. The focus on lessons learned helps to avoid potential pitfalls and improve the efficiency and effectiveness of LlamaDuo's synthetic data generation process.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Shayne Longpre", "paper_title": "The flan collection: Designing data and methods for effective instruction tuning", "reason": "FLAN is an instruction-tuned model, and instruction tuning is highly relevant to LlamaDuo. This paper provides valuable insight into the design and effectiveness of instruction tuning, which is a key part of LlamaDuo's methodology for transferring capabilities from service LLMs to smaller models.", "section_number": 2}]}