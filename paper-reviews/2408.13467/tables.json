[{"figure_path": "2408.13467/tables/table_7_0.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents the performance comparison of several service and local LLMs on four downstream tasks, evaluated by three different service LLMs as judges.", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_10_0.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents the performance comparison of service LLMs and their local counterparts fine-tuned on a 128K synthetic dataset, evaluated across four downstream tasks.", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_17_0.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents the performance comparison of various service and local LLMs on four downstream tasks (summarization, classification, coding, and closed QA) using specific metrics (precision and similarity).", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_18_0.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT40, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents the performance comparison of service LLMs and their corresponding fine-tuned smaller local LLMs across four downstream tasks (summarization, classification, coding, and closed QA), evaluated by three different service LLMs as judges.", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_18_1.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents a quantitative comparison of the performance of various service and fine-tuned local LLMs across four downstream tasks (summarization, classification, coding, and closed QA), evaluated using three different service LLMs as judges.", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_20_0.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents the performance comparison of service LLMs and their locally fine-tuned counterparts across four downstream tasks, evaluated using three service LLMs as judges.", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_21_0.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents the performance comparison of service LLMs and their fine-tuned smaller local LLMs counterparts across four downstream tasks, evaluated by three different service LLMs.", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_22_0.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents the performance comparison of service LLMs and their corresponding fine-tuned smaller local LLMs across four downstream tasks, evaluated by three different service LLMs as judges.", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_22_1.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents the performance comparison of service LLMs and fine-tuned smaller local LLMs across four downstream tasks (summarization, classification, coding, and closed QA), evaluated by three different service LLMs.", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_23_0.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents a quantitative comparison of the performance of several service and locally managed LLMs across four distinct downstream tasks, using three different service LLMs as evaluators.", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_24_0.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents the performance comparison of various service and fine-tuned local LLMs across four downstream tasks, evaluated by three different service LLMs as judges.", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_24_1.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents the performance comparison of service LLMs and fine-tuned smaller local LLMs across four downstream tasks (summarization, classification, coding, and closed QA), evaluated by three different service LLMs.", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_25_0.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents a comparison of the performance of various service and fine-tuned local LLMs across four downstream tasks (summarization, classification, coding, and closed QA), evaluated by three different service LLMs.", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_26_0.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents the performance comparison of service LLMs and their corresponding fine-tuned smaller local LLMs across four downstream tasks, evaluated by three different service LLMs as judges.", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_27_0.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents the performance comparison of service LLMs and their fine-tuned smaller counterparts across four downstream tasks (summarization, classification, coding, and closed QA), evaluated using service LLMs as judges.", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_28_0.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents the performance comparison of service LLMs and their smaller, fine-tuned counterparts across four tasks, evaluated by three different service LLMs.", "section": "4.3 Experimental Results"}, {"figure_path": "2408.13467/tables/table_29_0.html", "caption": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by GPT40, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset. Each entry is presented as mean score / coverage percentage (%) with 50 score. Scores in Red are the best results from service LLMs, while the scores in Blue are the best results from local LLMs. Perf. Matching represents performance matching which is defined as the best performance of the local LLM divided by the service LLM, where the best results are in Pink.", "description": "Table 1 presents a comparative analysis of the performance of several service and fine-tuned local LLMs across four downstream tasks, evaluated by three different service LLMs as judges.", "section": "4.3 Experimental Results"}]