[{"figure_path": "https://arxiv.org/html/2412.15200/x1.png", "caption": "Figure 1: Given condition images, DI-PCG can accurately estimate suitable parameters of procedural generators, resulting high fidelity 3D asset creation. Textures and materials are randomly assigned by the procedural generators for visualizations.", "description": "This figure showcases the capabilities of DI-PCG, a diffusion-based inverse procedural content generation method.  Given various input images (the 'condition images'), DI-PCG successfully identifies the optimal parameters to feed into procedural 3D model generators.  These generators then produce high-fidelity 3D models. Note that the textures and materials used in the final 3D models are randomly selected by the procedural generators themselves; this is purely for visualization purposes and does not impact the accuracy of the parameter estimation.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.15200/x2.png", "caption": "Figure 2: Overview of DI-PCG. (Left) The procedural generator consists of programs and parameters, and can be randomly sampled to produce various shapes. (Right) To control it with images, DI-PCG trains a denoising diffusion model directly upon canonicalized generator parameters, using DINOv2 to extract condition image features and inject them via cross attention. The resulting parameters are projected back to original ranges and then fed into the generator, delivering high-quality 3D generation with neat geometry and meshing.", "description": "This figure illustrates the DI-PCG framework. The left panel shows a procedural generator that takes parameters as input and produces 3D shapes.  The parameters are randomly sampled to generate a variety of shapes. The right panel shows how DI-PCG uses a denoising diffusion model to learn the relationship between images and the procedural generator's parameters.  Given a condition image, DI-PCG uses DINOv2 to extract image features, which are then fed into the diffusion model via cross-attention to guide the generation of parameters. These parameters are then transformed back to their original range before being used by the generator to create high-quality 3D assets with clean geometry and meshing.", "section": "3.2 Diffusion Model for Inverse PCG"}, {"figure_path": "https://arxiv.org/html/2412.15200/x3.png", "caption": "Figure 3: Qualitative results for chair, table, and vase generations. Input images are collected from the internet.", "description": "Figure 3 showcases the qualitative results of the DI-PCG model for generating 3D models of chairs, tables, and vases.  For each object category, the figure displays a series of input images sourced from the internet and the corresponding 3D models generated by DI-PCG. This visualization demonstrates DI-PCG's ability to generate high-fidelity 3D assets from a range of diverse image conditions, highlighting its effectiveness in image-to-3D translation and inverse procedural content generation.", "section": "4.1. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.15200/x4.png", "caption": "Figure 4: Qualitative comparisons of DI-PCG with baselines.", "description": "This figure shows a qualitative comparison of DI-PCG against other state-of-the-art 3D reconstruction and generation methods.  The comparison uses the same input image for each method to highlight the differences in generated 3D chair models. Baselines include Shap-E, SDFusion, Michelangelo, CraftsMan, and InstantMesh. The figure demonstrates DI-PCG's ability to generate high-fidelity 3D models that accurately reflect the input image, while the baselines show various shortcomings in terms of alignment, detail preservation, or overall model quality.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.15200/x5.png", "caption": "Figure 5: Sketch-conditioned generation results. Textures and materials are randomly picked by the procedural generators.", "description": "This figure showcases the results of 3D model generation using sketches as input conditions.  The models are created by a procedural generator, meaning they are algorithmically constructed based on rules and parameters.  Importantly, the textures and materials applied to the 3D models are chosen randomly by the procedural generator, demonstrating the system's ability to produce diverse results even with the same basic sketch input.", "section": "4.1. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.15200/x6.png", "caption": "Figure 6: Example of comparison with MCMC method.", "description": "This figure compares the results of the proposed DI-PCG method to a Markov Chain Monte Carlo (MCMC) method for inverse procedural content generation. It shows how the quality of 3D model generation improves with more iterations in MCMC, but at significantly higher time cost.  DI-PCG, in contrast, produces comparable results in only a few seconds.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.15200/x7.png", "caption": "Figure 7: DI-PCG supports easy editing by simply adjusting corresponding parameters.", "description": "Figure 7 shows how easy it is to edit 3D models generated by DI-PCG by simply modifying the corresponding parameters of the procedural generator.  The example shows various modifications applied to a chair model, illustrating changes such as leg thickness, height, presence of armrests, and overall width, all achieved by adjusting specific parameters within the generator. This highlights the ease of control offered by DI-PCG and its potential for efficient design and customization.", "section": "4.4 Editing Application"}]