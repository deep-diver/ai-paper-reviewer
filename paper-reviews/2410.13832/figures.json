[{"figure_path": "2410.13832/figures/figures_1_0.png", "caption": "Fig. 1. Given a casually-captured panning video, our method synthesizes a coherent panoramic video, depicting the full dynamic scene. Our framework projects the input video on top of a panoramic canvas and harnesses a generative video model to synthesize realistic and consistent dynamic content in the unknown regions. Note that the kayaker's paddle moves realistically, even when it is out of frame in the input video.", "description": "The figure shows a casually-captured panning video, its projection onto a panoramic canvas, and the resulting synthesized panoramic video.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13832/figures/figures_3_0.png", "caption": "Fig. 2. Temporal coarse-to-fine. The input video (a) is projected on to a unified panoramic canvas using estimated camera parameters. The reprojected input video (b) is temporally downsampled with temporal prefiltering. A base panoramic video is synthesized at the coarsest temporal scale (top), then gradually refined by temporal upsampling, merging, and resynthesis (c). Finally, a spatial super-resolution pass is applied and the original input pixels are merged with the result to produce the output video (d).", "description": "This figure illustrates the temporal coarse-to-fine approach used in the VidPanos system for generating panoramic videos from casually captured panning videos.", "section": "3 METHOD"}, {"figure_path": "2410.13832/figures/figures_4_0.png", "caption": "Fig. 2. Temporal coarse-to-fine. The input video (a) is projected on to a unified panoramic canvas using estimated camera parameters. The reprojected input video (b) is temporally downsampled with temporal prefiltering. A base panoramic video is synthesized at the coarsest temporal scale (top), then gradually refined by temporal upsampling, merging, and resynthesis (c). Finally, a spatial super-resolution pass is applied and the original input pixels are merged with the result to produce the output video (d).", "description": "The figure illustrates the temporal coarse-to-fine method used to synthesize a complete panoramic video from a casually-captured panning video.", "section": "3 METHOD"}, {"figure_path": "2410.13832/figures/figures_4_1.png", "caption": "Fig. 4. Spatial aggregation of predicted distributions. To generate a sample in the overlap (red), we linearly interpolate the two predicted probability distributions (purple, orange) and sample from the aggregated distribution (brown). With a token-based method the distribution is a discrete distribution over the vocabulary. With diffusion, the distribution is a Gaussian distribution over pixel values, represented by \u00b5 and \u03a3.", "description": "The figure illustrates how spatial aggregation is performed for both token-based and diffusion-based video generation models by averaging overlapping window predictions.", "section": "3 METHOD"}, {"figure_path": "2410.13832/figures/figures_6_0.png", "caption": "Fig. 5. Comparison with baseline methods. From top to bottom: linear interpolation between pixels based on time produces sharp results for stationary regions, but does not interpolate motion. ProPainter [Zhou et al. 2023] and E2FGVI [Li et al. 2022] are flow-based methods that can produce realistic results in stationary regions (scuba, Bangkok), but fail for moving cameras (skate, ski) or moving objects away from the input window (divers on left in scuba). MAGVIT [Yu et al. 2023] is a video-generation method but does not generate on a common panorama canvas, so it loses information away from the input window. Our results use a coarse-to-fine approach to build a consistent panoramic video and better match the ground-truth. Bottom: ground truth video with input window marked in yellow. See supplemental material for video results.", "description": "Figure 5 compares the proposed method with four baseline methods on four example videos, demonstrating the superior performance of the proposed method in handling both static and dynamic content in panoramic video generation.", "section": "4 Results"}, {"figure_path": "2410.13832/figures/figures_6_1.png", "caption": "Fig. 6. Comparison with Panoramic Video Textures [Agarwala et al. 2005]. PVT uses a graph-cut formulation to create a looping panoramic video. Our method can create similar videos, but can also include non-stationary features like the person walking behind the waterfall (boxed).", "description": "Figure 6 compares the results of the proposed method with the Panoramic Video Textures method, highlighting the ability of the proposed method to handle non-stationary features.", "section": "4 Results"}, {"figure_path": "2410.13832/figures/figures_6_2.png", "caption": "Fig. 6. Comparison with Panoramic Video Textures [Agarwala et al. 2005]. PVT uses a graph-cut formulation to create a looping panoramic video. Our method can create similar videos, but can also include non-stationary features like the person walking behind the waterfall (boxed).", "description": "Figure 6 compares the results of the proposed method with the Panoramic Video Textures method, highlighting the ability of the proposed method to handle non-stationary features.", "section": "4.2 Quantitative Evaluation"}, {"figure_path": "2410.13832/figures/figures_7_0.png", "caption": "Fig. 7. Results on synthetic panning videos. Left: Phenaki model results. Middle: Lumiere model results. Right: ground-truth panoramic video captured with wide-angle camera. Darkened boxed area is the input window shown to the model. Please see supplemental material for full video results.", "description": "Figure 7 shows the results of the proposed method on synthetic panning videos using two different video generation models, Phenaki and Lumiere, and compares them to ground truth panoramic videos.", "section": "4 Results"}, {"figure_path": "2410.13832/figures/figures_8_0.png", "caption": "Fig. 8. Results on real videos. Left: representative input frames. Middle: frames projected to panorama canvas. Right: our result. Our method synthesizes realistic motions for an unseen person entering the frame (top), ocean waves (middle), and for scenery around a moving camera (bottom). See supplemental material for videos.", "description": "Figure 8 shows the results of applying the VidPanos method to real-world panning videos, demonstrating its ability to synthesize realistic motions and complete panoramic views.", "section": "4 Results"}, {"figure_path": "2410.13832/figures/figures_9_0.png", "caption": "Fig. 9. Naive Lumiere vs. Ours. Left: Lumiere without panorama mask finetuning or temporal coarse-to-fine. Right: our result. Compare with our full method and ground-truth in Fig. 5.", "description": "The figure compares the results of the naive Lumiere model and the proposed method on four example videos, showing the improvements in visual quality and consistency achieved by the proposed method.", "section": "4 Results"}, {"figure_path": "2410.13832/figures/figures_9_1.png", "caption": "Fig. 10. Ablation of Temporal Coarse-to-Fine. Coarse-to-Fine synthesis (right) generates more consistent results over long videos than temporal MultiDiffusion (middle). With temporal MultiDiffusion, later generations can drift from the input pixels (orange box), while coarse-to-fine generates a plausible continuation of the pedestrian. Input pixels shown darkened.", "description": "Figure 10 compares the results of using temporal MultiDiffusion versus temporal coarse-to-fine methods for video generation, showing that coarse-to-fine produces more temporally consistent results.", "section": "4.5 Ablations"}, {"figure_path": "2410.13832/figures/figures_11_0.png", "caption": "Fig. 1. Given a casually-captured panning video, our method synthesizes a coherent panoramic video, depicting the full dynamic scene. Our framework projects the input video on top of a panoramic canvas and harnesses a generative video model to synthesize realistic and consistent dynamic content in the unknown regions. Note that the kayaker's paddle moves realistically, even when it is out of frame in the input video.", "description": "The figure shows a casually captured panning video as input, its projection onto a panoramic canvas, and the resulting generated panoramic video output.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13832/figures/figures_12_0.png", "caption": "Fig. 7. Results on synthetic panning videos. Left: Phenaki model results. Middle: Lumiere model results. Right: ground-truth panoramic video captured with wide-angle camera. Darkened boxed area is the input window shown to the model. Please see supplemental material for full video results.", "description": "Figure 7 presents a comparison of panoramic video generation results using Phenaki and Lumiere models against ground truth for four synthetic panning video examples.", "section": "4 Results"}, {"figure_path": "2410.13832/figures/figures_13_0.png", "caption": "Fig. 1. Given a casually-captured panning video, our method synthesizes a coherent panoramic video, depicting the full dynamic scene. Our framework projects the input video on top of a panoramic canvas and harnesses a generative video model to synthesize realistic and consistent dynamic content in the unknown regions. Note that the kayaker's paddle moves realistically, even when it is out of frame in the input video.", "description": "The figure shows the input panning video, the input projected onto a panoramic canvas, and the generated panoramic video, illustrating the system's ability to synthesize realistic and coherent panoramic videos from casually captured panning videos.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13832/figures/figures_14_0.png", "caption": "Fig. 7. Results on synthetic panning videos. Left: Phenaki model results. Middle: Lumiere model results. Right: ground-truth panoramic video captured with wide-angle camera. Darkened boxed area is the input window shown to the model. Please see supplemental material for full video results.", "description": "Figure 7 presents a comparison of video panorama generation results from two different models (Phenaki and Lumiere) against ground truth for four example videos, showing the models' ability to generate realistic and consistent content in regions outside of the input.", "section": "4 Results"}]